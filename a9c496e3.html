<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon/favicon_io/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon/favicon_io/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon/favicon_io/favicon-16x16.png">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.28/fancybox/fancybox.css" integrity="sha256-6cQIC71/iBIYXFK+0RHAvwmjwWzkWd+r7v/BX3/vZDc=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"saicat.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="【本文已在同名 微信公众号 &#x2F; 知乎 &#x2F; 个人博客linsight.cn 上线】  DeepSeek-R1以一己之力正面刚OpenAI和Anthropic。DeepSeek-R1能有这么强力的表现和DeepSeek-V3这个基模型的强大是分不开的。">
<meta property="og:type" content="article">
<meta property="og:title" content="DeepSeek-V3细节探索">
<meta property="og:url" content="https://saicat.github.io/a9c496e3.html">
<meta property="og:site_name" content="Linsight">
<meta property="og:description" content="【本文已在同名 微信公众号 &#x2F; 知乎 &#x2F; 个人博客linsight.cn 上线】  DeepSeek-R1以一己之力正面刚OpenAI和Anthropic。DeepSeek-R1能有这么强力的表现和DeepSeek-V3这个基模型的强大是分不开的。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/perf.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/ds3_archi.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/ds3_MLA.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/MLA_formula.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/MLA_cache.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/mtp_example.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/mtp_order.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/mtp_code_result.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/mtp_exps.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/mtp_sft.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/mtp_nlp_benchmark.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/mtp_summary.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/mtp_archi.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/ds3_mtp_module.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/BFD_FFD.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/best_fit_packing.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/packing_padding.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/bfp_perf1.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/bfp_perf2.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/token_boundary_1.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/token_boundary_2.png">
<meta property="og:image" content="https://saicat.github.io/a9c496e3/token_boundary_3.png">
<meta property="og:image" content="https://saicat.github.io/images/qrcode.jpg">
<meta property="og:image" content="https://saicat.github.io/images/wechat.png">
<meta property="article:published_time" content="2025-01-29T15:12:34.000Z">
<meta property="article:modified_time" content="2025-02-02T08:20:38.248Z">
<meta property="article:author" content="Lin">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="transformer">
<meta property="article:tag" content="SFT">
<meta property="article:tag" content="DeepSeek">
<meta property="article:tag" content="pretrain">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://saicat.github.io/a9c496e3/perf.png">


<link rel="canonical" href="https://saicat.github.io/a9c496e3.html">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://saicat.github.io/a9c496e3.html","path":"a9c496e3.html","title":"DeepSeek-V3细节探索"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>DeepSeek-V3细节探索 | Linsight</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Linsight</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">聊聊AI技术，也聊聊其他的</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#mla"><span class="nav-number">1.</span> <span class="nav-text">MLA</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%8Emha%E5%87%BA%E5%8F%91"><span class="nav-number">1.1.</span> <span class="nav-text">从MHA出发</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mla-1"><span class="nav-number">1.2.</span> <span class="nav-text">MLA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%BC%E5%AE%B9rope"><span class="nav-number">1.3.</span> <span class="nav-text">兼容RoPE</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#moe"><span class="nav-number">2.</span> <span class="nav-text">MoE</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E7%BB%93%E6%9E%84"><span class="nav-number">2.1.</span> <span class="nav-text">基础结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9F%E8%BD%BD%E5%B9%B3%E8%A1%A1"><span class="nav-number">2.2.</span> <span class="nav-text">负载平衡</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#no-token-dropping"><span class="nav-number">2.3.</span> <span class="nav-text">No Token-Dropping</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#multi-token-prediction"><span class="nav-number">3.</span> <span class="nav-text">Multi-Token Prediction</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8E%9F%E5%A7%8B%E7%9A%84mtp"><span class="nav-number">3.1.</span> <span class="nav-text">原始的MTP</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#deepseek-v3%E4%B8%AD%E7%9A%84mtp"><span class="nav-number">3.2.</span> <span class="nav-text">DeepSeek-V3中的MTP</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%9E%84%E5%BB%BA"><span class="nav-number">4.</span> <span class="nav-text">数据构建</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#document-packing"><span class="nav-number">4.1.</span> <span class="nav-text">document packing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fill-in-middlefim"><span class="nav-number">4.2.</span> <span class="nav-text">Fill-in-Middle（FIM）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tokenizer%E5%92%8Ctoken-boundary"><span class="nav-number">4.3.</span> <span class="nav-text">tokenizer和token boundary</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E8%AE%BE%E7%BD%AE"><span class="nav-number">5.</span> <span class="nav-text">训练设置</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="nav-number">5.1.</span> <span class="nav-text">预训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9%E9%BD%90"><span class="nav-number">5.2.</span> <span class="nav-text">对齐</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93"><span class="nav-number">6.</span> <span class="nav-text">小结</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reference"><span class="nav-number">7.</span> <span class="nav-text">Reference</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Lin"
      src="/images/avatar/Picasso_Elephant.png">
  <p class="site-author-name" itemprop="name">Lin</p>
  <div class="site-description" itemprop="description">AI | NLP</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">89</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">79</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="mailto:331603034@qq.com" title="E-Mail → mailto:331603034@qq.com" rel="noopener me" target="_blank"><i class="fa-regular fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

<!--
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/1.js?i=5acfv0hqzp5&amp;s=220&amp;m=1&amp;v=false&amp;r=false&amp;b=000000&amp;n=false&amp;c=ff0000" async="async"></script>
-->

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://saicat.github.io/a9c496e3.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar/Picasso_Elephant.png">
      <meta itemprop="name" content="Lin">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Linsight">
      <meta itemprop="description" content="AI | NLP">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="DeepSeek-V3细节探索 | Linsight">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          DeepSeek-V3细节探索
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-01-29 23:12:34" itemprop="dateCreated datePublished" datetime="2025-01-29T23:12:34+08:00">2025-01-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-02-02 16:20:38" itemprop="dateModified" datetime="2025-02-02T16:20:38+08:00">2025-02-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CS/" itemprop="url" rel="index"><span itemprop="name">CS</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CS/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CS/NLP/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>15k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>28 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>【本文已在同名 微信公众号 / 知乎 / <a target="_blank" rel="noopener" href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>DeepSeek-R1以一己之力正面刚OpenAI和Anthropic。DeepSeek-R1能有这么强力的表现和DeepSeek-V3这个基模型的强大是分不开的。</p>
<img src="/a9c496e3/perf.png" class title="dsv3">
<p>现在就来盘一下DeepSeek-V3的一些细节。（不包括infra部分）</p>
<p>相关文章链接：</p>
<p><a target="_blank" rel="noopener" href="http://www.linsight.cn/44e38c1b.html">DeepSeekMoE</a></p>
<p><a target="_blank" rel="noopener" href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2</a></p>
<p><a target="_blank" rel="noopener" href="https://www.linsight.cn/9e4b4e6d.html">DeepSeek-R1详解</a></p>
<h1 id="mla">MLA</h1>
<p>DeepSeek-V3模型的基础架构和V2一样：</p>
<img src="/a9c496e3/ds3_archi.png" class title="dsv3">
<p>先来看下MLA是怎么做的。（很熟悉MLA的朋友可以跳过这部分）</p>
<h2 id="从mha出发">从MHA出发</h2>
<p>先回顾下标准的MHA。假设 <span class="math inline">\(n_h\)</span>
是注意力头的数量，<span class="math inline">\(d_h\)</span>
是每个注意力头的大小，<span class="math inline">\(\mathbf{h}_{t}\in\mathbb{R}^{d}\)</span>
是第t个输入token。</p>
<p>MHA首先通过三个投影矩阵 <span class="math inline">\(W^{Q},W^{K},W^{V}\in\mathbb{R}^{d_{h}n_{h}\times
d}\)</span> 获得<span class="math inline">\(\mathbf{q}_t,\mathbf{k}_t,\mathbf{v}_t\in\mathbb{R}^{d_hn_h}\)</span>：</p>
<p><span class="math display">\[\mathbf{q}_t=W^Q\mathbf{h}_t\]</span></p>
<p><span class="math display">\[\mathbf{k}_t=W^K\mathbf{h}_t\]</span></p>
<p><span class="math display">\[\mathbf{v}_t=W^V\mathbf{h}_t\]</span></p>
<p>之后 <span class="math inline">\(\mathbf{q}_t,\mathbf{k}_t,\mathbf{v}_t\)</span>
就会被切成 <span class="math inline">\(n_h\)</span>
份，分别进行注意力计算：</p>
<p><span class="math display">\[[\mathbf{q}_{t,1};\mathbf{q}_{t,2};...;\mathbf{q}_{t,n_{h}}]=\mathbf{q}_{t}\]</span></p>
<p><span class="math display">\[[\mathbf{k}_{t,1};\mathbf{k}_{t,2};...;\mathbf{k}_{t,n_{h}}]=\mathbf{k}_{t}\]</span></p>
<p><span class="math display">\[[\mathbf{v}_{t,1};\mathbf{v}_{t,2};...;\mathbf{v}_{t,n_{h}}]=\mathbf{v}_{t}\]</span></p>
<p><span class="math display">\[\mathbf{o}_{t,i}=\sum_{j=1}^t\mathrm{Softmax}_j(\frac{\mathbf{q}_{t,i}^T\mathbf{k}_{j,i}}{\sqrt{d_h}})\mathbf{v}_{j,i}\]</span></p>
<p><span class="math display">\[\mathbf{u}_t=W^O[\mathbf{o}_{t,1};\mathbf{o}_{t,2};...;\mathbf{o}_{t,n_h}]\]</span></p>
<p>其中 <span class="math inline">\(\mathbf{q}_{t,i},\mathbf{k}_{t,i},\mathbf{v}_{t,i}\in\mathbb{R}^{d_{h}}\)</span>，<span class="math inline">\(W^O\in\mathbb{R}^{d\times d_hn_h}\)</span>。</p>
<p>在推理的时候，为了加速，会对前面已经计算过的K、V值进行缓存，那么每个token在模型每层就要保存
<span class="math inline">\(2{n}_{h}{d}_{h}\)</span> 个数值。</p>
<p>那么要减少缓存的量，一个方法就是减少使用的K/V。GQA/MQA就是通过共享参数减少K、V头的数量并重复使用，从而减少了需要缓存的KV的量。</p>
<h2 id="mla-1">MLA</h2>
<p>MLA通过对K和V做low-rank joint compression来压缩KV
cache，理论上可以更有效地压缩KV缓存值。</p>
<img src="/a9c496e3/ds3_MLA.png" class title="MLA">
<p>下面看下MLA具体是怎么做的。</p>
<p>在MHA中，K和V是对 <span class="math inline">\(h_t\)</span>
分别用投影矩阵进行变化得到的，而MLA把KV的变换改成使用一个共用的down-projection
matrix和两个up-projection matrices进行操作：</p>
<p><span class="math display">\[\mathbf{c}_t^{KV}=W^{DKV}\mathbf{h}_t\]</span></p>
<p><span class="math display">\[\mathbf{k}_t^C=W^{UK}\mathbf{c}_t^{KV}\]</span></p>
<p><span class="math display">\[\mathbf{v}_t^C=W^{UV}\mathbf{c}_t^{KV}\]</span></p>
<p><span class="math inline">\(\mathfrak{c}_t^{KV}\in\mathbb{R}^{d_c}\)</span>
就是K和V的compressed latent vector，这也是推理时要缓存的部分。</p>
<p>这里相当于把MHA中的 <span class="math inline">\(W^{K},W^{V}\)</span>
拆成两个矩阵：</p>
<p><span class="math display">\[\mathbf{k}_t=W^K\mathbf{h}_t\rightarrow
W^{UK}W^{DKV}\mathbf{h}_t\]</span></p>
<p><span class="math display">\[\mathbf{v}_t=W^V\mathbf{h}_t\rightarrow
W^{UV}W^{DKV}\mathbf{h}_t\]</span></p>
<p><span class="math inline">\(d_c\)</span> 是KV的压缩维度，让 <span class="math inline">\(d_c\ll
d_hn_h\)</span>，就可以大大减少需要推理时需要缓存的数据量。</p>
<p>再看回attention计算，在得到q、k、v之后，会计算权重矩阵并获得最终注意力输出结果：</p>
<p><span class="math display">\[\operatorname{Attention}(Q,K,V)=\operatorname{softmax}(\frac{Q^TK}{\sqrt{d}})V\]</span></p>
<p>而 <span class="math inline">\(Q^TK=H^T(W^Q)^TW^{UK}C\)</span>，因此
<span class="math inline">\(W^{UK}\)</span> 可以被吸收进 <span class="math inline">\(W^{Q}\)</span>
中，而不用在计算时显式算出K，只需调整 <span class="math inline">\(W^Q\)</span> 的shape后直接输入C即可。同理 <span class="math inline">\(W^{UV}\)</span> 可以被吸收进 <span class="math inline">\(W^{O}\)</span>。实操上，这样的矩阵合并可能会带来一些精度损失，这是一个值得注意的问题。</p>
<p>此外，MLA还对Q也做了low-rank compression，跟对K、V的操作类似：</p>
<p><span class="math display">\[\mathbf{c}_t^Q=W^{DQ}\mathbf{h}_t,\\\mathbf{q}_t^C=W^{UQ}\mathbf{c}_t^Q,\]</span></p>
<p>关于对Q进行压缩的原因，虽然V2原文说的是为了减少训练时的activation，但是两个矩阵所得的activation按道理应该比直接使用单个投影矩阵还要多一些。这里Q的压缩更可能是为了减少参数量和梯度，而非激活值。</p>
<h2 id="兼容rope">兼容RoPE</h2>
<p>到这里似乎MLA已经完成了，即减少了缓存的量，也不用引入其他overhead（两个up-projection
matrices都不用算了）。</p>
<p>但是实际上还有一个问题没有解决：位置编码使用的是RoPE，而RoPE是通过在Q、K上乘一个旋转矩阵来编码位置的。</p>
<p>而在上面MLA的设计中，已经没有显式计算K了，而RoPE也不能加在latent
vector上。一个方法是重新把K和V显式计算出来，但是这样计算量就会增加，MLA的推理加速效果就会打折扣。</p>
<p>针对这个问题，解决方案是使用decoupled RoPE：使用额外的multi-head
queries <span class="math inline">\(\mathbf{q}_{t,i}^R\in\mathbb{R}^{d_h^R}\)</span>
和一个shared key <span class="math inline">\(\mathbf{k}_t^R\in\mathbb{R}^{d_h^R}\)</span>
来携带RoPE的位置信息，<span class="math inline">\(d_h^R\)</span>
是decoupled queries的维度。</p>
<p>新增的q和k维度使用常规的RoPE计算，用于携带位置信息；而原来的维度依然使用低秩分解的方式计算，最后再计算attention的时候两个部分拼接起来。</p>
<p>最终完整的MLA计算如下</p>
<img src="/a9c496e3/MLA_formula.png" class title="MLA公式">
<p>蓝框中的部分就是推理时需要缓存的内容。</p>
<p>MLA所需的缓存量约等于组数为2.5的GQA：</p>
<img src="/a9c496e3/MLA_cache.png" class title="MLA缓存量">
<h1 id="moe">MoE</h1>
<h2 id="基础结构">基础结构</h2>
<p>DeepSeek-V3的MoE结构设计和DeepSeekMoE/DeepSeek-V2基本一致。和V2相比，有一些设置是一样的：</p>
<ul>
<li>初始化 standard deviation = 0.006<br>
</li>
<li>128个attention head，head size = 128<br>
</li>
<li>KV的compression dimension dc = 512<br>
</li>
<li>Q的compression dimension dc' = 1536<br>
</li>
<li>decoupled queries and key per head dimension = 64</li>
</ul>
<p>此外，也有一些具体设置和V2相比有变化：<br>
- layers = 61（比V2多1层）<br>
- hidden dimension = 7168（比V2的5120更大）<br>
- 前3层不使用MoE<br>
- 1个共享专家 +
8/256个路由专家，专家大小为2048（更多专家，专家维度更大）<br>
- 每个token最多只会被分发到4个节点<br>
- 总参数671B，激活参数37B<br>
- gating在计算affinity
score的时候先用sigmoid函数，再在选定的分数上进行归一化，而V2是直接使用softmax</p>
<p>V2的总参数为236B，激活参数为21B；而V3的总参数为671B，激活参数为37B。可以看到相比V2，V3多的参数主要在模型宽度和专家数量，而且MoE的激活更为稀疏。</p>
<h2 id="负载平衡">负载平衡</h2>
<p>1、Auxiliary-Loss-Free Load Balancing</p>
<p>先看下V3的MoE计算：</p>
<p><span class="math display">\[\mathbf{h}_t^{\prime}=\mathbf{u}_t+\sum_{i=1}^{N_s}\mathrm{FFN}_i^{(s)}\left(\mathbf{u}_t\right)+\sum_{i=1}^{N_r}g_{i,t}\mathrm{FFN}_i^{(r)}\left(\mathbf{u}_t\right)\]</span></p>
<p>第一项来自残差连接，第二项是共享专家的输出，第三项是路由专家的输出；Ns是shared
expert的数量，Nr是routed expert的数量，DeepSeek-V3中Ns=1，Nr=128。</p>
<p><span class="math display">\[g_{i,t}=\frac{g_{i,t}^\prime}{\sum_{j=1}^{N_r}g_{j,t}^\prime}\]</span></p>
<p>g'只保留top Nr个（DeepSeek-V3中Nr=8），其他都置零了。</p>
<p><span class="math display">\[g_{i,t}^{\prime}=\begin{cases}s_{i,t},&amp;s_{i,t}\in\mathrm{Topk}(\{s_{j,t}|1\leqslant
j\leqslant
N_r\},K_r)\\0,&amp;\text{otherwise}&amp;&amp;\end{cases}\]</span></p>
<p><span class="math display">\[s_{i,t}=\mathrm{Sigmoid}\left(\mathbf{u}_t{}^T\mathbf{e}_i\right)\]</span></p>
<p>Kr是activated routed expert的数量。</p>
<p>之前的版本使用auxiliary loss来对top affinity
score的分配不平衡进行惩罚，以此来缓解专家分配不平衡的问题。由于auxiliary
loss的设计并不关注模型的效果，因此过大的权重会对模型的训练效果产生损害。</p>
<p>为了避免模型效果的损失，DeepSeek-V3不使用auxiliary
loss来平衡负载，而是在affinity score上加了一个bias term，这个bias
term和expert是一一对应的：</p>
<p><span class="math display">\[g_{i,t}^{\prime}=\begin{cases}s_{i,t},&amp;s_{i,t}+b_i\in\mathrm{Topk}(\{s_{j,t}+b_j|1\leqslant
j\leqslant
N_r\},K_r)\\0,&amp;\text{otherwise}&amp;\end{cases}\]</span></p>
<p>这个bias term只用于routing，不用于和FFN的结果相乘输出专家的feature
vector。在每个训练step后，如果一个expert的负载过大了，就会把对应的bias
term减小𝛾，反之则把bias
term的数值增大𝛾。𝛾是个超参，控制负载平衡系统的变化速度。</p>
<p>2、Complementary Sequence-Wise Auxiliary Loss</p>
<p>虽然加了bias
term控制负载均衡，但是为了防止极端不平衡状况的出现，还是额外加了一个Auxiliary
Loss。</p>
<p>complementary sequence-wise balance loss是这么算的：</p>
<p><span class="math display">\[\mathcal{L}_\mathrm{Bal}=\alpha\sum_{i=1}^{N_r}f_iP_i\]</span></p>
<p>其中</p>
<p><span class="math display">\[P_i=\frac{1}{T}\sum_{t=1}^Ts_{i,t}^{\prime}\]</span></p>
<p>s'其实就是归一化的affinity score</p>
<p><span class="math display">\[s_{i,t}^\prime=\frac{s_{i,t}}{\sum_{j=1}^{N_r}s_{j,t}}\]</span></p>
<p>另外</p>
<p><span class="math display">\[f_i=\frac{N_r}{K_rT}\sum_{t=1}^T\mathbb{1}\left(s_{i,t}\in\mathrm{Topk}(\{s_{j,t}|1\leqslant
j\leqslant N_r\},K_r)\right)\]</span></p>
<p>求和部分其实就是某个token是否选择了expert i。训练中𝛼 = 0.0001。</p>
<p>fi是不可导的，Pi是可导的。</p>
<p>在完美负载平衡的情况下，affinity
score均匀分配，每个expert的得分相同，那么有</p>
<p><span class="math display">\[P_i=\frac{1}{T}\times T\times
\frac{1}{N_r}=\frac{1}{N_r}\]</span></p>
<p><span class="math display">\[f_i=\frac{N_r}{K_rT}\sum_{t=1}^T\frac{K_r}{N_r}=1\]</span></p>
<p>那么</p>
<p><span class="math display">\[\mathcal{L}_\mathrm{Bal}=\alpha\sum_{i=1}^{N_r}\frac{1}{N_r}=\alpha\]</span></p>
<p>complementary sequence-wise balance
loss其实就是DeepSeekMoE中的expert-level balance loss。</p>
<p>而在极端不平衡的情况下，比如所有token都选择了前Kr个expert激活，那么对于激活的expert
i，有</p>
<p><span class="math display">\[P_i=\frac{1}{T}\times T\times
1=1\]</span></p>
<p><span class="math display">\[f_i=\frac{N_r}{K_rT}\sum_{t=1}^T1=\frac{N_r}{K_r}\]</span></p>
<p>那么就有</p>
<p><span class="math display">\[\mathcal{L}_\mathrm{Bal}=\alpha\sum_{i=1}^{K_r}\frac{N_r}{K_r}=\alpha
N_r\]</span></p>
<p>3、Node-Limited Routing</p>
<p>在前面的基础上，最后还加了一个机制，限制每个token最多只能分发到M个节点上，而节点的选择是基于每个节点上的affinity
score的总和的。</p>
<p>举个例子，在Kr=8，M=4的情况下：</p>
<ul>
<li>如果8个得分最高的专家都分布在不同的node，那么只有top4个专家会被激活，其余的专家虽然得分排在top
Nr，但是由于激活节点的限制，不会被使用；<br>
</li>
<li>top8个专家分配在5个节点上：
<ul>
<li>节点1：0.1,0.1</li>
<li>节点2：0.1,0.1</li>
<li>节点3：0.1,0.1</li>
<li>节点4：0.25</li>
<li>节点5：0.15<br>
在这样的情况下，虽然节点5上的专家得分是第二高的，但是由于它所在的节点的得分总和不高，因此不会被激活</li>
</ul></li>
</ul>
<h2 id="no-token-dropping">No Token-Dropping</h2>
<p>由于前面的几个负载平衡策略基本上已经可以保持完全的负载平衡，因此DeepSeek-V3就不再使用token
dropping的策略了。</p>
<h1 id="multi-token-prediction">Multi-Token Prediction</h1>
<p>Multi-Token Prediction（MTP），顾名思义，在前向计算的时候一步可以预测
&gt;1 个token。</p>
<p>这样的多token预测策略可以在训练中使用，提升模型的远距离的理解能力；也可以用在推理中，加速inference输出，不过推理加速算是副产品了。</p>
<h2 id="原始的mtp">原始的MTP</h2>
<p>DeepSeek-V3中使用的MTP参考了24年4月的《Better &amp; Faster Large
Language Models via Multi-token
Prediction》，因此先来了解下这个工作。</p>
<p>1、MTP方案</p>
<p>标准的语言建模使用next-token
prediction，基于第1~t个token预测第t+1个token，loss是这样的：</p>
<p><span class="math display">\[\begin{aligned}L_1=-\sum_t\log
P_\theta(x_{t+1}\mid x_{t:1})\end{aligned}\]</span></p>
<p>和NTP不同，MTP要求模型在每一步要预测n个token，即第t+1~t+n个token，loss就写作这样：</p>
<p><span class="math display">\[\begin{aligned}L_n=-\sum_t\log
P_\theta(x_{t+n:t+1}\mid x_{t:1})\end{aligned}\]</span></p>
<p>那么怎么在一步内预测多个token呢？论文里的做法是利用多个output
head，每个head负责预测一个token。下面这个图就是当n=4的例子：</p>
<img src="/a9c496e3/mtp_example.png" class title="dsv3">
<p>head1根据token 1~t预测token
t+1，这和标准的NTP任务是一样的。而head2则是根据token 1~t预测token
t+2，head3和head4也是类似的，分别预测token t+3和token t+4。</p>
<p>所有的这些head共享同一个主干transformer fs的输出，单独的output
head参数fh，另外还共享着unembedding matrix
fu。第i个head的输出可以写作：</p>
<p><span class="math display">\[P_\theta(x_{t+i}\mid
x_{t:1})=\operatorname{softmax}(f_u(f_{h_i}(f_s(x_{t:1}))))\]</span></p>
<p>由于使用了多个输出头，计算的时候就多了额外的参数和激活值，因此相比NTP，MTP会使用更多的memory。为了缓解这个问题，文中给出串行计算（而不是并行）这些output
head的forward和backward的方法：</p>
<img src="/a9c496e3/mtp_order.png" class title="dsv3">
<p>多个head回传的梯度可以在共享的transformer主干处积累，这样就把增加的memory量从O(nV+d)降到了O(V+d)。</p>
<p>正常推理的时候就只使用head1，其他的head就可以不用了，这和标准的推理形式是一致的。但是如果在推理时使用类似<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/wOIGg9pJCXQxz3GgXApUQw?token=1318369845&amp;lang=zh_CN">投机解码</a>或者<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/e3Cn_zbPlbRUUd4-ngSLTg?token=1318369845&amp;lang=zh_CN">MEDUSA</a>这样的推理加速方案，其他的head2、head3、head4都可以直接派上用场，作为draft
model使用。</p>
<p>2、MTP的效果</p>
<p>MTP的效果怎么样呢？论文在&gt;=91B的代码数据上训练了从0.3B到13B参数量的模型，对比NTP和MTP的效果。在各个模型上，MTP相比NTP，在两个经典代码评测集MBPP和human-eval的效果对比如下：</p>
<img src="/a9c496e3/mtp_code_result.png" class title="dsv3">
<p>随着模型规模的提升，MTP的效果逐步提升，相比NTP的收益越来越大。</p>
<p>文中更多的预训练实验结果还有一些发现：</p>
<ul>
<li>随着训练的epoch数的提升，MTP的收益有所收窄，不过还是有一些的；不过现在通用预训练数据量基本够大，不太可能出现超过1个epoch的情况<br>
</li>
<li>MBPP和human-eval最佳的n为4，不过在APPS/Intro上n=6效果更好，n的设置可能和数据相关</li>
</ul>
<img src="/a9c496e3/mtp_exps.png" class title="dsv3">
<p>另一个需要了解的问题是，MTP在预训练上有效，那么对于在MTP上预训练的模型，微调时n应该设置为多少。下图中，n为预训练中每步预测的token数，n'为SFT训练中每步预测的token数，红线就是预训练和SFT都是NTP，黑色虚线预训练用MTP，SFT用NTP，而浅蓝色虚线是与预训练和SFT都用MTP：</p>
<img src="/a9c496e3/mtp_sft.png" class title="dsv3">
<p>结果上看，对MTP预训练模型使用NTP微调的效果是最好的。</p>
<p>前面的评测都是在code相关的任务上进行的，而在一些NLP
benchmark上，MTP的效果就不如NTP：</p>
<img src="/a9c496e3/mtp_nlp_benchmark.png" class title="dsv3">
<p>这里有几个可能的原因：</p>
<ul>
<li>可能需要更大的模型让MTP发挥效果<br>
</li>
<li>概率类或者选择题类的评测并不能很好地评估MTP学到的更远距离依赖的能力</li>
</ul>
<p>针对第二个猜测，另外使用了8个评测指标为ROUGH-L的任务，这些任务要求模型输出较长的文本（比如摘要）。在这类任务上，MTP模型的效果就比较好了</p>
<img src="/a9c496e3/mtp_summary.png" class title="dsv3">
<p>3、结构上的变体</p>
<p>上面的MTP设计是使用n个output
head，每个head「独立」地进行token预测，逻辑上这些输出头是并行的。实际上这些output
head的设计可以有多种变化，比如他们之间是并行还是串行，每个头的层数和类型。针对这些变体，研究人员也做了实验，各种变体的效果如下：</p>
<img src="/a9c496e3/mtp_archi.png" class title="dsv3">
<p>其中parallel就是前面的独立方式。causal就是head2的输出是以head1的输出为基础的，而anticausal则是先预测n个token中最后一个，然后第n-1个output
head再根据它的结果输出，以此类推，第1个token反而是最后输出的，并且参考后面的所有token。</p>
<p>除此之后，还有一种变体，那就是每个output head维护自己的unembedding
matrix，单独训练，不过这么一来参数量和训练的内存需求就会增大不少。</p>
<h2 id="deepseek-v3中的mtp">DeepSeek-V3中的MTP</h2>
<p>说回DeepSeek-V3。</p>
<p>DeepSeek-V3中的MTP在原实现的基础上做了一些细化和改进。</p>
<p>1、MTP module</p>
<p>首先是MTP
module的设计。DeepSeek-V3中，多个预测的token是有causal关系的，也就是output2会根据output1的特征进行输出。</p>
<p>前一个MTP module的输出向量经过RMSNorm之后和embedding
layer的feature拼接在了一起，再经过transformer block进行输出：</p>
<img src="/a9c496e3/ds3_mtp_module.png" class title="dsv3">
<p>这个图值得细细看，有几个要注意的地方：</p>
<ul>
<li>MTP module原始的输入来自embedding layer，而不是主干transformer
model的最后一层输出<br>
</li>
<li>main model的output
head是预测第t+1个token的，第一个MTP是预测第t+2个token的，第二个MTP是预测第t+3个token的<br>
</li>
<li>output head的参数是共享的，也就是每个MTP
module中，预测不同token的能力主要是由linear projection和transformer
block部分的参数习得；使用参数共享的考虑和《EAGLE: speculative sampling
requires rethinking feature
uncertainty》有些相近，不过目的不同，EAGLE是为了加速推理，而DeepSeek-V3是为了优化MTP的训练效果</li>
</ul>
<p>在推理的时候MTP
module可以完全不使用，回到正常的NTP的方式来生成结果。当然如果要考虑推理加速，这些module也可以用上。</p>
<p>2、训练的损失函数</p>
<p>MTP的损失是作为附加损失和main model一起训练的。几个MTP
module的损失就是取平均，再通过权重λ加入到总loss里：</p>
<p><span class="math display">\[\mathcal{L}_{MTP
}^{k}=CrossEntropy\left(P_{2+k: T+1}^{k}, t_{2+k:
T+1}\right)=-\frac{1}{T} \sum_{i=2+k}^{T+1} \log
P_{i}^{k}\left[t_{i}\right]\]</span></p>
<p><span class="math display">\[\mathcal{L}_{MTP}=\frac{\lambda}{D}
\sum_{k=1}^{D} \mathcal{L}_{MTP}^{k}\]</span></p>
<p>实际使用中，MTP深度D=1，也就是除了主模型的output head，只有一个MTP
module。</p>
<h1 id="数据构建">数据构建</h1>
<p>数据建设上，DeepSeek-V3没有给出特别详细的内容。相比V2，V3强调了几点变化：</p>
<ul>
<li>增加了数学和代码数据的比例<br>
</li>
<li>增加中英文之外其他语言的覆盖<br>
</li>
<li>强调了去重了保留多样性</li>
</ul>
<p>最终获得了14.8T的训练数据。</p>
<p>此外，文中还透露了以下几点。</p>
<h2 id="document-packing">document packing</h2>
<p>目前大部分的模型都是采用concat-then-split的方式，把文档分割成训练样本。这样的方式可以避免padding，从而提高训练效率。但是频繁的文档切分也会带来问题：训练数据的实际有效上下文缩短；被分割的文档缺失上下文信息，让模型在生成时需要靠想象补充缺失的部分，从而导致幻觉的产生。</p>
<p>DeepSeek-V3就参考《Better &amp; Faster Large Language Models via
Multi-token Prediction》的做法Best-fit Packing，优化document
packing。</p>
<p>那么简单介绍一下best-fit packing。</p>
<p>首先，假设模型的训练窗口长度时L，那么对于长度大于L的文档，首先就要切成长度为L的小块。这一步是无论什么训练策略都要做的，即使不进行任何拼接而对每个文档单独进行padding，也需要切分过长的文档。</p>
<p>那么接下来的任务就是把这些切分出来的文档chunk拼接成长度&lt;=L的训练样本，并且样本数量越少越好。样本数量越少，意味着数据密度越高，padding越少。</p>
<p>到这里，其实就转化成了一个背包问题。但是背包问题是NP-hard的，没法直接得到最优解，因此可以借用已有的高效近似解法，First-Fit-Decreasing
(FFD) 和Best-Fit-Decreasing (BFD) 来获得近似解。</p>
<p>算法如下：</p>
<img src="/a9c496e3/BFD_FFD.png" class title="dsv3">
<p>C就是文档集合，l(c)是文档的长度。每一步拼接中，FFD是对文档长度降序排序，然后选择第一个fit的文档加入；BFD是对文档长度降序排序，然后选择让bin的剩余空间最小的文档。实践中，使用segment
tree实现BFD上的快速搜索。</p>
<p>直观看下best-fit packing和concat-the-split的对比：</p>
<img src="/a9c496e3/best_fit_packing.png" class title="dsv3">
<p>那么best-fit
packing的会带来多少的额外padding呢？由于实际训练数据大部分其实不是很长，所以更容易pack得很紧密。在2k和8k的训练窗口下，best-fit
packing和concat-then-split相比基本没有可感知的训练样本增加，小于万分之一，并且随着训练窗口增大，这个差距还在减小：</p>
<img src="/a9c496e3/packing_padding.png" class title="dsv3">
<p>最终训练效果上，相比concat-then-split，best-fit
packing在阅读理解、NLI、Context Following上有明显的提升：</p>
<img src="/a9c496e3/bfp_perf1.png" class title="dsv3">
<img src="/a9c496e3/bfp_perf2.png" class title="dsv3">
<h2 id="fill-in-middlefim">Fill-in-Middle（FIM）</h2>
<p>为什么需要FIM的训练方式。我们知道GPT模型相比Bert类模型，有更高的训练效率；而从左到右自由生成的方式也使得GPT模型能够应用在更多场景，上限更高。但是传统的left-to-right的训练方式也有限制：如在代码补全的场景，需要模型同时兼顾上文和下文，对中间部分的内容进行补全，这种情况下left-to-right的训练方式就无法提供有效的信息，因为看不见下文。</p>
<p>为了解决这个问题，可以对模型的输入数据做一个transformation：把原本顺序正常的文档，切分成三部分，即prefix、middle和suffix，并把middle部分放到最后面。</p>
<p>document -&gt; (prefix; middle; suffix)  -&gt; (prefix; suffix;
middle)</p>
<p>训练的时候，模型需要根据给定的上文prefix和下文suffix，来生成中间的部分。</p>
<p>DeepSeek-V3中有10%的数据采用了FIM的格式变换，使用PSM的顺序。</p>
<h2 id="tokenizer和token-boundary">tokenizer和token boundary</h2>
<p>DeepSeek-V3的tokenizer除了加入其他语言的token之外，还增加了包含标点符号和line
break的token。这些新加的token可能会引入prompt boundary的问题。</p>
<p>什么是prompt
boundary？先来看一个例子。用stabilityai的stablelm-base-alpha-3b模型来给这句话进行补全：</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;The link is &lt;a href=&quot;http:&#39;</span></span></code></pre></div>
<p>正常来说，我们希望补全的结果是一个格式正确的链接。实际生成的结果是</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;The link is &lt;a href=&quot;http: //www.google.com/search?q&#39;</span></span></code></pre></div>
<p>注意"http:"后面多了个空格，这显然是无效的。这就有点奇怪了，按道理这样的格式在训练数据里是足够多的，模型没有道理学习不到有效的格式。</p>
<p>重新试一下生成，这次把输入prompt最后的冒号去掉</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;The link is &lt;a href=&quot;http&#39;</span></span></code></pre></div>
<p>再让模型补全：</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;The link is &lt;a href=&quot;http://www.youtube.com/v/s&#39;</span></span></code></pre></div>
<p>这下就可以正常生成了。</p>
<p>看来问题就出在 : 这里。把第一个prompt的token打印出来看看：</p>
<img src="/a9c496e3/token_boundary_1.png" class title="dsv3">
<p>再看看一个正常链接的token：</p>
<img src="/a9c496e3/token_boundary_2.png" class title="dsv3">
<p>发现 :// 是被当成一个token处理的。</p>
<p>大多数的tokenizer都是greedy
tokenization的策略。训练时可以看到完整的文本，因此所有链接中，://
都被当做一个token处理，也就是模型在训练时几乎没有见过 : token后面跟诊 //
token的情况，这就导致如果prompt中给了 : ，模型就会输出错误的结果。</p>
<p>词表中有很多以 :
开头的token，它们在训练时都被当做一个token处理了：</p>
<img src="/a9c496e3/token_boundary_3.png" class title="dsv3">
<p>也就是说，对于这34个token，模型几乎没有训练过它们的冒号被拆分出来的情况，那在推理时自然也就无法正常生成。</p>
<p>这个情况不仅存在于和 : 相关的token中，而是广泛存在于整个词表。</p>
<p>这个现象可以称之为token boundary bias。缓解token boundary
bias大致有两个方法。</p>
<p>第一个方法叫做token
healing。既然输入prompt中最后一个token有可能是训练数据的token中的一部分，那么就先把最后的一个token删去，然后再在后续的生成结果中，选择包含被删去字符的token作为生成结果。</p>
<p>比如前面的链接生成，输入的prompt是</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;The link is &lt;a href=&quot;http:&#39;</span></span></code></pre></div>
<p>tokenization之后 :
是最后一个token，那么就把它去掉。假设后续模型生成的top k个结果是</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>s</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>:\<span class="op">\</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>google</span></code></pre></div>
<p>那么就选择包含 : 的第二个结果。</p>
<p>token healing方法在guidance-ai中有实现。</p>
<p>另外一个方法是subword
regularization，就是在训练时，随机拆分已经分好的token，获得sub-optimal
tokenization的结果。这些结果不是最好的切分结果，但是可以帮助模型缓解token
boundary bias。</p>
<p>DeepSeek-V3用的就是第二种方法。</p>
<h1 id="训练设置">训练设置</h1>
<h2 id="预训练">预训练</h2>
<p>DeepSeek-V3有多阶段的预训练。</p>
<p>第一阶段（基础通用预训练）：</p>
<ul>
<li>长度4k<br>
</li>
<li>gradient clipping norm = 1.0<br>
</li>
<li>前2k步中，lr从0整张到2.2e-4，然后保持constant lr训练10T token<br>
</li>
<li>在之后的4.3T token，lr用cosine schedule下降到2.2e-5<br>
</li>
<li>在之后的333B，保持lr=2.2e-5<br>
</li>
<li>在最后的167B，lr切换到7.3e-6<br>
</li>
<li>batch
size在最初的469B数据，逐渐从3072提升到15360，之后保持15360<br>
</li>
<li>expert分配在8个节点64个GPU上<br>
</li>
<li>负载平衡速度𝛾在最初的14.3T设为1，之后设为0<br>
</li>
<li>MTP loss weight在前10T token 𝜆 = 0.3，后4.8T设为0.1</li>
</ul>
<p>第二阶段（长窗口预训练）：</p>
<ul>
<li>窗口长度从4k提升到32k，lr=7.3e-6，batch size =
1920，训练1000步<br>
</li>
<li>窗口长度从32k提升到128k，lr=7.3e-6，batch size =
1920，训练1000步</li>
</ul>
<h2 id="对齐">对齐</h2>
<p>SFT：</p>
<ul>
<li>SFT数据共有1.5M条，训练2个epoch<br>
</li>
<li>lr从5e-6降到1e-6，cosine schedule<br>
</li>
<li>使用sample masking strategy，各个sample不互相看见<br>
</li>
<li>reasoning data
<ul>
<li>部分来自DeepSeek-R1<br>
</li>
<li>对于每个领域，比如代码，数学，通过SFT +
RL训练领域专家模型，用于生成对应领域的数据<br>
</li>
<li>主要有两类格式：&lt;problem, original response&gt;，&lt;system
prompt, problem, R1 response&gt;<br>
</li>
</ul></li>
<li>non-reasoning data
<ul>
<li>用DeepSeek-V2.5来生成response<br>
</li>
<li>人力来对数据进行检查和更正</li>
</ul></li>
</ul>
<p>RL:</p>
<ul>
<li>使用Group Relative Policy Optimization</li>
</ul>
<h1 id="小结">小结</h1>
<ul>
<li>细节部分有不少优化，包括MTP，tokenizer，document packing等<br>
</li>
<li>MoE还是延续之前的做法，所谓的新的负载平衡应该没有特别大的影响<br>
</li>
<li>MLA + MTP在降低推理成本上有应该有比较重要的地位<br>
</li>
<li>实际上infra做了大量的工作，用于提升训练效率，这块有机会再盘<br>
</li>
<li>总的来说，DeepSeek-V3是算法和工程的优秀实践；踏实把每个细节做好最重要</li>
</ul>
<hr>
<p>博客：<a target="_blank" rel="noopener" href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a target="_blank" rel="noopener" href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
- 大模型算法题：<br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a target="_blank" rel="noopener" href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a target="_blank" rel="noopener" href="http://www.linsight.cn/1736008.html">(3)</a>、 <a target="_blank" rel="noopener" href="http://www.linsight.cn/1736008.html">(4)</a>、 <a target="_blank" rel="noopener" href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a target="_blank" rel="noopener" href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a target="_blank" rel="noopener" href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a target="_blank" rel="noopener" href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a target="_blank" rel="noopener" href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】DeepSeek-V3 Technical Report
https://arxiv.org/abs/2412.19437v1<br>
【2】Better &amp; Faster Large Language Models via Multi-token
Prediction https://arxiv.org/abs/2404.19737<br>
【3】大模型推理加速-投机解码，https://zhuanlan.zhihu.com/p/699670010<br>
【4】大模型推理加速-MEDUSA，https://zhuanlan.zhihu.com/p/703461293<br>
【5】DeepSeek-V2和MLA，https://zhuanlan.zhihu.com/p/708622695<br>
【6】DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts
Language Model，https://arxiv.org/abs/2405.04434<br>
【7】理解Attention:从起源到MHA,MQA和GQA，https://zhuanlan.zhihu.com/p/686149289<br>
【8】MoE模型的前世今生，http://www.linsight.cn/44e38c1b.html<br>
【9】Fewer Truncations Improve Language
Modeling，https://arxiv.org/abs/2404.10830<br>
【10】代码大模型(一)--业界现状，https://www.linsight.cn/a0b50049.html#fim<br>
【11】The Art of Prompt Design: Prompt Boundaries and Token
Healing，https://medium.com/towards-data-science/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Lin
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://saicat.github.io/a9c496e3.html" title="DeepSeek-V3细节探索">https://saicat.github.io/a9c496e3.html</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i> NLP</a>
              <a href="/tags/LLM/" rel="tag"><i class="fa fa-tag"></i> LLM</a>
              <a href="/tags/transformer/" rel="tag"><i class="fa fa-tag"></i> transformer</a>
              <a href="/tags/SFT/" rel="tag"><i class="fa fa-tag"></i> SFT</a>
              <a href="/tags/DeepSeek/" rel="tag"><i class="fa fa-tag"></i> DeepSeek</a>
              <a href="/tags/pretrain/" rel="tag"><i class="fa fa-tag"></i> pretrain</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/96393d3b.html" rel="prev" title="小米的移动UI多模态模型--MobileVLM">
                  <i class="fa fa-angle-left"></i> 小米的移动UI多模态模型--MobileVLM
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/6c0f6207.html" rel="next" title="Qwen2.5-1M技术解密">
                  Qwen2.5-1M技术解密 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Lin</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">750k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">22:43</span>
  </span>
</div>
<div class="busuanzi-count">
</div>

<!--
-->


<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.28/fancybox/fancybox.umd.js" integrity="sha256-ytMJGN3toR+a84u7g7NuHm91VIR06Q41kMWDr2pq7Zo=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/fancybox.js"></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"Saicat/comment-utterance","issue_term":"pathname","theme":"github-light"}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
