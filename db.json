{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","path":"images/logo-algolia-nebula-blue-full.svg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","path":"css/noscript.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","path":"js/bookmark.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","path":"js/comments-buttons.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","path":"js/comments.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","path":"js/motion.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","path":"js/next-boot.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/config.js","path":"js/config.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","path":"js/schedule.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","path":"js/pjax.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/addtoany.js","path":"js/third-party/addtoany.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/fancybox.js","path":"js/third-party/fancybox.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/pace.js","path":"js/third-party/pace.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","path":"js/third-party/quicklink.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","path":"js/third-party/analytics/baidu-analytics.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","path":"js/third-party/analytics/google-analytics.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/matomo.js","path":"js/third-party/analytics/matomo.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","path":"js/third-party/analytics/growingio.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","path":"js/third-party/chat/chatra.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","path":"js/third-party/chat/tidio.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","path":"js/third-party/comments/changyan.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","path":"js/third-party/comments/disqus.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","path":"js/third-party/comments/disqusjs.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","path":"js/third-party/comments/gitalk.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","path":"js/third-party/comments/isso.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","path":"js/third-party/comments/livere.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","path":"js/third-party/comments/utterances.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","path":"js/third-party/math/mathjax.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","path":"js/third-party/math/katex.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","path":"js/third-party/search/local-search.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","path":"js/third-party/search/algolia-search.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","path":"js/third-party/statistics/lean-analytics.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","path":"js/third-party/statistics/firestore.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","path":"js/third-party/tags/mermaid.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","path":"js/third-party/tags/pdf.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/wavedrom.js","path":"js/third-party/tags/wavedrom.js","modified":1,"renderable":1},{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"source/images/qrcode.jpg","path":"images/qrcode.jpg","modified":1,"renderable":0},{"_id":"source/images/cover.png","path":"images/cover.png","modified":1,"renderable":0},{"_id":"source/images/avatar/20180303210737_XsJVr.jpeg","path":"images/avatar/20180303210737_XsJVr.jpeg","modified":1,"renderable":0},{"_id":"source/images/avatar/Picasso_Elephant.png","path":"images/avatar/Picasso_Elephant.png","modified":1,"renderable":0},{"_id":"source/images/avatar/shadow.png","path":"images/avatar/shadow.png","modified":1,"renderable":0},{"_id":"source/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","path":"images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","modified":1,"renderable":0},{"_id":"source/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","path":"images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","modified":1,"renderable":0},{"_id":"source/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","path":"images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","modified":1,"renderable":0},{"_id":"source/images/background/wallhaven-2ywymm.png","path":"images/background/wallhaven-2ywymm.png","modified":1,"renderable":0},{"_id":"source/images/background/wallhaven-gpxpg3.png","path":"images/background/wallhaven-gpxpg3.png","modified":1,"renderable":0},{"_id":"source/images/background/wallhaven-x636oz.png","path":"images/background/wallhaven-x636oz.png","modified":1,"renderable":0},{"_id":"source/images/background/wallhaven-p97q73.png","path":"images/background/wallhaven-p97q73.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/about.txt","path":"images/favicon/favicon_io/about.txt","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/android-chrome-192x192.png","path":"images/favicon/favicon_io/android-chrome-192x192.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/android-chrome-512x512.png","path":"images/favicon/favicon_io/android-chrome-512x512.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/apple-touch-icon.png","path":"images/favicon/favicon_io/apple-touch-icon.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon-16x16.png","path":"images/favicon/favicon_io/favicon-16x16.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon-32x32.png","path":"images/favicon/favicon_io/favicon-32x32.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon.ico","path":"images/favicon/favicon_io/favicon.ico","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/site.webmanifest","path":"images/favicon/favicon_io/site.webmanifest","modified":1,"renderable":0}],"Cache":[{"_id":"node_modules/hexo-theme-next/LICENSE.md","hash":"68fc9a03d50fd4b5ea97092b05967d1819dea2c4","modified":1706697684353},{"_id":"node_modules/hexo-theme-next/_vendors.yml","hash":"4f6046ceb1470be9ff334ede20b73871c951d845","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/package.json","hash":"4b48877b223ec717e708540a2df03d64983c02ab","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/README.md","hash":"d6820f46d03a93bd6dc8b10f49f58aec82ad2b06","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/.DS_Store","hash":"1d67a44d93a3429d76ad084bde035dc4f20e3100","modified":1715072325708},{"_id":"node_modules/hexo-theme-next/docs/AUTHORS.md","hash":"a648823121563c34a177ae91f5a774b5e29f01a0","modified":1706697684352},{"_id":"node_modules/hexo-theme-next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1706697684352},{"_id":"node_modules/hexo-theme-next/source/.DS_Store","hash":"8b400bb5f7b29cca6335dd6ab550d517d0597767","modified":1715072325717},{"_id":"node_modules/hexo-theme-next/languages/ar.yml","hash":"7d0f39e8684284a04bb9808521c87fecda8bd131","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/scripts/.DS_Store","hash":"f185aa14c1236b351528993f4337fe74ba8c2af2","modified":1715072325671},{"_id":"node_modules/hexo-theme-next/languages/de.yml","hash":"79b37df731c29665dee6cd7c90d278e1edfb6e24","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/README.md","hash":"b2567e32805dda79601157351a07e5ca9fe01315","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/languages/bn.yml","hash":"564bed75da6e05b11dce6164508f97a15e2fb6c2","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/_config.yml","hash":"255c963c680da5da34c259c560dd8211b75188ca","modified":1708604632809},{"_id":"node_modules/hexo-theme-next/docs/LICENSE.txt","hash":"f5b14f791b7cfa1d16da981d929152e088a5d1b8","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/languages/es.yml","hash":"dffc63ef42e1266b88e0acf08994fd17a9908d53","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/fa.yml","hash":"f3ffc444599f4ac92d62e9ed00a1490ebc277d70","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/fr.yml","hash":"8ac44e58f71a38b7697a2f7f98a6971ed818cb5b","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/en.yml","hash":"ba0fd79a2b1d8db01a034180556061745965ff05","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/it.yml","hash":"16d716ecfd748def2f6486ef5a82d0ab7ceb4890","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/id.yml","hash":"929df147f4f17d638b07de5fe52ca13e2549ab1c","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/eo.yml","hash":"e34bb33ae827bf2f0727088599a73bc64bdad1b0","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/nl.yml","hash":"3cb3687696635ec71b4ca40c5fc43b56acc8843e","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/ja.yml","hash":"543222bfc516aab6c33e8534f807972ecb8943a9","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/ko.yml","hash":"d345a303310c8a5f4836c3683f3580f861ebd1b4","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/pt.yml","hash":"70de366e10ea584ba039d40d6b35ac97f93454ad","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/th.yml","hash":"6829e998b39f8f143e20b276bb1f62d95a29de58","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/pt-BR.yml","hash":"76b8576ce228d540a16b1f0af5af2cce20923194","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/si.yml","hash":"2d712eedf3f60d04d36c3108cf5a12e2a52e875c","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/tr.yml","hash":"a57e4ed089b893a95f5e1ecff17ce625165f4d46","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/ru.yml","hash":"c6d8de0ff7d8148d09993257cfd3b7aca755696c","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/zh-CN.yml","hash":"741d7efe0262c9cdc2c648014b55599665d90f6b","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/uk.yml","hash":"ff537047b4b4c3ca9a7b64fa7f428a9942751eeb","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/tk.yml","hash":"511726054873f6f8d7ce0d2e803f6731de0ddbe7","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/vi.yml","hash":"7ebcba5e1128784195e4681dffc9d34c4e873fec","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/zh-HK.yml","hash":"88ea50eeb9097ab4a87a44981a102d8594feb064","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/layout/category.njk","hash":"c68b7343d0f8145010f93351908cc36ef6212ec1","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_layout.njk","hash":"fc0a45112f2dcfc2642404e8934ea32a793c3bd7","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/archive.njk","hash":"d759f4d2cf5ddc6875ea250113a00662c1caf6d1","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/scripts/events/index.js","hash":"bd9ea82376cd87df611ea3ae077875c7c595a3df","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/layout/page.njk","hash":"b0660b2af0ac7d3fda14ca4d9f2c9e79ef06c6f9","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/.DS_Store","hash":"a2cb6a68d26ca70f79d97ee70364cdebc56f5b79","modified":1715072325706},{"_id":"node_modules/hexo-theme-next/layout/index.njk","hash":"dd63e488ae8cc144335a5958acedf6a16edd7a92","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/languages/zh-TW.yml","hash":"4695c87d6b81b3a23d16ad6513d9eaa925f8d8ad","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/scripts/filters/default-injects.js","hash":"872f01cb10e422a648ea505436532e776e92926b","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/scripts/filters/minify.js","hash":"447db39d17775b2bd18d8af9c9d65b7b8449f751","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/scripts/helpers/font.js","hash":"3394185a7f0393c16ce52c8028f90da3e9239c55","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/scripts/filters/locals.js","hash":"9eb5310664759931287dd28ea39165dfb67f12ed","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/scripts/helpers/navigation.js","hash":"78107021101553c3d23e89290f7530b60cf4aa86","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/layout/post.njk","hash":"0bfce9f133f501a9a4837257e3b862b3bbca15be","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/tag.njk","hash":"9e16ba20c28a7f2c6bc75aa427f48122301a30aa","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-url.js","hash":"6281d47c1de98eb38f3aa0f6df29bbb19d412173","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/filters/post.js","hash":"fdc8a0af90035e89c3fcb754a0eb189b8951a2bc","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-vendors.js","hash":"957241c28796ff352de7f4cffba7bb289b043586","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-paginator.js","hash":"e86c764b546e4fbb87970cabc4135a56f9ef9fe1","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/caniuse.js","hash":"935a311142a409c1896b3ae3f01fe7a9e2db1134","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/scripts/helpers/engine.js","hash":"d292b78485e8e8055712b0ed6de7cf559c5fbdcd","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/scripts/tags/group-pictures.js","hash":"9ed799c329abf830f623689d7e136991256a24ca","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/scripts/tags/center-quote.js","hash":"92c19d796bdb3320df9caea59bf52df7a95d9da9","modified":1706697684337},{"_id":"node_modules/hexo-theme-next/scripts/tags/index.js","hash":"1f6aba7820f1fb58b61969485148db21846e1aa9","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/tags/label.js","hash":"8a73348186113bae0a51ea2f891c1bb882fab05a","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/scripts/tags/mermaid.js","hash":"4fb01ca650fa8b256b8d48f50dc1b18350bd3d6d","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/scripts/tags/button.js","hash":"c6ad2ed544fbb25ecb5d820c36e76302504271b7","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/scripts/tags/pdf.js","hash":"344636b6fd7e27e8831c1e194039afc0d61931cd","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/note.js","hash":"7b94ddb46b7d4b0fe815f2fbe4bd375f07f55363","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/tabs.js","hash":"0eabe51da40b4b13e16419c8fe02452d9a4fef73","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/scripts/tags/video.js","hash":"2ee926448583be8f95af1f2884ae2c9c4830151d","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/scripts/tags/link-grid.js","hash":"18a483c2d5afd701f6080ffdddf2d1321370336c","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-config.js","hash":"ead37e9167b682f1fa34b5401c3050e18c7ee4a3","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/wavedrom.js","hash":"b44dfeeb58b41945d469141787f3dbce4b117d08","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/README.md","hash":"12a3e96581964a22b474cc739675d52ef93ff932","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"12a6631617695504d5cf2a94b57d87bd331bef6f","modified":1706697684353},{"_id":"node_modules/hexo-theme-next/docs/ru/README.md","hash":"29c89a41b371f893e56c87ea61adabc444ec58cc","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/source/css/_colors.styl","hash":"3c6798c10cc220d83481cb3f3782e78558cee789","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CONTRIBUTING.md","hash":"a089f7a8368ab0b7d7b9b7ec0ac3767a453435df","modified":1706697684353},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/css/main.styl","hash":"921a58577f411cf4eb5cfd66db0a241f8f88578c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1706697684330},{"_id":"node_modules/hexo-theme-next/source/css/_mixins.styl","hash":"83647a6207333b9609ba90b0946b3fa9548e6381","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/source/css/.DS_Store","hash":"31813a1741cf49d75d4d2d99255403d0d3bb7935","modified":1715072325704},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","hash":"770d63c26f22705311028a36b52e999cc8a2da82","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","hash":"dadc81256afb127b77eac6763d5ee0ec9c77f0a3","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","hash":"745bd828205da7e5fbd3f860cc3697097630f5f3","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/layout/_partials/comments.njk","hash":"d0c470b0f6690aa217e9ada848c5e2e73fb27c6f","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/source/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","hash":"b03ba78c6916ad2f390d55bc1bc18fafb64b0ebf","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/layout/_partials/languages.njk","hash":"e43f22198cccb5f6e306b1ce0d28d12a4fb891f8","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","hash":"d775148c2bf20c028622af5609a788167352bf1e","modified":1706697684350},{"_id":"node_modules/hexo-theme-next/layout/_third-party/addtoany.njk","hash":"ef64c6bfb8540cd874701236b9be47db2496e98e","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_partials/footer.njk","hash":"d77ec95cfee58b17807763dc2adb7946829cb316","modified":1706757600094},{"_id":"node_modules/hexo-theme-next/layout/_partials/widgets.njk","hash":"e7f988ecddb2159313699a00827a45eca5622bd4","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_partials/pagination.njk","hash":"bc719473ed5948ab6859449d60b8d36cfc1542b4","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_third-party/index.njk","hash":"dfd7cdd6ba89f8c3deabc27726c7a350cadafd11","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_partials/.DS_Store","hash":"168617768f812b394ad2e34bbffa6ca6fb8b2f98","modified":1715072325705},{"_id":"node_modules/hexo-theme-next/layout/_third-party/pace.njk","hash":"d7ad5714079f7f65446f880baf14722435ca9061","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_macro/post-collapse.njk","hash":"abda600685ee972e1f6b7a2dcc56f13e2daa6263","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_macro/sidebar.njk","hash":"547c62ab14d9e05d2d9116db9048a677fbe1fb6d","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_third-party/quicklink.njk","hash":"0efed71ed530447718c4ea5bbd5fc8695b0b0d5f","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_third-party/fancybox.njk","hash":"844559f46e2ff1c8be234d5763703106e2072a7b","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_scripts/index.njk","hash":"6668878a0f9a1166c6a879755f54a08d942da870","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_scripts/vendors.njk","hash":"be80b9fe415a9a09d74c28e230995fd292dfc123","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/utils.js","hash":"6853e5433e3eaa19ea43fa20b08d956ba4cec4ac","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/injects.js","hash":"d987709267a1bc6e5014411e9983d7c49c102c16","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/config.js","hash":"9ec51eb61f7fee612ffc5252f489003a0fa301fc","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/navigation.js","hash":"dd3562686d95a50375e6fd32e717ccb0d99c1e3d","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/vendors.js","hash":"464db1e7182e5b9cdbd32e8b5368d5e683b1d9c7","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/highlight.js","hash":"6aec7b2c38c50989a23bfaa0d560e75c7f553e12","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/common.js","hash":"19a402a225c31edffc50f202a14e0d582d3db23e","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqus.js","hash":"7f71d6b271ba65ff333d5682e7575711d368c0d2","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/gitalk.js","hash":"7bb7dafdd7f6bca8464b54e17e552ce7f1714195","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/layout/_macro/post.njk","hash":"cbe208445e4d1df82ebd1761e1eaced3eab77fb3","modified":1706698899947},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/changyan.js","hash":"5798cfc8f63665031dd3e01debed051628cec319","modified":1706697684338},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/default-config.js","hash":"93ee5f9109dad885dc38c49bcee630c10f9dce6e","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/utterances.js","hash":"d3bded697bc32dace689d2a6dfb6eb7514169d15","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/isso.js","hash":"ff8b5b5145220a17d0ecd9508ba9bd2d3b2da47d","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqusjs.js","hash":"a600a98e7436edeb31e291abca359885567df3c9","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/livere.js","hash":"5a07d8bb52bc1d51a624ca8db54be144566c306b","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/layout/_third-party/.DS_Store","hash":"10b75eb755ab25235244a5b6e64dafa853e092f5","modified":1715072325673},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Gemini.styl","hash":"96e0a7c2a65ce68215e17e369085b2ea2f1334f2","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Mist.styl","hash":"a1418c9dc8c0f1a0ad4ded0f4627c45bf0db1a10","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/.DS_Store","hash":"cba17e35154c352959c43a31374ed836c96990a4","modified":1715072325721},{"_id":"node_modules/hexo-theme-next/source/js/third-party/addtoany.js","hash":"5276c8f78ee562a8965216dc67d762e59cb4a9f2","modified":1706697684332},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Muse.styl","hash":"e3be898f5ebcf435a26542653a9297ff2c71aeb0","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_variables/base.styl","hash":"c4fc4e862d09221265ab1466085f057be2ad2e4d","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/source/js/third-party/fancybox.js","hash":"819f382c561fe5ec23c67cc5fabd63dd1cc22dc1","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","hash":"ba7ba2c129d1f240c6a22cec3e53f3f22af64b6b","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/source/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/js/third-party/.DS_Store","hash":"b7a4a8e8ee3bbf1c3d47bc7f5afa02917522698e","modified":1715072325672},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head-unique.njk","hash":"8da52a144060db1a0a088ccb2e6cc8376d1fce70","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-copyright.njk","hash":"bfff923526d6800218f08dba6ce0bbf5c17755fd","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-followme.njk","hash":"c1e33b4889f75acc490af3c8bde0ec56c518ff41","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-reward.njk","hash":"e8b8a7c41e9ec612d0c0c73419529d55d1c16256","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-meta.njk","hash":"9fa47e4fb342811da590ee4adc91cf81118c0a39","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head.njk","hash":"5388b157bba4a40b9312f4a45c6678974ccf0837","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-related.njk","hash":"e0986db00a0201dd3c60570f964829c84ba5bc68","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-share.njk","hash":"16696990e4ce65fc8db18c4635082a5d5d06ff07","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Pisces.styl","hash":"48f4f277946a168d0db1ea02804e85c22ca2c7db","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/brand.njk","hash":"dd9c4c03e99dfde0dfb8edefcb2c933f2f560efc","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/index.njk","hash":"650de421a8ce4cf685428ffbe0087ff84cbd1356","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu.njk","hash":"ee6fc2f111572d3eeab0a2fecbb2d6b3e37ab26b","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/categories.njk","hash":"17156d99941f28a225951ffdcfa9a115e20dc2d2","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/sub-menu.njk","hash":"06480d8ec5f0b87eafd47f082f07968d7282dd5c","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/page-header.njk","hash":"7ed4f102a1825195cff8d7995bf9219f323a9034","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/breadcrumb.njk","hash":"89825e75cc45e9709fa6ba89883669eedaff6f46","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/algolia-search.njk","hash":"efb2b6f19df02ba5ae623a1f274fff52aed21e6f","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/schedule.njk","hash":"0f4bc8e257da60f77c0c1738607b2bde55810684","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/tags.njk","hash":"a18d1598e36cc72f2b0b24c3cc3c5990dfaa3254","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu-item.njk","hash":"41a8b0cc16f60fa085cb719d07216d86b6bc4bf8","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/localsearch.njk","hash":"661f7acae43f0be694266323320f977d84119abe","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/cloudflare.njk","hash":"a5b8297c2c383124dd6a56e256ecc0c0dcf489be","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/baidu-analytics.njk","hash":"6215309aee028dcb734452beec448c5afb6c63fc","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/index.njk","hash":"f900306497b133e8b098bd9f4b96b93d1d96c185","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/growingio.njk","hash":"8afaa772c390bd9d53a5cff9645ac3168334eb98","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/matomo.njk","hash":"4e89648a8ec8194c5823064cbca39c938a799006","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/google-analytics.njk","hash":"d89066ff53879693f023e540d59c86137172c529","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/index.njk","hash":"8f6f256ab3b351ffc80f1f3f1d9834e9a7cfac31","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqus.njk","hash":"9375b19a89b7fa9474e558d085af5448d4c5c50c","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/umami.njk","hash":"3343750682fbd8535e50f8129be3003ad26015b4","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqusjs.njk","hash":"0749cb6902baecdfd01f779a2a2513f6d2f6a823","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/changyan.njk","hash":"d1c950f8fbdf85e7a3eae5463767a89e858e8220","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/gitalk.njk","hash":"b63b7e2ede0d3e66e732fa1a06bda9b19e1e85d4","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/plausible.njk","hash":"ef9f2bb7110507f1c4336800af9157d5fa9765bd","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/microsoft-clarity.njk","hash":"9dc00fcb0a05899f048eace9f9160b78956655d5","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/isso.njk","hash":"64cc3bdaf644fd32c0d0a247f29f5b6904da9af3","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/livere.njk","hash":"3b13b09fba84ec6000886890a6710736a2b8fafe","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/chatra.njk","hash":"d7263fca16d0278ccf1f6aa1c6df6902a6344a09","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_partials/sidebar/site-overview.njk","hash":"78a1a8cac44de7e963ab4cd51c988442eb3e789a","modified":1707031409664},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/utterances.njk","hash":"5a94032bc3512a10ad4328fc19ec07b819a1d687","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/tidio.njk","hash":"02aab857c27fc103216029be991688b12a73a525","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/localsearch.njk","hash":"e45ea3542cdc9ed7ec8447b5e6f35df4c5e82758","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/algolia-search.njk","hash":"24ed76e0c72a25ac152820c750a05826a706b6f4","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/katex.njk","hash":"1ebf658690468ea197bdd0416eb7cfa4bd0b083a","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/pdf.njk","hash":"2c81984cc4f5123103460442f6e046f5b6c97127","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/mathjax.njk","hash":"3677017fd4572b158311f5f5d870590ab25184e0","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/wavedrom.njk","hash":"02202bf563fb5eedde2ccad4d6c5b9109d30a703","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/index.njk","hash":"abf37fc55aa86702118e8fdf5bf2d389dd589aa0","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/firestore.njk","hash":"d32ebe94560fa95824478ebbff531bffc47b194d","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/busuanzi-counter.njk","hash":"a4bc501da0f22f7e420f0ca47e83988ce90b1368","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/lean-analytics.njk","hash":"2446e748cdc102c78492216319ac02148db7daf6","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/index.njk","hash":"568ddf7955d11d93fb5e842b403a7ac8b1b7fdb1","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/mermaid.njk","hash":"099e031f52fb8e47b3af5b2684737efc9e643ee7","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_header.styl","hash":"dafc6d23c80d6fe3e55a7711e94210d2479b629a","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_menu.styl","hash":"fb550935d374e0bdf1097fce187337dc05cad3e1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_posts-expand.styl","hash":"485d23ccb42c0d0c8ead7ea8930dd3e06d79a285","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_layout.styl","hash":"fa4fd8f76464e214fb7318f325b13c2b62f4b478","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_layout.styl","hash":"6569a6640f79d247a8235b3914772c0e2f99ead2","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_header.styl","hash":"3fbfab591f280e2e7f3b0265901c93bc4bd137ed","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/index.styl","hash":"ab16a3dcdc0393b9b582ef59dcc13db9320e917c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_menu.styl","hash":"82cda756f5b7092df2eee6641b9786df71623bdb","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sidebar.styl","hash":"547c0b5cd5e7ea10d21863d13a6b16579a49396c","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sub-menu.styl","hash":"c48ccd8d6651fe1a01faff8f01179456d39ba9b1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_layout.styl","hash":"26a0cba1eee5de45a45a5e14e17707f905390512","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_menu.styl","hash":"72dc825c50357402c342d62ab60fc0c478ab6bc1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_header.styl","hash":"ac2dc0ce9c775a83ef7132ae957b54539366ac9c","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/index.styl","hash":"8000075b227749a7495eaf417cac6ccfbe441580","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sidebar.styl","hash":"91dbf3ca5c3a613d4e30618c120da535bf2d0336","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/back-to-top.styl","hash":"7664491542046df9a3887cf40a06e00c0b4086a9","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/base.styl","hash":"d0a7c99095f490b0d2ed6b1be43d435960798cec","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/reading-progress.styl","hash":"90a86045a33c1bae49fc2f6fa1e1b53170c7f77b","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"778ed2ad5643b93970c95626b325defeb586733f","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/buttons.styl","hash":"a042571d85ff7265f799004239a45f36b716b8a6","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/index.styl","hash":"523fb7b653b87ae37fc91fc8813e4ffad87b0d7e","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/comments.styl","hash":"e4fecc889ba3317a64e9abba5842c79dff9b7827","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/pagination.styl","hash":"f4228c759db4a650c8d38745c2edd1dc83c45687","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/normalize.styl","hash":"b56367ea676ea8e8783ea89cd4ab150c7da7a060","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/index.styl","hash":"2298e521253b3bf376a2412271bc2a7d305051f3","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Gemini/index.styl","hash":"9dfe853c901bdc52fc950bacdf15484dbb9bf140","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","hash":"def07bcc7c17d8a0caad177fb1dd2f3a5e5b3536","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1706697684335},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tables.styl","hash":"e840b23d33023e6d45e018f6e84b683dd56efd8d","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/toggles.styl","hash":"782ee1fc5e669d3ddbfeb82b73ad7fe561f1a4fb","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","hash":"da361917d65e5dca8362f8cdeb6c8cc0e8316cec","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","hash":"1e826dea3f684c0515f362dc1352447a1f0eae71","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/mobile.styl","hash":"1dbf2c339adcd27026c3a2ded32ee91ce08cea26","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/index.styl","hash":"8e34df131830d4fa3725e4590a672ba1cf1903e5","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","hash":"4536cb6d0a9bbaaa86fab3fa0101f6a3a3ec5a76","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","hash":"835cbf54c49ef1327f47df70ff2636ad36b6f57d","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/.DS_Store","hash":"7dc4c0e066c7f2249dc291afac6065e647136cc8","modified":1715072325686},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","hash":"fdb7b7cef1a147d897e7f7cd8903b58368ec2062","modified":1706697684333},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","hash":"6bf821310342c5b87a631873e7650a475a0765f1","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/wavedrom.js","hash":"40dcd10df6edf124088c329346e0cc0bdac74ef1","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/categories.styl","hash":"b6e2eb1550a7845cb2adf86081a4ab6c7bde1e68","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","hash":"6e0682bb42170d61b13b786295f45f9c785f8b73","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/index.styl","hash":"7504dbc5c70262b048143b2c37d2b5aa2809afa2","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/breadcrumb.styl","hash":"8afdc311c6b8db121758371f95cf1c5e77354f42","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/schedule.styl","hash":"6b816c2511242ee503fb5f34cd3e4dcdafc06b85","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/tag-cloud.styl","hash":"1a81d1a71fcf0699629ce6e72dfd0a15f3a2dd0a","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/math.styl","hash":"9d995eb4871a6c273d9d51558676a1fdabf69e72","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/disqusjs.styl","hash":"877a537d5b95beb048142e4fdee6f17e6ef9c7bb","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/index.styl","hash":"54d12e2c5d9982f7b9e5b23be5133954a8514e9d","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/utterances.styl","hash":"56d90ae0559caa55b75f3c300ff2711f9ed65fc4","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/search.styl","hash":"e72799ce3f9b79753e365b2f8c8ef6c310668d4a","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/fold.styl","hash":"42a0b65491ad85438596b3fe0b7f23973e4cef34","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/index.styl","hash":"138f78147bc6bd6005f329ada34dc79b7625542d","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"393ff96234e4196b569d4b11496774eb78e147de","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"f634f94828620e88c3f5a8db56f7944f6ba232b0","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/gitalk.styl","hash":"8f094c4ac17e2ab45569b12d157747f9c7333c12","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/label.styl","hash":"debee14539272fbe3835a7d3853af2230baa3501","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/index.styl","hash":"22cd37bd5df9972d5074710896aba4424ad5161c","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/link-grid.styl","hash":"7f8a7345e6537a62cd9e9a94c8f7065b541d9b04","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/mermaid.styl","hash":"48d35dba575a7c9e8845b16652e76b7d4a4646de","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/pdf.styl","hash":"b6654a1d7cf82577d8263faffee8af3ad4a5c0e8","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/tabs.styl","hash":"33dd6ad015dde65fd46f34961655442e8e82b52e","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/index.styl","hash":"6e0d0796ef7fbbb62ffdfb448753a850de82c74f","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/github-banner.styl","hash":"38c64c2d04e46848382bfa246a0e9c508294767b","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/bookmark.styl","hash":"e74f4bb47a101b014ee2a1783c87f3b87323f9a0","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/wavedrom.styl","hash":"af113411ad9cca7674177be36af8dd399680834d","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/menu.styl","hash":"bbbc40b03cb299d2a6a568f329b2ce98e1cdc430","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/footer/index.styl","hash":"4e967702cf4c637132346bc74ec8854426f1a68c","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-body.styl","hash":"56d5b7ff73f466c9ae54f7204ae899281295d749","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/note.styl","hash":"98d4c20aff0f0fcfe1824017fb06ab21ef0d218e","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-meta.styl","hash":"a851e9d5aefcd027c95eeb323860b6da70f202d1","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-collapse.styl","hash":"7369928305330c73ae0b3f063a681a8384d8fde4","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-followme.styl","hash":"1ecfd64507954810b07a9d21fb5305b5378feda0","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/index.styl","hash":"098d4bd034e986fcf7e443eac4fc2193935461b7","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-gallery.styl","hash":"aa366d37389760c8595529b850f461569577a1c5","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-header.styl","hash":"1191f1bfa5c43e54be8e5b3cc0d802984e161747","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-footer.styl","hash":"11497388f124bfbb4001495a67d3629a9f618405","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-nav.styl","hash":"9ac6f477177264c26a46e8333b8456720a0444dc","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-widgets.styl","hash":"ebfba158a0a4af3d1dabcacbc58986664de52140","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-reward.styl","hash":"04cf4a69537fc14d3b8904f965d283356853847f","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"5b38ac4a0f1ade0e681aff0e3366c481d9cf3dcd","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"0847400d8579b0a2dd1bf662c78954c10adf2680","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/related-posts.styl","hash":"b05908f04ef95f2d91e6eba89b12411c378d050f","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/index.styl","hash":"da5e88f8debd5ac8d7af5c6ba6240df66104955f","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"46eece42510c2c89bb9209afb0262ad76a4b0b36","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"c2e354a565c8c1b32bd0ceacc972b17982758b67","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"c6a27beb3f741211a14576026f3b4cfc44cc6407","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"9a7c71560fbdc936ad4e736fe15063ea3e8a644b","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"d6418fd2bbfba7b73ddf11ec62db9637fdf5d8af","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"24752d145c6fb8f5344dca9c7b9640839c02e009","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-nav.styl","hash":"bf3ad8b4268f763a1e26377681644887694bc009","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/site-state.styl","hash":"26dd0adfcb1db6df29c6090c8d7e9b5a43583fb0","modified":1706697684374},{"_id":"source/.DS_Store","hash":"8a4547b0a77593664100672039134d17a90b155e","modified":1717158595589},{"_id":"source/_data/styles.styl","hash":"f4bb55ef0972c829e3382d1bae1786b3ab5d54ef","modified":1707045638288},{"_id":"source/tags/index.md","hash":"e995ed2b8452b1906600b3853b920f13423098b7","modified":1706698644396},{"_id":"source/CNAME","hash":"07ad8fed7f1b810bf3715a0e68324e5cbb1d47ac","modified":1706872557451},{"_id":"source/about/index.md","hash":"9294d008cc673abc2eaf740f101ebac560029267","modified":1706698701349},{"_id":"source/images/qrcode.jpg","hash":"eab6c8893d0c79cf705cf64d738f560563365b0c","modified":1707548301740},{"_id":"source/_posts/.DS_Store","hash":"85c3593335c39ede27a82394041e45245dd5748b","modified":1717158836367},{"_id":"source/images/cover.png","hash":"2f2aa6173619dd38425673ba110b50b9156d4d10","modified":1710684380714},{"_id":"source/images/.DS_Store","hash":"e08e12a8507734ef2fe42991af2aa57b0860cc60","modified":1715072325720},{"_id":"source/categories/index.md","hash":"f5c920fbc09ea3d8edf250de7e31bcc6b3e765ae","modified":1706698717077},{"_id":"source/_posts/cs/.DS_Store","hash":"c1a5712df55726caf258d33a83d406e63ed9b921","modified":1716541890298},{"_id":"source/images/avatar/20180303210737_XsJVr.jpeg","hash":"2ebbbcead4446a69b0ac0698ab227a1af49735ce","modified":1707030615190},{"_id":"source/_posts/cs/nlp/.DS_Store","hash":"dd1a71f9150891ad51d84b1252c8b4a1c95b0211","modified":1716541890298},{"_id":"source/images/avatar/.DS_Store","hash":"c3fa37607ceb3f7ba411cf4203d2a333f773d921","modified":1707118756919},{"_id":"source/images/background/.DS_Store","hash":"88f5d31d0db89adcf679f2a7fefc8947139a1c1f","modified":1709026807046},{"_id":"source/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","hash":"3e7db51e59a8261de04f55814475556494d153f6","modified":1707048498957},{"_id":"source/_posts/cs/nlp/2024/.DS_Store","hash":"6967a33fef433a8a3317254d63af402be33583fd","modified":1717260504398},{"_id":"source/images/favicon/.DS_Store","hash":"83ddccadffca5384db3dfc167728b7c7cacd9a87","modified":1707796842439},{"_id":"source/images/favicon/favicon_io/apple-touch-icon.png","hash":"5e36b0bfe67bcf96f4f0e92366a5de509fe9a78a","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/favicon-32x32.png","hash":"f42c950146a171afca4ad70ff425b5b4ff02f926","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/about.txt","hash":"d1f084751c81e98b1675b8c0745f6f306a1eaf66","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/favicon-16x16.png","hash":"fb63ee2fc88b9ce42dec5bf68088da14be9350f5","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/favicon.ico","hash":"fceaa7158ae89a98fe5b93152dd7dec2d36c1ff4","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/android-chrome-192x192.png","hash":"2164889376f2bd6f9dd2066ebdfe4abd547f49d9","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/site.webmanifest","hash":"bf31baf91bdd2fcde24a45e3f2a1be33733c6f69","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/android-chrome-512x512.png","hash":"419f0b0317ad2a31acfa7dc19e1502731e92e05f","modified":1706844814000},{"_id":"source/_posts/cs/nlp/2024/05/-.md","hash":"5a20cdecb741dac615b82bf8f603fa13b2136d70","modified":1716608290437},{"_id":"source/_posts/cs/nlp/2024/05/-DPO.md","hash":"7c876990c33a7d7d43fe3e58ed0972d85cd74611","modified":1716985993225},{"_id":"source/_posts/cs/nlp/2024/05/.DS_Store","hash":"2d2a0a1f4a33aaa8460628a46d05494760cd6cc7","modified":1717231744329},{"_id":"source/_posts/cs/nlp/2024/05/-6.md","hash":"b76808355cc79f0f22debf8717a9002404c4e988","modified":1715686009973},{"_id":"source/_posts/cs/nlp/2024/05/-simPO.md","hash":"6ccba993d6142187ac1b663b67f3d248feccd0c7","modified":1717260366697},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO.md","hash":"77ba73314fb67af0807703ad0dc91db8724b84f3","modified":1717158416345},{"_id":"source/_posts/cs/nlp/2024/05/-.md","hash":"9a46f37e067d004004a3e36f4f4d7cee608ad22b","modified":1715591178036},{"_id":"source/_posts/cs/nlp/2024/05/-5.md","hash":"ca99e9196eaf8743885528f6e6d6404bd8cf34ee","modified":1715323819009},{"_id":"source/_posts/cs/nlp/2024/02/.DS_Store","hash":"ab55fc2d73f2238323a1a5d7b8362d5620868275","modified":1715072325702},{"_id":"source/_posts/cs/nlp/2024/04/normalization-.md","hash":"0855c9b52c67a72d809c3f53240f4b62bb99de79","modified":1715323869591},{"_id":"source/_posts/cs/nlp/2024/02/LLM.md","hash":"dd73e47b1cdb864f26d5780ac4fe08603bcc9b3c","modified":1710314618942},{"_id":"source/_posts/cs/nlp/2024/04/-3.md","hash":"b56b2454ac63f79df92591824dda52efb8084e00","modified":1715323861150},{"_id":"source/_posts/cs/nlp/2024/05/.md","hash":"dc51b82a33b114c119bf88dab64733912bf050f2","modified":1715323820731},{"_id":"source/_posts/cs/nlp/2024/04/-4.md","hash":"419c28f934cfe340a4ca4751668fe1512a26ab57","modified":1715323846826},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention.md","hash":"31d3d5cbbde092ba1a855319647b24e32ffda8ef","modified":1710934710908},{"_id":"source/_posts/cs/nlp/2024/03/Yi-.md","hash":"1871881f3afdaf9b2930d31a03d62079ca4ff9db","modified":1711713217115},{"_id":"source/_posts/cs/nlp/2024/04/.DS_Store","hash":"327e321d722e88824dd22a5f789f1eaa20bc42e9","modified":1715351302735},{"_id":"source/_posts/cs/nlp/2024/03/-1.md","hash":"f25140aae947e215b4bf597973bf28a52bce575a","modified":1710685009511},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization.md","hash":"39dbb9473a0afe9654512f34be923e0210d5e1a0","modified":1712471774847},{"_id":"source/_posts/cs/nlp/2024/03/-2.md","hash":"d8b936ccac17c2d991f3894335231adb87aaabc9","modified":1711253769176},{"_id":"source/_posts/cs/nlp/2024/03/.DS_Store","hash":"de21b2ce8789af4981bcb8ca056033d4b22b7716","modified":1715072325687},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE.md","hash":"dbe2c4a5a96fe27437a53c4fa9054bb19e05f28a","modified":1712299467271},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA.md","hash":"bdd2ea9b7abe82b13ace07dff7be5b2a757db7f2","modified":1714231402122},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT.md","hash":"9af08ff8d7274b005502eba5fb0461ea1a0729d9","modified":1710646109054},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/.DS_Store","hash":"ef25e9af6860a6c162c9290b57f27b77606c64e9","modified":1717053157303},{"_id":"source/_posts/cs/nlp/2024/05/-5/yarn.png","hash":"ee124a0823b429842082acebe78a7162915cc11c","modified":1714809224728},{"_id":"source/_posts/cs/nlp/2024/05/-5/.DS_Store","hash":"b4bd95f9044124fd6715a9d1baf9ee4858fdc14b","modified":1714914202734},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/contingency_table.png","hash":"94b25e2d4803d9802d3c5455aed84911fe506089","modified":1717233039090},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/.DS_Store","hash":"cd2b24a23e8b1eacee378a5c5318f6bc30436b23","modified":1717259444329},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/reward_accuracy_compare.png","hash":"0b5526d61a1cbb3188e8e53ea858b1d9d1953660","modified":1717259353701},{"_id":"source/_posts/cs/nlp/2024/05/-/.DS_Store","hash":"816910ea2f357470b3d5c3908437853f6d744a13","modified":1716541217062},{"_id":"source/_posts/cs/nlp/2024/05/-/formula.png","hash":"65fe200098b51d1712b6c38d039aa8be22d38e82","modified":1716453725490},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/.DS_Store","hash":"2e33b8d145af72ec31cbad8c19fc2528fc2a909f","modified":1709014663934},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/complex_number.png","hash":"255aa16a44913d53784b09d0a08762df6d21dc16","modified":1708758408339},{"_id":"source/_posts/cs/nlp/2024/05/-/.DS_Store","hash":"24df341533adcab1c2da9cc93f748f972ed4e580","modified":1715591105150},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_attention_entropy.png","hash":"be91b6a49cfe30dd51ed4f8eb258eb4715a70e37","modified":1715176570143},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_attention_logits_explode.png","hash":"3cc54ee973126c1ca3bcd85d75039e490efc9acf","modified":1715175378314},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_middle_k.png","hash":"869d4b687714409a5a4b89c85dc5ee2c1f0c2c86","modified":1715240601309},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_starting_tokens_num.png","hash":"6a2c841a3d3fd354f57757aa7e663e83585545d6","modified":1715180059874},{"_id":"source/_posts/cs/nlp/2024/05//.DS_Store","hash":"a1143c0efa86d8f8e5cb1ddbbb0932d0adb854d9","modified":1715070961565},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/.DS_Store","hash":"96243fa0e625d8d7395157516c6299723b4ce769","modified":1712575638552},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/bn_and_ln.png","hash":"ff10e75b9d2c95aa713f013b104ed307f2121e57","modified":1711118594120},{"_id":"source/_posts/cs/nlp/2024/02/LLM/mix_precision_fp16.png","hash":"4da22fa80039f8271ffd839aba70f6b1aca7afe4","modified":1709188879237},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/.DS_Store","hash":"f036d46924a246fd06315b601bca6cc759d95300","modified":1710600789234},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/dilated_conv.png","hash":"bbc2ff2e9f891da4bfaf6d535ab8545acc18e8a6","modified":1710560488146},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_large_performance.jpeg","hash":"54e3ed874802ac9465580d6b5fcc5d6c1de96244","modified":1710250364698},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/.DS_Store","hash":"81b22bc167ab6b1514c4698d3c49fcf0836924dc","modified":1710670459475},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/receptive_field_cnn.png","hash":"46515aa3bce1eb0fc244f62ceee7b899c28183e8","modified":1710321816411},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_and_ln.png","hash":"ff10e75b9d2c95aa713f013b104ed307f2121e57","modified":1711118594120},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1711247018739},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_ln_gn_in.png","hash":"9783c818f5e0eaea33d169718476bbe8874cf945","modified":1711120826525},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/rmsnorm.png","hash":"55bbcb42145011f7b5adf90cc613e22e2b94f060","modified":1711165884464},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/realformer_attention.png","hash":"e9a92e5c07c8ca6873ea70671ad54eb2f1a13332","modified":1711206279560},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/.DS_Store","hash":"5d08a407b858db4f8a5bad5168cbb23224622856","modified":1714982537175},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/lossfunc_surface.jpeg","hash":"c78a8df335da0d507963fb73a62fe2c3d145c91f","modified":1711005649925},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/.DS_Store","hash":"ff49d0cafe604c7a0b14ed0b6d60dfd2cb5edac7","modified":1717219419238},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/.DS_Store","hash":"9d409e9dac238eb07cc6841f8e8d05eed83df842","modified":1710056957220},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/MQA.webp","hash":"456a8ab19cc1564912034c375e8c3c5a42be6837","modified":1709973970557},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/cnn_heatmap.png","hash":"cb0bde73c9c4d0646133947ebaab16c44c753667","modified":1709723125449},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/encoder.png","hash":"d6a3a39c420d90e50f02f8b34f127bfe34177331","modified":1709716888116},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/attention_calculation.png","hash":"1f020c39c78221e41c5c6953ef97239e9f42aa3c","modified":1709780575011},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/seq2seq_attention.png","hash":"b95046eee028b45dd9734639ecde8189e93b2374","modified":1709781776387},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/seq2seq.png","hash":"9baa57cc8000a918d0adca6dceaac3ea54791ea8","modified":1709716876496},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/softmax.png","hash":"de80ba20e55abf7457cac958aa87627d0a7e5d77","modified":1709821278308},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/decoder.png","hash":"28ee3d1ab68bd325ecb9d2066bc264a63d7de081","modified":1709716894560},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/.DS_Store","hash":"f40652099a252049ed00339c05066bc05635ded0","modified":1714093902405},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/qwen1.5_moe_tps.png","hash":"5478a6583a6c6fb68f1bc9429c103e84fe39efaf","modified":1713691310250},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/softplus.png","hash":"bdc66c39227441390f2241b4f26c0b1fbab331d9","modified":1713279943448},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_round_error.png","hash":"0172ba008837b3490a3e456306aa72be65636d90","modified":1713862612528},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/xiaomi_moe.jpg","hash":"d898ba33f1ee70efa136dbff3cb38983b461524f","modified":1711814228139},{"_id":"source/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","hash":"6a3863489ed0982c7fe19d575eae28ba8c286ccf","modified":1707048415396},{"_id":"source/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","hash":"0629f15db217c8b7037a5b46ef187d79b9a66a09","modified":1707048511782},{"_id":"source/_posts/cs/nlp/2024/03/MoE-.md","hash":"9769c51d515f2aee1cd86930faf7ddb9cc80f525","modified":1715323896425},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/gradient.png","hash":"0b72939cfd4770fc21727bb203efb9c5dd2491d1","modified":1716907479268},{"_id":"source/_posts/cs/nlp/2024/05/-5/bfloat16.jpeg","hash":"8678b705b0d6e0b7deb230bf28f2d92ce0d42088","modified":1714809868152},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/dpo_correlation.png","hash":"2c1dafd42b7ffa3318395a4934df87692ba5fd62","modified":1717259082807},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/reward_accuracy.png","hash":"7fb3d4dd64e3013534bd77eb1f2def23ec57c8cd","modified":1717257132254},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/simpo_contingency.png","hash":"773700b1d041aba8b3912deee9c8bc7886fec099","modified":1717259285268},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/simpo_hyperparameters.png","hash":"f976162c0882c49b43b503beb1384b447e2d5d00","modified":1717246443198},{"_id":"source/_posts/cs/nlp/2024/05/-/acce_draft_model_param.png","hash":"2e5b1852eaf4745f3d9bfc9b0fcccbd37621bb93","modified":1715691438739},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_choose_gamma.png","hash":"65be032bf276290ca97b7d983bc4e1e2deaa95fc","modified":1715673837432},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_speed_and_op_table.png","hash":"416238792292bff7178830267d53941da202eadc","modified":1716471229174},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/result_3.png","hash":"df5861c846176c90bd9a90bd5836919ef023b13e","modified":1716985497709},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/remote_attenuation.png","hash":"90aa3d8f76c9b5c18de8b7740649b499241cab0c","modified":1708957811757},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_ppl_200m.png","hash":"7b232b7bf3c7238836f71b4b99e492d9f6c285f0","modified":1715241848803},{"_id":"source/_posts/cs/nlp/2024/05/-/stremingllm_kv_cache.png","hash":"11b09e96662feb7cc246e60e1b21d7ffceb47ae6","modified":1714393259685},{"_id":"source/_posts/cs/nlp/2024/05//add_money.jpg","hash":"0b00f9f1dd128e5601f0c7502dd2cf9233898f0f","modified":1714982519785},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/cv_layernorm.jpeg","hash":"f0874ecc4b9d8da8bf3bff0e13a6313ed19a7b15","modified":1712494304517},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi_nosft.png","hash":"d3b46ed19e458973c3586f3bf2ef906deb4080a8","modified":1709199016415},{"_id":"source/_posts/cs/nlp/2024/02/LLM/rope_matrix.png","hash":"84372b91204098bab2ff09d2d865ad869f93bdbd","modified":1709262766062},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_architechture.png","hash":"5e4c347dc41d7f070f54b386fdccf675cfeb8f10","modified":1710255091449},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/big_bird_attention.png","hash":"ed6c76b9bb77b98d34c429333498d04dac8e3ed9","modified":1710561804292},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ellipse_2.png","hash":"ee20ecce8c3470d17b1a4bd43df811d16269ffd3","modified":1711006599335},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/prmsnorm.png","hash":"d5826342f665f4cb04fdbb2e3d83e0b2607355c9","modified":1711166590963},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/postnorm_prenorm.png","hash":"d8830735e89c73ca416baabf4a195d7891d9f0ed","modified":1711167201092},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/sigmoid.png","hash":"f1ced5f06861a2e0296050aff17eebfe3d023a6f","modified":1711246985230},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/model.png","hash":"631efc6d4e92da128d7a10a4dc6af307ee4ddcbf","modified":1711459921302},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/third_party.png","hash":"673fe2b2cad3b1f40c0fcfd190c0034d8dbe7f31","modified":1711615706510},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/multihead_attention.png","hash":"6f8ee285f2646dc163b6b3164a0639aa9ddd7f27","modified":1709637863252},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_long_perf_2.png","hash":"2b6be0099f1eeb0ee2d2056eae7cf541e146b636","modified":1713699160099},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/llama2_qga.png","hash":"5e0dea7d03de9144eb524a0a9adb102e91b52aaa","modified":1709983486925},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/sram_dram.png","hash":"ae7a9296b67c02608460a334fbbad3781b890302","modified":1709971938995},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_train_efficiency.png","hash":"c117407ada2adab8d97250268e2eafa533bb9083","modified":1713699487306},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/mistral_8_7b_perf.png","hash":"f90d9ff5b14326b0eef2a0026b3f5940e0d42f0a","modified":1713700127874},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/qwen1.5_moe_params.png","hash":"93d14a2645969b08a4fb80a31aa75fd8e5201ff8","modified":1713691207069},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/modular_connectionist.png","hash":"b5865cf34faba075b4f2316c2cae0559dac2d883","modified":1711981604894},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_load_function.png","hash":"644684f21f85d565328d98334d41bbe019acbbfc","modified":1712050094734},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/qwen1.5_moe_perf.png","hash":"b79ad1a909081fd0537bd9d44cfac2dc2133de6c","modified":1713691095074},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_more_add_bias.png","hash":"f0de5347918e4928dbcbc59a897c3f3227c3d30f","modified":1713856613597},{"_id":"source/images/avatar/Picasso_Elephant.png","hash":"6d38d4b24c13a3fd506ee37a562126b76a24de83","modified":1707118741657},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/margin_dist.png","hash":"cf82f48158e4ba3e503cbe25cc811910804489cf","modified":1717257492586},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_expected_token_num.png","hash":"52be2409ca9513de2e5ce10a0e77d8aa98dfc328","modified":1715672443581},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_speed_and_op.png","hash":"9b5e3a6c9276309e7aa5a8848d52ef361e62bb36","modified":1715674286748},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_walltime.png","hash":"b645987bec587e743021bc330de277416ef36d5e","modified":1715674129472},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/alpha.png","hash":"4e3ad45447f5757d2cfcdf9d9555351233456c3b","modified":1717157185206},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/summarization.png","hash":"4934f3e42f20bc3a44ad07267e91a14c4c005543","modified":1717155934841},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/rope.png","hash":"5535c87dbcfee3ed646db93662a0b6a1ccd3b31b","modified":1708958930811},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_structure.png","hash":"35f958d9ba50460689727c7038bf3a344000fa52","modified":1714288831777},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_downstream.png","hash":"192160ca6972003a61678e2f7f2467f5bbd94451","modified":1715242314359},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_process.png","hash":"259ea90040b7a5b98a27afcb992a6bee31707ab9","modified":1714289830786},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_quality.png","hash":"fab44d68fc27f7bb2c06f758e537b9b249be0699","modified":1714913630381},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/cv_batchnorm.png","hash":"d9e8d897c36125fddcf1cbcfa5c237a37158a939","modified":1712503708413},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/longformer_attention.png","hash":"64860379955872ecac5835b3f9d8c6d130c7e485","modified":1710560038203},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/rolling_buffer.png","hash":"34d4db9f4855926db561faa80e934dd971c0974e","modified":1710516051198},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/deepnorm.png","hash":"4726d8a40d1d0db397005408295e1ba54809a7e4","modified":1711207769375},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_algo.png","hash":"56f1ab55c0e94814e6e37c30421012ed82098d62","modified":1711028736211},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/deepnorm_result.png","hash":"138cafc159f1e2e02455c540b4754f7cbb7f521d","modified":1711208592246},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ics_define.png","hash":"6bf3240ef78bad2cf76897a29c05428f4c195fba","modified":1711116459766},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/9B.png","hash":"7de19972e48b1f43c501d60a1c43a28c46b198e8","modified":1711618166808},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/long_context_result.png","hash":"daea6f734d64bdf5d24c6e17a640f23b1bd35b5f","modified":1711531324567},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/mqa_result_1.png","hash":"a052b57f71eb78e3158ed2ee06ff0e5597607a2f","modified":1709975039443},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_model_param.png","hash":"7b838274937cf45d73e59ac1fb5c2034e46586ae","modified":1713683464457},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_expert_specialization.png","hash":"64947872486dd083a7076bfdfe67cb0626208579","modified":1712805838590},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_less_activated_expert.png","hash":"ac7ad86dabe94564bdb17399231c6ddc7da83962","modified":1712806182926},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_long_perf_1.png","hash":"bc9c40bde860e78882965a25056a848aa4a89c77","modified":1713699132210},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_compare_gpt3.png","hash":"311b21079599473054378e339885e0b87719e63e","modified":1712848303496},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/mistral_8_7b_active_perf.png","hash":"0c67d935657e62b9e8eeabc6403c269e09016626","modified":1713700314192},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_137b.png","hash":"fc11cfc87b2994956a9adbee76621fe5d964dc30","modified":1713447317172},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe.png","hash":"2a2ee095b8cc0727daa2cdd0c63891e4a470eae8","modified":1712042970123},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_z_loss_result.png","hash":"3edd8e9eb069c9557c98fd21600c02b3a1978cc5","modified":1713858286214},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_remove_multiplications.png","hash":"52bc94bfe726dfc832534dde409154efe0ce7b0f","modified":1713856140610},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_more_add_noise.png","hash":"12d13a9ee6c28553626e469ed18d022e0176a873","modified":1713856976557},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/vanilla_moe.png","hash":"7da0a6e9d529256107b5f6b287737ac47513a797","modified":1711962655018},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/result_4.png","hash":"6f554d1b6911c3db211a7e57e886e62f03b46ffd","modified":1716985505197},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/ablation.png","hash":"4c73c83eb527141e17d1109b6c2cff3488de6259","modified":1717250998001},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/benchmark.png","hash":"83084a5f64006000898d5252b3f8afccb635b3f0","modified":1717246623701},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/intro.png","hash":"3ff9cce772cd825ec8b88591d576a5f52982d679","modified":1717231737607},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_t5_result.png","hash":"83b63aafceeb8f2bc3f89ee6a1e3caec7987a1c9","modified":1715674742164},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_booksum.png","hash":"276dfa014d35f7d3375fbb7cee6eed21127f9955","modified":1715160163895},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_ablation.png","hash":"babe57024a1c2212405220124dfd376a8a2bcfb6","modified":1715242576296},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_language_modeling.png","hash":"a49a7cc02694e7017c2a20dc0666046108b3c4c5","modified":1715159444656},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_passkey.png","hash":"811c5c677f7616b6625a0b86f2004f6d3ebeefe9","modified":1715160050601},{"_id":"source/_posts/cs/nlp/2024/05/-/stremingllm_attention_sink.png","hash":"933f34f3bc1d04e2b36f3305ac9fe2acd8bc9939","modified":1714383008964},{"_id":"source/_posts/cs/nlp/2024/05/-/xl_vanilla_sw.png","hash":"3259a751066a0083ef249a0412f43fb582e6544c","modified":1715138182854},{"_id":"source/_posts/cs/nlp/2024/05//eng_ppl.png","hash":"fccca1509ebab2e89a3ceaad0dfeedc700de2691","modified":1714841526104},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_dataset_dist.png","hash":"e5754afcb70a45c0d11ee5db43c724350ec64257","modified":1714913484177},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_lost.png","hash":"b9d73b8022266af17789ab049c7adda621729cc9","modified":1714913979224},{"_id":"source/_posts/cs/nlp/2024/04/-4/transformer.png","hash":"9dddf171ca51f2ed1218baa9b84f4b98e9b911cf","modified":1713605151549},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/prefill_and_chunking.png","hash":"0c706e0728ea462b2b00c59a97c79ccf5f05b598","modified":1710516401027},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bs_bn.png","hash":"aa28241d75f914603b9f7f67cc54db4e61bac668","modified":1711113379148},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/base_model_eval.png","hash":"9b4e65d246865683f8e3348d74bfad03c937b65f","modified":1711616539832},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/rmsnorm_eff.png","hash":"350a7a2703eef1ee9357609ae5820bfc30835681","modified":1711166117196},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/sft.png","hash":"ea9aea143af836012f44d21956ab5455487e9bfb","modified":1711615463552},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/perf.png","hash":"3c068a423dcd32cac7f1630bd69fbe5a4c6789af","modified":1711614561095},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Scaled-dot-product-self-attention.pbm","hash":"03da711b1547c944deea60d9bf345eb30e7c566f","modified":1709638849226},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/GQA.png","hash":"d6b0cd20ae5f68a96f0865b3d0b6b129969caa4b","modified":1709982190361},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/GQA_result_1.png","hash":"87f2c3632fdf83828e8bd07a95cd8e7bf277fc88","modified":1709982952107},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/gpu_cache.png","hash":"edb6b1abdecd3099f2d68c2a729c0ca9b1fb0db7","modified":1709970717456},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/transformer_structure.png","hash":"87f0258e43922eface0277e13167a4ba8c1402bd","modified":1709802508932},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_vs_open_models.png","hash":"ba0348d3fe68a27f8c6435c4c3a6d08d9c8869c6","modified":1713698848988},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/cover.jpeg","hash":"6226a5276377816b37a20572a8b725af3ddf5760","modified":1714102101607},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_vs_closed_models.png","hash":"6211c05b7d69194f2d820622525273110467a0d5","modified":1713698929131},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_2b_less_expert.png","hash":"e03a00a194efd33890517b4ad642bca5566cf9df","modified":1713688653303},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_ablation.png","hash":"108dc8d66ae0e370e7969403efd85178b9a8523a","modified":1712805107241},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_family.png","hash":"9d813f12f82d8702886f7ede72c5a25151390ba9","modified":1712932424673},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_moe_family.png","hash":"21a6c80f1dac39eb364fb417ed83afde2b212675","modified":1713449413788},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/mistral_8_22b_multiling.png","hash":"4af49ffc09a0de3793ec7137d3dfbddc9c309d38","modified":1713706946046},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_related_model.png","hash":"0f109231fc1b425c7364401c41ce5f3aecfd76c7","modified":1713709711124},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_capacity_factor_speed.png","hash":"60b624b7595e1f90763ea745ea358b6852eeefb0","modified":1713881294917},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_distill_sft.png","hash":"7a59660f367634d68aa392698042f3d9cfe190da","modified":1712979135172},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_dropout.png","hash":"63f36ca61aaa71e88ddf23339e63a9ccd898a6ce","modified":1712934811239},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_init.png","hash":"f9fb36a0defac7a5990b5c58a614f3165af0ae3e","modified":1712934593938},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_more_dense_layer.png","hash":"67c37482d73cd7ece7e384c0bd73e6891fc752e1","modified":1713856367887},{"_id":"source/images/background/wallhaven-2ywymm.png","hash":"aeaf089b64472a1c72cdd9778c5deac4086eb38f","modified":1707045207190},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/result_2.png","hash":"6baa634147220fed9edfff7c70e83c56a2b24913","modified":1716985357034},{"_id":"source/_posts/cs/nlp/2024/05/-5/ntk_by_parts.png","hash":"5b49750dc6a2d1b878f34bc71e3961d96282499a","modified":1714809199814},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_sd_algo.png","hash":"576ffae274518c5a4e6c049d199e0714b06bba86","modified":1715671754784},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/odpo_intro.png","hash":"e32faada4824a2654e32d125cb7dded9895f87dc","modified":1717141374659},{"_id":"source/_posts/cs/nlp/2024/05/-/stremingllm_exp.png","hash":"cdd21419055ca9bed86b46675e118ad4bdf55544","modified":1714394556802},{"_id":"source/_posts/cs/nlp/2024/05/-/stremingllm_perf_4m.png","hash":"c4545d7f0bbd0c5f6baa85dd64b747a3723fdf2e","modified":1714394913980},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_design.png","hash":"e96faad18cb526d201ce069f9ce09dc3c9c0d16e","modified":1715245088507},{"_id":"source/_posts/cs/nlp/2024/05//eng_config.png","hash":"8edac537bd1aefea28406c414e0c0a4c888234be","modified":1714840521116},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/toxicity_control.png","hash":"42173484be1fd33e29244f43d658ba03ec9bacb2","modified":1717155371840},{"_id":"source/_posts/cs/nlp/2024/05//pose_passkey.png","hash":"6202690a895f0114c90f6821c8cf1ad7388e1592","modified":1714918508416},{"_id":"source/_posts/cs/nlp/2024/05//pose_ppl.png","hash":"fded043b94c97c1a782869963f5dea371e257b80","modified":1714918324529},{"_id":"source/_posts/cs/nlp/2024/05//pose_method.png","hash":"db8784c9e4b14c5963f62f072df6a4c3c5405874","modified":1714916547117},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi_explanation.png","hash":"df22d646e5a47e6223b7722007cfb7dde7c27280","modified":1709198077742},{"_id":"source/_posts/cs/nlp/2024/05/-/stremingllm_init_token_num.png","hash":"ffe78c31d901311249530e786bc5ed321e2e242f","modified":1714392839583},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/warmup_effect.png","hash":"3e936786065e1ab9cbad17f5b86a5b8129720270","modified":1711204451931},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/eval.png","hash":"e78d1d820de4c455b9301124d6016a19762eae1f","modified":1711446016442},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/pretrain_data_dist.png","hash":"8c66a625723cb87ee67a9ff60d3614b369f50592","modified":1711463822846},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/multimodal.png","hash":"b14a4eb4d377101acf7b50904b9ee0f1d473aacc","modified":1711614412507},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_infer_efficiency.png","hash":"34245e99c2b29dbd54104ccbbb3d8c15706307b9","modified":1713699575045},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_perf.png","hash":"ca3c95d4b1c1c8f986d7adcacbf04da444e91610","modified":1712049369889},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_capacity_factor.png","hash":"a960540d3419de77c3823d343247abfddeadde1c","modified":1713881000881},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_specilized.png","hash":"138ad5a388c77cc02202de794ec4cb734d633065","modified":1712043657680},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_multiling_specialization.png","hash":"bac636d8da61768adb5a9b7c5bd75547267ae470","modified":1714048322261},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_models.png","hash":"bc59770bc8ea44bfe480484a99aa9143acbaa6fa","modified":1713795616340},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_structure.png","hash":"d906a9148e035025683e8da1eee5fa3d87164aa5","modified":1713585604066},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/result_1.png","hash":"725c6be46c42e8fc8184304bb0cdf5071b09e8b2","modified":1716985304979},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/hyperparameters.png","hash":"b3595e75eff0cb8ea8f86fd9e2f8c6ae5f7892bc","modified":1717242348046},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/dpo_loss_code.png","hash":"279b32cc1c4dfefa8790fcbb597659e8b974ac61","modified":1716975072126},{"_id":"source/_posts/cs/nlp/2024/05/-/acce_alog.png","hash":"61f4653292bbd93debee66cebfb44ac7198e5818","modified":1715937599067},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/scaling_function.png","hash":"b554e0bd697e72bb2e5e24a16c678d80c2efcc52","modified":1717156880112},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_compare.png","hash":"9b272eba596a790b1d40ef3a8b041bdffe1660d3","modified":1715159194851},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/sentiment_control.png","hash":"fc200cc3802fdee9d80e4bc259f4baca7b425ae7","modified":1717154380930},{"_id":"source/_posts/cs/nlp/2024/05/-/acce_k.png","hash":"ca37e1983347a1a835389de4d17047e3b0d02af4","modified":1715691856143},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_ppl_figure.png","hash":"34397c88e268b769c97a2b6e2ad63bf6ad5270ef","modified":1715241210130},{"_id":"source/_posts/cs/nlp/2024/05/-/streamingllm_model_ppl.png","hash":"2fc1c11d7cfa2598b414e5e4c145181ec9e10648","modified":1714381636336},{"_id":"source/_posts/cs/nlp/2024/05/-/xl_attention.png","hash":"395424d6c048880e143b9b2f93585597fbebebd7","modified":1715138841538},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_perf.png","hash":"5cf407e2e2ee61bb2b6bdae0570c3b8c4a3a9374","modified":1714913782279},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi.png","hash":"193df6c392d71dd4b90202d9be97f000f37b68ec","modified":1709197025669},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_perf.png","hash":"c9d7ce0a301920c4e722e341200f311995923735","modified":1710558943189},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_swa.png","hash":"59037b91ba8f256fd89b3d60b8ce477e4c8f4b3a","modified":1710252446580},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/28.png","hash":"d438e857378575809c880b78ca715dc69e50b364","modified":1710643355076},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ics_measure.png","hash":"e9fe87cfea7dcef7cb66e1d76c17d883cbbc3cbd","modified":1711115709830},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/mqa_result_3.png","hash":"12e310102ace1f9e89c0e9a352cf4a3462335a60","modified":1709986493059},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_16b_perf_1.png","hash":"d540e8a58e166c0ba894708c9b0c277c31107487","modified":1713690316202},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_perf.png","hash":"035a992ef2e7e5a9165c9488e2696e38fa29165c","modified":1713699612482},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_structure.png","hash":"b0564913ecc1f78e5052dbc07eb65b3f048846e3","modified":1712752326556},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_model.png","hash":"63c95ea5ae77528f7d94a94b21cf12ed63e0bfdb","modified":1712849528728},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_16b_perf_2.png","hash":"6886e18a2e7601202e8721b83f998faf028e19eb","modified":1713690412061},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_upper_bound_13b.png","hash":"cec21f0cf3809011ce210dffda35da3671147008","modified":1712803976611},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_compare_gpt3_2.png","hash":"89172dda5ac569780ea47e85bc43eaef1d6918ae","modified":1712848396056},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_perf.png","hash":"b6ed751d2f171dac971bcf1d7f12e8a2d7fad388","modified":1712672178340},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_distill.png","hash":"be5cae81fafbda6b638ac185421bd04e00b7a60d","modified":1712978653926},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_scaling_dense.png","hash":"b8de8c427de51d62e69915da7a97bf4a9b505317","modified":1712976996880},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_scaling_time.png","hash":"b56568665d2680cc0723269ae39dc4b60de1b01c","modified":1712976813025},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_pretrain_result.png","hash":"880f70f371b8392e3021bf56d282be5640c232f7","modified":1712135713134},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/intro.png","hash":"c5175eef020ec3499aa67163813eab1c4c13a84a","modified":1716887862324},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/ln.png","hash":"4d4c831fc95c591ea0415e07dd5f46d1b1494e60","modified":1717251987860},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_example.png","hash":"edbdb72af30cac5f036e47b3d0d426919f336e62","modified":1715609386236},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_starting_tokens.png","hash":"18e3a03213a7275617201b71eb274cd8fd8b0bf9","modified":1715178569704},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_dataset.png","hash":"b85a9077cffcace583a7dbcdbd235ab646086ea1","modified":1714913285654},{"_id":"source/_posts/cs/nlp/2024/05//eng_tokens.png","hash":"b6c43c289004164e90de39c30e170de5ac1088aa","modified":1714890071917},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/21.png","hash":"fb2577b5fa73b06b786484b3723f7aa3819638a0","modified":1710643325115},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/realformer.png","hash":"3bc805db3177c7e6521362b063543941da8d2bd3","modified":1711206089667},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_sft.png","hash":"8bc26fcfef1aab448d0db0b28666852f62961a3b","modified":1712821943440},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_perf.png","hash":"01fb9d4f232e9f0b86abb91dbe5d8fb9fac456ce","modified":1712933127383},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_result.png","hash":"32ce9bba1b65ceb2e69c079e113d8b4c524bc479","modified":1712067349025},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_algo_1.png","hash":"aa8bcd982c78e4304c63f81e44b20519dc04f18f","modified":1712070648688},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_perf.png","hash":"27157f620e2b7c4be60b2a58f7857a888794cde1","modified":1713968695747},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_hierarchical_gating.png","hash":"067160c735e9c0b8cff777df60d52dfca21ea783","modified":1712045417559},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_perf.png","hash":"58927fa39b5e56e8da00144b417bbae5256d6bdf","modified":1714048423893},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_diff_expert_capacity.png","hash":"23f2234b45a2a05ff8b098e68a361a71f46816e9","modified":1712133650991},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_distill_diff_model.png","hash":"05c4d885f5563e3dbd56c055a52df5c1531677a7","modified":1712978973713},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/ln_effect.png","hash":"714ba6cdf67ea33d507e3358b113a94bcd24e1ce","modified":1717252559659},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_alpha.png","hash":"f7f5a24106f1b9d16fd805b3ed3d3c2efb4a8c03","modified":1715674930516},{"_id":"source/_posts/cs/nlp/2024/05//eng_data_dist.png","hash":"157286ce140bd5acc7c45fb12785649cc7214472","modified":1714837490239},{"_id":"source/_posts/cs/nlp/2024/05//eng_data.png","hash":"9262b3bbc1b415d9c776723fe85c0a43f9fb562a","modified":1714835715477},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_rope_ext.png","hash":"43e1131d146b8104e6936d73aca51d8ace4aff61","modified":1709206562025},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/20.png","hash":"5d42628c8dac91c9671a58535b730e91966c0cbc","modified":1710643321378},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/cover.png","hash":"0493fd58fd2dad33394399d960924dbff6b386b1","modified":1711459395465},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_comparison.png","hash":"20c7e90a390604d2146b4984678524046ad941b8","modified":1712803733203},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_model.png","hash":"5179df7e42bb7dc49fb6f92f4ec68ed820aeaae2","modified":1712068764148},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/mistral_8_22b_code.png","hash":"f9b78e0669e0c83f716c8e4abfa4d97b6f9b8143","modified":1713707063629},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/mistral_8_22b_reasoning.png","hash":"ce00d9ba7b8a65eb238c99f79a17ca7a3cd2238a","modified":1713706908014},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_encoder_specialization.png","hash":"382cb53fcc2e9172ffd7554a04714feed4d9706b","modified":1713882897561},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_scaling_step.png","hash":"dbfa55ab1c94e266344315fd215f2d646eaefda1","modified":1712976642477},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/main_results.png","hash":"a7388a503f0157c2ebe9ef765d63daab358b67d7","modified":1717249875414},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_gating.png","hash":"5e875c3de6bde62ea8e15ea4995a56f9fd28d67c","modified":1715159702016},{"_id":"source/_posts/cs/nlp/2024/05/-/streamingllm_compare.png","hash":"40394890082b1666d1221f302ed52a80fc358477","modified":1714317603622},{"_id":"source/_posts/cs/nlp/2024/05//eng_sample.png","hash":"6b0d7ed89b16a3e9c6297219f33e59f204923828","modified":1714890968069},{"_id":"source/_posts/cs/nlp/2024/05//eng_needle_comp.png","hash":"e4c9f5c51548faf11a7ff64d23aae5bd2927ab0e","modified":1714830276600},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/lihongyi_self_attention.png","hash":"39db6256143fd9a494e848240a8daa434aaddea5","modified":1709965340148},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_145b.png","hash":"502a377d8625d78d2e3ba7281bfb11732c14a61c","modified":1712822142201},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_upper_bound_2b.png","hash":"4338539fe123371c5512c730fc8a35864236323b","modified":1712803847753},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_capacity_effect.png","hash":"16268929d0ec965d3771fccbb595b75d82f05912","modified":1712134554223},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_sft_result.png","hash":"0dd7cfaac788c5d112dd10fe98f0052b369420bf","modified":1712978414761},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/vanilla_moe_result.png","hash":"e491e44cfff384422f3d9cf87cd5e52fd976aed7","modified":1711963546083},{"_id":"source/images/avatar/shadow.png","hash":"da67a14e556fb3223c1c4ec8b9f33eb52c612f50","modified":1706779539112},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/gradient.png","hash":"58524a8fd10f4d76e8e419e1ed46bd9a99cf58d5","modified":1717234900278},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_intro.png","hash":"6b810c88945281a9cdf9749941cdbe08346fd42b","modified":1714914076240},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_example.png","hash":"946cdbb0a425b9d11b0ff587007885990abc9f99","modified":1714912943693},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/17.png","hash":"8ea1c5d90f3da5c469eb17aea50f377cb9c28ba0","modified":1710643306201},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/18.png","hash":"fe0a8e7005110abca19bc7ae506f3e35042b70ec","modified":1710643312126},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/26.png","hash":"f74d03dae65109740f48924f30b52a742b5e4273","modified":1710643346427},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_ics.png","hash":"f92751ea20430f25caa3d6bb892c5894bf7509d6","modified":1711115173958},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/ict.png","hash":"2445ecbbf5ec6a96695f21c03a5fcbf67640b9f0","modified":1711615224209},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/10.png","hash":"2c52d4f90dd9356eeb4c9a39f1df1038ccec4693","modified":1710643262247},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/3.png","hash":"4c2c2a30d9ac8db03bab56da5d16ce2042ef73bc","modified":1710643220743},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/6.png","hash":"4b32a49bfead98f5238871b81076176e38168333","modified":1710643241274},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/pretrain_data_pipeline.png","hash":"91218a2272eab9284904c91bacd8d8a40e3c1580","modified":1711462239524},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/19.png","hash":"1d7b929e709657c9b7d7ca4da8eadc8c4ca4b3ca","modified":1710643317015},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/1.png","hash":"a8898b3f3b7c64fabc5fad9bf8ef5524501d2aeb","modified":1710643881936},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/24.png","hash":"b89b0a0ca774a4efc1ece628fb20379b5f6a0b69","modified":1710643338042},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Scaled-dot-product-self-attention.png","hash":"983eae2b767df413ef3211ddaf31f1b833d7c86f","modified":1709986303475},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/8.png","hash":"92b0e3b75ce97bbaf4aa69e484216702293589ee","modified":1710643252580},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/27.png","hash":"557c04a2134b6ae147e076cdb80de1730e937d9b","modified":1710643350772},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/9.png","hash":"2a0fb56563b13411035ed41a3ad882f66f948b26","modified":1710643257459},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/11.png","hash":"9b964f3aa6f82a09eb2f2f944508bf0a9d29efb3","modified":1710643267156},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/22.png","hash":"3b27321ef8d76844f6720e1a27d65d1946d48ea7","modified":1710643330423},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/23.png","hash":"9a9af6308620b59f2ee00a3d0da4e942d953e406","modified":1710643334116},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/25.png","hash":"aa259b58be90eed6af0c4ba800a991b9464453d0","modified":1710643342514},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/15.png","hash":"15abcbcf7340941e98dac7a0ab42d922e7fea1b4","modified":1710643288058},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/2.png","hash":"768421239ad7c838dd86714fd9f17b3c73cbb887","modified":1710643214870},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/4.png","hash":"4dacbfb89079d528da1208773961e1366debde9b","modified":1710643230331},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/14.png","hash":"71f75960246f7528b3b83b84f8f91775f9e2fb45","modified":1710643284450},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/12.png","hash":"c6c493b14e0a1cc4863a912c4ccc998de194bfc0","modified":1710643275198},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/16.png","hash":"ba7b2bc65e10389cf9a87ddef69f462e806304f5","modified":1710643293307},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/MQA.png","hash":"7e3f3037311be60e79a7b5388338febc9f3b6d7c","modified":1709986434286},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/7.png","hash":"95640525b0706d3118eeb88c6c4c6217a96c39d0","modified":1710643246812},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Markdown _  Nice.html","hash":"c905c942579a520c7b3c788a00cdb9ae359d4a32","modified":1709897264700},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/13.png","hash":"68b7da3e3074e4d6995eaf96c7d8cf622eadffb7","modified":1710643279584},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/5.png","hash":"7ae786b309a757c5f61a713c7ceef4d2824b024e","modified":1710643235878},{"_id":"source/_posts/cs/nlp/2024/05/-/speculative_decoding.png","hash":"fe277fa76f9f9c71e2030a41ca9eab458c33826a","modified":1716543143782},{"_id":"source/images/background/wallhaven-p97q73.png","hash":"e83875b9cd1dcfa87f0e76dc1a327cbe2921c5da","modified":1707045618160},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/ms_invest_mistral.png","hash":"faf324c0b57843516a0b256750e6475ec0c2ce93","modified":1710316114714},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ellipse_1.png","hash":"ef2470f6bf1511dc9aac9f1c6489b9d2ffdcb45f","modified":1711006814272},{"_id":"source/_posts/cs/nlp/2024/05/-/digimon.png","hash":"247f4059dd9671047f5d6707d8cef75a93d93f40","modified":1715070986892},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/norm_in_nlp.png","hash":"7be79b0e55d7d00ff6c16c247d0e506771453380","modified":1712575607757},{"_id":"source/images/background/wallhaven-gpxpg3.png","hash":"d798eaa0d679ed8a14022b0741098711c3402ca8","modified":1707045245660},{"_id":"source/images/background/wallhaven-x636oz.png","hash":"d419359164da44691e2bc8641256ca7c8111e227","modified":1706875075740},{"_id":"public/baidusitemap.xml","hash":"6bf4ff314dcba6530ea9f19f0656f712657b0a52","modified":1717292381214},{"_id":"public/search.xml","hash":"c7e2481cbd786a9c59cc8c185ba216f6ae239701","modified":1717292381214},{"_id":"public/sitemap.xml","hash":"f5fe98e51953d3ea66d7ac6a29b0e062eabb5c58","modified":1717292381214},{"_id":"public/sitemap.txt","hash":"bd9e194bbdf9a677761432a1834de75e17662805","modified":1717292381214},{"_id":"public/tags/index.html","hash":"d628104b823e7339a50a766747e4675b29484105","modified":1717292381214},{"_id":"public/about/index.html","hash":"f74720e8acaf1e5dee82d6fb9f4d6c5975cd9aee","modified":1717292381214},{"_id":"public/categories/index.html","hash":"184ce04272b3506d0cefe616f14a13e41d1dbc79","modified":1717292381214},{"_id":"public/280fa97a.html","hash":"9a9f9fce8ea52dad3d2bb903e69b0c76501a26c3","modified":1717292381214},{"_id":"public/da871ebe.html","hash":"08b3bb60642326b7f333ba4ed1325c8db25394ea","modified":1717292381214},{"_id":"public/473f2b43.html","hash":"bbda8ee496068f86a41ffff9814a15b5d80e2527","modified":1717292381214},{"_id":"public/7c04944d.html","hash":"3fb41e7d6b50e429f4057e6e5520a5dbea511771","modified":1717292381214},{"_id":"public/f5c015c.html","hash":"411bcc06c7854de6d15ff94a7b241f1614ad20e0","modified":1717292381214},{"_id":"public/45ee1a6d.html","hash":"15b1904277bdfbee1dedcd6f38295fc811d82501","modified":1717292381214},{"_id":"public/cc852861.html","hash":"43e448e9953feb12c337e7ff87ca39691b8aaf84","modified":1717292381214},{"_id":"public/336f2f3e.html","hash":"16179d59788a60abf8544c3249ed36db4c0865da","modified":1717292381214},{"_id":"public/1736008.html","hash":"c02d54eae16615f9b54d91cccf6d265e7795ccde","modified":1717292381214},{"_id":"public/b70b4a2d.html","hash":"1afbf5d2af337f784592d4dda60a4650cd0a41aa","modified":1717292381214},{"_id":"public/44e38c1b.html","hash":"8a22e64390e323eaa853e9af95a32abf8842f2fa","modified":1717292381214},{"_id":"public/41b6a819.html","hash":"f56755fa28b17d09205dce381f2e98ea320c713a","modified":1717292381214},{"_id":"public/ad0bba9d.html","hash":"2cb2336d7b986f2d90f87f7c380a03547fd4a109","modified":1717292381214},{"_id":"public/6a40bfa5.html","hash":"f4c787f1f7ba25af720c6a6e1761b300533098dd","modified":1717292381214},{"_id":"public/3345028a.html","hash":"4bbbddaa71304d7af7bf7588b0277eeb67928673","modified":1717292381214},{"_id":"public/c61d17e3.html","hash":"ecc5eba9b8749efd78aba13116eaf88a3a70de19","modified":1717292381214},{"_id":"public/3dc22f96.html","hash":"3e03e48dc9b1f5fc76b1a6c35812c3817b0696e1","modified":1717292381214},{"_id":"public/c4da56c0.html","hash":"f50d33eb6a21c399e466df5eb01d3dc7279ceaca","modified":1717292381214},{"_id":"public/14e576c.html","hash":"0cba72dc906abf8ed8483b37d2a5a907ca198509","modified":1717292381214},{"_id":"public/a051710f.html","hash":"42cd262a51f5e0abb2c4e306d1ef6d1df3096d41","modified":1717292381214},{"_id":"public/archives/index.html","hash":"dbee5b440a0a63fb518fb3daaeea4d54b253cfb8","modified":1717292381214},{"_id":"public/archives/page/2/index.html","hash":"ac6947a5e3e64ea9aa3a2607ce3bbbf2610240bf","modified":1717292381214},{"_id":"public/archives/page/3/index.html","hash":"17eaddf426fa8fffa9d135c011c29077540a1dfb","modified":1717292381214},{"_id":"public/archives/2023/index.html","hash":"982e7ee029636b65f107a89eef356f458be5a831","modified":1717292381214},{"_id":"public/archives/2023/03/index.html","hash":"58369f3cc4075414bbc7605721f3d9c2ec006bbb","modified":1717292381214},{"_id":"public/archives/2024/index.html","hash":"312e22607ab43f5108ff00b330e70ee53c56a1d6","modified":1717292381214},{"_id":"public/archives/2024/page/2/index.html","hash":"fecb5137bece91efd78c54baea9a02ac8ae700e5","modified":1717292381214},{"_id":"public/archives/2024/02/index.html","hash":"876e8caf1a37e1e5125fb511be9a43edb6e93c8f","modified":1717292381214},{"_id":"public/archives/2024/03/index.html","hash":"963ef05c0942bc5204d6ec0627c479fc70cb386d","modified":1717292381214},{"_id":"public/archives/2024/04/index.html","hash":"51195a4276403c8916119b04fd4b1af5f91d3d5d","modified":1717292381214},{"_id":"public/archives/2024/05/index.html","hash":"b7918bb67cac48cdbce3a1bc5503053572907245","modified":1717292381214},{"_id":"public/categories/CS/index.html","hash":"7ac57cc9060256d416e9cb4356f323a2fb4ba0e4","modified":1717292381214},{"_id":"public/categories/CS/page/2/index.html","hash":"86f75522816a095c1daca4d0769b46229e64b87c","modified":1717292381214},{"_id":"public/categories/CS/page/3/index.html","hash":"f8a9de3cd1c88b7dfca5ba9124552b704e40feaf","modified":1717292381214},{"_id":"public/categories/CS/NLP/index.html","hash":"190c0ad2ad4645b74990f33f4ec9063208d88bbe","modified":1717292381214},{"_id":"public/categories/CS/NLP/page/2/index.html","hash":"ebefabd7c56442ff3c671208cc16f3810d8424f0","modified":1717292381214},{"_id":"public/categories/CS/NLP/page/3/index.html","hash":"9650a8ae9acfd5b94ae2677324f35836b3e39843","modified":1717292381214},{"_id":"public/categories/CS/NLP/LLM/page/2/index.html","hash":"92500b4d546504e11d09e55ce82552498a332bea","modified":1717292381214},{"_id":"public/categories/CS/NLP/LLM/index.html","hash":"8c7bc651fae6f0c91f0323a3f15046b74ece51e2","modified":1717292381214},{"_id":"public/categories/CS/NLP/LLM/page/3/index.html","hash":"3e4530e296bf07e4c06652a03de716a6af8bcfe7","modified":1717292381214},{"_id":"public/index.html","hash":"82b58006396d9e0fec1b343b0c592805417986c9","modified":1717292381214},{"_id":"public/page/2/index.html","hash":"2101f9820dba1adb657b54b901d235c1e28f7a36","modified":1717292381214},{"_id":"public/tags/NLP/page/2/index.html","hash":"c638a7e17fa99e756b4a87612cd69ccd530d7145","modified":1717292381214},{"_id":"public/tags/NLP/index.html","hash":"bd855e34b366f54caa581ebdd382d4f7c8fe180d","modified":1717292381214},{"_id":"public/tags/NLP/page/3/index.html","hash":"02fad7e5c9fae4093615bf50e944ce46284330c5","modified":1717292381214},{"_id":"public/tags/LLM/index.html","hash":"e9c4254c730f7e6bb4a51a8ddf0e4a91ce05110c","modified":1717292381214},{"_id":"public/tags/LLM/page/2/index.html","hash":"d45c659173b08ad97ba5e7841ce1042287f31360","modified":1717292381214},{"_id":"public/tags/LLM/page/3/index.html","hash":"c3e82c68ea0499f1a93de4aaaa88646ae5ad7df5","modified":1717292381214},{"_id":"public/tags/transformer/index.html","hash":"bbe6d7d82dc644ba571e89b2e454ab5198e7495d","modified":1717292381214},{"_id":"public/tags/transformer/page/2/index.html","hash":"39194231a487189bd89c0ae423deb0b4c992060b","modified":1717292381214},{"_id":"public/tags//index.html","hash":"ea7c44de38f939f0933f5af2e1292b7ac4f9d15e","modified":1717292381214},{"_id":"public/tags//index.html","hash":"885628c7626cbeb9e6b1d3dc6b2c68369c1b6436","modified":1717292381214},{"_id":"public/tags/SFT/index.html","hash":"4da91dfda9eb276b392a812fbaa02a62cb4229e3","modified":1717292381214},{"_id":"public/tags//index.html","hash":"fb4b8b8eb312364e2d328c007eef92c858a7afd1","modified":1717292381214},{"_id":"public/tags//index.html","hash":"8ae6863506e73bc268cd9dc8bce12f91a897f568","modified":1717292381214},{"_id":"public/tags//index.html","hash":"4585cbfbd9c507279ed217504ed4309ff5af62c6","modified":1717292381214},{"_id":"public/tags//index.html","hash":"6e73e9620250ec4ac5b1559f197ca99131a597b1","modified":1717292381214},{"_id":"public/tags//index.html","hash":"7d2634d9f8a1bdffa62caa4c0b08e38e236a4a0d","modified":1717292381214},{"_id":"public/tags//index.html","hash":"eaf19301a8880bd135011aec3cf3ec4351d2b6ac","modified":1717292381214},{"_id":"public/tags/attention/index.html","hash":"9878ac482dee432662d04c6fbcf1597adb5d6e20","modified":1717292381214},{"_id":"public/tags//index.html","hash":"8af54ffcde8c15bfe9246bde3112d7f28811917e","modified":1717292381214},{"_id":"public/tags/positional-encoding/index.html","hash":"bb27301a2200a386b950bc09505ac40e08a74644","modified":1717292381214},{"_id":"public/tags/RoPE/index.html","hash":"f2fd8c7a4b80ecd855c5a34c52c407b323fc5cfe","modified":1717292381214},{"_id":"public/tags/layernorm/index.html","hash":"a05509fc00197e1a66aecb4d37023315b285d485","modified":1717292381214},{"_id":"public/tags/normalization/index.html","hash":"fda60f62efbef0d158198b03817750804884e70c","modified":1717292381214},{"_id":"public/tags/batchnorm/index.html","hash":"f9b93830e70f63263d3a4fc5000ddb1a0748dbf3","modified":1717292381214},{"_id":"public/tags/sliding-window-attention/index.html","hash":"65b8885fc322eb2b84fdbeb1b46d550f4403df16","modified":1717292381214},{"_id":"public/tags/sparse-attention/index.html","hash":"56fb56e5e8a7ce2edd0b8776bcf6281c2f38f449","modified":1717292381214},{"_id":"public/tags//index.html","hash":"1b7280e14480abde46c33a325269a3abfadb242b","modified":1717292381214},{"_id":"public/tags//index.html","hash":"a1431e9184bbbf6bc58dd0fad8dcf082af79d972","modified":1717292381214},{"_id":"public/tags/post-norm/index.html","hash":"19493ee63561776b3202821996d817a318e1aaa5","modified":1717292381214},{"_id":"public/tags/pre-norm/index.html","hash":"7f9720a5b62856240121d8a670ab6b7eae1d5c24","modified":1717292381214},{"_id":"public/tags/ChatGPT/index.html","hash":"b9e9849d7cb1922487ddf9d7e6237594fee3ab95","modified":1717292381214},{"_id":"public/tags/LaMDA/index.html","hash":"a5440bc366a317bbb742bb16cbeebf3dba31193e","modified":1717292381214},{"_id":"public/tags/Sparrow/index.html","hash":"5e29417c8baabdb422c131ad329193c32737c548","modified":1717292381214},{"_id":"public/tags/GopherCite/index.html","hash":"203cb903c5cc3ca354e359efa8b8dee5aa427e96","modified":1717292381214},{"_id":"public/tags/WebGPT/index.html","hash":"7b4c7eb140928a1304178cbdb77ab1679622dc4c","modified":1717292381214},{"_id":"public/tags/InstructGPT/index.html","hash":"995fe0377af2b6b6c0212e8d7a2039d4bea4e748","modified":1717292381214},{"_id":"public/tags/MoE/index.html","hash":"bd01ffecb5bbe001e80494e79ad0c10b3d5f54b2","modified":1717292381214},{"_id":"public/tags/KV-Cache/index.html","hash":"8a0504a201508a3a6800bdec6a5eb43cb35d96b2","modified":1717292381214},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1717292381214},{"_id":"public/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1717292381214},{"_id":"public/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1717292381214},{"_id":"public/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1717292381214},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1717292381214},{"_id":"public/images/qrcode.jpg","hash":"eab6c8893d0c79cf705cf64d738f560563365b0c","modified":1717292381214},{"_id":"public/images/cover.png","hash":"2f2aa6173619dd38425673ba110b50b9156d4d10","modified":1717292381214},{"_id":"public/CNAME","hash":"07ad8fed7f1b810bf3715a0e68324e5cbb1d47ac","modified":1717292381214},{"_id":"public/images/avatar/20180303210737_XsJVr.jpeg","hash":"2ebbbcead4446a69b0ac0698ab227a1af49735ce","modified":1717292381214},{"_id":"public/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","hash":"3e7db51e59a8261de04f55814475556494d153f6","modified":1717292381214},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1717292381214},{"_id":"public/images/favicon/favicon_io/android-chrome-512x512.png","hash":"419f0b0317ad2a31acfa7dc19e1502731e92e05f","modified":1717292381214},{"_id":"public/images/favicon/favicon_io/about.txt","hash":"d1f084751c81e98b1675b8c0745f6f306a1eaf66","modified":1717292381214},{"_id":"public/images/favicon/favicon_io/android-chrome-192x192.png","hash":"2164889376f2bd6f9dd2066ebdfe4abd547f49d9","modified":1717292381214},{"_id":"public/images/favicon/favicon_io/favicon-16x16.png","hash":"fb63ee2fc88b9ce42dec5bf68088da14be9350f5","modified":1717292381214},{"_id":"public/images/favicon/favicon_io/apple-touch-icon.png","hash":"5e36b0bfe67bcf96f4f0e92366a5de509fe9a78a","modified":1717292381214},{"_id":"public/images/favicon/favicon_io/favicon-32x32.png","hash":"f42c950146a171afca4ad70ff425b5b4ff02f926","modified":1717292381214},{"_id":"public/images/favicon/favicon_io/site.webmanifest","hash":"bf31baf91bdd2fcde24a45e3f2a1be33733c6f69","modified":1717292381214},{"_id":"public/images/favicon/favicon_io/favicon.ico","hash":"fceaa7158ae89a98fe5b93152dd7dec2d36c1ff4","modified":1717292381214},{"_id":"public/f5c015c/formula.png","hash":"65fe200098b51d1712b6c38d039aa8be22d38e82","modified":1717292381214},{"_id":"public/336f2f3e/yarn.png","hash":"ee124a0823b429842082acebe78a7162915cc11c","modified":1717292381214},{"_id":"public/45ee1a6d/lm_infinite_attention_logits_explode.png","hash":"3cc54ee973126c1ca3bcd85d75039e490efc9acf","modified":1717292381214},{"_id":"public/45ee1a6d/lm_infinite_attention_entropy.png","hash":"be91b6a49cfe30dd51ed4f8eb258eb4715a70e37","modified":1717292381214},{"_id":"public/45ee1a6d/lm_infinite_middle_k.png","hash":"869d4b687714409a5a4b89c85dc5ee2c1f0c2c86","modified":1717292381214},{"_id":"public/45ee1a6d/lm_infinite_starting_tokens_num.png","hash":"6a2c841a3d3fd354f57757aa7e663e83585545d6","modified":1717292381214},{"_id":"public/a051710f/complex_number.png","hash":"255aa16a44913d53784b09d0a08762df6d21dc16","modified":1717292381214},{"_id":"public/c4da56c0/mix_precision_fp16.png","hash":"4da22fa80039f8271ffd839aba70f6b1aca7afe4","modified":1717292381214},{"_id":"public/c61d17e3/mistral_large_performance.jpeg","hash":"54e3ed874802ac9465580d6b5fcc5d6c1de96244","modified":1717292381214},{"_id":"public/c61d17e3/dilated_conv.png","hash":"bbc2ff2e9f891da4bfaf6d535ab8545acc18e8a6","modified":1717292381214},{"_id":"public/c61d17e3/receptive_field_cnn.png","hash":"46515aa3bce1eb0fc244f62ceee7b899c28183e8","modified":1717292381214},{"_id":"public/b70b4a2d/bn_and_ln.png","hash":"ff10e75b9d2c95aa713f013b104ed307f2121e57","modified":1717292381214},{"_id":"public/280fa97a/contingency_table.png","hash":"94b25e2d4803d9802d3c5455aed84911fe506089","modified":1717292381214},{"_id":"public/280fa97a/reward_accuracy_compare.png","hash":"0b5526d61a1cbb3188e8e53ea858b1d9d1953660","modified":1717292381214},{"_id":"public/6a40bfa5/bn_and_ln.png","hash":"ff10e75b9d2c95aa713f013b104ed307f2121e57","modified":1717292381214},{"_id":"public/6a40bfa5/bn_ln_gn_in.png","hash":"9783c818f5e0eaea33d169718476bbe8874cf945","modified":1717292381214},{"_id":"public/6a40bfa5/lossfunc_surface.jpeg","hash":"c78a8df335da0d507963fb73a62fe2c3d145c91f","modified":1717292381214},{"_id":"public/6a40bfa5/rmsnorm.png","hash":"55bbcb42145011f7b5adf90cc613e22e2b94f060","modified":1717292381214},{"_id":"public/6a40bfa5/realformer_attention.png","hash":"e9a92e5c07c8ca6873ea70671ad54eb2f1a13332","modified":1717292381214},{"_id":"public/44e38c1b/qwen1.5_moe_tps.png","hash":"5478a6583a6c6fb68f1bc9429c103e84fe39efaf","modified":1717292381214},{"_id":"public/44e38c1b/softplus.png","hash":"bdc66c39227441390f2241b4f26c0b1fbab331d9","modified":1717292381214},{"_id":"public/44e38c1b/st_moe_round_error.png","hash":"0172ba008837b3490a3e456306aa72be65636d90","modified":1717292381214},{"_id":"public/44e38c1b/xiaomi_moe.jpg","hash":"d898ba33f1ee70efa136dbff3cb38983b461524f","modified":1717292381214},{"_id":"public/3dc22f96/cnn_heatmap.png","hash":"cb0bde73c9c4d0646133947ebaab16c44c753667","modified":1717292381214},{"_id":"public/3dc22f96/decoder.png","hash":"28ee3d1ab68bd325ecb9d2066bc264a63d7de081","modified":1717292381214},{"_id":"public/3dc22f96/attention_calculation.png","hash":"1f020c39c78221e41c5c6953ef97239e9f42aa3c","modified":1717292381214},{"_id":"public/3dc22f96/encoder.png","hash":"d6a3a39c420d90e50f02f8b34f127bfe34177331","modified":1717292381214},{"_id":"public/3dc22f96/MQA.webp","hash":"456a8ab19cc1564912034c375e8c3c5a42be6837","modified":1717292381214},{"_id":"public/3dc22f96/seq2seq.png","hash":"9baa57cc8000a918d0adca6dceaac3ea54791ea8","modified":1717292381214},{"_id":"public/3dc22f96/softmax.png","hash":"de80ba20e55abf7457cac958aa87627d0a7e5d77","modified":1717292381214},{"_id":"public/3dc22f96/seq2seq_attention.png","hash":"b95046eee028b45dd9734639ecde8189e93b2374","modified":1717292381214},{"_id":"public/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","hash":"0629f15db217c8b7037a5b46ef187d79b9a66a09","modified":1717292381214},{"_id":"public/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","hash":"6a3863489ed0982c7fe19d575eae28ba8c286ccf","modified":1717292381214},{"_id":"public/473f2b43/gradient.png","hash":"0b72939cfd4770fc21727bb203efb9c5dd2491d1","modified":1717292381214},{"_id":"public/473f2b43/result_3.png","hash":"df5861c846176c90bd9a90bd5836919ef023b13e","modified":1717292381214},{"_id":"public/f5c015c/acce_draft_model_param.png","hash":"2e5b1852eaf4745f3d9bfc9b0fcccbd37621bb93","modified":1717292381214},{"_id":"public/f5c015c/fi_choose_gamma.png","hash":"65be032bf276290ca97b7d983bc4e1e2deaa95fc","modified":1717292381214},{"_id":"public/f5c015c/fi_speed_and_op_table.png","hash":"416238792292bff7178830267d53941da202eadc","modified":1717292381214},{"_id":"public/336f2f3e/bfloat16.jpeg","hash":"8678b705b0d6e0b7deb230bf28f2d92ce0d42088","modified":1717292381214},{"_id":"public/45ee1a6d/lm_infinite_ppl_200m.png","hash":"7b232b7bf3c7238836f71b4b99e492d9f6c285f0","modified":1717292381214},{"_id":"public/45ee1a6d/stremingllm_kv_cache.png","hash":"11b09e96662feb7cc246e60e1b21d7ffceb47ae6","modified":1717292381214},{"_id":"public/c4da56c0/meta_pi_nosft.png","hash":"d3b46ed19e458973c3586f3bf2ef906deb4080a8","modified":1717292381214},{"_id":"public/c4da56c0/rope_matrix.png","hash":"84372b91204098bab2ff09d2d865ad869f93bdbd","modified":1717292381214},{"_id":"public/cc852861/add_money.jpg","hash":"0b00f9f1dd128e5601f0c7502dd2cf9233898f0f","modified":1717292381214},{"_id":"public/a051710f/remote_attenuation.png","hash":"90aa3d8f76c9b5c18de8b7740649b499241cab0c","modified":1717292381214},{"_id":"public/c61d17e3/big_bird_attention.png","hash":"ed6c76b9bb77b98d34c429333498d04dac8e3ed9","modified":1717292381214},{"_id":"public/c61d17e3/mistral_architechture.png","hash":"5e4c347dc41d7f070f54b386fdccf675cfeb8f10","modified":1717292381214},{"_id":"public/b70b4a2d/cv_layernorm.jpeg","hash":"f0874ecc4b9d8da8bf3bff0e13a6313ed19a7b15","modified":1717292381214},{"_id":"public/280fa97a/dpo_correlation.png","hash":"2c1dafd42b7ffa3318395a4934df87692ba5fd62","modified":1717292381214},{"_id":"public/280fa97a/simpo_contingency.png","hash":"773700b1d041aba8b3912deee9c8bc7886fec099","modified":1717292381214},{"_id":"public/280fa97a/reward_accuracy.png","hash":"7fb3d4dd64e3013534bd77eb1f2def23ec57c8cd","modified":1717292381214},{"_id":"public/280fa97a/simpo_hyperparameters.png","hash":"f976162c0882c49b43b503beb1384b447e2d5d00","modified":1717292381214},{"_id":"public/41b6a819/model.png","hash":"631efc6d4e92da128d7a10a4dc6af307ee4ddcbf","modified":1717292381214},{"_id":"public/41b6a819/third_party.png","hash":"673fe2b2cad3b1f40c0fcfd190c0034d8dbe7f31","modified":1717292381214},{"_id":"public/6a40bfa5/ellipse_2.png","hash":"ee20ecce8c3470d17b1a4bd43df811d16269ffd3","modified":1717292381214},{"_id":"public/6a40bfa5/prmsnorm.png","hash":"d5826342f665f4cb04fdbb2e3d83e0b2607355c9","modified":1717292381214},{"_id":"public/6a40bfa5/postnorm_prenorm.png","hash":"d8830735e89c73ca416baabf4a195d7891d9f0ed","modified":1717292381214},{"_id":"public/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1717292381214},{"_id":"public/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1717292381214},{"_id":"public/css/noscript.css","hash":"4cd5301e478e0e0d4b176740ec314087ec5cb707","modified":1717292381214},{"_id":"public/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1717292381214},{"_id":"public/css/main.css","hash":"6aea217c0462e6970606601a9fe39183cf15614c","modified":1717292381214},{"_id":"public/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1717292381214},{"_id":"public/js/next-boot.js","hash":"745bd828205da7e5fbd3f860cc3697097630f5f3","modified":1717292381214},{"_id":"public/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1717292381214},{"_id":"public/js/pjax.js","hash":"b03ba78c6916ad2f390d55bc1bc18fafb64b0ebf","modified":1717292381214},{"_id":"public/js/utils.js","hash":"d775148c2bf20c028622af5609a788167352bf1e","modified":1717292381214},{"_id":"public/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1717292381214},{"_id":"public/js/third-party/fancybox.js","hash":"819f382c561fe5ec23c67cc5fabd63dd1cc22dc1","modified":1717292381214},{"_id":"public/js/motion.js","hash":"770d63c26f22705311028a36b52e999cc8a2da82","modified":1717292381214},{"_id":"public/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1717292381214},{"_id":"public/js/third-party/addtoany.js","hash":"5276c8f78ee562a8965216dc67d762e59cb4a9f2","modified":1717292381214},{"_id":"public/js/schemes/muse.js","hash":"ba7ba2c129d1f240c6a22cec3e53f3f22af64b6b","modified":1717292381214},{"_id":"public/js/third-party/analytics/google-analytics.js","hash":"def07bcc7c17d8a0caad177fb1dd2f3a5e5b3536","modified":1717292381214},{"_id":"public/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1717292381214},{"_id":"public/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1717292381214},{"_id":"public/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1717292381214},{"_id":"public/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1717292381214},{"_id":"public/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1717292381214},{"_id":"public/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1717292381214},{"_id":"public/js/third-party/comments/disqus.js","hash":"da361917d65e5dca8362f8cdeb6c8cc0e8316cec","modified":1717292381214},{"_id":"public/js/third-party/comments/disqusjs.js","hash":"1e826dea3f684c0515f362dc1352447a1f0eae71","modified":1717292381214},{"_id":"public/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1717292381214},{"_id":"public/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1717292381214},{"_id":"public/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1717292381214},{"_id":"public/js/third-party/search/algolia-search.js","hash":"fdb7b7cef1a147d897e7f7cd8903b58368ec2062","modified":1717292381214},{"_id":"public/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1717292381214},{"_id":"public/js/third-party/search/local-search.js","hash":"4536cb6d0a9bbaaa86fab3fa0101f6a3a3ec5a76","modified":1717292381214},{"_id":"public/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1717292381214},{"_id":"public/js/third-party/tags/wavedrom.js","hash":"40dcd10df6edf124088c329346e0cc0bdac74ef1","modified":1717292381214},{"_id":"public/js/third-party/statistics/lean-analytics.js","hash":"835cbf54c49ef1327f47df70ff2636ad36b6f57d","modified":1717292381214},{"_id":"public/js/third-party/statistics/firestore.js","hash":"6e0682bb42170d61b13b786295f45f9c785f8b73","modified":1717292381214},{"_id":"public/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1717292381214},{"_id":"public/js/third-party/tags/mermaid.js","hash":"6bf821310342c5b87a631873e7650a475a0765f1","modified":1717292381214},{"_id":"public/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1717292381214},{"_id":"public/44e38c1b/dbrx_train_efficiency.png","hash":"c117407ada2adab8d97250268e2eafa533bb9083","modified":1717292381214},{"_id":"public/6a40bfa5/sigmoid.png","hash":"f1ced5f06861a2e0296050aff17eebfe3d023a6f","modified":1717292381214},{"_id":"public/44e38c1b/dbrx_long_perf_2.png","hash":"2b6be0099f1eeb0ee2d2056eae7cf541e146b636","modified":1717292381214},{"_id":"public/44e38c1b/qwen1.5_moe_params.png","hash":"93d14a2645969b08a4fb80a31aa75fd8e5201ff8","modified":1717292381214},{"_id":"public/44e38c1b/mistral_8_7b_perf.png","hash":"f90d9ff5b14326b0eef2a0026b3f5940e0d42f0a","modified":1717292381214},{"_id":"public/44e38c1b/modular_connectionist.png","hash":"b5865cf34faba075b4f2316c2cae0559dac2d883","modified":1717292381214},{"_id":"public/44e38c1b/qwen1.5_moe_perf.png","hash":"b79ad1a909081fd0537bd9d44cfac2dc2133de6c","modified":1717292381214},{"_id":"public/44e38c1b/rnn_moe_load_function.png","hash":"644684f21f85d565328d98334d41bbe019acbbfc","modified":1717292381214},{"_id":"public/44e38c1b/st_moe_more_add_bias.png","hash":"f0de5347918e4928dbcbc59a897c3f3227c3d30f","modified":1717292381214},{"_id":"public/3dc22f96/multihead_attention.png","hash":"6f8ee285f2646dc163b6b3164a0639aa9ddd7f27","modified":1717292381214},{"_id":"public/3dc22f96/llama2_qga.png","hash":"5e0dea7d03de9144eb524a0a9adb102e91b52aaa","modified":1717292381214},{"_id":"public/3dc22f96/sram_dram.png","hash":"ae7a9296b67c02608460a334fbbad3781b890302","modified":1717292381214},{"_id":"public/images/avatar/Picasso_Elephant.png","hash":"6d38d4b24c13a3fd506ee37a562126b76a24de83","modified":1717292381214},{"_id":"public/f5c015c/fi_speed_and_op.png","hash":"9b5e3a6c9276309e7aa5a8848d52ef361e62bb36","modified":1717292381214},{"_id":"public/f5c015c/fi_expected_token_num.png","hash":"52be2409ca9513de2e5ce10a0e77d8aa98dfc328","modified":1717292381214},{"_id":"public/f5c015c/fi_walltime.png","hash":"b645987bec587e743021bc330de277416ef36d5e","modified":1717292381214},{"_id":"public/da871ebe/alpha.png","hash":"4e3ad45447f5757d2cfcdf9d9555351233456c3b","modified":1717292381214},{"_id":"public/da871ebe/summarization.png","hash":"4934f3e42f20bc3a44ad07267e91a14c4c005543","modified":1717292381214},{"_id":"public/45ee1a6d/infini_attention_process.png","hash":"259ea90040b7a5b98a27afcb992a6bee31707ab9","modified":1717292381214},{"_id":"public/45ee1a6d/infini_attention_structure.png","hash":"35f958d9ba50460689727c7038bf3a344000fa52","modified":1717292381214},{"_id":"public/45ee1a6d/lm_infinite_downstream.png","hash":"192160ca6972003a61678e2f7f2467f5bbd94451","modified":1717292381214},{"_id":"public/a051710f/rope.png","hash":"5535c87dbcfee3ed646db93662a0b6a1ccd3b31b","modified":1717292381214},{"_id":"public/cc852861/paraphrasing_quality.png","hash":"fab44d68fc27f7bb2c06f758e537b9b249be0699","modified":1717292381214},{"_id":"public/c61d17e3/longformer_attention.png","hash":"64860379955872ecac5835b3f9d8c6d130c7e485","modified":1717292381214},{"_id":"public/c61d17e3/rolling_buffer.png","hash":"34d4db9f4855926db561faa80e934dd971c0974e","modified":1717292381214},{"_id":"public/b70b4a2d/cv_batchnorm.png","hash":"d9e8d897c36125fddcf1cbcfa5c237a37158a939","modified":1717292381214},{"_id":"public/280fa97a/margin_dist.png","hash":"cf82f48158e4ba3e503cbe25cc811910804489cf","modified":1717292381214},{"_id":"public/41b6a819/9B.png","hash":"7de19972e48b1f43c501d60a1c43a28c46b198e8","modified":1717292381214},{"_id":"public/41b6a819/long_context_result.png","hash":"daea6f734d64bdf5d24c6e17a640f23b1bd35b5f","modified":1717292381214},{"_id":"public/6a40bfa5/bn_algo.png","hash":"56f1ab55c0e94814e6e37c30421012ed82098d62","modified":1717292381214},{"_id":"public/6a40bfa5/deepnorm.png","hash":"4726d8a40d1d0db397005408295e1ba54809a7e4","modified":1717292381214},{"_id":"public/6a40bfa5/deepnorm_result.png","hash":"138cafc159f1e2e02455c540b4754f7cbb7f521d","modified":1717292381214},{"_id":"public/6a40bfa5/ics_define.png","hash":"6bf3240ef78bad2cf76897a29c05428f4c195fba","modified":1717292381214},{"_id":"public/44e38c1b/dbrx_long_perf_1.png","hash":"bc9c40bde860e78882965a25056a848aa4a89c77","modified":1717292381214},{"_id":"public/44e38c1b/ds_model_param.png","hash":"7b838274937cf45d73e59ac1fb5c2034e46586ae","modified":1717292381214},{"_id":"public/44e38c1b/ds_moe_expert_specialization.png","hash":"64947872486dd083a7076bfdfe67cb0626208579","modified":1717292381214},{"_id":"public/44e38c1b/ds_moe_less_activated_expert.png","hash":"ac7ad86dabe94564bdb17399231c6ddc7da83962","modified":1717292381214},{"_id":"public/44e38c1b/glam_compare_gpt3.png","hash":"311b21079599473054378e339885e0b87719e63e","modified":1717292381214},{"_id":"public/44e38c1b/rnn_moe_137b.png","hash":"fc11cfc87b2994956a9adbee76621fe5d964dc30","modified":1717292381214},{"_id":"public/44e38c1b/rnn_moe.png","hash":"2a2ee095b8cc0727daa2cdd0c63891e4a470eae8","modified":1717292381214},{"_id":"public/44e38c1b/mistral_8_7b_active_perf.png","hash":"0c67d935657e62b9e8eeabc6403c269e09016626","modified":1717292381214},{"_id":"public/44e38c1b/st_moe_more_add_noise.png","hash":"12d13a9ee6c28553626e469ed18d022e0176a873","modified":1717292381214},{"_id":"public/44e38c1b/st_moe_remove_multiplications.png","hash":"52bc94bfe726dfc832534dde409154efe0ce7b0f","modified":1717292381214},{"_id":"public/44e38c1b/st_moe_z_loss_result.png","hash":"3edd8e9eb069c9557c98fd21600c02b3a1978cc5","modified":1717292381214},{"_id":"public/44e38c1b/vanilla_moe.png","hash":"7da0a6e9d529256107b5f6b287737ac47513a797","modified":1717292381214},{"_id":"public/3dc22f96/mqa_result_1.png","hash":"a052b57f71eb78e3158ed2ee06ff0e5597607a2f","modified":1717292381214},{"_id":"public/473f2b43/result_4.png","hash":"6f554d1b6911c3db211a7e57e886e62f03b46ffd","modified":1717292381214},{"_id":"public/f5c015c/fi_t5_result.png","hash":"83b63aafceeb8f2bc3f89ee6a1e3caec7987a1c9","modified":1717292381214},{"_id":"public/45ee1a6d/infini_attention_booksum.png","hash":"276dfa014d35f7d3375fbb7cee6eed21127f9955","modified":1717292381214},{"_id":"public/45ee1a6d/infini_attention_passkey.png","hash":"811c5c677f7616b6625a0b86f2004f6d3ebeefe9","modified":1717292381214},{"_id":"public/45ee1a6d/infini_attention_language_modeling.png","hash":"a49a7cc02694e7017c2a20dc0666046108b3c4c5","modified":1717292381214},{"_id":"public/45ee1a6d/lm_infinite_ablation.png","hash":"babe57024a1c2212405220124dfd376a8a2bcfb6","modified":1717292381214},{"_id":"public/45ee1a6d/stremingllm_attention_sink.png","hash":"933f34f3bc1d04e2b36f3305ac9fe2acd8bc9939","modified":1717292381214},{"_id":"public/45ee1a6d/xl_vanilla_sw.png","hash":"3259a751066a0083ef249a0412f43fb582e6544c","modified":1717292381214},{"_id":"public/cc852861/paraphrasing_dataset_dist.png","hash":"e5754afcb70a45c0d11ee5db43c724350ec64257","modified":1717292381214},{"_id":"public/cc852861/eng_ppl.png","hash":"fccca1509ebab2e89a3ceaad0dfeedc700de2691","modified":1717292381214},{"_id":"public/cc852861/paraphrasing_lost.png","hash":"b9d73b8022266af17789ab049c7adda621729cc9","modified":1717292381214},{"_id":"public/c61d17e3/prefill_and_chunking.png","hash":"0c706e0728ea462b2b00c59a97c79ccf5f05b598","modified":1717292381214},{"_id":"public/280fa97a/ablation.png","hash":"4c73c83eb527141e17d1109b6c2cff3488de6259","modified":1717292381214},{"_id":"public/280fa97a/benchmark.png","hash":"83084a5f64006000898d5252b3f8afccb635b3f0","modified":1717292381214},{"_id":"public/280fa97a/intro.png","hash":"3ff9cce772cd825ec8b88591d576a5f52982d679","modified":1717292381214},{"_id":"public/1736008/transformer.png","hash":"9dddf171ca51f2ed1218baa9b84f4b98e9b911cf","modified":1717292381214},{"_id":"public/41b6a819/perf.png","hash":"3c068a423dcd32cac7f1630bd69fbe5a4c6789af","modified":1717292381214},{"_id":"public/41b6a819/base_model_eval.png","hash":"9b4e65d246865683f8e3348d74bfad03c937b65f","modified":1717292381214},{"_id":"public/41b6a819/sft.png","hash":"ea9aea143af836012f44d21956ab5455487e9bfb","modified":1717292381214},{"_id":"public/6a40bfa5/bs_bn.png","hash":"aa28241d75f914603b9f7f67cc54db4e61bac668","modified":1717292381214},{"_id":"public/6a40bfa5/rmsnorm_eff.png","hash":"350a7a2703eef1ee9357609ae5820bfc30835681","modified":1717292381214},{"_id":"public/3dc22f96/Markdown _  Nice.html","hash":"c905c942579a520c7b3c788a00cdb9ae359d4a32","modified":1717292381214},{"_id":"public/44e38c1b/cover.jpeg","hash":"6226a5276377816b37a20572a8b725af3ddf5760","modified":1717292381214},{"_id":"public/44e38c1b/dbrx_vs_open_models.png","hash":"ba0348d3fe68a27f8c6435c4c3a6d08d9c8869c6","modified":1717292381214},{"_id":"public/44e38c1b/dbrx_vs_closed_models.png","hash":"6211c05b7d69194f2d820622525273110467a0d5","modified":1717292381214},{"_id":"public/44e38c1b/ds_moe_ablation.png","hash":"108dc8d66ae0e370e7969403efd85178b9a8523a","modified":1717292381214},{"_id":"public/44e38c1b/glam_family.png","hash":"9d813f12f82d8702886f7ede72c5a25151390ba9","modified":1717292381214},{"_id":"public/44e38c1b/ds_2b_less_expert.png","hash":"e03a00a194efd33890517b4ad642bca5566cf9df","modified":1717292381214},{"_id":"public/44e38c1b/gshard_moe_family.png","hash":"21a6c80f1dac39eb364fb417ed83afde2b212675","modified":1717292381214},{"_id":"public/44e38c1b/glam_related_model.png","hash":"0f109231fc1b425c7364401c41ce5f3aecfd76c7","modified":1717292381214},{"_id":"public/44e38c1b/mistral_8_22b_multiling.png","hash":"4af49ffc09a0de3793ec7137d3dfbddc9c309d38","modified":1717292381214},{"_id":"public/44e38c1b/st_moe_capacity_factor_speed.png","hash":"60b624b7595e1f90763ea745ea358b6852eeefb0","modified":1717292381214},{"_id":"public/44e38c1b/st_moe_more_dense_layer.png","hash":"67c37482d73cd7ece7e384c0bd73e6891fc752e1","modified":1717292381214},{"_id":"public/44e38c1b/switch_transformer_dropout.png","hash":"63f36ca61aaa71e88ddf23339e63a9ccd898a6ce","modified":1717292381214},{"_id":"public/44e38c1b/switch_transformer_distill_sft.png","hash":"7a59660f367634d68aa392698042f3d9cfe190da","modified":1717292381214},{"_id":"public/44e38c1b/switch_transformer_init.png","hash":"f9fb36a0defac7a5990b5c58a614f3165af0ae3e","modified":1717292381214},{"_id":"public/3dc22f96/GQA_result_1.png","hash":"87f2c3632fdf83828e8bd07a95cd8e7bf277fc88","modified":1717292381214},{"_id":"public/3dc22f96/Scaled-dot-product-self-attention.pbm","hash":"03da711b1547c944deea60d9bf345eb30e7c566f","modified":1717292381214},{"_id":"public/3dc22f96/gpu_cache.png","hash":"edb6b1abdecd3099f2d68c2a729c0ca9b1fb0db7","modified":1717292381214},{"_id":"public/3dc22f96/GQA.png","hash":"d6b0cd20ae5f68a96f0865b3d0b6b129969caa4b","modified":1717292381214},{"_id":"public/3dc22f96/transformer_structure.png","hash":"87f0258e43922eface0277e13167a4ba8c1402bd","modified":1717292381214},{"_id":"public/images/background/wallhaven-2ywymm.png","hash":"aeaf089b64472a1c72cdd9778c5deac4086eb38f","modified":1717292381214},{"_id":"public/473f2b43/result_2.png","hash":"6baa634147220fed9edfff7c70e83c56a2b24913","modified":1717292381214},{"_id":"public/f5c015c/fi_sd_algo.png","hash":"576ffae274518c5a4e6c049d199e0714b06bba86","modified":1717292381214},{"_id":"public/da871ebe/odpo_intro.png","hash":"e32faada4824a2654e32d125cb7dded9895f87dc","modified":1717292381214},{"_id":"public/da871ebe/toxicity_control.png","hash":"42173484be1fd33e29244f43d658ba03ec9bacb2","modified":1717292381214},{"_id":"public/336f2f3e/ntk_by_parts.png","hash":"5b49750dc6a2d1b878f34bc71e3961d96282499a","modified":1717292381214},{"_id":"public/45ee1a6d/lm_infinite_design.png","hash":"e96faad18cb526d201ce069f9ce09dc3c9c0d16e","modified":1717292381214},{"_id":"public/45ee1a6d/stremingllm_exp.png","hash":"cdd21419055ca9bed86b46675e118ad4bdf55544","modified":1717292381214},{"_id":"public/45ee1a6d/stremingllm_init_token_num.png","hash":"ffe78c31d901311249530e786bc5ed321e2e242f","modified":1717292381214},{"_id":"public/45ee1a6d/stremingllm_perf_4m.png","hash":"c4545d7f0bbd0c5f6baa85dd64b747a3723fdf2e","modified":1717292381214},{"_id":"public/cc852861/eng_config.png","hash":"8edac537bd1aefea28406c414e0c0a4c888234be","modified":1717292381214},{"_id":"public/c4da56c0/meta_pi_explanation.png","hash":"df22d646e5a47e6223b7722007cfb7dde7c27280","modified":1717292381214},{"_id":"public/cc852861/pose_ppl.png","hash":"fded043b94c97c1a782869963f5dea371e257b80","modified":1717292381214},{"_id":"public/cc852861/pose_method.png","hash":"db8784c9e4b14c5963f62f072df6a4c3c5405874","modified":1717292381214},{"_id":"public/cc852861/pose_passkey.png","hash":"6202690a895f0114c90f6821c8cf1ad7388e1592","modified":1717292381214},{"_id":"public/41b6a819/eval.png","hash":"e78d1d820de4c455b9301124d6016a19762eae1f","modified":1717292381214},{"_id":"public/41b6a819/pretrain_data_dist.png","hash":"8c66a625723cb87ee67a9ff60d3614b369f50592","modified":1717292381214},{"_id":"public/41b6a819/multimodal.png","hash":"b14a4eb4d377101acf7b50904b9ee0f1d473aacc","modified":1717292381214},{"_id":"public/6a40bfa5/warmup_effect.png","hash":"3e936786065e1ab9cbad17f5b86a5b8129720270","modified":1717292381214},{"_id":"public/44e38c1b/dbrx_infer_efficiency.png","hash":"34245e99c2b29dbd54104ccbbb3d8c15706307b9","modified":1717292381214},{"_id":"public/44e38c1b/rnn_moe_perf.png","hash":"ca3c95d4b1c1c8f986d7adcacbf04da444e91610","modified":1717292381214},{"_id":"public/44e38c1b/st_moe_capacity_factor.png","hash":"a960540d3419de77c3823d343247abfddeadde1c","modified":1717292381214},{"_id":"public/44e38c1b/rnn_moe_specilized.png","hash":"138ad5a388c77cc02202de794ec4cb734d633065","modified":1717292381214},{"_id":"public/44e38c1b/st_moe_multiling_specialization.png","hash":"bac636d8da61768adb5a9b7c5bd75547267ae470","modified":1717292381214},{"_id":"public/44e38c1b/st_moe_models.png","hash":"bc59770bc8ea44bfe480484a99aa9143acbaa6fa","modified":1717292381214},{"_id":"public/44e38c1b/switch_transformer_structure.png","hash":"d906a9148e035025683e8da1eee5fa3d87164aa5","modified":1717292381214},{"_id":"public/473f2b43/dpo_loss_code.png","hash":"279b32cc1c4dfefa8790fcbb597659e8b974ac61","modified":1717292381214},{"_id":"public/473f2b43/result_1.png","hash":"725c6be46c42e8fc8184304bb0cdf5071b09e8b2","modified":1717292381214},{"_id":"public/f5c015c/acce_k.png","hash":"ca37e1983347a1a835389de4d17047e3b0d02af4","modified":1717292381214},{"_id":"public/f5c015c/acce_alog.png","hash":"61f4653292bbd93debee66cebfb44ac7198e5818","modified":1717292381214},{"_id":"public/da871ebe/sentiment_control.png","hash":"fc200cc3802fdee9d80e4bc259f4baca7b425ae7","modified":1717292381214},{"_id":"public/da871ebe/scaling_function.png","hash":"b554e0bd697e72bb2e5e24a16c678d80c2efcc52","modified":1717292381214},{"_id":"public/45ee1a6d/infini_attention_compare.png","hash":"9b272eba596a790b1d40ef3a8b041bdffe1660d3","modified":1717292381214},{"_id":"public/45ee1a6d/lm_infinite_ppl_figure.png","hash":"34397c88e268b769c97a2b6e2ad63bf6ad5270ef","modified":1717292381214},{"_id":"public/45ee1a6d/streamingllm_model_ppl.png","hash":"2fc1c11d7cfa2598b414e5e4c145181ec9e10648","modified":1717292381214},{"_id":"public/45ee1a6d/xl_attention.png","hash":"395424d6c048880e143b9b2f93585597fbebebd7","modified":1717292381214},{"_id":"public/c4da56c0/meta_pi.png","hash":"193df6c392d71dd4b90202d9be97f000f37b68ec","modified":1717292381214},{"_id":"public/cc852861/paraphrasing_perf.png","hash":"5cf407e2e2ee61bb2b6bdae0570c3b8c4a3a9374","modified":1717292381214},{"_id":"public/c61d17e3/mistral_perf.png","hash":"c9d7ce0a301920c4e722e341200f311995923735","modified":1717292381214},{"_id":"public/c61d17e3/mistral_swa.png","hash":"59037b91ba8f256fd89b3d60b8ce477e4c8f4b3a","modified":1717292381214},{"_id":"public/280fa97a/hyperparameters.png","hash":"b3595e75eff0cb8ea8f86fd9e2f8c6ae5f7892bc","modified":1717292381214},{"_id":"public/14e576c/28.png","hash":"d438e857378575809c880b78ca715dc69e50b364","modified":1717292381214},{"_id":"public/6a40bfa5/ics_measure.png","hash":"e9fe87cfea7dcef7cb66e1d76c17d883cbbc3cbd","modified":1717292381214},{"_id":"public/44e38c1b/ds_16b_perf_1.png","hash":"d540e8a58e166c0ba894708c9b0c277c31107487","modified":1717292381214},{"_id":"public/44e38c1b/dbrx_perf.png","hash":"035a992ef2e7e5a9165c9488e2696e38fa29165c","modified":1717292381214},{"_id":"public/44e38c1b/ds_16b_perf_2.png","hash":"6886e18a2e7601202e8721b83f998faf028e19eb","modified":1717292381214},{"_id":"public/44e38c1b/ds_moe_perf.png","hash":"b6ed751d2f171dac971bcf1d7f12e8a2d7fad388","modified":1717292381214},{"_id":"public/44e38c1b/ds_moe_structure.png","hash":"b0564913ecc1f78e5052dbc07eb65b3f048846e3","modified":1717292381214},{"_id":"public/44e38c1b/glam_compare_gpt3_2.png","hash":"89172dda5ac569780ea47e85bc43eaef1d6918ae","modified":1717292381214},{"_id":"public/44e38c1b/glam_model.png","hash":"63c95ea5ae77528f7d94a94b21cf12ed63e0bfdb","modified":1717292381214},{"_id":"public/44e38c1b/ds_moe_upper_bound_13b.png","hash":"cec21f0cf3809011ce210dffda35da3671147008","modified":1717292381214},{"_id":"public/44e38c1b/switch_transformer_distill.png","hash":"be5cae81fafbda6b638ac185421bd04e00b7a60d","modified":1717292381214},{"_id":"public/44e38c1b/switch_transformer_pretrain_result.png","hash":"880f70f371b8392e3021bf56d282be5640c232f7","modified":1717292381214},{"_id":"public/44e38c1b/switch_transformer_scaling_dense.png","hash":"b8de8c427de51d62e69915da7a97bf4a9b505317","modified":1717292381214},{"_id":"public/44e38c1b/switch_transformer_scaling_time.png","hash":"b56568665d2680cc0723269ae39dc4b60de1b01c","modified":1717292381214},{"_id":"public/3dc22f96/mqa_result_3.png","hash":"12e310102ace1f9e89c0e9a352cf4a3462335a60","modified":1717292381214},{"_id":"public/f5c015c/fi_example.png","hash":"edbdb72af30cac5f036e47b3d0d426919f336e62","modified":1717292381214},{"_id":"public/45ee1a6d/lm_infinite_starting_tokens.png","hash":"18e3a03213a7275617201b71eb274cd8fd8b0bf9","modified":1717292381214},{"_id":"public/cc852861/eng_tokens.png","hash":"b6c43c289004164e90de39c30e170de5ac1088aa","modified":1717292381214},{"_id":"public/cc852861/paraphrasing_dataset.png","hash":"b85a9077cffcace583a7dbcdbd235ab646086ea1","modified":1717292381214},{"_id":"public/473f2b43/intro.png","hash":"c5175eef020ec3499aa67163813eab1c4c13a84a","modified":1717292381214},{"_id":"public/280fa97a/ln.png","hash":"4d4c831fc95c591ea0415e07dd5f46d1b1494e60","modified":1717292381214},{"_id":"public/14e576c/21.png","hash":"fb2577b5fa73b06b786484b3723f7aa3819638a0","modified":1717292381214},{"_id":"public/6a40bfa5/realformer.png","hash":"3bc805db3177c7e6521362b063543941da8d2bd3","modified":1717292381214},{"_id":"public/44e38c1b/ds_moe_sft.png","hash":"8bc26fcfef1aab448d0db0b28666852f62961a3b","modified":1717292381214},{"_id":"public/44e38c1b/glam_perf.png","hash":"01fb9d4f232e9f0b86abb91dbe5d8fb9fac456ce","modified":1717292381214},{"_id":"public/44e38c1b/gshard_algo_1.png","hash":"aa8bcd982c78e4304c63f81e44b20519dc04f18f","modified":1717292381214},{"_id":"public/44e38c1b/gshard_perf.png","hash":"27157f620e2b7c4be60b2a58f7857a888794cde1","modified":1717292381214},{"_id":"public/44e38c1b/gshard_result.png","hash":"32ce9bba1b65ceb2e69c079e113d8b4c524bc479","modified":1717292381214},{"_id":"public/44e38c1b/rnn_moe_hierarchical_gating.png","hash":"067160c735e9c0b8cff777df60d52dfca21ea783","modified":1717292381214},{"_id":"public/44e38c1b/st_moe_perf.png","hash":"58927fa39b5e56e8da00144b417bbae5256d6bdf","modified":1717292381214},{"_id":"public/44e38c1b/switch_transformer_diff_expert_capacity.png","hash":"23f2234b45a2a05ff8b098e68a361a71f46816e9","modified":1717292381214},{"_id":"public/44e38c1b/switch_transformer_distill_diff_model.png","hash":"05c4d885f5563e3dbd56c055a52df5c1531677a7","modified":1717292381214},{"_id":"public/f5c015c/fi_alpha.png","hash":"f7f5a24106f1b9d16fd805b3ed3d3c2efb4a8c03","modified":1717292381214},{"_id":"public/c4da56c0/meta_rope_ext.png","hash":"43e1131d146b8104e6936d73aca51d8ace4aff61","modified":1717292381214},{"_id":"public/cc852861/eng_data.png","hash":"9262b3bbc1b415d9c776723fe85c0a43f9fb562a","modified":1717292381214},{"_id":"public/cc852861/eng_data_dist.png","hash":"157286ce140bd5acc7c45fb12785649cc7214472","modified":1717292381214},{"_id":"public/280fa97a/ln_effect.png","hash":"714ba6cdf67ea33d507e3358b113a94bcd24e1ce","modified":1717292381214},{"_id":"public/41b6a819/cover.png","hash":"0493fd58fd2dad33394399d960924dbff6b386b1","modified":1717292381214},{"_id":"public/14e576c/20.png","hash":"5d42628c8dac91c9671a58535b730e91966c0cbc","modified":1717292381214},{"_id":"public/44e38c1b/ds_moe_comparison.png","hash":"20c7e90a390604d2146b4984678524046ad941b8","modified":1717292381214},{"_id":"public/44e38c1b/gshard_model.png","hash":"5179df7e42bb7dc49fb6f92f4ec68ed820aeaae2","modified":1717292381214},{"_id":"public/44e38c1b/mistral_8_22b_code.png","hash":"f9b78e0669e0c83f716c8e4abfa4d97b6f9b8143","modified":1717292381214},{"_id":"public/44e38c1b/mistral_8_22b_reasoning.png","hash":"ce00d9ba7b8a65eb238c99f79a17ca7a3cd2238a","modified":1717292381214},{"_id":"public/44e38c1b/st_moe_encoder_specialization.png","hash":"382cb53fcc2e9172ffd7554a04714feed4d9706b","modified":1717292381214},{"_id":"public/44e38c1b/switch_transformer_scaling_step.png","hash":"dbfa55ab1c94e266344315fd215f2d646eaefda1","modified":1717292381214},{"_id":"public/45ee1a6d/infini_attention_gating.png","hash":"5e875c3de6bde62ea8e15ea4995a56f9fd28d67c","modified":1717292381214},{"_id":"public/45ee1a6d/streamingllm_compare.png","hash":"40394890082b1666d1221f302ed52a80fc358477","modified":1717292381214},{"_id":"public/cc852861/eng_sample.png","hash":"6b0d7ed89b16a3e9c6297219f33e59f204923828","modified":1717292381214},{"_id":"public/cc852861/eng_needle_comp.png","hash":"e4c9f5c51548faf11a7ff64d23aae5bd2927ab0e","modified":1717292381214},{"_id":"public/280fa97a/main_results.png","hash":"a7388a503f0157c2ebe9ef765d63daab358b67d7","modified":1717292381214},{"_id":"public/44e38c1b/ds_moe_145b.png","hash":"502a377d8625d78d2e3ba7281bfb11732c14a61c","modified":1717292381214},{"_id":"public/44e38c1b/ds_moe_upper_bound_2b.png","hash":"4338539fe123371c5512c730fc8a35864236323b","modified":1717292381214},{"_id":"public/44e38c1b/switch_transformer_capacity_effect.png","hash":"16268929d0ec965d3771fccbb595b75d82f05912","modified":1717292381214},{"_id":"public/44e38c1b/switch_transformer_sft_result.png","hash":"0dd7cfaac788c5d112dd10fe98f0052b369420bf","modified":1717292381214},{"_id":"public/44e38c1b/vanilla_moe_result.png","hash":"e491e44cfff384422f3d9cf87cd5e52fd976aed7","modified":1717292381214},{"_id":"public/3dc22f96/lihongyi_self_attention.png","hash":"39db6256143fd9a494e848240a8daa434aaddea5","modified":1717292381214},{"_id":"public/images/avatar/shadow.png","hash":"da67a14e556fb3223c1c4ec8b9f33eb52c612f50","modified":1717292381214},{"_id":"public/cc852861/paraphrasing_example.png","hash":"946cdbb0a425b9d11b0ff587007885990abc9f99","modified":1717292381214},{"_id":"public/cc852861/paraphrasing_intro.png","hash":"6b810c88945281a9cdf9749941cdbe08346fd42b","modified":1717292381214},{"_id":"public/280fa97a/gradient.png","hash":"58524a8fd10f4d76e8e419e1ed46bd9a99cf58d5","modified":1717292381214},{"_id":"public/41b6a819/ict.png","hash":"2445ecbbf5ec6a96695f21c03a5fcbf67640b9f0","modified":1717292381214},{"_id":"public/14e576c/17.png","hash":"8ea1c5d90f3da5c469eb17aea50f377cb9c28ba0","modified":1717292381214},{"_id":"public/14e576c/18.png","hash":"fe0a8e7005110abca19bc7ae506f3e35042b70ec","modified":1717292381214},{"_id":"public/14e576c/26.png","hash":"f74d03dae65109740f48924f30b52a742b5e4273","modified":1717292381214},{"_id":"public/6a40bfa5/bn_ics.png","hash":"f92751ea20430f25caa3d6bb892c5894bf7509d6","modified":1717292381214},{"_id":"public/41b6a819/pretrain_data_pipeline.png","hash":"91218a2272eab9284904c91bacd8d8a40e3c1580","modified":1717292381214},{"_id":"public/14e576c/10.png","hash":"2c52d4f90dd9356eeb4c9a39f1df1038ccec4693","modified":1717292381214},{"_id":"public/14e576c/3.png","hash":"4c2c2a30d9ac8db03bab56da5d16ce2042ef73bc","modified":1717292381214},{"_id":"public/14e576c/6.png","hash":"4b32a49bfead98f5238871b81076176e38168333","modified":1717292381214},{"_id":"public/14e576c/1.png","hash":"a8898b3f3b7c64fabc5fad9bf8ef5524501d2aeb","modified":1717292381214},{"_id":"public/14e576c/19.png","hash":"1d7b929e709657c9b7d7ca4da8eadc8c4ca4b3ca","modified":1717292381214},{"_id":"public/14e576c/24.png","hash":"b89b0a0ca774a4efc1ece628fb20379b5f6a0b69","modified":1717292381214},{"_id":"public/3dc22f96/Scaled-dot-product-self-attention.png","hash":"983eae2b767df413ef3211ddaf31f1b833d7c86f","modified":1717292381214},{"_id":"public/14e576c/8.png","hash":"92b0e3b75ce97bbaf4aa69e484216702293589ee","modified":1717292381214},{"_id":"public/14e576c/27.png","hash":"557c04a2134b6ae147e076cdb80de1730e937d9b","modified":1717292381214},{"_id":"public/14e576c/9.png","hash":"2a0fb56563b13411035ed41a3ad882f66f948b26","modified":1717292381214},{"_id":"public/14e576c/11.png","hash":"9b964f3aa6f82a09eb2f2f944508bf0a9d29efb3","modified":1717292381214},{"_id":"public/14e576c/22.png","hash":"3b27321ef8d76844f6720e1a27d65d1946d48ea7","modified":1717292381214},{"_id":"public/14e576c/23.png","hash":"9a9af6308620b59f2ee00a3d0da4e942d953e406","modified":1717292381214},{"_id":"public/14e576c/25.png","hash":"aa259b58be90eed6af0c4ba800a991b9464453d0","modified":1717292381214},{"_id":"public/14e576c/15.png","hash":"15abcbcf7340941e98dac7a0ab42d922e7fea1b4","modified":1717292381214},{"_id":"public/14e576c/2.png","hash":"768421239ad7c838dd86714fd9f17b3c73cbb887","modified":1717292381214},{"_id":"public/14e576c/4.png","hash":"4dacbfb89079d528da1208773961e1366debde9b","modified":1717292381214},{"_id":"public/14e576c/12.png","hash":"c6c493b14e0a1cc4863a912c4ccc998de194bfc0","modified":1717292381214},{"_id":"public/14e576c/14.png","hash":"71f75960246f7528b3b83b84f8f91775f9e2fb45","modified":1717292381214},{"_id":"public/14e576c/16.png","hash":"ba7b2bc65e10389cf9a87ddef69f462e806304f5","modified":1717292381214},{"_id":"public/3dc22f96/MQA.png","hash":"7e3f3037311be60e79a7b5388338febc9f3b6d7c","modified":1717292381214},{"_id":"public/14e576c/7.png","hash":"95640525b0706d3118eeb88c6c4c6217a96c39d0","modified":1717292381214},{"_id":"public/14e576c/13.png","hash":"68b7da3e3074e4d6995eaf96c7d8cf622eadffb7","modified":1717292381214},{"_id":"public/14e576c/5.png","hash":"7ae786b309a757c5f61a713c7ceef4d2824b024e","modified":1717292381214},{"_id":"public/f5c015c/speculative_decoding.png","hash":"fe277fa76f9f9c71e2030a41ca9eab458c33826a","modified":1717292381214},{"_id":"public/images/background/wallhaven-p97q73.png","hash":"e83875b9cd1dcfa87f0e76dc1a327cbe2921c5da","modified":1717292381214},{"_id":"public/c61d17e3/ms_invest_mistral.png","hash":"faf324c0b57843516a0b256750e6475ec0c2ce93","modified":1717292381214},{"_id":"public/6a40bfa5/ellipse_1.png","hash":"ef2470f6bf1511dc9aac9f1c6489b9d2ffdcb45f","modified":1717292381214},{"_id":"public/45ee1a6d/digimon.png","hash":"247f4059dd9671047f5d6707d8cef75a93d93f40","modified":1717292381214},{"_id":"public/b70b4a2d/norm_in_nlp.png","hash":"7be79b0e55d7d00ff6c16c247d0e506771453380","modified":1717292381214},{"_id":"public/images/background/wallhaven-gpxpg3.png","hash":"d798eaa0d679ed8a14022b0741098711c3402ca8","modified":1717292381214},{"_id":"public/images/background/wallhaven-x636oz.png","hash":"d419359164da44691e2bc8641256ca7c8111e227","modified":1717292381214}],"Category":[{"name":"CS","_id":"clwwvjmfc0004am4kbjnn53l9"},{"name":"NLP","parent":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfh000iam4k1o5i46zb"},{"name":"LLM","parent":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfk001ham4k1r6t008k"}],"Data":[{"_id":"styles","data":".post-toc .nav .nav-child {\n  display: block;\n}\n.post-toc ol {\n  font-size: 13px;\n}\nbody {\n  background: url(\"/images/background/wallhaven-p97q73.png\");\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-size: cover;\n  background-position: 50% 50%;\n}\n:root {\n  --content-bg-color: rgba(32,32,32,0.816);\n}\n"}],"Page":[{"title":"tags","date":"2024-01-31T10:50:02.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2024-01-31 18:50:02\ntype: \"tags\"\ncomments: false\n---\n","updated":"2024-01-31T10:57:24.396Z","path":"tags/index.html","layout":"page","_id":"clwwvjmf90000am4k53ny0ngg","content":"\n","length":0,"excerpt":"","more":"\n"},{"title":"about","date":"2024-01-31T10:57:44.000Z","type":"about","comments":0,"_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2024-01-31 18:57:44\ntype: \"about\"\ncomments: false\n---\n","updated":"2024-01-31T10:58:21.349Z","path":"about/index.html","layout":"page","_id":"clwwvjmfc0002am4k2i0pdyl9","content":"\n","length":0,"excerpt":"","more":"\n"},{"title":"categories","date":"2024-01-31T10:57:57.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2024-01-31 18:57:57\ntype: \"categories\"\ncomments: false\n---\n","updated":"2024-01-31T10:58:37.077Z","path":"categories/index.html","layout":"page","_id":"clwwvjmfd0006am4kbbxngw4i","content":"\n","length":0,"excerpt":"","more":"\n"}],"Post":[{"title":"-DPO","abbrlink":"473f2b43","date":"2024-05-26T14:01:48.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\nSFT  \n\nChatGPTSFT+RLHF PPOPPOPPOrewardPPOPPO  \n\nDPODirect Preference OptimizationPPODPOrewardPPO  \n\nDPO  \n\n#   \n\n  \n\n80%  \n\n  \n\n  \n\nresponseactionAI  \n\nSFTRLHF/RLAIFRLHF  \n\nDPORLHF  \n\n# RLHF\n\nRLHF  \n\n1. SFT Phase  \n\n $\\pi^{\\mathrm{SFT}}$  \n\n2. Reward Modelling Phase  \n\nprompt $x$ $(y_1,y_2)\\sim\\pi^\\text{SFT}(y|x)$ $y_1,y_2$(preference) $y_w\\succ y_l\\mid x$wlwinlose  \n\nlatent reward model $r^*(y,x)$ $(x,y)$  $r^*(y,x)$ RLHFreward model  \n\n $r^*(y,x)$preferenceBradley-Terry modelranked answersPlackett-Luce ranking models  \n\nBradley-Terry model $p^{*}$   \n\n$$\\begin{aligned}p^*(y_1\\succ y_2\\mid x)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}$$  \n\nrewardsoftmax  \n\n $p^{*}$  $\\mathcal{D}=\\left\\{x^{(i)},y_w^{(i)},y_l^{(i)}\\right\\}_{i=1}^N$  $\\pi^{\\mathrm{SFT}}$ reward $r_\\phi(x,y)$maximum likelihood $r^*(y,x)$negative log-likelihood loss  \n\n$$\\mathcal{L}_R(r_\\phi,\\mathcal{D})=-\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\begin{bmatrix}\\log\\sigma(r_\\phi(x,y_w)-r_\\phi(x,y_l))\\end{bmatrix}$$  \n\nreward functionreward $x$ $\\mathbb{E}_{x,y\\thicksim\\mathcal{D}}\\left[r_\\phi(x,y)\\right]=0$  \n\n3. RL Fine-Tuning Phase  \n\nreward  \n\n$$\\max_{\\pi_\\theta}\\mathbb{E}_{x\\sim\\mathcal{D},y\\sim\\pi_\\theta(y|x)}\\begin{bmatrix}r_\\phi(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi_\\theta(y\\mid x)\\mid\\mid\\pi_{\\mathrm{ref}}(y\\mid x)\\end{bmatrix}$$  \n\nrewardRLHFactor modelreward  \n\nKL $\\pi^{\\mathrm{SFT}}$rewardrewardmode-collapse$\\beta$   \n\nRL  \n\nRLreward fucntion  \n\n$$r(x,y)=r_\\phi(x,y)-\\beta(\\log\\pi_\\theta(y\\mid x)-\\log\\pi_\\text{ref}(y\\mid x))$$  \n\nPPO  \n\n# Direct Preference Optimization  \n\nDPOpolicy optimizationreward  \n\n{% asset_img intro.png DPO %}  \n\n## DPO  \n\nDPORLreward function $r(x,y)$reference model $\\pi_{\\mathrm{ref}}$  \n\n$$\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi(y|x)||\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}$$  \n\nKL  \n\n$$\\beta\\mathbb{D}_{\\mathrm{KL}}(\\pi\\|\\pi_{\\mathrm{ref}})=\\beta\\sum_y\\pi(y|x)\\log\\frac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}$$  \n\n  \n\n$$\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathbf{KL}}\\begin{bmatrix}\\pi(y|x)&\\mid\\mid\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}$$  \n\n$$\\begin{aligned}&=\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}$$  \n\n  \n\n$$\\begin{aligned}\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}-\\frac{1}{\\beta}r(x,y)\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}$$  \n\n  \n\n$$Z(x)=\\sum_y\\pi_\\text{ref}(y|x)\\exp\\left(\\frac1\\beta r(x,y)\\right)$$  \n\n  \n\n$$\\begin{aligned}\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\n  \n\n$$\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}-\\log Z(x)\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log Z(x)\\right]\\end{aligned}$$  \n\n $Z(x)$  $y$   \n\n$$\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log Z(x)\\right]\\end{aligned}$$  \n\n$$=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{E}_{y\\thicksim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}\\right]-\\log Z(x)\\right]$$  \n\n$$=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{D}_{\\text{KL}}(\\pi(y|x)\\mid\\mid\\pi^*(y|x))-\\log Z(x)\\right]$$  \n\n$Z(x)$  $\\pi$ KLKL0  \n\n$$\\begin{aligned}\\pi(y|x)=\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\n $Z(x)$   \n\n  \n\n$$\\begin{aligned}\\pi_r(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\n$$\\begin{aligned}\n\\log Z(x)+\\log \\pi_r(y|x)=\\log \\pi_{\\text{ref}}(y|x) +\\frac{1}{\\beta}r(x,y)\n\\end{aligned}$$  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\nBradley-Terry modelBradley-Terry model  \n\n$$\\begin{aligned}p^*(y_1\\succ y_2\\mid x)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}$$  \n\n  \n\n$$\\begin{aligned}\np^*(y_1\\succ y_2\\mid x)&=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\\\\n&=\\frac1{1+\\frac{\\exp(r^*(x,y_2))}{\\exp(r^*(x,y_1))}}\\\\\n&=\\frac1{1+\\exp(r^*(x,y_2)-r^*(x,y_1))}\n\\end{aligned}$$  \n\n $r$   \n\n$$p^*(y_1\\succ y_2\\mid x)=\\frac{1}{1+\\exp\\left(\\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{\\text{ref}}(y_2|x)}-\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{\\text{ref}}(y_1|x)}\\right)}$$  \n\noptimal policyrewardMLE  \n\n$$\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid x)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid x)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]$$  \n\nDPO loss  \n\n{% asset_img dpo_loss_code.png DPO %}  \n\n## DPO  \n\nDPOlossDPO  \n\n  \n\n$$u=\\beta\\log\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}-\\beta\\log\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\mathrm{ref}}(y_{l}|x)}$$  \n\n  \n\n$$L_{DPO}(\\pi_{\\theta};\\pi_{\\mathrm{ref}})=-\\min_{\\pi_{0}}E_{(x,y_{u},y_{t})\\sim D}[\\log\\sigma(u)]$$  \n\nsigmoid  \n\n$$\\frac\\partial{\\partial u}\\log\\sigma(u)=\\frac1{\\sigma(u)}\\cdot\\sigma(u)(1-\\sigma(u))=1-\\sigma(u)$$  \n\nsigmoid  \n\n$$1-\\sigma(u)=\\sigma(-u)$$  \n\n $u$   \n\n$$\\frac{\\partial u}{\\partial\\theta}=\\beta\\left(\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\mathrm{ref}}(y_w|x)}-\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}\\right)$$  \n\n $\\pi_{\\mathrm{ref}}$  $\\theta$  \n\n$$\\begin{aligned}\n\\frac\\partial{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_\\mathrm{ref}(y_w|x)}=&\\frac{1}{\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}}\\cdot\\frac{\\partial}{\\partial\\theta}\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}\\\\\n=&\\frac{1}{\\pi_{\\theta}(y_{w}|x)}\\cdot\\frac{\\partial}{\\partial\\theta}\\pi_{\\theta}(y_{w}|x)\\\\\n=&\\begin{aligned}\\nabla_\\theta\\log\\pi(y_w\\mid x)\\end{aligned}\n\\end{aligned}$$  \n\n  \n\n$$\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}=\\nabla_\\theta\\log\\pi(y_l\\mid x)$$  \n\nDPO  \n\n$$\\begin{aligned}\n&\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\beta\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}-\\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid x)\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}$$  \n\n  \n\n$$\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}$$  \n\nDPO  \n\n$$\\begin{aligned}\n&\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&=-\\beta\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\sigma\\left(\\hat{r}_\\theta(x,y_l)-\\hat{r}_\\theta(x,y_w)\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid x)\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}$$  \n\n  \n\n{% asset_img gradient.png DPO %}  \n\n$\\hat{r}_\\theta(x,y)$  $\\pi_{\\theta}$  $\\pi_{\\mathrm{ref}}$ reward  \n\n## DPO  \n\nDPO  \n- prompt $x$ $y_1,y_2\\sim\\pi_{\\text{ref}}(\\cdot\\mid x)$ $\\mathcal{D}=\\{x^{(i)},y_w^{(i)},y_l)^{(i)}\\}_{i=1}^N$  \n-  $\\mathcal{L}_{\\mathrm{DPO}}$ $\\pi_{\\mathrm{ref}}$$\\mathcal{D}$  $\\beta$  $\\pi\\theta $  \n\n  \n\n $\\pi^{\\mathrm{SFT}}$  $\\pi_{\\mathrm{ref}}=\\pi^{\\mathrm{SFT}}$ $(x,y_w)$  $\\pi_{\\mathrm{ref}}$   \n\n$$\\pi_{\\text{ref}}=\\arg\\max_\\pi\\mathbb{E}_{x,y_w\\thicksim\\mathcal{D}}\\left[\\log\\pi(y_w\\mid x)\\right]$$  \n\n $\\pi_{\\mathrm{ref}}$  reference distribution distribution shift  \n\n## Your Language Model Is Secretly a Reward Model  \n\nDPOlossreward  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\n $Z(x)$   \n\n\"Plackett-Luce/Bradley-Terryreward functionpreference distribution\"  \n\n> Under the Plackett-Luce preference framework, and in particular the BradleyTerry framework, two reward functions from the same equivalence class induce the same preference distribution  \n\nreward function $r(x,y)$  $r^{\\prime}(x,y)$   \n\n$$r'(x,y)=r(x,y)+f(x)$$  \n\nreward function(equivalence class)  \n\nprompt $x$  answer $y_1,\\ldots,y_K$ranking $\\tau$Plackett-Luce frameworkBradleyTerry  \n\n$$\\begin{aligned}\np_{r'}(\\tau|y_1,\\ldots,y_K,x)& =\\prod_{k=1}^K\\frac{\\exp(r'(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r'(x,y_{\\tau(j)}))}  \\\\\n&=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)})+f(x))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)})+f(x))} \\\\\n&=\\prod_{k=1}^K\\frac{\\exp(f(x))\\exp(r(x,y_{\\tau(k)}))}{\\exp(f(x))\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))} \\\\\n&=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))} \\\\\n&=p_r(\\tau|y_1,\\ldots,y_K,x)\n\\end{aligned}$$  \n\n $\\beta\\log Z(x)$ reward functionpreference distribution  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\n$$\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}$$  \n\nreward functionRLoptimal policy  \n\nDPOlossoptimal policy  \n\n$$\\begin{aligned}\\pi(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\nreward functionoptimal policy$r'(x,y)=r(x,y)+f(x)$$\\pi_r$  $\\pi_{r'}$ optimal policy  \n\n$$\\begin{aligned}\n\\pi_{r^{\\prime}}(y|x)& \\begin{aligned}&=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r'(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r'(x,y)\\right)\\end{aligned}  \\\\\n&=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right) \\\\\n&\\begin{aligned}=\\frac{1}{\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\end{aligned} \\\\\n&\\begin{aligned}&=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned} \\\\\n&=\\pi_r(y|x)\n\\end{aligned}$$  \n\nPlackett-LuceBradley-Terryreward $\\pi(y\\mid x)$  reference model $\\pi_{ref}(y\\mid x)$   \n\n$$r(x,y)=\\beta\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}$$  \n\nreward model  \n\n##   \n\n  \n- $\\beta=0.1$TL;DR summarization0.5  \n- batch size = 64  \n- RMSprop optimizer  \n- learning rate = 1e-6  \n- linearly warmup 0 to 1e-6 over 150 steps  \n\nPPOSFTDPO  \n\nDPO  \n\n{% asset_img result_1.png 1 %}  \n\n{% asset_img result_2.png 2 %}  \n\n{% asset_img result_3.png 3 %}  \n\n{% asset_img result_4.png 4 %}  \n\n#   \n\n- DPORLHF PPOreward  \n- DPOPPO  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n1Direct Preference Optimization: Your Language Model is Secretly a Reward Model https://arxiv.org/abs/2305.18290v2  ","source":"_posts/cs/nlp/2024/05/-DPO.md","raw":"---\ntitle: -DPO\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \n  - SFT\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 473f2b43\ndate: 2024-05-26 22:01:48\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\nSFT  \n\nChatGPTSFT+RLHF PPOPPOPPOrewardPPOPPO  \n\nDPODirect Preference OptimizationPPODPOrewardPPO  \n\nDPO  \n\n#   \n\n  \n\n80%  \n\n  \n\n  \n\nresponseactionAI  \n\nSFTRLHF/RLAIFRLHF  \n\nDPORLHF  \n\n# RLHF\n\nRLHF  \n\n1. SFT Phase  \n\n $\\pi^{\\mathrm{SFT}}$  \n\n2. Reward Modelling Phase  \n\nprompt $x$ $(y_1,y_2)\\sim\\pi^\\text{SFT}(y|x)$ $y_1,y_2$(preference) $y_w\\succ y_l\\mid x$wlwinlose  \n\nlatent reward model $r^*(y,x)$ $(x,y)$  $r^*(y,x)$ RLHFreward model  \n\n $r^*(y,x)$preferenceBradley-Terry modelranked answersPlackett-Luce ranking models  \n\nBradley-Terry model $p^{*}$   \n\n$$\\begin{aligned}p^*(y_1\\succ y_2\\mid x)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}$$  \n\nrewardsoftmax  \n\n $p^{*}$  $\\mathcal{D}=\\left\\{x^{(i)},y_w^{(i)},y_l^{(i)}\\right\\}_{i=1}^N$  $\\pi^{\\mathrm{SFT}}$ reward $r_\\phi(x,y)$maximum likelihood $r^*(y,x)$negative log-likelihood loss  \n\n$$\\mathcal{L}_R(r_\\phi,\\mathcal{D})=-\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\begin{bmatrix}\\log\\sigma(r_\\phi(x,y_w)-r_\\phi(x,y_l))\\end{bmatrix}$$  \n\nreward functionreward $x$ $\\mathbb{E}_{x,y\\thicksim\\mathcal{D}}\\left[r_\\phi(x,y)\\right]=0$  \n\n3. RL Fine-Tuning Phase  \n\nreward  \n\n$$\\max_{\\pi_\\theta}\\mathbb{E}_{x\\sim\\mathcal{D},y\\sim\\pi_\\theta(y|x)}\\begin{bmatrix}r_\\phi(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi_\\theta(y\\mid x)\\mid\\mid\\pi_{\\mathrm{ref}}(y\\mid x)\\end{bmatrix}$$  \n\nrewardRLHFactor modelreward  \n\nKL $\\pi^{\\mathrm{SFT}}$rewardrewardmode-collapse$\\beta$   \n\nRL  \n\nRLreward fucntion  \n\n$$r(x,y)=r_\\phi(x,y)-\\beta(\\log\\pi_\\theta(y\\mid x)-\\log\\pi_\\text{ref}(y\\mid x))$$  \n\nPPO  \n\n# Direct Preference Optimization  \n\nDPOpolicy optimizationreward  \n\n{% asset_img intro.png DPO %}  \n\n## DPO  \n\nDPORLreward function $r(x,y)$reference model $\\pi_{\\mathrm{ref}}$  \n\n$$\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi(y|x)||\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}$$  \n\nKL  \n\n$$\\beta\\mathbb{D}_{\\mathrm{KL}}(\\pi\\|\\pi_{\\mathrm{ref}})=\\beta\\sum_y\\pi(y|x)\\log\\frac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}$$  \n\n  \n\n$$\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathbf{KL}}\\begin{bmatrix}\\pi(y|x)&\\mid\\mid\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}$$  \n\n$$\\begin{aligned}&=\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}$$  \n\n  \n\n$$\\begin{aligned}\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}-\\frac{1}{\\beta}r(x,y)\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}$$  \n\n  \n\n$$Z(x)=\\sum_y\\pi_\\text{ref}(y|x)\\exp\\left(\\frac1\\beta r(x,y)\\right)$$  \n\n  \n\n$$\\begin{aligned}\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\n  \n\n$$\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}-\\log Z(x)\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log Z(x)\\right]\\end{aligned}$$  \n\n $Z(x)$  $y$   \n\n$$\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log Z(x)\\right]\\end{aligned}$$  \n\n$$=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{E}_{y\\thicksim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}\\right]-\\log Z(x)\\right]$$  \n\n$$=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{D}_{\\text{KL}}(\\pi(y|x)\\mid\\mid\\pi^*(y|x))-\\log Z(x)\\right]$$  \n\n$Z(x)$  $\\pi$ KLKL0  \n\n$$\\begin{aligned}\\pi(y|x)=\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\n $Z(x)$   \n\n  \n\n$$\\begin{aligned}\\pi_r(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\n$$\\begin{aligned}\n\\log Z(x)+\\log \\pi_r(y|x)=\\log \\pi_{\\text{ref}}(y|x) +\\frac{1}{\\beta}r(x,y)\n\\end{aligned}$$  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\nBradley-Terry modelBradley-Terry model  \n\n$$\\begin{aligned}p^*(y_1\\succ y_2\\mid x)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}$$  \n\n  \n\n$$\\begin{aligned}\np^*(y_1\\succ y_2\\mid x)&=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\\\\n&=\\frac1{1+\\frac{\\exp(r^*(x,y_2))}{\\exp(r^*(x,y_1))}}\\\\\n&=\\frac1{1+\\exp(r^*(x,y_2)-r^*(x,y_1))}\n\\end{aligned}$$  \n\n $r$   \n\n$$p^*(y_1\\succ y_2\\mid x)=\\frac{1}{1+\\exp\\left(\\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{\\text{ref}}(y_2|x)}-\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{\\text{ref}}(y_1|x)}\\right)}$$  \n\noptimal policyrewardMLE  \n\n$$\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid x)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid x)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]$$  \n\nDPO loss  \n\n{% asset_img dpo_loss_code.png DPO %}  \n\n## DPO  \n\nDPOlossDPO  \n\n  \n\n$$u=\\beta\\log\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}-\\beta\\log\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\mathrm{ref}}(y_{l}|x)}$$  \n\n  \n\n$$L_{DPO}(\\pi_{\\theta};\\pi_{\\mathrm{ref}})=-\\min_{\\pi_{0}}E_{(x,y_{u},y_{t})\\sim D}[\\log\\sigma(u)]$$  \n\nsigmoid  \n\n$$\\frac\\partial{\\partial u}\\log\\sigma(u)=\\frac1{\\sigma(u)}\\cdot\\sigma(u)(1-\\sigma(u))=1-\\sigma(u)$$  \n\nsigmoid  \n\n$$1-\\sigma(u)=\\sigma(-u)$$  \n\n $u$   \n\n$$\\frac{\\partial u}{\\partial\\theta}=\\beta\\left(\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\mathrm{ref}}(y_w|x)}-\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}\\right)$$  \n\n $\\pi_{\\mathrm{ref}}$  $\\theta$  \n\n$$\\begin{aligned}\n\\frac\\partial{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_\\mathrm{ref}(y_w|x)}=&\\frac{1}{\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}}\\cdot\\frac{\\partial}{\\partial\\theta}\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}\\\\\n=&\\frac{1}{\\pi_{\\theta}(y_{w}|x)}\\cdot\\frac{\\partial}{\\partial\\theta}\\pi_{\\theta}(y_{w}|x)\\\\\n=&\\begin{aligned}\\nabla_\\theta\\log\\pi(y_w\\mid x)\\end{aligned}\n\\end{aligned}$$  \n\n  \n\n$$\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}=\\nabla_\\theta\\log\\pi(y_l\\mid x)$$  \n\nDPO  \n\n$$\\begin{aligned}\n&\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\beta\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}-\\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid x)\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}$$  \n\n  \n\n$$\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}$$  \n\nDPO  \n\n$$\\begin{aligned}\n&\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&=-\\beta\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\sigma\\left(\\hat{r}_\\theta(x,y_l)-\\hat{r}_\\theta(x,y_w)\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid x)\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}$$  \n\n  \n\n{% asset_img gradient.png DPO %}  \n\n$\\hat{r}_\\theta(x,y)$  $\\pi_{\\theta}$  $\\pi_{\\mathrm{ref}}$ reward  \n\n## DPO  \n\nDPO  \n- prompt $x$ $y_1,y_2\\sim\\pi_{\\text{ref}}(\\cdot\\mid x)$ $\\mathcal{D}=\\{x^{(i)},y_w^{(i)},y_l)^{(i)}\\}_{i=1}^N$  \n-  $\\mathcal{L}_{\\mathrm{DPO}}$ $\\pi_{\\mathrm{ref}}$$\\mathcal{D}$  $\\beta$  $\\pi\\theta $  \n\n  \n\n $\\pi^{\\mathrm{SFT}}$  $\\pi_{\\mathrm{ref}}=\\pi^{\\mathrm{SFT}}$ $(x,y_w)$  $\\pi_{\\mathrm{ref}}$   \n\n$$\\pi_{\\text{ref}}=\\arg\\max_\\pi\\mathbb{E}_{x,y_w\\thicksim\\mathcal{D}}\\left[\\log\\pi(y_w\\mid x)\\right]$$  \n\n $\\pi_{\\mathrm{ref}}$  reference distribution distribution shift  \n\n## Your Language Model Is Secretly a Reward Model  \n\nDPOlossreward  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\n $Z(x)$   \n\n\"Plackett-Luce/Bradley-Terryreward functionpreference distribution\"  \n\n> Under the Plackett-Luce preference framework, and in particular the BradleyTerry framework, two reward functions from the same equivalence class induce the same preference distribution  \n\nreward function $r(x,y)$  $r^{\\prime}(x,y)$   \n\n$$r'(x,y)=r(x,y)+f(x)$$  \n\nreward function(equivalence class)  \n\nprompt $x$  answer $y_1,\\ldots,y_K$ranking $\\tau$Plackett-Luce frameworkBradleyTerry  \n\n$$\\begin{aligned}\np_{r'}(\\tau|y_1,\\ldots,y_K,x)& =\\prod_{k=1}^K\\frac{\\exp(r'(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r'(x,y_{\\tau(j)}))}  \\\\\n&=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)})+f(x))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)})+f(x))} \\\\\n&=\\prod_{k=1}^K\\frac{\\exp(f(x))\\exp(r(x,y_{\\tau(k)}))}{\\exp(f(x))\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))} \\\\\n&=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))} \\\\\n&=p_r(\\tau|y_1,\\ldots,y_K,x)\n\\end{aligned}$$  \n\n $\\beta\\log Z(x)$ reward functionpreference distribution  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\n$$\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}$$  \n\nreward functionRLoptimal policy  \n\nDPOlossoptimal policy  \n\n$$\\begin{aligned}\\pi(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\nreward functionoptimal policy$r'(x,y)=r(x,y)+f(x)$$\\pi_r$  $\\pi_{r'}$ optimal policy  \n\n$$\\begin{aligned}\n\\pi_{r^{\\prime}}(y|x)& \\begin{aligned}&=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r'(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r'(x,y)\\right)\\end{aligned}  \\\\\n&=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right) \\\\\n&\\begin{aligned}=\\frac{1}{\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\end{aligned} \\\\\n&\\begin{aligned}&=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned} \\\\\n&=\\pi_r(y|x)\n\\end{aligned}$$  \n\nPlackett-LuceBradley-Terryreward $\\pi(y\\mid x)$  reference model $\\pi_{ref}(y\\mid x)$   \n\n$$r(x,y)=\\beta\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}$$  \n\nreward model  \n\n##   \n\n  \n- $\\beta=0.1$TL;DR summarization0.5  \n- batch size = 64  \n- RMSprop optimizer  \n- learning rate = 1e-6  \n- linearly warmup 0 to 1e-6 over 150 steps  \n\nPPOSFTDPO  \n\nDPO  \n\n{% asset_img result_1.png 1 %}  \n\n{% asset_img result_2.png 2 %}  \n\n{% asset_img result_3.png 3 %}  \n\n{% asset_img result_4.png 4 %}  \n\n#   \n\n- DPORLHF PPOreward  \n- DPOPPO  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n1Direct Preference Optimization: Your Language Model is Secretly a Reward Model https://arxiv.org/abs/2305.18290v2  ","slug":"cs/nlp/2024/05/-DPO","published":1,"updated":"2024-05-29T12:33:13.225Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmfa0001am4k00qx9rl7","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>SFT</p>\n<p>ChatGPTSFT+RLHF\nPPOPPOPPOrewardPPOPPO</p>\n<p>DPODirect Preference\nOptimizationPPODPOrewardPPO</p>\n<p>DPO</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>80%</p>\n<p></p>\n<p></p>\n<p>responseactionAI</p>\n<p>SFTRLHF/RLAIFRLHF</p>\n<p>DPORLHF</p>\n<h1 id=\"rlhf\">RLHF</h1>\n<p>RLHF</p>\n<ol type=\"1\">\n<li>SFT Phase</li>\n</ol>\n<p> <span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span></p>\n<ol start=\"2\" type=\"1\">\n<li>Reward Modelling Phase</li>\n</ol>\n<p>prompt <span class=\"math inline\">\\(x\\)</span>\n<span class=\"math inline\">\\((y_1,y_2)\\sim\\pi^\\text{SFT}(y|x)\\)</span>\n<span class=\"math inline\">\\(y_1,y_2\\)</span>(preference)\n<span class=\"math inline\">\\(y_w\\succ y_l\\mid\nx\\)</span>wlwinlose</p>\n<p>latent reward model\n<span class=\"math inline\">\\(r^*(y,x)\\)</span> <span class=\"math inline\">\\((x,y)\\)</span>  <span class=\"math inline\">\\(r^*(y,x)\\)</span> RLHFreward\nmodel</p>\n<p> <span class=\"math inline\">\\(r^*(y,x)\\)</span>preferenceBradley-Terry\nmodelranked\nanswersPlackett-Luce ranking models</p>\n<p>Bradley-Terry model <span class=\"math inline\">\\(p^{*}\\)</span> </p>\n<p><span class=\"math display\">\\[\\begin{aligned}p^*(y_1\\succ y_2\\mid\nx)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}\\]</span></p>\n<p>rewardsoftmax</p>\n<p> <span class=\"math inline\">\\(p^{*}\\)</span>\n <span class=\"math inline\">\\(\\mathcal{D}=\\left\\{x^{(i)},y_w^{(i)},y_l^{(i)}\\right\\}_{i=1}^N\\)</span>\n <span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span> reward\n<span class=\"math inline\">\\(r_\\phi(x,y)\\)</span>maximum\nlikelihood <span class=\"math inline\">\\(r^*(y,x)\\)</span>negative\nlog-likelihood loss</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_R(r_\\phi,\\mathcal{D})=-\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\begin{bmatrix}\\log\\sigma(r_\\phi(x,y_w)-r_\\phi(x,y_l))\\end{bmatrix}\\]</span></p>\n<p>reward\nfunctionreward <span class=\"math inline\">\\(x\\)</span> <span class=\"math inline\">\\(\\mathbb{E}_{x,y\\thicksim\\mathcal{D}}\\left[r_\\phi(x,y)\\right]=0\\)</span></p>\n<ol start=\"3\" type=\"1\">\n<li>RL Fine-Tuning Phase</li>\n</ol>\n<p>reward</p>\n<p><span class=\"math display\">\\[\\max_{\\pi_\\theta}\\mathbb{E}_{x\\sim\\mathcal{D},y\\sim\\pi_\\theta(y|x)}\\begin{bmatrix}r_\\phi(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi_\\theta(y\\mid\nx)\\mid\\mid\\pi_{\\mathrm{ref}}(y\\mid x)\\end{bmatrix}\\]</span></p>\n<p>rewardRLHFactor\nmodelreward</p>\n<p>KL\n<span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span>rewardrewardmode-collapse<span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p>RL</p>\n<p>RLreward fucntion</p>\n<p><span class=\"math display\">\\[r(x,y)=r_\\phi(x,y)-\\beta(\\log\\pi_\\theta(y\\mid\nx)-\\log\\pi_\\text{ref}(y\\mid x))\\]</span></p>\n<p>PPO</p>\n<h1 id=\"direct-preference-optimization\">Direct Preference\nOptimization</h1>\n<p>DPOpolicy\noptimizationreward</p>\n<img src=\"/473f2b43/intro.png\" class title=\"DPO\">\n<h2 id=\"dpo\">DPO</h2>\n<p>DPORLreward function <span class=\"math inline\">\\(r(x,y)\\)</span>reference model <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span></p>\n<p><span class=\"math display\">\\[\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi(y|x)||\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}\\]</span></p>\n<p>KL</p>\n<p><span class=\"math display\">\\[\\beta\\mathbb{D}_{\\mathrm{KL}}(\\pi\\|\\pi_{\\mathrm{ref}})=\\beta\\sum_y\\pi(y|x)\\log\\frac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathbf{KL}}\\begin{bmatrix}\\pi(y|x)&amp;\\mid\\mid\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}-\\frac{1}{\\beta}r(x,y)\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[Z(x)=\\sum_y\\pi_\\text{ref}(y|x)\\exp\\left(\\frac1\\beta\nr(x,y)\\right)\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}-\\log\nZ(x)\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log\nZ(x)\\right]\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(Z(x)\\)</span>  <span class=\"math inline\">\\(y\\)</span> </p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log\nZ(x)\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{E}_{y\\thicksim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}\\right]-\\log\nZ(x)\\right]\\]</span></p>\n<p><span class=\"math display\">\\[=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{D}_{\\text{KL}}(\\pi(y|x)\\mid\\mid\\pi^*(y|x))-\\log\nZ(x)\\right]\\]</span></p>\n<p><span class=\"math inline\">\\(Z(x)\\)</span>  <span class=\"math inline\">\\(\\pi\\)</span>\nKLKL0</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi(y|x)=\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(Z(x)\\)</span>\n</p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi_r(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\log Z(x)+\\log \\pi_r(y|x)=\\log \\pi_{\\text{ref}}(y|x)\n+\\frac{1}{\\beta}r(x,y)\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p>Bradley-Terry modelBradley-Terry\nmodel</p>\n<p><span class=\"math display\">\\[\\begin{aligned}p^*(y_1\\succ y_2\\mid\nx)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\np^*(y_1\\succ y_2\\mid\nx)&amp;=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\\\\n&amp;=\\frac1{1+\\frac{\\exp(r^*(x,y_2))}{\\exp(r^*(x,y_1))}}\\\\\n&amp;=\\frac1{1+\\exp(r^*(x,y_2)-r^*(x,y_1))}\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(r\\)</span>\n</p>\n<p><span class=\"math display\">\\[p^*(y_1\\succ y_2\\mid\nx)=\\frac{1}{1+\\exp\\left(\\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{\\text{ref}}(y_2|x)}-\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{\\text{ref}}(y_1|x)}\\right)}\\]</span></p>\n<p>optimal\npolicyrewardMLE</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid\nx)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid\nx)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]\\]</span></p>\n<p>DPO loss</p>\n<img src=\"/473f2b43/dpo_loss_code.png\" class title=\"DPO\">\n<h2 id=\"dpo\">DPO</h2>\n<p>DPOlossDPO</p>\n<p></p>\n<p><span class=\"math display\">\\[u=\\beta\\log\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}-\\beta\\log\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\mathrm{ref}}(y_{l}|x)}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[L_{DPO}(\\pi_{\\theta};\\pi_{\\mathrm{ref}})=-\\min_{\\pi_{0}}E_{(x,y_{u},y_{t})\\sim\nD}[\\log\\sigma(u)]\\]</span></p>\n<p>sigmoid</p>\n<p><span class=\"math display\">\\[\\frac\\partial{\\partial\nu}\\log\\sigma(u)=\\frac1{\\sigma(u)}\\cdot\\sigma(u)(1-\\sigma(u))=1-\\sigma(u)\\]</span></p>\n<p>sigmoid</p>\n<p><span class=\"math display\">\\[1-\\sigma(u)=\\sigma(-u)\\]</span></p>\n<p> <span class=\"math inline\">\\(u\\)</span> </p>\n<p><span class=\"math display\">\\[\\frac{\\partial\nu}{\\partial\\theta}=\\beta\\left(\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\mathrm{ref}}(y_w|x)}-\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}\\right)\\]</span></p>\n<p> <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span>  <span class=\"math inline\">\\(\\theta\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\frac\\partial{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_\\mathrm{ref}(y_w|x)}=&amp;\\frac{1}{\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}}\\cdot\\frac{\\partial}{\\partial\\theta}\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}\\\\\n=&amp;\\frac{1}{\\pi_{\\theta}(y_{w}|x)}\\cdot\\frac{\\partial}{\\partial\\theta}\\pi_{\\theta}(y_{w}|x)\\\\\n=&amp;\\begin{aligned}\\nabla_\\theta\\log\\pi(y_w\\mid x)\\end{aligned}\n\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}=\\nabla_\\theta\\log\\pi(y_l\\mid\nx)\\]</span></p>\n<p>DPO</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&amp;=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\beta\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}-\\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid\nx)\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}\\]</span></p>\n<p>DPO</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&amp;=-\\beta\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\sigma\\left(\\hat{r}_\\theta(x,y_l)-\\hat{r}_\\theta(x,y_w)\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid\nx)\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}\\]</span></p>\n<p></p>\n<img src=\"/473f2b43/gradient.png\" class title=\"DPO\">\n<p><span class=\"math inline\">\\(\\hat{r}_\\theta(x,y)\\)</span>  <span class=\"math inline\">\\(\\pi_{\\theta}\\)</span>  <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span>\nreward</p>\n<h2 id=\"dpo\">DPO</h2>\n<p>DPO<br>\n- prompt <span class=\"math inline\">\\(x\\)</span> <span class=\"math inline\">\\(y_1,y_2\\sim\\pi_{\\text{ref}}(\\cdot\\mid\nx)\\)</span> <span class=\"math inline\">\\(\\mathcal{D}=\\{x^{(i)},y_w^{(i)},y_l)^{(i)}\\}_{i=1}^N\\)</span><br>\n-  <span class=\"math inline\">\\(\\mathcal{L}_{\\mathrm{DPO}}\\)</span>\n<span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span><span class=\"math inline\">\\(\\mathcal{D}\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span>  $$</p>\n<p></p>\n<p> <span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span> \n<span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}=\\pi^{\\mathrm{SFT}}\\)</span>\n<span class=\"math inline\">\\((x,y_w)\\)</span>  <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span> </p>\n<p><span class=\"math display\">\\[\\pi_{\\text{ref}}=\\arg\\max_\\pi\\mathbb{E}_{x,y_w\\thicksim\\mathcal{D}}\\left[\\log\\pi(y_w\\mid\nx)\\right]\\]</span></p>\n<p> <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span>  reference\ndistribution distribution shift</p>\n<h2 id=\"your-language-model-is-secretly-a-reward-model\">Your Language\nModel Is Secretly a Reward Model</h2>\n<p>DPOlossreward</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(Z(x)\\)</span>\n</p>\n<p>\"Plackett-Luce/Bradley-Terryreward\nfunctionpreference distribution\"</p>\n<blockquote>\n<p>Under the Plackett-Luce preference framework, and in particular the\nBradleyTerry framework, two reward functions from the same equivalence\nclass induce the same preference distribution</p>\n</blockquote>\n<p>reward function <span class=\"math inline\">\\(r(x,y)\\)</span>\n <span class=\"math inline\">\\(r^{\\prime}(x,y)\\)</span> </p>\n<p><span class=\"math display\">\\[r&#39;(x,y)=r(x,y)+f(x)\\]</span></p>\n<p>reward function(equivalence class)</p>\n<p>prompt <span class=\"math inline\">\\(x\\)</span>  answer <span class=\"math inline\">\\(y_1,\\ldots,y_K\\)</span>ranking <span class=\"math inline\">\\(\\tau\\)</span>Plackett-Luce\nframeworkBradleyTerry</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\np_{r&#39;}(\\tau|y_1,\\ldots,y_K,x)&amp;\n=\\prod_{k=1}^K\\frac{\\exp(r&#39;(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r&#39;(x,y_{\\tau(j)}))}  \\\\\n&amp;=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)})+f(x))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)})+f(x))}\n\\\\\n&amp;=\\prod_{k=1}^K\\frac{\\exp(f(x))\\exp(r(x,y_{\\tau(k)}))}{\\exp(f(x))\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))}\n\\\\\n&amp;=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))}\n\\\\\n&amp;=p_r(\\tau|y_1,\\ldots,y_K,x)\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\beta\\log\nZ(x)\\)</span> reward\nfunctionpreference distribution</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}\\]</span></p>\n<p>reward\nfunctionRLoptimal policy</p>\n<p>DPOlossoptimal policy</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p>reward functionoptimal\npolicy<span class=\"math inline\">\\(r&#39;(x,y)=r(x,y)+f(x)\\)</span><span class=\"math inline\">\\(\\pi_r\\)</span>  <span class=\"math inline\">\\(\\pi_{r&#39;}\\)</span> optimal\npolicy</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\pi_{r^{\\prime}}(y|x)&amp;\n\\begin{aligned}&amp;=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r&#39;(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r&#39;(x,y)\\right)\\end{aligned}  \\\\\n&amp;=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right)\n\\\\\n&amp;\\begin{aligned}=\\frac{1}{\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\end{aligned}\n\\\\\n&amp;\\begin{aligned}&amp;=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\n\\\\\n&amp;=\\pi_r(y|x)\n\\end{aligned}\\]</span></p>\n<p>Plackett-LuceBradley-Terryreward\n<span class=\"math inline\">\\(\\pi(y\\mid x)\\)</span>  reference\nmodel <span class=\"math inline\">\\(\\pi_{ref}(y\\mid x)\\)</span>\n</p>\n<p><span class=\"math display\">\\[r(x,y)=\\beta\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}\\]</span></p>\n<p>reward model</p>\n<h2 id=\"\"></h2>\n<p><br>\n- <span class=\"math inline\">\\(\\beta=0.1\\)</span>TL;DR\nsummarization0.5<br>\n- batch size = 64<br>\n- RMSprop optimizer<br>\n- learning rate = 1e-6<br>\n- linearly warmup 0 to 1e-6 over 150 steps</p>\n<p>PPOSFTDPO</p>\n<p>DPO</p>\n<img src=\"/473f2b43/result_1.png\" class title=\"1\">\n<img src=\"/473f2b43/result_2.png\" class title=\"2\">\n<img src=\"/473f2b43/result_3.png\" class title=\"3\">\n<img src=\"/473f2b43/result_4.png\" class title=\"4\">\n<h1 id=\"\"></h1>\n<ul>\n<li>DPORLHF\nPPOreward<br>\n</li>\n<li>DPOPPO</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Direct Preference Optimization: Your Language Model is Secretly\na Reward Model https://arxiv.org/abs/2305.18290v2</p>\n","length":13170,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>SFT</p>\n<p>ChatGPTSFT+RLHF\nPPOPPOPPOrewardPPOPPO</p>\n<p>DPODirect Preference\nOptimizationPPODPOrewardPPO</p>\n<p>DPO</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>80%</p>\n<p></p>\n<p></p>\n<p>responseactionAI</p>\n<p>SFTRLHF/RLAIFRLHF</p>\n<p>DPORLHF</p>\n<h1 id=\"rlhf\">RLHF</h1>\n<p>RLHF</p>\n<ol type=\"1\">\n<li>SFT Phase</li>\n</ol>\n<p> <span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span></p>\n<ol start=\"2\" type=\"1\">\n<li>Reward Modelling Phase</li>\n</ol>\n<p>prompt <span class=\"math inline\">\\(x\\)</span>\n<span class=\"math inline\">\\((y_1,y_2)\\sim\\pi^\\text{SFT}(y|x)\\)</span>\n<span class=\"math inline\">\\(y_1,y_2\\)</span>(preference)\n<span class=\"math inline\">\\(y_w\\succ y_l\\mid\nx\\)</span>wlwinlose</p>\n<p>latent reward model\n<span class=\"math inline\">\\(r^*(y,x)\\)</span> <span class=\"math inline\">\\((x,y)\\)</span>  <span class=\"math inline\">\\(r^*(y,x)\\)</span> RLHFreward\nmodel</p>\n<p> <span class=\"math inline\">\\(r^*(y,x)\\)</span>preferenceBradley-Terry\nmodelranked\nanswersPlackett-Luce ranking models</p>\n<p>Bradley-Terry model <span class=\"math inline\">\\(p^{*}\\)</span> </p>\n<p><span class=\"math display\">\\[\\begin{aligned}p^*(y_1\\succ y_2\\mid\nx)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}\\]</span></p>\n<p>rewardsoftmax</p>\n<p> <span class=\"math inline\">\\(p^{*}\\)</span>\n <span class=\"math inline\">\\(\\mathcal{D}=\\left\\{x^{(i)},y_w^{(i)},y_l^{(i)}\\right\\}_{i=1}^N\\)</span>\n <span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span> reward\n<span class=\"math inline\">\\(r_\\phi(x,y)\\)</span>maximum\nlikelihood <span class=\"math inline\">\\(r^*(y,x)\\)</span>negative\nlog-likelihood loss</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_R(r_\\phi,\\mathcal{D})=-\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\begin{bmatrix}\\log\\sigma(r_\\phi(x,y_w)-r_\\phi(x,y_l))\\end{bmatrix}\\]</span></p>\n<p>reward\nfunctionreward <span class=\"math inline\">\\(x\\)</span> <span class=\"math inline\">\\(\\mathbb{E}_{x,y\\thicksim\\mathcal{D}}\\left[r_\\phi(x,y)\\right]=0\\)</span></p>\n<ol start=\"3\" type=\"1\">\n<li>RL Fine-Tuning Phase</li>\n</ol>\n<p>reward</p>\n<p><span class=\"math display\">\\[\\max_{\\pi_\\theta}\\mathbb{E}_{x\\sim\\mathcal{D},y\\sim\\pi_\\theta(y|x)}\\begin{bmatrix}r_\\phi(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi_\\theta(y\\mid\nx)\\mid\\mid\\pi_{\\mathrm{ref}}(y\\mid x)\\end{bmatrix}\\]</span></p>\n<p>rewardRLHFactor\nmodelreward</p>\n<p>KL\n<span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span>rewardrewardmode-collapse<span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p>RL</p>\n<p>RLreward fucntion</p>\n<p><span class=\"math display\">\\[r(x,y)=r_\\phi(x,y)-\\beta(\\log\\pi_\\theta(y\\mid\nx)-\\log\\pi_\\text{ref}(y\\mid x))\\]</span></p>\n<p>PPO</p>\n<h1 id=\"direct-preference-optimization\">Direct Preference\nOptimization</h1>\n<p>DPOpolicy\noptimizationreward</p>\n<img src=\"/473f2b43/intro.png\" class title=\"DPO\">\n<h2 id=\"dpo\">DPO</h2>\n<p>DPORLreward function <span class=\"math inline\">\\(r(x,y)\\)</span>reference model <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span></p>\n<p><span class=\"math display\">\\[\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi(y|x)||\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}\\]</span></p>\n<p>KL</p>\n<p><span class=\"math display\">\\[\\beta\\mathbb{D}_{\\mathrm{KL}}(\\pi\\|\\pi_{\\mathrm{ref}})=\\beta\\sum_y\\pi(y|x)\\log\\frac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathbf{KL}}\\begin{bmatrix}\\pi(y|x)&amp;\\mid\\mid\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}-\\frac{1}{\\beta}r(x,y)\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[Z(x)=\\sum_y\\pi_\\text{ref}(y|x)\\exp\\left(\\frac1\\beta\nr(x,y)\\right)\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}-\\log\nZ(x)\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log\nZ(x)\\right]\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(Z(x)\\)</span>  <span class=\"math inline\">\\(y\\)</span> </p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log\nZ(x)\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{E}_{y\\thicksim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}\\right]-\\log\nZ(x)\\right]\\]</span></p>\n<p><span class=\"math display\">\\[=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{D}_{\\text{KL}}(\\pi(y|x)\\mid\\mid\\pi^*(y|x))-\\log\nZ(x)\\right]\\]</span></p>\n<p><span class=\"math inline\">\\(Z(x)\\)</span>  <span class=\"math inline\">\\(\\pi\\)</span>\nKLKL0</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi(y|x)=\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(Z(x)\\)</span>\n</p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi_r(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\log Z(x)+\\log \\pi_r(y|x)=\\log \\pi_{\\text{ref}}(y|x)\n+\\frac{1}{\\beta}r(x,y)\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p>Bradley-Terry modelBradley-Terry\nmodel</p>\n<p><span class=\"math display\">\\[\\begin{aligned}p^*(y_1\\succ y_2\\mid\nx)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\np^*(y_1\\succ y_2\\mid\nx)&amp;=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\\\\n&amp;=\\frac1{1+\\frac{\\exp(r^*(x,y_2))}{\\exp(r^*(x,y_1))}}\\\\\n&amp;=\\frac1{1+\\exp(r^*(x,y_2)-r^*(x,y_1))}\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(r\\)</span>\n</p>\n<p><span class=\"math display\">\\[p^*(y_1\\succ y_2\\mid\nx)=\\frac{1}{1+\\exp\\left(\\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{\\text{ref}}(y_2|x)}-\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{\\text{ref}}(y_1|x)}\\right)}\\]</span></p>\n<p>optimal\npolicyrewardMLE</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid\nx)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid\nx)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]\\]</span></p>\n<p>DPO loss</p>\n<img src=\"/473f2b43/dpo_loss_code.png\" class title=\"DPO\">\n<h2 id=\"dpo\">DPO</h2>\n<p>DPOlossDPO</p>\n<p></p>\n<p><span class=\"math display\">\\[u=\\beta\\log\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}-\\beta\\log\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\mathrm{ref}}(y_{l}|x)}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[L_{DPO}(\\pi_{\\theta};\\pi_{\\mathrm{ref}})=-\\min_{\\pi_{0}}E_{(x,y_{u},y_{t})\\sim\nD}[\\log\\sigma(u)]\\]</span></p>\n<p>sigmoid</p>\n<p><span class=\"math display\">\\[\\frac\\partial{\\partial\nu}\\log\\sigma(u)=\\frac1{\\sigma(u)}\\cdot\\sigma(u)(1-\\sigma(u))=1-\\sigma(u)\\]</span></p>\n<p>sigmoid</p>\n<p><span class=\"math display\">\\[1-\\sigma(u)=\\sigma(-u)\\]</span></p>\n<p> <span class=\"math inline\">\\(u\\)</span> </p>\n<p><span class=\"math display\">\\[\\frac{\\partial\nu}{\\partial\\theta}=\\beta\\left(\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\mathrm{ref}}(y_w|x)}-\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}\\right)\\]</span></p>\n<p> <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span>  <span class=\"math inline\">\\(\\theta\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\frac\\partial{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_\\mathrm{ref}(y_w|x)}=&amp;\\frac{1}{\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}}\\cdot\\frac{\\partial}{\\partial\\theta}\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}\\\\\n=&amp;\\frac{1}{\\pi_{\\theta}(y_{w}|x)}\\cdot\\frac{\\partial}{\\partial\\theta}\\pi_{\\theta}(y_{w}|x)\\\\\n=&amp;\\begin{aligned}\\nabla_\\theta\\log\\pi(y_w\\mid x)\\end{aligned}\n\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}=\\nabla_\\theta\\log\\pi(y_l\\mid\nx)\\]</span></p>\n<p>DPO</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&amp;=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\beta\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}-\\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid\nx)\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}\\]</span></p>\n<p>DPO</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&amp;=-\\beta\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\sigma\\left(\\hat{r}_\\theta(x,y_l)-\\hat{r}_\\theta(x,y_w)\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid\nx)\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}\\]</span></p>\n<p></p>\n<img src=\"/473f2b43/gradient.png\" class title=\"DPO\">\n<p><span class=\"math inline\">\\(\\hat{r}_\\theta(x,y)\\)</span>  <span class=\"math inline\">\\(\\pi_{\\theta}\\)</span>  <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span>\nreward</p>\n<h2 id=\"dpo\">DPO</h2>\n<p>DPO<br>\n- prompt <span class=\"math inline\">\\(x\\)</span> <span class=\"math inline\">\\(y_1,y_2\\sim\\pi_{\\text{ref}}(\\cdot\\mid\nx)\\)</span> <span class=\"math inline\">\\(\\mathcal{D}=\\{x^{(i)},y_w^{(i)},y_l)^{(i)}\\}_{i=1}^N\\)</span><br>\n-  <span class=\"math inline\">\\(\\mathcal{L}_{\\mathrm{DPO}}\\)</span>\n<span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span><span class=\"math inline\">\\(\\mathcal{D}\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span>  $$</p>\n<p></p>\n<p> <span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span> \n<span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}=\\pi^{\\mathrm{SFT}}\\)</span>\n<span class=\"math inline\">\\((x,y_w)\\)</span>  <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span> </p>\n<p><span class=\"math display\">\\[\\pi_{\\text{ref}}=\\arg\\max_\\pi\\mathbb{E}_{x,y_w\\thicksim\\mathcal{D}}\\left[\\log\\pi(y_w\\mid\nx)\\right]\\]</span></p>\n<p> <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span>  reference\ndistribution distribution shift</p>\n<h2 id=\"your-language-model-is-secretly-a-reward-model\">Your Language\nModel Is Secretly a Reward Model</h2>\n<p>DPOlossreward</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(Z(x)\\)</span>\n</p>\n<p>\"Plackett-Luce/Bradley-Terryreward\nfunctionpreference distribution\"</p>\n<blockquote>\n<p>Under the Plackett-Luce preference framework, and in particular the\nBradleyTerry framework, two reward functions from the same equivalence\nclass induce the same preference distribution</p>\n</blockquote>\n<p>reward function <span class=\"math inline\">\\(r(x,y)\\)</span>\n <span class=\"math inline\">\\(r^{\\prime}(x,y)\\)</span> </p>\n<p><span class=\"math display\">\\[r&#39;(x,y)=r(x,y)+f(x)\\]</span></p>\n<p>reward function(equivalence class)</p>\n<p>prompt <span class=\"math inline\">\\(x\\)</span>  answer <span class=\"math inline\">\\(y_1,\\ldots,y_K\\)</span>ranking <span class=\"math inline\">\\(\\tau\\)</span>Plackett-Luce\nframeworkBradleyTerry</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\np_{r&#39;}(\\tau|y_1,\\ldots,y_K,x)&amp;\n=\\prod_{k=1}^K\\frac{\\exp(r&#39;(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r&#39;(x,y_{\\tau(j)}))}  \\\\\n&amp;=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)})+f(x))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)})+f(x))}\n\\\\\n&amp;=\\prod_{k=1}^K\\frac{\\exp(f(x))\\exp(r(x,y_{\\tau(k)}))}{\\exp(f(x))\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))}\n\\\\\n&amp;=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))}\n\\\\\n&amp;=p_r(\\tau|y_1,\\ldots,y_K,x)\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\beta\\log\nZ(x)\\)</span> reward\nfunctionpreference distribution</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}\\]</span></p>\n<p>reward\nfunctionRLoptimal policy</p>\n<p>DPOlossoptimal policy</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p>reward functionoptimal\npolicy<span class=\"math inline\">\\(r&#39;(x,y)=r(x,y)+f(x)\\)</span><span class=\"math inline\">\\(\\pi_r\\)</span>  <span class=\"math inline\">\\(\\pi_{r&#39;}\\)</span> optimal\npolicy</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\pi_{r^{\\prime}}(y|x)&amp;\n\\begin{aligned}&amp;=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r&#39;(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r&#39;(x,y)\\right)\\end{aligned}  \\\\\n&amp;=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right)\n\\\\\n&amp;\\begin{aligned}=\\frac{1}{\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\end{aligned}\n\\\\\n&amp;\\begin{aligned}&amp;=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\n\\\\\n&amp;=\\pi_r(y|x)\n\\end{aligned}\\]</span></p>\n<p>Plackett-LuceBradley-Terryreward\n<span class=\"math inline\">\\(\\pi(y\\mid x)\\)</span>  reference\nmodel <span class=\"math inline\">\\(\\pi_{ref}(y\\mid x)\\)</span>\n</p>\n<p><span class=\"math display\">\\[r(x,y)=\\beta\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}\\]</span></p>\n<p>reward model</p>\n<h2 id=\"\"></h2>\n<p><br>\n- <span class=\"math inline\">\\(\\beta=0.1\\)</span>TL;DR\nsummarization0.5<br>\n- batch size = 64<br>\n- RMSprop optimizer<br>\n- learning rate = 1e-6<br>\n- linearly warmup 0 to 1e-6 over 150 steps</p>\n<p>PPOSFTDPO</p>\n<p>DPO</p>\n<img src=\"/473f2b43/result_1.png\" class title=\"1\">\n<img src=\"/473f2b43/result_2.png\" class title=\"2\">\n<img src=\"/473f2b43/result_3.png\" class title=\"3\">\n<img src=\"/473f2b43/result_4.png\" class title=\"4\">\n<h1 id=\"\"></h1>\n<ul>\n<li>DPORLHF\nPPOreward<br>\n</li>\n<li>DPOPPO</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Direct Preference Optimization: Your Language Model is Secretly\na Reward Model https://arxiv.org/abs/2305.18290v2</p>\n"},{"title":"-ODPO","abbrlink":"da871ebe","date":"2024-05-30T07:23:05.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nDPO[-DPO](http://www.linsight.cn/473f2b43.html)  \n\nDPORLHFODPODPO with an offsetDPODPO  \n\n#   \n\n  \n\nmaximize the response log-likelihoodgap  \n\nmisalignmentmaximum likelihood  \n\n> Training with the maximum likelihood objective makes the model assign nonzero probability mass to all responses in SFT dataset, even those of lower quality.  \n\nRLHFRLrewardreward  \n\nrewardmodelingpointwise rewardpairwise preference  \n\npointwise rewardrewardpositivereward1negativereward0toxicity/classifier  \n\npairwise preference  \n\nRLHFDPO  \n\n# BradleyTerry model  \n\nDPO  \n\n$$\\begin{aligned}\n\\mathcal{L}^{\\mathrm{DPO}}(\\boldsymbol{\\theta})& =-\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\text{HF}}}\\left[\\log\\sigma\\Big(\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}-\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}\\Big)\\right]  \\\\\n&=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\thicksim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)\\right)\\right]\n\\end{aligned}$$  \n\n  \n\n$$\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y})=\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}|\\boldsymbol{x})}{\\pi_{\\mathrm{SFT}}(\\boldsymbol{y}|\\boldsymbol{x})}$$  \n\nestimated reward  \n\nDPOBradleyTerry modelBradleyTerry modelresponseresponse  \n\nresponse  \n\nODPOresponseoffset  \n\n# DPO with an Offset  \n\n $\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)$ Gumbel noise  \n\n$$\\tilde{r}_w\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),1)$$  \n\n$$\\tilde{r}_l\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l),1)$$  \n\n  \n\n$$p\\big(\\tilde{r}_w-\\tilde{r}_l>\\Delta_r\\big)=\\sigma(\\Delta_{\\hat{r}_\\theta}-\\Delta_r)$$  \n\nODPO  \n\n$$\\mathcal{L}^{\\mathrm{ODPO}}(\\boldsymbol{\\theta})=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma{\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)-\\Delta_r\\right)}\\right]$$  \n\npreferred responseestimated rewarddispreferred responseestimated rewardoffset  \n\noffset=0ODPODPO  \n\nODPOsoftmax margin loss/marginal losslossmarginpenalization  \n\nODPOoffsetresponseactual rewardincreasing scaling function  \n\n$$\\Delta_r=\\alpha\\mathbf{f}\\big(\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_l)\\big)$$  \n\n $\\alpha$   \n\n{% asset_img odpo_intro.png intro %}  \n\n#   \n\n  \n\n## sentiment control  \n\nsentiment controlpositiveresponse  \n\nGPT2-LargeIMDB datasetfinetuneSFTsentiment classifierrewardresponse  \n\n$$r_{negative}(\\boldsymbol{x},\\boldsymbol{y}) = 1-p(\\text{negative}\\mid\\cdot)$$  \n\n$$r_{positive}(\\boldsymbol{x},\\boldsymbol{y}) = 1+p(\\text{positive}\\mid\\cdot)$$  \n\nrewardpromptrewardresponse  \n\nDPOODPOoffset  \n\n$$\\Delta_r=\\log\\left(r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\right)$$  \n\n $\\alpha$ 1  \n\nrandom seedSFT2  \n\n $\\beta$ 14 $\\{0.1,0.2,\\ldots,1\\}\\cup\\{1,2,3,4,5\\}$   \n\n250007500,10000DPOODPO2314=84  \n\nsentimentSFTKL divergence  \n\n{% asset_img sentiment_control.png sentiment control %}  \n\nsentimentSFTODPODPO  \n\n## toxicity control  \n\ntoxicity controlsentiment controlresponse  \n\nGPT-neo-2.7b$\\beta$  $\\{0.05,0.1,0.2,0.3,0.4,0.5\\}$REALTOXICITYPROMPTS100000.3prompt  \n\n  \n\n{% asset_img toxicity_control.png toxicity control %}  \n\n8000 & 9000ODPO  \n\n## summarization  \n\nREDDIT TL;DRGPTJ-6B  \n\nDPOODPO100prompttemperatureGPT-4  \n\n{% asset_img summarization.png summarization %}  \n\nDPOODPOSFTtemperatureDPOODPOhuman-written  \n\n## scaling function  \n\noffsetrewardlog  \n\n$$\\Delta_r=\\log r(\\boldsymbol{y}_w)-\\log r(\\boldsymbol{y}_l)$$  \n\n$$\\begin{array}{rcl}\\Delta_r=r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\end{array}$$  \n\n5000sentiment control$\\beta \\in \\{0.1,0.2,\\ldots,0.9\\}\\cup\\{1,2,3,4,5\\}$  \n\n  \n\n{% asset_img scaling_function.png scaling function %}  \n\nlog scalingODPOKL divergence0.40.8rewardlog scalingKL divergencereward  \n\n##   \n\n7500sentiment control$\\beta=0.5$$\\alpha\\in\\{0.0,0.1,0.2,0.3,0.5,0.8,1.\\}$  \n\n{% asset_img alpha.png alpha %}  \n\n $\\alpha$ SFTreward  \n\n#   \n\nODPODPOoffset  \n\nODPOLLAMA  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n1Direct Preference Optimization with an Offset https://arxiv.org/pdf/2402.10571  ","source":"_posts/cs/nlp/2024/05/-ODPO.md","raw":"---\ntitle: -ODPO\nabbrlink: da871ebe\ndate: 2024-05-30 15:23:05\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \n  - SFT\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nDPO[-DPO](http://www.linsight.cn/473f2b43.html)  \n\nDPORLHFODPODPO with an offsetDPODPO  \n\n#   \n\n  \n\nmaximize the response log-likelihoodgap  \n\nmisalignmentmaximum likelihood  \n\n> Training with the maximum likelihood objective makes the model assign nonzero probability mass to all responses in SFT dataset, even those of lower quality.  \n\nRLHFRLrewardreward  \n\nrewardmodelingpointwise rewardpairwise preference  \n\npointwise rewardrewardpositivereward1negativereward0toxicity/classifier  \n\npairwise preference  \n\nRLHFDPO  \n\n# BradleyTerry model  \n\nDPO  \n\n$$\\begin{aligned}\n\\mathcal{L}^{\\mathrm{DPO}}(\\boldsymbol{\\theta})& =-\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\text{HF}}}\\left[\\log\\sigma\\Big(\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}-\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}\\Big)\\right]  \\\\\n&=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\thicksim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)\\right)\\right]\n\\end{aligned}$$  \n\n  \n\n$$\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y})=\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}|\\boldsymbol{x})}{\\pi_{\\mathrm{SFT}}(\\boldsymbol{y}|\\boldsymbol{x})}$$  \n\nestimated reward  \n\nDPOBradleyTerry modelBradleyTerry modelresponseresponse  \n\nresponse  \n\nODPOresponseoffset  \n\n# DPO with an Offset  \n\n $\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)$ Gumbel noise  \n\n$$\\tilde{r}_w\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),1)$$  \n\n$$\\tilde{r}_l\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l),1)$$  \n\n  \n\n$$p\\big(\\tilde{r}_w-\\tilde{r}_l>\\Delta_r\\big)=\\sigma(\\Delta_{\\hat{r}_\\theta}-\\Delta_r)$$  \n\nODPO  \n\n$$\\mathcal{L}^{\\mathrm{ODPO}}(\\boldsymbol{\\theta})=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma{\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)-\\Delta_r\\right)}\\right]$$  \n\npreferred responseestimated rewarddispreferred responseestimated rewardoffset  \n\noffset=0ODPODPO  \n\nODPOsoftmax margin loss/marginal losslossmarginpenalization  \n\nODPOoffsetresponseactual rewardincreasing scaling function  \n\n$$\\Delta_r=\\alpha\\mathbf{f}\\big(\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_l)\\big)$$  \n\n $\\alpha$   \n\n{% asset_img odpo_intro.png intro %}  \n\n#   \n\n  \n\n## sentiment control  \n\nsentiment controlpositiveresponse  \n\nGPT2-LargeIMDB datasetfinetuneSFTsentiment classifierrewardresponse  \n\n$$r_{negative}(\\boldsymbol{x},\\boldsymbol{y}) = 1-p(\\text{negative}\\mid\\cdot)$$  \n\n$$r_{positive}(\\boldsymbol{x},\\boldsymbol{y}) = 1+p(\\text{positive}\\mid\\cdot)$$  \n\nrewardpromptrewardresponse  \n\nDPOODPOoffset  \n\n$$\\Delta_r=\\log\\left(r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\right)$$  \n\n $\\alpha$ 1  \n\nrandom seedSFT2  \n\n $\\beta$ 14 $\\{0.1,0.2,\\ldots,1\\}\\cup\\{1,2,3,4,5\\}$   \n\n250007500,10000DPOODPO2314=84  \n\nsentimentSFTKL divergence  \n\n{% asset_img sentiment_control.png sentiment control %}  \n\nsentimentSFTODPODPO  \n\n## toxicity control  \n\ntoxicity controlsentiment controlresponse  \n\nGPT-neo-2.7b$\\beta$  $\\{0.05,0.1,0.2,0.3,0.4,0.5\\}$REALTOXICITYPROMPTS100000.3prompt  \n\n  \n\n{% asset_img toxicity_control.png toxicity control %}  \n\n8000 & 9000ODPO  \n\n## summarization  \n\nREDDIT TL;DRGPTJ-6B  \n\nDPOODPO100prompttemperatureGPT-4  \n\n{% asset_img summarization.png summarization %}  \n\nDPOODPOSFTtemperatureDPOODPOhuman-written  \n\n## scaling function  \n\noffsetrewardlog  \n\n$$\\Delta_r=\\log r(\\boldsymbol{y}_w)-\\log r(\\boldsymbol{y}_l)$$  \n\n$$\\begin{array}{rcl}\\Delta_r=r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\end{array}$$  \n\n5000sentiment control$\\beta \\in \\{0.1,0.2,\\ldots,0.9\\}\\cup\\{1,2,3,4,5\\}$  \n\n  \n\n{% asset_img scaling_function.png scaling function %}  \n\nlog scalingODPOKL divergence0.40.8rewardlog scalingKL divergencereward  \n\n##   \n\n7500sentiment control$\\beta=0.5$$\\alpha\\in\\{0.0,0.1,0.2,0.3,0.5,0.8,1.\\}$  \n\n{% asset_img alpha.png alpha %}  \n\n $\\alpha$ SFTreward  \n\n#   \n\nODPODPOoffset  \n\nODPOLLAMA  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n1Direct Preference Optimization with an Offset https://arxiv.org/pdf/2402.10571  ","slug":"cs/nlp/2024/05/-ODPO","published":1,"updated":"2024-05-31T12:26:56.345Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmfc0003am4kezrjgw6a","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>DPO<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a></p>\n<p>DPORLHFODPODPO\nwith an\noffsetDPODPO</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>maximize the response\nlog-likelihoodgap</p>\n<p>misalignmentmaximum\nlikelihood</p>\n<blockquote>\n<p>Training with the maximum likelihood objective makes the model assign\nnonzero probability mass to all responses in SFT dataset, even those of\nlower quality.</p>\n</blockquote>\n<p>RLHFRLrewardreward</p>\n<p>rewardmodelingpointwise rewardpairwise preference</p>\n<p>pointwise\nrewardrewardpositivereward1negativereward0toxicity/classifier</p>\n<p>pairwise\npreference</p>\n<p>RLHFDPO</p>\n<h1 id=\"bradleyterry-model\">BradleyTerry model</h1>\n<p>DPO</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}^{\\mathrm{DPO}}(\\boldsymbol{\\theta})&amp;\n=-\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\text{HF}}}\\left[\\log\\sigma\\Big(\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}-\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}\\Big)\\right]  \\\\\n&amp;=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\thicksim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)\\right)\\right]\n\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y})=\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}|\\boldsymbol{x})}{\\pi_{\\mathrm{SFT}}(\\boldsymbol{y}|\\boldsymbol{x})}\\]</span></p>\n<p>estimated reward</p>\n<p>DPOBradleyTerry\nmodelBradleyTerry\nmodelresponseresponse</p>\n<p>response</p>\n<p>ODPOresponseoffset</p>\n<h1 id=\"dpo-with-an-offset\">DPO with an Offset</h1>\n<p> <span class=\"math inline\">\\(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)\\)</span>\nGumbel noise</p>\n<p><span class=\"math display\">\\[\\tilde{r}_w\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),1)\\]</span></p>\n<p><span class=\"math display\">\\[\\tilde{r}_l\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l),1)\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[p\\big(\\tilde{r}_w-\\tilde{r}_l&gt;\\Delta_r\\big)=\\sigma(\\Delta_{\\hat{r}_\\theta}-\\Delta_r)\\]</span></p>\n<p>ODPO</p>\n<p><span class=\"math display\">\\[\\mathcal{L}^{\\mathrm{ODPO}}(\\boldsymbol{\\theta})=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma{\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)-\\Delta_r\\right)}\\right]\\]</span></p>\n<p>preferred responseestimated rewarddispreferred\nresponseestimated rewardoffset</p>\n<p>offset=0ODPODPO</p>\n<p>ODPOsoftmax margin loss/marginal\nlosslossmarginpenalization</p>\n<p>ODPOoffsetresponseactual rewardincreasing scaling\nfunction</p>\n<p><span class=\"math display\">\\[\\Delta_r=\\alpha\\mathbf{f}\\big(\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_l)\\big)\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<img src=\"/da871ebe/odpo_intro.png\" class title=\"intro\">\n<h1 id=\"\"></h1>\n<p></p>\n<h2 id=\"sentiment-control\">sentiment control</h2>\n<p>sentiment controlpositiveresponse</p>\n<p>GPT2-LargeIMDB\ndatasetfinetuneSFTsentiment\nclassifierrewardresponse</p>\n<p><span class=\"math display\">\\[r_{negative}(\\boldsymbol{x},\\boldsymbol{y}) =\n1-p(\\text{negative}\\mid\\cdot)\\]</span></p>\n<p><span class=\"math display\">\\[r_{positive}(\\boldsymbol{x},\\boldsymbol{y}) =\n1+p(\\text{positive}\\mid\\cdot)\\]</span></p>\n<p>rewardpromptrewardresponse</p>\n<p>DPOODPOoffset</p>\n<p><span class=\"math display\">\\[\\Delta_r=\\log\\left(r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\right)\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha\\)</span> 1</p>\n<p>random\nseedSFT2</p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span> 14\n<span class=\"math inline\">\\(\\{0.1,0.2,\\ldots,1\\}\\cup\\{1,2,3,4,5\\}\\)</span>\n</p>\n<p>250007500,10000DPOODPO2314=84</p>\n<p>sentimentSFTKL\ndivergence</p>\n<img src=\"/da871ebe/sentiment_control.png\" class title=\"sentiment control\">\n<p>sentimentSFTODPODPO</p>\n<h2 id=\"toxicity-control\">toxicity control</h2>\n<p>toxicity controlsentiment\ncontrolresponse</p>\n<p>GPT-neo-2.7b<span class=\"math inline\">\\(\\beta\\)</span>\n <span class=\"math inline\">\\(\\{0.05,0.1,0.2,0.3,0.4,0.5\\}\\)</span>REALTOXICITYPROMPTS100000.3prompt</p>\n<p></p>\n<img src=\"/da871ebe/toxicity_control.png\" class title=\"toxicity control\">\n<p>8000 &amp; 9000ODPO</p>\n<h2 id=\"summarization\">summarization</h2>\n<p>REDDIT TL;DRGPTJ-6B</p>\n<p>DPOODPO100prompttemperatureGPT-4</p>\n<img src=\"/da871ebe/summarization.png\" class title=\"summarization\">\n<p>DPOODPOSFTtemperatureDPOODPOhuman-written</p>\n<h2 id=\"scaling-function\">scaling function</h2>\n<p>offsetrewardlog</p>\n<p><span class=\"math display\">\\[\\Delta_r=\\log r(\\boldsymbol{y}_w)-\\log\nr(\\boldsymbol{y}_l)\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{array}{rcl}\\Delta_r=r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\end{array}\\]</span></p>\n<p>5000sentiment control<span class=\"math inline\">\\(\\beta\n\\in \\{0.1,0.2,\\ldots,0.9\\}\\cup\\{1,2,3,4,5\\}\\)</span></p>\n<p></p>\n<img src=\"/da871ebe/scaling_function.png\" class title=\"scaling function\">\n<p>log scalingODPOKL\ndivergence0.40.8rewardlog\nscalingKL divergencereward</p>\n<h2 id=\"\"></h2>\n<p>7500sentiment control<span class=\"math inline\">\\(\\beta=0.5\\)</span><span class=\"math inline\">\\(\\alpha\\in\\{0.0,0.1,0.2,0.3,0.5,0.8,1.\\}\\)</span></p>\n<img src=\"/da871ebe/alpha.png\" class title=\"alpha\">\n<p> <span class=\"math inline\">\\(\\alpha\\)</span>\nSFTreward</p>\n<h1 id=\"\"></h1>\n<p>ODPODPOoffset</p>\n<p>ODPOLLAMA</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Direct Preference Optimization with an Offset\nhttps://arxiv.org/pdf/2402.10571</p>\n","length":5653,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>DPO<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a></p>\n<p>DPORLHFODPODPO\nwith an\noffsetDPODPO</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>maximize the response\nlog-likelihoodgap</p>\n<p>misalignmentmaximum\nlikelihood</p>\n<blockquote>\n<p>Training with the maximum likelihood objective makes the model assign\nnonzero probability mass to all responses in SFT dataset, even those of\nlower quality.</p>\n</blockquote>\n<p>RLHFRLrewardreward</p>\n<p>rewardmodelingpointwise rewardpairwise preference</p>\n<p>pointwise\nrewardrewardpositivereward1negativereward0toxicity/classifier</p>\n<p>pairwise\npreference</p>\n<p>RLHFDPO</p>\n<h1 id=\"bradleyterry-model\">BradleyTerry model</h1>\n<p>DPO</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}^{\\mathrm{DPO}}(\\boldsymbol{\\theta})&amp;\n=-\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\text{HF}}}\\left[\\log\\sigma\\Big(\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}-\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}\\Big)\\right]  \\\\\n&amp;=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\thicksim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)\\right)\\right]\n\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y})=\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}|\\boldsymbol{x})}{\\pi_{\\mathrm{SFT}}(\\boldsymbol{y}|\\boldsymbol{x})}\\]</span></p>\n<p>estimated reward</p>\n<p>DPOBradleyTerry\nmodelBradleyTerry\nmodelresponseresponse</p>\n<p>response</p>\n<p>ODPOresponseoffset</p>\n<h1 id=\"dpo-with-an-offset\">DPO with an Offset</h1>\n<p> <span class=\"math inline\">\\(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)\\)</span>\nGumbel noise</p>\n<p><span class=\"math display\">\\[\\tilde{r}_w\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),1)\\]</span></p>\n<p><span class=\"math display\">\\[\\tilde{r}_l\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l),1)\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[p\\big(\\tilde{r}_w-\\tilde{r}_l&gt;\\Delta_r\\big)=\\sigma(\\Delta_{\\hat{r}_\\theta}-\\Delta_r)\\]</span></p>\n<p>ODPO</p>\n<p><span class=\"math display\">\\[\\mathcal{L}^{\\mathrm{ODPO}}(\\boldsymbol{\\theta})=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma{\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)-\\Delta_r\\right)}\\right]\\]</span></p>\n<p>preferred responseestimated rewarddispreferred\nresponseestimated rewardoffset</p>\n<p>offset=0ODPODPO</p>\n<p>ODPOsoftmax margin loss/marginal\nlosslossmarginpenalization</p>\n<p>ODPOoffsetresponseactual rewardincreasing scaling\nfunction</p>\n<p><span class=\"math display\">\\[\\Delta_r=\\alpha\\mathbf{f}\\big(\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_l)\\big)\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<img src=\"/da871ebe/odpo_intro.png\" class title=\"intro\">\n<h1 id=\"\"></h1>\n<p></p>\n<h2 id=\"sentiment-control\">sentiment control</h2>\n<p>sentiment controlpositiveresponse</p>\n<p>GPT2-LargeIMDB\ndatasetfinetuneSFTsentiment\nclassifierrewardresponse</p>\n<p><span class=\"math display\">\\[r_{negative}(\\boldsymbol{x},\\boldsymbol{y}) =\n1-p(\\text{negative}\\mid\\cdot)\\]</span></p>\n<p><span class=\"math display\">\\[r_{positive}(\\boldsymbol{x},\\boldsymbol{y}) =\n1+p(\\text{positive}\\mid\\cdot)\\]</span></p>\n<p>rewardpromptrewardresponse</p>\n<p>DPOODPOoffset</p>\n<p><span class=\"math display\">\\[\\Delta_r=\\log\\left(r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\right)\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha\\)</span> 1</p>\n<p>random\nseedSFT2</p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span> 14\n<span class=\"math inline\">\\(\\{0.1,0.2,\\ldots,1\\}\\cup\\{1,2,3,4,5\\}\\)</span>\n</p>\n<p>250007500,10000DPOODPO2314=84</p>\n<p>sentimentSFTKL\ndivergence</p>\n<img src=\"/da871ebe/sentiment_control.png\" class title=\"sentiment control\">\n<p>sentimentSFTODPODPO</p>\n<h2 id=\"toxicity-control\">toxicity control</h2>\n<p>toxicity controlsentiment\ncontrolresponse</p>\n<p>GPT-neo-2.7b<span class=\"math inline\">\\(\\beta\\)</span>\n <span class=\"math inline\">\\(\\{0.05,0.1,0.2,0.3,0.4,0.5\\}\\)</span>REALTOXICITYPROMPTS100000.3prompt</p>\n<p></p>\n<img src=\"/da871ebe/toxicity_control.png\" class title=\"toxicity control\">\n<p>8000 &amp; 9000ODPO</p>\n<h2 id=\"summarization\">summarization</h2>\n<p>REDDIT TL;DRGPTJ-6B</p>\n<p>DPOODPO100prompttemperatureGPT-4</p>\n<img src=\"/da871ebe/summarization.png\" class title=\"summarization\">\n<p>DPOODPOSFTtemperatureDPOODPOhuman-written</p>\n<h2 id=\"scaling-function\">scaling function</h2>\n<p>offsetrewardlog</p>\n<p><span class=\"math display\">\\[\\Delta_r=\\log r(\\boldsymbol{y}_w)-\\log\nr(\\boldsymbol{y}_l)\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{array}{rcl}\\Delta_r=r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\end{array}\\]</span></p>\n<p>5000sentiment control<span class=\"math inline\">\\(\\beta\n\\in \\{0.1,0.2,\\ldots,0.9\\}\\cup\\{1,2,3,4,5\\}\\)</span></p>\n<p></p>\n<img src=\"/da871ebe/scaling_function.png\" class title=\"scaling function\">\n<p>log scalingODPOKL\ndivergence0.40.8rewardlog\nscalingKL divergencereward</p>\n<h2 id=\"\"></h2>\n<p>7500sentiment control<span class=\"math inline\">\\(\\beta=0.5\\)</span><span class=\"math inline\">\\(\\alpha\\in\\{0.0,0.1,0.2,0.3,0.5,0.8,1.\\}\\)</span></p>\n<img src=\"/da871ebe/alpha.png\" class title=\"alpha\">\n<p> <span class=\"math inline\">\\(\\alpha\\)</span>\nSFTreward</p>\n<h1 id=\"\"></h1>\n<p>ODPODPOoffset</p>\n<p>ODPOLLAMA</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Direct Preference Optimization with an Offset\nhttps://arxiv.org/pdf/2402.10571</p>\n"},{"title":"-simPO","abbrlink":"280fa97a","date":"2024-05-31T14:09:23.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nDPOODPO[-DPO](http://www.linsight.cn/473f2b43.html)[-ODPO](http://www.linsight.cn/da871ebe.html)  \n\nsimPODPOsimPOreference modelsimPO  \n\n{% asset_img intro.png simPO %}  \n\n# DPO  \n\nDPODPOreward functionclosed-form expression  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\nBradley-Terry model  \n\n$$\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid x)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid x)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]$$  \n\nDPORLHFDPO  \n- reference model  \n- reward  \n\nresponsetokenlog likelihood  \n\n$$\\begin{aligned}p_\\theta(y\\mid x)=\\frac{1}{|y|}\\log\\pi_\\theta(y\\mid x)=\\frac{1}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid x,y_{<i})\\end{aligned}$$  \n\ngreedy decodingbeam searchtop-k samplingresponse/  \n\nDPOrewardreferenc modelDPO $r(x,y_w)>r(x,y_l)$  $p_\\theta(y_w\\mid x)>p_\\theta(y_l\\mid x)$  \n\nDPO $r(x,y_w)>r(x,y_l)$  $p_\\theta(y_w\\mid x)>p_\\theta(y_l\\mid x)$ 50%  \n\n{% asset_img contingency_table.png contingency table %}  \n\n  \n\nsimPO  \n\n{% asset_img simpo_contingency.png simPO contingency table %}  \n\n# simPO  \n\n##   \n\nreward  \n\n$$\\begin{aligned}r^*(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}\\end{aligned}$$  \n\nZ\n\n  \n\n$$\\begin{aligned}r_{\\text{SimPO}}(x,y)=\\frac{\\beta}{|y|}\\log\\pi_\\theta(y\\mid x)=\\frac{\\beta}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid x,y_{<i})\\end{aligned}$$  \n\n  \n\nrewardsimPOIPOODPOreward marginwinning responselosing responserewardreward margin  \n\n$$p(y_w\\succ y_l\\mid x)=\\sigma\\left(r(x,y_w)-r(x,y_l)-\\gamma\\right)$$  \n\nmarginmargin  \n\nsimPO  \n\n$$\\mathcal{L}_{\\text{SimPO}}(\\pi_\\theta)=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\frac{\\beta}{|y_w|}\\log\\pi_\\theta(y_w|x)-\\frac{\\beta}{|y_l|}\\log\\pi_\\theta(y_l|x)-\\gamma\\right)\\right]$$  \n\n## simPO  \n\nDPOsimPO  \n\n{% asset_img gradient.png  %}  \n\nDPOsimPO  \n- simPOreference modelpolicy modeldispreferred responserewardcase  \n- simPOlength-normalizedDisentangling length from quality in direct preference optimizationDPOtoken  \n\n#   \n\n##   \n\nLlama3-8BMistral-7Bbaseinstruct  \n\nbaseUltraChat-200kSFT UltraFeedbackpreference optimization  \n\ninstructIterative DPO alignmentSFTpreferenceUltraFeedbackprompttemperature=0.8SFT5responsePairRMLLM-Blender: Ensembling large language models with pairwise ranking and generative fusion5responsepreferred responsedispreferred response  \n\nLlama3-Base, Llama3-Instruct, Mistral-BaseMistral-Instruct  \n\npreference optimization  \n\n{% asset_img hyperparameters.png  %}  \n\n{% asset_img simpo_hyperparameters.png  %}  \n\nbatch size  \n\n  \n\n{% asset_img benchmark.png benchmark %}  \n\n##   \n\n  \n\n{% asset_img main_results.png  %}  \n\nLClength-controlledwin rate  \n\n  \n- MT-BenchFrom live data to high-quality benchmarks: The Arena-Hard pipeline  \n- instructbase  \n- AlpacaEval 2Arena-HardsimPOraw win ratelength-controlled win rate  \n\n##   \n\nsimPOlength normalizationmargin  \n\n{% asset_img ablation.png  %}  \n\nlength normalizationmargin  \n\n  \n\nsimPO  \n\nsimPOwin ratesimPO  \n\nwr  \n\n{% asset_img ln.png  %}  \n\nrewardrewardymargin  \n\nreward  \n\n{% asset_img ln_effect.png  %}  \n\nsimPOpositive reward marginwinning responsenegative reward difference  \n\nbcrewardresponse length  \n\nreward margin  \n\nreward accuracypolicy modelwinning responserewardlosing responsemarginreward accuracy  \n\n{% asset_img reward_accuracy.png reward accuracy %}  \n\nreward marginreward differencewinning responsewinning response  \n\n{% asset_img margin_dist.png  %}  \n\nmargin  \n\n## DPOsimPO  \n\n1. DPOrewardreference modellength biasDPOrewardsimPO  \n\n{% asset_img dpo_correlation.png correlation %}  \n\n2. simPODPOreward accuracysimPOreward  \n\n{% asset_img reward_accuracy_compare.png reward accuracy %}  \n\n#   \n\nsimPOpolicy modelsimPOreference model  \n\nLLAMAMistral  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n1KTO: Model Alignment as Prospect Theoretic Optimization https://arxiv.org/abs/2402.01306    \n","source":"_posts/cs/nlp/2024/05/-simPO.md","raw":"---\ntitle: -simPO\nabbrlink: 280fa97a\ndate: 2024-05-31 22:09:23\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \n  - SFT\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nDPOODPO[-DPO](http://www.linsight.cn/473f2b43.html)[-ODPO](http://www.linsight.cn/da871ebe.html)  \n\nsimPODPOsimPOreference modelsimPO  \n\n{% asset_img intro.png simPO %}  \n\n# DPO  \n\nDPODPOreward functionclosed-form expression  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\nBradley-Terry model  \n\n$$\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid x)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid x)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]$$  \n\nDPORLHFDPO  \n- reference model  \n- reward  \n\nresponsetokenlog likelihood  \n\n$$\\begin{aligned}p_\\theta(y\\mid x)=\\frac{1}{|y|}\\log\\pi_\\theta(y\\mid x)=\\frac{1}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid x,y_{<i})\\end{aligned}$$  \n\ngreedy decodingbeam searchtop-k samplingresponse/  \n\nDPOrewardreferenc modelDPO $r(x,y_w)>r(x,y_l)$  $p_\\theta(y_w\\mid x)>p_\\theta(y_l\\mid x)$  \n\nDPO $r(x,y_w)>r(x,y_l)$  $p_\\theta(y_w\\mid x)>p_\\theta(y_l\\mid x)$ 50%  \n\n{% asset_img contingency_table.png contingency table %}  \n\n  \n\nsimPO  \n\n{% asset_img simpo_contingency.png simPO contingency table %}  \n\n# simPO  \n\n##   \n\nreward  \n\n$$\\begin{aligned}r^*(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}\\end{aligned}$$  \n\nZ\n\n  \n\n$$\\begin{aligned}r_{\\text{SimPO}}(x,y)=\\frac{\\beta}{|y|}\\log\\pi_\\theta(y\\mid x)=\\frac{\\beta}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid x,y_{<i})\\end{aligned}$$  \n\n  \n\nrewardsimPOIPOODPOreward marginwinning responselosing responserewardreward margin  \n\n$$p(y_w\\succ y_l\\mid x)=\\sigma\\left(r(x,y_w)-r(x,y_l)-\\gamma\\right)$$  \n\nmarginmargin  \n\nsimPO  \n\n$$\\mathcal{L}_{\\text{SimPO}}(\\pi_\\theta)=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\frac{\\beta}{|y_w|}\\log\\pi_\\theta(y_w|x)-\\frac{\\beta}{|y_l|}\\log\\pi_\\theta(y_l|x)-\\gamma\\right)\\right]$$  \n\n## simPO  \n\nDPOsimPO  \n\n{% asset_img gradient.png  %}  \n\nDPOsimPO  \n- simPOreference modelpolicy modeldispreferred responserewardcase  \n- simPOlength-normalizedDisentangling length from quality in direct preference optimizationDPOtoken  \n\n#   \n\n##   \n\nLlama3-8BMistral-7Bbaseinstruct  \n\nbaseUltraChat-200kSFT UltraFeedbackpreference optimization  \n\ninstructIterative DPO alignmentSFTpreferenceUltraFeedbackprompttemperature=0.8SFT5responsePairRMLLM-Blender: Ensembling large language models with pairwise ranking and generative fusion5responsepreferred responsedispreferred response  \n\nLlama3-Base, Llama3-Instruct, Mistral-BaseMistral-Instruct  \n\npreference optimization  \n\n{% asset_img hyperparameters.png  %}  \n\n{% asset_img simpo_hyperparameters.png  %}  \n\nbatch size  \n\n  \n\n{% asset_img benchmark.png benchmark %}  \n\n##   \n\n  \n\n{% asset_img main_results.png  %}  \n\nLClength-controlledwin rate  \n\n  \n- MT-BenchFrom live data to high-quality benchmarks: The Arena-Hard pipeline  \n- instructbase  \n- AlpacaEval 2Arena-HardsimPOraw win ratelength-controlled win rate  \n\n##   \n\nsimPOlength normalizationmargin  \n\n{% asset_img ablation.png  %}  \n\nlength normalizationmargin  \n\n  \n\nsimPO  \n\nsimPOwin ratesimPO  \n\nwr  \n\n{% asset_img ln.png  %}  \n\nrewardrewardymargin  \n\nreward  \n\n{% asset_img ln_effect.png  %}  \n\nsimPOpositive reward marginwinning responsenegative reward difference  \n\nbcrewardresponse length  \n\nreward margin  \n\nreward accuracypolicy modelwinning responserewardlosing responsemarginreward accuracy  \n\n{% asset_img reward_accuracy.png reward accuracy %}  \n\nreward marginreward differencewinning responsewinning response  \n\n{% asset_img margin_dist.png  %}  \n\nmargin  \n\n## DPOsimPO  \n\n1. DPOrewardreference modellength biasDPOrewardsimPO  \n\n{% asset_img dpo_correlation.png correlation %}  \n\n2. simPODPOreward accuracysimPOreward  \n\n{% asset_img reward_accuracy_compare.png reward accuracy %}  \n\n#   \n\nsimPOpolicy modelsimPOreference model  \n\nLLAMAMistral  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n1KTO: Model Alignment as Prospect Theoretic Optimization https://arxiv.org/abs/2402.01306    \n","slug":"cs/nlp/2024/05/-simPO","published":1,"updated":"2024-06-01T16:46:06.697Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmfd0007am4kdqcmbpcw","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>DPOODPO<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a></p>\n<p>simPODPOsimPOreference\nmodelsimPO</p>\n<img src=\"/280fa97a/intro.png\" class title=\"simPO\">\n<h1 id=\"dpo\">DPO</h1>\n<p>DPODPOreward functionclosed-form expression</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p>Bradley-Terry model</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid\nx)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid\nx)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]\\]</span></p>\n<p>DPORLHFDPO<br>\n- reference model<br>\n-\nreward</p>\n<p>responsetokenlog\nlikelihood</p>\n<p><span class=\"math display\">\\[\\begin{aligned}p_\\theta(y\\mid\nx)=\\frac{1}{|y|}\\log\\pi_\\theta(y\\mid\nx)=\\frac{1}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid\nx,y_{&lt;i})\\end{aligned}\\]</span></p>\n<p>greedy\ndecodingbeam searchtop-k\nsamplingresponse/</p>\n<p>DPOrewardreferenc\nmodelDPO <span class=\"math inline\">\\(r(x,y_w)&gt;r(x,y_l)\\)</span>\n <span class=\"math inline\">\\(p_\\theta(y_w\\mid\nx)&gt;p_\\theta(y_l\\mid x)\\)</span></p>\n<p>DPO <span class=\"math inline\">\\(r(x,y_w)&gt;r(x,y_l)\\)</span>  <span class=\"math inline\">\\(p_\\theta(y_w\\mid x)&gt;p_\\theta(y_l\\mid\nx)\\)</span> 50%</p>\n<img src=\"/280fa97a/contingency_table.png\" class title=\"contingency table\">\n<p></p>\n<p>simPO</p>\n<img src=\"/280fa97a/simpo_contingency.png\" class title=\"simPO contingency table\">\n<h1 id=\"simpo\">simPO</h1>\n<h2 id=\"\"></h2>\n<p>reward</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r^*(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}\\end{aligned}\\]</span></p>\n<p>Z</p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}r_{\\text{SimPO}}(x,y)=\\frac{\\beta}{|y|}\\log\\pi_\\theta(y\\mid\nx)=\\frac{\\beta}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid\nx,y_{&lt;i})\\end{aligned}\\]</span></p>\n<p></p>\n<p>rewardsimPOIPOODPOreward\nmarginwinning responselosing\nresponserewardreward margin</p>\n<p><span class=\"math display\">\\[p(y_w\\succ y_l\\mid\nx)=\\sigma\\left(r(x,y_w)-r(x,y_l)-\\gamma\\right)\\]</span></p>\n<p>marginmargin</p>\n<p>simPO</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{SimPO}}(\\pi_\\theta)=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\frac{\\beta}{|y_w|}\\log\\pi_\\theta(y_w|x)-\\frac{\\beta}{|y_l|}\\log\\pi_\\theta(y_l|x)-\\gamma\\right)\\right]\\]</span></p>\n<h2 id=\"simpo\">simPO</h2>\n<p>DPOsimPO</p>\n<img src=\"/280fa97a/gradient.png\" class title=\"\">\n<p>DPOsimPO<br>\n- simPOreference modelpolicy\nmodeldispreferred\nresponserewardcase<br>\n- simPOlength-normalizedDisentangling length from\nquality in direct preference\noptimizationDPOtoken</p>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>Llama3-8BMistral-7Bbaseinstruct</p>\n<p>baseUltraChat-200kSFT\nUltraFeedbackpreference optimization</p>\n<p>instructIterative DPO\nalignmentSFTpreferenceUltraFeedbackprompttemperature=0.8SFT5responsePairRMLLM-Blender:\nEnsembling large language models with pairwise ranking and generative\nfusion5responsepreferred\nresponsedispreferred response</p>\n<p>Llama3-Base, Llama3-Instruct,\nMistral-BaseMistral-Instruct</p>\n<p>preference\noptimization</p>\n<img src=\"/280fa97a/hyperparameters.png\" class title=\"\">\n<img src=\"/280fa97a/simpo_hyperparameters.png\" class title=\"\">\n<p>batch size</p>\n<p></p>\n<img src=\"/280fa97a/benchmark.png\" class title=\"benchmark\">\n<h2 id=\"\"></h2>\n<p></p>\n<img src=\"/280fa97a/main_results.png\" class title=\"\">\n<p>LClength-controlledwin rate</p>\n<p><br>\n-\nMT-BenchFrom\nlive data to high-quality benchmarks: The Arena-Hard\npipeline<br>\n-\ninstructbase<br>\n- AlpacaEval 2Arena-HardsimPOraw win ratelength-controlled\nwin rate</p>\n<h2 id=\"\"></h2>\n<p>simPOlength\nnormalizationmargin</p>\n<img src=\"/280fa97a/ablation.png\" class title=\"\">\n<p>length normalizationmargin</p>\n<p></p>\n<p>simPO</p>\n<p>simPOwin\nratesimPO</p>\n<p>wr</p>\n<img src=\"/280fa97a/ln.png\" class title=\"\">\n<p>rewardrewardymargin</p>\n<p>reward</p>\n<img src=\"/280fa97a/ln_effect.png\" class title=\"\">\n<p>simPOpositive\nreward marginwinning\nresponsenegative reward\ndifference</p>\n<p>bcrewardresponse\nlength</p>\n<p>reward margin</p>\n<p>reward accuracypolicy modelwinning\nresponserewardlosing\nresponsemarginreward\naccuracy</p>\n<img src=\"/280fa97a/reward_accuracy.png\" class title=\"reward accuracy\">\n<p>reward marginreward differencewinning\nresponsewinning\nresponse</p>\n<img src=\"/280fa97a/margin_dist.png\" class title=\"\">\n<p>margin</p>\n<h2 id=\"dposimpo\">DPOsimPO</h2>\n<ol type=\"1\">\n<li>DPOrewardreference\nmodellength\nbiasDPOrewardsimPO</li>\n</ol>\n<img src=\"/280fa97a/dpo_correlation.png\" class title=\"correlation\">\n<ol start=\"2\" type=\"1\">\n<li>simPODPOreward\naccuracysimPOreward</li>\n</ol>\n<img src=\"/280fa97a/reward_accuracy_compare.png\" class title=\"reward accuracy\">\n<h1 id=\"\"></h1>\n<p>simPOpolicy\nmodelsimPOreference\nmodel</p>\n<p>LLAMAMistral</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1KTO: Model Alignment as Prospect Theoretic Optimization\nhttps://arxiv.org/abs/2402.01306</p>\n","length":5186,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>DPOODPO<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a></p>\n<p>simPODPOsimPOreference\nmodelsimPO</p>\n<img src=\"/280fa97a/intro.png\" class title=\"simPO\">\n<h1 id=\"dpo\">DPO</h1>\n<p>DPODPOreward functionclosed-form expression</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p>Bradley-Terry model</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid\nx)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid\nx)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]\\]</span></p>\n<p>DPORLHFDPO<br>\n- reference model<br>\n-\nreward</p>\n<p>responsetokenlog\nlikelihood</p>\n<p><span class=\"math display\">\\[\\begin{aligned}p_\\theta(y\\mid\nx)=\\frac{1}{|y|}\\log\\pi_\\theta(y\\mid\nx)=\\frac{1}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid\nx,y_{&lt;i})\\end{aligned}\\]</span></p>\n<p>greedy\ndecodingbeam searchtop-k\nsamplingresponse/</p>\n<p>DPOrewardreferenc\nmodelDPO <span class=\"math inline\">\\(r(x,y_w)&gt;r(x,y_l)\\)</span>\n <span class=\"math inline\">\\(p_\\theta(y_w\\mid\nx)&gt;p_\\theta(y_l\\mid x)\\)</span></p>\n<p>DPO <span class=\"math inline\">\\(r(x,y_w)&gt;r(x,y_l)\\)</span>  <span class=\"math inline\">\\(p_\\theta(y_w\\mid x)&gt;p_\\theta(y_l\\mid\nx)\\)</span> 50%</p>\n<img src=\"/280fa97a/contingency_table.png\" class title=\"contingency table\">\n<p></p>\n<p>simPO</p>\n<img src=\"/280fa97a/simpo_contingency.png\" class title=\"simPO contingency table\">\n<h1 id=\"simpo\">simPO</h1>\n<h2 id=\"\"></h2>\n<p>reward</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r^*(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}\\end{aligned}\\]</span></p>\n<p>Z</p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}r_{\\text{SimPO}}(x,y)=\\frac{\\beta}{|y|}\\log\\pi_\\theta(y\\mid\nx)=\\frac{\\beta}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid\nx,y_{&lt;i})\\end{aligned}\\]</span></p>\n<p></p>\n<p>rewardsimPOIPOODPOreward\nmarginwinning responselosing\nresponserewardreward margin</p>\n<p><span class=\"math display\">\\[p(y_w\\succ y_l\\mid\nx)=\\sigma\\left(r(x,y_w)-r(x,y_l)-\\gamma\\right)\\]</span></p>\n<p>marginmargin</p>\n<p>simPO</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{SimPO}}(\\pi_\\theta)=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\frac{\\beta}{|y_w|}\\log\\pi_\\theta(y_w|x)-\\frac{\\beta}{|y_l|}\\log\\pi_\\theta(y_l|x)-\\gamma\\right)\\right]\\]</span></p>\n<h2 id=\"simpo\">simPO</h2>\n<p>DPOsimPO</p>\n<img src=\"/280fa97a/gradient.png\" class title=\"\">\n<p>DPOsimPO<br>\n- simPOreference modelpolicy\nmodeldispreferred\nresponserewardcase<br>\n- simPOlength-normalizedDisentangling length from\nquality in direct preference\noptimizationDPOtoken</p>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>Llama3-8BMistral-7Bbaseinstruct</p>\n<p>baseUltraChat-200kSFT\nUltraFeedbackpreference optimization</p>\n<p>instructIterative DPO\nalignmentSFTpreferenceUltraFeedbackprompttemperature=0.8SFT5responsePairRMLLM-Blender:\nEnsembling large language models with pairwise ranking and generative\nfusion5responsepreferred\nresponsedispreferred response</p>\n<p>Llama3-Base, Llama3-Instruct,\nMistral-BaseMistral-Instruct</p>\n<p>preference\noptimization</p>\n<img src=\"/280fa97a/hyperparameters.png\" class title=\"\">\n<img src=\"/280fa97a/simpo_hyperparameters.png\" class title=\"\">\n<p>batch size</p>\n<p></p>\n<img src=\"/280fa97a/benchmark.png\" class title=\"benchmark\">\n<h2 id=\"\"></h2>\n<p></p>\n<img src=\"/280fa97a/main_results.png\" class title=\"\">\n<p>LClength-controlledwin rate</p>\n<p><br>\n-\nMT-BenchFrom\nlive data to high-quality benchmarks: The Arena-Hard\npipeline<br>\n-\ninstructbase<br>\n- AlpacaEval 2Arena-HardsimPOraw win ratelength-controlled\nwin rate</p>\n<h2 id=\"\"></h2>\n<p>simPOlength\nnormalizationmargin</p>\n<img src=\"/280fa97a/ablation.png\" class title=\"\">\n<p>length normalizationmargin</p>\n<p></p>\n<p>simPO</p>\n<p>simPOwin\nratesimPO</p>\n<p>wr</p>\n<img src=\"/280fa97a/ln.png\" class title=\"\">\n<p>rewardrewardymargin</p>\n<p>reward</p>\n<img src=\"/280fa97a/ln_effect.png\" class title=\"\">\n<p>simPOpositive\nreward marginwinning\nresponsenegative reward\ndifference</p>\n<p>bcrewardresponse\nlength</p>\n<p>reward margin</p>\n<p>reward accuracypolicy modelwinning\nresponserewardlosing\nresponsemarginreward\naccuracy</p>\n<img src=\"/280fa97a/reward_accuracy.png\" class title=\"reward accuracy\">\n<p>reward marginreward differencewinning\nresponsewinning\nresponse</p>\n<img src=\"/280fa97a/margin_dist.png\" class title=\"\">\n<p>margin</p>\n<h2 id=\"dposimpo\">DPOsimPO</h2>\n<ol type=\"1\">\n<li>DPOrewardreference\nmodellength\nbiasDPOrewardsimPO</li>\n</ol>\n<img src=\"/280fa97a/dpo_correlation.png\" class title=\"correlation\">\n<ol start=\"2\" type=\"1\">\n<li>simPODPOreward\naccuracysimPOreward</li>\n</ol>\n<img src=\"/280fa97a/reward_accuracy_compare.png\" class title=\"reward accuracy\">\n<h1 id=\"\"></h1>\n<p>simPOpolicy\nmodelsimPOreference\nmodel</p>\n<p>LLAMAMistral</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1KTO: Model Alignment as Prospect Theoretic Optimization\nhttps://arxiv.org/abs/2402.01306</p>\n"},{"title":"-","abbrlink":"f5c015c","date":"2024-05-13T08:47:13.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n  \n\nspeculative decoding  \n\n#   \n\n202211GoogleFast Inference from Transformers via Speculative DecodingDeepMind2023Accelerating Large Language Model Decoding with Speculative SamplingideaGoogleDeepMind  \n\nspeculative decoding  \n- HintonDistilling the Knowledge in a Neural NetworkKnowledge Distillation: A SurveytransformerTinyBERT: Distilling BERT for Natural Language UnderstandingDistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter  \n- Quantized Neural Networks: Training Neural Networks with Low Precision Weights and ActivationsLLM.int8(): 8-bit Matrix Multiplication for Transformers at ScaleZeroquant: Efficient and affordable post-training quantization for large-scale transformersint8int4  \n- Sparse is Enough in Scaling TransformersKVMQAFast Transformer Decoding: One Write-Head is All You NeedGQAGQA: Training Generalized Multi-Query Transformer Models from Multi-Head CheckpointsDeepSeek-V2MLAPrimer: Searching for Efficient Transformers for Language Modeling  \n\n  \n\nstepstep  \n- Dynamic Neural Networks: A Survey  \n- Adaptive Attention Span in Transformers  \n- Consistent Accelerated Inference via Confident Adaptive Transformers  \n- Why should we add early exits to neural networks?  \n- Controlling Computation versus Quality for Neural Sequence Models  \n- The Right Tool for the Job: Matching Model and Instance Complexities  \n- Depth-Adaptive Transformer  \n-   \n\nMoE  \n\nTraining compute-optimal large language modelsscaling law  \n\n  \n\nBlockwise Parallel Decoding for Deep Autoregressive ModelsLossless Acceleration for Seq2seq Generation with Aggressive Decoding  \n\nspeculative decoding2x-3x  \n\n# speculative decoding  \n\ntokentokentokentokentoken  \n\n```\n      \n      \n      EOS\n```\n\ntoken  \n\n```\nstep1\nstep2\nstep3\nstep4EOS\n```\n\n   EOStokendraft token  \n\n```\n      \n      \n      EOS\n```\n\ntoken4  \n\n   EOSdraft tokentoken  \n\n```\n      \n      \n      EOS\n```\n\ntoken  \n\n  token > 0  \n\nspeculative decodingapproximation modeldraft modeltarget model  \n\n  \n\n{% asset_img fi_example.png  %}  \n\napproximation modeltarget modeltokentokentoken  \n\ntarget938token  \n\n  \n\n$M_p$ target model $M_q$ approximation modelprefix  \n\n $M_q$  $\\gamma$ draft token $M_p$  $\\gamma$ draft tokentoken $M_p$ tokentoken  \n\nGoogle  \n\n{% asset_img fi_sd_algo.png  %}  \n\nDeepMind  \n\ntoken $n$ draft token $M_p$ token $n+1$ tokenapproximation model $\\gamma$ draft token $\\gamma+1$ token1target  \n\n  \n- speculative samplingtarget modelapproximation modeltokendraft token  \n-  $\\gamma$   \n- approximation modelapproximation model  \n\nDeepMindGoogleDeepMindGoogle  \n\n{% asset_img acce_alog.png DeepMind %}  \n\n $(.)_+$  $(f(x))_+=\\frac{\\max(0,f(x))}{\\sum_x\\max(0,f(x))}$   \n\n# speculative sampling  \n\ntarget model  \n\ntransformerargmaxtop-klogits  \n\n $p(x)$ target model $M_p$  $q(x)$ approximation model $M_q$   \n\n $x\\sim q(x)$ $q(x)\\leq p(x)$ $x$ $1-\\frac{p(x)}{q(x)}$  $x$ $p'(x)=norm(max(0,p(x)-q(x)))$  $x$   \n\n $norm(max(0,p(x)-q(x)))=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}$  \n\n $q(x)$  $p(x)$target model  \n\napproximation model $\\tilde{x}\\sim q$ $X$  $\\mathbb{P}(X=x)=p(x)$  \n\n $X=x$ $\\tilde{x}=x$  $\\tilde{x}$  $\\tilde{x}$  $\\tilde{x}=x$   \n\n$$\\mathbb{P}(X=x)\\\\=\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\textit{ accepted}|\\tilde{x}=x)\\\\+\\mathbb{P}(\\tilde{x}\\textit{ rejected})\\mathbb{P}(X=x|\\tilde{x}\\textit{ rejected})$$  \n\n  \n\n$$\n\\begin{aligned}\n&\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\text{ }d|\\tilde{x}=x)\\\\=&q(x)\\min\\left(1,\\frac{p(x)}{q(x)}\\right)\\\\=&\\min\\left(q(x),p(x)\\right)\n\\end{aligned}\n$$  \n\n  \n\n$$\\begin{gathered}\n\\mathbb{P}(\\tilde{x}\\textit{ rejected})=1-\\mathbb{P}(\\tilde{x}\\textit{ accepted}) \\\\\n=1-\\sum_{x^{\\prime}}\\mathbb{P}(X=x^{\\prime},\\tilde{x}\\text{ }d) \\\\\n=1-\\sum_{x'}\\min(q(x'),p(x')) \\\\\n=\\sum_{x'}\\max(0,p(x')-q(x')) \\\\\n\\end{gathered}$$  \n\n1ba+b1a $\\sum_{x'}\\max(0,p(x')-q(x'))$  \n\n{% asset_img formula.png  %}  \n\n  \n\n$$\\mathbb{P}(X=x|\\tilde{x}\\text{ rejected})=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}$$  \n\n  \n\n$$\\mathbb{P}(\\tilde{x}\\text{ rejected})\\mathbb{P}(X=x|\\tilde{x}\\text{ rejected})=\\max(0,p(x)-q(x))$$  \n\n  \n\n$$\\mathbb{P}(X=x)\\\\=\\min(q(x),p(x))+\\max(0,p(x)-q(x))\\\\=p(x)$$  \n\ntarget model  \n\n# approximation model  \n\napproximation model $x\\sim q(x)$ target model $\\beta$acceptance rate  \n\n $E(\\beta)$ approximation modeltarget model  \n\n$E(\\beta)$ tokentoken  \n\n $\\alpha=E(\\beta)$ $\\beta$ i.i.d.tokencapped geometric variable  \n\n$$E(\\#\\textit{ generated tokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$$  \n\n $\\gamma$   \n\n{% asset_img fi_expected_token_num.png  %}  \n\n $\\alpha$   \n\n $M_p$  $M_q$ divergence $D_{LK}$  \n\n$$\\begin{aligned}D_{LK}(p,q)=\\sum_x|p(x)-M(x)|=\\sum_x|q(x)-M(x)|\\end{aligned}$$  \n\n $M(x)=\\frac{p(x)+q(x)}2$  \n\n  \n\n$$\n\\begin{aligned}\n&\\sum_x|p(x)-M(x)|\\\\=&\\sum_x\\frac{|p-q|}{2}\\\\=&1-\\sum_x\\frac{p+q-|p-q|}2\\\\=&1-\\sum_x\\min(p(x),q(x))\n\\end{aligned}\n$$  \n\n  \n\n$$D_{LK}(p,q)=1-\\sum_x\\min(p(x),q(x))$$  \n\n$D_{LK}(p,q)$ $M_p$  $M_q$  $D_{LK}(p,q)=0$ $p=q$ $D_{LK}(p,q)=1$ $p$  $q$   \n\n $\\beta$   \n\n$$\n\\begin{aligned}\n\\beta=&E_{x\\sim q(x)}\\begin{cases}1&q(x)\\leq p(x)\\\\\\frac{p(x)}{q(x)}&q(x)>p(x)\\end{cases}\\\\\n=&E_{x\\thicksim q(x)}\\min(1,\\frac{p(x)}{q(x)})\\\\\n=&\\sum_x\\min(p(x),q(x))\\\\\n=&1-D_{LK}(p,q)\n\\end{aligned}\n$$  \n\n  \n\n$$\\alpha=E(\\beta)=1-E(D_{LK}(p,q))=E(\\min(p,q))$$\n\napproximation modeltarget model $\\alpha$   \n\n{% asset_img fi_alpha.png alpha %}  \n\n#   \n\ncost coefficient $c$ $M_q$   $M_p$   \n\n $\\alpha$ $c$  $c$ 0.05  \n\n $M_p$  $T$ $Tc\\gamma+T$  \n\ntoken $E(\\#\\textit{ generated tokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$ token $\\frac{(c\\gamma+1)(1-\\alpha)}{1-\\alpha^{\\gamma+1}}T$improvement factor  \n\n$$\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma c+1)}$$  \n\n $\\alpha>c$ $\\gamma$improvement factor $\\frac{1+\\alpha}{1+c}$$\\gamma=1$  \n\n#   \n\n$M_p$  $\\gamma+1$ tokentokentoken  \n\n $\\hat{c}$  $M_q$  $M_p$ tokenarithmetic operations$\\hat{T}$  $M_p$ tokenarithmetic operations  \n\n $\\hat{T}\\hat{c}\\gamma+\\hat{T}(\\gamma+1)$token $\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$ token $\\hat{T}\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}$  \n\n$\\alpha$ $\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}$   \n\nKV cache\n\n# $\\gamma$   \n\n $\\alpha$  $c$ $\\gamma$ walltime improvement factor $\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma c+1)}$  \n\n $\\alpha$  $c$  $\\gamma$   \n\n{% asset_img fi_choose_gamma.png gamma %}  \n\ntradeoff $\\gamma$   \n\n{% asset_img fi_speed_and_op_table.png  %}  \n\n{% asset_img fi_speed_and_op.png  %}  \n\n{% asset_img fi_walltime.png walltime %}  \n\n$\\beta$  $\\beta$  $\\gamma$  \n\n# approximation model  \n\napproximation modelapproximation modeltarget model  \n\nn-gramapproximation model $\\alpha$   \n\ncopy tokenapproximation model $\\alpha$   \n\napproximation modelBlockwise parallel decoding for deep autoregressive models  \n\n#   \n\nT5approximation modelT5-XXL3+    \n\n{% asset_img fi_t5_result.png T5 %}  \n\n $\\alpha$   \n\n{% asset_img fi_alpha.png alpha %}  \n\ntarget modelapproximation model0.50.9 $\\alpha$ Targmax $\\alpha$   \n\n#   \n\n- 2~3  \n- n-gram  \n- target modelapproximation model  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n# Reference  \n\n1Fast Inference from Transformers via Speculative Decoding https://arxiv.org/abs/2211.17192  \n2Accelerating Large Language Model Decoding with Speculative Sampling https://arxiv.org/abs/2302.01318  \n","source":"_posts/cs/nlp/2024/05/-.md","raw":"---\ntitle: -\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: f5c015c\ndate: 2024-05-13 16:47:13\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n  \n\nspeculative decoding  \n\n#   \n\n202211GoogleFast Inference from Transformers via Speculative DecodingDeepMind2023Accelerating Large Language Model Decoding with Speculative SamplingideaGoogleDeepMind  \n\nspeculative decoding  \n- HintonDistilling the Knowledge in a Neural NetworkKnowledge Distillation: A SurveytransformerTinyBERT: Distilling BERT for Natural Language UnderstandingDistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter  \n- Quantized Neural Networks: Training Neural Networks with Low Precision Weights and ActivationsLLM.int8(): 8-bit Matrix Multiplication for Transformers at ScaleZeroquant: Efficient and affordable post-training quantization for large-scale transformersint8int4  \n- Sparse is Enough in Scaling TransformersKVMQAFast Transformer Decoding: One Write-Head is All You NeedGQAGQA: Training Generalized Multi-Query Transformer Models from Multi-Head CheckpointsDeepSeek-V2MLAPrimer: Searching for Efficient Transformers for Language Modeling  \n\n  \n\nstepstep  \n- Dynamic Neural Networks: A Survey  \n- Adaptive Attention Span in Transformers  \n- Consistent Accelerated Inference via Confident Adaptive Transformers  \n- Why should we add early exits to neural networks?  \n- Controlling Computation versus Quality for Neural Sequence Models  \n- The Right Tool for the Job: Matching Model and Instance Complexities  \n- Depth-Adaptive Transformer  \n-   \n\nMoE  \n\nTraining compute-optimal large language modelsscaling law  \n\n  \n\nBlockwise Parallel Decoding for Deep Autoregressive ModelsLossless Acceleration for Seq2seq Generation with Aggressive Decoding  \n\nspeculative decoding2x-3x  \n\n# speculative decoding  \n\ntokentokentokentokentoken  \n\n```\n      \n      \n      EOS\n```\n\ntoken  \n\n```\nstep1\nstep2\nstep3\nstep4EOS\n```\n\n   EOStokendraft token  \n\n```\n      \n      \n      EOS\n```\n\ntoken4  \n\n   EOSdraft tokentoken  \n\n```\n      \n      \n      EOS\n```\n\ntoken  \n\n  token > 0  \n\nspeculative decodingapproximation modeldraft modeltarget model  \n\n  \n\n{% asset_img fi_example.png  %}  \n\napproximation modeltarget modeltokentokentoken  \n\ntarget938token  \n\n  \n\n$M_p$ target model $M_q$ approximation modelprefix  \n\n $M_q$  $\\gamma$ draft token $M_p$  $\\gamma$ draft tokentoken $M_p$ tokentoken  \n\nGoogle  \n\n{% asset_img fi_sd_algo.png  %}  \n\nDeepMind  \n\ntoken $n$ draft token $M_p$ token $n+1$ tokenapproximation model $\\gamma$ draft token $\\gamma+1$ token1target  \n\n  \n- speculative samplingtarget modelapproximation modeltokendraft token  \n-  $\\gamma$   \n- approximation modelapproximation model  \n\nDeepMindGoogleDeepMindGoogle  \n\n{% asset_img acce_alog.png DeepMind %}  \n\n $(.)_+$  $(f(x))_+=\\frac{\\max(0,f(x))}{\\sum_x\\max(0,f(x))}$   \n\n# speculative sampling  \n\ntarget model  \n\ntransformerargmaxtop-klogits  \n\n $p(x)$ target model $M_p$  $q(x)$ approximation model $M_q$   \n\n $x\\sim q(x)$ $q(x)\\leq p(x)$ $x$ $1-\\frac{p(x)}{q(x)}$  $x$ $p'(x)=norm(max(0,p(x)-q(x)))$  $x$   \n\n $norm(max(0,p(x)-q(x)))=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}$  \n\n $q(x)$  $p(x)$target model  \n\napproximation model $\\tilde{x}\\sim q$ $X$  $\\mathbb{P}(X=x)=p(x)$  \n\n $X=x$ $\\tilde{x}=x$  $\\tilde{x}$  $\\tilde{x}$  $\\tilde{x}=x$   \n\n$$\\mathbb{P}(X=x)\\\\=\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\textit{ accepted}|\\tilde{x}=x)\\\\+\\mathbb{P}(\\tilde{x}\\textit{ rejected})\\mathbb{P}(X=x|\\tilde{x}\\textit{ rejected})$$  \n\n  \n\n$$\n\\begin{aligned}\n&\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\text{ }d|\\tilde{x}=x)\\\\=&q(x)\\min\\left(1,\\frac{p(x)}{q(x)}\\right)\\\\=&\\min\\left(q(x),p(x)\\right)\n\\end{aligned}\n$$  \n\n  \n\n$$\\begin{gathered}\n\\mathbb{P}(\\tilde{x}\\textit{ rejected})=1-\\mathbb{P}(\\tilde{x}\\textit{ accepted}) \\\\\n=1-\\sum_{x^{\\prime}}\\mathbb{P}(X=x^{\\prime},\\tilde{x}\\text{ }d) \\\\\n=1-\\sum_{x'}\\min(q(x'),p(x')) \\\\\n=\\sum_{x'}\\max(0,p(x')-q(x')) \\\\\n\\end{gathered}$$  \n\n1ba+b1a $\\sum_{x'}\\max(0,p(x')-q(x'))$  \n\n{% asset_img formula.png  %}  \n\n  \n\n$$\\mathbb{P}(X=x|\\tilde{x}\\text{ rejected})=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}$$  \n\n  \n\n$$\\mathbb{P}(\\tilde{x}\\text{ rejected})\\mathbb{P}(X=x|\\tilde{x}\\text{ rejected})=\\max(0,p(x)-q(x))$$  \n\n  \n\n$$\\mathbb{P}(X=x)\\\\=\\min(q(x),p(x))+\\max(0,p(x)-q(x))\\\\=p(x)$$  \n\ntarget model  \n\n# approximation model  \n\napproximation model $x\\sim q(x)$ target model $\\beta$acceptance rate  \n\n $E(\\beta)$ approximation modeltarget model  \n\n$E(\\beta)$ tokentoken  \n\n $\\alpha=E(\\beta)$ $\\beta$ i.i.d.tokencapped geometric variable  \n\n$$E(\\#\\textit{ generated tokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$$  \n\n $\\gamma$   \n\n{% asset_img fi_expected_token_num.png  %}  \n\n $\\alpha$   \n\n $M_p$  $M_q$ divergence $D_{LK}$  \n\n$$\\begin{aligned}D_{LK}(p,q)=\\sum_x|p(x)-M(x)|=\\sum_x|q(x)-M(x)|\\end{aligned}$$  \n\n $M(x)=\\frac{p(x)+q(x)}2$  \n\n  \n\n$$\n\\begin{aligned}\n&\\sum_x|p(x)-M(x)|\\\\=&\\sum_x\\frac{|p-q|}{2}\\\\=&1-\\sum_x\\frac{p+q-|p-q|}2\\\\=&1-\\sum_x\\min(p(x),q(x))\n\\end{aligned}\n$$  \n\n  \n\n$$D_{LK}(p,q)=1-\\sum_x\\min(p(x),q(x))$$  \n\n$D_{LK}(p,q)$ $M_p$  $M_q$  $D_{LK}(p,q)=0$ $p=q$ $D_{LK}(p,q)=1$ $p$  $q$   \n\n $\\beta$   \n\n$$\n\\begin{aligned}\n\\beta=&E_{x\\sim q(x)}\\begin{cases}1&q(x)\\leq p(x)\\\\\\frac{p(x)}{q(x)}&q(x)>p(x)\\end{cases}\\\\\n=&E_{x\\thicksim q(x)}\\min(1,\\frac{p(x)}{q(x)})\\\\\n=&\\sum_x\\min(p(x),q(x))\\\\\n=&1-D_{LK}(p,q)\n\\end{aligned}\n$$  \n\n  \n\n$$\\alpha=E(\\beta)=1-E(D_{LK}(p,q))=E(\\min(p,q))$$\n\napproximation modeltarget model $\\alpha$   \n\n{% asset_img fi_alpha.png alpha %}  \n\n#   \n\ncost coefficient $c$ $M_q$   $M_p$   \n\n $\\alpha$ $c$  $c$ 0.05  \n\n $M_p$  $T$ $Tc\\gamma+T$  \n\ntoken $E(\\#\\textit{ generated tokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$ token $\\frac{(c\\gamma+1)(1-\\alpha)}{1-\\alpha^{\\gamma+1}}T$improvement factor  \n\n$$\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma c+1)}$$  \n\n $\\alpha>c$ $\\gamma$improvement factor $\\frac{1+\\alpha}{1+c}$$\\gamma=1$  \n\n#   \n\n$M_p$  $\\gamma+1$ tokentokentoken  \n\n $\\hat{c}$  $M_q$  $M_p$ tokenarithmetic operations$\\hat{T}$  $M_p$ tokenarithmetic operations  \n\n $\\hat{T}\\hat{c}\\gamma+\\hat{T}(\\gamma+1)$token $\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$ token $\\hat{T}\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}$  \n\n$\\alpha$ $\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}$   \n\nKV cache\n\n# $\\gamma$   \n\n $\\alpha$  $c$ $\\gamma$ walltime improvement factor $\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma c+1)}$  \n\n $\\alpha$  $c$  $\\gamma$   \n\n{% asset_img fi_choose_gamma.png gamma %}  \n\ntradeoff $\\gamma$   \n\n{% asset_img fi_speed_and_op_table.png  %}  \n\n{% asset_img fi_speed_and_op.png  %}  \n\n{% asset_img fi_walltime.png walltime %}  \n\n$\\beta$  $\\beta$  $\\gamma$  \n\n# approximation model  \n\napproximation modelapproximation modeltarget model  \n\nn-gramapproximation model $\\alpha$   \n\ncopy tokenapproximation model $\\alpha$   \n\napproximation modelBlockwise parallel decoding for deep autoregressive models  \n\n#   \n\nT5approximation modelT5-XXL3+    \n\n{% asset_img fi_t5_result.png T5 %}  \n\n $\\alpha$   \n\n{% asset_img fi_alpha.png alpha %}  \n\ntarget modelapproximation model0.50.9 $\\alpha$ Targmax $\\alpha$   \n\n#   \n\n- 2~3  \n- n-gram  \n- target modelapproximation model  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n# Reference  \n\n1Fast Inference from Transformers via Speculative Decoding https://arxiv.org/abs/2211.17192  \n2Accelerating Large Language Model Decoding with Speculative Sampling https://arxiv.org/abs/2302.01318  \n","slug":"cs/nlp/2024/05/-","published":1,"updated":"2024-05-25T03:38:10.437Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmfe0008am4k2kd9fnjj","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p></p>\n<p>speculative\ndecoding</p>\n<h1 id=\"\"></h1>\n<p>202211GoogleFast Inference from Transformers via\nSpeculative\nDecodingDeepMind2023Accelerating\nLarge Language Model Decoding with Speculative\nSamplingideaGoogleDeepMind</p>\n<p>speculative\ndecoding<br>\n- HintonDistilling the Knowledge in a Neural\nNetworkKnowledge\nDistillation: A\nSurveytransformerTinyBERT:\nDistilling BERT for Natural Language UnderstandingDistilBERT, a\ndistilled version of BERT: smaller, faster, cheaper and\nlighter<br>\n- Quantized Neural Networks: Training Neural Networks with\nLow Precision Weights and ActivationsLLM.int8(): 8-bit Matrix\nMultiplication for Transformers at ScaleZeroquant: Efficient and\naffordable post-training quantization for large-scale\ntransformersint8int4<br>\n- Sparse is Enough in Scaling\nTransformersKVMQAFast Transformer Decoding: One\nWrite-Head is All You NeedGQAGQA: Training Generalized\nMulti-Query Transformer Models from Multi-Head\nCheckpointsDeepSeek-V2MLAPrimer:\nSearching for Efficient Transformers for Language Modeling</p>\n<p></p>\n<p>stepstep<br>\n- Dynamic Neural Networks: A Survey<br>\n- Adaptive Attention Span in Transformers<br>\n- Consistent Accelerated Inference via Confident Adaptive\nTransformers<br>\n- Why should we add early exits to neural networks?<br>\n- Controlling Computation versus Quality for Neural Sequence\nModels<br>\n- The Right Tool for the Job: Matching Model and Instance\nComplexities<br>\n- Depth-Adaptive Transformer<br>\n- </p>\n<p>MoE</p>\n<p>Training compute-optimal large language modelsscaling\nlaw</p>\n<p></p>\n<p>Blockwise Parallel Decoding\nfor Deep Autoregressive ModelsLossless Acceleration for Seq2seq\nGeneration with Aggressive Decoding</p>\n<p>speculative\ndecoding2x-3x</p>\n<h1 id=\"speculative-decoding\">speculative decoding</h1>\n<p>tokentokentokentokentoken</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">      </span><br><span class=\"line\">      EOS</span><br></pre></td></tr></table></figure>\n<p>token</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">step1</span><br><span class=\"line\">step2</span><br><span class=\"line\">step3</span><br><span class=\"line\">step4EOS</span><br></pre></td></tr></table></figure>\n<p>  \nEOStokendraft\ntoken</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">      </span><br><span class=\"line\">      EOS</span><br></pre></td></tr></table></figure>\n<p>token4</p>\n<p>\n  EOSdraft\ntokentoken</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">      </span><br><span class=\"line\">      EOS</span><br></pre></td></tr></table></figure>\n<p>token</p>\n<p>\n\ntoken\n&gt; 0</p>\n<p>speculative\ndecodingapproximation\nmodeldraft modeltarget model</p>\n<p></p>\n<img src=\"/f5c015c/fi_example.png\" class title=\"\">\n<p>approximation modeltarget\nmodeltokentokentoken</p>\n<p>target938token</p>\n<p></p>\n<p><span class=\"math inline\">\\(M_p\\)</span> target model <span class=\"math inline\">\\(M_q\\)</span> approximation\nmodelprefix</p>\n<p> <span class=\"math inline\">\\(M_q\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span> draft token <span class=\"math inline\">\\(M_p\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span> draft\ntokentoken\n<span class=\"math inline\">\\(M_p\\)</span>\ntokentoken</p>\n<p>Google</p>\n<img src=\"/f5c015c/fi_sd_algo.png\" class title=\"\">\n<p>DeepMind</p>\n<p>token <span class=\"math inline\">\\(n\\)</span> draft token <span class=\"math inline\">\\(M_p\\)</span>\ntoken <span class=\"math inline\">\\(n+1\\)</span> tokenapproximation\nmodel <span class=\"math inline\">\\(\\gamma\\)</span> draft\ntoken <span class=\"math inline\">\\(\\gamma+1\\)</span>\ntoken1target</p>\n<p><br>\n- speculative samplingtarget modelapproximation\nmodeltokendraft\ntoken<br>\n-  <span class=\"math inline\">\\(\\gamma\\)</span> <br>\n- approximation modelapproximation\nmodel</p>\n<p>DeepMindGoogleDeepMindGoogle</p>\n<img src=\"/f5c015c/acce_alog.png\" class title=\"DeepMind\">\n<p> <span class=\"math inline\">\\((.)_+\\)</span>  <span class=\"math inline\">\\((f(x))_+=\\frac{\\max(0,f(x))}{\\sum_x\\max(0,f(x))}\\)</span>\n</p>\n<h1 id=\"speculative-sampling\">speculative sampling</h1>\n<p>target\nmodel</p>\n<p>transformerargmaxtop-klogits</p>\n<p> <span class=\"math inline\">\\(p(x)\\)</span> target model <span class=\"math inline\">\\(M_p\\)</span>  <span class=\"math inline\">\\(q(x)\\)</span> approximation model <span class=\"math inline\">\\(M_q\\)</span> </p>\n<p> <span class=\"math inline\">\\(x\\sim\nq(x)\\)</span> <span class=\"math inline\">\\(q(x)\\leq\np(x)\\)</span> <span class=\"math inline\">\\(x\\)</span>\n<span class=\"math inline\">\\(1-\\frac{p(x)}{q(x)}\\)</span> \n<span class=\"math inline\">\\(x\\)</span> <span class=\"math inline\">\\(p&#39;(x)=norm(max(0,p(x)-q(x)))\\)</span>\n <span class=\"math inline\">\\(x\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(norm(max(0,p(x)-q(x)))=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(q(x)\\)</span> \n<span class=\"math inline\">\\(p(x)\\)</span>target\nmodel</p>\n<p>approximation model <span class=\"math inline\">\\(\\tilde{x}\\sim q\\)</span> <span class=\"math inline\">\\(X\\)</span>  <span class=\"math inline\">\\(\\mathbb{P}(X=x)=p(x)\\)</span></p>\n<p> <span class=\"math inline\">\\(X=x\\)</span> <span class=\"math inline\">\\(\\tilde{x}=x\\)</span>  <span class=\"math inline\">\\(\\tilde{x}\\)</span>  <span class=\"math inline\">\\(\\tilde{x}\\)</span>  <span class=\"math inline\">\\(\\tilde{x}=x\\)</span> </p>\n<p><span class=\"math display\">\\[\\mathbb{P}(X=x)\\\\=\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\textit{\naccepted}|\\tilde{x}=x)\\\\+\\mathbb{P}(\\tilde{x}\\textit{\nrejected})\\mathbb{P}(X=x|\\tilde{x}\\textit{ rejected})\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\text{\n}d|\\tilde{x}=x)\\\\=&amp;q(x)\\min\\left(1,\\frac{p(x)}{q(x)}\\right)\\\\=&amp;\\min\\left(q(x),p(x)\\right)\n\\end{aligned}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{gathered}\n\\mathbb{P}(\\tilde{x}\\textit{ rejected})=1-\\mathbb{P}(\\tilde{x}\\textit{\naccepted}) \\\\\n=1-\\sum_{x^{\\prime}}\\mathbb{P}(X=x^{\\prime},\\tilde{x}\\text{ }d)\n\\\\\n=1-\\sum_{x&#39;}\\min(q(x&#39;),p(x&#39;)) \\\\\n=\\sum_{x&#39;}\\max(0,p(x&#39;)-q(x&#39;)) \\\\\n\\end{gathered}\\]</span></p>\n<p>1ba+b1a\n<span class=\"math inline\">\\(\\sum_{x&#39;}\\max(0,p(x&#39;)-q(x&#39;))\\)</span></p>\n<img src=\"/f5c015c/formula.png\" class title=\"\">\n<p></p>\n<p><span class=\"math display\">\\[\\mathbb{P}(X=x|\\tilde{x}\\text{\nrejected})=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\mathbb{P}(\\tilde{x}\\text{\nrejected})\\mathbb{P}(X=x|\\tilde{x}\\text{\nrejected})=\\max(0,p(x)-q(x))\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\mathbb{P}(X=x)\\\\=\\min(q(x),p(x))+\\max(0,p(x)-q(x))\\\\=p(x)\\]</span></p>\n<p>target\nmodel</p>\n<h1 id=\"approximation-model\">approximation model</h1>\n<p>approximation model <span class=\"math inline\">\\(x\\sim\nq(x)\\)</span> target model <span class=\"math inline\">\\(\\beta\\)</span>acceptance\nrate</p>\n<p> <span class=\"math inline\">\\(E(\\beta)\\)</span>\napproximation modeltarget model</p>\n<p><span class=\"math inline\">\\(E(\\beta)\\)</span>\ntokentoken</p>\n<p> <span class=\"math inline\">\\(\\alpha=E(\\beta)\\)</span>\n<span class=\"math inline\">\\(\\beta\\)</span>\ni.i.d.tokencapped\ngeometric variable</p>\n<p><span class=\"math display\">\\[E(\\#\\textit{ generated\ntokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\gamma\\)</span> </p>\n<img src=\"/f5c015c/fi_expected_token_num.png\" class title=\"\">\n<p> <span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<p> <span class=\"math inline\">\\(M_p\\)</span>  <span class=\"math inline\">\\(M_q\\)</span> divergence <span class=\"math inline\">\\(D_{LK}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}D_{LK}(p,q)=\\sum_x|p(x)-M(x)|=\\sum_x|q(x)-M(x)|\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(M(x)=\\frac{p(x)+q(x)}2\\)</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\sum_x|p(x)-M(x)|\\\\=&amp;\\sum_x\\frac{|p-q|}{2}\\\\=&amp;1-\\sum_x\\frac{p+q-|p-q|}2\\\\=&amp;1-\\sum_x\\min(p(x),q(x))\n\\end{aligned}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[D_{LK}(p,q)=1-\\sum_x\\min(p(x),q(x))\\]</span></p>\n<p><span class=\"math inline\">\\(D_{LK}(p,q)\\)</span> <span class=\"math inline\">\\(M_p\\)</span>  <span class=\"math inline\">\\(M_q\\)</span>  <span class=\"math inline\">\\(D_{LK}(p,q)=0\\)</span> <span class=\"math inline\">\\(p=q\\)</span> <span class=\"math inline\">\\(D_{LK}(p,q)=1\\)</span> <span class=\"math inline\">\\(p\\)</span>  <span class=\"math inline\">\\(q\\)</span> </p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n\\beta=&amp;E_{x\\sim q(x)}\\begin{cases}1&amp;q(x)\\leq\np(x)\\\\\\frac{p(x)}{q(x)}&amp;q(x)&gt;p(x)\\end{cases}\\\\\n=&amp;E_{x\\thicksim q(x)}\\min(1,\\frac{p(x)}{q(x)})\\\\\n=&amp;\\sum_x\\min(p(x),q(x))\\\\\n=&amp;1-D_{LK}(p,q)\n\\end{aligned}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\alpha=E(\\beta)=1-E(D_{LK}(p,q))=E(\\min(p,q))\\]</span></p>\n<p>approximation modeltarget model <span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<img src=\"/f5c015c/fi_alpha.png\" class title=\"alpha\">\n<h1 id=\"\"></h1>\n<p>cost coefficient <span class=\"math inline\">\\(c\\)</span>\n<span class=\"math inline\">\\(M_q\\)</span>   <span class=\"math inline\">\\(M_p\\)</span> </p>\n<p> <span class=\"math inline\">\\(\\alpha\\)</span>\n<span class=\"math inline\">\\(c\\)</span>\n <span class=\"math inline\">\\(c\\)</span> 0.05</p>\n<p> <span class=\"math inline\">\\(M_p\\)</span> \n<span class=\"math inline\">\\(T\\)</span> <span class=\"math inline\">\\(Tc\\gamma+T\\)</span></p>\n<p>token <span class=\"math inline\">\\(E(\\#\\textit{ generated\ntokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}\\)</span>\ntoken <span class=\"math inline\">\\(\\frac{(c\\gamma+1)(1-\\alpha)}{1-\\alpha^{\\gamma+1}}T\\)</span>improvement\nfactor</p>\n<p><span class=\"math display\">\\[\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma\nc+1)}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha&gt;c\\)</span>\n<span class=\"math inline\">\\(\\gamma\\)</span>improvement\nfactor <span class=\"math inline\">\\(\\frac{1+\\alpha}{1+c}\\)</span><span class=\"math inline\">\\(\\gamma=1\\)</span></p>\n<h1 id=\"\"></h1>\n<p><span class=\"math inline\">\\(M_p\\)</span>  <span class=\"math inline\">\\(\\gamma+1\\)</span>\ntokentokentoken</p>\n<p> <span class=\"math inline\">\\(\\hat{c}\\)</span>  <span class=\"math inline\">\\(M_q\\)</span>  <span class=\"math inline\">\\(M_p\\)</span> tokenarithmetic\noperations<span class=\"math inline\">\\(\\hat{T}\\)</span>  <span class=\"math inline\">\\(M_p\\)</span> tokenarithmetic\noperations</p>\n<p> <span class=\"math inline\">\\(\\hat{T}\\hat{c}\\gamma+\\hat{T}(\\gamma+1)\\)</span>token\n<span class=\"math inline\">\\(\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}\\)</span>\ntoken <span class=\"math inline\">\\(\\hat{T}\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}\\)</span></p>\n<p><span class=\"math inline\">\\(\\alpha\\)</span> <span class=\"math inline\">\\(\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}\\)</span>\n</p>\n<p>KV cache</p>\n<h1 id=\"gamma-\"><span class=\"math inline\">\\(\\gamma\\)</span>\n</h1>\n<p> <span class=\"math inline\">\\(\\alpha\\)</span>  <span class=\"math inline\">\\(c\\)</span> <span class=\"math inline\">\\(\\gamma\\)</span> walltime improvement\nfactor <span class=\"math inline\">\\(\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma\nc+1)}\\)</span></p>\n<p> <span class=\"math inline\">\\(\\alpha\\)</span>  <span class=\"math inline\">\\(c\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span> </p>\n<img src=\"/f5c015c/fi_choose_gamma.png\" class title=\"gamma\">\n<p>tradeoff <span class=\"math inline\">\\(\\gamma\\)</span>\n</p>\n<img src=\"/f5c015c/fi_speed_and_op_table.png\" class title=\"\">\n<img src=\"/f5c015c/fi_speed_and_op.png\" class title=\"\">\n<img src=\"/f5c015c/fi_walltime.png\" class title=\"walltime\">\n<p><span class=\"math inline\">\\(\\beta\\)</span>\n <span class=\"math inline\">\\(\\beta\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span></p>\n<h1 id=\"approximation-model\">approximation model</h1>\n<p>approximation\nmodelapproximation modeltarget\nmodel</p>\n<p>n-gramapproximation\nmodel <span class=\"math inline\">\\(\\alpha\\)</span>\n</p>\n<p>copy\ntokenapproximation model <span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<p>approximation modelBlockwise parallel decoding for\ndeep autoregressive models</p>\n<h1 id=\"\"></h1>\n<p>T5approximation\nmodelT5-XXL3+</p>\n<img src=\"/f5c015c/fi_t5_result.png\" class title=\"T5\">\n<p> <span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<img src=\"/f5c015c/fi_alpha.png\" class title=\"alpha\">\n<p>target modelapproximation\nmodel0.50.9 <span class=\"math inline\">\\(\\alpha\\)</span>\nTargmax\n<span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<h1 id=\"\"></h1>\n<ul>\n<li>2~3<br>\n</li>\n<li>n-gram<br>\n</li>\n<li>target modelapproximation\nmodel</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Fast Inference from Transformers via Speculative Decoding\nhttps://arxiv.org/abs/2211.17192<br>\n2Accelerating Large Language Model Decoding with Speculative\nSampling https://arxiv.org/abs/2302.01318</p>\n","length":10078,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p></p>\n<p>speculative\ndecoding</p>\n<h1 id=\"\"></h1>\n<p>202211GoogleFast Inference from Transformers via\nSpeculative\nDecodingDeepMind2023Accelerating\nLarge Language Model Decoding with Speculative\nSamplingideaGoogleDeepMind</p>\n<p>speculative\ndecoding<br>\n- HintonDistilling the Knowledge in a Neural\nNetworkKnowledge\nDistillation: A\nSurveytransformerTinyBERT:\nDistilling BERT for Natural Language UnderstandingDistilBERT, a\ndistilled version of BERT: smaller, faster, cheaper and\nlighter<br>\n- Quantized Neural Networks: Training Neural Networks with\nLow Precision Weights and ActivationsLLM.int8(): 8-bit Matrix\nMultiplication for Transformers at ScaleZeroquant: Efficient and\naffordable post-training quantization for large-scale\ntransformersint8int4<br>\n- Sparse is Enough in Scaling\nTransformersKVMQAFast Transformer Decoding: One\nWrite-Head is All You NeedGQAGQA: Training Generalized\nMulti-Query Transformer Models from Multi-Head\nCheckpointsDeepSeek-V2MLAPrimer:\nSearching for Efficient Transformers for Language Modeling</p>\n<p></p>\n<p>stepstep<br>\n- Dynamic Neural Networks: A Survey<br>\n- Adaptive Attention Span in Transformers<br>\n- Consistent Accelerated Inference via Confident Adaptive\nTransformers<br>\n- Why should we add early exits to neural networks?<br>\n- Controlling Computation versus Quality for Neural Sequence\nModels<br>\n- The Right Tool for the Job: Matching Model and Instance\nComplexities<br>\n- Depth-Adaptive Transformer<br>\n- </p>\n<p>MoE</p>\n<p>Training compute-optimal large language modelsscaling\nlaw</p>\n<p></p>\n<p>Blockwise Parallel Decoding\nfor Deep Autoregressive ModelsLossless Acceleration for Seq2seq\nGeneration with Aggressive Decoding</p>\n<p>speculative\ndecoding2x-3x</p>\n<h1 id=\"speculative-decoding\">speculative decoding</h1>\n<p>tokentokentokentokentoken</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">      </span><br><span class=\"line\">      EOS</span><br></pre></td></tr></table></figure>\n<p>token</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">step1</span><br><span class=\"line\">step2</span><br><span class=\"line\">step3</span><br><span class=\"line\">step4EOS</span><br></pre></td></tr></table></figure>\n<p>  \nEOStokendraft\ntoken</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">      </span><br><span class=\"line\">      EOS</span><br></pre></td></tr></table></figure>\n<p>token4</p>\n<p>\n  EOSdraft\ntokentoken</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">      </span><br><span class=\"line\">      EOS</span><br></pre></td></tr></table></figure>\n<p>token</p>\n<p>\n\ntoken\n&gt; 0</p>\n<p>speculative\ndecodingapproximation\nmodeldraft modeltarget model</p>\n<p></p>\n<img src=\"/f5c015c/fi_example.png\" class title=\"\">\n<p>approximation modeltarget\nmodeltokentokentoken</p>\n<p>target938token</p>\n<p></p>\n<p><span class=\"math inline\">\\(M_p\\)</span> target model <span class=\"math inline\">\\(M_q\\)</span> approximation\nmodelprefix</p>\n<p> <span class=\"math inline\">\\(M_q\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span> draft token <span class=\"math inline\">\\(M_p\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span> draft\ntokentoken\n<span class=\"math inline\">\\(M_p\\)</span>\ntokentoken</p>\n<p>Google</p>\n<img src=\"/f5c015c/fi_sd_algo.png\" class title=\"\">\n<p>DeepMind</p>\n<p>token <span class=\"math inline\">\\(n\\)</span> draft token <span class=\"math inline\">\\(M_p\\)</span>\ntoken <span class=\"math inline\">\\(n+1\\)</span> tokenapproximation\nmodel <span class=\"math inline\">\\(\\gamma\\)</span> draft\ntoken <span class=\"math inline\">\\(\\gamma+1\\)</span>\ntoken1target</p>\n<p><br>\n- speculative samplingtarget modelapproximation\nmodeltokendraft\ntoken<br>\n-  <span class=\"math inline\">\\(\\gamma\\)</span> <br>\n- approximation modelapproximation\nmodel</p>\n<p>DeepMindGoogleDeepMindGoogle</p>\n<img src=\"/f5c015c/acce_alog.png\" class title=\"DeepMind\">\n<p> <span class=\"math inline\">\\((.)_+\\)</span>  <span class=\"math inline\">\\((f(x))_+=\\frac{\\max(0,f(x))}{\\sum_x\\max(0,f(x))}\\)</span>\n</p>\n<h1 id=\"speculative-sampling\">speculative sampling</h1>\n<p>target\nmodel</p>\n<p>transformerargmaxtop-klogits</p>\n<p> <span class=\"math inline\">\\(p(x)\\)</span> target model <span class=\"math inline\">\\(M_p\\)</span>  <span class=\"math inline\">\\(q(x)\\)</span> approximation model <span class=\"math inline\">\\(M_q\\)</span> </p>\n<p> <span class=\"math inline\">\\(x\\sim\nq(x)\\)</span> <span class=\"math inline\">\\(q(x)\\leq\np(x)\\)</span> <span class=\"math inline\">\\(x\\)</span>\n<span class=\"math inline\">\\(1-\\frac{p(x)}{q(x)}\\)</span> \n<span class=\"math inline\">\\(x\\)</span> <span class=\"math inline\">\\(p&#39;(x)=norm(max(0,p(x)-q(x)))\\)</span>\n <span class=\"math inline\">\\(x\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(norm(max(0,p(x)-q(x)))=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(q(x)\\)</span> \n<span class=\"math inline\">\\(p(x)\\)</span>target\nmodel</p>\n<p>approximation model <span class=\"math inline\">\\(\\tilde{x}\\sim q\\)</span> <span class=\"math inline\">\\(X\\)</span>  <span class=\"math inline\">\\(\\mathbb{P}(X=x)=p(x)\\)</span></p>\n<p> <span class=\"math inline\">\\(X=x\\)</span> <span class=\"math inline\">\\(\\tilde{x}=x\\)</span>  <span class=\"math inline\">\\(\\tilde{x}\\)</span>  <span class=\"math inline\">\\(\\tilde{x}\\)</span>  <span class=\"math inline\">\\(\\tilde{x}=x\\)</span> </p>\n<p><span class=\"math display\">\\[\\mathbb{P}(X=x)\\\\=\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\textit{\naccepted}|\\tilde{x}=x)\\\\+\\mathbb{P}(\\tilde{x}\\textit{\nrejected})\\mathbb{P}(X=x|\\tilde{x}\\textit{ rejected})\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\text{\n}d|\\tilde{x}=x)\\\\=&amp;q(x)\\min\\left(1,\\frac{p(x)}{q(x)}\\right)\\\\=&amp;\\min\\left(q(x),p(x)\\right)\n\\end{aligned}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{gathered}\n\\mathbb{P}(\\tilde{x}\\textit{ rejected})=1-\\mathbb{P}(\\tilde{x}\\textit{\naccepted}) \\\\\n=1-\\sum_{x^{\\prime}}\\mathbb{P}(X=x^{\\prime},\\tilde{x}\\text{ }d)\n\\\\\n=1-\\sum_{x&#39;}\\min(q(x&#39;),p(x&#39;)) \\\\\n=\\sum_{x&#39;}\\max(0,p(x&#39;)-q(x&#39;)) \\\\\n\\end{gathered}\\]</span></p>\n<p>1ba+b1a\n<span class=\"math inline\">\\(\\sum_{x&#39;}\\max(0,p(x&#39;)-q(x&#39;))\\)</span></p>\n<img src=\"/f5c015c/formula.png\" class title=\"\">\n<p></p>\n<p><span class=\"math display\">\\[\\mathbb{P}(X=x|\\tilde{x}\\text{\nrejected})=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\mathbb{P}(\\tilde{x}\\text{\nrejected})\\mathbb{P}(X=x|\\tilde{x}\\text{\nrejected})=\\max(0,p(x)-q(x))\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\mathbb{P}(X=x)\\\\=\\min(q(x),p(x))+\\max(0,p(x)-q(x))\\\\=p(x)\\]</span></p>\n<p>target\nmodel</p>\n<h1 id=\"approximation-model\">approximation model</h1>\n<p>approximation model <span class=\"math inline\">\\(x\\sim\nq(x)\\)</span> target model <span class=\"math inline\">\\(\\beta\\)</span>acceptance\nrate</p>\n<p> <span class=\"math inline\">\\(E(\\beta)\\)</span>\napproximation modeltarget model</p>\n<p><span class=\"math inline\">\\(E(\\beta)\\)</span>\ntokentoken</p>\n<p> <span class=\"math inline\">\\(\\alpha=E(\\beta)\\)</span>\n<span class=\"math inline\">\\(\\beta\\)</span>\ni.i.d.tokencapped\ngeometric variable</p>\n<p><span class=\"math display\">\\[E(\\#\\textit{ generated\ntokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\gamma\\)</span> </p>\n<img src=\"/f5c015c/fi_expected_token_num.png\" class title=\"\">\n<p> <span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<p> <span class=\"math inline\">\\(M_p\\)</span>  <span class=\"math inline\">\\(M_q\\)</span> divergence <span class=\"math inline\">\\(D_{LK}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}D_{LK}(p,q)=\\sum_x|p(x)-M(x)|=\\sum_x|q(x)-M(x)|\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(M(x)=\\frac{p(x)+q(x)}2\\)</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\sum_x|p(x)-M(x)|\\\\=&amp;\\sum_x\\frac{|p-q|}{2}\\\\=&amp;1-\\sum_x\\frac{p+q-|p-q|}2\\\\=&amp;1-\\sum_x\\min(p(x),q(x))\n\\end{aligned}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[D_{LK}(p,q)=1-\\sum_x\\min(p(x),q(x))\\]</span></p>\n<p><span class=\"math inline\">\\(D_{LK}(p,q)\\)</span> <span class=\"math inline\">\\(M_p\\)</span>  <span class=\"math inline\">\\(M_q\\)</span>  <span class=\"math inline\">\\(D_{LK}(p,q)=0\\)</span> <span class=\"math inline\">\\(p=q\\)</span> <span class=\"math inline\">\\(D_{LK}(p,q)=1\\)</span> <span class=\"math inline\">\\(p\\)</span>  <span class=\"math inline\">\\(q\\)</span> </p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n\\beta=&amp;E_{x\\sim q(x)}\\begin{cases}1&amp;q(x)\\leq\np(x)\\\\\\frac{p(x)}{q(x)}&amp;q(x)&gt;p(x)\\end{cases}\\\\\n=&amp;E_{x\\thicksim q(x)}\\min(1,\\frac{p(x)}{q(x)})\\\\\n=&amp;\\sum_x\\min(p(x),q(x))\\\\\n=&amp;1-D_{LK}(p,q)\n\\end{aligned}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\alpha=E(\\beta)=1-E(D_{LK}(p,q))=E(\\min(p,q))\\]</span></p>\n<p>approximation modeltarget model <span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<img src=\"/f5c015c/fi_alpha.png\" class title=\"alpha\">\n<h1 id=\"\"></h1>\n<p>cost coefficient <span class=\"math inline\">\\(c\\)</span>\n<span class=\"math inline\">\\(M_q\\)</span>   <span class=\"math inline\">\\(M_p\\)</span> </p>\n<p> <span class=\"math inline\">\\(\\alpha\\)</span>\n<span class=\"math inline\">\\(c\\)</span>\n <span class=\"math inline\">\\(c\\)</span> 0.05</p>\n<p> <span class=\"math inline\">\\(M_p\\)</span> \n<span class=\"math inline\">\\(T\\)</span> <span class=\"math inline\">\\(Tc\\gamma+T\\)</span></p>\n<p>token <span class=\"math inline\">\\(E(\\#\\textit{ generated\ntokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}\\)</span>\ntoken <span class=\"math inline\">\\(\\frac{(c\\gamma+1)(1-\\alpha)}{1-\\alpha^{\\gamma+1}}T\\)</span>improvement\nfactor</p>\n<p><span class=\"math display\">\\[\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma\nc+1)}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha&gt;c\\)</span>\n<span class=\"math inline\">\\(\\gamma\\)</span>improvement\nfactor <span class=\"math inline\">\\(\\frac{1+\\alpha}{1+c}\\)</span><span class=\"math inline\">\\(\\gamma=1\\)</span></p>\n<h1 id=\"\"></h1>\n<p><span class=\"math inline\">\\(M_p\\)</span>  <span class=\"math inline\">\\(\\gamma+1\\)</span>\ntokentokentoken</p>\n<p> <span class=\"math inline\">\\(\\hat{c}\\)</span>  <span class=\"math inline\">\\(M_q\\)</span>  <span class=\"math inline\">\\(M_p\\)</span> tokenarithmetic\noperations<span class=\"math inline\">\\(\\hat{T}\\)</span>  <span class=\"math inline\">\\(M_p\\)</span> tokenarithmetic\noperations</p>\n<p> <span class=\"math inline\">\\(\\hat{T}\\hat{c}\\gamma+\\hat{T}(\\gamma+1)\\)</span>token\n<span class=\"math inline\">\\(\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}\\)</span>\ntoken <span class=\"math inline\">\\(\\hat{T}\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}\\)</span></p>\n<p><span class=\"math inline\">\\(\\alpha\\)</span> <span class=\"math inline\">\\(\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}\\)</span>\n</p>\n<p>KV cache</p>\n<h1 id=\"gamma-\"><span class=\"math inline\">\\(\\gamma\\)</span>\n</h1>\n<p> <span class=\"math inline\">\\(\\alpha\\)</span>  <span class=\"math inline\">\\(c\\)</span> <span class=\"math inline\">\\(\\gamma\\)</span> walltime improvement\nfactor <span class=\"math inline\">\\(\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma\nc+1)}\\)</span></p>\n<p> <span class=\"math inline\">\\(\\alpha\\)</span>  <span class=\"math inline\">\\(c\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span> </p>\n<img src=\"/f5c015c/fi_choose_gamma.png\" class title=\"gamma\">\n<p>tradeoff <span class=\"math inline\">\\(\\gamma\\)</span>\n</p>\n<img src=\"/f5c015c/fi_speed_and_op_table.png\" class title=\"\">\n<img src=\"/f5c015c/fi_speed_and_op.png\" class title=\"\">\n<img src=\"/f5c015c/fi_walltime.png\" class title=\"walltime\">\n<p><span class=\"math inline\">\\(\\beta\\)</span>\n <span class=\"math inline\">\\(\\beta\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span></p>\n<h1 id=\"approximation-model\">approximation model</h1>\n<p>approximation\nmodelapproximation modeltarget\nmodel</p>\n<p>n-gramapproximation\nmodel <span class=\"math inline\">\\(\\alpha\\)</span>\n</p>\n<p>copy\ntokenapproximation model <span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<p>approximation modelBlockwise parallel decoding for\ndeep autoregressive models</p>\n<h1 id=\"\"></h1>\n<p>T5approximation\nmodelT5-XXL3+</p>\n<img src=\"/f5c015c/fi_t5_result.png\" class title=\"T5\">\n<p> <span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<img src=\"/f5c015c/fi_alpha.png\" class title=\"alpha\">\n<p>target modelapproximation\nmodel0.50.9 <span class=\"math inline\">\\(\\alpha\\)</span>\nTargmax\n<span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<h1 id=\"\"></h1>\n<ul>\n<li>2~3<br>\n</li>\n<li>n-gram<br>\n</li>\n<li>target modelapproximation\nmodel</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Fast Inference from Transformers via Speculative Decoding\nhttps://arxiv.org/abs/2211.17192<br>\n2Accelerating Large Language Model Decoding with Speculative\nSampling https://arxiv.org/abs/2302.01318</p>\n"},{"title":"(5)","abbrlink":"336f2f3e","date":"2024-05-04T07:47:14.000Z","_content":"\n![](/images/cover.png)  \n\n\n\n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~  \n\n~\n\n***  \n\n# 1.bf16fp16  \n\n16bit  \n\nfp161510fp16bf16bf16  \n\nbf16187fp16bf16fp16  \n\nbf16fp16  \n\n{% asset_img bfloat16.jpeg bf16 %}  \n\n\n# 2.NTK-aware interpolation  \n\n1.NTK  \n\n2.NTKRoPEbase  \n\n# 3.LLMNTK-by-parts  \n\nNTK-by-partsNTKNTK-awareRoPENTK-by-parts<=1/32  \n\nNTK-by-parts  \n\n{% asset_img ntk_by_parts.png NTK-by-parts %}  \n\n# 4.LLMYaRN  \n\nPI/NTK/NTK-by-partstokensoftmax  \n\nRoPEtoken  \n\nsoftmax t>1RoPEt  \n\nYaRNNTK-by-partsattention score  \n\n{% asset_img yarn.png YaRN %}  \n\n# 5.Group-Query Attentionhidden size=DQhdD=dhkvnsbatch size=bLkv cache  \n\nkv cacheKV  \n\nGQAnKVQD/hKVsD/h2LnsD/hbatch2bLnsD/h\n4bLnsD/h  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  ","source":"_posts/cs/nlp/2024/05/-5.md","raw":"---\ntitle: (5)\nabbrlink: 336f2f3e\ndate: 2024-05-04 15:47:14\ntags:\n  - NLP\n  - LLM\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n![](/images/cover.png)  \n\n\n\n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~  \n\n~\n\n***  \n\n# 1.bf16fp16  \n\n16bit  \n\nfp161510fp16bf16bf16  \n\nbf16187fp16bf16fp16  \n\nbf16fp16  \n\n{% asset_img bfloat16.jpeg bf16 %}  \n\n\n# 2.NTK-aware interpolation  \n\n1.NTK  \n\n2.NTKRoPEbase  \n\n# 3.LLMNTK-by-parts  \n\nNTK-by-partsNTKNTK-awareRoPENTK-by-parts<=1/32  \n\nNTK-by-parts  \n\n{% asset_img ntk_by_parts.png NTK-by-parts %}  \n\n# 4.LLMYaRN  \n\nPI/NTK/NTK-by-partstokensoftmax  \n\nRoPEtoken  \n\nsoftmax t>1RoPEt  \n\nYaRNNTK-by-partsattention score  \n\n{% asset_img yarn.png YaRN %}  \n\n# 5.Group-Query Attentionhidden size=DQhdD=dhkvnsbatch size=bLkv cache  \n\nkv cacheKV  \n\nGQAnKVQD/hKVsD/h2LnsD/hbatch2bLnsD/h\n4bLnsD/h  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  ","slug":"cs/nlp/2024/05/-5","published":1,"updated":"2024-05-10T06:50:19.009Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmfe0009am4kbhfkdk91","content":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM~</p>\n<p>~</p>\n<hr>\n<h1 id=\"bf16fp16\">1.bf16fp16</h1>\n<p>16bit</p>\n<p>fp161510fp16bf16bf16</p>\n<p>bf16187fp16bf16fp16</p>\n<p>bf16fp16</p>\n<img src=\"/336f2f3e/bfloat16.jpeg\" class title=\"bf16\">\n<h1 id=\"ntk-aware-interpolation\">2.NTK-aware\ninterpolation</h1>\n<p>1.NTK</p>\n<p>2.NTKRoPEbase</p>\n<h1 id=\"llmntk-by-parts\">3.LLMNTK-by-parts</h1>\n<p>NTK-by-partsNTKNTK-awareRoPENTK-by-parts&lt;=1/32</p>\n<p>NTK-by-parts</p>\n<img src=\"/336f2f3e/ntk_by_parts.png\" class title=\"NTK-by-parts\">\n<h1 id=\"llmyarn\">4.LLMYaRN</h1>\n<p>PI/NTK/NTK-by-partstokensoftmax</p>\n<p>RoPEtoken</p>\n<p>softmax\nt&gt;1RoPEt</p>\n<p>YaRNNTK-by-partsattention score</p>\n<img src=\"/336f2f3e/yarn.png\" class title=\"YaRN\">\n<h1 id=\"group-query-attentionhidden-sizedqhdddhkvnsbatch-sizeblkv-cache\">5.Group-Query\nAttentionhidden\nsize=DQhdD=dhkvnsbatch\nsize=bLkv cache</h1>\n<p>kv cacheKV</p>\n<p>GQAnKVQD/hKVsD/h2LnsD/hbatch2bLnsD/h\n4bLnsD/h</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n","length":1925,"excerpt":"","more":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM~</p>\n<p>~</p>\n<hr>\n<h1 id=\"bf16fp16\">1.bf16fp16</h1>\n<p>16bit</p>\n<p>fp161510fp16bf16bf16</p>\n<p>bf16187fp16bf16fp16</p>\n<p>bf16fp16</p>\n<img src=\"/336f2f3e/bfloat16.jpeg\" class title=\"bf16\">\n<h1 id=\"ntk-aware-interpolation\">2.NTK-aware\ninterpolation</h1>\n<p>1.NTK</p>\n<p>2.NTKRoPEbase</p>\n<h1 id=\"llmntk-by-parts\">3.LLMNTK-by-parts</h1>\n<p>NTK-by-partsNTKNTK-awareRoPENTK-by-parts&lt;=1/32</p>\n<p>NTK-by-parts</p>\n<img src=\"/336f2f3e/ntk_by_parts.png\" class title=\"NTK-by-parts\">\n<h1 id=\"llmyarn\">4.LLMYaRN</h1>\n<p>PI/NTK/NTK-by-partstokensoftmax</p>\n<p>RoPEtoken</p>\n<p>softmax\nt&gt;1RoPEt</p>\n<p>YaRNNTK-by-partsattention score</p>\n<img src=\"/336f2f3e/yarn.png\" class title=\"YaRN\">\n<h1 id=\"group-query-attentionhidden-sizedqhdddhkvnsbatch-sizeblkv-cache\">5.Group-Query\nAttentionhidden\nsize=DQhdD=dhkvnsbatch\nsize=bLkv cache</h1>\n<p>kv cacheKV</p>\n<p>GQAnKVQD/hKVsD/h2LnsD/hbatch2bLnsD/h\n4bLnsD/h</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n"},{"title":"(6)","abbrlink":"7c04944d","date":"2024-05-14T11:21:09.000Z","_content":"\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~  \n\n~\n\n***  \n\n# 1.Xavier  \n\n2010Understanding the difficulty of training deep feedforward neural networksXavier  \n\n1  \n\n2  \n\n3magnitude  \n\n4  \n\nXavierfan-infan-out  \n\n11/fan-in  \n\n21/fan-out  \n\n32/fan-in + fan-out1/fan-in  \n\n# 2.RLHFRewardCritic  \n\nCriticPPOCriticadvantage functionactorCriticactor  \n\nRewardRLHFRewardActorActor\nRLHFActorCriticRewardActorobjective1Reward/Critic2Actor-CriticActorCriticCritic3  \n\nCriticRewardActor  \n\n# 3.Kaiming  \n\nKaimingXavierXavierReLUXavier\nKaimingReLUReLU00Xaviersqrt(1/N)sqrt(2/N)ReLU  \n\n# 4.Bert[CLS]  \n\nself-attentiontokentokenBert[CLS]token[CLS][CLS][CLS]token\n\n\n# 5.pytorchregister_buffer  \n\nregister_buffernn.Moduleregister_bufferregister_bufferstate_dictbatchnormregister_bufferregister_bufferstate_dict\n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  ","source":"_posts/cs/nlp/2024/05/-6.md","raw":"---\ntitle: (6)\ntags:\n  - NLP\n  - LLM\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 7c04944d\ndate: 2024-05-14 19:21:09\n---\n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~  \n\n~\n\n***  \n\n# 1.Xavier  \n\n2010Understanding the difficulty of training deep feedforward neural networksXavier  \n\n1  \n\n2  \n\n3magnitude  \n\n4  \n\nXavierfan-infan-out  \n\n11/fan-in  \n\n21/fan-out  \n\n32/fan-in + fan-out1/fan-in  \n\n# 2.RLHFRewardCritic  \n\nCriticPPOCriticadvantage functionactorCriticactor  \n\nRewardRLHFRewardActorActor\nRLHFActorCriticRewardActorobjective1Reward/Critic2Actor-CriticActorCriticCritic3  \n\nCriticRewardActor  \n\n# 3.Kaiming  \n\nKaimingXavierXavierReLUXavier\nKaimingReLUReLU00Xaviersqrt(1/N)sqrt(2/N)ReLU  \n\n# 4.Bert[CLS]  \n\nself-attentiontokentokenBert[CLS]token[CLS][CLS][CLS]token\n\n\n# 5.pytorchregister_buffer  \n\nregister_buffernn.Moduleregister_bufferregister_bufferstate_dictbatchnormregister_bufferregister_bufferstate_dict\n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  ","slug":"cs/nlp/2024/05/-6","published":1,"updated":"2024-05-14T11:26:49.973Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmff000cam4k0xy63bue","content":"<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM~</p>\n<p>~</p>\n<hr>\n<h1 id=\"xavier\">1.Xavier</h1>\n<p>2010Understanding the difficulty of training deep feedforward\nneural\nnetworksXavier</p>\n<p>1</p>\n<p>2</p>\n<p>3magnitude</p>\n<p>4</p>\n<p>Xavierfan-infan-out</p>\n<p>11/fan-in</p>\n<p>21/fan-out</p>\n<p>32/fan-in +\nfan-out1/fan-in</p>\n<h1 id=\"rlhfrewardcritic\">2.RLHFRewardCritic</h1>\n<p>CriticPPOCriticadvantage\nfunctionactorCriticactor</p>\n<p>RewardRLHFRewardActorActor\nRLHFActorCriticRewardActorobjective1Reward/Critic2Actor-CriticActorCriticCritic3</p>\n<p>CriticRewardActor</p>\n<h1 id=\"kaiming\">3.Kaiming</h1>\n<p>KaimingXavierXavierReLUXavier\nKaimingReLUReLU00Xaviersqrt(1/N)sqrt(2/N)ReLU</p>\n<h1 id=\"bertcls\">4.Bert[CLS]</h1>\n<p>self-attentiontokentokenBert[CLS]token[CLS][CLS][CLS]token</p>\n<h1 id=\"pytorchregister_buffer\">5.pytorchregister_buffer</h1>\n<p>register_buffernn.Moduleregister_bufferregister_bufferstate_dictbatchnormregister_bufferregister_bufferstate_dict</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n","length":2799,"excerpt":"","more":"<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM~</p>\n<p>~</p>\n<hr>\n<h1 id=\"xavier\">1.Xavier</h1>\n<p>2010Understanding the difficulty of training deep feedforward\nneural\nnetworksXavier</p>\n<p>1</p>\n<p>2</p>\n<p>3magnitude</p>\n<p>4</p>\n<p>Xavierfan-infan-out</p>\n<p>11/fan-in</p>\n<p>21/fan-out</p>\n<p>32/fan-in +\nfan-out1/fan-in</p>\n<h1 id=\"rlhfrewardcritic\">2.RLHFRewardCritic</h1>\n<p>CriticPPOCriticadvantage\nfunctionactorCriticactor</p>\n<p>RewardRLHFRewardActorActor\nRLHFActorCriticRewardActorobjective1Reward/Critic2Actor-CriticActorCriticCritic3</p>\n<p>CriticRewardActor</p>\n<h1 id=\"kaiming\">3.Kaiming</h1>\n<p>KaimingXavierXavierReLUXavier\nKaimingReLUReLU00Xaviersqrt(1/N)sqrt(2/N)ReLU</p>\n<h1 id=\"bertcls\">4.Bert[CLS]</h1>\n<p>self-attentiontokentokenBert[CLS]token[CLS][CLS][CLS]token</p>\n<h1 id=\"pytorchregister_buffer\">5.pytorchregister_buffer</h1>\n<p>register_buffernn.Moduleregister_bufferregister_bufferstate_dictbatchnormregister_bufferregister_bufferstate_dict</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n"},{"title":"-","abbrlink":"45ee1a6d","date":"2024-05-06T08:22:38.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n2024Q2RAGAgent  \n\n()128k+  \n\n  \n\n# StreamingLLM  \n\nEfficient Streaming Language Models with Attention Sinks  \n\n20239  \n\n4M  \n\n  \n\nMITCMUNVIDIAMETAStreamingLLM -- LLAMA2, MPT, FalconPythia4MStreamingLLMPPL  \n\nStreamingLLMPPL  \n\n##   \n\n  \n- KVKVGPU H100A10040G/80GKV  \n- RoPERoPE  \n\n  \n- Longformerwindow attentionKVwindow attentiontoken  \n- sliding window with recomputationhttps://github.com/mit-han-lab/streaming-llm/issues/51KVrecomputation  \n- NTKYaRNNSFW()https://kaiokendev.github.io/til#extending-context-to-8k  \n- Efficiently scaling transformer inferenceSmoothQuant: Accurate and efficient post-training quantization for large language modelsDynamic context pruning for efficient and interpretable autoregressive transformersSpatten: Efficient sparse attention architecture with cascade token and head pruningH2o: Heavyhitter oracle for efficient generative inference of large language models  \n- FlashAttention  \n- Big BirdLinformer  \n\nStreamingLLMattention sinkwindow attentionattention sinkPPLsliding window with recomputationStreamingLLM22+  \n\nStreamingLLMPPLStreamingLLMPPL  \n\n{% asset_img streamingllm_model_ppl.png PPL %}  \n\n## attention sink  \n\nwindow attentionLLMtoken  \n\n{% asset_img stremingllm_attention_sink.png attention sink %}  \n\nattention scoretokentoken  \n\ntokenattention sinktoken  \n\nsoftmaxsoftmaxtoken1tokentokentoken1  \n\ntokentokentokentokenattention sink  \n\n4token\\ntokenattention sinktoken  \n\nquantSmoothQuant: Accurate and efficient post-training quantization for large language modelsQuantizable transformers: Removing outliers by helping attention heads do nothing  \n\n##   \n\nStreamingLLMStreamingLLMattention sink  \n\nStreamingLLMwindow attentionattention sink tokenKVtokenKVtokenKV cachetokenStreamingLLMattention  \n\n{% asset_img streamingllm_compare.png StreamingLLM %}  \n\ntoken  \n\n{% asset_img stremingllm_init_token_num.png attention sink number %}  \n\n4tokenPPLtoken  \n\nwindow attentionStreamingLLMKV cache  \n- 4tokenattention sink  \n- KV cache  \n\nStreamingLLMKV  \n\n{% asset_img stremingllm_kv_cache.png cache %}  \n\nStreamingLLMdistancetokencachedistanceRoPEtokenKVtokenLM-Infinite  \n\nattention sinktokenattention sink tokentoken  \n\ntokensoftmaxsoftmax-off-by-oneattentionsoftmax  \n\nsoftmax-off-by-one  \n\n$$\\text{SoftMax}_1(x)_i=\\frac{e^{x_i}}{1+\\sum_{j=1}^Ne^{x_j}}$$  \n\nsoftmax-off-by-one1attention score1KV0tokentoken1tokenattention sink  \n\ntokenattention sinksoftmax-off-by-one3160M  \n\n{% asset_img stremingllm_exp.png  %}  \n\nsoftmax-off-by-onezero sinktokenattention sink  \n\nLLAMA2, MPT, FalconPythiaStreamingLLM4MMPTALIBIRoPE\n\n{% asset_img stremingllm_perf_4m.png  %}  \n\n4MStreamingLLMPPL  \n\nStreamingLLMwindow attentioncacheStreamingLLMPPLStreamingLLM  \n\n# LM-Infinite  \n\nLM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models  \n\n20238  \n\n2k4k200MPPL  \n\n  \n\nLM-InfiniteLLMPasskey RetrievalQasper2.77.5  \n\n##   \n\nLM-InfiniteTransformer LLM3  \n\n- 1challenges in handling unseen distances among tokens  \n\ntokentokenattention logits  \n\nLLAMA2ArXiv8kattention logits  \n\n{% asset_img lm_infinite_attention_logits_explode.png logits %}  \n\nLLAMA24k4kattention logits  \n\ntokenlogits  \n\n- 2attending to unseen numbers of tokens  \n\ntokenlogits(attention entropy)  \n\ntokentoken  \n\n8k  \n\n{% asset_img lm_infinite_attention_entropy.png  %}  \n\nattention context size  \n\nwindow attentiontokentokenhandle12XPosLongformer  \n\nwindow attention  \n\n- 3starting tokens occupy a distinct feature space  \n\ntoken  \n\nThe impact of positional encoding on length generalization in transformers1tokentokentoken  \n\nStreamingLLMattention sinktokenwindow attentiontoken  \n\nLLAMA2hidden state outputPCA2  \n\n{% asset_img lm_infinite_starting_tokens.png token %}  \n\ntokentokentokentokentokentoken  \n\ntoken  \n\n##   \n\nLM-InfiniteLLMzero-shotLM-Infinite-shaped attention maskDistance ceiling  \n\n- -shaped attention mask  \n\n-shaped attention maskLongformerLongNetBig Bird  \n\n-shaped attention maskwindow attentiontokentokentoken1$n_{\\mathrm{starting}}$ token2$L_{\\mathrm{pretrain}}$ token$n_{\\mathrm{starting}}$ $L_{\\mathrm{pretrain}}$ tokentoken  \n\n $n_{\\mathrm{starting}}$  $n_{\\mathrm{starting}}\\in[5,100]$   \n\n{% asset_img lm_infinite_starting_tokens_num.png token %}  \n\n  \n\n-shaped attention mask23  \n\n- Distance ceiling  \n\nLM-Infinite $L_{\\mathrm{pretrain}}$token  \n\nattention logit $w(\\mathbf{q},\\mathbf{k},d)$ $d$ tokenDistance ceilingattention logit  \n\n$$\\text{attention logits}=w(\\mathbf{q},\\mathbf{k},d')$$  \n\n$$d'=\\min(d,L_\\text{pretrain})$$  \n\nDistance ceiling1  \n\n- Optionally attending to top-k tokens in the middle  \n\n-shaped attention maskDistance ceilingtokentokentokenattentiontoken  \n\ntoken $k$ attention logitstokenattention  $k$ token $d=\\frac12L_\\text{pre-train}$  \n\n $k$ Passkey Retrievalvalidation set  \n\n{% asset_img lm_infinite_middle_k.png k %}  \n\n $k=5$>5  \n\nmiddle tokenmiddle token2  \n\nLM-Infinite  \n\n{% asset_img lm_infinite_design.png  %}  \n\n##   \n\nLLaMA-7BLLaMA2-7BMPT-7BGPT-J-6BLM-InfiniteMPT-7BAlibiRoPE  \n\n- Language Modeling  \n\nArXivOpenWebText2  \n\nLM-Infinite0-12kPPL  \n\n{% asset_img lm_infinite_ppl_figure.png PPL %}  \n\nLLAMA210KNaN32KOOM  \n\nPPLLM-InfinitePPL\n\nLM-InfiniteLM-Infinite + Llama2ArXiv200M tokenPPL200MPPL  \n\n{% asset_img lm_infinite_ppl_200m.png PPL 200M %}  \n\n- Passkey Retrieval and Qapser  \n\nPasskey RetrievalQapsertop-5middle tokenattention  \n\nPasskey Retrieval0LM-Infinite  \n\n{% asset_img lm_infinite_downstream.png  %}  \n\n- Ablation study  \n\nLM-Infinite-shaped attention maskDistance ceiling  \n\n{% asset_img lm_infinite_ablation.png  %}  \n\n-shaped attention maskdistance ceilingPPL  \n\n# Transformer-XL  \n\nInfini-TransformerTransformer-XL  \n\nTransformer-XL20196CMUGoogle Brain  \n\ntransformersegmentsegment  \n\n{% asset_img xl_vanilla_sw.png vanilla transformer %}  \n\n  \n\nTransformer-XLattentionsegmentLsegment $[\\mathbf{s}_{\\tau}=x_{\\tau,1},\\cdots,x_{\\tau,L}]$  $\\mathbf{s}_{\\tau+1}=[x_{\\tau+1,1},\\cdots,x_{\\tau+1,L}]$  \n\n$$\\begin{aligned}&\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}=\\left[\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\right]\\\\&\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n=\\mathbf{h}_{\\tau+1}^{n-1}\\mathbf{W}_q^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_k^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_v^\\top\\\\&\\mathbf{h}_{\\tau+1}^n=\\text{Transformer-Layer}\\left(\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n\\right)\\end{aligned}$$  \n\nSGstop gradient$\\begin{bmatrix}\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\end{bmatrix}$ hqkv $n$  $n-1$   \n\nsegmentattentionTransformer-XLsegmentKVsegmentsegmentsegment  \n\n{% asset_img xl_attention.png Transformer-XL %}  \n\n# Infini-Transformer  \n\nLeave No Context Behind:Efficient Infinite Context Transformers with Infini-attention  \n\n20244  \n\n500k/1M  \n\n/  \n\nGoogleInfini-attention  \n\n## \n\n  \n\ntransformer  \n\nStreamingLLM  \n\nMetalearned neural memory/Enhancing the transformer with explicit relational encoding for math problem solvingcompressive memorycontextsimplicitytradeoff  \n\n## Infini-attention  \n\nInfini-TransformerInfini-attentionTransformer blockmask local attentionlong term linear attention  \n\n{% asset_img infini_attention_structure.png infini-attention %}  \n\nInfini-attentionQKVQInfini-attentionconcat  \n\nTransformerLLM  \n\nInfini-attentionTransformer-XL  \n\n{% asset_img infini_attention_process.png infini-attention %}  \n\nTransformer-XLInfini-TransformersegmentTransformer-XLsegment  \n\nInfini-Transformersegment  \n\ncompressive memorysimplicitycomputational efficiencyLearning associative inference using fast weight memorymemoryassociative matrix  \n\n  \n\n-   \n\nsegment $N$ $Q\\in\\mathbf{R}^{N\\times d_{key}}$ memory $M_{s-1}\\in\\mathbf{R}^{d_{key}\\times d_{value}}$   \n\n$$A_{\\text{mm}}=\\frac{\\sigma(Q)M_{s-1}}{\\sigma(Q)z_{s-1}}$$  \n\n$\\sigma$element-wise ELU + 1  \n\n$z_{s-1}\\in\\mathbf{R}^{d_{key}}$normalization termnormalization termTransformers are rnns: Fast autoregressive transformers with linear attentionK  \n\n-   \n\nnormalization termmemory  \n\n$$M_s\\leftarrow M_{s-1}+\\sigma(K)^TV$$  \n\n$$z_s\\leftarrow z_{s-1}+\\sum_{t=1}^N\\sigma(K_t)$$  \n\nmemoryMetalearned neural memoryLearning associative inference using fast weight memorydelta rule  \n\n$$M_s\\leftarrow M_{s-1}+\\sigma(K)^T(V-\\frac{\\sigma(K)M_{s-1}}{\\sigma(K)z_{s-1}})$$  \n\n- local attention  \n\nsegment $A_{mem}$ local attention state $A_{dot}$   \n\n$$A=sigmoid(\\beta)\\odot A_{mem}+(1-sigmoid(\\beta))\\odot A_{dot}$$  \n\n $\\beta$   \n\n $\\beta$ 0segment101  \n\n{% asset_img infini_attention_gating.png gating %}  \n\nsegment-level memory  \n\n{% asset_img infini_attention_compare.png  %}  \n\n## \n\nsegment $N$ 20483276816segment  \n\n- \n\nInfini-TransformerTransformer-XL/Memorzing Transformer/RMTPG-19Arxivlanguage modeling  \n\n{% asset_img infini_attention_language_modeling.png  %}  \n\nInfini-TransformerMemorizing Transformers1%  \n\n100kInfini-TransformerPPL  \n\n- \n\n1B  \n- batch size = 64\n- step = 30k\n-  > 4k\n- segment length = 2k\n\n1Mpasskey retrievalzero-shotfine-tune  \n\n{% asset_img infini_attention_passkey.png passkey %}  \n\nInfini-Transformerlinear + delta1Mpasskey retrieval  \n\n8B8k30k500kBookSum  \n\n{% asset_img infini_attention_booksum.png booksum %}  \n\nInfini-Transformerlinear + delta  \n\n#   \n\n1. StreamingLM-Infiniteattention sinktokenPPLLM-Infinitedistance ceilingtokenMPPLtoken  \n2. Infini-Transformer  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n# Reference  \n1Efficient Streaming Language Models with Attention Sinks https://arxiv.org/abs/2309.17453  \n2Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://arxiv.org/abs/1901.02860  \n3Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention https://arxiv.org/abs/2404.07143  \n4LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models https://arxiv.org/abs/2308.16137  ","source":"_posts/cs/nlp/2024/05/-.md","raw":"---\ntitle: -\nabbrlink: 45ee1a6d\ndate: 2024-05-06 16:22:38\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \n  - \n  - \n  - attention\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n2024Q2RAGAgent  \n\n()128k+  \n\n  \n\n# StreamingLLM  \n\nEfficient Streaming Language Models with Attention Sinks  \n\n20239  \n\n4M  \n\n  \n\nMITCMUNVIDIAMETAStreamingLLM -- LLAMA2, MPT, FalconPythia4MStreamingLLMPPL  \n\nStreamingLLMPPL  \n\n##   \n\n  \n- KVKVGPU H100A10040G/80GKV  \n- RoPERoPE  \n\n  \n- Longformerwindow attentionKVwindow attentiontoken  \n- sliding window with recomputationhttps://github.com/mit-han-lab/streaming-llm/issues/51KVrecomputation  \n- NTKYaRNNSFW()https://kaiokendev.github.io/til#extending-context-to-8k  \n- Efficiently scaling transformer inferenceSmoothQuant: Accurate and efficient post-training quantization for large language modelsDynamic context pruning for efficient and interpretable autoregressive transformersSpatten: Efficient sparse attention architecture with cascade token and head pruningH2o: Heavyhitter oracle for efficient generative inference of large language models  \n- FlashAttention  \n- Big BirdLinformer  \n\nStreamingLLMattention sinkwindow attentionattention sinkPPLsliding window with recomputationStreamingLLM22+  \n\nStreamingLLMPPLStreamingLLMPPL  \n\n{% asset_img streamingllm_model_ppl.png PPL %}  \n\n## attention sink  \n\nwindow attentionLLMtoken  \n\n{% asset_img stremingllm_attention_sink.png attention sink %}  \n\nattention scoretokentoken  \n\ntokenattention sinktoken  \n\nsoftmaxsoftmaxtoken1tokentokentoken1  \n\ntokentokentokentokenattention sink  \n\n4token\\ntokenattention sinktoken  \n\nquantSmoothQuant: Accurate and efficient post-training quantization for large language modelsQuantizable transformers: Removing outliers by helping attention heads do nothing  \n\n##   \n\nStreamingLLMStreamingLLMattention sink  \n\nStreamingLLMwindow attentionattention sink tokenKVtokenKVtokenKV cachetokenStreamingLLMattention  \n\n{% asset_img streamingllm_compare.png StreamingLLM %}  \n\ntoken  \n\n{% asset_img stremingllm_init_token_num.png attention sink number %}  \n\n4tokenPPLtoken  \n\nwindow attentionStreamingLLMKV cache  \n- 4tokenattention sink  \n- KV cache  \n\nStreamingLLMKV  \n\n{% asset_img stremingllm_kv_cache.png cache %}  \n\nStreamingLLMdistancetokencachedistanceRoPEtokenKVtokenLM-Infinite  \n\nattention sinktokenattention sink tokentoken  \n\ntokensoftmaxsoftmax-off-by-oneattentionsoftmax  \n\nsoftmax-off-by-one  \n\n$$\\text{SoftMax}_1(x)_i=\\frac{e^{x_i}}{1+\\sum_{j=1}^Ne^{x_j}}$$  \n\nsoftmax-off-by-one1attention score1KV0tokentoken1tokenattention sink  \n\ntokenattention sinksoftmax-off-by-one3160M  \n\n{% asset_img stremingllm_exp.png  %}  \n\nsoftmax-off-by-onezero sinktokenattention sink  \n\nLLAMA2, MPT, FalconPythiaStreamingLLM4MMPTALIBIRoPE\n\n{% asset_img stremingllm_perf_4m.png  %}  \n\n4MStreamingLLMPPL  \n\nStreamingLLMwindow attentioncacheStreamingLLMPPLStreamingLLM  \n\n# LM-Infinite  \n\nLM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models  \n\n20238  \n\n2k4k200MPPL  \n\n  \n\nLM-InfiniteLLMPasskey RetrievalQasper2.77.5  \n\n##   \n\nLM-InfiniteTransformer LLM3  \n\n- 1challenges in handling unseen distances among tokens  \n\ntokentokenattention logits  \n\nLLAMA2ArXiv8kattention logits  \n\n{% asset_img lm_infinite_attention_logits_explode.png logits %}  \n\nLLAMA24k4kattention logits  \n\ntokenlogits  \n\n- 2attending to unseen numbers of tokens  \n\ntokenlogits(attention entropy)  \n\ntokentoken  \n\n8k  \n\n{% asset_img lm_infinite_attention_entropy.png  %}  \n\nattention context size  \n\nwindow attentiontokentokenhandle12XPosLongformer  \n\nwindow attention  \n\n- 3starting tokens occupy a distinct feature space  \n\ntoken  \n\nThe impact of positional encoding on length generalization in transformers1tokentokentoken  \n\nStreamingLLMattention sinktokenwindow attentiontoken  \n\nLLAMA2hidden state outputPCA2  \n\n{% asset_img lm_infinite_starting_tokens.png token %}  \n\ntokentokentokentokentokentoken  \n\ntoken  \n\n##   \n\nLM-InfiniteLLMzero-shotLM-Infinite-shaped attention maskDistance ceiling  \n\n- -shaped attention mask  \n\n-shaped attention maskLongformerLongNetBig Bird  \n\n-shaped attention maskwindow attentiontokentokentoken1$n_{\\mathrm{starting}}$ token2$L_{\\mathrm{pretrain}}$ token$n_{\\mathrm{starting}}$ $L_{\\mathrm{pretrain}}$ tokentoken  \n\n $n_{\\mathrm{starting}}$  $n_{\\mathrm{starting}}\\in[5,100]$   \n\n{% asset_img lm_infinite_starting_tokens_num.png token %}  \n\n  \n\n-shaped attention mask23  \n\n- Distance ceiling  \n\nLM-Infinite $L_{\\mathrm{pretrain}}$token  \n\nattention logit $w(\\mathbf{q},\\mathbf{k},d)$ $d$ tokenDistance ceilingattention logit  \n\n$$\\text{attention logits}=w(\\mathbf{q},\\mathbf{k},d')$$  \n\n$$d'=\\min(d,L_\\text{pretrain})$$  \n\nDistance ceiling1  \n\n- Optionally attending to top-k tokens in the middle  \n\n-shaped attention maskDistance ceilingtokentokentokenattentiontoken  \n\ntoken $k$ attention logitstokenattention  $k$ token $d=\\frac12L_\\text{pre-train}$  \n\n $k$ Passkey Retrievalvalidation set  \n\n{% asset_img lm_infinite_middle_k.png k %}  \n\n $k=5$>5  \n\nmiddle tokenmiddle token2  \n\nLM-Infinite  \n\n{% asset_img lm_infinite_design.png  %}  \n\n##   \n\nLLaMA-7BLLaMA2-7BMPT-7BGPT-J-6BLM-InfiniteMPT-7BAlibiRoPE  \n\n- Language Modeling  \n\nArXivOpenWebText2  \n\nLM-Infinite0-12kPPL  \n\n{% asset_img lm_infinite_ppl_figure.png PPL %}  \n\nLLAMA210KNaN32KOOM  \n\nPPLLM-InfinitePPL\n\nLM-InfiniteLM-Infinite + Llama2ArXiv200M tokenPPL200MPPL  \n\n{% asset_img lm_infinite_ppl_200m.png PPL 200M %}  \n\n- Passkey Retrieval and Qapser  \n\nPasskey RetrievalQapsertop-5middle tokenattention  \n\nPasskey Retrieval0LM-Infinite  \n\n{% asset_img lm_infinite_downstream.png  %}  \n\n- Ablation study  \n\nLM-Infinite-shaped attention maskDistance ceiling  \n\n{% asset_img lm_infinite_ablation.png  %}  \n\n-shaped attention maskdistance ceilingPPL  \n\n# Transformer-XL  \n\nInfini-TransformerTransformer-XL  \n\nTransformer-XL20196CMUGoogle Brain  \n\ntransformersegmentsegment  \n\n{% asset_img xl_vanilla_sw.png vanilla transformer %}  \n\n  \n\nTransformer-XLattentionsegmentLsegment $[\\mathbf{s}_{\\tau}=x_{\\tau,1},\\cdots,x_{\\tau,L}]$  $\\mathbf{s}_{\\tau+1}=[x_{\\tau+1,1},\\cdots,x_{\\tau+1,L}]$  \n\n$$\\begin{aligned}&\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}=\\left[\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\right]\\\\&\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n=\\mathbf{h}_{\\tau+1}^{n-1}\\mathbf{W}_q^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_k^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_v^\\top\\\\&\\mathbf{h}_{\\tau+1}^n=\\text{Transformer-Layer}\\left(\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n\\right)\\end{aligned}$$  \n\nSGstop gradient$\\begin{bmatrix}\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\end{bmatrix}$ hqkv $n$  $n-1$   \n\nsegmentattentionTransformer-XLsegmentKVsegmentsegmentsegment  \n\n{% asset_img xl_attention.png Transformer-XL %}  \n\n# Infini-Transformer  \n\nLeave No Context Behind:Efficient Infinite Context Transformers with Infini-attention  \n\n20244  \n\n500k/1M  \n\n/  \n\nGoogleInfini-attention  \n\n## \n\n  \n\ntransformer  \n\nStreamingLLM  \n\nMetalearned neural memory/Enhancing the transformer with explicit relational encoding for math problem solvingcompressive memorycontextsimplicitytradeoff  \n\n## Infini-attention  \n\nInfini-TransformerInfini-attentionTransformer blockmask local attentionlong term linear attention  \n\n{% asset_img infini_attention_structure.png infini-attention %}  \n\nInfini-attentionQKVQInfini-attentionconcat  \n\nTransformerLLM  \n\nInfini-attentionTransformer-XL  \n\n{% asset_img infini_attention_process.png infini-attention %}  \n\nTransformer-XLInfini-TransformersegmentTransformer-XLsegment  \n\nInfini-Transformersegment  \n\ncompressive memorysimplicitycomputational efficiencyLearning associative inference using fast weight memorymemoryassociative matrix  \n\n  \n\n-   \n\nsegment $N$ $Q\\in\\mathbf{R}^{N\\times d_{key}}$ memory $M_{s-1}\\in\\mathbf{R}^{d_{key}\\times d_{value}}$   \n\n$$A_{\\text{mm}}=\\frac{\\sigma(Q)M_{s-1}}{\\sigma(Q)z_{s-1}}$$  \n\n$\\sigma$element-wise ELU + 1  \n\n$z_{s-1}\\in\\mathbf{R}^{d_{key}}$normalization termnormalization termTransformers are rnns: Fast autoregressive transformers with linear attentionK  \n\n-   \n\nnormalization termmemory  \n\n$$M_s\\leftarrow M_{s-1}+\\sigma(K)^TV$$  \n\n$$z_s\\leftarrow z_{s-1}+\\sum_{t=1}^N\\sigma(K_t)$$  \n\nmemoryMetalearned neural memoryLearning associative inference using fast weight memorydelta rule  \n\n$$M_s\\leftarrow M_{s-1}+\\sigma(K)^T(V-\\frac{\\sigma(K)M_{s-1}}{\\sigma(K)z_{s-1}})$$  \n\n- local attention  \n\nsegment $A_{mem}$ local attention state $A_{dot}$   \n\n$$A=sigmoid(\\beta)\\odot A_{mem}+(1-sigmoid(\\beta))\\odot A_{dot}$$  \n\n $\\beta$   \n\n $\\beta$ 0segment101  \n\n{% asset_img infini_attention_gating.png gating %}  \n\nsegment-level memory  \n\n{% asset_img infini_attention_compare.png  %}  \n\n## \n\nsegment $N$ 20483276816segment  \n\n- \n\nInfini-TransformerTransformer-XL/Memorzing Transformer/RMTPG-19Arxivlanguage modeling  \n\n{% asset_img infini_attention_language_modeling.png  %}  \n\nInfini-TransformerMemorizing Transformers1%  \n\n100kInfini-TransformerPPL  \n\n- \n\n1B  \n- batch size = 64\n- step = 30k\n-  > 4k\n- segment length = 2k\n\n1Mpasskey retrievalzero-shotfine-tune  \n\n{% asset_img infini_attention_passkey.png passkey %}  \n\nInfini-Transformerlinear + delta1Mpasskey retrieval  \n\n8B8k30k500kBookSum  \n\n{% asset_img infini_attention_booksum.png booksum %}  \n\nInfini-Transformerlinear + delta  \n\n#   \n\n1. StreamingLM-Infiniteattention sinktokenPPLLM-Infinitedistance ceilingtokenMPPLtoken  \n2. Infini-Transformer  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n# Reference  \n1Efficient Streaming Language Models with Attention Sinks https://arxiv.org/abs/2309.17453  \n2Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://arxiv.org/abs/1901.02860  \n3Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention https://arxiv.org/abs/2404.07143  \n4LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models https://arxiv.org/abs/2308.16137  ","slug":"cs/nlp/2024/05/-","published":1,"updated":"2024-05-13T09:06:18.036Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmfg000dam4k8l3m65oc","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>2024Q2RAGAgent</p>\n<p>()128k+</p>\n<p></p>\n<h1 id=\"streamingllm\">StreamingLLM</h1>\n<p>Efficient Streaming Language Models with Attention Sinks</p>\n<p>20239</p>\n<p>4M</p>\n<p></p>\n<p>MITCMUNVIDIAMETAStreamingLLM\n-- LLAMA2, MPT,\nFalconPythia4MStreamingLLMPPL</p>\n<p>StreamingLLMPPL</p>\n<h2 id=\"\"></h2>\n<p><br>\n-\nKVKVGPU\nH100A10040G/80GKV<br>\n-\nRoPERoPE</p>\n<p><br>\n- Longformerwindow\nattentionKVwindow\nattentiontoken<br>\n- sliding window with\nrecomputationhttps://github.com/mit-han-lab/streaming-llm/issues/51KVrecomputation<br>\n-\nNTKYaRNNSFW()https://kaiokendev.github.io/til#extending-context-to-8k<br>\n- Efficiently scaling transformer\ninferenceSmoothQuant: Accurate and efficient post-training\nquantization for large language modelsDynamic context pruning for\nefficient and interpretable autoregressive transformersSpatten:\nEfficient sparse attention architecture with cascade token and head\npruningH2o: Heavyhitter oracle for efficient generative inference\nof large language models<br>\n-\nFlashAttention<br>\n- Big BirdLinformer</p>\n<p>StreamingLLMattention\nsinkwindow\nattentionattention\nsinkPPLsliding\nwindow with\nrecomputationStreamingLLM22+</p>\n<p>StreamingLLMPPLStreamingLLMPPL</p>\n<img src=\"/45ee1a6d/streamingllm_model_ppl.png\" class title=\"PPL\">\n<h2 id=\"attention-sink\">attention sink</h2>\n<p>window\nattentionLLMtoken</p>\n<img src=\"/45ee1a6d/stremingllm_attention_sink.png\" class title=\"attention sink\">\n<p>attention\nscoretokentoken</p>\n<p>tokenattention\nsinktoken</p>\n<p>softmaxsoftmaxtoken1tokentokentoken1</p>\n<p>tokentokentokentokenattention\nsink</p>\n<p>4tokentokenattention\nsinktoken</p>\n<p>quantSmoothQuant: Accurate and\nefficient post-training quantization for large language\nmodelsQuantizable transformers: Removing outliers by helping\nattention heads do nothing</p>\n<h2 id=\"\"></h2>\n<p>StreamingLLMStreamingLLMattention\nsink</p>\n<p>StreamingLLMwindow\nattentionattention sink\ntokenKVtokenKVtokenKV\ncachetokenStreamingLLMattention</p>\n<img src=\"/45ee1a6d/streamingllm_compare.png\" class title=\"StreamingLLM\">\n<p>token</p>\n<img src=\"/45ee1a6d/stremingllm_init_token_num.png\" class title=\"attention sink number\">\n<p>4tokenPPLtoken</p>\n<p>window attentionStreamingLLMKV\ncache<br>\n- 4tokenattention sink<br>\n- KV cache</p>\n<p>StreamingLLMKV</p>\n<img src=\"/45ee1a6d/stremingllm_kv_cache.png\" class title=\"cache\">\n<p>StreamingLLMdistancetokencachedistanceRoPEtokenKVtokenLM-Infinite</p>\n<p>attention\nsinktokenattention\nsink tokentoken</p>\n<p>tokensoftmaxsoftmax-off-by-oneattentionsoftmax</p>\n<p>softmax-off-by-one</p>\n<p><span class=\"math display\">\\[\\text{SoftMax}_1(x)_i=\\frac{e^{x_i}}{1+\\sum_{j=1}^Ne^{x_j}}\\]</span></p>\n<p>softmax-off-by-one1attention\nscore1KV0tokentoken1tokenattention\nsink</p>\n<p>tokenattention\nsinksoftmax-off-by-one3160M</p>\n<img src=\"/45ee1a6d/stremingllm_exp.png\" class title=\"\">\n<p>softmax-off-by-onezero\nsinktokenattention sink</p>\n<p>LLAMA2, MPT,\nFalconPythiaStreamingLLM4MMPTALIBIRoPE</p>\n<img src=\"/45ee1a6d/stremingllm_perf_4m.png\" class title=\"\">\n<p>4MStreamingLLMPPL</p>\n<p>StreamingLLMwindow\nattentioncacheStreamingLLMPPLStreamingLLM</p>\n<h1 id=\"lm-infinite\">LM-Infinite</h1>\n<p>LM-Infinite: Zero-Shot Extreme Length Generalization for Large\nLanguage Models</p>\n<p>20238</p>\n<p>2k4k200MPPL</p>\n<p></p>\n<p>LM-InfiniteLLMPasskey\nRetrievalQasper2.77.5</p>\n<h2 id=\"\"></h2>\n<p>LM-InfiniteTransformer\nLLM3</p>\n<ul>\n<li>1challenges in handling unseen distances among tokens</li>\n</ul>\n<p>tokentokenattention\nlogits</p>\n<p>LLAMA2ArXiv8kattention\nlogits</p>\n<img src=\"/45ee1a6d/lm_infinite_attention_logits_explode.png\" class title=\"logits\">\n<p>LLAMA24k4kattention\nlogits</p>\n<p>tokenlogits</p>\n<ul>\n<li>2attending to unseen numbers of tokens</li>\n</ul>\n<p>tokenlogits(attention\nentropy)</p>\n<p>tokentoken</p>\n<p>8k</p>\n<img src=\"/45ee1a6d/lm_infinite_attention_entropy.png\" class title=\"\">\n<p>attention context\nsize</p>\n<p>window\nattentiontokentokenhandle12XPosLongformer</p>\n<p>window attention</p>\n<ul>\n<li>3starting tokens occupy a distinct feature space</li>\n</ul>\n<p>token</p>\n<p>The impact of positional encoding on length\ngeneralization in\ntransformers1tokentokentoken</p>\n<p>StreamingLLMattention\nsinktokenwindow\nattentiontoken</p>\n<p>LLAMA2hidden state\noutputPCA2</p>\n<img src=\"/45ee1a6d/lm_infinite_starting_tokens.png\" class title=\"token\">\n<p>tokentokentokentokentokentoken</p>\n<p>token</p>\n<h2 id=\"\"></h2>\n<p>LM-InfiniteLLMzero-shotLM-Infinite-shaped\nattention maskDistance ceiling</p>\n<ul>\n<li>-shaped attention mask</li>\n</ul>\n<p>-shaped attention maskLongformerLongNetBig\nBird</p>\n<p>-shaped attention maskwindow\nattentiontokentokentoken1<span class=\"math inline\">\\(n_{\\mathrm{starting}}\\)</span>\ntoken2<span class=\"math inline\">\\(L_{\\mathrm{pretrain}}\\)</span>\ntoken<span class=\"math inline\">\\(n_{\\mathrm{starting}}\\)</span> <span class=\"math inline\">\\(L_{\\mathrm{pretrain}}\\)</span>\ntokentoken</p>\n<p> <span class=\"math inline\">\\(n_{\\mathrm{starting}}\\)</span>\n <span class=\"math inline\">\\(n_{\\mathrm{starting}}\\in[5,100]\\)</span>\n</p>\n<img src=\"/45ee1a6d/lm_infinite_starting_tokens_num.png\" class title=\"token\">\n<p></p>\n<p>-shaped attention mask23</p>\n<ul>\n<li>Distance ceiling</li>\n</ul>\n<p>LM-Infinite <span class=\"math inline\">\\(L_{\\mathrm{pretrain}}\\)</span>token</p>\n<p>attention logit <span class=\"math inline\">\\(w(\\mathbf{q},\\mathbf{k},d)\\)</span> <span class=\"math inline\">\\(d\\)</span> tokenDistance\nceilingattention logit</p>\n<p><span class=\"math display\">\\[\\text{attention\nlogits}=w(\\mathbf{q},\\mathbf{k},d&#39;)\\]</span></p>\n<p><span class=\"math display\">\\[d&#39;=\\min(d,L_\\text{pretrain})\\]</span></p>\n<p>Distance ceiling1</p>\n<ul>\n<li>Optionally attending to top-k tokens in the middle</li>\n</ul>\n<p>-shaped attention maskDistance\nceilingtokentokentokenattentiontoken</p>\n<p>token <span class=\"math inline\">\\(k\\)</span> attention\nlogitstokenattention  <span class=\"math inline\">\\(k\\)</span> token <span class=\"math inline\">\\(d=\\frac12L_\\text{pre-train}\\)</span></p>\n<p> <span class=\"math inline\">\\(k\\)</span> Passkey\nRetrievalvalidation set</p>\n<img src=\"/45ee1a6d/lm_infinite_middle_k.png\" class title=\"k\">\n<p> <span class=\"math inline\">\\(k=5\\)</span>&gt;5</p>\n<p>middle\ntokenmiddle\ntoken2</p>\n<p>LM-Infinite</p>\n<img src=\"/45ee1a6d/lm_infinite_design.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p>LLaMA-7BLLaMA2-7BMPT-7BGPT-J-6BLM-InfiniteMPT-7BAlibiRoPE</p>\n<ul>\n<li>Language Modeling</li>\n</ul>\n<p>ArXivOpenWebText2</p>\n<p>LM-Infinite0-12kPPL</p>\n<img src=\"/45ee1a6d/lm_infinite_ppl_figure.png\" class title=\"PPL\">\n<p>LLAMA210KNaN32KOOM</p>\n<p>PPLLM-InfinitePPL</p>\n<p>LM-InfiniteLM-Infinite +\nLlama2ArXiv200M\ntokenPPL200MPPL</p>\n<img src=\"/45ee1a6d/lm_infinite_ppl_200m.png\" class title=\"PPL 200M\">\n<ul>\n<li>Passkey Retrieval and Qapser</li>\n</ul>\n<p>Passkey\nRetrievalQapsertop-5middle\ntokenattention</p>\n<p>Passkey\nRetrieval0LM-Infinite</p>\n<img src=\"/45ee1a6d/lm_infinite_downstream.png\" class title=\"\">\n<ul>\n<li>Ablation study</li>\n</ul>\n<p>LM-Infinite-shaped attention maskDistance\nceiling</p>\n<img src=\"/45ee1a6d/lm_infinite_ablation.png\" class title=\"\">\n<p>-shaped attention maskdistance\nceilingPPL</p>\n<h1 id=\"transformer-xl\">Transformer-XL</h1>\n<p>Infini-TransformerTransformer-XL</p>\n<p>Transformer-XL20196CMUGoogle\nBrain</p>\n<p>transformersegmentsegment</p>\n<img src=\"/45ee1a6d/xl_vanilla_sw.png\" class title=\"vanilla transformer\">\n<p></p>\n<p>Transformer-XLattentionsegmentLsegment\n<span class=\"math inline\">\\([\\mathbf{s}_{\\tau}=x_{\\tau,1},\\cdots,x_{\\tau,L}]\\)</span>\n <span class=\"math inline\">\\(\\mathbf{s}_{\\tau+1}=[x_{\\tau+1,1},\\cdots,x_{\\tau+1,L}]\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}=\\left[\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\right]\\\\&amp;\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n=\\mathbf{h}_{\\tau+1}^{n-1}\\mathbf{W}_q^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_k^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_v^\\top\\\\&amp;\\mathbf{h}_{\\tau+1}^n=\\text{Transformer-Layer}\\left(\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n\\right)\\end{aligned}\\]</span></p>\n<p>SGstop\ngradient<span class=\"math inline\">\\(\\begin{bmatrix}\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\end{bmatrix}\\)</span>\nhqkv <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(n-1\\)</span> </p>\n<p>segmentattentionTransformer-XLsegmentKVsegmentsegmentsegment</p>\n<img src=\"/45ee1a6d/xl_attention.png\" class title=\"Transformer-XL\">\n<h1 id=\"infini-transformer\">Infini-Transformer</h1>\n<p>Leave No Context Behind:Efficient Infinite Context Transformers\nwith Infini-attention</p>\n<p>20244</p>\n<p>500k/1M</p>\n<p>/</p>\n<p>GoogleInfini-attention</p>\n<h2 id=\"-1\"></h2>\n<p></p>\n<p>transformer</p>\n<p>StreamingLLM</p>\n<p>Metalearned neural\nmemory/Enhancing the transformer with explicit relational encoding\nfor math problem solvingcompressive\nmemorycontextsimplicitytradeoff</p>\n<h2 id=\"infini-attention\">Infini-attention</h2>\n<p>Infini-TransformerInfini-attentionTransformer\nblockmask local attentionlong term\nlinear attention</p>\n<img src=\"/45ee1a6d/infini_attention_structure.png\" class title=\"infini-attention\">\n<p>Infini-attentionQKVQInfini-attentionconcat</p>\n<p>TransformerLLM</p>\n<p>Infini-attentionTransformer-XL</p>\n<img src=\"/45ee1a6d/infini_attention_process.png\" class title=\"infini-attention\">\n<p>Transformer-XLInfini-TransformersegmentTransformer-XLsegment</p>\n<p>Infini-Transformersegment</p>\n<p>compressive memorysimplicitycomputational\nefficiencyLearning associative inference using fast\nweight memorymemoryassociative matrix</p>\n<p></p>\n<ul>\n<li></li>\n</ul>\n<p>segment <span class=\"math inline\">\\(N\\)</span> <span class=\"math inline\">\\(Q\\in\\mathbf{R}^{N\\times d_{key}}\\)</span> memory\n<span class=\"math inline\">\\(M_{s-1}\\in\\mathbf{R}^{d_{key}\\times\nd_{value}}\\)</span> </p>\n<p><span class=\"math display\">\\[A_{\\text{mm}}=\\frac{\\sigma(Q)M_{s-1}}{\\sigma(Q)z_{s-1}}\\]</span></p>\n<p><span class=\"math inline\">\\(\\sigma\\)</span>element-wise\nELU + 1</p>\n<p><span class=\"math inline\">\\(z_{s-1}\\in\\mathbf{R}^{d_{key}}\\)</span>normalization\ntermnormalization termTransformers are rnns: Fast\nautoregressive transformers with linear\nattentionK</p>\n<ul>\n<li></li>\n</ul>\n<p>normalization\ntermmemory</p>\n<p><span class=\"math display\">\\[M_s\\leftarrow\nM_{s-1}+\\sigma(K)^TV\\]</span></p>\n<p><span class=\"math display\">\\[z_s\\leftarrow\nz_{s-1}+\\sum_{t=1}^N\\sigma(K_t)\\]</span></p>\n<p>memoryMetalearned neural memoryLearning\nassociative inference using fast weight memorydelta\nrule</p>\n<p><span class=\"math display\">\\[M_s\\leftarrow\nM_{s-1}+\\sigma(K)^T(V-\\frac{\\sigma(K)M_{s-1}}{\\sigma(K)z_{s-1}})\\]</span></p>\n<ul>\n<li>local attention</li>\n</ul>\n<p>segment <span class=\"math inline\">\\(A_{mem}\\)</span> local attention state <span class=\"math inline\">\\(A_{dot}\\)</span> </p>\n<p><span class=\"math display\">\\[A=sigmoid(\\beta)\\odot\nA_{mem}+(1-sigmoid(\\beta))\\odot A_{dot}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span>\n0segment101</p>\n<img src=\"/45ee1a6d/infini_attention_gating.png\" class title=\"gating\">\n<p>segment-level\nmemory</p>\n<img src=\"/45ee1a6d/infini_attention_compare.png\" class title=\"\">\n<h2 id=\"-1\"></h2>\n<p>segment <span class=\"math inline\">\\(N\\)</span>\n20483276816segment</p>\n<ul>\n<li></li>\n</ul>\n<p>Infini-TransformerTransformer-XL/Memorzing\nTransformer/RMTPG-19Arxivlanguage modeling</p>\n<img src=\"/45ee1a6d/infini_attention_language_modeling.png\" class title=\"\">\n<p>Infini-TransformerMemorizing\nTransformers1%</p>\n<p>100kInfini-TransformerPPL</p>\n<ul>\n<li></li>\n</ul>\n<p>1B<br>\n- batch size = 64 - step = 30k -  &gt; 4k - segment length =\n2k</p>\n<p>1Mpasskey\nretrievalzero-shotfine-tune</p>\n<img src=\"/45ee1a6d/infini_attention_passkey.png\" class title=\"passkey\">\n<p>Infini-Transformerlinear + delta1Mpasskey\nretrieval</p>\n<p>8B8k30k500kBookSum</p>\n<img src=\"/45ee1a6d/infini_attention_booksum.png\" class title=\"booksum\">\n<p>Infini-Transformerlinear + delta</p>\n<h1 id=\"\"></h1>\n<ol type=\"1\">\n<li>StreamingLM-Infiniteattention\nsinktokenPPLLM-Infinitedistance\nceilingtokenMPPLtoken<br>\n</li>\n<li>Infini-Transformer</li>\n</ol>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Efficient Streaming Language Models with Attention Sinks\nhttps://arxiv.org/abs/2309.17453<br>\n2Transformer-XL: Attentive Language Models Beyond a Fixed-Length\nContext https://arxiv.org/abs/1901.02860<br>\n3Leave No Context Behind: Efficient Infinite Context Transformers\nwith Infini-attention https://arxiv.org/abs/2404.07143<br>\n4LM-Infinite: Zero-Shot Extreme Length Generalization for Large\nLanguage Models https://arxiv.org/abs/2308.16137</p>\n","length":14036,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>2024Q2RAGAgent</p>\n<p>()128k+</p>\n<p></p>\n<h1 id=\"streamingllm\">StreamingLLM</h1>\n<p>Efficient Streaming Language Models with Attention Sinks</p>\n<p>20239</p>\n<p>4M</p>\n<p></p>\n<p>MITCMUNVIDIAMETAStreamingLLM\n-- LLAMA2, MPT,\nFalconPythia4MStreamingLLMPPL</p>\n<p>StreamingLLMPPL</p>\n<h2 id=\"\"></h2>\n<p><br>\n-\nKVKVGPU\nH100A10040G/80GKV<br>\n-\nRoPERoPE</p>\n<p><br>\n- Longformerwindow\nattentionKVwindow\nattentiontoken<br>\n- sliding window with\nrecomputationhttps://github.com/mit-han-lab/streaming-llm/issues/51KVrecomputation<br>\n-\nNTKYaRNNSFW()https://kaiokendev.github.io/til#extending-context-to-8k<br>\n- Efficiently scaling transformer\ninferenceSmoothQuant: Accurate and efficient post-training\nquantization for large language modelsDynamic context pruning for\nefficient and interpretable autoregressive transformersSpatten:\nEfficient sparse attention architecture with cascade token and head\npruningH2o: Heavyhitter oracle for efficient generative inference\nof large language models<br>\n-\nFlashAttention<br>\n- Big BirdLinformer</p>\n<p>StreamingLLMattention\nsinkwindow\nattentionattention\nsinkPPLsliding\nwindow with\nrecomputationStreamingLLM22+</p>\n<p>StreamingLLMPPLStreamingLLMPPL</p>\n<img src=\"/45ee1a6d/streamingllm_model_ppl.png\" class title=\"PPL\">\n<h2 id=\"attention-sink\">attention sink</h2>\n<p>window\nattentionLLMtoken</p>\n<img src=\"/45ee1a6d/stremingllm_attention_sink.png\" class title=\"attention sink\">\n<p>attention\nscoretokentoken</p>\n<p>tokenattention\nsinktoken</p>\n<p>softmaxsoftmaxtoken1tokentokentoken1</p>\n<p>tokentokentokentokenattention\nsink</p>\n<p>4tokentokenattention\nsinktoken</p>\n<p>quantSmoothQuant: Accurate and\nefficient post-training quantization for large language\nmodelsQuantizable transformers: Removing outliers by helping\nattention heads do nothing</p>\n<h2 id=\"\"></h2>\n<p>StreamingLLMStreamingLLMattention\nsink</p>\n<p>StreamingLLMwindow\nattentionattention sink\ntokenKVtokenKVtokenKV\ncachetokenStreamingLLMattention</p>\n<img src=\"/45ee1a6d/streamingllm_compare.png\" class title=\"StreamingLLM\">\n<p>token</p>\n<img src=\"/45ee1a6d/stremingllm_init_token_num.png\" class title=\"attention sink number\">\n<p>4tokenPPLtoken</p>\n<p>window attentionStreamingLLMKV\ncache<br>\n- 4tokenattention sink<br>\n- KV cache</p>\n<p>StreamingLLMKV</p>\n<img src=\"/45ee1a6d/stremingllm_kv_cache.png\" class title=\"cache\">\n<p>StreamingLLMdistancetokencachedistanceRoPEtokenKVtokenLM-Infinite</p>\n<p>attention\nsinktokenattention\nsink tokentoken</p>\n<p>tokensoftmaxsoftmax-off-by-oneattentionsoftmax</p>\n<p>softmax-off-by-one</p>\n<p><span class=\"math display\">\\[\\text{SoftMax}_1(x)_i=\\frac{e^{x_i}}{1+\\sum_{j=1}^Ne^{x_j}}\\]</span></p>\n<p>softmax-off-by-one1attention\nscore1KV0tokentoken1tokenattention\nsink</p>\n<p>tokenattention\nsinksoftmax-off-by-one3160M</p>\n<img src=\"/45ee1a6d/stremingllm_exp.png\" class title=\"\">\n<p>softmax-off-by-onezero\nsinktokenattention sink</p>\n<p>LLAMA2, MPT,\nFalconPythiaStreamingLLM4MMPTALIBIRoPE</p>\n<img src=\"/45ee1a6d/stremingllm_perf_4m.png\" class title=\"\">\n<p>4MStreamingLLMPPL</p>\n<p>StreamingLLMwindow\nattentioncacheStreamingLLMPPLStreamingLLM</p>\n<h1 id=\"lm-infinite\">LM-Infinite</h1>\n<p>LM-Infinite: Zero-Shot Extreme Length Generalization for Large\nLanguage Models</p>\n<p>20238</p>\n<p>2k4k200MPPL</p>\n<p></p>\n<p>LM-InfiniteLLMPasskey\nRetrievalQasper2.77.5</p>\n<h2 id=\"\"></h2>\n<p>LM-InfiniteTransformer\nLLM3</p>\n<ul>\n<li>1challenges in handling unseen distances among tokens</li>\n</ul>\n<p>tokentokenattention\nlogits</p>\n<p>LLAMA2ArXiv8kattention\nlogits</p>\n<img src=\"/45ee1a6d/lm_infinite_attention_logits_explode.png\" class title=\"logits\">\n<p>LLAMA24k4kattention\nlogits</p>\n<p>tokenlogits</p>\n<ul>\n<li>2attending to unseen numbers of tokens</li>\n</ul>\n<p>tokenlogits(attention\nentropy)</p>\n<p>tokentoken</p>\n<p>8k</p>\n<img src=\"/45ee1a6d/lm_infinite_attention_entropy.png\" class title=\"\">\n<p>attention context\nsize</p>\n<p>window\nattentiontokentokenhandle12XPosLongformer</p>\n<p>window attention</p>\n<ul>\n<li>3starting tokens occupy a distinct feature space</li>\n</ul>\n<p>token</p>\n<p>The impact of positional encoding on length\ngeneralization in\ntransformers1tokentokentoken</p>\n<p>StreamingLLMattention\nsinktokenwindow\nattentiontoken</p>\n<p>LLAMA2hidden state\noutputPCA2</p>\n<img src=\"/45ee1a6d/lm_infinite_starting_tokens.png\" class title=\"token\">\n<p>tokentokentokentokentokentoken</p>\n<p>token</p>\n<h2 id=\"\"></h2>\n<p>LM-InfiniteLLMzero-shotLM-Infinite-shaped\nattention maskDistance ceiling</p>\n<ul>\n<li>-shaped attention mask</li>\n</ul>\n<p>-shaped attention maskLongformerLongNetBig\nBird</p>\n<p>-shaped attention maskwindow\nattentiontokentokentoken1<span class=\"math inline\">\\(n_{\\mathrm{starting}}\\)</span>\ntoken2<span class=\"math inline\">\\(L_{\\mathrm{pretrain}}\\)</span>\ntoken<span class=\"math inline\">\\(n_{\\mathrm{starting}}\\)</span> <span class=\"math inline\">\\(L_{\\mathrm{pretrain}}\\)</span>\ntokentoken</p>\n<p> <span class=\"math inline\">\\(n_{\\mathrm{starting}}\\)</span>\n <span class=\"math inline\">\\(n_{\\mathrm{starting}}\\in[5,100]\\)</span>\n</p>\n<img src=\"/45ee1a6d/lm_infinite_starting_tokens_num.png\" class title=\"token\">\n<p></p>\n<p>-shaped attention mask23</p>\n<ul>\n<li>Distance ceiling</li>\n</ul>\n<p>LM-Infinite <span class=\"math inline\">\\(L_{\\mathrm{pretrain}}\\)</span>token</p>\n<p>attention logit <span class=\"math inline\">\\(w(\\mathbf{q},\\mathbf{k},d)\\)</span> <span class=\"math inline\">\\(d\\)</span> tokenDistance\nceilingattention logit</p>\n<p><span class=\"math display\">\\[\\text{attention\nlogits}=w(\\mathbf{q},\\mathbf{k},d&#39;)\\]</span></p>\n<p><span class=\"math display\">\\[d&#39;=\\min(d,L_\\text{pretrain})\\]</span></p>\n<p>Distance ceiling1</p>\n<ul>\n<li>Optionally attending to top-k tokens in the middle</li>\n</ul>\n<p>-shaped attention maskDistance\nceilingtokentokentokenattentiontoken</p>\n<p>token <span class=\"math inline\">\\(k\\)</span> attention\nlogitstokenattention  <span class=\"math inline\">\\(k\\)</span> token <span class=\"math inline\">\\(d=\\frac12L_\\text{pre-train}\\)</span></p>\n<p> <span class=\"math inline\">\\(k\\)</span> Passkey\nRetrievalvalidation set</p>\n<img src=\"/45ee1a6d/lm_infinite_middle_k.png\" class title=\"k\">\n<p> <span class=\"math inline\">\\(k=5\\)</span>&gt;5</p>\n<p>middle\ntokenmiddle\ntoken2</p>\n<p>LM-Infinite</p>\n<img src=\"/45ee1a6d/lm_infinite_design.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p>LLaMA-7BLLaMA2-7BMPT-7BGPT-J-6BLM-InfiniteMPT-7BAlibiRoPE</p>\n<ul>\n<li>Language Modeling</li>\n</ul>\n<p>ArXivOpenWebText2</p>\n<p>LM-Infinite0-12kPPL</p>\n<img src=\"/45ee1a6d/lm_infinite_ppl_figure.png\" class title=\"PPL\">\n<p>LLAMA210KNaN32KOOM</p>\n<p>PPLLM-InfinitePPL</p>\n<p>LM-InfiniteLM-Infinite +\nLlama2ArXiv200M\ntokenPPL200MPPL</p>\n<img src=\"/45ee1a6d/lm_infinite_ppl_200m.png\" class title=\"PPL 200M\">\n<ul>\n<li>Passkey Retrieval and Qapser</li>\n</ul>\n<p>Passkey\nRetrievalQapsertop-5middle\ntokenattention</p>\n<p>Passkey\nRetrieval0LM-Infinite</p>\n<img src=\"/45ee1a6d/lm_infinite_downstream.png\" class title=\"\">\n<ul>\n<li>Ablation study</li>\n</ul>\n<p>LM-Infinite-shaped attention maskDistance\nceiling</p>\n<img src=\"/45ee1a6d/lm_infinite_ablation.png\" class title=\"\">\n<p>-shaped attention maskdistance\nceilingPPL</p>\n<h1 id=\"transformer-xl\">Transformer-XL</h1>\n<p>Infini-TransformerTransformer-XL</p>\n<p>Transformer-XL20196CMUGoogle\nBrain</p>\n<p>transformersegmentsegment</p>\n<img src=\"/45ee1a6d/xl_vanilla_sw.png\" class title=\"vanilla transformer\">\n<p></p>\n<p>Transformer-XLattentionsegmentLsegment\n<span class=\"math inline\">\\([\\mathbf{s}_{\\tau}=x_{\\tau,1},\\cdots,x_{\\tau,L}]\\)</span>\n <span class=\"math inline\">\\(\\mathbf{s}_{\\tau+1}=[x_{\\tau+1,1},\\cdots,x_{\\tau+1,L}]\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}=\\left[\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\right]\\\\&amp;\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n=\\mathbf{h}_{\\tau+1}^{n-1}\\mathbf{W}_q^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_k^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_v^\\top\\\\&amp;\\mathbf{h}_{\\tau+1}^n=\\text{Transformer-Layer}\\left(\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n\\right)\\end{aligned}\\]</span></p>\n<p>SGstop\ngradient<span class=\"math inline\">\\(\\begin{bmatrix}\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\end{bmatrix}\\)</span>\nhqkv <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(n-1\\)</span> </p>\n<p>segmentattentionTransformer-XLsegmentKVsegmentsegmentsegment</p>\n<img src=\"/45ee1a6d/xl_attention.png\" class title=\"Transformer-XL\">\n<h1 id=\"infini-transformer\">Infini-Transformer</h1>\n<p>Leave No Context Behind:Efficient Infinite Context Transformers\nwith Infini-attention</p>\n<p>20244</p>\n<p>500k/1M</p>\n<p>/</p>\n<p>GoogleInfini-attention</p>\n<h2 id=\"-1\"></h2>\n<p></p>\n<p>transformer</p>\n<p>StreamingLLM</p>\n<p>Metalearned neural\nmemory/Enhancing the transformer with explicit relational encoding\nfor math problem solvingcompressive\nmemorycontextsimplicitytradeoff</p>\n<h2 id=\"infini-attention\">Infini-attention</h2>\n<p>Infini-TransformerInfini-attentionTransformer\nblockmask local attentionlong term\nlinear attention</p>\n<img src=\"/45ee1a6d/infini_attention_structure.png\" class title=\"infini-attention\">\n<p>Infini-attentionQKVQInfini-attentionconcat</p>\n<p>TransformerLLM</p>\n<p>Infini-attentionTransformer-XL</p>\n<img src=\"/45ee1a6d/infini_attention_process.png\" class title=\"infini-attention\">\n<p>Transformer-XLInfini-TransformersegmentTransformer-XLsegment</p>\n<p>Infini-Transformersegment</p>\n<p>compressive memorysimplicitycomputational\nefficiencyLearning associative inference using fast\nweight memorymemoryassociative matrix</p>\n<p></p>\n<ul>\n<li></li>\n</ul>\n<p>segment <span class=\"math inline\">\\(N\\)</span> <span class=\"math inline\">\\(Q\\in\\mathbf{R}^{N\\times d_{key}}\\)</span> memory\n<span class=\"math inline\">\\(M_{s-1}\\in\\mathbf{R}^{d_{key}\\times\nd_{value}}\\)</span> </p>\n<p><span class=\"math display\">\\[A_{\\text{mm}}=\\frac{\\sigma(Q)M_{s-1}}{\\sigma(Q)z_{s-1}}\\]</span></p>\n<p><span class=\"math inline\">\\(\\sigma\\)</span>element-wise\nELU + 1</p>\n<p><span class=\"math inline\">\\(z_{s-1}\\in\\mathbf{R}^{d_{key}}\\)</span>normalization\ntermnormalization termTransformers are rnns: Fast\nautoregressive transformers with linear\nattentionK</p>\n<ul>\n<li></li>\n</ul>\n<p>normalization\ntermmemory</p>\n<p><span class=\"math display\">\\[M_s\\leftarrow\nM_{s-1}+\\sigma(K)^TV\\]</span></p>\n<p><span class=\"math display\">\\[z_s\\leftarrow\nz_{s-1}+\\sum_{t=1}^N\\sigma(K_t)\\]</span></p>\n<p>memoryMetalearned neural memoryLearning\nassociative inference using fast weight memorydelta\nrule</p>\n<p><span class=\"math display\">\\[M_s\\leftarrow\nM_{s-1}+\\sigma(K)^T(V-\\frac{\\sigma(K)M_{s-1}}{\\sigma(K)z_{s-1}})\\]</span></p>\n<ul>\n<li>local attention</li>\n</ul>\n<p>segment <span class=\"math inline\">\\(A_{mem}\\)</span> local attention state <span class=\"math inline\">\\(A_{dot}\\)</span> </p>\n<p><span class=\"math display\">\\[A=sigmoid(\\beta)\\odot\nA_{mem}+(1-sigmoid(\\beta))\\odot A_{dot}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span>\n0segment101</p>\n<img src=\"/45ee1a6d/infini_attention_gating.png\" class title=\"gating\">\n<p>segment-level\nmemory</p>\n<img src=\"/45ee1a6d/infini_attention_compare.png\" class title=\"\">\n<h2 id=\"-1\"></h2>\n<p>segment <span class=\"math inline\">\\(N\\)</span>\n20483276816segment</p>\n<ul>\n<li></li>\n</ul>\n<p>Infini-TransformerTransformer-XL/Memorzing\nTransformer/RMTPG-19Arxivlanguage modeling</p>\n<img src=\"/45ee1a6d/infini_attention_language_modeling.png\" class title=\"\">\n<p>Infini-TransformerMemorizing\nTransformers1%</p>\n<p>100kInfini-TransformerPPL</p>\n<ul>\n<li></li>\n</ul>\n<p>1B<br>\n- batch size = 64 - step = 30k -  &gt; 4k - segment length =\n2k</p>\n<p>1Mpasskey\nretrievalzero-shotfine-tune</p>\n<img src=\"/45ee1a6d/infini_attention_passkey.png\" class title=\"passkey\">\n<p>Infini-Transformerlinear + delta1Mpasskey\nretrieval</p>\n<p>8B8k30k500kBookSum</p>\n<img src=\"/45ee1a6d/infini_attention_booksum.png\" class title=\"booksum\">\n<p>Infini-Transformerlinear + delta</p>\n<h1 id=\"\"></h1>\n<ol type=\"1\">\n<li>StreamingLM-Infiniteattention\nsinktokenPPLLM-Infinitedistance\nceilingtokenMPPLtoken<br>\n</li>\n<li>Infini-Transformer</li>\n</ol>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Efficient Streaming Language Models with Attention Sinks\nhttps://arxiv.org/abs/2309.17453<br>\n2Transformer-XL: Attentive Language Models Beyond a Fixed-Length\nContext https://arxiv.org/abs/1901.02860<br>\n3Leave No Context Behind: Efficient Infinite Context Transformers\nwith Infini-attention https://arxiv.org/abs/2404.07143<br>\n4LM-Infinite: Zero-Shot Extreme Length Generalization for Large\nLanguage Models https://arxiv.org/abs/2308.16137</p>\n"},{"title":"","abbrlink":"cc852861","date":"2024-05-04T11:05:48.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n2024Q2RAGAgent  \n\n()CoT/ToT  \n\n  \n\n# 128k  \n\nData Engineering for Scaling Language Models to 128K Context  \n\n20242  \n\n  \n\n128k\n\n##   \n\nPPL  \n\nvalidation datasetPPL Needle in a Haystack  \n\nPPL  \n\n{% asset_img eng_ppl.png PPL %}  \n\nLongLoRAMistralYaRN>100kGPT-4  \n\n{% asset_img eng_needle_comp.png  %}  \n\n##   \n\n<=4k128ke.g. <5B token  \n\n32k400B tokenEffective long-context scaling of foundation modelsXverse  \n\n  \n\nLLAMALLAMASlimPajama  \n\ngithub  \n\n  \n- Cut at 4K4k4kLLAMA  \n- Cut at 128K128kLongLoRA  \n- Per-source Upsampling  \n- Global Upsampling  \n- Upsample Arxiv/ Book/ Github  \n\n  \n\nSlimPajama  \n\n{% asset_img eng_data_dist.png  %}  \n\nPer-source Upsampling  \n\n##   \n\n80kLLAMA2-7B64kLLAMA2-13B  \n\nFlashAttentionAttentionGPUCPUGPUGPUconstantlinear80k4k3400  \n\njiaqian  \n\n{% asset_img add_money.jpg  %}  \n\nPer-source Upsampling  \n\n{% asset_img eng_data.png  %}  \n\n  \n\n{% asset_img eng_config.png  %}  \n\n  \n- lr = 2e-5  \n- RoPE base1,0000500,000  \n- batch size = 4M token  \n\n##   \n\ntoken  \n\n100M300M500M1B5B10B tokencheckpointPPL  \n\n{% asset_img eng_tokens.png  %}  \n\n500M token5B token10B token  \n\n##   \n\nLLAMA2-7B5B token  \n\nLLAMA24k0-4k4k-128k  \n\n  \n\n{% asset_img eng_sample.png  %}  \n\n  \n- 0-4kPer-source Upsampling  \n- BookGithub  \n- 4k-128kPer-source Upsampling  \n\nlength upsamplingPer-source Upsampling  \n\n80kLLAMA2-7BPer-source UpsamplingPPLPer-source Upsampling  \n\n{% asset_img eng_ppl.png PPL %}  \n\n## \n\n  \n- PPL  \n-   \n-   \n\n# Paraphrasing  \n\nTraining With \"Paraphrasing the Original Text\" Improves Long-Context Performance  \n\n202312  \n\n  \n\n50k  \n\n{% asset_img paraphrasing_intro.png paraphrasing %}  \n\n##   \n\n  \n\nNTKYaRN  \n\nlost in the middlemiddleretrieval  \n\n##   \n\n  \n- TogetherLLaMA-2-7B-32Khttps://huggingface.co/datasets/togethercomputer/Long-Data-CollectionsTogetherMultipassage-QA-from-Natural-QuestionsBookSum  \n- LongAlpacaLongLoRA: Efficient Fine-tuning of Long-Context Large Language Models  \n- Ziya-ReaderNever Lost in the Middle:Improving Large Language Models via Attention Strengthening Question Answering  \n\n  \n-   \n-   \n-   \n\npromptCoT  \n\npromptClaude-2.1promptHere is the most relevant sentence in the context27%98%https://www.anthropic.com/news/claude-2-1-prompting  \n\n  \n- LongLLMLinguaLongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression  \n- Attention SortingAttention Sorting Combats Recency Bias In Long Context Language Modelsdecode  \n\n##   \n\nretrieval relevancetokenn-gram $x$  $R(x)$   \n\n$$R(x)=\\frac{n^\\prime}n\\log\\frac N{N^\\prime+1}$$  \n\nTF-IDF$n^\\prime$  $x$ gold-chunk $n$ gold-chunktoken$N$ chunk$N^\\prime$ xchunk  \n\ntoken $x$  $R(x)$  $S$   \n\n$$\\mathcal{R}(S)=\\frac{1}{|S_a|}\\sum_{x\\in\\mathcal{S}_a}R(x)$$  \n\n $S_a$  $S$   \n\n $\\mathcal{R}(S)$ $\\mathcal{R}(S)$     \n\ngold-chunkparaphrasing the original text  \n\nparaphrasing  \n\n{% asset_img paraphrasing_example.png paraphrasing %}  \n\ntokentokenparaphrasing  \n\nGPT-4paraphrasing  \n\n{% asset_img paraphrasing_dataset.png  %}  \n\n10,8258,4548k32k  \n\n{% asset_img paraphrasing_dataset_dist.png  %}  \n\nMulti-passage-QA-from-NQ  \n\n{% asset_img paraphrasing_quality.png  %}  \n\nLongBench  \n\n{% asset_img paraphrasing_perf.png  %}  \n\nlost in the middle  \n\n{% asset_img paraphrasing_lost.png lost in the middle %}  \n\n# PoSE  \n\nPoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training  \n\n20239  \n\n  \n\n128k\n\n##   \n\nRoPE  \n\nNTKYaRN8k32k128k...  \n\n**Po**sitional **S**kip-wis**E**PoSE2k128k128k  \n\nRandPosRandomized positional encodings boost length generalization of transformersRandPostokenPoSEtoken  \n\n##   \n\nPoSE  \n- index128k1-128k  \n-   \n\n $L_c$ $N$ chunk $c_0,c_1,\\ldots,c_{N-1}$ $l_0,l_1,\\ldots,l_{N-1}$chunk $i$token  \n\n$$\\mathrm{Pos}(c_i)=\\{st_i,st_i+1,\\ldots,st_i+l_i-1\\},\\quad st_i=\\sum_{j=0}^{i-1}l_j$$  \n\nchunkuniform distribution $\\mathcal{U}(S)$ skipping bias $u_i$biaschunktoken  \n\n$$\\mathrm{PoSE}(c_i)=\\{u_i+st_i,u_i+st_i+1,\\ldots,u_i+st_i+l_i-1\\}$$\n\nchunkoverlap $u_i\\geq u_{i-1}$  \n\nskipping biassamplechunkskipping bias  \n\nindexchunkindex  \n\nchunktoken  \n\ntoken$v_i\\sim\\mathcal{U}(\\{v_{i-1},\\ldots,L_x-L_c\\})$ $c_i$ token  \n\n$$c_i=\\boldsymbol{x}[v_i+st_i:v_i+st_i+l_i]$$  \n\n $v_i=u_i$$v_i=0$  $v_i$   \n\n$N$ 2 $u_0$  $v_0$ 0  \n\nPoSE  \n\n{% asset_img pose_method.png PoSE %}  \n\nLLAMA-7B2k1,000batch size64lr=2e-5warmup step=10  \n\nPoSEPPLFull-length  \n\n{% asset_img pose_ppl.png PPL %}  \n\npasskey retrieval  \n\n{% asset_img pose_passkey.png passkey %}  \n\nPoSEPoSE1M  \n\n#   \n\n1. FlashAttention128k5Btoken  \n2.   \n3. \n4. M/PoSE/  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n# Reference  \n1Data Engineering for Scaling Language Models to 128K Context https://arxiv.org/abs/2402.10171  \n2Training With \"Paraphrasing the Original Text\" Improves Long-Context Performance https://arxiv.org/abs/2312.11193  \n3PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training https://arxiv.org/abs/2309.10400  \n","source":"_posts/cs/nlp/2024/05/.md","raw":"---\ntitle: \nabbrlink: cc852861\ndate: 2024-05-04 19:05:48\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \n  - \n  - attention\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n2024Q2RAGAgent  \n\n()CoT/ToT  \n\n  \n\n# 128k  \n\nData Engineering for Scaling Language Models to 128K Context  \n\n20242  \n\n  \n\n128k\n\n##   \n\nPPL  \n\nvalidation datasetPPL Needle in a Haystack  \n\nPPL  \n\n{% asset_img eng_ppl.png PPL %}  \n\nLongLoRAMistralYaRN>100kGPT-4  \n\n{% asset_img eng_needle_comp.png  %}  \n\n##   \n\n<=4k128ke.g. <5B token  \n\n32k400B tokenEffective long-context scaling of foundation modelsXverse  \n\n  \n\nLLAMALLAMASlimPajama  \n\ngithub  \n\n  \n- Cut at 4K4k4kLLAMA  \n- Cut at 128K128kLongLoRA  \n- Per-source Upsampling  \n- Global Upsampling  \n- Upsample Arxiv/ Book/ Github  \n\n  \n\nSlimPajama  \n\n{% asset_img eng_data_dist.png  %}  \n\nPer-source Upsampling  \n\n##   \n\n80kLLAMA2-7B64kLLAMA2-13B  \n\nFlashAttentionAttentionGPUCPUGPUGPUconstantlinear80k4k3400  \n\njiaqian  \n\n{% asset_img add_money.jpg  %}  \n\nPer-source Upsampling  \n\n{% asset_img eng_data.png  %}  \n\n  \n\n{% asset_img eng_config.png  %}  \n\n  \n- lr = 2e-5  \n- RoPE base1,0000500,000  \n- batch size = 4M token  \n\n##   \n\ntoken  \n\n100M300M500M1B5B10B tokencheckpointPPL  \n\n{% asset_img eng_tokens.png  %}  \n\n500M token5B token10B token  \n\n##   \n\nLLAMA2-7B5B token  \n\nLLAMA24k0-4k4k-128k  \n\n  \n\n{% asset_img eng_sample.png  %}  \n\n  \n- 0-4kPer-source Upsampling  \n- BookGithub  \n- 4k-128kPer-source Upsampling  \n\nlength upsamplingPer-source Upsampling  \n\n80kLLAMA2-7BPer-source UpsamplingPPLPer-source Upsampling  \n\n{% asset_img eng_ppl.png PPL %}  \n\n## \n\n  \n- PPL  \n-   \n-   \n\n# Paraphrasing  \n\nTraining With \"Paraphrasing the Original Text\" Improves Long-Context Performance  \n\n202312  \n\n  \n\n50k  \n\n{% asset_img paraphrasing_intro.png paraphrasing %}  \n\n##   \n\n  \n\nNTKYaRN  \n\nlost in the middlemiddleretrieval  \n\n##   \n\n  \n- TogetherLLaMA-2-7B-32Khttps://huggingface.co/datasets/togethercomputer/Long-Data-CollectionsTogetherMultipassage-QA-from-Natural-QuestionsBookSum  \n- LongAlpacaLongLoRA: Efficient Fine-tuning of Long-Context Large Language Models  \n- Ziya-ReaderNever Lost in the Middle:Improving Large Language Models via Attention Strengthening Question Answering  \n\n  \n-   \n-   \n-   \n\npromptCoT  \n\npromptClaude-2.1promptHere is the most relevant sentence in the context27%98%https://www.anthropic.com/news/claude-2-1-prompting  \n\n  \n- LongLLMLinguaLongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression  \n- Attention SortingAttention Sorting Combats Recency Bias In Long Context Language Modelsdecode  \n\n##   \n\nretrieval relevancetokenn-gram $x$  $R(x)$   \n\n$$R(x)=\\frac{n^\\prime}n\\log\\frac N{N^\\prime+1}$$  \n\nTF-IDF$n^\\prime$  $x$ gold-chunk $n$ gold-chunktoken$N$ chunk$N^\\prime$ xchunk  \n\ntoken $x$  $R(x)$  $S$   \n\n$$\\mathcal{R}(S)=\\frac{1}{|S_a|}\\sum_{x\\in\\mathcal{S}_a}R(x)$$  \n\n $S_a$  $S$   \n\n $\\mathcal{R}(S)$ $\\mathcal{R}(S)$     \n\ngold-chunkparaphrasing the original text  \n\nparaphrasing  \n\n{% asset_img paraphrasing_example.png paraphrasing %}  \n\ntokentokenparaphrasing  \n\nGPT-4paraphrasing  \n\n{% asset_img paraphrasing_dataset.png  %}  \n\n10,8258,4548k32k  \n\n{% asset_img paraphrasing_dataset_dist.png  %}  \n\nMulti-passage-QA-from-NQ  \n\n{% asset_img paraphrasing_quality.png  %}  \n\nLongBench  \n\n{% asset_img paraphrasing_perf.png  %}  \n\nlost in the middle  \n\n{% asset_img paraphrasing_lost.png lost in the middle %}  \n\n# PoSE  \n\nPoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training  \n\n20239  \n\n  \n\n128k\n\n##   \n\nRoPE  \n\nNTKYaRN8k32k128k...  \n\n**Po**sitional **S**kip-wis**E**PoSE2k128k128k  \n\nRandPosRandomized positional encodings boost length generalization of transformersRandPostokenPoSEtoken  \n\n##   \n\nPoSE  \n- index128k1-128k  \n-   \n\n $L_c$ $N$ chunk $c_0,c_1,\\ldots,c_{N-1}$ $l_0,l_1,\\ldots,l_{N-1}$chunk $i$token  \n\n$$\\mathrm{Pos}(c_i)=\\{st_i,st_i+1,\\ldots,st_i+l_i-1\\},\\quad st_i=\\sum_{j=0}^{i-1}l_j$$  \n\nchunkuniform distribution $\\mathcal{U}(S)$ skipping bias $u_i$biaschunktoken  \n\n$$\\mathrm{PoSE}(c_i)=\\{u_i+st_i,u_i+st_i+1,\\ldots,u_i+st_i+l_i-1\\}$$\n\nchunkoverlap $u_i\\geq u_{i-1}$  \n\nskipping biassamplechunkskipping bias  \n\nindexchunkindex  \n\nchunktoken  \n\ntoken$v_i\\sim\\mathcal{U}(\\{v_{i-1},\\ldots,L_x-L_c\\})$ $c_i$ token  \n\n$$c_i=\\boldsymbol{x}[v_i+st_i:v_i+st_i+l_i]$$  \n\n $v_i=u_i$$v_i=0$  $v_i$   \n\n$N$ 2 $u_0$  $v_0$ 0  \n\nPoSE  \n\n{% asset_img pose_method.png PoSE %}  \n\nLLAMA-7B2k1,000batch size64lr=2e-5warmup step=10  \n\nPoSEPPLFull-length  \n\n{% asset_img pose_ppl.png PPL %}  \n\npasskey retrieval  \n\n{% asset_img pose_passkey.png passkey %}  \n\nPoSEPoSE1M  \n\n#   \n\n1. FlashAttention128k5Btoken  \n2.   \n3. \n4. M/PoSE/  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n# Reference  \n1Data Engineering for Scaling Language Models to 128K Context https://arxiv.org/abs/2402.10171  \n2Training With \"Paraphrasing the Original Text\" Improves Long-Context Performance https://arxiv.org/abs/2312.11193  \n3PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training https://arxiv.org/abs/2309.10400  \n","slug":"cs/nlp/2024/05/","published":1,"updated":"2024-05-10T06:50:20.731Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmfg000gam4k1bflb3wt","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>2024Q2RAGAgent</p>\n<p>()CoT/ToT</p>\n<p></p>\n<h1 id=\"128k\">128k</h1>\n<p>Data Engineering for Scaling Language Models to 128K\nContext</p>\n<p>20242</p>\n<p></p>\n<p>128k</p>\n<h2 id=\"\"></h2>\n<p>PPL</p>\n<p>validation\ndatasetPPL\nNeedle in a\nHaystack</p>\n<p>PPL</p>\n<img src=\"/cc852861/eng_ppl.png\" class title=\"PPL\">\n<p>LongLoRAMistralYaRN&gt;100kGPT-4</p>\n<img src=\"/cc852861/eng_needle_comp.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p>&lt;=4k128ke.g.\n&lt;5B token</p>\n<p>32k400B\ntokenEffective long-context scaling of foundation\nmodelsXverse</p>\n<p></p>\n<p>LLAMALLAMASlimPajama</p>\n<p>github</p>\n<p><br>\n- Cut at\n4K4k4kLLAMA<br>\n- Cut at\n128K128kLongLoRA<br>\n- Per-source\nUpsampling<br>\n- Global Upsampling<br>\n- Upsample Arxiv/ Book/\nGithub</p>\n<p></p>\n<p>SlimPajama</p>\n<img src=\"/cc852861/eng_data_dist.png\" class title=\"\">\n<p>Per-source\nUpsampling</p>\n<h2 id=\"\"></h2>\n<p>80kLLAMA2-7B64kLLAMA2-13B</p>\n<p>FlashAttentionAttentionGPUCPUGPUGPUconstantlinear80k4k3400</p>\n<p>jiaqian</p>\n<img src=\"/cc852861/add_money.jpg\" class title=\"\">\n<p>Per-source Upsampling</p>\n<img src=\"/cc852861/eng_data.png\" class title=\"\">\n<p></p>\n<img src=\"/cc852861/eng_config.png\" class title=\"\">\n<p><br>\n- lr = 2e-5<br>\n- RoPE base1,0000500,000<br>\n- batch size = 4M token</p>\n<h2 id=\"\"></h2>\n<p>token</p>\n<p>100M300M500M1B5B10B\ntokencheckpointPPL</p>\n<img src=\"/cc852861/eng_tokens.png\" class title=\"\">\n<p>500M\ntoken5B\ntoken10B\ntoken</p>\n<h2 id=\"\"></h2>\n<p>LLAMA2-7B5B\ntoken</p>\n<p>LLAMA24k0-4k4k-128k</p>\n<p></p>\n<img src=\"/cc852861/eng_sample.png\" class title=\"\">\n<p><br>\n- 0-4kPer-source\nUpsampling<br>\n-\nBookGithub<br>\n- 4k-128kPer-source\nUpsampling</p>\n<p>length upsamplingPer-source\nUpsampling</p>\n<p>80kLLAMA2-7BPer-source\nUpsamplingPPLPer-source\nUpsampling</p>\n<img src=\"/cc852861/eng_ppl.png\" class title=\"PPL\">\n<h2 id=\"\"></h2>\n<p><br>\n-\nPPL<br>\n- <br>\n- </p>\n<h1 id=\"paraphrasing\">Paraphrasing</h1>\n<p>Training With \"Paraphrasing the Original Text\" Improves\nLong-Context Performance</p>\n<p>202312</p>\n<p></p>\n<p>50k</p>\n<img src=\"/cc852861/paraphrasing_intro.png\" class title=\"paraphrasing\">\n<h2 id=\"\"></h2>\n<p></p>\n<p>NTKYaRN</p>\n<p>lost in the\nmiddlemiddleretrieval</p>\n<h2 id=\"\"></h2>\n<p><br>\n-\nTogetherLLaMA-2-7B-32Khttps://huggingface.co/datasets/togethercomputer/Long-Data-CollectionsTogetherMultipassage-QA-from-Natural-QuestionsBookSum<br>\n- LongAlpacaLongLoRA: Efficient Fine-tuning of Long-Context Large\nLanguage Models<br>\n- Ziya-ReaderNever Lost in the Middle:Improving Large Language\nModels via Attention Strengthening Question Answering</p>\n<p><br>\n- <br>\n- <br>\n-\n</p>\n<p>promptCoT</p>\n<p>promptClaude-2.1promptHere\nis the most relevant sentence in the\ncontext27%98%https://www.anthropic.com/news/claude-2-1-prompting</p>\n<p><br>\n- LongLLMLinguaLongLLMLingua: Accelerating and Enhancing LLMs in\nLong Context Scenarios via Prompt\nCompression<br>\n- Attention SortingAttention Sorting Combats Recency Bias In Long\nContext Language\nModelsdecode</p>\n<h2 id=\"\"></h2>\n<p>retrieval\nrelevancetokenn-gram <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(R(x)\\)</span> </p>\n<p><span class=\"math display\">\\[R(x)=\\frac{n^\\prime}n\\log\\frac\nN{N^\\prime+1}\\]</span></p>\n<p>TF-IDF<span class=\"math inline\">\\(n^\\prime\\)</span>  <span class=\"math inline\">\\(x\\)</span> gold-chunk <span class=\"math inline\">\\(n\\)</span> gold-chunktoken<span class=\"math inline\">\\(N\\)</span> chunk<span class=\"math inline\">\\(N^\\prime\\)</span> xchunk</p>\n<p>token <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(R(x)\\)</span>  <span class=\"math inline\">\\(S\\)</span> </p>\n<p><span class=\"math display\">\\[\\mathcal{R}(S)=\\frac{1}{|S_a|}\\sum_{x\\in\\mathcal{S}_a}R(x)\\]</span></p>\n<p> <span class=\"math inline\">\\(S_a\\)</span>  <span class=\"math inline\">\\(S\\)</span> </p>\n<p> <span class=\"math inline\">\\(\\mathcal{R}(S)\\)</span>\n<span class=\"math inline\">\\(\\mathcal{R}(S)\\)</span>\n</p>\n<p>gold-chunkparaphrasing\nthe original text</p>\n<p>paraphrasing</p>\n<img src=\"/cc852861/paraphrasing_example.png\" class title=\"paraphrasing\">\n<p>tokentokenparaphrasing</p>\n<p>GPT-4paraphrasing</p>\n<img src=\"/cc852861/paraphrasing_dataset.png\" class title=\"\">\n<p>10,8258,4548k32k</p>\n<img src=\"/cc852861/paraphrasing_dataset_dist.png\" class title=\"\">\n<p>Multi-passage-QA-from-NQ</p>\n<img src=\"/cc852861/paraphrasing_quality.png\" class title=\"\">\n<p>LongBench</p>\n<img src=\"/cc852861/paraphrasing_perf.png\" class title=\"\">\n<p>lost in the\nmiddle</p>\n<img src=\"/cc852861/paraphrasing_lost.png\" class title=\"lost in the middle\">\n<h1 id=\"pose\">PoSE</h1>\n<p>PoSE: Efficient Context Window Extension of LLMs via Positional\nSkip-wise Training</p>\n<p>20239</p>\n<p></p>\n<p>128k</p>\n<h2 id=\"\"></h2>\n<p>RoPE</p>\n<p>NTKYaRN8k32k128k...</p>\n<p><strong>Po</strong>sitional\n<strong>S</strong>kip-wis<strong>E</strong>PoSE2k128k128k</p>\n<p>RandPosRandomized\npositional encodings boost length generalization of\ntransformersRandPostokenPoSEtoken</p>\n<h2 id=\"\"></h2>\n<p>PoSE<br>\n-\nindex128k1-128k<br>\n-\n</p>\n<p> <span class=\"math inline\">\\(L_c\\)</span> <span class=\"math inline\">\\(N\\)</span> chunk <span class=\"math inline\">\\(c_0,c_1,\\ldots,c_{N-1}\\)</span> <span class=\"math inline\">\\(l_0,l_1,\\ldots,l_{N-1}\\)</span>chunk <span class=\"math inline\">\\(i\\)</span>token</p>\n<p><span class=\"math display\">\\[\\mathrm{Pos}(c_i)=\\{st_i,st_i+1,\\ldots,st_i+l_i-1\\},\\quad\nst_i=\\sum_{j=0}^{i-1}l_j\\]</span></p>\n<p>chunkuniform distribution <span class=\"math inline\">\\(\\mathcal{U}(S)\\)</span> skipping\nbias <span class=\"math inline\">\\(u_i\\)</span>biaschunktoken</p>\n<p><span class=\"math display\">\\[\\mathrm{PoSE}(c_i)=\\{u_i+st_i,u_i+st_i+1,\\ldots,u_i+st_i+l_i-1\\}\\]</span></p>\n<p>chunkoverlap\n<span class=\"math inline\">\\(u_i\\geq u_{i-1}\\)</span></p>\n<p>skipping\nbiassamplechunkskipping\nbias</p>\n<p>indexchunkindex</p>\n<p>chunktoken</p>\n<p>token<span class=\"math inline\">\\(v_i\\sim\\mathcal{U}(\\{v_{i-1},\\ldots,L_x-L_c\\})\\)</span>\n<span class=\"math inline\">\\(c_i\\)</span> token</p>\n<p><span class=\"math display\">\\[c_i=\\boldsymbol{x}[v_i+st_i:v_i+st_i+l_i]\\]</span></p>\n<p> <span class=\"math inline\">\\(v_i=u_i\\)</span><span class=\"math inline\">\\(v_i=0\\)</span>\n <span class=\"math inline\">\\(v_i\\)</span> </p>\n<p><span class=\"math inline\">\\(N\\)</span>\n2 <span class=\"math inline\">\\(u_0\\)</span>  <span class=\"math inline\">\\(v_0\\)</span> 0</p>\n<p>PoSE</p>\n<img src=\"/cc852861/pose_method.png\" class title=\"PoSE\">\n<p>LLAMA-7B2k1,000batch\nsize64lr=2e-5warmup step=10</p>\n<p>PoSEPPLFull-length</p>\n<img src=\"/cc852861/pose_ppl.png\" class title=\"PPL\">\n<p>passkey retrieval</p>\n<img src=\"/cc852861/pose_passkey.png\" class title=\"passkey\">\n<p>PoSEPoSE1M</p>\n<h1 id=\"\"></h1>\n<ol type=\"1\">\n<li>FlashAttention128k5Btoken<br>\n</li>\n<li><br>\n</li>\n<li></li>\n<li>M/PoSE/</li>\n</ol>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Data Engineering for Scaling Language Models to 128K Context\nhttps://arxiv.org/abs/2402.10171<br>\n2Training With \"Paraphrasing the Original Text\" Improves\nLong-Context Performance https://arxiv.org/abs/2312.11193<br>\n3PoSE: Efficient Context Window Extension of LLMs via Positional\nSkip-wise Training https://arxiv.org/abs/2309.10400</p>\n","length":8097,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>2024Q2RAGAgent</p>\n<p>()CoT/ToT</p>\n<p></p>\n<h1 id=\"128k\">128k</h1>\n<p>Data Engineering for Scaling Language Models to 128K\nContext</p>\n<p>20242</p>\n<p></p>\n<p>128k</p>\n<h2 id=\"\"></h2>\n<p>PPL</p>\n<p>validation\ndatasetPPL\nNeedle in a\nHaystack</p>\n<p>PPL</p>\n<img src=\"/cc852861/eng_ppl.png\" class title=\"PPL\">\n<p>LongLoRAMistralYaRN&gt;100kGPT-4</p>\n<img src=\"/cc852861/eng_needle_comp.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p>&lt;=4k128ke.g.\n&lt;5B token</p>\n<p>32k400B\ntokenEffective long-context scaling of foundation\nmodelsXverse</p>\n<p></p>\n<p>LLAMALLAMASlimPajama</p>\n<p>github</p>\n<p><br>\n- Cut at\n4K4k4kLLAMA<br>\n- Cut at\n128K128kLongLoRA<br>\n- Per-source\nUpsampling<br>\n- Global Upsampling<br>\n- Upsample Arxiv/ Book/\nGithub</p>\n<p></p>\n<p>SlimPajama</p>\n<img src=\"/cc852861/eng_data_dist.png\" class title=\"\">\n<p>Per-source\nUpsampling</p>\n<h2 id=\"\"></h2>\n<p>80kLLAMA2-7B64kLLAMA2-13B</p>\n<p>FlashAttentionAttentionGPUCPUGPUGPUconstantlinear80k4k3400</p>\n<p>jiaqian</p>\n<img src=\"/cc852861/add_money.jpg\" class title=\"\">\n<p>Per-source Upsampling</p>\n<img src=\"/cc852861/eng_data.png\" class title=\"\">\n<p></p>\n<img src=\"/cc852861/eng_config.png\" class title=\"\">\n<p><br>\n- lr = 2e-5<br>\n- RoPE base1,0000500,000<br>\n- batch size = 4M token</p>\n<h2 id=\"\"></h2>\n<p>token</p>\n<p>100M300M500M1B5B10B\ntokencheckpointPPL</p>\n<img src=\"/cc852861/eng_tokens.png\" class title=\"\">\n<p>500M\ntoken5B\ntoken10B\ntoken</p>\n<h2 id=\"\"></h2>\n<p>LLAMA2-7B5B\ntoken</p>\n<p>LLAMA24k0-4k4k-128k</p>\n<p></p>\n<img src=\"/cc852861/eng_sample.png\" class title=\"\">\n<p><br>\n- 0-4kPer-source\nUpsampling<br>\n-\nBookGithub<br>\n- 4k-128kPer-source\nUpsampling</p>\n<p>length upsamplingPer-source\nUpsampling</p>\n<p>80kLLAMA2-7BPer-source\nUpsamplingPPLPer-source\nUpsampling</p>\n<img src=\"/cc852861/eng_ppl.png\" class title=\"PPL\">\n<h2 id=\"\"></h2>\n<p><br>\n-\nPPL<br>\n- <br>\n- </p>\n<h1 id=\"paraphrasing\">Paraphrasing</h1>\n<p>Training With \"Paraphrasing the Original Text\" Improves\nLong-Context Performance</p>\n<p>202312</p>\n<p></p>\n<p>50k</p>\n<img src=\"/cc852861/paraphrasing_intro.png\" class title=\"paraphrasing\">\n<h2 id=\"\"></h2>\n<p></p>\n<p>NTKYaRN</p>\n<p>lost in the\nmiddlemiddleretrieval</p>\n<h2 id=\"\"></h2>\n<p><br>\n-\nTogetherLLaMA-2-7B-32Khttps://huggingface.co/datasets/togethercomputer/Long-Data-CollectionsTogetherMultipassage-QA-from-Natural-QuestionsBookSum<br>\n- LongAlpacaLongLoRA: Efficient Fine-tuning of Long-Context Large\nLanguage Models<br>\n- Ziya-ReaderNever Lost in the Middle:Improving Large Language\nModels via Attention Strengthening Question Answering</p>\n<p><br>\n- <br>\n- <br>\n-\n</p>\n<p>promptCoT</p>\n<p>promptClaude-2.1promptHere\nis the most relevant sentence in the\ncontext27%98%https://www.anthropic.com/news/claude-2-1-prompting</p>\n<p><br>\n- LongLLMLinguaLongLLMLingua: Accelerating and Enhancing LLMs in\nLong Context Scenarios via Prompt\nCompression<br>\n- Attention SortingAttention Sorting Combats Recency Bias In Long\nContext Language\nModelsdecode</p>\n<h2 id=\"\"></h2>\n<p>retrieval\nrelevancetokenn-gram <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(R(x)\\)</span> </p>\n<p><span class=\"math display\">\\[R(x)=\\frac{n^\\prime}n\\log\\frac\nN{N^\\prime+1}\\]</span></p>\n<p>TF-IDF<span class=\"math inline\">\\(n^\\prime\\)</span>  <span class=\"math inline\">\\(x\\)</span> gold-chunk <span class=\"math inline\">\\(n\\)</span> gold-chunktoken<span class=\"math inline\">\\(N\\)</span> chunk<span class=\"math inline\">\\(N^\\prime\\)</span> xchunk</p>\n<p>token <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(R(x)\\)</span>  <span class=\"math inline\">\\(S\\)</span> </p>\n<p><span class=\"math display\">\\[\\mathcal{R}(S)=\\frac{1}{|S_a|}\\sum_{x\\in\\mathcal{S}_a}R(x)\\]</span></p>\n<p> <span class=\"math inline\">\\(S_a\\)</span>  <span class=\"math inline\">\\(S\\)</span> </p>\n<p> <span class=\"math inline\">\\(\\mathcal{R}(S)\\)</span>\n<span class=\"math inline\">\\(\\mathcal{R}(S)\\)</span>\n</p>\n<p>gold-chunkparaphrasing\nthe original text</p>\n<p>paraphrasing</p>\n<img src=\"/cc852861/paraphrasing_example.png\" class title=\"paraphrasing\">\n<p>tokentokenparaphrasing</p>\n<p>GPT-4paraphrasing</p>\n<img src=\"/cc852861/paraphrasing_dataset.png\" class title=\"\">\n<p>10,8258,4548k32k</p>\n<img src=\"/cc852861/paraphrasing_dataset_dist.png\" class title=\"\">\n<p>Multi-passage-QA-from-NQ</p>\n<img src=\"/cc852861/paraphrasing_quality.png\" class title=\"\">\n<p>LongBench</p>\n<img src=\"/cc852861/paraphrasing_perf.png\" class title=\"\">\n<p>lost in the\nmiddle</p>\n<img src=\"/cc852861/paraphrasing_lost.png\" class title=\"lost in the middle\">\n<h1 id=\"pose\">PoSE</h1>\n<p>PoSE: Efficient Context Window Extension of LLMs via Positional\nSkip-wise Training</p>\n<p>20239</p>\n<p></p>\n<p>128k</p>\n<h2 id=\"\"></h2>\n<p>RoPE</p>\n<p>NTKYaRN8k32k128k...</p>\n<p><strong>Po</strong>sitional\n<strong>S</strong>kip-wis<strong>E</strong>PoSE2k128k128k</p>\n<p>RandPosRandomized\npositional encodings boost length generalization of\ntransformersRandPostokenPoSEtoken</p>\n<h2 id=\"\"></h2>\n<p>PoSE<br>\n-\nindex128k1-128k<br>\n-\n</p>\n<p> <span class=\"math inline\">\\(L_c\\)</span> <span class=\"math inline\">\\(N\\)</span> chunk <span class=\"math inline\">\\(c_0,c_1,\\ldots,c_{N-1}\\)</span> <span class=\"math inline\">\\(l_0,l_1,\\ldots,l_{N-1}\\)</span>chunk <span class=\"math inline\">\\(i\\)</span>token</p>\n<p><span class=\"math display\">\\[\\mathrm{Pos}(c_i)=\\{st_i,st_i+1,\\ldots,st_i+l_i-1\\},\\quad\nst_i=\\sum_{j=0}^{i-1}l_j\\]</span></p>\n<p>chunkuniform distribution <span class=\"math inline\">\\(\\mathcal{U}(S)\\)</span> skipping\nbias <span class=\"math inline\">\\(u_i\\)</span>biaschunktoken</p>\n<p><span class=\"math display\">\\[\\mathrm{PoSE}(c_i)=\\{u_i+st_i,u_i+st_i+1,\\ldots,u_i+st_i+l_i-1\\}\\]</span></p>\n<p>chunkoverlap\n<span class=\"math inline\">\\(u_i\\geq u_{i-1}\\)</span></p>\n<p>skipping\nbiassamplechunkskipping\nbias</p>\n<p>indexchunkindex</p>\n<p>chunktoken</p>\n<p>token<span class=\"math inline\">\\(v_i\\sim\\mathcal{U}(\\{v_{i-1},\\ldots,L_x-L_c\\})\\)</span>\n<span class=\"math inline\">\\(c_i\\)</span> token</p>\n<p><span class=\"math display\">\\[c_i=\\boldsymbol{x}[v_i+st_i:v_i+st_i+l_i]\\]</span></p>\n<p> <span class=\"math inline\">\\(v_i=u_i\\)</span><span class=\"math inline\">\\(v_i=0\\)</span>\n <span class=\"math inline\">\\(v_i\\)</span> </p>\n<p><span class=\"math inline\">\\(N\\)</span>\n2 <span class=\"math inline\">\\(u_0\\)</span>  <span class=\"math inline\">\\(v_0\\)</span> 0</p>\n<p>PoSE</p>\n<img src=\"/cc852861/pose_method.png\" class title=\"PoSE\">\n<p>LLAMA-7B2k1,000batch\nsize64lr=2e-5warmup step=10</p>\n<p>PoSEPPLFull-length</p>\n<img src=\"/cc852861/pose_ppl.png\" class title=\"PPL\">\n<p>passkey retrieval</p>\n<img src=\"/cc852861/pose_passkey.png\" class title=\"passkey\">\n<p>PoSEPoSE1M</p>\n<h1 id=\"\"></h1>\n<ol type=\"1\">\n<li>FlashAttention128k5Btoken<br>\n</li>\n<li><br>\n</li>\n<li></li>\n<li>M/PoSE/</li>\n</ol>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Data Engineering for Scaling Language Models to 128K Context\nhttps://arxiv.org/abs/2402.10171<br>\n2Training With \"Paraphrasing the Original Text\" Improves\nLong-Context Performance https://arxiv.org/abs/2312.11193<br>\n3PoSE: Efficient Context Window Extension of LLMs via Positional\nSkip-wise Training https://arxiv.org/abs/2309.10400</p>\n"},{"title":"LLM","abbrlink":"c4da56c0","date":"2024-02-28T07:19:28.000Z","_content":"\n  \n\nRoPERoPE[](http://www.linsight.cn/a051710f.html) [](https://zhuanlan.zhihu.com/p/684072868) [](https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&mid=2247483759&idx=1&sn=f7b59b879476b8687a340606b5568eae&chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&token=88551061&lang=zh_CN#rd)\n\n#   \n\n2023LLM20235Claude100k tokens67ChatGPT3.516kChatGLM2-B32k  \n\nChatGLMAgentChatGLM3ChatGLM4  \n\nLM-SYSLongChatMosaicLMMPT16k\n\nQwen-1.532k  \n\n<center>\n\n|  |  |\n| :----: | :----: |\n| Baichuan2 | 192k |\n| GPT4-turbo  | 128k |\n| Yi | 200k |\n| Kimi Chat | 128k(20) |\n| Claude2 | 200k |  \n\n</center>\n\n  \n\n  \n\n#   \n\ntokenizertokentoken>1.5tokenizer2200ktoken30w  \n\n27  \n\n<big><u>****</u></big>  \n\nRAGRetrieval-augmented generationRAG  \n\n<big><u>****</u></big>prompt  \n\nprompt  \n\n1ppl2attention\n\n# \n\n  \n\n2k/4k8k16kPPLRoPE<u>****</u>  \n\n## \n\n2k/4k8k/16k/32k+  \n\n  \n\n1.  \n\n32k  \n\n4k8attention maskattention mask  \n\n>  \n\n2.  \n\ntransformer  \n\n $l$  $V$ hidden size $h$ batch size $b$  $s$ Adam1  \n\n(1) \n\n $\\Phi$  =  + $l$ * decoder = $Vh + l(12h^2 + 13h)$  \n\n $s$   \n\n(2)   \n\n = logits + $l$ *  $\\approx2bshV + l*(24bsh^2+4bs^2h)$\n\nsoftmaxsoftmax $s$ \n\n\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\frac{}{}\n&=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n$$\n\n $s << h$  $h$ 1k1w $s$  $sh$   \n\n(3)   \n\noptimizer\n\n$\\Phi$$\\Phi$$2\\Phi$ $(\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi + 2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]$ \n\n{% asset_img mix_precision_fp16.png  %}  \n\n\n\n  \n\nsoftmaxdropout  \n\nattention $x$  $QKV$  $x$  $QK$  $QK$ softmax $QK^T$  $11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2$  $l$  $l$   \n\n $s$ 4k32k64GPUbatch sizegradient accumulation<big><u>****</u></big>  \n\n2B7B16k32k200k34B70B+  \n\n2k4k  \n\n  \n\n##  Position Interpolation\n\n236Meta[EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION](https://arxiv.org/pdf/2306.15595.pdf)RoPEPIPosition Interpolation2k32k1kstep\n\n{% asset_img meta_pi.png PI %}  \n{% asset_img LLM/meta_pi.png PI %}  \n\n>In contrast, LLaMA models that are extended via direct fine-tuning only saw a minimal increase of the effective context window size kmax from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear indication of an acceleration in the increase of window size.  \n\n1w20482560\n\nRoPE  \n\nRoPERoPE $\\left|m-n \\right|$ <2048attention $\\left|m-n \\right|$ \n\n{% asset_img meta_rope_ext.png RoPE %}  \n\n3000attention score\n\n\n\nPI  \n\n{% asset_img meta_pi_nosft.png PI %}  \n\n2k2k2k4k\n\n{% asset_img meta_pi_explanation.png PI %}  \n\n123...11.522.5...0.5  \n\nattention score $\\tilde{a}(s)=a(Ls/L^{\\prime})$ $L$ 2048$L^{\\prime}$ 8k/16k/32k\n\nRoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}'(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L'}\\right)\n\\end{aligned}\n\\end{equation}\n$$\n\n $m$ 1 ${L}/{L'}$  \n\n  \n\n\n\n\n\n## NTK-Aware Interpolation \n\ncosNTK-Aware InterpolationRoPE<u>****</u>NTK-Aware Scaled RoPECodeLlama1M  \n\nNTKNeural Tangent KernelGLM4  \n\n>Neural Tangent Kernel (NTK) NTK   \n Neural Tangent Kernel  \nNTK   \nNTK \n\nNTK  \n\n  \n\nRoPE $m$   \n\n{% asset_img rope_matrix.png RoPE %}  \n\n22 $d/2$  $\\theta_j=10000^{-2j/d}$  $j$ $j$  $base=10000$  $base$   \n\n  \n\n  \n\n[RoPE](https://www.zhihu.com/people/us4ever)2  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle= \\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n$$  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n $h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*$ $s=m-n$   \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n $\\alpha=L'/L>1$  $s$   \n\nNTK-Aware Scaled RoPE $\\theta_j$ baseRoPE10000  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n $\\theta$  $\\alpha^{\\frac{-2j}{d-2}}$  $j$  $\\alpha^{\\frac{-2j}{d-2}}$ 1 $j$  $j$ 0 $d/2 - 1$$\\alpha^{\\frac{-2j}{d-2}}$  $\\alpha^{-1}$ \n\n[](https://zhuanlan.zhihu.com/p/645770522)NTK-Aware Interpolation  \n\n>RoPE 12 3 60  RoPE 1/60  1/60 4 RoPE NTK-Aware RoPE  1.5  2  90  24  129.6k  43.2k   \n\nRoPE[](https://kexue.fm/archives/9675)  \n\nYaRN[](https://arxiv.org/pdf/2309.00071.pdf)NTK  \n\n>Given the results from [6], this method performs much better at extending the context size of non-finetuned models compared to PI [9]. However, one major disadvantage of this method is that given it is not just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\" interpolation [6] yields inferior results to PI [9]. Furthermore, due to the \"out-of-bound\" values, the theoretical scale factor s does not accurately describe the true context extension scale. In practice, the scale value s has to be set higher than the expected scale for a given context length extension.  \n\nNTK4k32k $\\alpha=L'/L$ 816\n\n## NTK-by-parts\n\nNTK-by-partsNTKNTK-awareRoPENTK-by-parts  \n\n $j$ RoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n$\\lambda_j$  $j$   \n\n $j$  $L$ RoPE $sin$ 1/40~1-1~0 $j$   \n\nNTK-by-parts  \n\n-  $j$  $\\lambda_j$    \n-  $\\lambda_j\\geq$   \n- NTK-aware interpolation  \n\n $r(j)=\\frac{L}{\\lambda_j}$  $\\beta_1\\beta_2$  $r(j)<\\beta_1$  $r(j)\\geq \\beta_2$   \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&if&r(j)<\\beta_1\\\\1&if&r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n$$  \n\nNTK-by-parts $\\theta_j$   \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n$$\n\n $\\beta_1\\beta_2$  $\\beta_1=1\\beta_2=32$ 1/32   \n\n## Dynamically NTK Scaled RoPE  \n\nNTK-Aware InterpolationRoPEattention score $l$  $L$  $\\alpha$ baseDynamically NTK Scaled RoPENTK  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n$$  \n\n $l$  $l>L$  $\\alpha$ 1 $l\\leq L$   \n\nkv-cacheRoPE  \n\n## YaRN  \n\ntokensoftmaxRoPEtoken  \n\nRoPEsoftmaxlogitsoftmax $t>1$ RoPE $\\sqrt{t}$ RoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n$$  \n\nLlama 1Llama 2$\\begin{aligned}\\sqrt{\\frac1t}&=0.1\\ln(\\alpha)+1.\\end{aligned}$Llama  \n\nYaRNNTK-by-partsattention score  \n\nYaRN\n\n## logn  \n\nlognattention $\\sqrt{d}$ logn[](https://zhuanlan.zhihu.com/p/678755776)YaRN  \n\ntokentokentokenattention score  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L'}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n$$  \n\n $L'>L$ YaRN\n\n## \n\nwindow attentionstreaming LLMLongLoRAFocus Transformer\n\n#   \n\n2k4k  \n\n-   \n- token  \n\nattention score  \n\nPINTKNTKlognYaRN  \n\n# Reference  \n1transformerKV cache https://zhuanlan.zhihu.com/p/624740065  \n2EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION https://arxiv.org/pdf/2306.15595.pdf  \n3Transformer10RoPE https://kexue.fm/archives/9675  \n4YaRN: Efficient Context Window Extension of Large\nLanguage Models https://arxiv.org/pdf/2309.00071.pdf  \n5RoPE https://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ  \n6LLM https://zhuanlan.zhihu.com/p/645770522\n7prompt https://cloud.tencent.com/developer/article/2330611  \n8Transformer8 https://spaces.ac.cn/archives/9444  \n9RoPE192K https://zhuanlan.zhihu.com/p/678755776\n***\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)\n","source":"_posts/cs/nlp/2024/02/LLM.md","raw":"---\ntitle: LLM\nabbrlink: c4da56c0\ndate: 2024-02-28 15:19:28\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  \n\nRoPERoPE[](http://www.linsight.cn/a051710f.html) [](https://zhuanlan.zhihu.com/p/684072868) [](https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&mid=2247483759&idx=1&sn=f7b59b879476b8687a340606b5568eae&chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&token=88551061&lang=zh_CN#rd)\n\n#   \n\n2023LLM20235Claude100k tokens67ChatGPT3.516kChatGLM2-B32k  \n\nChatGLMAgentChatGLM3ChatGLM4  \n\nLM-SYSLongChatMosaicLMMPT16k\n\nQwen-1.532k  \n\n<center>\n\n|  |  |\n| :----: | :----: |\n| Baichuan2 | 192k |\n| GPT4-turbo  | 128k |\n| Yi | 200k |\n| Kimi Chat | 128k(20) |\n| Claude2 | 200k |  \n\n</center>\n\n  \n\n  \n\n#   \n\ntokenizertokentoken>1.5tokenizer2200ktoken30w  \n\n27  \n\n<big><u>****</u></big>  \n\nRAGRetrieval-augmented generationRAG  \n\n<big><u>****</u></big>prompt  \n\nprompt  \n\n1ppl2attention\n\n# \n\n  \n\n2k/4k8k16kPPLRoPE<u>****</u>  \n\n## \n\n2k/4k8k/16k/32k+  \n\n  \n\n1.  \n\n32k  \n\n4k8attention maskattention mask  \n\n>  \n\n2.  \n\ntransformer  \n\n $l$  $V$ hidden size $h$ batch size $b$  $s$ Adam1  \n\n(1) \n\n $\\Phi$  =  + $l$ * decoder = $Vh + l(12h^2 + 13h)$  \n\n $s$   \n\n(2)   \n\n = logits + $l$ *  $\\approx2bshV + l*(24bsh^2+4bs^2h)$\n\nsoftmaxsoftmax $s$ \n\n\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\frac{}{}\n&=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n$$\n\n $s << h$  $h$ 1k1w $s$  $sh$   \n\n(3)   \n\noptimizer\n\n$\\Phi$$\\Phi$$2\\Phi$ $(\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi + 2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]$ \n\n{% asset_img mix_precision_fp16.png  %}  \n\n\n\n  \n\nsoftmaxdropout  \n\nattention $x$  $QKV$  $x$  $QK$  $QK$ softmax $QK^T$  $11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2$  $l$  $l$   \n\n $s$ 4k32k64GPUbatch sizegradient accumulation<big><u>****</u></big>  \n\n2B7B16k32k200k34B70B+  \n\n2k4k  \n\n  \n\n##  Position Interpolation\n\n236Meta[EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION](https://arxiv.org/pdf/2306.15595.pdf)RoPEPIPosition Interpolation2k32k1kstep\n\n{% asset_img meta_pi.png PI %}  \n{% asset_img LLM/meta_pi.png PI %}  \n\n>In contrast, LLaMA models that are extended via direct fine-tuning only saw a minimal increase of the effective context window size kmax from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear indication of an acceleration in the increase of window size.  \n\n1w20482560\n\nRoPE  \n\nRoPERoPE $\\left|m-n \\right|$ <2048attention $\\left|m-n \\right|$ \n\n{% asset_img meta_rope_ext.png RoPE %}  \n\n3000attention score\n\n\n\nPI  \n\n{% asset_img meta_pi_nosft.png PI %}  \n\n2k2k2k4k\n\n{% asset_img meta_pi_explanation.png PI %}  \n\n123...11.522.5...0.5  \n\nattention score $\\tilde{a}(s)=a(Ls/L^{\\prime})$ $L$ 2048$L^{\\prime}$ 8k/16k/32k\n\nRoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}'(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L'}\\right)\n\\end{aligned}\n\\end{equation}\n$$\n\n $m$ 1 ${L}/{L'}$  \n\n  \n\n\n\n\n\n## NTK-Aware Interpolation \n\ncosNTK-Aware InterpolationRoPE<u>****</u>NTK-Aware Scaled RoPECodeLlama1M  \n\nNTKNeural Tangent KernelGLM4  \n\n>Neural Tangent Kernel (NTK) NTK   \n Neural Tangent Kernel  \nNTK   \nNTK \n\nNTK  \n\n  \n\nRoPE $m$   \n\n{% asset_img rope_matrix.png RoPE %}  \n\n22 $d/2$  $\\theta_j=10000^{-2j/d}$  $j$ $j$  $base=10000$  $base$   \n\n  \n\n  \n\n[RoPE](https://www.zhihu.com/people/us4ever)2  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle= \\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n$$  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n $h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*$ $s=m-n$   \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n $\\alpha=L'/L>1$  $s$   \n\nNTK-Aware Scaled RoPE $\\theta_j$ baseRoPE10000  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n $\\theta$  $\\alpha^{\\frac{-2j}{d-2}}$  $j$  $\\alpha^{\\frac{-2j}{d-2}}$ 1 $j$  $j$ 0 $d/2 - 1$$\\alpha^{\\frac{-2j}{d-2}}$  $\\alpha^{-1}$ \n\n[](https://zhuanlan.zhihu.com/p/645770522)NTK-Aware Interpolation  \n\n>RoPE 12 3 60  RoPE 1/60  1/60 4 RoPE NTK-Aware RoPE  1.5  2  90  24  129.6k  43.2k   \n\nRoPE[](https://kexue.fm/archives/9675)  \n\nYaRN[](https://arxiv.org/pdf/2309.00071.pdf)NTK  \n\n>Given the results from [6], this method performs much better at extending the context size of non-finetuned models compared to PI [9]. However, one major disadvantage of this method is that given it is not just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\" interpolation [6] yields inferior results to PI [9]. Furthermore, due to the \"out-of-bound\" values, the theoretical scale factor s does not accurately describe the true context extension scale. In practice, the scale value s has to be set higher than the expected scale for a given context length extension.  \n\nNTK4k32k $\\alpha=L'/L$ 816\n\n## NTK-by-parts\n\nNTK-by-partsNTKNTK-awareRoPENTK-by-parts  \n\n $j$ RoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n$\\lambda_j$  $j$   \n\n $j$  $L$ RoPE $sin$ 1/40~1-1~0 $j$   \n\nNTK-by-parts  \n\n-  $j$  $\\lambda_j$    \n-  $\\lambda_j\\geq$   \n- NTK-aware interpolation  \n\n $r(j)=\\frac{L}{\\lambda_j}$  $\\beta_1\\beta_2$  $r(j)<\\beta_1$  $r(j)\\geq \\beta_2$   \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&if&r(j)<\\beta_1\\\\1&if&r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n$$  \n\nNTK-by-parts $\\theta_j$   \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n$$\n\n $\\beta_1\\beta_2$  $\\beta_1=1\\beta_2=32$ 1/32   \n\n## Dynamically NTK Scaled RoPE  \n\nNTK-Aware InterpolationRoPEattention score $l$  $L$  $\\alpha$ baseDynamically NTK Scaled RoPENTK  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n$$  \n\n $l$  $l>L$  $\\alpha$ 1 $l\\leq L$   \n\nkv-cacheRoPE  \n\n## YaRN  \n\ntokensoftmaxRoPEtoken  \n\nRoPEsoftmaxlogitsoftmax $t>1$ RoPE $\\sqrt{t}$ RoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n$$  \n\nLlama 1Llama 2$\\begin{aligned}\\sqrt{\\frac1t}&=0.1\\ln(\\alpha)+1.\\end{aligned}$Llama  \n\nYaRNNTK-by-partsattention score  \n\nYaRN\n\n## logn  \n\nlognattention $\\sqrt{d}$ logn[](https://zhuanlan.zhihu.com/p/678755776)YaRN  \n\ntokentokentokenattention score  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L'}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n$$  \n\n $L'>L$ YaRN\n\n## \n\nwindow attentionstreaming LLMLongLoRAFocus Transformer\n\n#   \n\n2k4k  \n\n-   \n- token  \n\nattention score  \n\nPINTKNTKlognYaRN  \n\n# Reference  \n1transformerKV cache https://zhuanlan.zhihu.com/p/624740065  \n2EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION https://arxiv.org/pdf/2306.15595.pdf  \n3Transformer10RoPE https://kexue.fm/archives/9675  \n4YaRN: Efficient Context Window Extension of Large\nLanguage Models https://arxiv.org/pdf/2309.00071.pdf  \n5RoPE https://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ  \n6LLM https://zhuanlan.zhihu.com/p/645770522\n7prompt https://cloud.tencent.com/developer/article/2330611  \n8Transformer8 https://spaces.ac.cn/archives/9444  \n9RoPE192K https://zhuanlan.zhihu.com/p/678755776\n***\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)\n","slug":"cs/nlp/2024/02/LLM","published":1,"updated":"2024-03-13T07:23:38.942Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmfg000ham4k57oldeuh","content":"<p></p>\n<p>RoPERoPE<a href=\"http://www.linsight.cn/a051710f.html\"></a> <a href=\"https://zhuanlan.zhihu.com/p/684072868\"></a> <a href=\"https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&amp;mid=2247483759&amp;idx=1&amp;sn=f7b59b879476b8687a340606b5568eae&amp;chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&amp;token=88551061&amp;lang=zh_CN#rd\"></a></p>\n<h1 id=\"\"></h1>\n<p>2023LLM20235Claude100k\ntokens67ChatGPT3.516kChatGLM2-B32k</p>\n<p>ChatGLMAgentChatGLM3ChatGLM4</p>\n<p>LM-SYSLongChatMosaicLMMPT16k</p>\n<p>Qwen-1.532k</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Baichuan2</td>\n<td style=\"text-align: center;\">192k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">GPT4-turbo</td>\n<td style=\"text-align: center;\">128k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Yi</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Kimi Chat</td>\n<td style=\"text-align: center;\">128k(20)</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Claude2</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p></p>\n<p></p>\n<h1 id=\"\"></h1>\n<p>tokenizertokentoken&gt;1.5tokenizer2200ktoken30w</p>\n<p>27</p>\n<p><big><u><strong></strong></u></big></p>\n<p>RAGRetrieval-augmented\ngenerationRAG</p>\n<p><big><u><strong></strong></u></big>prompt</p>\n<p>prompt</p>\n<p>1ppl2attention</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>2k/4k8k16kPPLRoPE<u><strong></strong></u></p>\n<h2 id=\"\"></h2>\n<p>2k/4k8k/16k/32k+</p>\n<p></p>\n<p>1.</p>\n<p>32k</p>\n<p>4k8attention\nmaskattention\nmask</p>\n<p>&gt;</p>\n<p>2.</p>\n<p>transformer</p>\n<p> <span class=\"math inline\">\\(l\\)</span> \n<span class=\"math inline\">\\(V\\)</span> hidden size <span class=\"math inline\">\\(h\\)</span> batch size <span class=\"math inline\">\\(b\\)</span>  <span class=\"math inline\">\\(s\\)</span>\nAdam1</p>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p> <span class=\"math inline\">\\(\\Phi\\)</span> = \n+ <span class=\"math inline\">\\(l\\)</span> * decoder = <span class=\"math inline\">\\(Vh + l(12h^2 + 13h)\\)</span></p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p> = logits + <span class=\"math inline\">\\(l\\)</span> *  <span class=\"math inline\">\\(\\approx2bshV + l*(24bsh^2+4bs^2h)\\)</span></p>\n<p>softmaxsoftmax <span class=\"math inline\">\\(s\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{}{}\n&amp;=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&amp;\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(s &lt;&lt; h\\)</span>\n <span class=\"math inline\">\\(h\\)</span> 1k1w\n<span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(sh\\)</span>\n</p>\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>optimizer</p>\n<p><span class=\"math inline\">\\(\\Phi\\)</span><span class=\"math inline\">\\(\\Phi\\)</span><span class=\"math inline\">\\(2\\Phi\\)</span>\n<span class=\"math inline\">\\((\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi +\n2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]\\)</span>\n</p>\n<img src=\"/c4da56c0/mix_precision_fp16.png\" class title=\"\">\n<p></p>\n<p></p>\n<p>softmaxdropout</p>\n<p>attention <span class=\"math inline\">\\(x\\)</span>\n <span class=\"math inline\">\\(QKV\\)</span>  <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(QK\\)</span>  <span class=\"math inline\">\\(QK\\)</span> softmax\n<span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2\\)</span> \n<span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(l\\)</span> </p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n4k32k64GPUbatch\nsizegradient\naccumulation<big><u><strong></strong></u></big></p>\n<p>2B7B16k32k200k34B70B+</p>\n<p>2k4k</p>\n<p></p>\n<h2 id=\"-position-interpolation\"> Position\nInterpolation</h2>\n<p>236Meta<a href=\"https://arxiv.org/pdf/2306.15595.pdf\">EXTENDING CONTEXT WINDOW\nOF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION</a>RoPEPIPosition\nInterpolation2k32k1kstep</p>\n<img src=\"/c4da56c0/meta_pi.png\" class title=\"PI\">\n\n<blockquote>\n<p>In contrast, LLaMA models that are extended via direct fine-tuning\nonly saw a minimal increase of the effective context window size kmax\nfrom 2048 to 2560, even after fine-tuning for more than 10000 steps,\nwith no clear indication of an acceleration in the increase of window\nsize.</p>\n</blockquote>\n<p>1w20482560</p>\n<p>RoPE</p>\n<p>RoPERoPE\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n&lt;2048attention\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n</p>\n<img src=\"/c4da56c0/meta_rope_ext.png\" class title=\"RoPE\">\n<p>3000attention\nscore</p>\n<p></p>\n<p>PI</p>\n<img src=\"/c4da56c0/meta_pi_nosft.png\" class title=\"PI\">\n<p>2k2k2k4k</p>\n<img src=\"/c4da56c0/meta_pi_explanation.png\" class title=\"PI\">\n<p>123...11.522.5...0.5</p>\n<p>attention\nscore <span class=\"math inline\">\\(\\tilde{a}(s)=a(Ls/L^{\\prime})\\)</span> <span class=\"math inline\">\\(L\\)</span> 2048<span class=\"math inline\">\\(L^{\\prime}\\)</span>\n8k/16k/32k</p>\n<p>RoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}&#39;(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L&#39;}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(m\\)</span> 1\n<span class=\"math inline\">\\({L}/{L&#39;}\\)</span></p>\n<p></p>\n<p></p>\n<p></p>\n<h2 id=\"ntk-aware-interpolation\">NTK-Aware Interpolation</h2>\n<p>cosNTK-Aware\nInterpolationRoPE<u><strong></strong></u>NTK-Aware\nScaled RoPECodeLlama1M</p>\n<p>NTKNeural Tangent\nKernelGLM4</p>\n<blockquote>\n<p>Neural Tangent Kernel (NTK)\nNTK\n<br>\n\nNeural Tangent\nKernel<br>\nNTK\n<br>\nNTK\n</p>\n</blockquote>\n<p>NTK</p>\n<p></p>\n<p>RoPE <span class=\"math inline\">\\(m\\)</span>\n</p>\n<img src=\"/c4da56c0/rope_matrix.png\" class title=\"RoPE\">\n<p>22 <span class=\"math inline\">\\(d/2\\)</span>\n\n<span class=\"math inline\">\\(\\theta_j=10000^{-2j/d}\\)</span> \n<span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(base=10000\\)</span>  <span class=\"math inline\">\\(base\\)</span>\n</p>\n<p></p>\n<p></p>\n<p><a href=\"https://www.zhihu.com/people/us4ever\">RoPE</a>2</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=\n\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&amp;\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*\\)</span>\n<span class=\"math inline\">\\(s=m-n\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha=L&#39;/L&gt;1\\)</span>\n <span class=\"math inline\">\\(s\\)</span> </p>\n<p>NTK-Aware Scaled RoPE <span class=\"math inline\">\\(\\theta_j\\)</span>\nbaseRoPE10000</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\theta\\)</span>  <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>  <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>\n1 <span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(j\\)</span> 0\n<span class=\"math inline\">\\(d/2 - 1\\)</span><span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>  <span class=\"math inline\">\\(\\alpha^{-1}\\)</span> </p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/645770522\"></a>NTK-Aware\nInterpolation</p>\n<blockquote>\n<p>RoPE\n12 3 60 \nRoPE 1/60 \n1/60 4 RoPE\nNTK-Aware\nRoPE  1.5\n 2  90  24\n 129.6k  43.2k\n</p>\n</blockquote>\n<p>RoPE<a href=\"https://kexue.fm/archives/9675\"></a></p>\n<p>YaRN<a href=\"https://arxiv.org/pdf/2309.00071.pdf\"></a>NTK</p>\n<blockquote>\n<p>Given the results from [6], this method performs much better at\nextending the context size of non-finetuned models compared to PI [9].\nHowever, one major disadvantage of this method is that given it is not\njust an interpolation scheme, some dimensions are slightly extrapolated\nto \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\"\ninterpolation [6] yields inferior results to PI [9]. Furthermore, due to\nthe \"out-of-bound\" values, the theoretical scale factor s does not\naccurately describe the true context extension scale. In practice, the\nscale value s has to be set higher than the expected scale for a given\ncontext length extension.</p>\n</blockquote>\n<p>NTK4k32k\n<span class=\"math inline\">\\(\\alpha=L&#39;/L\\)</span>\n816</p>\n<h2 id=\"ntk-by-parts\">NTK-by-parts</h2>\n<p>NTK-by-partsNTKNTK-awareRoPENTK-by-parts</p>\n<p> <span class=\"math inline\">\\(j\\)</span> RoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p><span class=\"math inline\">\\(\\lambda_j\\)</span>\n <span class=\"math inline\">\\(j\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(L\\)</span>\nRoPE\n<span class=\"math inline\">\\(sin\\)</span>\n1/40<sub>1-1</sub>0\n<span class=\"math inline\">\\(j\\)</span>\n</p>\n<p>NTK-by-parts</p>\n<ul>\n<li> <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(\\lambda_j\\)</span> \n<br>\n</li>\n<li> <span class=\"math inline\">\\(\\lambda_j\\geq\\)</span>\n<br>\n</li>\n<li>NTK-aware interpolation</li>\n</ul>\n<p> <span class=\"math inline\">\\(r(j)=\\frac{L}{\\lambda_j}\\)</span>\n <span class=\"math inline\">\\(\\beta_1\\beta_2\\)</span> \n<span class=\"math inline\">\\(r(j)&lt;\\beta_1\\)</span>\n <span class=\"math inline\">\\(r(j)\\geq\n\\beta_2\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&amp;if&amp;r(j)&lt;\\beta_1\\\\1&amp;if&amp;r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&amp;otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>NTK-by-parts <span class=\"math inline\">\\(\\theta_j\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\beta_1\\beta_2\\)</span>\n <span class=\"math inline\">\\(\\beta_1=1\\beta_2=32\\)</span>\n1/32</p>\n<h2 id=\"dynamically-ntk-scaled-rope\">Dynamically NTK Scaled RoPE</h2>\n<p>NTK-Aware\nInterpolationRoPEattention\nscore <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(L\\)</span>  <span class=\"math inline\">\\(\\alpha\\)</span>\nbaseDynamically NTK Scaled\nRoPENTK</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(l\\)</span> \n<span class=\"math inline\">\\(l&gt;L\\)</span>  <span class=\"math inline\">\\(\\alpha\\)</span> 1 <span class=\"math inline\">\\(l\\leq L\\)</span> </p>\n<p>kv-cacheRoPE</p>\n<h2 id=\"yarn\">YaRN</h2>\n<p>tokensoftmaxRoPEtoken</p>\n<p>RoPEsoftmaxlogitsoftmax\n<span class=\"math inline\">\\(t&gt;1\\)</span>\nRoPE <span class=\"math inline\">\\(\\sqrt{t}\\)</span>\nRoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>Llama 1Llama 2<span class=\"math inline\">\\(\\begin{aligned}\\sqrt{\\frac1t}&amp;=0.1\\ln(\\alpha)+1.\\end{aligned}\\)</span>Llama</p>\n<p>YaRNNTK-by-partsattention\nscore</p>\n<p>YaRN</p>\n<h2 id=\"logn\">logn</h2>\n<p>lognattention <span class=\"math inline\">\\(\\sqrt{d}\\)</span>\nlogn<a href=\"https://zhuanlan.zhihu.com/p/678755776\"></a>YaRN</p>\n<p>tokentokentokenattention\nscore</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L&#39;}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(L&#39;&gt;L\\)</span>\nYaRN</p>\n<h2 id=\"\"></h2>\n<p>window\nattentionstreaming LLMLongLoRAFocus\nTransformer</p>\n<h1 id=\"\"></h1>\n<p>2k4k</p>\n<ul>\n<li><br>\n</li>\n<li>token</li>\n</ul>\n<p>attention score</p>\n<p>PINTKNTKlognYaRN</p>\n<h1 id=\"reference\">Reference</h1>\n<p>1transformerKV cache\nhttps://zhuanlan.zhihu.com/p/624740065<br>\n2EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION https://arxiv.org/pdf/2306.15595.pdf<br>\n3Transformer10RoPE\nhttps://kexue.fm/archives/9675<br>\n4YaRN: Efficient Context Window Extension of Large Language Models\nhttps://arxiv.org/pdf/2309.00071.pdf<br>\n5RoPE\nhttps://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ<br>\n6LLM https://zhuanlan.zhihu.com/p/645770522\n7prompt\nhttps://cloud.tencent.com/developer/article/2330611<br>\n8Transformer8\nhttps://spaces.ac.cn/archives/9444<br>\n9RoPE192K\nhttps://zhuanlan.zhihu.com/p/678755776 ***</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n","length":12689,"excerpt":"","more":"<p></p>\n<p>RoPERoPE<a href=\"http://www.linsight.cn/a051710f.html\"></a> <a href=\"https://zhuanlan.zhihu.com/p/684072868\"></a> <a href=\"https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&amp;mid=2247483759&amp;idx=1&amp;sn=f7b59b879476b8687a340606b5568eae&amp;chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&amp;token=88551061&amp;lang=zh_CN#rd\"></a></p>\n<h1 id=\"\"></h1>\n<p>2023LLM20235Claude100k\ntokens67ChatGPT3.516kChatGLM2-B32k</p>\n<p>ChatGLMAgentChatGLM3ChatGLM4</p>\n<p>LM-SYSLongChatMosaicLMMPT16k</p>\n<p>Qwen-1.532k</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Baichuan2</td>\n<td style=\"text-align: center;\">192k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">GPT4-turbo</td>\n<td style=\"text-align: center;\">128k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Yi</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Kimi Chat</td>\n<td style=\"text-align: center;\">128k(20)</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Claude2</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p></p>\n<p></p>\n<h1 id=\"\"></h1>\n<p>tokenizertokentoken&gt;1.5tokenizer2200ktoken30w</p>\n<p>27</p>\n<p><big><u><strong></strong></u></big></p>\n<p>RAGRetrieval-augmented\ngenerationRAG</p>\n<p><big><u><strong></strong></u></big>prompt</p>\n<p>prompt</p>\n<p>1ppl2attention</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>2k/4k8k16kPPLRoPE<u><strong></strong></u></p>\n<h2 id=\"\"></h2>\n<p>2k/4k8k/16k/32k+</p>\n<p></p>\n<p>1.</p>\n<p>32k</p>\n<p>4k8attention\nmaskattention\nmask</p>\n<p>&gt;</p>\n<p>2.</p>\n<p>transformer</p>\n<p> <span class=\"math inline\">\\(l\\)</span> \n<span class=\"math inline\">\\(V\\)</span> hidden size <span class=\"math inline\">\\(h\\)</span> batch size <span class=\"math inline\">\\(b\\)</span>  <span class=\"math inline\">\\(s\\)</span>\nAdam1</p>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p> <span class=\"math inline\">\\(\\Phi\\)</span> = \n+ <span class=\"math inline\">\\(l\\)</span> * decoder = <span class=\"math inline\">\\(Vh + l(12h^2 + 13h)\\)</span></p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p> = logits + <span class=\"math inline\">\\(l\\)</span> *  <span class=\"math inline\">\\(\\approx2bshV + l*(24bsh^2+4bs^2h)\\)</span></p>\n<p>softmaxsoftmax <span class=\"math inline\">\\(s\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{}{}\n&amp;=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&amp;\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(s &lt;&lt; h\\)</span>\n <span class=\"math inline\">\\(h\\)</span> 1k1w\n<span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(sh\\)</span>\n</p>\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>optimizer</p>\n<p><span class=\"math inline\">\\(\\Phi\\)</span><span class=\"math inline\">\\(\\Phi\\)</span><span class=\"math inline\">\\(2\\Phi\\)</span>\n<span class=\"math inline\">\\((\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi +\n2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]\\)</span>\n</p>\n<img src=\"/c4da56c0/mix_precision_fp16.png\" class title=\"\">\n<p></p>\n<p></p>\n<p>softmaxdropout</p>\n<p>attention <span class=\"math inline\">\\(x\\)</span>\n <span class=\"math inline\">\\(QKV\\)</span>  <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(QK\\)</span>  <span class=\"math inline\">\\(QK\\)</span> softmax\n<span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2\\)</span> \n<span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(l\\)</span> </p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n4k32k64GPUbatch\nsizegradient\naccumulation<big><u><strong></strong></u></big></p>\n<p>2B7B16k32k200k34B70B+</p>\n<p>2k4k</p>\n<p></p>\n<h2 id=\"-position-interpolation\"> Position\nInterpolation</h2>\n<p>236Meta<a href=\"https://arxiv.org/pdf/2306.15595.pdf\">EXTENDING CONTEXT WINDOW\nOF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION</a>RoPEPIPosition\nInterpolation2k32k1kstep</p>\n<img src=\"/c4da56c0/meta_pi.png\" class title=\"PI\">\n\n<blockquote>\n<p>In contrast, LLaMA models that are extended via direct fine-tuning\nonly saw a minimal increase of the effective context window size kmax\nfrom 2048 to 2560, even after fine-tuning for more than 10000 steps,\nwith no clear indication of an acceleration in the increase of window\nsize.</p>\n</blockquote>\n<p>1w20482560</p>\n<p>RoPE</p>\n<p>RoPERoPE\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n&lt;2048attention\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n</p>\n<img src=\"/c4da56c0/meta_rope_ext.png\" class title=\"RoPE\">\n<p>3000attention\nscore</p>\n<p></p>\n<p>PI</p>\n<img src=\"/c4da56c0/meta_pi_nosft.png\" class title=\"PI\">\n<p>2k2k2k4k</p>\n<img src=\"/c4da56c0/meta_pi_explanation.png\" class title=\"PI\">\n<p>123...11.522.5...0.5</p>\n<p>attention\nscore <span class=\"math inline\">\\(\\tilde{a}(s)=a(Ls/L^{\\prime})\\)</span> <span class=\"math inline\">\\(L\\)</span> 2048<span class=\"math inline\">\\(L^{\\prime}\\)</span>\n8k/16k/32k</p>\n<p>RoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}&#39;(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L&#39;}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(m\\)</span> 1\n<span class=\"math inline\">\\({L}/{L&#39;}\\)</span></p>\n<p></p>\n<p></p>\n<p></p>\n<h2 id=\"ntk-aware-interpolation\">NTK-Aware Interpolation</h2>\n<p>cosNTK-Aware\nInterpolationRoPE<u><strong></strong></u>NTK-Aware\nScaled RoPECodeLlama1M</p>\n<p>NTKNeural Tangent\nKernelGLM4</p>\n<blockquote>\n<p>Neural Tangent Kernel (NTK)\nNTK\n<br>\n\nNeural Tangent\nKernel<br>\nNTK\n<br>\nNTK\n</p>\n</blockquote>\n<p>NTK</p>\n<p></p>\n<p>RoPE <span class=\"math inline\">\\(m\\)</span>\n</p>\n<img src=\"/c4da56c0/rope_matrix.png\" class title=\"RoPE\">\n<p>22 <span class=\"math inline\">\\(d/2\\)</span>\n\n<span class=\"math inline\">\\(\\theta_j=10000^{-2j/d}\\)</span> \n<span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(base=10000\\)</span>  <span class=\"math inline\">\\(base\\)</span>\n</p>\n<p></p>\n<p></p>\n<p><a href=\"https://www.zhihu.com/people/us4ever\">RoPE</a>2</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=\n\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&amp;\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*\\)</span>\n<span class=\"math inline\">\\(s=m-n\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha=L&#39;/L&gt;1\\)</span>\n <span class=\"math inline\">\\(s\\)</span> </p>\n<p>NTK-Aware Scaled RoPE <span class=\"math inline\">\\(\\theta_j\\)</span>\nbaseRoPE10000</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\theta\\)</span>  <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>  <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>\n1 <span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(j\\)</span> 0\n<span class=\"math inline\">\\(d/2 - 1\\)</span><span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>  <span class=\"math inline\">\\(\\alpha^{-1}\\)</span> </p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/645770522\"></a>NTK-Aware\nInterpolation</p>\n<blockquote>\n<p>RoPE\n12 3 60 \nRoPE 1/60 \n1/60 4 RoPE\nNTK-Aware\nRoPE  1.5\n 2  90  24\n 129.6k  43.2k\n</p>\n</blockquote>\n<p>RoPE<a href=\"https://kexue.fm/archives/9675\"></a></p>\n<p>YaRN<a href=\"https://arxiv.org/pdf/2309.00071.pdf\"></a>NTK</p>\n<blockquote>\n<p>Given the results from [6], this method performs much better at\nextending the context size of non-finetuned models compared to PI [9].\nHowever, one major disadvantage of this method is that given it is not\njust an interpolation scheme, some dimensions are slightly extrapolated\nto \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\"\ninterpolation [6] yields inferior results to PI [9]. Furthermore, due to\nthe \"out-of-bound\" values, the theoretical scale factor s does not\naccurately describe the true context extension scale. In practice, the\nscale value s has to be set higher than the expected scale for a given\ncontext length extension.</p>\n</blockquote>\n<p>NTK4k32k\n<span class=\"math inline\">\\(\\alpha=L&#39;/L\\)</span>\n816</p>\n<h2 id=\"ntk-by-parts\">NTK-by-parts</h2>\n<p>NTK-by-partsNTKNTK-awareRoPENTK-by-parts</p>\n<p> <span class=\"math inline\">\\(j\\)</span> RoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p><span class=\"math inline\">\\(\\lambda_j\\)</span>\n <span class=\"math inline\">\\(j\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(L\\)</span>\nRoPE\n<span class=\"math inline\">\\(sin\\)</span>\n1/40<sub>1-1</sub>0\n<span class=\"math inline\">\\(j\\)</span>\n</p>\n<p>NTK-by-parts</p>\n<ul>\n<li> <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(\\lambda_j\\)</span> \n<br>\n</li>\n<li> <span class=\"math inline\">\\(\\lambda_j\\geq\\)</span>\n<br>\n</li>\n<li>NTK-aware interpolation</li>\n</ul>\n<p> <span class=\"math inline\">\\(r(j)=\\frac{L}{\\lambda_j}\\)</span>\n <span class=\"math inline\">\\(\\beta_1\\beta_2\\)</span> \n<span class=\"math inline\">\\(r(j)&lt;\\beta_1\\)</span>\n <span class=\"math inline\">\\(r(j)\\geq\n\\beta_2\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&amp;if&amp;r(j)&lt;\\beta_1\\\\1&amp;if&amp;r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&amp;otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>NTK-by-parts <span class=\"math inline\">\\(\\theta_j\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\beta_1\\beta_2\\)</span>\n <span class=\"math inline\">\\(\\beta_1=1\\beta_2=32\\)</span>\n1/32</p>\n<h2 id=\"dynamically-ntk-scaled-rope\">Dynamically NTK Scaled RoPE</h2>\n<p>NTK-Aware\nInterpolationRoPEattention\nscore <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(L\\)</span>  <span class=\"math inline\">\\(\\alpha\\)</span>\nbaseDynamically NTK Scaled\nRoPENTK</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(l\\)</span> \n<span class=\"math inline\">\\(l&gt;L\\)</span>  <span class=\"math inline\">\\(\\alpha\\)</span> 1 <span class=\"math inline\">\\(l\\leq L\\)</span> </p>\n<p>kv-cacheRoPE</p>\n<h2 id=\"yarn\">YaRN</h2>\n<p>tokensoftmaxRoPEtoken</p>\n<p>RoPEsoftmaxlogitsoftmax\n<span class=\"math inline\">\\(t&gt;1\\)</span>\nRoPE <span class=\"math inline\">\\(\\sqrt{t}\\)</span>\nRoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>Llama 1Llama 2<span class=\"math inline\">\\(\\begin{aligned}\\sqrt{\\frac1t}&amp;=0.1\\ln(\\alpha)+1.\\end{aligned}\\)</span>Llama</p>\n<p>YaRNNTK-by-partsattention\nscore</p>\n<p>YaRN</p>\n<h2 id=\"logn\">logn</h2>\n<p>lognattention <span class=\"math inline\">\\(\\sqrt{d}\\)</span>\nlogn<a href=\"https://zhuanlan.zhihu.com/p/678755776\"></a>YaRN</p>\n<p>tokentokentokenattention\nscore</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L&#39;}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(L&#39;&gt;L\\)</span>\nYaRN</p>\n<h2 id=\"\"></h2>\n<p>window\nattentionstreaming LLMLongLoRAFocus\nTransformer</p>\n<h1 id=\"\"></h1>\n<p>2k4k</p>\n<ul>\n<li><br>\n</li>\n<li>token</li>\n</ul>\n<p>attention score</p>\n<p>PINTKNTKlognYaRN</p>\n<h1 id=\"reference\">Reference</h1>\n<p>1transformerKV cache\nhttps://zhuanlan.zhihu.com/p/624740065<br>\n2EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION https://arxiv.org/pdf/2306.15595.pdf<br>\n3Transformer10RoPE\nhttps://kexue.fm/archives/9675<br>\n4YaRN: Efficient Context Window Extension of Large Language Models\nhttps://arxiv.org/pdf/2309.00071.pdf<br>\n5RoPE\nhttps://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ<br>\n6LLM https://zhuanlan.zhihu.com/p/645770522\n7prompt\nhttps://cloud.tencent.com/developer/article/2330611<br>\n8Transformer8\nhttps://spaces.ac.cn/archives/9444<br>\n9RoPE192K\nhttps://zhuanlan.zhihu.com/p/678755776 ***</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n"},{"title":"LLM:RoPE","abbrlink":"a051710f","date":"2024-02-21T13:18:13.000Z","mathjax":true,"_content":"\nLLMRoPE\n\n# RoPE\n\nRoPERotary Position Embedding2021TransformerRoPE<big><u>****</u></big>\n\n2023AlibiRoPE20232024RoPEAlibiAlibi\n\nRoPERoPE  \n\n# \n\nRoPE\n\n  \n\nBert256/512token  \n  \n<u>****</u>token-2token-1token-10002token-10001  \n<u>****</u><u>****</u>self-attention<u>****</u>  \n<u>****</u><u>****</u><u>****</u><u>****</u>  \n\n  \n\n3  \n\n## \n\nself-attention  \n\n$$\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}$$  \n\n\n$x_i$  $x_j$  $i$  $j$ $p$   \n\n$p$ $x$  $p$ attentionsoftmaxelement-wise addition\n\n $x + p$   $x * p$ \n\n## \n\n $x$  $p$   \n\n1 $e_1 = x_ + p_1$ 18 $e_8 = x_ + p_8$  $e_1$  $e_8$ <u>****</u>  \n\n15121512=512handle\n\n1 $q_{i}k_{j}^{T}$  \n\n$$\\begin{align*}q_ik_j^\\top&=\\left(x_i+p_i\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{\\color{red}W_K^\\top p_j^\\top}\\right)\\end{align*}\\tag{2}$$  \n\n $p_iW_\\mathbb{Q}$  $W_K^\\top p_j^\\top$   \n\n### Google\n\nGoogleSelf-Attention with Relative Position Representations $p_iW_\\mathbb{Q}$  $j$ $W_K^\\top p_j^\\top$  $i$$j$  $R_{ij}^K$attention<u>**input projection**</u>  \n\n$$\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n$$  \n\n$R_{ij}^K$ clip  \n\n$$\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n$$\n\n $p_\\mathrm{K}$  \n\nclip****tokentoken256>256\n\nGoogleinput\n\n$$\\begin{align*}\no_\\mathrm{i}&=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} + {\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}$$\n\nGoogle $p_{j}W_{\\mathrm{V}}$ \n\n$$\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}$$\n\n$R_{\\mathrm{i,j}}^{\\mathrm{V}}$  $R_{ij}^K$  + clip\n\n### XLNET\n\nXLNETGoogle  \n\n2  \n\n$$\\begin{align*}\nq_ik_j^T\n&= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&= \nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n$$  \n\n\n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n$$    \n\n $p_i$  $u$  $\\nu$  $p_j$  $R_{i-j}^\\top$   \n\n $u$  $\\nu$  $u$  $\\nu$ \n\n$$\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n$$   \n\nXLNET  \n\n$$\\begin{align*}\no_\\mathrm{i}\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}$$\n\nGoogleXLNET $\\mathrm{a_{i,j}}$ 2 $i$  $j$ clip\n\nT5  \n\n### T5\n\n6 $i$  $j$   \n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n$$    \n\nXLNETDeBertaT5\n\n## \n\nattention  \n\n1softmax33\n\n8433  \n\n\n\n\n\nself-attentionlinear attention  \n\n# RoPE\n\n## attention\n\nRoPE\n\n  \n\nself-attention1 =  + softmaxsoftmax  \n\n  \n\n$$\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n$$  \n\n $q_m$  $m$ query$k_n$  $n$ key$f_q$  $f_k$ querykey  \n\n $f_q$  $f_k$  $g$ 11  \n\nRoPE  \n\n## \n\n11 $g$ \n\n2  \n\n{% asset_img complex_number.png 282 401  %}\n\nquerykey2  \n hidden size = 2   \n\n211Roformer  \n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta} \\\\\nf_k(\\boldsymbol{k}_n,n)&=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta} \\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n$$  \n\n $\\boldsymbol{k}_n^*$  $\\boldsymbol{k}_n$   \n\n\n\n  \n\n$$\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib \n\\end{gathered}\n\\tag{13}\n$$\n\n11  \n\n  \n\n  \n\n$$\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n$$\n\n22 $q_m$   \n\n$$\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n$$\n\n\n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m) \n&= \\boldsymbol{q}_me^{im\\theta} \\\\\n&= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&= (q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n$$  \n\n16  \n\n$$\nf_q(\\boldsymbol{q}_m,m) = \\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n$$  \n\n1transpose  \n\n\n\n$$\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n) \n&= (k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n$$  \n\n\n\n$$\nf_k(\\boldsymbol{k}_n,n) = \\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n$$  \n\n  \n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)) \\\\&+ (q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\\\\\n=&q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right) \\\\\n&+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right) \\\\\n&+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta)) \\\\\n&+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta)) \\\\\n=&q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta) \\\\\n&-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta) +q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n$$\n\n\n$$\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n$$\n\n $g$  \n\n$$\n\\begin{aligned}\n&g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n= &\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right] \\\\\n= &\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) - i(q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})](\\cos((m - n)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n$$\n\n\n\n## \n\n17\n\n$$\nf_q(\\boldsymbol{q}_m,m)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n$$\n\n  \n\n$$\nf_k(\\boldsymbol{k}_n,n)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n$$\n\n  \n\n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta) + \\sin(m\\theta)\\sin(n\\theta)}&\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n$$  \n\n2223  \n\n$$\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n$$  \n\n $f_q$  $f_k$   \n\n\n\n## 2\n\n2 $f_q$  $f_k$  $g$ 11  \n\n  \n\n$$\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\\\0&0&0&0&\\cdots&\\sin m\\theta_{d/2-1}&\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n$$  \n\n$d$  $d/2$  $d/2$  $\\theta$\n\n$$\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n$$  \n\n $m$  $n$  $R_m$  $R_n$self-attention  \n\n $\\theta$ GoogleAttention is All You Need\n\n## \n\n25  \n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n$$\n\nelement-wise  \n\nLLAMAdecoder\n\n## \n\n  \n\n\n\n $\\theta$   \n\n[Roformer](https://arxiv.org/abs/2104.09864)[](https://spaces.ac.cn/archives/8265)  \n\n $d = 128$ \n\n{% asset_img remote_attenuation.png 775 457  %}  \n\n#   \n\nRoPEtransformer\n\n# Reference\n1Transformerhttps://spaces.ac.cn/archives/8130  \n2Transformer2https://spaces.ac.cn/archives/8265  \n3RoFormer: Enhanced Transformer with Rotary Position Embedding https://arxiv.org/abs/2104.09864  \n4RoPE https://zhuanlan.zhihu.com/p/647109286  \n\n***\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)\n","source":"_posts/cs/nlp/2024/02/LLMRoPE.md","raw":"---\ntitle: LLM:RoPE\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - positional encoding\n  - RoPE\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: a051710f\ndate: 2024-02-21 21:18:13\nmathjax: true\n---\n\nLLMRoPE\n\n# RoPE\n\nRoPERotary Position Embedding2021TransformerRoPE<big><u>****</u></big>\n\n2023AlibiRoPE20232024RoPEAlibiAlibi\n\nRoPERoPE  \n\n# \n\nRoPE\n\n  \n\nBert256/512token  \n  \n<u>****</u>token-2token-1token-10002token-10001  \n<u>****</u><u>****</u>self-attention<u>****</u>  \n<u>****</u><u>****</u><u>****</u><u>****</u>  \n\n  \n\n3  \n\n## \n\nself-attention  \n\n$$\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}$$  \n\n\n$x_i$  $x_j$  $i$  $j$ $p$   \n\n$p$ $x$  $p$ attentionsoftmaxelement-wise addition\n\n $x + p$   $x * p$ \n\n## \n\n $x$  $p$   \n\n1 $e_1 = x_ + p_1$ 18 $e_8 = x_ + p_8$  $e_1$  $e_8$ <u>****</u>  \n\n15121512=512handle\n\n1 $q_{i}k_{j}^{T}$  \n\n$$\\begin{align*}q_ik_j^\\top&=\\left(x_i+p_i\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{\\color{red}W_K^\\top p_j^\\top}\\right)\\end{align*}\\tag{2}$$  \n\n $p_iW_\\mathbb{Q}$  $W_K^\\top p_j^\\top$   \n\n### Google\n\nGoogleSelf-Attention with Relative Position Representations $p_iW_\\mathbb{Q}$  $j$ $W_K^\\top p_j^\\top$  $i$$j$  $R_{ij}^K$attention<u>**input projection**</u>  \n\n$$\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n$$  \n\n$R_{ij}^K$ clip  \n\n$$\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n$$\n\n $p_\\mathrm{K}$  \n\nclip****tokentoken256>256\n\nGoogleinput\n\n$$\\begin{align*}\no_\\mathrm{i}&=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} + {\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}$$\n\nGoogle $p_{j}W_{\\mathrm{V}}$ \n\n$$\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}$$\n\n$R_{\\mathrm{i,j}}^{\\mathrm{V}}$  $R_{ij}^K$  + clip\n\n### XLNET\n\nXLNETGoogle  \n\n2  \n\n$$\\begin{align*}\nq_ik_j^T\n&= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&= \nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n$$  \n\n\n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n$$    \n\n $p_i$  $u$  $\\nu$  $p_j$  $R_{i-j}^\\top$   \n\n $u$  $\\nu$  $u$  $\\nu$ \n\n$$\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n$$   \n\nXLNET  \n\n$$\\begin{align*}\no_\\mathrm{i}\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}$$\n\nGoogleXLNET $\\mathrm{a_{i,j}}$ 2 $i$  $j$ clip\n\nT5  \n\n### T5\n\n6 $i$  $j$   \n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n$$    \n\nXLNETDeBertaT5\n\n## \n\nattention  \n\n1softmax33\n\n8433  \n\n\n\n\n\nself-attentionlinear attention  \n\n# RoPE\n\n## attention\n\nRoPE\n\n  \n\nself-attention1 =  + softmaxsoftmax  \n\n  \n\n$$\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n$$  \n\n $q_m$  $m$ query$k_n$  $n$ key$f_q$  $f_k$ querykey  \n\n $f_q$  $f_k$  $g$ 11  \n\nRoPE  \n\n## \n\n11 $g$ \n\n2  \n\n{% asset_img complex_number.png 282 401  %}\n\nquerykey2  \n hidden size = 2   \n\n211Roformer  \n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta} \\\\\nf_k(\\boldsymbol{k}_n,n)&=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta} \\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n$$  \n\n $\\boldsymbol{k}_n^*$  $\\boldsymbol{k}_n$   \n\n\n\n  \n\n$$\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib \n\\end{gathered}\n\\tag{13}\n$$\n\n11  \n\n  \n\n  \n\n$$\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n$$\n\n22 $q_m$   \n\n$$\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n$$\n\n\n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m) \n&= \\boldsymbol{q}_me^{im\\theta} \\\\\n&= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&= (q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n$$  \n\n16  \n\n$$\nf_q(\\boldsymbol{q}_m,m) = \\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n$$  \n\n1transpose  \n\n\n\n$$\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n) \n&= (k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n$$  \n\n\n\n$$\nf_k(\\boldsymbol{k}_n,n) = \\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n$$  \n\n  \n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)) \\\\&+ (q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\\\\\n=&q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right) \\\\\n&+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right) \\\\\n&+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta)) \\\\\n&+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta)) \\\\\n=&q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta) \\\\\n&-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta) +q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n$$\n\n\n$$\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n$$\n\n $g$  \n\n$$\n\\begin{aligned}\n&g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n= &\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right] \\\\\n= &\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) - i(q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})](\\cos((m - n)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n$$\n\n\n\n## \n\n17\n\n$$\nf_q(\\boldsymbol{q}_m,m)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n$$\n\n  \n\n$$\nf_k(\\boldsymbol{k}_n,n)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n$$\n\n  \n\n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta) + \\sin(m\\theta)\\sin(n\\theta)}&\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n$$  \n\n2223  \n\n$$\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n$$  \n\n $f_q$  $f_k$   \n\n\n\n## 2\n\n2 $f_q$  $f_k$  $g$ 11  \n\n  \n\n$$\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\\\0&0&0&0&\\cdots&\\sin m\\theta_{d/2-1}&\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n$$  \n\n$d$  $d/2$  $d/2$  $\\theta$\n\n$$\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n$$  \n\n $m$  $n$  $R_m$  $R_n$self-attention  \n\n $\\theta$ GoogleAttention is All You Need\n\n## \n\n25  \n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n$$\n\nelement-wise  \n\nLLAMAdecoder\n\n## \n\n  \n\n\n\n $\\theta$   \n\n[Roformer](https://arxiv.org/abs/2104.09864)[](https://spaces.ac.cn/archives/8265)  \n\n $d = 128$ \n\n{% asset_img remote_attenuation.png 775 457  %}  \n\n#   \n\nRoPEtransformer\n\n# Reference\n1Transformerhttps://spaces.ac.cn/archives/8130  \n2Transformer2https://spaces.ac.cn/archives/8265  \n3RoFormer: Enhanced Transformer with Rotary Position Embedding https://arxiv.org/abs/2104.09864  \n4RoPE https://zhuanlan.zhihu.com/p/647109286  \n\n***\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)\n","slug":"cs/nlp/2024/02/LLMRoPE","published":1,"updated":"2024-04-05T06:44:27.271Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmfh000kam4k8roy12jb","content":"<p>LLMRoPE</p>\n<h1 id=\"rope\">RoPE</h1>\n<p>RoPERotary Position\nEmbedding2021TransformerRoPE<big><u><strong></strong></u></big></p>\n<p>2023AlibiRoPE20232024RoPEAlibiAlibi</p>\n<p>RoPERoPE</p>\n<h1 id=\"\"></h1>\n<p>RoPE</p>\n<p></p>\n<p>Bert256/512token<br>\n<br>\n<u><strong></strong></u>token-2token-1token-10002token-10001<br>\n<u><strong></strong></u><u><strong></strong></u>self-attention<u><strong></strong></u><br>\n<u><strong></strong></u><u><strong></strong></u><u><strong></strong></u><u><strong></strong></u></p>\n<p></p>\n<p>3</p>\n<h2 id=\"\"></h2>\n<p>self-attention</p>\n<p><span class=\"math display\">\\[\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}\\]</span></p>\n<p><span class=\"math inline\">\\(x_i\\)</span>  <span class=\"math inline\">\\(x_j\\)</span>  <span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(p\\)</span> </p>\n<p><span class=\"math inline\">\\(p\\)</span>\n<span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(p\\)</span>\nattentionsoftmaxelement-wise\naddition</p>\n<p>\n<span class=\"math inline\">\\(x + p\\)</span>  <span class=\"math inline\">\\(x * p\\)</span> </p>\n<h2 id=\"\"></h2>\n<p> <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(p\\)</span>\n</p>\n<p>1 <span class=\"math inline\">\\(e_1 =\nx_ + p_1\\)</span>\n18\n<span class=\"math inline\">\\(e_8 = x_ + p_8\\)</span>  <span class=\"math inline\">\\(e_1\\)</span>  <span class=\"math inline\">\\(e_8\\)</span>\n<u><strong></strong></u></p>\n<p>15121512=512handle</p>\n<p>1 <span class=\"math inline\">\\(q_{i}k_{j}^{T}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{align*}q_ik_j^\\top&amp;=\\left(x_i+p_i\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&amp;=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{\\color{red}W_K^\\top\np_j^\\top}\\right)\\end{align*}\\tag{2}\\]</span></p>\n<p> <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span> </p>\n<h3 id=\"google\">Google</h3>\n<p>GoogleSelf-Attention with Relative\nPosition Representations <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n\n<span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span>  <span class=\"math inline\">\\(i\\)</span><span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(R_{ij}^K\\)</span>attention<u><strong>input\nprojection</strong></u></p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n\\]</span></p>\n<p><span class=\"math inline\">\\(R_{ij}^K\\)</span>\nclip</p>\n<p><span class=\"math display\">\\[\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n\\]</span></p>\n<p> <span class=\"math inline\">\\(p_\\mathrm{K}\\)</span>\n</p>\n<p>clip<strong></strong>tokentoken256&gt;256</p>\n<p>Googleinput</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}&amp;=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} +\n{\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}\\]</span></p>\n<p>Google <span class=\"math inline\">\\(p_{j}W_{\\mathrm{V}}\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}\\]</span></p>\n<p><span class=\"math inline\">\\(R_{\\mathrm{i,j}}^{\\mathrm{V}}\\)</span> \n<span class=\"math inline\">\\(R_{ij}^K\\)</span> \n+ clip</p>\n<h3 id=\"xlnet\">XLNET</h3>\n<p>XLNETGoogle</p>\n<p>2</p>\n<p><span class=\"math display\">\\[\\begin{align*}\nq_ik_j^T\n&amp;= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&amp;=\nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu}\nW_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(p_i\\)</span> \n<span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span>  <span class=\"math inline\">\\(p_j\\)</span>  <span class=\"math inline\">\\(R_{i-j}^\\top\\)</span> </p>\n<p> <span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span>\n <span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span> </p>\n<p><span class=\"math display\">\\[\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n\\]</span></p>\n<p>XLNET</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}\\]</span></p>\n<p>GoogleXLNET <span class=\"math inline\">\\(\\mathrm{a_{i,j}}\\)</span>\n2\n<span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span>\nclip</p>\n<p>T5</p>\n<h3 id=\"t5\">T5</h3>\n<p>6\n<span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span>\n</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n\\]</span></p>\n<p>XLNETDeBertaT5</p>\n<h2 id=\"\"></h2>\n<p>attention</p>\n<p>1softmax33</p>\n<p>8433</p>\n<p></p>\n<p></p>\n<p>self-attentionlinear\nattention</p>\n<h1 id=\"rope\">RoPE</h1>\n<h2 id=\"attention\">attention</h2>\n<p>RoPE</p>\n<p></p>\n<p>self-attention1\n=  +\nsoftmaxsoftmax</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(q_m\\)</span>  <span class=\"math inline\">\\(m\\)</span> query<span class=\"math inline\">\\(k_n\\)</span>  <span class=\"math inline\">\\(n\\)</span> key<span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>\nquerykey</p>\n<p> <span class=\"math inline\">\\(f_q\\)</span> \n<span class=\"math inline\">\\(f_k\\)</span>  <span class=\"math inline\">\\(g\\)</span> 11</p>\n<p>RoPE</p>\n<h2 id=\"\"></h2>\n<p>11 <span class=\"math inline\">\\(g\\)</span>\n</p>\n<p>2</p>\n<img src=\"/a051710f/complex_number.png\" class width=\"282\" height=\"401\" title=\"\">\n<p>querykey2<br>\n hidden size = 2 </p>\n<p>211Roformer</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&amp;=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta}\n\\\\\nf_k(\\boldsymbol{k}_n,n)&amp;=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta}\n\\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&amp;=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\boldsymbol{k}_n^*\\)</span>  <span class=\"math inline\">\\(\\boldsymbol{k}_n\\)</span> </p>\n<p></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib\n\\end{gathered}\n\\tag{13}\n\\]</span></p>\n<p>11</p>\n<p></p>\n<p></p>\n<p><span class=\"math display\">\\[\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n\\]</span></p>\n<p>22 <span class=\"math inline\">\\(q_m\\)</span> </p>\n<p><span class=\"math display\">\\[\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)\n&amp;= \\boldsymbol{q}_me^{im\\theta} \\\\\n&amp;= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&amp;=\n(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n\\]</span></p>\n<p>16</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m) =\n\\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n\\]</span></p>\n<p>1transpose</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n)\n&amp;=\n(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n) =\n\\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n\\]</span></p>\n<p><br>\n<span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&amp;(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))\n\\\\&amp;+ (q_m^{(1)}\\sin(m\\theta) +\nq_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) +\nk_n^{(2)}\\cos(n\\theta))\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta))\n\\\\\n&amp;+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta))\n\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta)\n\\\\\n&amp;-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta)\n+q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n\\]</span></p>\n<p> <span class=\"math display\">\\[\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(g\\)</span></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n=\n&amp;\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\\\\n= &amp;\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) -\ni(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})](\\cos((m -\nn)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 -\nq_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n\\]</span></p>\n<p></p>\n<h2 id=\"\"></h2>\n<p>17</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta)\n+ \\sin(m\\theta)\\sin(n\\theta)}&amp;\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&amp;\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&amp;{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&amp;{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n\\]</span></p>\n<p>2223</p>\n<p><span class=\"math display\">\\[\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n\\]</span></p>\n<p> <span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>\n</p>\n<p></p>\n<h2 id=\"2\">2</h2>\n<p>2 <span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>  <span class=\"math inline\">\\(g\\)</span>\n11</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&amp;-\\sin\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\\\sin m\\theta_0&amp;\\cos\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\cos\nm\\theta_1&amp;-\\sin m\\theta_1&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\sin\nm\\theta_1&amp;\\cos\nm\\theta_1&amp;\\cdots&amp;0&amp;0\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\cos\nm\\theta_{d/2-1}&amp;-\\sin\nm\\theta_{d/2-1}\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\sin\nm\\theta_{d/2-1}&amp;\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n\\]</span></p>\n<p><span class=\"math inline\">\\(d\\)</span>\n <span class=\"math inline\">\\(d/2\\)</span>  <span class=\"math inline\">\\(d/2\\)</span> \n<span class=\"math inline\">\\(\\theta\\)</span></p>\n<p><span class=\"math display\">\\[\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(m\\)</span>\n <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(R_m\\)</span>  <span class=\"math inline\">\\(R_n\\)</span>self-attention</p>\n<p> <span class=\"math inline\">\\(\\theta\\)</span>\nGoogleAttention is All You\nNeed</p>\n<h2 id=\"\"></h2>\n<p>25</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n\\]</span></p>\n<p>element-wise</p>\n<p>LLAMAdecoder</p>\n<h2 id=\"\"></h2>\n<p></p>\n<p></p>\n<p>\n<span class=\"math inline\">\\(\\theta\\)</span>\n</p>\n<p><a href=\"https://arxiv.org/abs/2104.09864\">Roformer</a><a href=\"https://spaces.ac.cn/archives/8265\"></a></p>\n<p> <span class=\"math inline\">\\(d = 128\\)</span>\n</p>\n<img src=\"/a051710f/remote_attenuation.png\" class width=\"775\" height=\"457\" title=\"\">\n<h1 id=\"\"></h1>\n<p>RoPEtransformer</p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Transformerhttps://spaces.ac.cn/archives/8130<br>\n2Transformer2https://spaces.ac.cn/archives/8265<br>\n3RoFormer: Enhanced Transformer with Rotary Position Embedding\nhttps://arxiv.org/abs/2104.09864<br>\n4RoPE\nhttps://zhuanlan.zhihu.com/p/647109286</p>\n<hr>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n","length":14264,"excerpt":"","more":"<p>LLMRoPE</p>\n<h1 id=\"rope\">RoPE</h1>\n<p>RoPERotary Position\nEmbedding2021TransformerRoPE<big><u><strong></strong></u></big></p>\n<p>2023AlibiRoPE20232024RoPEAlibiAlibi</p>\n<p>RoPERoPE</p>\n<h1 id=\"\"></h1>\n<p>RoPE</p>\n<p></p>\n<p>Bert256/512token<br>\n<br>\n<u><strong></strong></u>token-2token-1token-10002token-10001<br>\n<u><strong></strong></u><u><strong></strong></u>self-attention<u><strong></strong></u><br>\n<u><strong></strong></u><u><strong></strong></u><u><strong></strong></u><u><strong></strong></u></p>\n<p></p>\n<p>3</p>\n<h2 id=\"\"></h2>\n<p>self-attention</p>\n<p><span class=\"math display\">\\[\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}\\]</span></p>\n<p><span class=\"math inline\">\\(x_i\\)</span>  <span class=\"math inline\">\\(x_j\\)</span>  <span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(p\\)</span> </p>\n<p><span class=\"math inline\">\\(p\\)</span>\n<span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(p\\)</span>\nattentionsoftmaxelement-wise\naddition</p>\n<p>\n<span class=\"math inline\">\\(x + p\\)</span>  <span class=\"math inline\">\\(x * p\\)</span> </p>\n<h2 id=\"\"></h2>\n<p> <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(p\\)</span>\n</p>\n<p>1 <span class=\"math inline\">\\(e_1 =\nx_ + p_1\\)</span>\n18\n<span class=\"math inline\">\\(e_8 = x_ + p_8\\)</span>  <span class=\"math inline\">\\(e_1\\)</span>  <span class=\"math inline\">\\(e_8\\)</span>\n<u><strong></strong></u></p>\n<p>15121512=512handle</p>\n<p>1 <span class=\"math inline\">\\(q_{i}k_{j}^{T}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{align*}q_ik_j^\\top&amp;=\\left(x_i+p_i\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&amp;=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{\\color{red}W_K^\\top\np_j^\\top}\\right)\\end{align*}\\tag{2}\\]</span></p>\n<p> <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span> </p>\n<h3 id=\"google\">Google</h3>\n<p>GoogleSelf-Attention with Relative\nPosition Representations <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n\n<span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span>  <span class=\"math inline\">\\(i\\)</span><span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(R_{ij}^K\\)</span>attention<u><strong>input\nprojection</strong></u></p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n\\]</span></p>\n<p><span class=\"math inline\">\\(R_{ij}^K\\)</span>\nclip</p>\n<p><span class=\"math display\">\\[\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n\\]</span></p>\n<p> <span class=\"math inline\">\\(p_\\mathrm{K}\\)</span>\n</p>\n<p>clip<strong></strong>tokentoken256&gt;256</p>\n<p>Googleinput</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}&amp;=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} +\n{\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}\\]</span></p>\n<p>Google <span class=\"math inline\">\\(p_{j}W_{\\mathrm{V}}\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}\\]</span></p>\n<p><span class=\"math inline\">\\(R_{\\mathrm{i,j}}^{\\mathrm{V}}\\)</span> \n<span class=\"math inline\">\\(R_{ij}^K\\)</span> \n+ clip</p>\n<h3 id=\"xlnet\">XLNET</h3>\n<p>XLNETGoogle</p>\n<p>2</p>\n<p><span class=\"math display\">\\[\\begin{align*}\nq_ik_j^T\n&amp;= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&amp;=\nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu}\nW_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(p_i\\)</span> \n<span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span>  <span class=\"math inline\">\\(p_j\\)</span>  <span class=\"math inline\">\\(R_{i-j}^\\top\\)</span> </p>\n<p> <span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span>\n <span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span> </p>\n<p><span class=\"math display\">\\[\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n\\]</span></p>\n<p>XLNET</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}\\]</span></p>\n<p>GoogleXLNET <span class=\"math inline\">\\(\\mathrm{a_{i,j}}\\)</span>\n2\n<span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span>\nclip</p>\n<p>T5</p>\n<h3 id=\"t5\">T5</h3>\n<p>6\n<span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span>\n</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n\\]</span></p>\n<p>XLNETDeBertaT5</p>\n<h2 id=\"\"></h2>\n<p>attention</p>\n<p>1softmax33</p>\n<p>8433</p>\n<p></p>\n<p></p>\n<p>self-attentionlinear\nattention</p>\n<h1 id=\"rope\">RoPE</h1>\n<h2 id=\"attention\">attention</h2>\n<p>RoPE</p>\n<p></p>\n<p>self-attention1\n=  +\nsoftmaxsoftmax</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(q_m\\)</span>  <span class=\"math inline\">\\(m\\)</span> query<span class=\"math inline\">\\(k_n\\)</span>  <span class=\"math inline\">\\(n\\)</span> key<span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>\nquerykey</p>\n<p> <span class=\"math inline\">\\(f_q\\)</span> \n<span class=\"math inline\">\\(f_k\\)</span>  <span class=\"math inline\">\\(g\\)</span> 11</p>\n<p>RoPE</p>\n<h2 id=\"\"></h2>\n<p>11 <span class=\"math inline\">\\(g\\)</span>\n</p>\n<p>2</p>\n<img src=\"/a051710f/complex_number.png\" class width=\"282\" height=\"401\" title=\"\">\n<p>querykey2<br>\n hidden size = 2 </p>\n<p>211Roformer</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&amp;=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta}\n\\\\\nf_k(\\boldsymbol{k}_n,n)&amp;=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta}\n\\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&amp;=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\boldsymbol{k}_n^*\\)</span>  <span class=\"math inline\">\\(\\boldsymbol{k}_n\\)</span> </p>\n<p></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib\n\\end{gathered}\n\\tag{13}\n\\]</span></p>\n<p>11</p>\n<p></p>\n<p></p>\n<p><span class=\"math display\">\\[\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n\\]</span></p>\n<p>22 <span class=\"math inline\">\\(q_m\\)</span> </p>\n<p><span class=\"math display\">\\[\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)\n&amp;= \\boldsymbol{q}_me^{im\\theta} \\\\\n&amp;= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&amp;=\n(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n\\]</span></p>\n<p>16</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m) =\n\\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n\\]</span></p>\n<p>1transpose</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n)\n&amp;=\n(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n) =\n\\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n\\]</span></p>\n<p><br>\n<span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&amp;(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))\n\\\\&amp;+ (q_m^{(1)}\\sin(m\\theta) +\nq_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) +\nk_n^{(2)}\\cos(n\\theta))\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta))\n\\\\\n&amp;+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta))\n\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta)\n\\\\\n&amp;-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta)\n+q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n\\]</span></p>\n<p> <span class=\"math display\">\\[\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(g\\)</span></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n=\n&amp;\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\\\\n= &amp;\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) -\ni(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})](\\cos((m -\nn)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 -\nq_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n\\]</span></p>\n<p></p>\n<h2 id=\"\"></h2>\n<p>17</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta)\n+ \\sin(m\\theta)\\sin(n\\theta)}&amp;\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&amp;\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&amp;{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&amp;{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n\\]</span></p>\n<p>2223</p>\n<p><span class=\"math display\">\\[\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n\\]</span></p>\n<p> <span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>\n</p>\n<p></p>\n<h2 id=\"2\">2</h2>\n<p>2 <span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>  <span class=\"math inline\">\\(g\\)</span>\n11</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&amp;-\\sin\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\\\sin m\\theta_0&amp;\\cos\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\cos\nm\\theta_1&amp;-\\sin m\\theta_1&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\sin\nm\\theta_1&amp;\\cos\nm\\theta_1&amp;\\cdots&amp;0&amp;0\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\cos\nm\\theta_{d/2-1}&amp;-\\sin\nm\\theta_{d/2-1}\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\sin\nm\\theta_{d/2-1}&amp;\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n\\]</span></p>\n<p><span class=\"math inline\">\\(d\\)</span>\n <span class=\"math inline\">\\(d/2\\)</span>  <span class=\"math inline\">\\(d/2\\)</span> \n<span class=\"math inline\">\\(\\theta\\)</span></p>\n<p><span class=\"math display\">\\[\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(m\\)</span>\n <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(R_m\\)</span>  <span class=\"math inline\">\\(R_n\\)</span>self-attention</p>\n<p> <span class=\"math inline\">\\(\\theta\\)</span>\nGoogleAttention is All You\nNeed</p>\n<h2 id=\"\"></h2>\n<p>25</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n\\]</span></p>\n<p>element-wise</p>\n<p>LLAMAdecoder</p>\n<h2 id=\"\"></h2>\n<p></p>\n<p></p>\n<p>\n<span class=\"math inline\">\\(\\theta\\)</span>\n</p>\n<p><a href=\"https://arxiv.org/abs/2104.09864\">Roformer</a><a href=\"https://spaces.ac.cn/archives/8265\"></a></p>\n<p> <span class=\"math inline\">\\(d = 128\\)</span>\n</p>\n<img src=\"/a051710f/remote_attenuation.png\" class width=\"775\" height=\"457\" title=\"\">\n<h1 id=\"\"></h1>\n<p>RoPEtransformer</p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Transformerhttps://spaces.ac.cn/archives/8130<br>\n2Transformer2https://spaces.ac.cn/archives/8265<br>\n3RoFormer: Enhanced Transformer with Rotary Position Embedding\nhttps://arxiv.org/abs/2104.09864<br>\n4RoPE\nhttps://zhuanlan.zhihu.com/p/647109286</p>\n<hr>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n"},{"title":"normalization-","abbrlink":"b70b4a2d","date":"2024-04-06T04:24:25.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)normalizationnorm  \n\nnorm  \n\n[https://github.com/Saicat/normalization_exp](https://github.com/Saicat/normalization_exp)\n\n# \n\nnormalizationCNN  \n\n```python\nimport torch\nfrom torch import nn\n\n# epsilon\neps = 1e-8\n\n# \nbatch_size = 3\nfeature_num = 4\ntorch.manual_seed(0)  # \ninputs = torch.randn(batch_size, feature_num)\nprint(':\\n', inputs)\n```\n\n34batch size=34  \n\n```python\n:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\n```\n\n## batchnorm  \n\npytorchBatchNorm1d  \n\n```python\n# torchbatchnorm\ntorch_bn = nn.BatchNorm1d(num_features=feature_num, affine=True)  # batchnorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(1)  # \ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(feature_num))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(feature_num))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_bn(inputs)\nprint('torch bn:\\n', torch_normed)\n```\n\nbatchnorm/layernorm+BatchNorm\"affine\"\"affine\"False  \n\npytorchnorm1.00  \n\n```python\nweight:\n Parameter containing:\ntensor([0.6614, 0.2669, 0.0617, 0.6213], requires_grad=True)\nbias:\n Parameter containing:\ntensor([-0.4519, -0.1661, -1.5228,  0.3817], requires_grad=True) \n```\n\nweightbias4  \n\nbatchnorm  \n\n```python\ntorch bn:\n tensor([[ 0.4756,  0.0513, -1.6033,  0.4715],\n        [-1.0197, -0.5421, -1.4535,  1.0937],\n        [-0.8117, -0.0077, -1.5115, -0.4202]],\n       grad_fn=<NativeBatchNormBackward0>)\n```\n\nbatchnorm  \n\n```python\n# bn\n\n# \nmean = torch.mean(inputs, dim=0, keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=0, keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias\nprint('bn:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(isclose)\n```\n\ndim=0batchsamplefeature    \n\n```python\n:\n tensor([[-0.0876, -0.6985, -0.7907,  0.5295]])\n:\n tensor([[1.1612, 0.4971, 1.0630, 0.2692]]) \n```\n\nmeanstdkeepdimTrueFalsebroadcast  \n\nstdunbiasedFalsetorchbatchnorm  \n\nbatchnorm  \n\n```python\nbn:\n tensor([[ 0.4756,  0.0514, -1.6033,  0.4715],\n        [-1.0197, -0.5421, -1.4535,  1.0937],\n        [-0.8117, -0.0077, -1.5115, -0.4202]], grad_fn=<AddBackward0>)\n```\n\ntorch.isclosebatchnormbatchnorm  \n\n```python\ntensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\nequal1e-5~1e-4eps  \n\n## layernorm  \n\nlayernorm34\n\ntorch\n\n```python\n# torchlayernorm\ntorch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=True)  # layernorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(2)  # \ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(feature_num))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(feature_num))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n```\n\nlayernorm  \n\n```python\nweight:\n Parameter containing:\ntensor([ 0.3923, -0.2236, -0.3195, -1.2050], requires_grad=True)\nbias:\n Parameter containing:\ntensor([ 1.0445, -0.6332,  0.5731,  0.5409], requires_grad=True) \n```\n  \n\nlayernorm  \n\n```python\ntorch ln:\n tensor([[ 1.5120, -0.6001,  1.0604, -0.0392],\n        [ 0.7249, -0.3772,  0.3331, -0.9155],\n        [ 0.6645, -0.6209,  0.7693, -1.4324]],\n       grad_fn=<NativeLayerNormBackward0>)\n```\n\n  \n\ndim=1  \n\n```python\n# ln\n\n# \nmean = torch.mean(inputs, dim=1, keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=1, keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\nprint('ln:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(isclose)\n```\n\n  \n\n```python\n:\n tensor([[-0.0907],\n        [-0.3104],\n        [-0.3843]])\n:\n tensor([[1.3691],\n        [0.9502],\n        [0.3458]]) \n```\n\n  \n\n```python\nln:\n tensor([[ 1.5120, -0.6001,  1.0604, -0.0392],\n        [ 0.7249, -0.3772,  0.3331, -0.9155],\n        [ 0.6645, -0.6209,  0.7693, -1.4325]], grad_fn=<AddBackward0>)\n:\n tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\n##   \n\nbatchnormlayernorm  \n\n{% asset_img bn_and_ln.png bnln %}  \n\nbatchnormdim=0batchlayernormdim=1  \n\nbatchnormlayernorm  \n\n# CV\n\nCV  \n\nCV[N,C,H,W]Nbatch sizeCchannelHWfeature mapCV  \n\n```python\n# [N,C,H,W]\nbatch_size = 2\nchannel = 2\nheight = 2\nwidth = 3\ntorch.manual_seed(3)  # \ninputs = torch.randn(batch_size, channel, height, width)\nprint(':\\n', inputs)\n```\n\n  \n\n```python\n:\n tensor([[[[-0.0766,  0.3599, -0.7820],\n          [ 0.0715,  0.6648, -0.2868]],\n\n         [[ 1.6206, -1.5967,  0.4046],\n          [ 0.6113,  0.7604, -0.0336]]],\n\n\n        [[[-0.3448,  0.4937, -0.0776],\n          [-1.8054,  0.4851,  0.2052]],\n\n         [[ 0.3384,  1.3528,  0.3736],\n          [ 0.0134,  0.7737, -0.1092]]]])\n```\n\n## batchnorm  \n\nBatchNorm2dchannel  \n\n```python\n# torchbatchnorm\ntorch_bn = nn.BatchNorm2d(num_features=channel, affine=True)  # batchnorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(4)  # \ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(channel))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(channel))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_bn(inputs)\nprint('torch bn:\\n', torch_normed)\n```\n\nchannel  \n\n```python\nweight:\n Parameter containing:\ntensor([-1.6053,  0.2325], requires_grad=True)\nbias:\n Parameter containing:\ntensor([2.2399, 0.8473], requires_grad=True) \n```\n\ntorchbatchnorm2d  \n\n```python\ntorch bn:\n tensor([[[[2.2043, 1.1275, 3.9442],\n          [1.8388, 0.3753, 2.7226]],\n\n         [[1.2185, 0.2591, 0.8559],\n          [0.9175, 0.9620, 0.7252]]],\n\n\n        [[[2.8658, 0.7975, 2.2066],\n          [6.4684, 0.8186, 1.5090]],\n\n         [[0.8362, 1.1387, 0.8467],\n          [0.7392, 0.9660, 0.7027]]]], grad_fn=<NativeBatchNormBackward0>)\n```\n\nbatchnorm2d  \n\n```python\n# bn\n\nmanual_normed = []\n# channel\nfor c in range(channel):\n    # \n    mean = torch.mean(inputs[:, c, :, :])\n    std = torch.std(inputs[:, c, :, :], unbiased=False)\n    normed = (inputs[:, c, :, :] - mean) / (std + eps) * torch_bn.weight[c] + torch_bn.bias[c]\n    normed = normed.unsqueeze(1)\n    manual_normed.append(normed)\nmanual_normed = torch.cat(manual_normed, 1)\nprint('bn:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\nCVchannel\"batch\"  \n\nNHW  \n\n{% asset_img cv_batchnorm.png CVbatchnorm %}  \n\n  \n\n```python\nbn:\n tensor([[[[2.2043, 1.1275, 3.9442],\n          [1.8388, 0.3752, 2.7226]],\n\n         [[1.2185, 0.2591, 0.8559],\n          [0.9175, 0.9620, 0.7252]]],\n\n\n        [[[2.8658, 0.7975, 2.2066],\n          [6.4685, 0.8186, 1.5089]],\n\n         [[0.8362, 1.1387, 0.8467],\n          [0.7392, 0.9660, 0.7027]]]], grad_fn=<CatBackward0>)\n:\n tensor([[[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]],\n\n\n        [[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]]])\n```\n\n## layernorm  \n\n[torchlayernorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)layernorm  \n\n```python\n# torchlayernorm\ntorch_ln = nn.LayerNorm(\n    normalized_shape=[channel, height, width], \n    elementwise_affine=True\n)  # layernorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(5)  # \ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(channel, height, width))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(channel, height, width))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n```\n\n\n\n{% asset_img cv_layernorm.jpeg CVlayernorm %}  \n\n[channel, height, width]  \n\n```python\nweight:\n Parameter containing:\ntensor([[[-0.4868, -0.6038, -0.5581],\n         [ 0.6675, -0.1974,  1.9428]],\n\n        [[-1.4017, -0.7626,  0.6312],\n         [-0.8991, -0.5578,  0.6907]]], requires_grad=True)\nbias:\n Parameter containing:\ntensor([[[ 0.2225, -0.6662,  0.6846],\n         [ 0.5740, -0.5829,  0.7679]],\n\n        [[ 0.0571, -1.1894, -0.5659],\n         [-0.8327,  0.9014,  0.2116]]], requires_grad=True) \n```\n\nchannel  \n\nlayernorm  \n\n```python\ntorch ln:\n tensor([[[[ 0.3594, -0.8338,  1.3456],\n          [ 0.5128, -0.7147, -0.3012]],\n\n         [[-2.5939,  0.5089, -0.3546],\n          [-1.3715,  0.4607,  0.0553]]],\n\n\n        [[[ 0.5477, -0.9583,  0.8526],\n          [-1.2112, -0.6760,  0.9378]],\n\n         [[-0.3219, -2.4580, -0.3647],\n          [-0.6744,  0.4171, -0.0264]]]], grad_fn=<NativeLayerNormBackward0>)\n```\n\nlayernorm  \n\n```python\n# ln\n\nmanual_normed = []\n# channel\nfor b in range(batch_size):\n    # \n    mean = torch.mean(inputs[b, :, :, :])\n    std = torch.std(inputs[b, :, :, :], unbiased=False)\n    normed = (inputs[b, :, :, :] - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\n    normed = normed.unsqueeze(0)\n    manual_normed.append(normed)\nmanual_normed = torch.cat(manual_normed, 0)\nprint('ln:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\nchannel  \n\n  \n\n```python\nln:\n tensor([[[[ 0.3594, -0.8338,  1.3456],\n          [ 0.5128, -0.7147, -0.3012]],\n\n         [[-2.5939,  0.5090, -0.3546],\n          [-1.3715,  0.4607,  0.0553]]],\n\n\n        [[[ 0.5477, -0.9583,  0.8527],\n          [-1.2112, -0.6760,  0.9378]],\n\n         [[-0.3219, -2.4581, -0.3647],\n          [-0.6744,  0.4171, -0.0264]]]], grad_fn=<CatBackward0>)\n:\n tensor([[[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]],\n\n\n        [[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]]])\n```\n\n# NLP  \n\nNLP  \n\nNbatch sizeSsequence lengthHhidden size  \n\n```python\n# [N,S,H]\nbatch_size = 2\nseq_len = 3\nhidden_size = 4\ntorch.manual_seed(6)  # \ninputs = torch.randn(batch_size, seq_len, hidden_size)\nprint(':\\n', inputs)\n```\n\n## batchnorm  \n\n  \n\n```python\n# torchbatchnorm\ntorch_bn = nn.BatchNorm1d(num_features=hidden_size, affine=True)  # batchnorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(7)  # \ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(hidden_size))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(hidden_size))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# # \ntorch_normed = torch_bn(inputs.transpose(1, 2)).transpose(1, 2)\nprint('torch bn:\\n', torch_normed)\n```\n\ntransposetranspose  \n\n  \n\n```python\nweight:\n Parameter containing:\ntensor([-0.1468,  0.7861,  0.9468, -1.1143], requires_grad=True)\nbias:\n Parameter containing:\ntensor([ 1.6908, -0.8948, -0.3556,  1.2324], requires_grad=True) \n\ntorch bn:\n tensor([[[ 1.8740, -0.7037, -1.8222,  2.3385],\n         [ 1.7413, -1.8119,  0.3641,  0.0200],\n         [ 1.4615, -0.2676,  0.1081,  1.3450]],\n\n        [[ 1.7084, -1.9653,  1.0169,  0.5785],\n         [ 1.8213, -0.8614, -0.8056,  2.9892],\n         [ 1.5383,  0.2409, -0.9949,  0.1231]]], grad_fn=<TransposeBackward0>)\n```\n\nbatchnorm  \n\n  \n\n```python\n# bn\n\n# \nmean = torch.mean(inputs, dim=(0, 1) , keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=(0, 1), keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias\nprint('bn:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\ndim=(0,1)[N, S, H][NS, H]batchnorm  \n\n  \n\n```python\n:\n tensor([[[-0.2151,  0.5444, -0.2633, -0.5424]]])\n:\n tensor([[[0.7984, 0.3537, 0.7799, 0.7986]]]) \n\nbn:\n tensor([[[ 1.8740, -0.7037, -1.8222,  2.3385],\n         [ 1.7413, -1.8119,  0.3641,  0.0200],\n         [ 1.4615, -0.2676,  0.1081,  1.3450]],\n\n        [[ 1.7084, -1.9653,  1.0169,  0.5785],\n         [ 1.8213, -0.8614, -0.8056,  2.9892],\n         [ 1.5383,  0.2409, -0.9950,  0.1231]]], grad_fn=<AddBackward0>)\n:\n tensor([[[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]],\n\n        [[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]]])\n```\n\n## layernorm  \n\nNLPlayernormhuggingfacebertlayernorm  \n\n```python\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states: torch.normTensor, input_tensor: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n```\n\n  \n\n```python\n# torchlayernorm\ntorch_ln = nn.LayerNorm(normalized_shape=hidden_size, elementwise_affine=True)  # layernorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(8)  # \ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(hidden_size))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(hidden_size))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n```\n\nhidden size  \n\n```python\nweight:\n Parameter containing:\ntensor([ 0.2713, -1.2729,  0.5027,  0.4181], requires_grad=True)\nbias:\n Parameter containing:\ntensor([-0.6394, -0.6608, -0.1433, -0.1043], requires_grad=True) \n\ntorch ln:\n tensor([[[-0.7547, -2.8528, -0.5092, -0.3423],\n         [-1.0957, -0.8780,  0.2388,  0.2097],\n         [-0.3502, -1.6158, -0.3133, -0.7224]],\n\n        [[-0.9134, -0.4490,  0.6868, -0.3029],\n         [-0.7116, -2.5589, -0.1039, -0.6493],\n         [-0.5076, -2.1031, -0.9346, -0.1230]]],\n       grad_fn=<NativeLayerNormBackward0>)\n```\n\n  \n\n```python\n# ln\n\n# \nmean = torch.mean(inputs, dim=2, keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=2, keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\nprint('ln:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\n  \n\n```python\n:\n tensor([[[-0.8469],\n         [ 0.0745],\n         [ 0.3386]],\n\n        [[ 0.1364],\n         [-0.7003],\n         [ 0.2831]]])\n:\n tensor([[[0.8578],\n         [0.3354],\n         [0.6505]],\n\n        [[0.4426],\n         [0.8448],\n         [0.6816]]]) \n```\n\nsampletoken  \n\n  \n\n```python\nln:\n tensor([[[-0.7547, -2.8528, -0.5092, -0.3423],\n         [-1.0957, -0.8780,  0.2388,  0.2097],\n         [-0.3502, -1.6158, -0.3133, -0.7224]],\n\n        [[-0.9134, -0.4490,  0.6868, -0.3029],\n         [-0.7116, -2.5590, -0.1039, -0.6493],\n         [-0.5076, -2.1031, -0.9347, -0.1230]]], grad_fn=<AddBackward0>)\n:\n tensor([[[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]],\n\n        [[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]]])\n```\n\n#   \n\n\n\nbatchnorm  \n\n```python\n# \nbatch_size = 3\nfeature_num = 4\ntorch.manual_seed(0)  # \ninputs = torch.randn(batch_size, feature_num)\nprint(':\\n', inputs)\n\n# \nmean = torch.mean(inputs, dim=0, keepdim=True)\n# print(':\\n', mean)\nstd = torch.std(inputs, dim=0, keepdim=True, unbiased=False)\n# print(':\\n', std, '\\n')\n\n# torchbatchnorm\ntorch_bn = nn.BatchNorm1d(num_features=feature_num, affine=True)\n\n# \ntorch_bn.weight = nn.Parameter(std)\ntorch_bn.bias =  nn.Parameter(mean)\n# print('weight:\\n', torch_bn.weight)\n# print('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_bn(inputs)\nprint('torch bn:\\n', torch_normed)\n\nisclose = torch.isclose(torch_normed, inputs, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\n  \n\n```python\n:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\ntorch bn:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1821]],\n       grad_fn=<NativeBatchNormBackward0>)\n:\n tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\nbatchnorm  \n\nlayernorm  \n\n```python\nprint(':\\n', inputs)\n\n# \nmean = torch.mean(inputs, dim=1, keepdim=True)\n# print(':\\n', mean)\nstd = torch.std(inputs, dim=1, keepdim=True, unbiased=False)\n# print(':\\n', std, '\\n')\n\n# torchlayernorm\ntorch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=True)  # layernorm\n\n# \ntorch_bn.weight = nn.Parameter(std)\ntorch_bn.bias =  nn.Parameter(mean)\n# print('weight:\\n', torch_bn.weight)\n# print('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n\nisclose = torch.isclose(torch_normed, inputs, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\n\n\n```python\n:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\ntorch ln:\n tensor([[ 1.1918, -0.1481, -1.5251,  0.4814],\n        [-0.8146, -1.1451,  0.7512,  1.2086],\n        [-0.9685, -0.0551, -0.6140,  1.6376]],\n       grad_fn=<NativeLayerNormBackward0>)\n:\n tensor([[False, False, False, False],\n        [False, False, False, False],\n        [False, False, False, False]])\n```\n\nlayernormlayernorm  \n\nCVNLP\n\nbatchnormlayernorm  \n\n# \n\nbatchnormlayernorm  \n\nbatchnormlayernorm  \n\nbatchnorm\"\"\"\"batchbatchbatchfeature map  \n\nlayernorm  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***\n\n# Reference  \n1LAYERNORM https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html  \n2BATCHNORM1D https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html  \n3BATCHNORM2D https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html  \n","source":"_posts/cs/nlp/2024/04/normalization-.md","raw":"---\ntitle: normalization-\nabbrlink: b70b4a2d\ndate: 2024-04-06 12:24:25\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - layernorm\n  - normalization\n  - batchnorm\ncategories:\n  - CS\n  - NLP\n  - LLM\n\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)normalizationnorm  \n\nnorm  \n\n[https://github.com/Saicat/normalization_exp](https://github.com/Saicat/normalization_exp)\n\n# \n\nnormalizationCNN  \n\n```python\nimport torch\nfrom torch import nn\n\n# epsilon\neps = 1e-8\n\n# \nbatch_size = 3\nfeature_num = 4\ntorch.manual_seed(0)  # \ninputs = torch.randn(batch_size, feature_num)\nprint(':\\n', inputs)\n```\n\n34batch size=34  \n\n```python\n:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\n```\n\n## batchnorm  \n\npytorchBatchNorm1d  \n\n```python\n# torchbatchnorm\ntorch_bn = nn.BatchNorm1d(num_features=feature_num, affine=True)  # batchnorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(1)  # \ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(feature_num))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(feature_num))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_bn(inputs)\nprint('torch bn:\\n', torch_normed)\n```\n\nbatchnorm/layernorm+BatchNorm\"affine\"\"affine\"False  \n\npytorchnorm1.00  \n\n```python\nweight:\n Parameter containing:\ntensor([0.6614, 0.2669, 0.0617, 0.6213], requires_grad=True)\nbias:\n Parameter containing:\ntensor([-0.4519, -0.1661, -1.5228,  0.3817], requires_grad=True) \n```\n\nweightbias4  \n\nbatchnorm  \n\n```python\ntorch bn:\n tensor([[ 0.4756,  0.0513, -1.6033,  0.4715],\n        [-1.0197, -0.5421, -1.4535,  1.0937],\n        [-0.8117, -0.0077, -1.5115, -0.4202]],\n       grad_fn=<NativeBatchNormBackward0>)\n```\n\nbatchnorm  \n\n```python\n# bn\n\n# \nmean = torch.mean(inputs, dim=0, keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=0, keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias\nprint('bn:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(isclose)\n```\n\ndim=0batchsamplefeature    \n\n```python\n:\n tensor([[-0.0876, -0.6985, -0.7907,  0.5295]])\n:\n tensor([[1.1612, 0.4971, 1.0630, 0.2692]]) \n```\n\nmeanstdkeepdimTrueFalsebroadcast  \n\nstdunbiasedFalsetorchbatchnorm  \n\nbatchnorm  \n\n```python\nbn:\n tensor([[ 0.4756,  0.0514, -1.6033,  0.4715],\n        [-1.0197, -0.5421, -1.4535,  1.0937],\n        [-0.8117, -0.0077, -1.5115, -0.4202]], grad_fn=<AddBackward0>)\n```\n\ntorch.isclosebatchnormbatchnorm  \n\n```python\ntensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\nequal1e-5~1e-4eps  \n\n## layernorm  \n\nlayernorm34\n\ntorch\n\n```python\n# torchlayernorm\ntorch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=True)  # layernorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(2)  # \ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(feature_num))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(feature_num))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n```\n\nlayernorm  \n\n```python\nweight:\n Parameter containing:\ntensor([ 0.3923, -0.2236, -0.3195, -1.2050], requires_grad=True)\nbias:\n Parameter containing:\ntensor([ 1.0445, -0.6332,  0.5731,  0.5409], requires_grad=True) \n```\n  \n\nlayernorm  \n\n```python\ntorch ln:\n tensor([[ 1.5120, -0.6001,  1.0604, -0.0392],\n        [ 0.7249, -0.3772,  0.3331, -0.9155],\n        [ 0.6645, -0.6209,  0.7693, -1.4324]],\n       grad_fn=<NativeLayerNormBackward0>)\n```\n\n  \n\ndim=1  \n\n```python\n# ln\n\n# \nmean = torch.mean(inputs, dim=1, keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=1, keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\nprint('ln:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(isclose)\n```\n\n  \n\n```python\n:\n tensor([[-0.0907],\n        [-0.3104],\n        [-0.3843]])\n:\n tensor([[1.3691],\n        [0.9502],\n        [0.3458]]) \n```\n\n  \n\n```python\nln:\n tensor([[ 1.5120, -0.6001,  1.0604, -0.0392],\n        [ 0.7249, -0.3772,  0.3331, -0.9155],\n        [ 0.6645, -0.6209,  0.7693, -1.4325]], grad_fn=<AddBackward0>)\n:\n tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\n##   \n\nbatchnormlayernorm  \n\n{% asset_img bn_and_ln.png bnln %}  \n\nbatchnormdim=0batchlayernormdim=1  \n\nbatchnormlayernorm  \n\n# CV\n\nCV  \n\nCV[N,C,H,W]Nbatch sizeCchannelHWfeature mapCV  \n\n```python\n# [N,C,H,W]\nbatch_size = 2\nchannel = 2\nheight = 2\nwidth = 3\ntorch.manual_seed(3)  # \ninputs = torch.randn(batch_size, channel, height, width)\nprint(':\\n', inputs)\n```\n\n  \n\n```python\n:\n tensor([[[[-0.0766,  0.3599, -0.7820],\n          [ 0.0715,  0.6648, -0.2868]],\n\n         [[ 1.6206, -1.5967,  0.4046],\n          [ 0.6113,  0.7604, -0.0336]]],\n\n\n        [[[-0.3448,  0.4937, -0.0776],\n          [-1.8054,  0.4851,  0.2052]],\n\n         [[ 0.3384,  1.3528,  0.3736],\n          [ 0.0134,  0.7737, -0.1092]]]])\n```\n\n## batchnorm  \n\nBatchNorm2dchannel  \n\n```python\n# torchbatchnorm\ntorch_bn = nn.BatchNorm2d(num_features=channel, affine=True)  # batchnorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(4)  # \ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(channel))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(channel))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_bn(inputs)\nprint('torch bn:\\n', torch_normed)\n```\n\nchannel  \n\n```python\nweight:\n Parameter containing:\ntensor([-1.6053,  0.2325], requires_grad=True)\nbias:\n Parameter containing:\ntensor([2.2399, 0.8473], requires_grad=True) \n```\n\ntorchbatchnorm2d  \n\n```python\ntorch bn:\n tensor([[[[2.2043, 1.1275, 3.9442],\n          [1.8388, 0.3753, 2.7226]],\n\n         [[1.2185, 0.2591, 0.8559],\n          [0.9175, 0.9620, 0.7252]]],\n\n\n        [[[2.8658, 0.7975, 2.2066],\n          [6.4684, 0.8186, 1.5090]],\n\n         [[0.8362, 1.1387, 0.8467],\n          [0.7392, 0.9660, 0.7027]]]], grad_fn=<NativeBatchNormBackward0>)\n```\n\nbatchnorm2d  \n\n```python\n# bn\n\nmanual_normed = []\n# channel\nfor c in range(channel):\n    # \n    mean = torch.mean(inputs[:, c, :, :])\n    std = torch.std(inputs[:, c, :, :], unbiased=False)\n    normed = (inputs[:, c, :, :] - mean) / (std + eps) * torch_bn.weight[c] + torch_bn.bias[c]\n    normed = normed.unsqueeze(1)\n    manual_normed.append(normed)\nmanual_normed = torch.cat(manual_normed, 1)\nprint('bn:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\nCVchannel\"batch\"  \n\nNHW  \n\n{% asset_img cv_batchnorm.png CVbatchnorm %}  \n\n  \n\n```python\nbn:\n tensor([[[[2.2043, 1.1275, 3.9442],\n          [1.8388, 0.3752, 2.7226]],\n\n         [[1.2185, 0.2591, 0.8559],\n          [0.9175, 0.9620, 0.7252]]],\n\n\n        [[[2.8658, 0.7975, 2.2066],\n          [6.4685, 0.8186, 1.5089]],\n\n         [[0.8362, 1.1387, 0.8467],\n          [0.7392, 0.9660, 0.7027]]]], grad_fn=<CatBackward0>)\n:\n tensor([[[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]],\n\n\n        [[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]]])\n```\n\n## layernorm  \n\n[torchlayernorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)layernorm  \n\n```python\n# torchlayernorm\ntorch_ln = nn.LayerNorm(\n    normalized_shape=[channel, height, width], \n    elementwise_affine=True\n)  # layernorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(5)  # \ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(channel, height, width))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(channel, height, width))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n```\n\n\n\n{% asset_img cv_layernorm.jpeg CVlayernorm %}  \n\n[channel, height, width]  \n\n```python\nweight:\n Parameter containing:\ntensor([[[-0.4868, -0.6038, -0.5581],\n         [ 0.6675, -0.1974,  1.9428]],\n\n        [[-1.4017, -0.7626,  0.6312],\n         [-0.8991, -0.5578,  0.6907]]], requires_grad=True)\nbias:\n Parameter containing:\ntensor([[[ 0.2225, -0.6662,  0.6846],\n         [ 0.5740, -0.5829,  0.7679]],\n\n        [[ 0.0571, -1.1894, -0.5659],\n         [-0.8327,  0.9014,  0.2116]]], requires_grad=True) \n```\n\nchannel  \n\nlayernorm  \n\n```python\ntorch ln:\n tensor([[[[ 0.3594, -0.8338,  1.3456],\n          [ 0.5128, -0.7147, -0.3012]],\n\n         [[-2.5939,  0.5089, -0.3546],\n          [-1.3715,  0.4607,  0.0553]]],\n\n\n        [[[ 0.5477, -0.9583,  0.8526],\n          [-1.2112, -0.6760,  0.9378]],\n\n         [[-0.3219, -2.4580, -0.3647],\n          [-0.6744,  0.4171, -0.0264]]]], grad_fn=<NativeLayerNormBackward0>)\n```\n\nlayernorm  \n\n```python\n# ln\n\nmanual_normed = []\n# channel\nfor b in range(batch_size):\n    # \n    mean = torch.mean(inputs[b, :, :, :])\n    std = torch.std(inputs[b, :, :, :], unbiased=False)\n    normed = (inputs[b, :, :, :] - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\n    normed = normed.unsqueeze(0)\n    manual_normed.append(normed)\nmanual_normed = torch.cat(manual_normed, 0)\nprint('ln:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\nchannel  \n\n  \n\n```python\nln:\n tensor([[[[ 0.3594, -0.8338,  1.3456],\n          [ 0.5128, -0.7147, -0.3012]],\n\n         [[-2.5939,  0.5090, -0.3546],\n          [-1.3715,  0.4607,  0.0553]]],\n\n\n        [[[ 0.5477, -0.9583,  0.8527],\n          [-1.2112, -0.6760,  0.9378]],\n\n         [[-0.3219, -2.4581, -0.3647],\n          [-0.6744,  0.4171, -0.0264]]]], grad_fn=<CatBackward0>)\n:\n tensor([[[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]],\n\n\n        [[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]]])\n```\n\n# NLP  \n\nNLP  \n\nNbatch sizeSsequence lengthHhidden size  \n\n```python\n# [N,S,H]\nbatch_size = 2\nseq_len = 3\nhidden_size = 4\ntorch.manual_seed(6)  # \ninputs = torch.randn(batch_size, seq_len, hidden_size)\nprint(':\\n', inputs)\n```\n\n## batchnorm  \n\n  \n\n```python\n# torchbatchnorm\ntorch_bn = nn.BatchNorm1d(num_features=hidden_size, affine=True)  # batchnorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(7)  # \ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(hidden_size))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(hidden_size))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# # \ntorch_normed = torch_bn(inputs.transpose(1, 2)).transpose(1, 2)\nprint('torch bn:\\n', torch_normed)\n```\n\ntransposetranspose  \n\n  \n\n```python\nweight:\n Parameter containing:\ntensor([-0.1468,  0.7861,  0.9468, -1.1143], requires_grad=True)\nbias:\n Parameter containing:\ntensor([ 1.6908, -0.8948, -0.3556,  1.2324], requires_grad=True) \n\ntorch bn:\n tensor([[[ 1.8740, -0.7037, -1.8222,  2.3385],\n         [ 1.7413, -1.8119,  0.3641,  0.0200],\n         [ 1.4615, -0.2676,  0.1081,  1.3450]],\n\n        [[ 1.7084, -1.9653,  1.0169,  0.5785],\n         [ 1.8213, -0.8614, -0.8056,  2.9892],\n         [ 1.5383,  0.2409, -0.9949,  0.1231]]], grad_fn=<TransposeBackward0>)\n```\n\nbatchnorm  \n\n  \n\n```python\n# bn\n\n# \nmean = torch.mean(inputs, dim=(0, 1) , keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=(0, 1), keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias\nprint('bn:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\ndim=(0,1)[N, S, H][NS, H]batchnorm  \n\n  \n\n```python\n:\n tensor([[[-0.2151,  0.5444, -0.2633, -0.5424]]])\n:\n tensor([[[0.7984, 0.3537, 0.7799, 0.7986]]]) \n\nbn:\n tensor([[[ 1.8740, -0.7037, -1.8222,  2.3385],\n         [ 1.7413, -1.8119,  0.3641,  0.0200],\n         [ 1.4615, -0.2676,  0.1081,  1.3450]],\n\n        [[ 1.7084, -1.9653,  1.0169,  0.5785],\n         [ 1.8213, -0.8614, -0.8056,  2.9892],\n         [ 1.5383,  0.2409, -0.9950,  0.1231]]], grad_fn=<AddBackward0>)\n:\n tensor([[[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]],\n\n        [[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]]])\n```\n\n## layernorm  \n\nNLPlayernormhuggingfacebertlayernorm  \n\n```python\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states: torch.normTensor, input_tensor: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n```\n\n  \n\n```python\n# torchlayernorm\ntorch_ln = nn.LayerNorm(normalized_shape=hidden_size, elementwise_affine=True)  # layernorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(8)  # \ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(hidden_size))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(hidden_size))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n```\n\nhidden size  \n\n```python\nweight:\n Parameter containing:\ntensor([ 0.2713, -1.2729,  0.5027,  0.4181], requires_grad=True)\nbias:\n Parameter containing:\ntensor([-0.6394, -0.6608, -0.1433, -0.1043], requires_grad=True) \n\ntorch ln:\n tensor([[[-0.7547, -2.8528, -0.5092, -0.3423],\n         [-1.0957, -0.8780,  0.2388,  0.2097],\n         [-0.3502, -1.6158, -0.3133, -0.7224]],\n\n        [[-0.9134, -0.4490,  0.6868, -0.3029],\n         [-0.7116, -2.5589, -0.1039, -0.6493],\n         [-0.5076, -2.1031, -0.9346, -0.1230]]],\n       grad_fn=<NativeLayerNormBackward0>)\n```\n\n  \n\n```python\n# ln\n\n# \nmean = torch.mean(inputs, dim=2, keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=2, keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\nprint('ln:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\n  \n\n```python\n:\n tensor([[[-0.8469],\n         [ 0.0745],\n         [ 0.3386]],\n\n        [[ 0.1364],\n         [-0.7003],\n         [ 0.2831]]])\n:\n tensor([[[0.8578],\n         [0.3354],\n         [0.6505]],\n\n        [[0.4426],\n         [0.8448],\n         [0.6816]]]) \n```\n\nsampletoken  \n\n  \n\n```python\nln:\n tensor([[[-0.7547, -2.8528, -0.5092, -0.3423],\n         [-1.0957, -0.8780,  0.2388,  0.2097],\n         [-0.3502, -1.6158, -0.3133, -0.7224]],\n\n        [[-0.9134, -0.4490,  0.6868, -0.3029],\n         [-0.7116, -2.5590, -0.1039, -0.6493],\n         [-0.5076, -2.1031, -0.9347, -0.1230]]], grad_fn=<AddBackward0>)\n:\n tensor([[[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]],\n\n        [[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]]])\n```\n\n#   \n\n\n\nbatchnorm  \n\n```python\n# \nbatch_size = 3\nfeature_num = 4\ntorch.manual_seed(0)  # \ninputs = torch.randn(batch_size, feature_num)\nprint(':\\n', inputs)\n\n# \nmean = torch.mean(inputs, dim=0, keepdim=True)\n# print(':\\n', mean)\nstd = torch.std(inputs, dim=0, keepdim=True, unbiased=False)\n# print(':\\n', std, '\\n')\n\n# torchbatchnorm\ntorch_bn = nn.BatchNorm1d(num_features=feature_num, affine=True)\n\n# \ntorch_bn.weight = nn.Parameter(std)\ntorch_bn.bias =  nn.Parameter(mean)\n# print('weight:\\n', torch_bn.weight)\n# print('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_bn(inputs)\nprint('torch bn:\\n', torch_normed)\n\nisclose = torch.isclose(torch_normed, inputs, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\n  \n\n```python\n:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\ntorch bn:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1821]],\n       grad_fn=<NativeBatchNormBackward0>)\n:\n tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\nbatchnorm  \n\nlayernorm  \n\n```python\nprint(':\\n', inputs)\n\n# \nmean = torch.mean(inputs, dim=1, keepdim=True)\n# print(':\\n', mean)\nstd = torch.std(inputs, dim=1, keepdim=True, unbiased=False)\n# print(':\\n', std, '\\n')\n\n# torchlayernorm\ntorch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=True)  # layernorm\n\n# \ntorch_bn.weight = nn.Parameter(std)\ntorch_bn.bias =  nn.Parameter(mean)\n# print('weight:\\n', torch_bn.weight)\n# print('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n\nisclose = torch.isclose(torch_normed, inputs, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\n\n\n```python\n:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\ntorch ln:\n tensor([[ 1.1918, -0.1481, -1.5251,  0.4814],\n        [-0.8146, -1.1451,  0.7512,  1.2086],\n        [-0.9685, -0.0551, -0.6140,  1.6376]],\n       grad_fn=<NativeLayerNormBackward0>)\n:\n tensor([[False, False, False, False],\n        [False, False, False, False],\n        [False, False, False, False]])\n```\n\nlayernormlayernorm  \n\nCVNLP\n\nbatchnormlayernorm  \n\n# \n\nbatchnormlayernorm  \n\nbatchnormlayernorm  \n\nbatchnorm\"\"\"\"batchbatchbatchfeature map  \n\nlayernorm  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***\n\n# Reference  \n1LAYERNORM https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html  \n2BATCHNORM1D https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html  \n3BATCHNORM2D https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html  \n","slug":"cs/nlp/2024/04/normalization-","published":1,"updated":"2024-05-10T06:51:09.591Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmfh000lam4k3yolgekg","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a>normalizationnorm</p>\n<p>norm</p>\n<p><a href=\"https://github.com/Saicat/normalization_exp\">https://github.com/Saicat/normalization_exp</a></p>\n<h1 id=\"\"></h1>\n<p>normalizationCNN</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># epsilon</span></span><br><span class=\"line\">eps = <span class=\"number\">1e-8</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">batch_size = <span class=\"number\">3</span></span><br><span class=\"line\">feature_num = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">0</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, feature_num)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<p>34batch\nsize=34</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm\">batchnorm</h2>\n<p>pytorchBatchNorm1d</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=feature_num, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># batchnorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">1</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(feature_num))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(feature_num))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>batchnorm/layernorm+BatchNorm\"affine\"\"affine\"False</p>\n<p>pytorchnorm1.00</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([<span class=\"number\">0.6614</span>, <span class=\"number\">0.2669</span>, <span class=\"number\">0.0617</span>, <span class=\"number\">0.6213</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.4519</span>, -<span class=\"number\">0.1661</span>, -<span class=\"number\">1.5228</span>,  <span class=\"number\">0.3817</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>weightbias4</p>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">0.4756</span>,  <span class=\"number\">0.0513</span>, -<span class=\"number\">1.6033</span>,  <span class=\"number\">0.4715</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0197</span>, -<span class=\"number\">0.5421</span>, -<span class=\"number\">1.4535</span>,  <span class=\"number\">1.0937</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8117</span>, -<span class=\"number\">0.0077</span>, -<span class=\"number\">1.5115</span>, -<span class=\"number\">0.4202</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># bn</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bn:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(isclose)</span><br></pre></td></tr></table></figure>\n<p>dim=0batchsamplefeature</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[-<span class=\"number\">0.0876</span>, -<span class=\"number\">0.6985</span>, -<span class=\"number\">0.7907</span>,  <span class=\"number\">0.5295</span>]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"number\">1.1612</span>, <span class=\"number\">0.4971</span>, <span class=\"number\">1.0630</span>, <span class=\"number\">0.2692</span>]]) </span><br></pre></td></tr></table></figure>\n<p>meanstdkeepdimTrueFalsebroadcast</p>\n<p>stdunbiasedFalsetorchbatchnorm</p>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bn:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">0.4756</span>,  <span class=\"number\">0.0514</span>, -<span class=\"number\">1.6033</span>,  <span class=\"number\">0.4715</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0197</span>, -<span class=\"number\">0.5421</span>, -<span class=\"number\">1.4535</span>,  <span class=\"number\">1.0937</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8117</span>, -<span class=\"number\">0.0077</span>, -<span class=\"number\">1.5115</span>, -<span class=\"number\">0.4202</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>torch.isclosebatchnormbatchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<p>equal1e-5~1e-4eps</p>\n<h2 id=\"layernorm\">layernorm</h2>\n<p>layernorm34</p>\n<p>torch</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">2</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(feature_num))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(feature_num))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">0.3923</span>, -<span class=\"number\">0.2236</span>, -<span class=\"number\">0.3195</span>, -<span class=\"number\">1.2050</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">1.0445</span>, -<span class=\"number\">0.6332</span>,  <span class=\"number\">0.5731</span>,  <span class=\"number\">0.5409</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p></p>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5120</span>, -<span class=\"number\">0.6001</span>,  <span class=\"number\">1.0604</span>, -<span class=\"number\">0.0392</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.7249</span>, -<span class=\"number\">0.3772</span>,  <span class=\"number\">0.3331</span>, -<span class=\"number\">0.9155</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.6645</span>, -<span class=\"number\">0.6209</span>,  <span class=\"number\">0.7693</span>, -<span class=\"number\">1.4324</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p></p>\n<p>dim=1</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ln</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;ln:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[-<span class=\"number\">0.0907</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.3104</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.3843</span>]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"number\">1.3691</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.9502</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.3458</span>]]) </span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5120</span>, -<span class=\"number\">0.6001</span>,  <span class=\"number\">1.0604</span>, -<span class=\"number\">0.0392</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.7249</span>, -<span class=\"number\">0.3772</span>,  <span class=\"number\">0.3331</span>, -<span class=\"number\">0.9155</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.6645</span>, -<span class=\"number\">0.6209</span>,  <span class=\"number\">0.7693</span>, -<span class=\"number\">1.4325</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"\"></h2>\n<p>batchnormlayernorm</p>\n<img src=\"/b70b4a2d/bn_and_ln.png\" class title=\"bnln\">\n<p>batchnormdim=0batchlayernormdim=1</p>\n<p>batchnormlayernorm</p>\n<h1 id=\"cv\">CV</h1>\n<p>CV</p>\n<p>CV[N,C,H,W]Nbatch\nsizeCchannelHWfeature\nmapCV</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># [N,C,H,W]</span></span><br><span class=\"line\">batch_size = <span class=\"number\">2</span></span><br><span class=\"line\">channel = <span class=\"number\">2</span></span><br><span class=\"line\">height = <span class=\"number\">2</span></span><br><span class=\"line\">width = <span class=\"number\">3</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">3</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, channel, height, width)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[[-<span class=\"number\">0.0766</span>,  <span class=\"number\">0.3599</span>, -<span class=\"number\">0.7820</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.0715</span>,  <span class=\"number\">0.6648</span>, -<span class=\"number\">0.2868</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[ <span class=\"number\">1.6206</span>, -<span class=\"number\">1.5967</span>,  <span class=\"number\">0.4046</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.6113</span>,  <span class=\"number\">0.7604</span>, -<span class=\"number\">0.0336</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[-<span class=\"number\">0.3448</span>,  <span class=\"number\">0.4937</span>, -<span class=\"number\">0.0776</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.8054</span>,  <span class=\"number\">0.4851</span>,  <span class=\"number\">0.2052</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[ <span class=\"number\">0.3384</span>,  <span class=\"number\">1.3528</span>,  <span class=\"number\">0.3736</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.0134</span>,  <span class=\"number\">0.7737</span>, -<span class=\"number\">0.1092</span>]]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm-1\">batchnorm</h2>\n<p>BatchNorm2dchannel</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm2d(num_features=channel, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># batchnorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">4</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(channel))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(channel))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>channel</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">1.6053</span>,  <span class=\"number\">0.2325</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([<span class=\"number\">2.2399</span>, <span class=\"number\">0.8473</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>torchbatchnorm2d</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[[[<span class=\"number\">2.2043</span>, <span class=\"number\">1.1275</span>, <span class=\"number\">3.9442</span>],</span><br><span class=\"line\">          [<span class=\"number\">1.8388</span>, <span class=\"number\">0.3753</span>, <span class=\"number\">2.7226</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">1.2185</span>, <span class=\"number\">0.2591</span>, <span class=\"number\">0.8559</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.9175</span>, <span class=\"number\">0.9620</span>, <span class=\"number\">0.7252</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"number\">2.8658</span>, <span class=\"number\">0.7975</span>, <span class=\"number\">2.2066</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.4684</span>, <span class=\"number\">0.8186</span>, <span class=\"number\">1.5090</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">0.8362</span>, <span class=\"number\">1.1387</span>, <span class=\"number\">0.8467</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.7392</span>, <span class=\"number\">0.9660</span>, <span class=\"number\">0.7027</span>]]]], grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>batchnorm2d</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># bn</span></span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = []</span><br><span class=\"line\"><span class=\"comment\"># channel</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(channel):</span><br><span class=\"line\">    <span class=\"comment\"># </span></span><br><span class=\"line\">    mean = torch.mean(inputs[:, c, :, :])</span><br><span class=\"line\">    std = torch.std(inputs[:, c, :, :], unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    normed = (inputs[:, c, :, :] - mean) / (std + eps) * torch_bn.weight[c] + torch_bn.bias[c]</span><br><span class=\"line\">    normed = normed.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">    manual_normed.append(normed)</span><br><span class=\"line\">manual_normed = torch.cat(manual_normed, <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bn:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>CVchannel\"batch\"</p>\n<p>NHW</p>\n<img src=\"/b70b4a2d/cv_batchnorm.png\" class title=\"CVbatchnorm\">\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bn:</span><br><span class=\"line\"> tensor([[[[<span class=\"number\">2.2043</span>, <span class=\"number\">1.1275</span>, <span class=\"number\">3.9442</span>],</span><br><span class=\"line\">          [<span class=\"number\">1.8388</span>, <span class=\"number\">0.3752</span>, <span class=\"number\">2.7226</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">1.2185</span>, <span class=\"number\">0.2591</span>, <span class=\"number\">0.8559</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.9175</span>, <span class=\"number\">0.9620</span>, <span class=\"number\">0.7252</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"number\">2.8658</span>, <span class=\"number\">0.7975</span>, <span class=\"number\">2.2066</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.4685</span>, <span class=\"number\">0.8186</span>, <span class=\"number\">1.5089</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">0.8362</span>, <span class=\"number\">1.1387</span>, <span class=\"number\">0.8467</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.7392</span>, <span class=\"number\">0.9660</span>, <span class=\"number\">0.7027</span>]]]], grad_fn=&lt;CatBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"layernorm-1\">layernorm</h2>\n<p><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\">torchlayernorm</a>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(</span><br><span class=\"line\">    normalized_shape=[channel, height, width], </span><br><span class=\"line\">    elementwise_affine=<span class=\"literal\">True</span></span><br><span class=\"line\">)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">5</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(channel, height, width))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(channel, height, width))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p></p>\n<img src=\"/b70b4a2d/cv_layernorm.jpeg\" class title=\"CVlayernorm\">\n<p>[channel, height, width]</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([[[-<span class=\"number\">0.4868</span>, -<span class=\"number\">0.6038</span>, -<span class=\"number\">0.5581</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.6675</span>, -<span class=\"number\">0.1974</span>,  <span class=\"number\">1.9428</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">1.4017</span>, -<span class=\"number\">0.7626</span>,  <span class=\"number\">0.6312</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.8991</span>, -<span class=\"number\">0.5578</span>,  <span class=\"number\">0.6907</span>]]], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([[[ <span class=\"number\">0.2225</span>, -<span class=\"number\">0.6662</span>,  <span class=\"number\">0.6846</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.5740</span>, -<span class=\"number\">0.5829</span>,  <span class=\"number\">0.7679</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">0.0571</span>, -<span class=\"number\">1.1894</span>, -<span class=\"number\">0.5659</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.8327</span>,  <span class=\"number\">0.9014</span>,  <span class=\"number\">0.2116</span>]]], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>channel</p>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[[[ <span class=\"number\">0.3594</span>, -<span class=\"number\">0.8338</span>,  <span class=\"number\">1.3456</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.5128</span>, -<span class=\"number\">0.7147</span>, -<span class=\"number\">0.3012</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">2.5939</span>,  <span class=\"number\">0.5089</span>, -<span class=\"number\">0.3546</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.3715</span>,  <span class=\"number\">0.4607</span>,  <span class=\"number\">0.0553</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[ <span class=\"number\">0.5477</span>, -<span class=\"number\">0.9583</span>,  <span class=\"number\">0.8526</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.2112</span>, -<span class=\"number\">0.6760</span>,  <span class=\"number\">0.9378</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">0.3219</span>, -<span class=\"number\">2.4580</span>, -<span class=\"number\">0.3647</span>],</span><br><span class=\"line\">          [-<span class=\"number\">0.6744</span>,  <span class=\"number\">0.4171</span>, -<span class=\"number\">0.0264</span>]]]], grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ln</span></span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = []</span><br><span class=\"line\"><span class=\"comment\"># channel</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> b <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(batch_size):</span><br><span class=\"line\">    <span class=\"comment\"># </span></span><br><span class=\"line\">    mean = torch.mean(inputs[b, :, :, :])</span><br><span class=\"line\">    std = torch.std(inputs[b, :, :, :], unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    normed = (inputs[b, :, :, :] - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\">    normed = normed.unsqueeze(<span class=\"number\">0</span>)</span><br><span class=\"line\">    manual_normed.append(normed)</span><br><span class=\"line\">manual_normed = torch.cat(manual_normed, <span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;ln:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>channel</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln:</span><br><span class=\"line\"> tensor([[[[ <span class=\"number\">0.3594</span>, -<span class=\"number\">0.8338</span>,  <span class=\"number\">1.3456</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.5128</span>, -<span class=\"number\">0.7147</span>, -<span class=\"number\">0.3012</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">2.5939</span>,  <span class=\"number\">0.5090</span>, -<span class=\"number\">0.3546</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.3715</span>,  <span class=\"number\">0.4607</span>,  <span class=\"number\">0.0553</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[ <span class=\"number\">0.5477</span>, -<span class=\"number\">0.9583</span>,  <span class=\"number\">0.8527</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.2112</span>, -<span class=\"number\">0.6760</span>,  <span class=\"number\">0.9378</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">0.3219</span>, -<span class=\"number\">2.4581</span>, -<span class=\"number\">0.3647</span>],</span><br><span class=\"line\">          [-<span class=\"number\">0.6744</span>,  <span class=\"number\">0.4171</span>, -<span class=\"number\">0.0264</span>]]]], grad_fn=&lt;CatBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]]])</span><br></pre></td></tr></table></figure>\n<h1 id=\"nlp\">NLP</h1>\n<p>NLP</p>\n<p>Nbatch sizeSsequence lengthHhidden size</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># [N,S,H]</span></span><br><span class=\"line\">batch_size = <span class=\"number\">2</span></span><br><span class=\"line\">seq_len = <span class=\"number\">3</span></span><br><span class=\"line\">hidden_size = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">6</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, seq_len, hidden_size)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm-2\">batchnorm</h2>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=hidden_size, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># batchnorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">7</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(hidden_size))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(hidden_size))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># # </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>transposetranspose</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.1468</span>,  <span class=\"number\">0.7861</span>,  <span class=\"number\">0.9468</span>, -<span class=\"number\">1.1143</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">1.6908</span>, -<span class=\"number\">0.8948</span>, -<span class=\"number\">0.3556</span>,  <span class=\"number\">1.2324</span>], requires_grad=<span class=\"literal\">True</span>) </span><br><span class=\"line\"></span><br><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[[ <span class=\"number\">1.8740</span>, -<span class=\"number\">0.7037</span>, -<span class=\"number\">1.8222</span>,  <span class=\"number\">2.3385</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.7413</span>, -<span class=\"number\">1.8119</span>,  <span class=\"number\">0.3641</span>,  <span class=\"number\">0.0200</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.4615</span>, -<span class=\"number\">0.2676</span>,  <span class=\"number\">0.1081</span>,  <span class=\"number\">1.3450</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">1.7084</span>, -<span class=\"number\">1.9653</span>,  <span class=\"number\">1.0169</span>,  <span class=\"number\">0.5785</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.8213</span>, -<span class=\"number\">0.8614</span>, -<span class=\"number\">0.8056</span>,  <span class=\"number\">2.9892</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.5383</span>,  <span class=\"number\">0.2409</span>, -<span class=\"number\">0.9949</span>,  <span class=\"number\">0.1231</span>]]], grad_fn=&lt;TransposeBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>batchnorm</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># bn</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=(<span class=\"number\">0</span>, <span class=\"number\">1</span>) , keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=(<span class=\"number\">0</span>, <span class=\"number\">1</span>), keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bn:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>dim=(0,1)[N,\nS, H][NS,\nH]batchnorm</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.2151</span>,  <span class=\"number\">0.5444</span>, -<span class=\"number\">0.2633</span>, -<span class=\"number\">0.5424</span>]]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"number\">0.7984</span>, <span class=\"number\">0.3537</span>, <span class=\"number\">0.7799</span>, <span class=\"number\">0.7986</span>]]]) </span><br><span class=\"line\"></span><br><span class=\"line\">bn:</span><br><span class=\"line\"> tensor([[[ <span class=\"number\">1.8740</span>, -<span class=\"number\">0.7037</span>, -<span class=\"number\">1.8222</span>,  <span class=\"number\">2.3385</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.7413</span>, -<span class=\"number\">1.8119</span>,  <span class=\"number\">0.3641</span>,  <span class=\"number\">0.0200</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.4615</span>, -<span class=\"number\">0.2676</span>,  <span class=\"number\">0.1081</span>,  <span class=\"number\">1.3450</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">1.7084</span>, -<span class=\"number\">1.9653</span>,  <span class=\"number\">1.0169</span>,  <span class=\"number\">0.5785</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.8213</span>, -<span class=\"number\">0.8614</span>, -<span class=\"number\">0.8056</span>,  <span class=\"number\">2.9892</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.5383</span>,  <span class=\"number\">0.2409</span>, -<span class=\"number\">0.9950</span>,  <span class=\"number\">0.1231</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"layernorm-2\">layernorm</h2>\n<p>NLPlayernormhuggingfacebertlayernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BertSelfOutput</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, config</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class=\"line\">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class=\"line\">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, hidden_states: torch.normTensor, input_tensor: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class=\"line\">        hidden_states = self.dense(hidden_states)</span><br><span class=\"line\">        hidden_states = self.dropout(hidden_states)</span><br><span class=\"line\">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> hidden_states</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=hidden_size, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">8</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(hidden_size))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(hidden_size))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>hidden size</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">0.2713</span>, -<span class=\"number\">1.2729</span>,  <span class=\"number\">0.5027</span>,  <span class=\"number\">0.4181</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.6394</span>, -<span class=\"number\">0.6608</span>, -<span class=\"number\">0.1433</span>, -<span class=\"number\">0.1043</span>], requires_grad=<span class=\"literal\">True</span>) </span><br><span class=\"line\"></span><br><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.7547</span>, -<span class=\"number\">2.8528</span>, -<span class=\"number\">0.5092</span>, -<span class=\"number\">0.3423</span>],</span><br><span class=\"line\">         [-<span class=\"number\">1.0957</span>, -<span class=\"number\">0.8780</span>,  <span class=\"number\">0.2388</span>,  <span class=\"number\">0.2097</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.3502</span>, -<span class=\"number\">1.6158</span>, -<span class=\"number\">0.3133</span>, -<span class=\"number\">0.7224</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">0.9134</span>, -<span class=\"number\">0.4490</span>,  <span class=\"number\">0.6868</span>, -<span class=\"number\">0.3029</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7116</span>, -<span class=\"number\">2.5589</span>, -<span class=\"number\">0.1039</span>, -<span class=\"number\">0.6493</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.5076</span>, -<span class=\"number\">2.1031</span>, -<span class=\"number\">0.9346</span>, -<span class=\"number\">0.1230</span>]]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ln</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">2</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">2</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;ln:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.8469</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.0745</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.3386</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">0.1364</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7003</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.2831</span>]]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"number\">0.8578</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.3354</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.6505</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"number\">0.4426</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.8448</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.6816</span>]]]) </span><br></pre></td></tr></table></figure>\n<p>sampletoken</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.7547</span>, -<span class=\"number\">2.8528</span>, -<span class=\"number\">0.5092</span>, -<span class=\"number\">0.3423</span>],</span><br><span class=\"line\">         [-<span class=\"number\">1.0957</span>, -<span class=\"number\">0.8780</span>,  <span class=\"number\">0.2388</span>,  <span class=\"number\">0.2097</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.3502</span>, -<span class=\"number\">1.6158</span>, -<span class=\"number\">0.3133</span>, -<span class=\"number\">0.7224</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">0.9134</span>, -<span class=\"number\">0.4490</span>,  <span class=\"number\">0.6868</span>, -<span class=\"number\">0.3029</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7116</span>, -<span class=\"number\">2.5590</span>, -<span class=\"number\">0.1039</span>, -<span class=\"number\">0.6493</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.5076</span>, -<span class=\"number\">2.1031</span>, -<span class=\"number\">0.9347</span>, -<span class=\"number\">0.1230</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]])</span><br></pre></td></tr></table></figure>\n<h1 id=\"\"></h1>\n<p></p>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">batch_size = <span class=\"number\">3</span></span><br><span class=\"line\">feature_num = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">0</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, feature_num)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, mean)</span></span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, std, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=feature_num, affine=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(std)</span><br><span class=\"line\">torch_bn.bias =  nn.Parameter(mean)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;weight:\\n&#x27;, torch_bn.weight)</span></span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;bias:\\n&#x27;, torch_bn.bias, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br><span class=\"line\"></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, inputs, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1821</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<p>batchnorm</p>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, mean)</span></span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, std, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(std)</span><br><span class=\"line\">torch_bn.bias =  nn.Parameter(mean)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;weight:\\n&#x27;, torch_bn.weight)</span></span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;bias:\\n&#x27;, torch_bn.bias, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br><span class=\"line\"></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, inputs, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.1918</span>, -<span class=\"number\">0.1481</span>, -<span class=\"number\">1.5251</span>,  <span class=\"number\">0.4814</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8146</span>, -<span class=\"number\">1.1451</span>,  <span class=\"number\">0.7512</span>,  <span class=\"number\">1.2086</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.9685</span>, -<span class=\"number\">0.0551</span>, -<span class=\"number\">0.6140</span>,  <span class=\"number\">1.6376</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>],</span><br><span class=\"line\">        [<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>],</span><br><span class=\"line\">        [<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>]])</span><br></pre></td></tr></table></figure>\n<p>layernormlayernorm</p>\n<p>CVNLP</p>\n<p>batchnormlayernorm</p>\n<h1 id=\"\"></h1>\n<p>batchnormlayernorm</p>\n<p>batchnormlayernorm</p>\n<p>batchnorm\"\"\"\"batchbatchbatchfeature\nmap</p>\n<p>layernorm</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1LAYERNORM\nhttps://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html<br>\n2BATCHNORM1D\nhttps://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html<br>\n3BATCHNORM2D\nhttps://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html</p>\n","length":18670,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a>normalizationnorm</p>\n<p>norm</p>\n<p><a href=\"https://github.com/Saicat/normalization_exp\">https://github.com/Saicat/normalization_exp</a></p>\n<h1 id=\"\"></h1>\n<p>normalizationCNN</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># epsilon</span></span><br><span class=\"line\">eps = <span class=\"number\">1e-8</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">batch_size = <span class=\"number\">3</span></span><br><span class=\"line\">feature_num = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">0</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, feature_num)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<p>34batch\nsize=34</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm\">batchnorm</h2>\n<p>pytorchBatchNorm1d</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=feature_num, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># batchnorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">1</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(feature_num))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(feature_num))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>batchnorm/layernorm+BatchNorm\"affine\"\"affine\"False</p>\n<p>pytorchnorm1.00</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([<span class=\"number\">0.6614</span>, <span class=\"number\">0.2669</span>, <span class=\"number\">0.0617</span>, <span class=\"number\">0.6213</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.4519</span>, -<span class=\"number\">0.1661</span>, -<span class=\"number\">1.5228</span>,  <span class=\"number\">0.3817</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>weightbias4</p>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">0.4756</span>,  <span class=\"number\">0.0513</span>, -<span class=\"number\">1.6033</span>,  <span class=\"number\">0.4715</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0197</span>, -<span class=\"number\">0.5421</span>, -<span class=\"number\">1.4535</span>,  <span class=\"number\">1.0937</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8117</span>, -<span class=\"number\">0.0077</span>, -<span class=\"number\">1.5115</span>, -<span class=\"number\">0.4202</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># bn</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bn:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(isclose)</span><br></pre></td></tr></table></figure>\n<p>dim=0batchsamplefeature</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[-<span class=\"number\">0.0876</span>, -<span class=\"number\">0.6985</span>, -<span class=\"number\">0.7907</span>,  <span class=\"number\">0.5295</span>]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"number\">1.1612</span>, <span class=\"number\">0.4971</span>, <span class=\"number\">1.0630</span>, <span class=\"number\">0.2692</span>]]) </span><br></pre></td></tr></table></figure>\n<p>meanstdkeepdimTrueFalsebroadcast</p>\n<p>stdunbiasedFalsetorchbatchnorm</p>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bn:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">0.4756</span>,  <span class=\"number\">0.0514</span>, -<span class=\"number\">1.6033</span>,  <span class=\"number\">0.4715</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0197</span>, -<span class=\"number\">0.5421</span>, -<span class=\"number\">1.4535</span>,  <span class=\"number\">1.0937</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8117</span>, -<span class=\"number\">0.0077</span>, -<span class=\"number\">1.5115</span>, -<span class=\"number\">0.4202</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>torch.isclosebatchnormbatchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<p>equal1e-5~1e-4eps</p>\n<h2 id=\"layernorm\">layernorm</h2>\n<p>layernorm34</p>\n<p>torch</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">2</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(feature_num))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(feature_num))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">0.3923</span>, -<span class=\"number\">0.2236</span>, -<span class=\"number\">0.3195</span>, -<span class=\"number\">1.2050</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">1.0445</span>, -<span class=\"number\">0.6332</span>,  <span class=\"number\">0.5731</span>,  <span class=\"number\">0.5409</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p></p>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5120</span>, -<span class=\"number\">0.6001</span>,  <span class=\"number\">1.0604</span>, -<span class=\"number\">0.0392</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.7249</span>, -<span class=\"number\">0.3772</span>,  <span class=\"number\">0.3331</span>, -<span class=\"number\">0.9155</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.6645</span>, -<span class=\"number\">0.6209</span>,  <span class=\"number\">0.7693</span>, -<span class=\"number\">1.4324</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p></p>\n<p>dim=1</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ln</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;ln:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[-<span class=\"number\">0.0907</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.3104</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.3843</span>]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"number\">1.3691</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.9502</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.3458</span>]]) </span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5120</span>, -<span class=\"number\">0.6001</span>,  <span class=\"number\">1.0604</span>, -<span class=\"number\">0.0392</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.7249</span>, -<span class=\"number\">0.3772</span>,  <span class=\"number\">0.3331</span>, -<span class=\"number\">0.9155</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.6645</span>, -<span class=\"number\">0.6209</span>,  <span class=\"number\">0.7693</span>, -<span class=\"number\">1.4325</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"\"></h2>\n<p>batchnormlayernorm</p>\n<img src=\"/b70b4a2d/bn_and_ln.png\" class title=\"bnln\">\n<p>batchnormdim=0batchlayernormdim=1</p>\n<p>batchnormlayernorm</p>\n<h1 id=\"cv\">CV</h1>\n<p>CV</p>\n<p>CV[N,C,H,W]Nbatch\nsizeCchannelHWfeature\nmapCV</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># [N,C,H,W]</span></span><br><span class=\"line\">batch_size = <span class=\"number\">2</span></span><br><span class=\"line\">channel = <span class=\"number\">2</span></span><br><span class=\"line\">height = <span class=\"number\">2</span></span><br><span class=\"line\">width = <span class=\"number\">3</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">3</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, channel, height, width)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[[-<span class=\"number\">0.0766</span>,  <span class=\"number\">0.3599</span>, -<span class=\"number\">0.7820</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.0715</span>,  <span class=\"number\">0.6648</span>, -<span class=\"number\">0.2868</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[ <span class=\"number\">1.6206</span>, -<span class=\"number\">1.5967</span>,  <span class=\"number\">0.4046</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.6113</span>,  <span class=\"number\">0.7604</span>, -<span class=\"number\">0.0336</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[-<span class=\"number\">0.3448</span>,  <span class=\"number\">0.4937</span>, -<span class=\"number\">0.0776</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.8054</span>,  <span class=\"number\">0.4851</span>,  <span class=\"number\">0.2052</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[ <span class=\"number\">0.3384</span>,  <span class=\"number\">1.3528</span>,  <span class=\"number\">0.3736</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.0134</span>,  <span class=\"number\">0.7737</span>, -<span class=\"number\">0.1092</span>]]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm-1\">batchnorm</h2>\n<p>BatchNorm2dchannel</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm2d(num_features=channel, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># batchnorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">4</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(channel))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(channel))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>channel</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">1.6053</span>,  <span class=\"number\">0.2325</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([<span class=\"number\">2.2399</span>, <span class=\"number\">0.8473</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>torchbatchnorm2d</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[[[<span class=\"number\">2.2043</span>, <span class=\"number\">1.1275</span>, <span class=\"number\">3.9442</span>],</span><br><span class=\"line\">          [<span class=\"number\">1.8388</span>, <span class=\"number\">0.3753</span>, <span class=\"number\">2.7226</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">1.2185</span>, <span class=\"number\">0.2591</span>, <span class=\"number\">0.8559</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.9175</span>, <span class=\"number\">0.9620</span>, <span class=\"number\">0.7252</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"number\">2.8658</span>, <span class=\"number\">0.7975</span>, <span class=\"number\">2.2066</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.4684</span>, <span class=\"number\">0.8186</span>, <span class=\"number\">1.5090</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">0.8362</span>, <span class=\"number\">1.1387</span>, <span class=\"number\">0.8467</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.7392</span>, <span class=\"number\">0.9660</span>, <span class=\"number\">0.7027</span>]]]], grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>batchnorm2d</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># bn</span></span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = []</span><br><span class=\"line\"><span class=\"comment\"># channel</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(channel):</span><br><span class=\"line\">    <span class=\"comment\"># </span></span><br><span class=\"line\">    mean = torch.mean(inputs[:, c, :, :])</span><br><span class=\"line\">    std = torch.std(inputs[:, c, :, :], unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    normed = (inputs[:, c, :, :] - mean) / (std + eps) * torch_bn.weight[c] + torch_bn.bias[c]</span><br><span class=\"line\">    normed = normed.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">    manual_normed.append(normed)</span><br><span class=\"line\">manual_normed = torch.cat(manual_normed, <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bn:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>CVchannel\"batch\"</p>\n<p>NHW</p>\n<img src=\"/b70b4a2d/cv_batchnorm.png\" class title=\"CVbatchnorm\">\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bn:</span><br><span class=\"line\"> tensor([[[[<span class=\"number\">2.2043</span>, <span class=\"number\">1.1275</span>, <span class=\"number\">3.9442</span>],</span><br><span class=\"line\">          [<span class=\"number\">1.8388</span>, <span class=\"number\">0.3752</span>, <span class=\"number\">2.7226</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">1.2185</span>, <span class=\"number\">0.2591</span>, <span class=\"number\">0.8559</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.9175</span>, <span class=\"number\">0.9620</span>, <span class=\"number\">0.7252</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"number\">2.8658</span>, <span class=\"number\">0.7975</span>, <span class=\"number\">2.2066</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.4685</span>, <span class=\"number\">0.8186</span>, <span class=\"number\">1.5089</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">0.8362</span>, <span class=\"number\">1.1387</span>, <span class=\"number\">0.8467</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.7392</span>, <span class=\"number\">0.9660</span>, <span class=\"number\">0.7027</span>]]]], grad_fn=&lt;CatBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"layernorm-1\">layernorm</h2>\n<p><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\">torchlayernorm</a>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(</span><br><span class=\"line\">    normalized_shape=[channel, height, width], </span><br><span class=\"line\">    elementwise_affine=<span class=\"literal\">True</span></span><br><span class=\"line\">)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">5</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(channel, height, width))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(channel, height, width))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p></p>\n<img src=\"/b70b4a2d/cv_layernorm.jpeg\" class title=\"CVlayernorm\">\n<p>[channel, height, width]</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([[[-<span class=\"number\">0.4868</span>, -<span class=\"number\">0.6038</span>, -<span class=\"number\">0.5581</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.6675</span>, -<span class=\"number\">0.1974</span>,  <span class=\"number\">1.9428</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">1.4017</span>, -<span class=\"number\">0.7626</span>,  <span class=\"number\">0.6312</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.8991</span>, -<span class=\"number\">0.5578</span>,  <span class=\"number\">0.6907</span>]]], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([[[ <span class=\"number\">0.2225</span>, -<span class=\"number\">0.6662</span>,  <span class=\"number\">0.6846</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.5740</span>, -<span class=\"number\">0.5829</span>,  <span class=\"number\">0.7679</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">0.0571</span>, -<span class=\"number\">1.1894</span>, -<span class=\"number\">0.5659</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.8327</span>,  <span class=\"number\">0.9014</span>,  <span class=\"number\">0.2116</span>]]], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>channel</p>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[[[ <span class=\"number\">0.3594</span>, -<span class=\"number\">0.8338</span>,  <span class=\"number\">1.3456</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.5128</span>, -<span class=\"number\">0.7147</span>, -<span class=\"number\">0.3012</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">2.5939</span>,  <span class=\"number\">0.5089</span>, -<span class=\"number\">0.3546</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.3715</span>,  <span class=\"number\">0.4607</span>,  <span class=\"number\">0.0553</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[ <span class=\"number\">0.5477</span>, -<span class=\"number\">0.9583</span>,  <span class=\"number\">0.8526</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.2112</span>, -<span class=\"number\">0.6760</span>,  <span class=\"number\">0.9378</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">0.3219</span>, -<span class=\"number\">2.4580</span>, -<span class=\"number\">0.3647</span>],</span><br><span class=\"line\">          [-<span class=\"number\">0.6744</span>,  <span class=\"number\">0.4171</span>, -<span class=\"number\">0.0264</span>]]]], grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ln</span></span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = []</span><br><span class=\"line\"><span class=\"comment\"># channel</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> b <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(batch_size):</span><br><span class=\"line\">    <span class=\"comment\"># </span></span><br><span class=\"line\">    mean = torch.mean(inputs[b, :, :, :])</span><br><span class=\"line\">    std = torch.std(inputs[b, :, :, :], unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    normed = (inputs[b, :, :, :] - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\">    normed = normed.unsqueeze(<span class=\"number\">0</span>)</span><br><span class=\"line\">    manual_normed.append(normed)</span><br><span class=\"line\">manual_normed = torch.cat(manual_normed, <span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;ln:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>channel</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln:</span><br><span class=\"line\"> tensor([[[[ <span class=\"number\">0.3594</span>, -<span class=\"number\">0.8338</span>,  <span class=\"number\">1.3456</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.5128</span>, -<span class=\"number\">0.7147</span>, -<span class=\"number\">0.3012</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">2.5939</span>,  <span class=\"number\">0.5090</span>, -<span class=\"number\">0.3546</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.3715</span>,  <span class=\"number\">0.4607</span>,  <span class=\"number\">0.0553</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[ <span class=\"number\">0.5477</span>, -<span class=\"number\">0.9583</span>,  <span class=\"number\">0.8527</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.2112</span>, -<span class=\"number\">0.6760</span>,  <span class=\"number\">0.9378</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">0.3219</span>, -<span class=\"number\">2.4581</span>, -<span class=\"number\">0.3647</span>],</span><br><span class=\"line\">          [-<span class=\"number\">0.6744</span>,  <span class=\"number\">0.4171</span>, -<span class=\"number\">0.0264</span>]]]], grad_fn=&lt;CatBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]]])</span><br></pre></td></tr></table></figure>\n<h1 id=\"nlp\">NLP</h1>\n<p>NLP</p>\n<p>Nbatch sizeSsequence lengthHhidden size</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># [N,S,H]</span></span><br><span class=\"line\">batch_size = <span class=\"number\">2</span></span><br><span class=\"line\">seq_len = <span class=\"number\">3</span></span><br><span class=\"line\">hidden_size = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">6</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, seq_len, hidden_size)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm-2\">batchnorm</h2>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=hidden_size, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># batchnorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">7</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(hidden_size))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(hidden_size))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># # </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>transposetranspose</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.1468</span>,  <span class=\"number\">0.7861</span>,  <span class=\"number\">0.9468</span>, -<span class=\"number\">1.1143</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">1.6908</span>, -<span class=\"number\">0.8948</span>, -<span class=\"number\">0.3556</span>,  <span class=\"number\">1.2324</span>], requires_grad=<span class=\"literal\">True</span>) </span><br><span class=\"line\"></span><br><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[[ <span class=\"number\">1.8740</span>, -<span class=\"number\">0.7037</span>, -<span class=\"number\">1.8222</span>,  <span class=\"number\">2.3385</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.7413</span>, -<span class=\"number\">1.8119</span>,  <span class=\"number\">0.3641</span>,  <span class=\"number\">0.0200</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.4615</span>, -<span class=\"number\">0.2676</span>,  <span class=\"number\">0.1081</span>,  <span class=\"number\">1.3450</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">1.7084</span>, -<span class=\"number\">1.9653</span>,  <span class=\"number\">1.0169</span>,  <span class=\"number\">0.5785</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.8213</span>, -<span class=\"number\">0.8614</span>, -<span class=\"number\">0.8056</span>,  <span class=\"number\">2.9892</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.5383</span>,  <span class=\"number\">0.2409</span>, -<span class=\"number\">0.9949</span>,  <span class=\"number\">0.1231</span>]]], grad_fn=&lt;TransposeBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>batchnorm</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># bn</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=(<span class=\"number\">0</span>, <span class=\"number\">1</span>) , keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=(<span class=\"number\">0</span>, <span class=\"number\">1</span>), keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bn:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>dim=(0,1)[N,\nS, H][NS,\nH]batchnorm</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.2151</span>,  <span class=\"number\">0.5444</span>, -<span class=\"number\">0.2633</span>, -<span class=\"number\">0.5424</span>]]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"number\">0.7984</span>, <span class=\"number\">0.3537</span>, <span class=\"number\">0.7799</span>, <span class=\"number\">0.7986</span>]]]) </span><br><span class=\"line\"></span><br><span class=\"line\">bn:</span><br><span class=\"line\"> tensor([[[ <span class=\"number\">1.8740</span>, -<span class=\"number\">0.7037</span>, -<span class=\"number\">1.8222</span>,  <span class=\"number\">2.3385</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.7413</span>, -<span class=\"number\">1.8119</span>,  <span class=\"number\">0.3641</span>,  <span class=\"number\">0.0200</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.4615</span>, -<span class=\"number\">0.2676</span>,  <span class=\"number\">0.1081</span>,  <span class=\"number\">1.3450</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">1.7084</span>, -<span class=\"number\">1.9653</span>,  <span class=\"number\">1.0169</span>,  <span class=\"number\">0.5785</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.8213</span>, -<span class=\"number\">0.8614</span>, -<span class=\"number\">0.8056</span>,  <span class=\"number\">2.9892</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.5383</span>,  <span class=\"number\">0.2409</span>, -<span class=\"number\">0.9950</span>,  <span class=\"number\">0.1231</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"layernorm-2\">layernorm</h2>\n<p>NLPlayernormhuggingfacebertlayernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BertSelfOutput</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, config</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class=\"line\">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class=\"line\">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, hidden_states: torch.normTensor, input_tensor: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class=\"line\">        hidden_states = self.dense(hidden_states)</span><br><span class=\"line\">        hidden_states = self.dropout(hidden_states)</span><br><span class=\"line\">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> hidden_states</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=hidden_size, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">8</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(hidden_size))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(hidden_size))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>hidden size</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">0.2713</span>, -<span class=\"number\">1.2729</span>,  <span class=\"number\">0.5027</span>,  <span class=\"number\">0.4181</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.6394</span>, -<span class=\"number\">0.6608</span>, -<span class=\"number\">0.1433</span>, -<span class=\"number\">0.1043</span>], requires_grad=<span class=\"literal\">True</span>) </span><br><span class=\"line\"></span><br><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.7547</span>, -<span class=\"number\">2.8528</span>, -<span class=\"number\">0.5092</span>, -<span class=\"number\">0.3423</span>],</span><br><span class=\"line\">         [-<span class=\"number\">1.0957</span>, -<span class=\"number\">0.8780</span>,  <span class=\"number\">0.2388</span>,  <span class=\"number\">0.2097</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.3502</span>, -<span class=\"number\">1.6158</span>, -<span class=\"number\">0.3133</span>, -<span class=\"number\">0.7224</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">0.9134</span>, -<span class=\"number\">0.4490</span>,  <span class=\"number\">0.6868</span>, -<span class=\"number\">0.3029</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7116</span>, -<span class=\"number\">2.5589</span>, -<span class=\"number\">0.1039</span>, -<span class=\"number\">0.6493</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.5076</span>, -<span class=\"number\">2.1031</span>, -<span class=\"number\">0.9346</span>, -<span class=\"number\">0.1230</span>]]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ln</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">2</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">2</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;ln:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.8469</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.0745</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.3386</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">0.1364</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7003</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.2831</span>]]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"number\">0.8578</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.3354</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.6505</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"number\">0.4426</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.8448</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.6816</span>]]]) </span><br></pre></td></tr></table></figure>\n<p>sampletoken</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.7547</span>, -<span class=\"number\">2.8528</span>, -<span class=\"number\">0.5092</span>, -<span class=\"number\">0.3423</span>],</span><br><span class=\"line\">         [-<span class=\"number\">1.0957</span>, -<span class=\"number\">0.8780</span>,  <span class=\"number\">0.2388</span>,  <span class=\"number\">0.2097</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.3502</span>, -<span class=\"number\">1.6158</span>, -<span class=\"number\">0.3133</span>, -<span class=\"number\">0.7224</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">0.9134</span>, -<span class=\"number\">0.4490</span>,  <span class=\"number\">0.6868</span>, -<span class=\"number\">0.3029</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7116</span>, -<span class=\"number\">2.5590</span>, -<span class=\"number\">0.1039</span>, -<span class=\"number\">0.6493</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.5076</span>, -<span class=\"number\">2.1031</span>, -<span class=\"number\">0.9347</span>, -<span class=\"number\">0.1230</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]])</span><br></pre></td></tr></table></figure>\n<h1 id=\"\"></h1>\n<p></p>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">batch_size = <span class=\"number\">3</span></span><br><span class=\"line\">feature_num = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">0</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, feature_num)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, mean)</span></span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, std, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=feature_num, affine=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(std)</span><br><span class=\"line\">torch_bn.bias =  nn.Parameter(mean)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;weight:\\n&#x27;, torch_bn.weight)</span></span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;bias:\\n&#x27;, torch_bn.bias, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br><span class=\"line\"></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, inputs, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1821</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<p>batchnorm</p>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, mean)</span></span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, std, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(std)</span><br><span class=\"line\">torch_bn.bias =  nn.Parameter(mean)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;weight:\\n&#x27;, torch_bn.weight)</span></span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;bias:\\n&#x27;, torch_bn.bias, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br><span class=\"line\"></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, inputs, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.1918</span>, -<span class=\"number\">0.1481</span>, -<span class=\"number\">1.5251</span>,  <span class=\"number\">0.4814</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8146</span>, -<span class=\"number\">1.1451</span>,  <span class=\"number\">0.7512</span>,  <span class=\"number\">1.2086</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.9685</span>, -<span class=\"number\">0.0551</span>, -<span class=\"number\">0.6140</span>,  <span class=\"number\">1.6376</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>],</span><br><span class=\"line\">        [<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>],</span><br><span class=\"line\">        [<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>]])</span><br></pre></td></tr></table></figure>\n<p>layernormlayernorm</p>\n<p>CVNLP</p>\n<p>batchnormlayernorm</p>\n<h1 id=\"\"></h1>\n<p>batchnormlayernorm</p>\n<p>batchnormlayernorm</p>\n<p>batchnorm\"\"\"\"batchbatchbatchfeature\nmap</p>\n<p>layernorm</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1LAYERNORM\nhttps://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html<br>\n2BATCHNORM1D\nhttps://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html<br>\n3BATCHNORM2D\nhttps://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html</p>\n"},{"title":"(3)","abbrlink":"1736008","date":"2024-04-05T06:08:31.000Z","_content":"\n![](/images/cover.png)  \n\n\n\n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~~\n\n***  \n\n# 1.RoPE  \n\nRoPERoPE  \n\nRoPE2k2kAlibiNTKYaRN  \n\n# 2.batchnormmomentum  \n\nbatchnormbatchnormalization  \n\nmoving_mean = momentum  moving_mean + (1.0  momentum)  mean  \n\nmoving_var = momentum  moving_var + (1.0  momentum)  var  \n\nmomentumbatch sizemini batchmomentum  \n\n# 3.  \n\n  \n\n  \n\n  \n\n  \n\n# 4.kv cache  \n\nGPT  \n\ntoken  \n\ntokenkv  \n\nkvL2kv  \n\n# 5.ReLU  \n\n1max(0, x)21/  \n\n1020  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  ","source":"_posts/cs/nlp/2024/04/-3.md","raw":"---\ntitle: (3)\ntags:\n  - NLP\n  - LLM\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: '1736008'\ndate: 2024-04-05 14:08:31\n---\n\n![](/images/cover.png)  \n\n\n\n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~~\n\n***  \n\n# 1.RoPE  \n\nRoPERoPE  \n\nRoPE2k2kAlibiNTKYaRN  \n\n# 2.batchnormmomentum  \n\nbatchnormbatchnormalization  \n\nmoving_mean = momentum  moving_mean + (1.0  momentum)  mean  \n\nmoving_var = momentum  moving_var + (1.0  momentum)  var  \n\nmomentumbatch sizemini batchmomentum  \n\n# 3.  \n\n  \n\n  \n\n  \n\n  \n\n# 4.kv cache  \n\nGPT  \n\ntoken  \n\ntokenkv  \n\nkvL2kv  \n\n# 5.ReLU  \n\n1max(0, x)21/  \n\n1020  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  ","slug":"cs/nlp/2024/04/-3","published":1,"updated":"2024-05-10T06:51:01.150Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmfi000oam4kdgkle53r","content":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM<sub></sub></p>\n<hr>\n<h1 id=\"rope\">1.RoPE</h1>\n<p>RoPERoPE</p>\n<p>RoPE2k2kAlibiNTKYaRN</p>\n<h1 id=\"batchnormmomentum\">2.batchnormmomentum</h1>\n<p>batchnormbatchnormalization</p>\n<p>moving_mean = momentum  moving_mean + (1.0  momentum)  mean</p>\n<p>moving_var = momentum  moving_var + (1.0  momentum)  var</p>\n<p>momentumbatch\nsizemini batchmomentum</p>\n<h1 id=\"\">3.</h1>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<h1 id=\"kv-cache\">4.kv cache</h1>\n<p>GPT</p>\n<p>token</p>\n<p>tokenkv</p>\n<p>kvL2kv</p>\n<h1 id=\"relu\">5.ReLU</h1>\n<p>1max(0,\nx)21/</p>\n<p>1020</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n","length":1653,"excerpt":"","more":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM<sub></sub></p>\n<hr>\n<h1 id=\"rope\">1.RoPE</h1>\n<p>RoPERoPE</p>\n<p>RoPE2k2kAlibiNTKYaRN</p>\n<h1 id=\"batchnormmomentum\">2.batchnormmomentum</h1>\n<p>batchnormbatchnormalization</p>\n<p>moving_mean = momentum  moving_mean + (1.0  momentum)  mean</p>\n<p>moving_var = momentum  moving_var + (1.0  momentum)  var</p>\n<p>momentumbatch\nsizemini batchmomentum</p>\n<h1 id=\"\">3.</h1>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<h1 id=\"kv-cache\">4.kv cache</h1>\n<p>GPT</p>\n<p>token</p>\n<p>tokenkv</p>\n<p>kvL2kv</p>\n<h1 id=\"relu\">5.ReLU</h1>\n<p>1max(0,\nx)21/</p>\n<p>1020</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n"},{"title":"(4)","abbrlink":"1736008","date":"2024-04-20T08:56:45.000Z","_content":"\n![](/images/cover.png)  \n\n\n\n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~~\n\n***  \n\n# 1.Transformerlayernormbatchnorm  \n\nNLPpaddingpaddingnormalizationbatchnormbatchtokentokenPowerNorm: Rethinking Batch Normalization in TransformersNLPbatchnormlayernorm  \n\n# 2.transformerencdoerdecoder  \n\ndecoderself-attentionencoderattentionKVdecoderQcross-attention  \n\n{% asset_img transformer.png transformer %}  \n\n# 3.PyTorchTensorview()reshape()  \n\n1.view()reshape()tensorview()reshape  \n\n2.view()tensor  \n\n3.reshape()tensortensorview()  \n\n4.is_contiguous()contiguous()  \n\n# 4.RLHFPPO  \n\nPPO4  \n\n1.ActorSFTactionCriticloss  \n\n2.ReferenceSFTRLHFReferenceActorActorReferenceKL penaltyActor  \n\n3.RewardSFTRLHF  \n\n4.CriticRewardActortoken  \n\n# 5.GPTLVhidden sizeHbatch sizeBSAdamN  \n\noptimizer  \n\n1.VH12H^2+13H=VH+L(12H^2+13H)2  \n\n2.2  \n\n3.optimizer(+)*2+(++2)*4=2016  \n\n4.softmaxdropout34BSH+5BNS^22  \n\n\nGPT3175BH=12288L=96N=96350GB=1S=102490GS=81923420G  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  ","source":"_posts/cs/nlp/2024/04/-4.md","raw":"---\ntitle: (4)\nabbrlink: '1736008'\ndate: 2024-04-20 16:56:45\ntags:\n  - NLP\n  - LLM\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n![](/images/cover.png)  \n\n\n\n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~~\n\n***  \n\n# 1.Transformerlayernormbatchnorm  \n\nNLPpaddingpaddingnormalizationbatchnormbatchtokentokenPowerNorm: Rethinking Batch Normalization in TransformersNLPbatchnormlayernorm  \n\n# 2.transformerencdoerdecoder  \n\ndecoderself-attentionencoderattentionKVdecoderQcross-attention  \n\n{% asset_img transformer.png transformer %}  \n\n# 3.PyTorchTensorview()reshape()  \n\n1.view()reshape()tensorview()reshape  \n\n2.view()tensor  \n\n3.reshape()tensortensorview()  \n\n4.is_contiguous()contiguous()  \n\n# 4.RLHFPPO  \n\nPPO4  \n\n1.ActorSFTactionCriticloss  \n\n2.ReferenceSFTRLHFReferenceActorActorReferenceKL penaltyActor  \n\n3.RewardSFTRLHF  \n\n4.CriticRewardActortoken  \n\n# 5.GPTLVhidden sizeHbatch sizeBSAdamN  \n\noptimizer  \n\n1.VH12H^2+13H=VH+L(12H^2+13H)2  \n\n2.2  \n\n3.optimizer(+)*2+(++2)*4=2016  \n\n4.softmaxdropout34BSH+5BNS^22  \n\n\nGPT3175BH=12288L=96N=96350GB=1S=102490GS=81923420G  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  ","slug":"cs/nlp/2024/04/-4","published":1,"updated":"2024-05-10T06:50:46.826Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmfi000pam4kglhffgkz","content":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM<sub></sub></p>\n<hr>\n<h1 id=\"transformerlayernormbatchnorm\">1.Transformerlayernormbatchnorm</h1>\n<p>NLPpaddingpaddingnormalizationbatchnormbatchtokentokenPowerNorm:\nRethinking Batch Normalization in\nTransformersNLPbatchnormlayernorm</p>\n<h1 id=\"transformerencdoerdecoder\">2.transformerencdoerdecoder</h1>\n<p>decoderself-attentionencoderattentionKVdecoderQcross-attention</p>\n<img src=\"/1736008/transformer.png\" class title=\"transformer\">\n<h1 id=\"pytorchtensorviewreshape\">3.PyTorchTensorview()reshape()</h1>\n<p>1.view()reshape()tensorview()reshape</p>\n<p>2.view()tensor</p>\n<p>3.reshape()tensortensorview()</p>\n<p>4.is_contiguous()contiguous()</p>\n<h1 id=\"rlhfppo\">4.RLHFPPO</h1>\n<p>PPO4</p>\n<p>1.ActorSFTactionCriticloss</p>\n<p>2.ReferenceSFTRLHFReferenceActorActorReferenceKL\npenaltyActor</p>\n<p>3.RewardSFTRLHF</p>\n<p>4.CriticRewardActortoken</p>\n<h1 id=\"gptlvhidden-sizehbatch-sizebsadamn\">5.GPTLVhidden\nsizeHbatch\nsizeBSAdamN</h1>\n<p>optimizer</p>\n<p>1.VH12H<sup>2+13H=VH+L(12H</sup>2+13H)2</p>\n<p>2.2</p>\n<p>3.optimizer(+)<em>2+(++2)</em>4=2016</p>\n<p>4.softmaxdropout34BSH+5BNS^22</p>\n<p>\nGPT3175BH=12288L=96N=96350GB=1S=102490GS=81923420G</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a></p>\n","length":2305,"excerpt":"","more":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM<sub></sub></p>\n<hr>\n<h1 id=\"transformerlayernormbatchnorm\">1.Transformerlayernormbatchnorm</h1>\n<p>NLPpaddingpaddingnormalizationbatchnormbatchtokentokenPowerNorm:\nRethinking Batch Normalization in\nTransformersNLPbatchnormlayernorm</p>\n<h1 id=\"transformerencdoerdecoder\">2.transformerencdoerdecoder</h1>\n<p>decoderself-attentionencoderattentionKVdecoderQcross-attention</p>\n<img src=\"/1736008/transformer.png\" class title=\"transformer\">\n<h1 id=\"pytorchtensorviewreshape\">3.PyTorchTensorview()reshape()</h1>\n<p>1.view()reshape()tensorview()reshape</p>\n<p>2.view()tensor</p>\n<p>3.reshape()tensortensorview()</p>\n<p>4.is_contiguous()contiguous()</p>\n<h1 id=\"rlhfppo\">4.RLHFPPO</h1>\n<p>PPO4</p>\n<p>1.ActorSFTactionCriticloss</p>\n<p>2.ReferenceSFTRLHFReferenceActorActorReferenceKL\npenaltyActor</p>\n<p>3.RewardSFTRLHF</p>\n<p>4.CriticRewardActortoken</p>\n<h1 id=\"gptlvhidden-sizehbatch-sizebsadamn\">5.GPTLVhidden\nsizeHbatch\nsizeBSAdamN</h1>\n<p>optimizer</p>\n<p>1.VH12H<sup>2+13H=VH+L(12H</sup>2+13H)2</p>\n<p>2.2</p>\n<p>3.optimizer(+)<em>2+(++2)</em>4=2016</p>\n<p>4.softmaxdropout34BSH+5BNS^22</p>\n<p>\nGPT3175BH=12288L=96N=96350GB=1S=102490GS=81923420G</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a></p>\n"},{"title":":sliding window attention","abbrlink":"c61d17e3","date":"2024-03-12T09:26:00.000Z","_content":"\n//  \n\nLLM  \n\n[LLM](http://www.linsight.cn/c4da56c0.html)32k+/128k+[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)MQAGQAKV  \n\nSWAsliding window attention  \n\nQwenMistralSWA  \n\nMistral  \n\nMistral AIAI202352023912Mistral 7BMoEMistral 8x7B  \n\n20242  \n\n{% asset_img ms_invest_mistral.png MS %}  \n\n20242Mistral Large & 32kMMLUGPT4  \n\n{% asset_img mistral_large_performance.jpeg Mistral Large MMLU Performance %}  \n\nMistralOPENAIMETA  \n\n# SWA\n\nSWAMistralMistral 7BSWA  \n\n## Mistral 7B\n\n202310MistralMistral 7B[](https://arxiv.org/pdf/2310.06825.pdf)LlamaMistralGQASWA  \n\nMistral 7B  \n\n{% asset_img mistral_architechture.png Mistral Architechture %}  \n\nMistralkv=8GQAintermediate sizeLlama211008  \n\n## \n\ncausal attentiontokentoken  \n\n $s$ 1 $s^2$ KV Cache  \n\n/ $s$ \n\n1\n\n $[m,n]\\times[n,p]$  $[m,p]$ $m\\times p$  $n$  $n$  $2mpn$ floating point operationsFLOPs  \n\n[Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)  \n\nMHA $s$  $L$ hidden size $d_{model}$  $d_{q}$   $n_{q}$ $d_{model} = n_{q}\\times d_{q}$ operationFLOPs  \n\n<center>\n\n| Operation | FLOPsMHA |\n| :---- | :----: |\n| Attention: QKV | $6\\times s\\times h_{model}^{2}$  |\n| Attention: QK logits ( $QK^T$ ) | $n_{q}\\times 2\\times s^2\\times h_{q}$ |\n| Attention: Softmax | $n_{q}\\times 3\\times s^2$ |\n| Attention: Reduction (apply to $V$) | $n_{q}\\times 2\\times s^2\\times h_{q}$ |\n| Attention: Outupt Linear Project | $2\\times s\\times h_{model}^{2}$ |\n\n</center>\n\nSoftmax $[1,s]$ softmax $3s$  $s$ exp $s$  $s$  $[s,s]$ softmax  $3s^2$  $n_{q}$ \n\noperationscalingdropout\n\nMistral 7BGQA  \n\nKVkv $n_{kv}$\n\n<center>\n\n| Operation | FLOPsGQA |\n| :---- | :----: |\n| Attention: QKV | $2\\times s\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times n_{kv})$  |\n\n</center>\n\nQK logitsSoftmaxReduction $s$   \n\n2\n\nKV Cache  \n\n$$\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\n\n\n$$\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\nMistral 7B16kKV_Cache2G  \n\nGQA16k+\n\n## SWA\n\nattention $s$  $s$ \n\nCNN  \n\n{% asset_img receptive_field_cnn.png CNN Receptive Field %}  \n\n3 $3\\times 3$ CNNsliding window  \n\nlayer 3layer 2 $3\\times 3$ layer 2  \n\nlayer 2layer 1 $3\\times 3$ layer 2 $3\\times 3$ layer 1 $5\\times 5$ layer 3<u>****</u> $5\\times 5$   \n\nlayer 4layer 4layer 1  $7\\times 7$   \n\n  \n\n  \n\nCNN  \n\n  \n\n  \n\n\n\nMistralSWA  \n\n{% asset_img mistral_swa.png Mistral SWA %}  \n\ncausal attentionattention mask  \n\nSWAattention mask33  \n\nCNNLLM  \n\nMistral 7B409632 $4096\\times 32=131,072$ 131k  \n\nattentionQK logitsSoftmaxReduction $s$ SWAoperation4k131k131k $32\\times 32=1024$   \n\n $s$ 131k $31/32$   \n\nSWA4kcausal attention4k\n\n>In practice, for a sequence length of 16K and W = 4096, changes made to FlashAttention [11] and xFormers [18] yield a 2x speed improvement over a vanilla attention baseline.\n\nMistralSWAFlashAttentionxFormers16k2  \n\n## KV Cache\n\nsliding windowKV Cache  \n\nSWAkv  \n\n $W=4$ 5token1tokenkv  \n\n{% asset_img rolling_buffer.png swa rolling buffer %}  \n\nthroughputcase  \n\n## Prompt\n\nRAGfunciton callprompt  \n\nGPT4system promptOPENAI  \n\n>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\" and the user's locale is \"en-US\"\nYour knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07.\nImage input capabilities: Enabled\n>\n>Tools\n>\n>python\n>\n>When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 60.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is disabled. Do not make external web requests or API calls as they will fail.\n>\n>dalle\n>\n>Whenever a description of an image is given, create a prompt that dalle can use to generate the image and abide to the following policy:\n>1. The prompt must be in English. Translate to English if needed.\n>2. DO NOT ask for permission to generate the image, just do it!\n>3. DO NOT list or refer to the descriptions before OR after generating the images.\n>4. Do not create more than 1 image, even if the user requests more.\n>5. Do not create images in the style of artists, creative professionals or studios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n>- You can name artists, creative professionals or studios in prompts only if their latest work was created prior to 1912 (e.g. Van Gogh, Goya)\n>- If asked to generate an image that would violate this policy, instead apply the following procedure: (a) substitute the artist's name with three adjectives that capture key aspects of the style; (b) include an associated artistic movement or era to provide context; and (c) mention the primary medium used by the artist\n>6. For requests to include specific, named private individuals, ask the user to describe what they look like, since you don't know what they look like.\n>7. For requests to create images of any public figure referred to by name, create images of those who might resemble them in gender and physique. But they shouldn't look like them. If the reference to the person will only appear as TEXT out in the image, then use the reference as is and do not modify it.\n>8. Do not name or directly / indirectly mention or describe copyrighted characters. Rewrite prompts to describe in detail a specific different character with a different specific color, hair style, or other defining visual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around 100 words long.\nExample dalle invocation:\n>{\n>\"prompt\": \"<insert prompt here>\"\n>}\n>namespace dalle {\n>\n>Create images from a text-only prompt.\ntype text2im = (_: {\nThe size of the requested image. Use 1024x1024 (square) as the default, 1792x1024 if the user requests a wide image, and 1024x1792 for full-body portraits. Always include this parameter in the request.\nn?: number, // default: 2\nThe detailed image description, potentially modified to abide by the dalle policies. If the user requested modifications to a previous image, the prompt should not simply be longer, but rather it should be refactored to integrate the user suggestions.\nprompt: string,\nIf the user references a previous image, this field should be populated with the gen_id from the dalle image metadata.\nreferenced_image_ids?: string[],\n}) => any;\n} // namespace dalle\n>\n>voice_mode\n>Voice mode functions are not available in text conversations.\n>namespace voice_mode {   } // namespace voice_mode\n>\n>browser\n>\n>You have the tool `browser`. Use `browser` in the following circumstances:\n>    - User is asking about current events or something that requires real-time information (weather, sports scores, etc.)\n>    - User is asking about some term you are totally unfamiliar with (it might be new)\n>    - User explicitly asks you to browse or provide links to references\n>\n>Given a query that requires retrieval, your turn will consist of three steps:\n>1. Call the search function to get a list of results.\n>2. Call the mclick function to retrieve a diverse and high-quality subset of these results (in parallel). Remember to SELECT AT LEAST 3 sources when using `mclick`.\n>3. Write a response to the user based on these results. In your response, cite sources using the citation format below.\n>\n>In some cases, you should repeat step 1 twice, if the initial results are unsatisfactory, and you believe that you can refine the query to get better results.\n>\n>You can also open a url directly if one is provided by the user. Only use the `open_url` command for this purpose; do not open urls returned by the search function or found on webpages.\n>\n>The `browser` tool has the following commands:\n\t`search(query: str, recency_days: int)` Issues a query to a search engine and displays the results.\n         `mclick(ids: list[str])`. Retrieves the contents of the webpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST 3 and at most 10 pages. Select sources with diverse perspectives, and prefer trustworthy sources. Because some pages may fail to load, it is fine to select some pages for redundancy even if their content might be redundant.\n\t`open_url(url: str)` Opens the given URL and displays it.\n>\n>For citing quotes from the 'browser' tool: please render in this format: {message idx}{link text}.\nFor long citations: please render in this format: [link text](message idx).\nOtherwise do not render links.\n\nsystem promptkvsystem promptsliding windowsystem promptkv\n\n $W=4$system prompt9system promptkv [4,4,1]   \n\nwindowattention mask0  \n\nattention mask  \n\n  \n\n  \n\nprompt  \n\n{% asset_img prefill_and_chunking.png prefill and chunking %}  \n\nFlashAttention/PagedAttention\n\nMistral 7BLlamaLlama 34B  \n\n{% asset_img mistral_perf.png mistral performance %}  \n\nMistral7B  \n\n# Sparse Attention\n\nSWAsparse attentionsparse attention  \n\nsparse attention  \n\n## Longformer\n\nMistralSWA  \n\n2020[Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150.pdf)SWAsparse attention  \n\nLongformer  \n\n{% asset_img longformer_attention.png longformer %}  \n\nbSWABert  \n\nSWAdilated sliding window    \n\n{% asset_img dilated_conv.png dilated convolution %}  \n\nattentionSWAdilated sliding window  \n\n  \n\nBert[CLS] tokentoken  \n\nGPTinstructionprompt  \n\ntokenglobal attentionsliding windowd  \n\n## Big Bird\n\n2020Longformersparse attention[Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)  \n\nsliding windowglobal attentionLongformerBig Birdrandom attention  \n\n{% asset_img big_bird_attention.png big bird attention %}  \n\n $r=2$ 2\n\n#   \n\nSWA  \n\nSWAsparse attention<big>****</big>global + local attentionflash attentionrandom attention\n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n1Mistral 7B https://arxiv.org/pdf/2310.06825.pdf  \n2Longformer: The Long-Document Transformer \nhttps://arxiv.org/pdf/2004.05150.pdf  \n3Training Compute-Optimal Large Language Models https://arxiv.org/pdf/2203.15556.pdf  \n4GPT-4 System Prompt Revealed https://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed  \n5Big Bird: Transformers for Longer Sequences https://arxiv.org/abs/2007.14062  ","source":"_posts/cs/nlp/2024/03/LLM-sliding-window-attention.md","raw":"---\ntitle: ':sliding window attention'\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - attention\n  - sliding window attention\n  - sparse attention\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: c61d17e3\ndate: 2024-03-12 17:26:00\n---\n\n//  \n\nLLM  \n\n[LLM](http://www.linsight.cn/c4da56c0.html)32k+/128k+[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)MQAGQAKV  \n\nSWAsliding window attention  \n\nQwenMistralSWA  \n\nMistral  \n\nMistral AIAI202352023912Mistral 7BMoEMistral 8x7B  \n\n20242  \n\n{% asset_img ms_invest_mistral.png MS %}  \n\n20242Mistral Large & 32kMMLUGPT4  \n\n{% asset_img mistral_large_performance.jpeg Mistral Large MMLU Performance %}  \n\nMistralOPENAIMETA  \n\n# SWA\n\nSWAMistralMistral 7BSWA  \n\n## Mistral 7B\n\n202310MistralMistral 7B[](https://arxiv.org/pdf/2310.06825.pdf)LlamaMistralGQASWA  \n\nMistral 7B  \n\n{% asset_img mistral_architechture.png Mistral Architechture %}  \n\nMistralkv=8GQAintermediate sizeLlama211008  \n\n## \n\ncausal attentiontokentoken  \n\n $s$ 1 $s^2$ KV Cache  \n\n/ $s$ \n\n1\n\n $[m,n]\\times[n,p]$  $[m,p]$ $m\\times p$  $n$  $n$  $2mpn$ floating point operationsFLOPs  \n\n[Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)  \n\nMHA $s$  $L$ hidden size $d_{model}$  $d_{q}$   $n_{q}$ $d_{model} = n_{q}\\times d_{q}$ operationFLOPs  \n\n<center>\n\n| Operation | FLOPsMHA |\n| :---- | :----: |\n| Attention: QKV | $6\\times s\\times h_{model}^{2}$  |\n| Attention: QK logits ( $QK^T$ ) | $n_{q}\\times 2\\times s^2\\times h_{q}$ |\n| Attention: Softmax | $n_{q}\\times 3\\times s^2$ |\n| Attention: Reduction (apply to $V$) | $n_{q}\\times 2\\times s^2\\times h_{q}$ |\n| Attention: Outupt Linear Project | $2\\times s\\times h_{model}^{2}$ |\n\n</center>\n\nSoftmax $[1,s]$ softmax $3s$  $s$ exp $s$  $s$  $[s,s]$ softmax  $3s^2$  $n_{q}$ \n\noperationscalingdropout\n\nMistral 7BGQA  \n\nKVkv $n_{kv}$\n\n<center>\n\n| Operation | FLOPsGQA |\n| :---- | :----: |\n| Attention: QKV | $2\\times s\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times n_{kv})$  |\n\n</center>\n\nQK logitsSoftmaxReduction $s$   \n\n2\n\nKV Cache  \n\n$$\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\n\n\n$$\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\nMistral 7B16kKV_Cache2G  \n\nGQA16k+\n\n## SWA\n\nattention $s$  $s$ \n\nCNN  \n\n{% asset_img receptive_field_cnn.png CNN Receptive Field %}  \n\n3 $3\\times 3$ CNNsliding window  \n\nlayer 3layer 2 $3\\times 3$ layer 2  \n\nlayer 2layer 1 $3\\times 3$ layer 2 $3\\times 3$ layer 1 $5\\times 5$ layer 3<u>****</u> $5\\times 5$   \n\nlayer 4layer 4layer 1  $7\\times 7$   \n\n  \n\n  \n\nCNN  \n\n  \n\n  \n\n\n\nMistralSWA  \n\n{% asset_img mistral_swa.png Mistral SWA %}  \n\ncausal attentionattention mask  \n\nSWAattention mask33  \n\nCNNLLM  \n\nMistral 7B409632 $4096\\times 32=131,072$ 131k  \n\nattentionQK logitsSoftmaxReduction $s$ SWAoperation4k131k131k $32\\times 32=1024$   \n\n $s$ 131k $31/32$   \n\nSWA4kcausal attention4k\n\n>In practice, for a sequence length of 16K and W = 4096, changes made to FlashAttention [11] and xFormers [18] yield a 2x speed improvement over a vanilla attention baseline.\n\nMistralSWAFlashAttentionxFormers16k2  \n\n## KV Cache\n\nsliding windowKV Cache  \n\nSWAkv  \n\n $W=4$ 5token1tokenkv  \n\n{% asset_img rolling_buffer.png swa rolling buffer %}  \n\nthroughputcase  \n\n## Prompt\n\nRAGfunciton callprompt  \n\nGPT4system promptOPENAI  \n\n>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\" and the user's locale is \"en-US\"\nYour knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07.\nImage input capabilities: Enabled\n>\n>Tools\n>\n>python\n>\n>When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 60.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is disabled. Do not make external web requests or API calls as they will fail.\n>\n>dalle\n>\n>Whenever a description of an image is given, create a prompt that dalle can use to generate the image and abide to the following policy:\n>1. The prompt must be in English. Translate to English if needed.\n>2. DO NOT ask for permission to generate the image, just do it!\n>3. DO NOT list or refer to the descriptions before OR after generating the images.\n>4. Do not create more than 1 image, even if the user requests more.\n>5. Do not create images in the style of artists, creative professionals or studios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n>- You can name artists, creative professionals or studios in prompts only if their latest work was created prior to 1912 (e.g. Van Gogh, Goya)\n>- If asked to generate an image that would violate this policy, instead apply the following procedure: (a) substitute the artist's name with three adjectives that capture key aspects of the style; (b) include an associated artistic movement or era to provide context; and (c) mention the primary medium used by the artist\n>6. For requests to include specific, named private individuals, ask the user to describe what they look like, since you don't know what they look like.\n>7. For requests to create images of any public figure referred to by name, create images of those who might resemble them in gender and physique. But they shouldn't look like them. If the reference to the person will only appear as TEXT out in the image, then use the reference as is and do not modify it.\n>8. Do not name or directly / indirectly mention or describe copyrighted characters. Rewrite prompts to describe in detail a specific different character with a different specific color, hair style, or other defining visual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around 100 words long.\nExample dalle invocation:\n>{\n>\"prompt\": \"<insert prompt here>\"\n>}\n>namespace dalle {\n>\n>Create images from a text-only prompt.\ntype text2im = (_: {\nThe size of the requested image. Use 1024x1024 (square) as the default, 1792x1024 if the user requests a wide image, and 1024x1792 for full-body portraits. Always include this parameter in the request.\nn?: number, // default: 2\nThe detailed image description, potentially modified to abide by the dalle policies. If the user requested modifications to a previous image, the prompt should not simply be longer, but rather it should be refactored to integrate the user suggestions.\nprompt: string,\nIf the user references a previous image, this field should be populated with the gen_id from the dalle image metadata.\nreferenced_image_ids?: string[],\n}) => any;\n} // namespace dalle\n>\n>voice_mode\n>Voice mode functions are not available in text conversations.\n>namespace voice_mode {   } // namespace voice_mode\n>\n>browser\n>\n>You have the tool `browser`. Use `browser` in the following circumstances:\n>    - User is asking about current events or something that requires real-time information (weather, sports scores, etc.)\n>    - User is asking about some term you are totally unfamiliar with (it might be new)\n>    - User explicitly asks you to browse or provide links to references\n>\n>Given a query that requires retrieval, your turn will consist of three steps:\n>1. Call the search function to get a list of results.\n>2. Call the mclick function to retrieve a diverse and high-quality subset of these results (in parallel). Remember to SELECT AT LEAST 3 sources when using `mclick`.\n>3. Write a response to the user based on these results. In your response, cite sources using the citation format below.\n>\n>In some cases, you should repeat step 1 twice, if the initial results are unsatisfactory, and you believe that you can refine the query to get better results.\n>\n>You can also open a url directly if one is provided by the user. Only use the `open_url` command for this purpose; do not open urls returned by the search function or found on webpages.\n>\n>The `browser` tool has the following commands:\n\t`search(query: str, recency_days: int)` Issues a query to a search engine and displays the results.\n         `mclick(ids: list[str])`. Retrieves the contents of the webpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST 3 and at most 10 pages. Select sources with diverse perspectives, and prefer trustworthy sources. Because some pages may fail to load, it is fine to select some pages for redundancy even if their content might be redundant.\n\t`open_url(url: str)` Opens the given URL and displays it.\n>\n>For citing quotes from the 'browser' tool: please render in this format: {message idx}{link text}.\nFor long citations: please render in this format: [link text](message idx).\nOtherwise do not render links.\n\nsystem promptkvsystem promptsliding windowsystem promptkv\n\n $W=4$system prompt9system promptkv [4,4,1]   \n\nwindowattention mask0  \n\nattention mask  \n\n  \n\n  \n\nprompt  \n\n{% asset_img prefill_and_chunking.png prefill and chunking %}  \n\nFlashAttention/PagedAttention\n\nMistral 7BLlamaLlama 34B  \n\n{% asset_img mistral_perf.png mistral performance %}  \n\nMistral7B  \n\n# Sparse Attention\n\nSWAsparse attentionsparse attention  \n\nsparse attention  \n\n## Longformer\n\nMistralSWA  \n\n2020[Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150.pdf)SWAsparse attention  \n\nLongformer  \n\n{% asset_img longformer_attention.png longformer %}  \n\nbSWABert  \n\nSWAdilated sliding window    \n\n{% asset_img dilated_conv.png dilated convolution %}  \n\nattentionSWAdilated sliding window  \n\n  \n\nBert[CLS] tokentoken  \n\nGPTinstructionprompt  \n\ntokenglobal attentionsliding windowd  \n\n## Big Bird\n\n2020Longformersparse attention[Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)  \n\nsliding windowglobal attentionLongformerBig Birdrandom attention  \n\n{% asset_img big_bird_attention.png big bird attention %}  \n\n $r=2$ 2\n\n#   \n\nSWA  \n\nSWAsparse attention<big>****</big>global + local attentionflash attentionrandom attention\n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n1Mistral 7B https://arxiv.org/pdf/2310.06825.pdf  \n2Longformer: The Long-Document Transformer \nhttps://arxiv.org/pdf/2004.05150.pdf  \n3Training Compute-Optimal Large Language Models https://arxiv.org/pdf/2203.15556.pdf  \n4GPT-4 System Prompt Revealed https://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed  \n5Big Bird: Transformers for Longer Sequences https://arxiv.org/abs/2007.14062  ","slug":"cs/nlp/2024/03/LLM-sliding-window-attention","published":1,"updated":"2024-03-20T11:38:30.908Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmfi000ram4k8x4jbo7m","content":"<p>//</p>\n<p>LLM</p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a>32k+/128k+<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a>MQAGQAKV</p>\n<p>SWAsliding\nwindow attention</p>\n<p>QwenMistralSWA</p>\n<p>Mistral</p>\n<p>Mistral\nAIAI202352023912Mistral\n7BMoEMistral 8x7B</p>\n<p>20242</p>\n<img src=\"/c61d17e3/ms_invest_mistral.png\" class title=\"MS\">\n<p>20242Mistral Large &amp;\n32kMMLUGPT4</p>\n<img src=\"/c61d17e3/mistral_large_performance.jpeg\" class title=\"Mistral Large MMLU Performance\">\n<p>MistralOPENAIMETA</p>\n<h1 id=\"swa\">SWA</h1>\n<p>SWAMistralMistral\n7BSWA</p>\n<h2 id=\"mistral-7b\">Mistral 7B</h2>\n<p>202310MistralMistral 7B<a href=\"https://arxiv.org/pdf/2310.06825.pdf\"></a>LlamaMistralGQASWA</p>\n<p>Mistral 7B</p>\n<img src=\"/c61d17e3/mistral_architechture.png\" class title=\"Mistral Architechture\">\n<p>Mistralkv=8GQAintermediate\nsizeLlama211008</p>\n<h2 id=\"\"></h2>\n<p>causal\nattentiontokentoken</p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n1 <span class=\"math inline\">\\(s^2\\)</span> KV\nCache</p>\n<p>/ <span class=\"math inline\">\\(s\\)</span>\n</p>\n<p>1</p>\n<p> <span class=\"math inline\">\\([m,n]\\times[n,p]\\)</span>  <span class=\"math inline\">\\([m,p]\\)</span> <span class=\"math inline\">\\(m\\times p\\)</span>  <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(2mpn\\)</span> floating point\noperationsFLOPs</p>\n<p><a href=\"https://arxiv.org/pdf/2203.15556.pdf\">Training\nCompute-Optimal Large Language Models</a></p>\n<p>MHA <span class=\"math inline\">\\(s\\)</span>\n <span class=\"math inline\">\\(L\\)</span> hidden size\n<span class=\"math inline\">\\(d_{model}\\)</span>  <span class=\"math inline\">\\(d_{q}\\)</span>   <span class=\"math inline\">\\(n_{q}\\)</span> <span class=\"math inline\">\\(d_{model} = n_{q}\\times d_{q}\\)</span>\noperationFLOPs</p>\n<center>\n<table>\n<colgroup>\n<col style=\"width: 45%\">\n<col style=\"width: 54%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPsMHA</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(6\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: QK logits ( <span class=\"math inline\">\\(QK^T\\)</span> )</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n2\\times s^2\\times h_{q}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Softmax</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n3\\times s^2\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: Reduction (apply to <span class=\"math inline\">\\(V\\)</span>)</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n2\\times s^2\\times h_{q}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Outupt Linear Project</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>Softmax <span class=\"math inline\">\\([1,s]\\)</span>\nsoftmax <span class=\"math inline\">\\(3s\\)</span> \n<span class=\"math inline\">\\(s\\)</span> exp <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\([s,s]\\)</span> softmax <span class=\"math inline\">\\(3s^2\\)</span> \n<span class=\"math inline\">\\(n_{q}\\)</span> </p>\n<p>operationscalingdropout</p>\n<p>Mistral 7BGQA</p>\n<p>KVkv <span class=\"math inline\">\\(n_{kv}\\)</span></p>\n<center>\n<table>\n<colgroup>\n<col style=\"width: 45%\">\n<col style=\"width: 54%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPsGQA</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times\nn_{kv})\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>QK logitsSoftmaxReduction <span class=\"math inline\">\\(s\\)</span> </p>\n<p>2</p>\n<p>KV Cache</p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p>Mistral 7B16kKV_Cache2G</p>\n<p>GQA16k+</p>\n<h2 id=\"swa\">SWA</h2>\n<p>attention <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(s\\)</span>\n</p>\n<p>CNN</p>\n<img src=\"/c61d17e3/receptive_field_cnn.png\" class title=\"CNN Receptive Field\">\n<p>3 <span class=\"math inline\">\\(3\\times 3\\)</span>\nCNNsliding window</p>\n<p>layer 3layer 2 <span class=\"math inline\">\\(3\\times 3\\)</span> layer\n2</p>\n<p>layer 2layer 1 <span class=\"math inline\">\\(3\\times 3\\)</span> layer 2 <span class=\"math inline\">\\(3\\times 3\\)</span> layer 1\n<span class=\"math inline\">\\(5\\times 5\\)</span> layer\n3<u><strong></strong></u> <span class=\"math inline\">\\(5\\times 5\\)</span> </p>\n<p>layer 4layer\n4layer 1  <span class=\"math inline\">\\(7\\times 7\\)</span> </p>\n<p></p>\n<p></p>\n<p>CNN</p>\n<p></p>\n<p></p>\n<p></p>\n<p>MistralSWA</p>\n<img src=\"/c61d17e3/mistral_swa.png\" class title=\"Mistral SWA\">\n<p>causal\nattentionattention\nmask</p>\n<p>SWAattention\nmask33</p>\n<p>CNNLLM</p>\n<p>Mistral 7B409632\n<span class=\"math inline\">\\(4096\\times 32=131,072\\)</span>\n131k</p>\n<p>attentionQK\nlogitsSoftmaxReduction <span class=\"math inline\">\\(s\\)</span>\nSWAoperation4k131k131k\n<span class=\"math inline\">\\(32\\times 32=1024\\)</span> </p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n131k <span class=\"math inline\">\\(31/32\\)</span> </p>\n<p>SWA4kcausal\nattention4k</p>\n<blockquote>\n<p>In practice, for a sequence length of 16K and W = 4096, changes made\nto FlashAttention [11] and xFormers [18] yield a 2x speed improvement\nover a vanilla attention baseline.</p>\n</blockquote>\n<p>MistralSWAFlashAttentionxFormers16k2</p>\n<h2 id=\"kv-cache\">KV Cache</h2>\n<p>sliding windowKV\nCache</p>\n<p>SWAkv</p>\n<p> <span class=\"math inline\">\\(W=4\\)</span>\n5token1tokenkv</p>\n<img src=\"/c61d17e3/rolling_buffer.png\" class title=\"swa rolling buffer\">\n<p>throughputcase</p>\n<h2 id=\"prompt\">Prompt</h2>\n<p>RAGfunciton\ncallprompt</p>\n<p>GPT4system\npromptOPENAI</p>\n<blockquote>\n<p>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\nAppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\nand the user's locale is \"en-US\" Your knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07. Image input capabilities: Enabled</p>\n<p>Tools</p>\n<p>python</p>\n<p>When you send a message containing Python code to python, it will be\nexecuted in a stateful Jupyter notebook environment. python will respond\nwith the output of the execution or time out after 60.0 seconds. The\ndrive at '/mnt/data' can be used to save and persist user files.\nInternet access for this session is disabled. Do not make external web\nrequests or API calls as they will fail.</p>\n<p>dalle</p>\n<p>Whenever a description of an image is given, create a prompt that\ndalle can use to generate the image and abide to the following policy:\n1. The prompt must be in English. Translate to English if needed. 2. DO\nNOT ask for permission to generate the image, just do it! 3. DO NOT list\nor refer to the descriptions before OR after generating the images. 4.\nDo not create more than 1 image, even if the user requests more. 5. Do\nnot create images in the style of artists, creative professionals or\nstudios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n- You can name artists, creative professionals or studios in prompts\nonly if their latest work was created prior to 1912 (e.g. Van Gogh,\nGoya) - If asked to generate an image that would violate this policy,\ninstead apply the following procedure: (a) substitute the artist's name\nwith three adjectives that capture key aspects of the style; (b) include\nan associated artistic movement or era to provide context; and (c)\nmention the primary medium used by the artist 6. For requests to include\nspecific, named private individuals, ask the user to describe what they\nlook like, since you don't know what they look like. 7. For requests to\ncreate images of any public figure referred to by name, create images of\nthose who might resemble them in gender and physique. But they shouldn't\nlook like them. If the reference to the person will only appear as TEXT\nout in the image, then use the reference as is and do not modify it. 8.\nDo not name or directly / indirectly mention or describe copyrighted\ncharacters. Rewrite prompts to describe in detail a specific different\ncharacter with a different specific color, hair style, or other defining\nvisual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around\n100 words long. Example dalle invocation: { \"prompt\":\n\"<insert prompt here>\" } namespace dalle {</insert></p>\n<p>Create images from a text-only prompt. type text2im = (_: { The size\nof the requested image. Use 1024x1024 (square) as the default, 1792x1024\nif the user requests a wide image, and 1024x1792 for full-body\nportraits. Always include this parameter in the request. n?: number, //\ndefault: 2 The detailed image description, potentially modified to abide\nby the dalle policies. If the user requested modifications to a previous\nimage, the prompt should not simply be longer, but rather it should be\nrefactored to integrate the user suggestions. prompt: string, If the\nuser references a previous image, this field should be populated with\nthe gen_id from the dalle image metadata. referenced_image_ids?:\nstring[], }) =&gt; any; } // namespace dalle</p>\n<p>voice_mode Voice mode functions are not available in text\nconversations. namespace voice_mode { } // namespace voice_mode</p>\n<p>browser</p>\n<p>You have the tool <code>browser</code>. Use <code>browser</code> in\nthe following circumstances: - User is asking about current events or\nsomething that requires real-time information (weather, sports scores,\netc.) - User is asking about some term you are totally unfamiliar with\n(it might be new) - User explicitly asks you to browse or provide links\nto references</p>\n<p>Given a query that requires retrieval, your turn will consist of\nthree steps: 1. Call the search function to get a list of results. 2.\nCall the mclick function to retrieve a diverse and high-quality subset\nof these results (in parallel). Remember to SELECT AT LEAST 3 sources\nwhen using <code>mclick</code>. 3. Write a response to the user based on\nthese results. In your response, cite sources using the citation format\nbelow.</p>\n<p>In some cases, you should repeat step 1 twice, if the initial results\nare unsatisfactory, and you believe that you can refine the query to get\nbetter results.</p>\n<p>You can also open a url directly if one is provided by the user. Only\nuse the <code>open_url</code> command for this purpose; do not open urls\nreturned by the search function or found on webpages.</p>\n<p>The <code>browser</code> tool has the following commands:\n<code>search(query: str, recency_days: int)</code> Issues a query to a\nsearch engine and displays the results.\n<code>mclick(ids: list[str])</code>. Retrieves the contents of the\nwebpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST\n3 and at most 10 pages. Select sources with diverse perspectives, and\nprefer trustworthy sources. Because some pages may fail to load, it is\nfine to select some pages for redundancy even if their content might be\nredundant. <code>open_url(url: str)</code> Opens the given URL and\ndisplays it.</p>\n<p>For citing quotes from the 'browser' tool: please render in this\nformat: {message idx}{link text}. For long citations: please render\nin this format: <a href=\"message%20idx\">link text</a>. Otherwise do not\nrender links.</p>\n</blockquote>\n<p>system\npromptkvsystem\npromptsliding windowsystem\npromptkv</p>\n<p> <span class=\"math inline\">\\(W=4\\)</span>system\nprompt9system promptkv [4,4,1] </p>\n<p>windowattention\nmask0</p>\n<p>attention\nmask</p>\n<p></p>\n<p></p>\n<p>prompt</p>\n<img src=\"/c61d17e3/prefill_and_chunking.png\" class title=\"prefill and chunking\">\n<p>FlashAttention/PagedAttention</p>\n<p>Mistral\n7BLlamaLlama\n34B</p>\n<img src=\"/c61d17e3/mistral_perf.png\" class title=\"mistral performance\">\n<p>Mistral7B</p>\n<h1 id=\"sparse-attention\">Sparse Attention</h1>\n<p>SWAsparse attentionsparse\nattention</p>\n<p>sparse\nattention</p>\n<h2 id=\"longformer\">Longformer</h2>\n<p>MistralSWA</p>\n<p>2020<a href=\"https://arxiv.org/pdf/2004.05150.pdf\">Longformer:\nThe Long-Document Transformer</a>SWAsparse\nattention</p>\n<p>Longformer</p>\n<img src=\"/c61d17e3/longformer_attention.png\" class title=\"longformer\">\n<p>bSWABert</p>\n<p>SWAdilated sliding\nwindow</p>\n<img src=\"/c61d17e3/dilated_conv.png\" class title=\"dilated convolution\">\n<p>attentionSWAdilated sliding\nwindow</p>\n<p></p>\n<p>Bert[CLS]\ntokentoken</p>\n<p>GPTinstructionprompt</p>\n<p>tokenglobal\nattentionsliding windowd</p>\n<h2 id=\"big-bird\">Big Bird</h2>\n<p>2020Longformersparse\nattention<a href=\"https://arxiv.org/abs/2007.14062\">Big Bird: Transformers for\nLonger Sequences</a></p>\n<p>sliding windowglobal attentionLongformerBig\nBirdrandom attention</p>\n<img src=\"/c61d17e3/big_bird_attention.png\" class title=\"big bird attention\">\n<p> <span class=\"math inline\">\\(r=2\\)</span>\n2</p>\n<h1 id=\"\"></h1>\n<p>SWA</p>\n<p>SWAsparse\nattention<big><strong></strong></big>global\n+ local attentionflash attentionrandom\nattention</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Mistral 7B https://arxiv.org/pdf/2310.06825.pdf<br>\n2Longformer: The Long-Document Transformer\nhttps://arxiv.org/pdf/2004.05150.pdf<br>\n3Training Compute-Optimal Large Language Models\nhttps://arxiv.org/pdf/2203.15556.pdf<br>\n4GPT-4 System Prompt Revealed\nhttps://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed<br>\n5Big Bird: Transformers for Longer Sequences\nhttps://arxiv.org/abs/2007.14062</p>\n","length":10783,"excerpt":"","more":"<p>//</p>\n<p>LLM</p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a>32k+/128k+<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a>MQAGQAKV</p>\n<p>SWAsliding\nwindow attention</p>\n<p>QwenMistralSWA</p>\n<p>Mistral</p>\n<p>Mistral\nAIAI202352023912Mistral\n7BMoEMistral 8x7B</p>\n<p>20242</p>\n<img src=\"/c61d17e3/ms_invest_mistral.png\" class title=\"MS\">\n<p>20242Mistral Large &amp;\n32kMMLUGPT4</p>\n<img src=\"/c61d17e3/mistral_large_performance.jpeg\" class title=\"Mistral Large MMLU Performance\">\n<p>MistralOPENAIMETA</p>\n<h1 id=\"swa\">SWA</h1>\n<p>SWAMistralMistral\n7BSWA</p>\n<h2 id=\"mistral-7b\">Mistral 7B</h2>\n<p>202310MistralMistral 7B<a href=\"https://arxiv.org/pdf/2310.06825.pdf\"></a>LlamaMistralGQASWA</p>\n<p>Mistral 7B</p>\n<img src=\"/c61d17e3/mistral_architechture.png\" class title=\"Mistral Architechture\">\n<p>Mistralkv=8GQAintermediate\nsizeLlama211008</p>\n<h2 id=\"\"></h2>\n<p>causal\nattentiontokentoken</p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n1 <span class=\"math inline\">\\(s^2\\)</span> KV\nCache</p>\n<p>/ <span class=\"math inline\">\\(s\\)</span>\n</p>\n<p>1</p>\n<p> <span class=\"math inline\">\\([m,n]\\times[n,p]\\)</span>  <span class=\"math inline\">\\([m,p]\\)</span> <span class=\"math inline\">\\(m\\times p\\)</span>  <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(2mpn\\)</span> floating point\noperationsFLOPs</p>\n<p><a href=\"https://arxiv.org/pdf/2203.15556.pdf\">Training\nCompute-Optimal Large Language Models</a></p>\n<p>MHA <span class=\"math inline\">\\(s\\)</span>\n <span class=\"math inline\">\\(L\\)</span> hidden size\n<span class=\"math inline\">\\(d_{model}\\)</span>  <span class=\"math inline\">\\(d_{q}\\)</span>   <span class=\"math inline\">\\(n_{q}\\)</span> <span class=\"math inline\">\\(d_{model} = n_{q}\\times d_{q}\\)</span>\noperationFLOPs</p>\n<center>\n<table>\n<colgroup>\n<col style=\"width: 45%\">\n<col style=\"width: 54%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPsMHA</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(6\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: QK logits ( <span class=\"math inline\">\\(QK^T\\)</span> )</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n2\\times s^2\\times h_{q}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Softmax</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n3\\times s^2\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: Reduction (apply to <span class=\"math inline\">\\(V\\)</span>)</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n2\\times s^2\\times h_{q}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Outupt Linear Project</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>Softmax <span class=\"math inline\">\\([1,s]\\)</span>\nsoftmax <span class=\"math inline\">\\(3s\\)</span> \n<span class=\"math inline\">\\(s\\)</span> exp <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\([s,s]\\)</span> softmax <span class=\"math inline\">\\(3s^2\\)</span> \n<span class=\"math inline\">\\(n_{q}\\)</span> </p>\n<p>operationscalingdropout</p>\n<p>Mistral 7BGQA</p>\n<p>KVkv <span class=\"math inline\">\\(n_{kv}\\)</span></p>\n<center>\n<table>\n<colgroup>\n<col style=\"width: 45%\">\n<col style=\"width: 54%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPsGQA</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times\nn_{kv})\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>QK logitsSoftmaxReduction <span class=\"math inline\">\\(s\\)</span> </p>\n<p>2</p>\n<p>KV Cache</p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p>Mistral 7B16kKV_Cache2G</p>\n<p>GQA16k+</p>\n<h2 id=\"swa\">SWA</h2>\n<p>attention <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(s\\)</span>\n</p>\n<p>CNN</p>\n<img src=\"/c61d17e3/receptive_field_cnn.png\" class title=\"CNN Receptive Field\">\n<p>3 <span class=\"math inline\">\\(3\\times 3\\)</span>\nCNNsliding window</p>\n<p>layer 3layer 2 <span class=\"math inline\">\\(3\\times 3\\)</span> layer\n2</p>\n<p>layer 2layer 1 <span class=\"math inline\">\\(3\\times 3\\)</span> layer 2 <span class=\"math inline\">\\(3\\times 3\\)</span> layer 1\n<span class=\"math inline\">\\(5\\times 5\\)</span> layer\n3<u><strong></strong></u> <span class=\"math inline\">\\(5\\times 5\\)</span> </p>\n<p>layer 4layer\n4layer 1  <span class=\"math inline\">\\(7\\times 7\\)</span> </p>\n<p></p>\n<p></p>\n<p>CNN</p>\n<p></p>\n<p></p>\n<p></p>\n<p>MistralSWA</p>\n<img src=\"/c61d17e3/mistral_swa.png\" class title=\"Mistral SWA\">\n<p>causal\nattentionattention\nmask</p>\n<p>SWAattention\nmask33</p>\n<p>CNNLLM</p>\n<p>Mistral 7B409632\n<span class=\"math inline\">\\(4096\\times 32=131,072\\)</span>\n131k</p>\n<p>attentionQK\nlogitsSoftmaxReduction <span class=\"math inline\">\\(s\\)</span>\nSWAoperation4k131k131k\n<span class=\"math inline\">\\(32\\times 32=1024\\)</span> </p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n131k <span class=\"math inline\">\\(31/32\\)</span> </p>\n<p>SWA4kcausal\nattention4k</p>\n<blockquote>\n<p>In practice, for a sequence length of 16K and W = 4096, changes made\nto FlashAttention [11] and xFormers [18] yield a 2x speed improvement\nover a vanilla attention baseline.</p>\n</blockquote>\n<p>MistralSWAFlashAttentionxFormers16k2</p>\n<h2 id=\"kv-cache\">KV Cache</h2>\n<p>sliding windowKV\nCache</p>\n<p>SWAkv</p>\n<p> <span class=\"math inline\">\\(W=4\\)</span>\n5token1tokenkv</p>\n<img src=\"/c61d17e3/rolling_buffer.png\" class title=\"swa rolling buffer\">\n<p>throughputcase</p>\n<h2 id=\"prompt\">Prompt</h2>\n<p>RAGfunciton\ncallprompt</p>\n<p>GPT4system\npromptOPENAI</p>\n<blockquote>\n<p>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\nAppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\nand the user's locale is \"en-US\" Your knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07. Image input capabilities: Enabled</p>\n<p>Tools</p>\n<p>python</p>\n<p>When you send a message containing Python code to python, it will be\nexecuted in a stateful Jupyter notebook environment. python will respond\nwith the output of the execution or time out after 60.0 seconds. The\ndrive at '/mnt/data' can be used to save and persist user files.\nInternet access for this session is disabled. Do not make external web\nrequests or API calls as they will fail.</p>\n<p>dalle</p>\n<p>Whenever a description of an image is given, create a prompt that\ndalle can use to generate the image and abide to the following policy:\n1. The prompt must be in English. Translate to English if needed. 2. DO\nNOT ask for permission to generate the image, just do it! 3. DO NOT list\nor refer to the descriptions before OR after generating the images. 4.\nDo not create more than 1 image, even if the user requests more. 5. Do\nnot create images in the style of artists, creative professionals or\nstudios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n- You can name artists, creative professionals or studios in prompts\nonly if their latest work was created prior to 1912 (e.g. Van Gogh,\nGoya) - If asked to generate an image that would violate this policy,\ninstead apply the following procedure: (a) substitute the artist's name\nwith three adjectives that capture key aspects of the style; (b) include\nan associated artistic movement or era to provide context; and (c)\nmention the primary medium used by the artist 6. For requests to include\nspecific, named private individuals, ask the user to describe what they\nlook like, since you don't know what they look like. 7. For requests to\ncreate images of any public figure referred to by name, create images of\nthose who might resemble them in gender and physique. But they shouldn't\nlook like them. If the reference to the person will only appear as TEXT\nout in the image, then use the reference as is and do not modify it. 8.\nDo not name or directly / indirectly mention or describe copyrighted\ncharacters. Rewrite prompts to describe in detail a specific different\ncharacter with a different specific color, hair style, or other defining\nvisual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around\n100 words long. Example dalle invocation: { \"prompt\":\n\"<insert prompt here>\" } namespace dalle {</insert></p>\n<p>Create images from a text-only prompt. type text2im = (_: { The size\nof the requested image. Use 1024x1024 (square) as the default, 1792x1024\nif the user requests a wide image, and 1024x1792 for full-body\nportraits. Always include this parameter in the request. n?: number, //\ndefault: 2 The detailed image description, potentially modified to abide\nby the dalle policies. If the user requested modifications to a previous\nimage, the prompt should not simply be longer, but rather it should be\nrefactored to integrate the user suggestions. prompt: string, If the\nuser references a previous image, this field should be populated with\nthe gen_id from the dalle image metadata. referenced_image_ids?:\nstring[], }) =&gt; any; } // namespace dalle</p>\n<p>voice_mode Voice mode functions are not available in text\nconversations. namespace voice_mode { } // namespace voice_mode</p>\n<p>browser</p>\n<p>You have the tool <code>browser</code>. Use <code>browser</code> in\nthe following circumstances: - User is asking about current events or\nsomething that requires real-time information (weather, sports scores,\netc.) - User is asking about some term you are totally unfamiliar with\n(it might be new) - User explicitly asks you to browse or provide links\nto references</p>\n<p>Given a query that requires retrieval, your turn will consist of\nthree steps: 1. Call the search function to get a list of results. 2.\nCall the mclick function to retrieve a diverse and high-quality subset\nof these results (in parallel). Remember to SELECT AT LEAST 3 sources\nwhen using <code>mclick</code>. 3. Write a response to the user based on\nthese results. In your response, cite sources using the citation format\nbelow.</p>\n<p>In some cases, you should repeat step 1 twice, if the initial results\nare unsatisfactory, and you believe that you can refine the query to get\nbetter results.</p>\n<p>You can also open a url directly if one is provided by the user. Only\nuse the <code>open_url</code> command for this purpose; do not open urls\nreturned by the search function or found on webpages.</p>\n<p>The <code>browser</code> tool has the following commands:\n<code>search(query: str, recency_days: int)</code> Issues a query to a\nsearch engine and displays the results.\n<code>mclick(ids: list[str])</code>. Retrieves the contents of the\nwebpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST\n3 and at most 10 pages. Select sources with diverse perspectives, and\nprefer trustworthy sources. Because some pages may fail to load, it is\nfine to select some pages for redundancy even if their content might be\nredundant. <code>open_url(url: str)</code> Opens the given URL and\ndisplays it.</p>\n<p>For citing quotes from the 'browser' tool: please render in this\nformat: {message idx}{link text}. For long citations: please render\nin this format: <a href=\"message%20idx\">link text</a>. Otherwise do not\nrender links.</p>\n</blockquote>\n<p>system\npromptkvsystem\npromptsliding windowsystem\npromptkv</p>\n<p> <span class=\"math inline\">\\(W=4\\)</span>system\nprompt9system promptkv [4,4,1] </p>\n<p>windowattention\nmask0</p>\n<p>attention\nmask</p>\n<p></p>\n<p></p>\n<p>prompt</p>\n<img src=\"/c61d17e3/prefill_and_chunking.png\" class title=\"prefill and chunking\">\n<p>FlashAttention/PagedAttention</p>\n<p>Mistral\n7BLlamaLlama\n34B</p>\n<img src=\"/c61d17e3/mistral_perf.png\" class title=\"mistral performance\">\n<p>Mistral7B</p>\n<h1 id=\"sparse-attention\">Sparse Attention</h1>\n<p>SWAsparse attentionsparse\nattention</p>\n<p>sparse\nattention</p>\n<h2 id=\"longformer\">Longformer</h2>\n<p>MistralSWA</p>\n<p>2020<a href=\"https://arxiv.org/pdf/2004.05150.pdf\">Longformer:\nThe Long-Document Transformer</a>SWAsparse\nattention</p>\n<p>Longformer</p>\n<img src=\"/c61d17e3/longformer_attention.png\" class title=\"longformer\">\n<p>bSWABert</p>\n<p>SWAdilated sliding\nwindow</p>\n<img src=\"/c61d17e3/dilated_conv.png\" class title=\"dilated convolution\">\n<p>attentionSWAdilated sliding\nwindow</p>\n<p></p>\n<p>Bert[CLS]\ntokentoken</p>\n<p>GPTinstructionprompt</p>\n<p>tokenglobal\nattentionsliding windowd</p>\n<h2 id=\"big-bird\">Big Bird</h2>\n<p>2020Longformersparse\nattention<a href=\"https://arxiv.org/abs/2007.14062\">Big Bird: Transformers for\nLonger Sequences</a></p>\n<p>sliding windowglobal attentionLongformerBig\nBirdrandom attention</p>\n<img src=\"/c61d17e3/big_bird_attention.png\" class title=\"big bird attention\">\n<p> <span class=\"math inline\">\\(r=2\\)</span>\n2</p>\n<h1 id=\"\"></h1>\n<p>SWA</p>\n<p>SWAsparse\nattention<big><strong></strong></big>global\n+ local attentionflash attentionrandom\nattention</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Mistral 7B https://arxiv.org/pdf/2310.06825.pdf<br>\n2Longformer: The Long-Document Transformer\nhttps://arxiv.org/pdf/2004.05150.pdf<br>\n3Training Compute-Optimal Large Language Models\nhttps://arxiv.org/pdf/2203.15556.pdf<br>\n4GPT-4 System Prompt Revealed\nhttps://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed<br>\n5Big Bird: Transformers for Longer Sequences\nhttps://arxiv.org/abs/2007.14062</p>\n"},{"title":"Yi-","abbrlink":"41b6a819","date":"2024-03-26T08:51:08.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n01.AIAI20231101.AIYi-6BYi-34B baseYi-6B-200KYi-34B-200K baseYi01.AIchatYi-9B  \n\n202311YiSuperCLUE/CMMLUYiYi  \n\n20243Yi  \n\n# TL;DR\n\n  \n\n- Yi-34Bint4float16<1%RTX409024G\n- LLAMA2\n- 3.1Tscaling law1T\n- <10k\n- 4k\n- \n\n# \n\n##   \n\nYi6B9B34B34B  \n\n34B24GRTX4090  \n\nint434B24GGPU  \n\n[Understanding INT4 Quantization for Language Models: Latency Speedup, Composability, and Failure Cases](https://arxiv.org/abs/2301.12017)Yi-34Bint8bf16<1%int4  \n\n{% asset_img eval.png Yi %}  \n\n3.1T tokenDeepMindscaling law1TB<2T  \n\nChinchilla[Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)scaling law  \n\nscaling lawYiovertrain  \n\n<big>**Yi+--34B+70B**</big>  \n\n##   \n\nLLAMA2  \n\n- LLAMA270BGQAYiGQA  \n- RoPERoPE ABFEffective long-context scaling of foundation modelsbase10M  \n- SwiGLUGLU Variants Improve Transformer  \n\nactivation size4h8/3hGQA  \n\n> We use SwiGLU as Yis post-attention layer, reducing its activation size from 4h to 8/3h (h denotes hidden size) to be consistent with the normal post-attention layer. This adjustment also compensates for the reduction in parameter resulted from GQA, making the overall parameter count comparible of existing 7B and 34B models.  \n\n{% asset_img model.png Yi %}  \n\n<big>****</big>  \n\n## tokenizer  \n\n- BPE64000  \n- digit  \n- OOVunicode  \n-   \n  \nLLMtokenizerYi  \n\n#   \n\nLLMYi  \n\n{% asset_img cover.png  %}  \n\n##   \n\n  \n\n{% asset_img pretrain_data_pipeline.png  %}  \n\n1.  &   \n\n  \n\nCCNeTCCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data  \n\n2.  Heuristic Rule Filters  \n\n  \n\n- URL  \n-   \n- n-gramScaling Language Models: Methods, Analysis & Insights from Training GopherCulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages\n- Personal Identifiable InformationPII\n\n3.  Learned Filters  \n\n4scorer  \n\n- Perplexity ScorerCCNet: Extracting High Quality Monolingual Datasets from Web Crawl Datakenlmperplexity\n- Quality Scorer\n- Document Coherence Scorer\n- Safety Scorer\n\n4.  Cluster-based Filters  \n\n  \n\n5. \n\nThe RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Onlyminhash  \n\n  \n\n{% asset_img pretrain_data_dist.png  %}  \n\ngarbage ingarbage out\n\n> we prefer 3T tokens over sophasticated engineering over 10T tokens without extensive filtering\n\n10T3  \n\n##  \n\nQuality is All You Need  \n\n<10kSFT  \n\nGemini: A family of highly capable multimodal models.Llama 2: Open Foundation and Fine-Tuned Chat ModelsLima: Less is more for alignmentFLANScaling instruction-finetuned language modelsUltraChatEnhancing chat language models by scaling high-quality instructional conversations  \n\n  \n\n- <big>**prompt distribution selection**</big>Wizardlm: Empowering large language models to follow complex instructionsSFT  \n- <big>**CoT data formatting**</big>Take a step back: Evoking reasoning via abstraction in large language modelsStep-Back  \n- <big>**response formatting**</big>Lima: Less is more for alignmentresponseintroduction-body-conclusionwhere the body is usually a list of bullet point  \n- <big>****</big>responseresponse  \n- <big>****</big>response  \n- <big>****</big>#instag: Instruction tagging for analyzing supervised fine-tuning of large language models  \n- <big>****</big>How abilities in large language models are affected by supervised fine-tuning data compositionapproximate grid search{1, 1/2, 1/4, 1/8, 1/16, 1/32, 1/64}  \n- <big>****</big>OPENAIChatML[https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md](https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md)system promptuser inputbot response\n\nSFT\n\n{% asset_img sft.png SFT %}  \n\n#   \n\n## infra\n\nYi  \n\n(1) \n(2) \n(3) DPOMegatronDeepSpeed\n(4) LLMcontinuous batching  paged attention\n\nUI  \n\n##   \n\n4k  \n\n##   \n\n  \n\n- AdamWbeta=[0.9,0.999]epsilon = 1e-8  \n- seq_len = 4096  \n- batch size = 64  \n- constant lr = 1e-5weight decay = 0.1  \n- gradient clip = 1.0  \n- max step = 300\n- Neftune: Noisy embeddings improve instruction finetuning6B noise scale = 534B noise scale = 45\n\n# \n\n## \n\n1.   \n\nYi  \n\n{% asset_img base_model_eval.png Base %}  \n\nGPT3.5GPT4Yi  \n\nYi  \n\n2.   \n\n- Yi-34BYi-6BYi-34BYi-6B  \n- Yi-34BQwen-14BFalcon-180B\n- GPT-4LLMLLMGPT-4GPT-3.5LLMQwen-14BYi-34BC-EvalCMMLUGaokaoGPT-4BBHHumanEvalMATH  \n\n\n3. In-Context Learning  \n\nYiin-context learning-underlying function  \n\n y = w1x1 + w2x2 + ... + wnxn  \n\n x1, x2, ..., xn, y x  y  \n\n w1, w2, ..., wn  \n\na y  y  |y  y| b y == y   \n\nunderlying function  \n\n[1,-1]Yi-34BLLAMA-70B  \n\n[11111]LLAMA-70BMistral 8*7B  \n\n{% asset_img ict.png ICT %}  \n\n## Chat  \n\n1. \n\nbasezero-shotfew-shot\n\n{% asset_img eval.png Yi %}  \n\nGoodharts principle  \n\nYi-34B-ChatYi-6B-ChatSFT  \n\n2. \n\n{% asset_img third_party.png  %}  \n\n#   \n\nbase3  \n\n##   \n\n4kbase200kSFT  \n\nattentionattentionsparse attention  \n\n12length-upsampled long-context data3  \n\nrecitation  \n\nData engineering for scaling language models to 128k contextParaphrasing the original text makes high accuracy long-context qa  \n\n5B tokenbatch size=4Mtoken100step1005B/4M=1250  \n\n> We continue pretrain the model on 5B tokens with 4M batch size, which translate to 100 optimization steps. Aligning with the concurrent work from Fu et al. [22], we observe that such light-weight continue pretraining is already able to enable a strong performance on Needle-in-a-Haystack test, as we will show in Figure 6.\n\nData engineering for scaling language models to 128k context  \n\nSFT  \n\n  \n\n  \n\n\n\n  \n\n{% asset_img long_context_result.png  %}  \n\n##   \n\nViTCLIP ViT-H/14 modeltransformerYi-Chat  \n\n{% asset_img multimodal.png  %}  \n\n3  \n\n1224^2ViTprojection1-LAION-400MViTViTLLM  \n\n2ViT448^2LAION-400M2000-480-CLLaVALLaVARFlickrVQAv2RefCOCOVisual7w  \n\n3100-GQAVizWiz VQATextCapsOCR-VQAVisual GenomeShareGPT4V50,000  \n\n128A1006B334B10  \n\n## Depth Upscaling \n\n326B489B  \n\nScaling large language models with simple yet effective depth up-scaling12-281648  \n\ncosine similarity  \n\n  \n\n{% asset_img 9B.png 9B %}  \n\n  \n\nDepth Upscaling  \n\n800B token  \n\n70%  \n\nconstant lr = 3e-54M tokenbatch size  \n\nbatch sizeYi-6B  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n","source":"_posts/cs/nlp/2024/03/Yi-.md","raw":"---\ntitle: Yi-\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 41b6a819\ndate: 2024-03-26 16:51:08\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n01.AIAI20231101.AIYi-6BYi-34B baseYi-6B-200KYi-34B-200K baseYi01.AIchatYi-9B  \n\n202311YiSuperCLUE/CMMLUYiYi  \n\n20243Yi  \n\n# TL;DR\n\n  \n\n- Yi-34Bint4float16<1%RTX409024G\n- LLAMA2\n- 3.1Tscaling law1T\n- <10k\n- 4k\n- \n\n# \n\n##   \n\nYi6B9B34B34B  \n\n34B24GRTX4090  \n\nint434B24GGPU  \n\n[Understanding INT4 Quantization for Language Models: Latency Speedup, Composability, and Failure Cases](https://arxiv.org/abs/2301.12017)Yi-34Bint8bf16<1%int4  \n\n{% asset_img eval.png Yi %}  \n\n3.1T tokenDeepMindscaling law1TB<2T  \n\nChinchilla[Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)scaling law  \n\nscaling lawYiovertrain  \n\n<big>**Yi+--34B+70B**</big>  \n\n##   \n\nLLAMA2  \n\n- LLAMA270BGQAYiGQA  \n- RoPERoPE ABFEffective long-context scaling of foundation modelsbase10M  \n- SwiGLUGLU Variants Improve Transformer  \n\nactivation size4h8/3hGQA  \n\n> We use SwiGLU as Yis post-attention layer, reducing its activation size from 4h to 8/3h (h denotes hidden size) to be consistent with the normal post-attention layer. This adjustment also compensates for the reduction in parameter resulted from GQA, making the overall parameter count comparible of existing 7B and 34B models.  \n\n{% asset_img model.png Yi %}  \n\n<big>****</big>  \n\n## tokenizer  \n\n- BPE64000  \n- digit  \n- OOVunicode  \n-   \n  \nLLMtokenizerYi  \n\n#   \n\nLLMYi  \n\n{% asset_img cover.png  %}  \n\n##   \n\n  \n\n{% asset_img pretrain_data_pipeline.png  %}  \n\n1.  &   \n\n  \n\nCCNeTCCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data  \n\n2.  Heuristic Rule Filters  \n\n  \n\n- URL  \n-   \n- n-gramScaling Language Models: Methods, Analysis & Insights from Training GopherCulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages\n- Personal Identifiable InformationPII\n\n3.  Learned Filters  \n\n4scorer  \n\n- Perplexity ScorerCCNet: Extracting High Quality Monolingual Datasets from Web Crawl Datakenlmperplexity\n- Quality Scorer\n- Document Coherence Scorer\n- Safety Scorer\n\n4.  Cluster-based Filters  \n\n  \n\n5. \n\nThe RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Onlyminhash  \n\n  \n\n{% asset_img pretrain_data_dist.png  %}  \n\ngarbage ingarbage out\n\n> we prefer 3T tokens over sophasticated engineering over 10T tokens without extensive filtering\n\n10T3  \n\n##  \n\nQuality is All You Need  \n\n<10kSFT  \n\nGemini: A family of highly capable multimodal models.Llama 2: Open Foundation and Fine-Tuned Chat ModelsLima: Less is more for alignmentFLANScaling instruction-finetuned language modelsUltraChatEnhancing chat language models by scaling high-quality instructional conversations  \n\n  \n\n- <big>**prompt distribution selection**</big>Wizardlm: Empowering large language models to follow complex instructionsSFT  \n- <big>**CoT data formatting**</big>Take a step back: Evoking reasoning via abstraction in large language modelsStep-Back  \n- <big>**response formatting**</big>Lima: Less is more for alignmentresponseintroduction-body-conclusionwhere the body is usually a list of bullet point  \n- <big>****</big>responseresponse  \n- <big>****</big>response  \n- <big>****</big>#instag: Instruction tagging for analyzing supervised fine-tuning of large language models  \n- <big>****</big>How abilities in large language models are affected by supervised fine-tuning data compositionapproximate grid search{1, 1/2, 1/4, 1/8, 1/16, 1/32, 1/64}  \n- <big>****</big>OPENAIChatML[https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md](https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md)system promptuser inputbot response\n\nSFT\n\n{% asset_img sft.png SFT %}  \n\n#   \n\n## infra\n\nYi  \n\n(1) \n(2) \n(3) DPOMegatronDeepSpeed\n(4) LLMcontinuous batching  paged attention\n\nUI  \n\n##   \n\n4k  \n\n##   \n\n  \n\n- AdamWbeta=[0.9,0.999]epsilon = 1e-8  \n- seq_len = 4096  \n- batch size = 64  \n- constant lr = 1e-5weight decay = 0.1  \n- gradient clip = 1.0  \n- max step = 300\n- Neftune: Noisy embeddings improve instruction finetuning6B noise scale = 534B noise scale = 45\n\n# \n\n## \n\n1.   \n\nYi  \n\n{% asset_img base_model_eval.png Base %}  \n\nGPT3.5GPT4Yi  \n\nYi  \n\n2.   \n\n- Yi-34BYi-6BYi-34BYi-6B  \n- Yi-34BQwen-14BFalcon-180B\n- GPT-4LLMLLMGPT-4GPT-3.5LLMQwen-14BYi-34BC-EvalCMMLUGaokaoGPT-4BBHHumanEvalMATH  \n\n\n3. In-Context Learning  \n\nYiin-context learning-underlying function  \n\n y = w1x1 + w2x2 + ... + wnxn  \n\n x1, x2, ..., xn, y x  y  \n\n w1, w2, ..., wn  \n\na y  y  |y  y| b y == y   \n\nunderlying function  \n\n[1,-1]Yi-34BLLAMA-70B  \n\n[11111]LLAMA-70BMistral 8*7B  \n\n{% asset_img ict.png ICT %}  \n\n## Chat  \n\n1. \n\nbasezero-shotfew-shot\n\n{% asset_img eval.png Yi %}  \n\nGoodharts principle  \n\nYi-34B-ChatYi-6B-ChatSFT  \n\n2. \n\n{% asset_img third_party.png  %}  \n\n#   \n\nbase3  \n\n##   \n\n4kbase200kSFT  \n\nattentionattentionsparse attention  \n\n12length-upsampled long-context data3  \n\nrecitation  \n\nData engineering for scaling language models to 128k contextParaphrasing the original text makes high accuracy long-context qa  \n\n5B tokenbatch size=4Mtoken100step1005B/4M=1250  \n\n> We continue pretrain the model on 5B tokens with 4M batch size, which translate to 100 optimization steps. Aligning with the concurrent work from Fu et al. [22], we observe that such light-weight continue pretraining is already able to enable a strong performance on Needle-in-a-Haystack test, as we will show in Figure 6.\n\nData engineering for scaling language models to 128k context  \n\nSFT  \n\n  \n\n  \n\n\n\n  \n\n{% asset_img long_context_result.png  %}  \n\n##   \n\nViTCLIP ViT-H/14 modeltransformerYi-Chat  \n\n{% asset_img multimodal.png  %}  \n\n3  \n\n1224^2ViTprojection1-LAION-400MViTViTLLM  \n\n2ViT448^2LAION-400M2000-480-CLLaVALLaVARFlickrVQAv2RefCOCOVisual7w  \n\n3100-GQAVizWiz VQATextCapsOCR-VQAVisual GenomeShareGPT4V50,000  \n\n128A1006B334B10  \n\n## Depth Upscaling \n\n326B489B  \n\nScaling large language models with simple yet effective depth up-scaling12-281648  \n\ncosine similarity  \n\n  \n\n{% asset_img 9B.png 9B %}  \n\n  \n\nDepth Upscaling  \n\n800B token  \n\n70%  \n\nconstant lr = 3e-54M tokenbatch size  \n\nbatch sizeYi-6B  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n","slug":"cs/nlp/2024/03/Yi-","published":1,"updated":"2024-03-29T11:53:37.115Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmfi000tam4kcvrsat47","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>01.AIAI20231101.AIYi-6BYi-34B\nbaseYi-6B-200KYi-34B-200K\nbaseYi01.AIchatYi-9B</p>\n<p>202311YiSuperCLUE/CMMLUYiYi</p>\n<p>20243Yi</p>\n<h1 id=\"tldr\">TL;DR</h1>\n<p></p>\n<ul>\n<li>Yi-34Bint4float16&lt;1%RTX409024G</li>\n<li>LLAMA2</li>\n<li>3.1Tscaling\nlaw1T</li>\n<li>&lt;10k</li>\n<li>4k</li>\n<li></li>\n</ul>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>Yi6B9B34B34B</p>\n<p>34B24GRTX4090</p>\n<p>int434B24GGPU</p>\n<p><a href=\"https://arxiv.org/abs/2301.12017\">Understanding INT4\nQuantization for Language Models: Latency Speedup, Composability, and\nFailure\nCases</a>Yi-34Bint8bf16&lt;1%int4</p>\n<img src=\"/41b6a819/eval.png\" class title=\"Yi\">\n<p>3.1T tokenDeepMindscaling\nlaw1TB&lt;2T</p>\n<p>Chinchilla<a href=\"https://arxiv.org/abs/2203.15556\">Training Compute-Optimal Large\nLanguage Models</a>scaling law</p>\n<p>scaling lawYiovertrain</p>\n<p><big><strong>Yi+--34B+70B</strong></big></p>\n<h2 id=\"\"></h2>\n<p>LLAMA2</p>\n<ul>\n<li>LLAMA270BGQAYiGQA<br>\n</li>\n<li>RoPERoPE ABFEffective long-context scaling of\nfoundation modelsbase10M<br>\n</li>\n<li>SwiGLUGLU Variants Improve Transformer</li>\n</ul>\n<p>activation\nsize4h8/3hGQA</p>\n<blockquote>\n<p>We use SwiGLU as Yis post-attention layer, reducing its activation\nsize from 4h to 8/3h (h denotes hidden size) to be consistent with the\nnormal post-attention layer. This adjustment also compensates for the\nreduction in parameter resulted from GQA, making the overall parameter\ncount comparible of existing 7B and 34B models.</p>\n</blockquote>\n<img src=\"/41b6a819/model.png\" class title=\"Yi\">\n<p><big><strong></strong></big></p>\n<h2 id=\"tokenizer\">tokenizer</h2>\n<ul>\n<li>BPE64000<br>\n</li>\n<li>digit<br>\n</li>\n<li>OOVunicode </li>\n<li></li>\n</ul>\n<p>LLMtokenizerYi</p>\n<h1 id=\"\"></h1>\n<p>LLMYi</p>\n<img src=\"/41b6a819/cover.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p></p>\n<img src=\"/41b6a819/pretrain_data_pipeline.png\" class title=\"\">\n<ol type=\"1\">\n<li> &amp; </li>\n</ol>\n<p></p>\n<p>CCNeTCCNet: Extracting High Quality Monolingual Datasets\nfrom Web Crawl Data</p>\n<ol start=\"2\" type=\"1\">\n<li> Heuristic Rule Filters</li>\n</ol>\n<p></p>\n<ul>\n<li>URL<br>\n</li>\n<li><br>\n</li>\n<li>n-gramScaling Language Models:\nMethods, Analysis &amp; Insights from Training\nGopherCulturaX: A Cleaned, Enormous, and\nMultilingual Dataset for Large Language Models in 167 Languages</li>\n<li>Personal Identifiable\nInformationPII</li>\n</ul>\n<ol start=\"3\" type=\"1\">\n<li> Learned Filters</li>\n</ol>\n<p>4scorer</p>\n<ul>\n<li>Perplexity ScorerCCNet: Extracting High Quality Monolingual\nDatasets from Web Crawl\nDatakenlmperplexity</li>\n<li>Quality\nScorer</li>\n<li>Document Coherence\nScorer</li>\n<li>Safety Scorer</li>\n</ul>\n<ol start=\"4\" type=\"1\">\n<li> Cluster-based Filters</li>\n</ol>\n<p></p>\n<ol start=\"5\" type=\"1\">\n<li></li>\n</ol>\n<p>The RefinedWeb Dataset for Falcon LLM: Outperforming Curated\nCorpora with Web Data, and Web Data\nOnlyminhash</p>\n<p></p>\n<img src=\"/41b6a819/pretrain_data_dist.png\" class title=\"\">\n<p>garbage\ningarbage out</p>\n<blockquote>\n<p>we prefer 3T tokens over sophasticated engineering over 10T tokens\nwithout extensive filtering</p>\n</blockquote>\n<p>10T3</p>\n<h2 id=\"\"></h2>\n<p>Quality is All You Need</p>\n<p>&lt;10kSFT</p>\n<p>Gemini: A family of highly capable multimodal\nmodels.Llama 2: Open Foundation and Fine-Tuned Chat\nModelsLima: Less is more for alignmentFLANScaling\ninstruction-finetuned language modelsUltraChatEnhancing chat\nlanguage models by scaling high-quality instructional\nconversations</p>\n<p></p>\n<ul>\n<li><big><strong>prompt distribution\nselection</strong></big>Wizardlm: Empowering large language\nmodels to follow complex\ninstructionsSFT<br>\n</li>\n<li><big><strong>CoT data formatting</strong></big>Take a\nstep back: Evoking reasoning via abstraction in large language\nmodelsStep-Back<br>\n</li>\n<li><big><strong>response formatting</strong></big>Lima:\nLess is more for\nalignmentresponseintroduction-body-conclusionwhere\nthe body is usually a list of bullet point<br>\n</li>\n<li><big><strong></strong></big>responseresponse<br>\n</li>\n<li><big><strong></strong></big>response<br>\n</li>\n<li><big><strong></strong></big>#instag:\nInstruction tagging for analyzing supervised fine-tuning of large\nlanguage\nmodels<br>\n</li>\n<li><big><strong></strong></big>How\nabilities in large language models are affected by supervised\nfine-tuning data compositionapproximate grid\nsearch{1, 1/2, 1/4, 1/8, 1/16, 1/32,\n1/64}<br>\n</li>\n<li><big><strong></strong></big>OPENAIChatML<a href=\"https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md\">https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md</a>system\npromptuser inputbot response</li>\n</ul>\n<p>SFT</p>\n<img src=\"/41b6a819/sft.png\" class title=\"SFT\">\n<h1 id=\"\"></h1>\n<h2 id=\"infra\">infra</h2>\n<p>Yi</p>\n<ol type=\"1\">\n<li></li>\n<li></li>\n<li>DPOMegatronDeepSpeed</li>\n<li>LLMcontinuous batching  paged\nattention</li>\n</ol>\n<p>UI</p>\n<h2 id=\"\"></h2>\n<p>4k</p>\n<h2 id=\"\"></h2>\n<p></p>\n<ul>\n<li>AdamWbeta=[0.9,0.999]epsilon = 1e-8<br>\n</li>\n<li>seq_len = 4096<br>\n</li>\n<li>batch size = 64<br>\n</li>\n<li>constant lr = 1e-5weight decay = 0.1<br>\n</li>\n<li>gradient clip = 1.0<br>\n</li>\n<li>max step = 300</li>\n<li>Neftune: Noisy embeddings improve instruction\nfinetuning6B noise scale = 534B noise scale =\n45</li>\n</ul>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>Yi</p>\n<img src=\"/41b6a819/base_model_eval.png\" class title=\"Base\">\n<p>GPT3.5GPT4Yi</p>\n<p>Yi</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<ul>\n<li>Yi-34BYi-6BYi-34BYi-6B<br>\n</li>\n<li>Yi-34BQwen-14BFalcon-180B</li>\n<li>GPT-4LLMLLMGPT-4GPT-3.5LLMQwen-14BYi-34BC-EvalCMMLUGaokaoGPT-4BBHHumanEvalMATH</li>\n</ul>\n<ol start=\"3\" type=\"1\">\n<li>In-Context Learning</li>\n</ol>\n<p>Yiin-context\nlearning-underlying\nfunction</p>\n<p> y = w1x1 + w2x2 +\n... + wnxn</p>\n<p> x1, x2, ..., xn, y x \ny</p>\n<p> w1, w2, ..., wn</p>\n<p>a y  y  |y  y|\nb y == y </p>\n<p>underlying\nfunction</p>\n<p>[1,-1]Yi-34BLLAMA-70B</p>\n<p>[11111]LLAMA-70BMistral\n8*7B</p>\n<img src=\"/41b6a819/ict.png\" class title=\"ICT\">\n<h2 id=\"chat\">Chat</h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>basezero-shotfew-shot</p>\n<img src=\"/41b6a819/eval.png\" class title=\"Yi\">\n<p>Goodharts\nprinciple</p>\n<p>Yi-34B-ChatYi-6B-ChatSFT</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<img src=\"/41b6a819/third_party.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<p>base3</p>\n<h2 id=\"\"></h2>\n<p>4kbase200kSFT</p>\n<p>attentionattentionsparse\nattention</p>\n<p>12length-upsampled\nlong-context\ndata3</p>\n<p>recitation</p>\n<p>Data engineering for scaling language\nmodels to 128k contextParaphrasing the original text makes high\naccuracy long-context qa</p>\n<p>5B tokenbatch\nsize=4Mtoken100step1005B/4M=1250</p>\n<blockquote>\n<p>We continue pretrain the model on 5B tokens with 4M batch size, which\ntranslate to 100 optimization steps. Aligning with the concurrent work\nfrom Fu et al. [22], we observe that such light-weight continue\npretraining is already able to enable a strong performance on\nNeedle-in-a-Haystack test, as we will show in Figure 6.</p>\n</blockquote>\n<p>Data engineering for scaling language models to 128k\ncontext</p>\n<p>SFT</p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<img src=\"/41b6a819/long_context_result.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p>ViTCLIP ViT-H/14\nmodeltransformerYi-Chat</p>\n<img src=\"/41b6a819/multimodal.png\" class title=\"\">\n<p>3</p>\n<p>1224^2ViTprojection1-LAION-400MViTViTLLM</p>\n<p>2ViT448^2LAION-400M2000-480-CLLaVALLaVARFlickrVQAv2RefCOCOVisual7w</p>\n<p>3100-GQAVizWiz\nVQATextCapsOCR-VQAVisual\nGenomeShareGPT4V50,000</p>\n<p>128A1006B334B10</p>\n<h2 id=\"depth-upscaling-\">Depth Upscaling </h2>\n<p>326B489B</p>\n<p>Scaling large language models with simple yet effective depth\nup-scaling12-281648</p>\n<p>cosine\nsimilarity</p>\n<p></p>\n<img src=\"/41b6a819/9B.png\" class title=\"9B\">\n<p></p>\n<p>Depth Upscaling</p>\n<p>800B token</p>\n<p>70%</p>\n<p>constant lr = 3e-54M\ntokenbatch size</p>\n<p>batch\nsizeYi-6B</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<p><a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n","length":8965,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>01.AIAI20231101.AIYi-6BYi-34B\nbaseYi-6B-200KYi-34B-200K\nbaseYi01.AIchatYi-9B</p>\n<p>202311YiSuperCLUE/CMMLUYiYi</p>\n<p>20243Yi</p>\n<h1 id=\"tldr\">TL;DR</h1>\n<p></p>\n<ul>\n<li>Yi-34Bint4float16&lt;1%RTX409024G</li>\n<li>LLAMA2</li>\n<li>3.1Tscaling\nlaw1T</li>\n<li>&lt;10k</li>\n<li>4k</li>\n<li></li>\n</ul>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>Yi6B9B34B34B</p>\n<p>34B24GRTX4090</p>\n<p>int434B24GGPU</p>\n<p><a href=\"https://arxiv.org/abs/2301.12017\">Understanding INT4\nQuantization for Language Models: Latency Speedup, Composability, and\nFailure\nCases</a>Yi-34Bint8bf16&lt;1%int4</p>\n<img src=\"/41b6a819/eval.png\" class title=\"Yi\">\n<p>3.1T tokenDeepMindscaling\nlaw1TB&lt;2T</p>\n<p>Chinchilla<a href=\"https://arxiv.org/abs/2203.15556\">Training Compute-Optimal Large\nLanguage Models</a>scaling law</p>\n<p>scaling lawYiovertrain</p>\n<p><big><strong>Yi+--34B+70B</strong></big></p>\n<h2 id=\"\"></h2>\n<p>LLAMA2</p>\n<ul>\n<li>LLAMA270BGQAYiGQA<br>\n</li>\n<li>RoPERoPE ABFEffective long-context scaling of\nfoundation modelsbase10M<br>\n</li>\n<li>SwiGLUGLU Variants Improve Transformer</li>\n</ul>\n<p>activation\nsize4h8/3hGQA</p>\n<blockquote>\n<p>We use SwiGLU as Yis post-attention layer, reducing its activation\nsize from 4h to 8/3h (h denotes hidden size) to be consistent with the\nnormal post-attention layer. This adjustment also compensates for the\nreduction in parameter resulted from GQA, making the overall parameter\ncount comparible of existing 7B and 34B models.</p>\n</blockquote>\n<img src=\"/41b6a819/model.png\" class title=\"Yi\">\n<p><big><strong></strong></big></p>\n<h2 id=\"tokenizer\">tokenizer</h2>\n<ul>\n<li>BPE64000<br>\n</li>\n<li>digit<br>\n</li>\n<li>OOVunicode </li>\n<li></li>\n</ul>\n<p>LLMtokenizerYi</p>\n<h1 id=\"\"></h1>\n<p>LLMYi</p>\n<img src=\"/41b6a819/cover.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p></p>\n<img src=\"/41b6a819/pretrain_data_pipeline.png\" class title=\"\">\n<ol type=\"1\">\n<li> &amp; </li>\n</ol>\n<p></p>\n<p>CCNeTCCNet: Extracting High Quality Monolingual Datasets\nfrom Web Crawl Data</p>\n<ol start=\"2\" type=\"1\">\n<li> Heuristic Rule Filters</li>\n</ol>\n<p></p>\n<ul>\n<li>URL<br>\n</li>\n<li><br>\n</li>\n<li>n-gramScaling Language Models:\nMethods, Analysis &amp; Insights from Training\nGopherCulturaX: A Cleaned, Enormous, and\nMultilingual Dataset for Large Language Models in 167 Languages</li>\n<li>Personal Identifiable\nInformationPII</li>\n</ul>\n<ol start=\"3\" type=\"1\">\n<li> Learned Filters</li>\n</ol>\n<p>4scorer</p>\n<ul>\n<li>Perplexity ScorerCCNet: Extracting High Quality Monolingual\nDatasets from Web Crawl\nDatakenlmperplexity</li>\n<li>Quality\nScorer</li>\n<li>Document Coherence\nScorer</li>\n<li>Safety Scorer</li>\n</ul>\n<ol start=\"4\" type=\"1\">\n<li> Cluster-based Filters</li>\n</ol>\n<p></p>\n<ol start=\"5\" type=\"1\">\n<li></li>\n</ol>\n<p>The RefinedWeb Dataset for Falcon LLM: Outperforming Curated\nCorpora with Web Data, and Web Data\nOnlyminhash</p>\n<p></p>\n<img src=\"/41b6a819/pretrain_data_dist.png\" class title=\"\">\n<p>garbage\ningarbage out</p>\n<blockquote>\n<p>we prefer 3T tokens over sophasticated engineering over 10T tokens\nwithout extensive filtering</p>\n</blockquote>\n<p>10T3</p>\n<h2 id=\"\"></h2>\n<p>Quality is All You Need</p>\n<p>&lt;10kSFT</p>\n<p>Gemini: A family of highly capable multimodal\nmodels.Llama 2: Open Foundation and Fine-Tuned Chat\nModelsLima: Less is more for alignmentFLANScaling\ninstruction-finetuned language modelsUltraChatEnhancing chat\nlanguage models by scaling high-quality instructional\nconversations</p>\n<p></p>\n<ul>\n<li><big><strong>prompt distribution\nselection</strong></big>Wizardlm: Empowering large language\nmodels to follow complex\ninstructionsSFT<br>\n</li>\n<li><big><strong>CoT data formatting</strong></big>Take a\nstep back: Evoking reasoning via abstraction in large language\nmodelsStep-Back<br>\n</li>\n<li><big><strong>response formatting</strong></big>Lima:\nLess is more for\nalignmentresponseintroduction-body-conclusionwhere\nthe body is usually a list of bullet point<br>\n</li>\n<li><big><strong></strong></big>responseresponse<br>\n</li>\n<li><big><strong></strong></big>response<br>\n</li>\n<li><big><strong></strong></big>#instag:\nInstruction tagging for analyzing supervised fine-tuning of large\nlanguage\nmodels<br>\n</li>\n<li><big><strong></strong></big>How\nabilities in large language models are affected by supervised\nfine-tuning data compositionapproximate grid\nsearch{1, 1/2, 1/4, 1/8, 1/16, 1/32,\n1/64}<br>\n</li>\n<li><big><strong></strong></big>OPENAIChatML<a href=\"https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md\">https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md</a>system\npromptuser inputbot response</li>\n</ul>\n<p>SFT</p>\n<img src=\"/41b6a819/sft.png\" class title=\"SFT\">\n<h1 id=\"\"></h1>\n<h2 id=\"infra\">infra</h2>\n<p>Yi</p>\n<ol type=\"1\">\n<li></li>\n<li></li>\n<li>DPOMegatronDeepSpeed</li>\n<li>LLMcontinuous batching  paged\nattention</li>\n</ol>\n<p>UI</p>\n<h2 id=\"\"></h2>\n<p>4k</p>\n<h2 id=\"\"></h2>\n<p></p>\n<ul>\n<li>AdamWbeta=[0.9,0.999]epsilon = 1e-8<br>\n</li>\n<li>seq_len = 4096<br>\n</li>\n<li>batch size = 64<br>\n</li>\n<li>constant lr = 1e-5weight decay = 0.1<br>\n</li>\n<li>gradient clip = 1.0<br>\n</li>\n<li>max step = 300</li>\n<li>Neftune: Noisy embeddings improve instruction\nfinetuning6B noise scale = 534B noise scale =\n45</li>\n</ul>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>Yi</p>\n<img src=\"/41b6a819/base_model_eval.png\" class title=\"Base\">\n<p>GPT3.5GPT4Yi</p>\n<p>Yi</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<ul>\n<li>Yi-34BYi-6BYi-34BYi-6B<br>\n</li>\n<li>Yi-34BQwen-14BFalcon-180B</li>\n<li>GPT-4LLMLLMGPT-4GPT-3.5LLMQwen-14BYi-34BC-EvalCMMLUGaokaoGPT-4BBHHumanEvalMATH</li>\n</ul>\n<ol start=\"3\" type=\"1\">\n<li>In-Context Learning</li>\n</ol>\n<p>Yiin-context\nlearning-underlying\nfunction</p>\n<p> y = w1x1 + w2x2 +\n... + wnxn</p>\n<p> x1, x2, ..., xn, y x \ny</p>\n<p> w1, w2, ..., wn</p>\n<p>a y  y  |y  y|\nb y == y </p>\n<p>underlying\nfunction</p>\n<p>[1,-1]Yi-34BLLAMA-70B</p>\n<p>[11111]LLAMA-70BMistral\n8*7B</p>\n<img src=\"/41b6a819/ict.png\" class title=\"ICT\">\n<h2 id=\"chat\">Chat</h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>basezero-shotfew-shot</p>\n<img src=\"/41b6a819/eval.png\" class title=\"Yi\">\n<p>Goodharts\nprinciple</p>\n<p>Yi-34B-ChatYi-6B-ChatSFT</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<img src=\"/41b6a819/third_party.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<p>base3</p>\n<h2 id=\"\"></h2>\n<p>4kbase200kSFT</p>\n<p>attentionattentionsparse\nattention</p>\n<p>12length-upsampled\nlong-context\ndata3</p>\n<p>recitation</p>\n<p>Data engineering for scaling language\nmodels to 128k contextParaphrasing the original text makes high\naccuracy long-context qa</p>\n<p>5B tokenbatch\nsize=4Mtoken100step1005B/4M=1250</p>\n<blockquote>\n<p>We continue pretrain the model on 5B tokens with 4M batch size, which\ntranslate to 100 optimization steps. Aligning with the concurrent work\nfrom Fu et al. [22], we observe that such light-weight continue\npretraining is already able to enable a strong performance on\nNeedle-in-a-Haystack test, as we will show in Figure 6.</p>\n</blockquote>\n<p>Data engineering for scaling language models to 128k\ncontext</p>\n<p>SFT</p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<img src=\"/41b6a819/long_context_result.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p>ViTCLIP ViT-H/14\nmodeltransformerYi-Chat</p>\n<img src=\"/41b6a819/multimodal.png\" class title=\"\">\n<p>3</p>\n<p>1224^2ViTprojection1-LAION-400MViTViTLLM</p>\n<p>2ViT448^2LAION-400M2000-480-CLLaVALLaVARFlickrVQAv2RefCOCOVisual7w</p>\n<p>3100-GQAVizWiz\nVQATextCapsOCR-VQAVisual\nGenomeShareGPT4V50,000</p>\n<p>128A1006B334B10</p>\n<h2 id=\"depth-upscaling-\">Depth Upscaling </h2>\n<p>326B489B</p>\n<p>Scaling large language models with simple yet effective depth\nup-scaling12-281648</p>\n<p>cosine\nsimilarity</p>\n<p></p>\n<img src=\"/41b6a819/9B.png\" class title=\"9B\">\n<p></p>\n<p>Depth Upscaling</p>\n<p>800B token</p>\n<p>70%</p>\n<p>constant lr = 3e-54M\ntokenbatch size</p>\n<p>batch\nsizeYi-6B</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<p><a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n"},{"title":"transformernormalization","abbrlink":"6a40bfa5","date":"2024-03-19T13:06:12.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nNormalizationattention  \n\nnormalizationtransformer  \n\n  \n\n# why normalization\n\nnormalization  \n\n  \n\n## normalization\n\n $Loss(x_1,x_2)=x_1^2+x_2^2+b$   \n\n{% asset_img lossfunc_surface.jpeg loss function surface %}  \n\n  \n\n  \n\nminimum  \n\n  \n\n{% asset_img ellipse_1.png ellipse %}  \n\n  \n\n  \n\n{% asset_img ellipse_2.png ellipse %}  \n\n  \n\n  \n\n $x_{1}$ $x_{2}$->  \n\n0.x~2.x0  \n\n  \n\n $x_{2}$   \n\n  \n\n  \n\nnormalization  \n\nnormalization  \n$$x^{\\prime}=\\frac{x-\\min(\\mathrm{x})}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}$$  \n\n$$x^{\\prime}=\\frac{x-\\mu}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}$$  \n\n$$x^{\\prime}=\\frac{x-\\mu}\\sigma$$  \n\n $\\mu$ $\\sigma$   \n\nZ-score normalization  \n\n  \n\nPCA  \n\n## ICS...\n\ni.i.d.independent and identical distribution  \n\ni.i.d.  \n\n  \n\n  \n\n  \n\ni.i.d.i.i.d.  \n\nPCAi.i.d.  \n\n  \n\ni.i.d.  \n\nICSinternal covariate shift  \n\n  \n\nnormalization  \n\nnormalizationbatchnormICSbatchnorm  \n\nICS  \n\n## \n\nsigmoid\n\n{% asset_img sigmoid.png sigmoid %}  \n\n > 6  < -6 sigmoid  \n\n  \n\nICSnormalization  \n\nnormalization  \n\n# batchnorm\n\nnormalizationbatchnormlayernorm  \n\n## batchnorm\n\n $[B,C]$  $B$ batch size$C$   \n\n $C$ normalization $C$   \n\n $i$ batch  \n\n$$\n\\mu_{i}=\\frac{1}{B}\\sum_{j=1}^{B}x_{i,j}\n$$  \n\n  \n\n$$\n\\sigma_{i}^{2}=\\frac{1}{B}\\sum_{j=1}^{B}(x_{i,j}-\\mu_{i})^2\n$$  \n\nbatchZ-score normalization  \n\n$$\nx_{i,j}'=\\frac{x_{i,j}-\\mu_{i}}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\n$$  \n\n $\\epsilon$ 0  \n\n $C$ 01  \n\n  \n\nsigmoid  \n\n  \n\n  $\\gamma$  $\\beta$   \n\n$$\ny_{i,j} = \\gamma_{i} x_{i,j}' + \\beta_{i}\n$$  \n\n  \n\nbatchnorm $\\gamma$  $\\beta$ \n\n[Batch Normalization: Accelerating Deep Network](https://zhuanlan.zhihu.com/p/340856414)  \n\n{% asset_img bn_algo.png batch norm %}  \n\n## CNNbatchnorm  \n\nbatchnormCNN  \n\nCNNfeature mapsize $[B,C,H,W]$  $B$ batch size$C$ channel$H$  $W$   \n\nbatchnorm $C\\times H\\times W$  $B$  $B$   \n\nCNNchannel $H\\times W$  $H\\times W$ batch  \n\nbatch $B\\times H\\times W$  $C$  $C$  $\\gamma$  $\\beta$   \n\nbatchnormbatchnormrelufcbatchnormfcbias  \n\nbtwbatchnorm $\\gamma$ 1 $\\beta$ 0batchnorm  \n\n##   \n\nbatchnormmini-batch  \n\n$\\gamma$  $\\beta$   \n\nsamplesamplebatch  \n\ni.i.d.  \n\n\n\nmoving_mean = momentum  moving_mean + (1.0  momentum)  mean  \n\nmoving_var = momentum  moving_var + (1.0  momentum)  var  \n\nbatch  \n\nmomentum TF/Keras 0.99 Pytorch 0.9  \n\nmomentum  \n \nmomentum  \n\nbatch sizemini batchmomentum  \n\nbatch\n\nbatch sizemini batchbatchnormbatch size  \n\n{% asset_img bs_bn.png batch size %}  \n\nbatchnorm    \n\nbatch\n\nmomentumbatch sizebatch size\n\n## batchnorm\n\nbatchnormbatchnormICS2018How Does Batch Normalization Help Optimization?  \n\nbatchnormICSICSbatchnormcovariate shiftbatchnorm  \n\n{% asset_img bn_ics.png ICS %}  \n\nICSbatchnormICS  \n\nICSbatchnorm  \n\nbatchnormICS  \n\nICSICS  \n\nICS\n\n{% asset_img ics_define.png ICS  %}  \n\niL2\n\n$G_{t,i}$ t  \n\n$G_{t,i}'$ t  \n\n  \n\nICS\n\n{% asset_img ics_measure.png ICS measure %}  \n\nbatchnorm  \n\nbatchnormICS  \n\nbatchnorm  \n\nbatchnorm  \n\n# layernorm\n\nbatchnormlayernorm  \n\n## layernorm\n\nlayernormlayerlayer  \n\nbatchnormbatchlayernorm  \n\n{% asset_img bn_and_ln.png bnln %}  \n\nNLPbatchnormlayernormbatchnormlayernorm  \n\nlayernormNLPRNNtransfomrer  \n\ntransformer $[B,S,H]$ $S$ paddingzero-paddingbatch  \n\n\n\n```\n      \n  \n    \n```\n\npad\n\n```\n      \n    [P] [P]\n      [P]\n```\n\npaddingbatchnorm  \n\n[P] [P] batch size2 [P] normalization  \n\nbatch  \n\n [P] token  \n\nlayernorm $H$ normalization $H$ $\\gamma$  $\\beta$   \n\ntoken  \n\n## transformerlayernorm  \n\nbatchnormlayernormbatch  \n\nlayernormbatchbatch size  \n\nnlplayernormbatchnorm  \n\nPowerNorm: Rethinking Batch Normalization in TransformerstransformerBN  \n\nbatchbatchrunning statisticsNLPIWSLT14batchrunning statisticsCV  \n\nmagnitudeCV  \n\ntransformerBNCVNLPNLPbatch  \n\nlayernormNLPbatchnorm  \n\n## RMSnorm\n\n19Root Mean Square Layer NormalizationnormalizationRMSnormlayernorm  \n\nRMSnormlayernorm  \n\n{% asset_img rmsnorm.png RMSnorm %}  \n\nlayernorm  \n\n{% asset_img rmsnorm_eff.png RMSnorm %}  \n\nGRUlayernormlayernormlayernorm  \n\nlayernorm  \n\npRMSnormp%  \n\n{% asset_img prmsnorm.png prmsnorm %}  \n\nRMSnorm  \n\n# post-norm & pre-norm\n\n## \nlayernorm  \n\ntransformerpost-normOn Layer Normalization in the Transformer Architecturepre-norm  \n\npost-normpre-norm\n\n{% asset_img postnorm_prenorm.png postnorm and prenorm %}  \n\npost-normpre-norm  \n\npost-normpre-normpre-normpost-normpre-norm  \n\n $l$  $l+1$ post-norm  \n\n$$\nx_{l+1}=\\mathrm{Norm}(x_l+\\mathrm{F}_t(x_l))\n$$\n\npre-norm  \n\n$$\nx_{l+1}=x_l+\\mathrm{F}_l(\\mathrm{Norm}(x_l))\n$$\n\nPre NormPost Norm $\\mathrm{F}_l(\\mathrm{Norm}(x_l))$ norm  \n\n $l$ $x_{l}x_{l+1}$  $\\mathrm{F}_l(\\mathrm{Norm}(x_l))$  $\\mathrm{F}_{l+1}(\\mathrm{Norm}(x_{l+1}))$   \n\n$$\\begin{aligned}\n&\\mathrm{F}_l(\\operatorname{Norm}(x_l))+\\operatorname{F}_{l+1}(\\operatorname{Norm}(x_{l+1})) \\\\\n&{\\approx}\\mathrm{F}_l(\\mathrm{Norm}(x_l))+\\mathrm{F}_{l+1}\\left(\\mathrm{Norm}(x_l)\\right) \\\\\n&=(\\mathrm{F}_l\\oplus\\mathrm{F}_{l+1})(\\mathrm{Norm}(\\pmb{x_l}))\n\\end{aligned}$$\n\n $l$  $l+1$  $l$   \n\npre-norm $l$ post-norm  \n\nnormalizationpre-norm--  \n\n  \n\npost-normlossnormpost-normpre-norm  \n\n## warmup  \n\nOn Layer Normalization in the Transformer Architecturepre-normpost-normtransformerwarmup  \n\nwarmup  \n\npre-normtransformerwarmuppost-norm+warmuppost-normwarmup  \n\n{% asset_img warmup_effect.png warmup %}  \n\n## Deepnorm  \n\n2022DeepNet: Scaling Transformers to 1,000 Layerstransformer  \n\nDeepnorm  \n\n{% asset_img deepnorm.png deepnorm %}  \n\n $\\alpha>1$ post-norm  \n\n $\\beta$  $G_{l}$   \n\ndeepnormpre-normpost-norm  \n\n{% asset_img deepnorm_result.png deepnorm result %}  \n\npost-norm  \n\n## Realformer--residual attention  \n\npost-normpre-normRealFormer: Transformer Likes Residual Attention  \n\n{% asset_img realformer.png realformer %}  \n\nRealFormerTransformerSoftmax\n\n\n\n{% asset_img realformer_attention.png realformer attention %}  \n\n $Prev'$ softmax $\\frac{Q^{\\prime}K^{\\prime T}}{\\sqrt{d_k}}+Prev'$ attention  \n\n# \n\nnormalizationbatchnormlayernormtransformer\n\nrmsnorm + prenorm  \n\nnormalization    \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n***\n\n# Reference  \n1https://www.zhihu.com/question/487766088  \n2Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization https://arxiv.org/abs/2001.06838  \n3Transformer()& https://zhuanlan.zhihu.com/p/476102712  \n4Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://arxiv.org/abs/1502.03167  \n5How Does Batch Normalization Help Optimization? https://arxiv.org/abs/1805.11604  \n6Batch Normalization: Accelerating Deep Network https://zhuanlan.zhihu.com/p/340856414  \n7Layer Normalization https://arxiv.org/abs/1607.06450  \n8NormalizationBN/LN/WN https://zhuanlan.zhihu.com/p/33173246  \n9Transformer()BatchNormalization https://zhuanlan.zhihu.com/p/481277619  \n10Layer Normalization https://arxiv.org/abs/1607.06450  \n11PowerNorm: Rethinking Batch Normalization in Transformers https://arxiv.org/abs/2003.07845  \n12Root Mean Square Layer Normalization https://arxiv.org/abs/1910.07467  \n13On Layer Normalization in the Transformer Architecture https://arxiv.org/abs/2002.04745  \n14Pre NormPost Norm https://spaces.ac.cn/archives/9009  \n15Understanding the Difficulty of Training Transformers https://arxiv.org/abs/2004.08249  \n16RealFormer: Transformer Likes Residual Attention https://arxiv.org/abs/2012.11747  \n17DeepNet: Scaling Transformers to 1,000 Layers https://arxiv.org/abs/2203.00555  \n\n","source":"_posts/cs/nlp/2024/03/Transformernormalization.md","raw":"---\ntitle: transformernormalization\nabbrlink: 6a40bfa5\ndate: 2024-03-19 21:06:12\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - layernorm\n  - post-norm\n  - pre-norm\n  - normalization\n  - batchnorm\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nNormalizationattention  \n\nnormalizationtransformer  \n\n  \n\n# why normalization\n\nnormalization  \n\n  \n\n## normalization\n\n $Loss(x_1,x_2)=x_1^2+x_2^2+b$   \n\n{% asset_img lossfunc_surface.jpeg loss function surface %}  \n\n  \n\n  \n\nminimum  \n\n  \n\n{% asset_img ellipse_1.png ellipse %}  \n\n  \n\n  \n\n{% asset_img ellipse_2.png ellipse %}  \n\n  \n\n  \n\n $x_{1}$ $x_{2}$->  \n\n0.x~2.x0  \n\n  \n\n $x_{2}$   \n\n  \n\n  \n\nnormalization  \n\nnormalization  \n$$x^{\\prime}=\\frac{x-\\min(\\mathrm{x})}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}$$  \n\n$$x^{\\prime}=\\frac{x-\\mu}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}$$  \n\n$$x^{\\prime}=\\frac{x-\\mu}\\sigma$$  \n\n $\\mu$ $\\sigma$   \n\nZ-score normalization  \n\n  \n\nPCA  \n\n## ICS...\n\ni.i.d.independent and identical distribution  \n\ni.i.d.  \n\n  \n\n  \n\n  \n\ni.i.d.i.i.d.  \n\nPCAi.i.d.  \n\n  \n\ni.i.d.  \n\nICSinternal covariate shift  \n\n  \n\nnormalization  \n\nnormalizationbatchnormICSbatchnorm  \n\nICS  \n\n## \n\nsigmoid\n\n{% asset_img sigmoid.png sigmoid %}  \n\n > 6  < -6 sigmoid  \n\n  \n\nICSnormalization  \n\nnormalization  \n\n# batchnorm\n\nnormalizationbatchnormlayernorm  \n\n## batchnorm\n\n $[B,C]$  $B$ batch size$C$   \n\n $C$ normalization $C$   \n\n $i$ batch  \n\n$$\n\\mu_{i}=\\frac{1}{B}\\sum_{j=1}^{B}x_{i,j}\n$$  \n\n  \n\n$$\n\\sigma_{i}^{2}=\\frac{1}{B}\\sum_{j=1}^{B}(x_{i,j}-\\mu_{i})^2\n$$  \n\nbatchZ-score normalization  \n\n$$\nx_{i,j}'=\\frac{x_{i,j}-\\mu_{i}}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\n$$  \n\n $\\epsilon$ 0  \n\n $C$ 01  \n\n  \n\nsigmoid  \n\n  \n\n  $\\gamma$  $\\beta$   \n\n$$\ny_{i,j} = \\gamma_{i} x_{i,j}' + \\beta_{i}\n$$  \n\n  \n\nbatchnorm $\\gamma$  $\\beta$ \n\n[Batch Normalization: Accelerating Deep Network](https://zhuanlan.zhihu.com/p/340856414)  \n\n{% asset_img bn_algo.png batch norm %}  \n\n## CNNbatchnorm  \n\nbatchnormCNN  \n\nCNNfeature mapsize $[B,C,H,W]$  $B$ batch size$C$ channel$H$  $W$   \n\nbatchnorm $C\\times H\\times W$  $B$  $B$   \n\nCNNchannel $H\\times W$  $H\\times W$ batch  \n\nbatch $B\\times H\\times W$  $C$  $C$  $\\gamma$  $\\beta$   \n\nbatchnormbatchnormrelufcbatchnormfcbias  \n\nbtwbatchnorm $\\gamma$ 1 $\\beta$ 0batchnorm  \n\n##   \n\nbatchnormmini-batch  \n\n$\\gamma$  $\\beta$   \n\nsamplesamplebatch  \n\ni.i.d.  \n\n\n\nmoving_mean = momentum  moving_mean + (1.0  momentum)  mean  \n\nmoving_var = momentum  moving_var + (1.0  momentum)  var  \n\nbatch  \n\nmomentum TF/Keras 0.99 Pytorch 0.9  \n\nmomentum  \n \nmomentum  \n\nbatch sizemini batchmomentum  \n\nbatch\n\nbatch sizemini batchbatchnormbatch size  \n\n{% asset_img bs_bn.png batch size %}  \n\nbatchnorm    \n\nbatch\n\nmomentumbatch sizebatch size\n\n## batchnorm\n\nbatchnormbatchnormICS2018How Does Batch Normalization Help Optimization?  \n\nbatchnormICSICSbatchnormcovariate shiftbatchnorm  \n\n{% asset_img bn_ics.png ICS %}  \n\nICSbatchnormICS  \n\nICSbatchnorm  \n\nbatchnormICS  \n\nICSICS  \n\nICS\n\n{% asset_img ics_define.png ICS  %}  \n\niL2\n\n$G_{t,i}$ t  \n\n$G_{t,i}'$ t  \n\n  \n\nICS\n\n{% asset_img ics_measure.png ICS measure %}  \n\nbatchnorm  \n\nbatchnormICS  \n\nbatchnorm  \n\nbatchnorm  \n\n# layernorm\n\nbatchnormlayernorm  \n\n## layernorm\n\nlayernormlayerlayer  \n\nbatchnormbatchlayernorm  \n\n{% asset_img bn_and_ln.png bnln %}  \n\nNLPbatchnormlayernormbatchnormlayernorm  \n\nlayernormNLPRNNtransfomrer  \n\ntransformer $[B,S,H]$ $S$ paddingzero-paddingbatch  \n\n\n\n```\n      \n  \n    \n```\n\npad\n\n```\n      \n    [P] [P]\n      [P]\n```\n\npaddingbatchnorm  \n\n[P] [P] batch size2 [P] normalization  \n\nbatch  \n\n [P] token  \n\nlayernorm $H$ normalization $H$ $\\gamma$  $\\beta$   \n\ntoken  \n\n## transformerlayernorm  \n\nbatchnormlayernormbatch  \n\nlayernormbatchbatch size  \n\nnlplayernormbatchnorm  \n\nPowerNorm: Rethinking Batch Normalization in TransformerstransformerBN  \n\nbatchbatchrunning statisticsNLPIWSLT14batchrunning statisticsCV  \n\nmagnitudeCV  \n\ntransformerBNCVNLPNLPbatch  \n\nlayernormNLPbatchnorm  \n\n## RMSnorm\n\n19Root Mean Square Layer NormalizationnormalizationRMSnormlayernorm  \n\nRMSnormlayernorm  \n\n{% asset_img rmsnorm.png RMSnorm %}  \n\nlayernorm  \n\n{% asset_img rmsnorm_eff.png RMSnorm %}  \n\nGRUlayernormlayernormlayernorm  \n\nlayernorm  \n\npRMSnormp%  \n\n{% asset_img prmsnorm.png prmsnorm %}  \n\nRMSnorm  \n\n# post-norm & pre-norm\n\n## \nlayernorm  \n\ntransformerpost-normOn Layer Normalization in the Transformer Architecturepre-norm  \n\npost-normpre-norm\n\n{% asset_img postnorm_prenorm.png postnorm and prenorm %}  \n\npost-normpre-norm  \n\npost-normpre-normpre-normpost-normpre-norm  \n\n $l$  $l+1$ post-norm  \n\n$$\nx_{l+1}=\\mathrm{Norm}(x_l+\\mathrm{F}_t(x_l))\n$$\n\npre-norm  \n\n$$\nx_{l+1}=x_l+\\mathrm{F}_l(\\mathrm{Norm}(x_l))\n$$\n\nPre NormPost Norm $\\mathrm{F}_l(\\mathrm{Norm}(x_l))$ norm  \n\n $l$ $x_{l}x_{l+1}$  $\\mathrm{F}_l(\\mathrm{Norm}(x_l))$  $\\mathrm{F}_{l+1}(\\mathrm{Norm}(x_{l+1}))$   \n\n$$\\begin{aligned}\n&\\mathrm{F}_l(\\operatorname{Norm}(x_l))+\\operatorname{F}_{l+1}(\\operatorname{Norm}(x_{l+1})) \\\\\n&{\\approx}\\mathrm{F}_l(\\mathrm{Norm}(x_l))+\\mathrm{F}_{l+1}\\left(\\mathrm{Norm}(x_l)\\right) \\\\\n&=(\\mathrm{F}_l\\oplus\\mathrm{F}_{l+1})(\\mathrm{Norm}(\\pmb{x_l}))\n\\end{aligned}$$\n\n $l$  $l+1$  $l$   \n\npre-norm $l$ post-norm  \n\nnormalizationpre-norm--  \n\n  \n\npost-normlossnormpost-normpre-norm  \n\n## warmup  \n\nOn Layer Normalization in the Transformer Architecturepre-normpost-normtransformerwarmup  \n\nwarmup  \n\npre-normtransformerwarmuppost-norm+warmuppost-normwarmup  \n\n{% asset_img warmup_effect.png warmup %}  \n\n## Deepnorm  \n\n2022DeepNet: Scaling Transformers to 1,000 Layerstransformer  \n\nDeepnorm  \n\n{% asset_img deepnorm.png deepnorm %}  \n\n $\\alpha>1$ post-norm  \n\n $\\beta$  $G_{l}$   \n\ndeepnormpre-normpost-norm  \n\n{% asset_img deepnorm_result.png deepnorm result %}  \n\npost-norm  \n\n## Realformer--residual attention  \n\npost-normpre-normRealFormer: Transformer Likes Residual Attention  \n\n{% asset_img realformer.png realformer %}  \n\nRealFormerTransformerSoftmax\n\n\n\n{% asset_img realformer_attention.png realformer attention %}  \n\n $Prev'$ softmax $\\frac{Q^{\\prime}K^{\\prime T}}{\\sqrt{d_k}}+Prev'$ attention  \n\n# \n\nnormalizationbatchnormlayernormtransformer\n\nrmsnorm + prenorm  \n\nnormalization    \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n***\n\n# Reference  \n1https://www.zhihu.com/question/487766088  \n2Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization https://arxiv.org/abs/2001.06838  \n3Transformer()& https://zhuanlan.zhihu.com/p/476102712  \n4Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://arxiv.org/abs/1502.03167  \n5How Does Batch Normalization Help Optimization? https://arxiv.org/abs/1805.11604  \n6Batch Normalization: Accelerating Deep Network https://zhuanlan.zhihu.com/p/340856414  \n7Layer Normalization https://arxiv.org/abs/1607.06450  \n8NormalizationBN/LN/WN https://zhuanlan.zhihu.com/p/33173246  \n9Transformer()BatchNormalization https://zhuanlan.zhihu.com/p/481277619  \n10Layer Normalization https://arxiv.org/abs/1607.06450  \n11PowerNorm: Rethinking Batch Normalization in Transformers https://arxiv.org/abs/2003.07845  \n12Root Mean Square Layer Normalization https://arxiv.org/abs/1910.07467  \n13On Layer Normalization in the Transformer Architecture https://arxiv.org/abs/2002.04745  \n14Pre NormPost Norm https://spaces.ac.cn/archives/9009  \n15Understanding the Difficulty of Training Transformers https://arxiv.org/abs/2004.08249  \n16RealFormer: Transformer Likes Residual Attention https://arxiv.org/abs/2012.11747  \n17DeepNet: Scaling Transformers to 1,000 Layers https://arxiv.org/abs/2203.00555  \n\n","slug":"cs/nlp/2024/03/Transformernormalization","published":1,"updated":"2024-04-07T06:36:14.847Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmfj000vam4k7ll2hgro","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>Normalizationattention</p>\n<p>normalizationtransformer</p>\n<p></p>\n<h1 id=\"why-normalization\">why normalization</h1>\n<p>normalization</p>\n<p></p>\n<h2 id=\"normalization\">normalization</h2>\n<p> <span class=\"math inline\">\\(Loss(x_1,x_2)=x_1^2+x_2^2+b\\)</span>\n</p>\n<img src=\"/6a40bfa5/lossfunc_surface.jpeg\" class title=\"loss function surface\">\n<p></p>\n<p></p>\n<p>minimum</p>\n<p></p>\n<img src=\"/6a40bfa5/ellipse_1.png\" class title=\"ellipse\">\n<p></p>\n<p></p>\n<img src=\"/6a40bfa5/ellipse_2.png\" class title=\"ellipse\">\n<p></p>\n<p></p>\n<p> <span class=\"math inline\">\\(x_{1}\\)</span> <span class=\"math inline\">\\(x_{2}\\)</span>-&gt;</p>\n<p>0.x~2.x0</p>\n<p></p>\n<p> <span class=\"math inline\">\\(x_{2}\\)</span>\n</p>\n<p></p>\n<p></p>\n<p>normalization</p>\n<p>normalization<br>\n<span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\min(\\mathrm{x})}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}\\]</span></p>\n<p><span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\mu}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}\\]</span></p>\n<p><span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\mu}\\sigma\\]</span></p>\n<p> <span class=\"math inline\">\\(\\mu\\)</span> <span class=\"math inline\">\\(\\sigma\\)</span> </p>\n<p>Z-score\nnormalization</p>\n<p></p>\n<p>PCA</p>\n<h2 id=\"ics...\">ICS...</h2>\n<p>i.i.d.independent and identical\ndistribution</p>\n<p>i.i.d.</p>\n<p></p>\n<p></p>\n<p></p>\n<p>i.i.d.i.i.d.</p>\n<p>PCAi.i.d.</p>\n<p></p>\n<p>i.i.d.</p>\n<p>ICSinternal covariate shift</p>\n<p></p>\n<p>normalization</p>\n<p>normalizationbatchnormICSbatchnorm</p>\n<p>ICS</p>\n<h2 id=\"\"></h2>\n<p>sigmoid</p>\n<img src=\"/6a40bfa5/sigmoid.png\" class title=\"sigmoid\">\n<p> &gt; 6  &lt; -6\nsigmoid</p>\n<p></p>\n<p>ICSnormalization</p>\n<p>normalization</p>\n<h1 id=\"batchnorm\">batchnorm</h1>\n<p>normalizationbatchnormlayernorm</p>\n<h2 id=\"batchnorm\">batchnorm</h2>\n<p> <span class=\"math inline\">\\([B,C]\\)</span>\n <span class=\"math inline\">\\(B\\)</span> batch size<span class=\"math inline\">\\(C\\)</span> </p>\n<p> <span class=\"math inline\">\\(C\\)</span>\nnormalization\n<span class=\"math inline\">\\(C\\)</span> </p>\n<p> <span class=\"math inline\">\\(i\\)</span>\nbatch</p>\n<p><span class=\"math display\">\\[\n\\mu_{i}=\\frac{1}{B}\\sum_{j=1}^{B}x_{i,j}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\sigma_{i}^{2}=\\frac{1}{B}\\sum_{j=1}^{B}(x_{i,j}-\\mu_{i})^2\n\\]</span></p>\n<p>batchZ-score\nnormalization</p>\n<p><span class=\"math display\">\\[\nx_{i,j}&#39;=\\frac{x_{i,j}-\\mu_{i}}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\epsilon\\)</span>\n0</p>\n<p> <span class=\"math inline\">\\(C\\)</span>\n01</p>\n<p></p>\n<p>sigmoid</p>\n<p></p>\n<p>\n <span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p><span class=\"math display\">\\[\ny_{i,j} = \\gamma_{i} x_{i,j}&#39; + \\beta_{i}\n\\]</span></p>\n<p></p>\n<p>batchnorm\n<span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span>\n</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/340856414\">Batch\nNormalization: Accelerating Deep Network</a></p>\n<img src=\"/6a40bfa5/bn_algo.png\" class title=\"batch norm\">\n<h2 id=\"cnnbatchnorm\">CNNbatchnorm</h2>\n<p>batchnormCNN</p>\n<p>CNNfeature mapsize <span class=\"math inline\">\\([B,C,H,W]\\)</span>  <span class=\"math inline\">\\(B\\)</span> batch size<span class=\"math inline\">\\(C\\)</span> channel<span class=\"math inline\">\\(H\\)</span>  <span class=\"math inline\">\\(W\\)</span> </p>\n<p>batchnorm <span class=\"math inline\">\\(C\\times H\\times W\\)</span> \n<span class=\"math inline\">\\(B\\)</span>  <span class=\"math inline\">\\(B\\)</span> </p>\n<p>CNNchannel <span class=\"math inline\">\\(H\\times W\\)</span>\n <span class=\"math inline\">\\(H\\times W\\)</span>\nbatch</p>\n<p>batch <span class=\"math inline\">\\(B\\times H\\times W\\)</span>  <span class=\"math inline\">\\(C\\)</span>  <span class=\"math inline\">\\(C\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p>batchnormbatchnormrelufcbatchnormfcbias</p>\n<p>btwbatchnorm <span class=\"math inline\">\\(\\gamma\\)</span> 1 <span class=\"math inline\">\\(\\beta\\)</span>\n0batchnorm</p>\n<h2 id=\"\"></h2>\n<p>batchnormmini-batch</p>\n<p><span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span>\n</p>\n<p>samplesamplebatch</p>\n<p>i.i.d.</p>\n<p></p>\n<p>moving_mean = momentum  moving_mean + (1.0  momentum)  mean</p>\n<p>moving_var = momentum  moving_var + (1.0  momentum)  var</p>\n<p>batch</p>\n<p>momentum TF/Keras 0.99 Pytorch\n0.9</p>\n<p>momentum</p>\n<p>momentum</p>\n<p>batch sizemini\nbatchmomentum</p>\n<p>batch</p>\n<p>batch sizemini\nbatchbatchnormbatch\nsize</p>\n<img src=\"/6a40bfa5/bs_bn.png\" class title=\"batch size\">\n<p>batchnorm</p>\n<p>batch</p>\n<p>momentumbatch\nsizebatch size</p>\n<h2 id=\"batchnorm\">batchnorm</h2>\n<p>batchnormbatchnormICS2018How\nDoes Batch Normalization Help Optimization?</p>\n<p>batchnormICSICSbatchnormcovariate\nshiftbatchnorm</p>\n<img src=\"/6a40bfa5/bn_ics.png\" class title=\"ICS\">\n<p>ICSbatchnormICS</p>\n<p>ICSbatchnorm</p>\n<p>batchnormICS</p>\n<p>ICSICS</p>\n<p>ICS</p>\n<img src=\"/6a40bfa5/ics_define.png\" class title=\"ICS \">\n<p>iL2</p>\n<p><span class=\"math inline\">\\(G_{t,i}\\)</span>\nt</p>\n<p><span class=\"math inline\">\\(G_{t,i}&#39;\\)</span>\nt</p>\n<p></p>\n<p>ICS</p>\n<img src=\"/6a40bfa5/ics_measure.png\" class title=\"ICS measure\">\n<p>batchnorm</p>\n<p>batchnormICS</p>\n<p>batchnorm</p>\n<p>batchnorm</p>\n<h1 id=\"layernorm\">layernorm</h1>\n<p>batchnormlayernorm</p>\n<h2 id=\"layernorm\">layernorm</h2>\n<p>layernormlayerlayer</p>\n<p>batchnormbatchlayernorm</p>\n<img src=\"/6a40bfa5/bn_and_ln.png\" class title=\"bnln\">\n<p>NLPbatchnormlayernormbatchnormlayernorm</p>\n<p>layernormNLPRNNtransfomrer</p>\n<p>transformer <span class=\"math inline\">\\([B,S,H]\\)</span> <span class=\"math inline\">\\(S\\)</span>\npaddingzero-paddingbatch</p>\n<p></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">  </span><br><span class=\"line\">    </span><br></pre></td></tr></table></figure>\n<p>pad</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">    [P] [P]</span><br><span class=\"line\">      [P]</span><br></pre></td></tr></table></figure>\n<p>paddingbatchnorm</p>\n<p>[P] [P]\nbatch size2 [P]\nnormalization</p>\n<p>batch</p>\n<p> [P]\ntoken</p>\n<p>layernorm <span class=\"math inline\">\\(H\\)</span>\nnormalization <span class=\"math inline\">\\(H\\)</span>\n<span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p>token</p>\n<h2 id=\"transformerlayernorm\">transformerlayernorm</h2>\n<p>batchnormlayernormbatch</p>\n<p>layernormbatchbatch size</p>\n<p>nlplayernormbatchnorm</p>\n<p>PowerNorm: Rethinking Batch Normalization in\nTransformerstransformerBN</p>\n<p>batchbatchrunning\nstatisticsNLPIWSLT14batchrunning\nstatisticsCV</p>\n<p>magnitudeCV</p>\n<p>transformerBNCVNLPNLPbatch</p>\n<p>layernormNLPbatchnorm</p>\n<h2 id=\"rmsnorm\">RMSnorm</h2>\n<p>19Root Mean Square Layer\nNormalizationnormalizationRMSnormlayernorm</p>\n<p>RMSnormlayernorm</p>\n<img src=\"/6a40bfa5/rmsnorm.png\" class title=\"RMSnorm\">\n<p>layernorm</p>\n<img src=\"/6a40bfa5/rmsnorm_eff.png\" class title=\"RMSnorm\">\n<p>GRUlayernormlayernormlayernorm</p>\n<p>layernorm</p>\n<p>pRMSnormp%</p>\n<img src=\"/6a40bfa5/prmsnorm.png\" class title=\"prmsnorm\">\n<p>RMSnorm</p>\n<h1 id=\"post-norm-pre-norm\">post-norm &amp; pre-norm</h1>\n<h2 id=\"\"></h2>\n<p>layernorm</p>\n<p>transformerpost-normOn Layer Normalization in\nthe Transformer Architecturepre-norm</p>\n<p>post-normpre-norm</p>\n<img src=\"/6a40bfa5/postnorm_prenorm.png\" class title=\"postnorm and prenorm\">\n<p>post-normpre-norm</p>\n<p>post-normpre-normpre-normpost-normpre-norm</p>\n<p> <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(l+1\\)</span> post-norm</p>\n<p><span class=\"math display\">\\[\nx_{l+1}=\\mathrm{Norm}(x_l+\\mathrm{F}_t(x_l))\n\\]</span></p>\n<p>pre-norm</p>\n<p><span class=\"math display\">\\[\nx_{l+1}=x_l+\\mathrm{F}_l(\\mathrm{Norm}(x_l))\n\\]</span></p>\n<p>Pre NormPost Norm\n<span class=\"math inline\">\\(\\mathrm{F}_l(\\mathrm{Norm}(x_l))\\)</span>\nnorm</p>\n<p> <span class=\"math inline\">\\(l\\)</span> <span class=\"math inline\">\\(x_{l}x_{l+1}\\)</span>  <span class=\"math inline\">\\(\\mathrm{F}_l(\\mathrm{Norm}(x_l))\\)</span>  <span class=\"math inline\">\\(\\mathrm{F}_{l+1}(\\mathrm{Norm}(x_{l+1}))\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\mathrm{F}_l(\\operatorname{Norm}(x_l))+\\operatorname{F}_{l+1}(\\operatorname{Norm}(x_{l+1}))\n\\\\\n&amp;{\\approx}\\mathrm{F}_l(\\mathrm{Norm}(x_l))+\\mathrm{F}_{l+1}\\left(\\mathrm{Norm}(x_l)\\right)\n\\\\\n&amp;=(\\mathrm{F}_l\\oplus\\mathrm{F}_{l+1})(\\mathrm{Norm}(\\pmb{x_l}))\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(l+1\\)</span>  <span class=\"math inline\">\\(l\\)</span> </p>\n<p>pre-norm <span class=\"math inline\">\\(l\\)</span>\npost-norm</p>\n<p>normalizationpre-norm--</p>\n<p></p>\n<p>post-normlossnormpost-normpre-norm</p>\n<h2 id=\"warmup\">warmup</h2>\n<p>On Layer Normalization in the Transformer\nArchitecturepre-normpost-normtransformerwarmup</p>\n<p>warmup</p>\n<p>pre-normtransformerwarmuppost-norm+warmuppost-normwarmup</p>\n<img src=\"/6a40bfa5/warmup_effect.png\" class title=\"warmup\">\n<h2 id=\"deepnorm\">Deepnorm</h2>\n<p>2022DeepNet: Scaling Transformers to 1,000\nLayerstransformer</p>\n<p>Deepnorm</p>\n<img src=\"/6a40bfa5/deepnorm.png\" class title=\"deepnorm\">\n<p> <span class=\"math inline\">\\(\\alpha&gt;1\\)</span>\npost-norm</p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span> \n<span class=\"math inline\">\\(G_{l}\\)</span>\n</p>\n<p>deepnormpre-normpost-norm</p>\n<img src=\"/6a40bfa5/deepnorm_result.png\" class title=\"deepnorm result\">\n<p>post-norm</p>\n<h2 id=\"realformer--residual-attention\">Realformer--residual\nattention</h2>\n<p>post-normpre-normRealFormer:\nTransformer Likes Residual Attention</p>\n<img src=\"/6a40bfa5/realformer.png\" class title=\"realformer\">\n<p>RealFormerTransformerSoftmax</p>\n<p></p>\n<img src=\"/6a40bfa5/realformer_attention.png\" class title=\"realformer attention\">\n<p> <span class=\"math inline\">\\(Prev&#39;\\)</span>\nsoftmax\n<span class=\"math inline\">\\(\\frac{Q^{\\prime}K^{\\prime\nT}}{\\sqrt{d_k}}+Prev&#39;\\)</span> attention</p>\n<h1 id=\"\"></h1>\n<p>normalizationbatchnormlayernormtransformer</p>\n<p>rmsnorm + prenorm</p>\n<p>normalization</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1https://www.zhihu.com/question/487766088<br>\n2Towards Stabilizing Batch Statistics in Backward Propagation of\nBatch Normalization https://arxiv.org/abs/2001.06838<br>\n3Transformer()&amp;\nhttps://zhuanlan.zhihu.com/p/476102712<br>\n4Batch Normalization: Accelerating Deep Network Training by Reducing\nInternal Covariate Shift https://arxiv.org/abs/1502.03167<br>\n5How Does Batch Normalization Help Optimization?\nhttps://arxiv.org/abs/1805.11604<br>\n6Batch Normalization: Accelerating Deep Network\nhttps://zhuanlan.zhihu.com/p/340856414<br>\n7Layer Normalization https://arxiv.org/abs/1607.06450<br>\n8NormalizationBN/LN/WN\nhttps://zhuanlan.zhihu.com/p/33173246<br>\n9Transformer()BatchNormalization\nhttps://zhuanlan.zhihu.com/p/481277619<br>\n10Layer Normalization https://arxiv.org/abs/1607.06450<br>\n11PowerNorm: Rethinking Batch Normalization in Transformers\nhttps://arxiv.org/abs/2003.07845<br>\n12Root Mean Square Layer Normalization\nhttps://arxiv.org/abs/1910.07467<br>\n13On Layer Normalization in the Transformer Architecture\nhttps://arxiv.org/abs/2002.04745<br>\n14Pre NormPost Norm\nhttps://spaces.ac.cn/archives/9009<br>\n15Understanding the Difficulty of Training Transformers\nhttps://arxiv.org/abs/2004.08249<br>\n16RealFormer: Transformer Likes Residual Attention\nhttps://arxiv.org/abs/2012.11747<br>\n17DeepNet: Scaling Transformers to 1,000 Layers\nhttps://arxiv.org/abs/2203.00555</p>\n","length":11986,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>Normalizationattention</p>\n<p>normalizationtransformer</p>\n<p></p>\n<h1 id=\"why-normalization\">why normalization</h1>\n<p>normalization</p>\n<p></p>\n<h2 id=\"normalization\">normalization</h2>\n<p> <span class=\"math inline\">\\(Loss(x_1,x_2)=x_1^2+x_2^2+b\\)</span>\n</p>\n<img src=\"/6a40bfa5/lossfunc_surface.jpeg\" class title=\"loss function surface\">\n<p></p>\n<p></p>\n<p>minimum</p>\n<p></p>\n<img src=\"/6a40bfa5/ellipse_1.png\" class title=\"ellipse\">\n<p></p>\n<p></p>\n<img src=\"/6a40bfa5/ellipse_2.png\" class title=\"ellipse\">\n<p></p>\n<p></p>\n<p> <span class=\"math inline\">\\(x_{1}\\)</span> <span class=\"math inline\">\\(x_{2}\\)</span>-&gt;</p>\n<p>0.x~2.x0</p>\n<p></p>\n<p> <span class=\"math inline\">\\(x_{2}\\)</span>\n</p>\n<p></p>\n<p></p>\n<p>normalization</p>\n<p>normalization<br>\n<span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\min(\\mathrm{x})}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}\\]</span></p>\n<p><span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\mu}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}\\]</span></p>\n<p><span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\mu}\\sigma\\]</span></p>\n<p> <span class=\"math inline\">\\(\\mu\\)</span> <span class=\"math inline\">\\(\\sigma\\)</span> </p>\n<p>Z-score\nnormalization</p>\n<p></p>\n<p>PCA</p>\n<h2 id=\"ics...\">ICS...</h2>\n<p>i.i.d.independent and identical\ndistribution</p>\n<p>i.i.d.</p>\n<p></p>\n<p></p>\n<p></p>\n<p>i.i.d.i.i.d.</p>\n<p>PCAi.i.d.</p>\n<p></p>\n<p>i.i.d.</p>\n<p>ICSinternal covariate shift</p>\n<p></p>\n<p>normalization</p>\n<p>normalizationbatchnormICSbatchnorm</p>\n<p>ICS</p>\n<h2 id=\"\"></h2>\n<p>sigmoid</p>\n<img src=\"/6a40bfa5/sigmoid.png\" class title=\"sigmoid\">\n<p> &gt; 6  &lt; -6\nsigmoid</p>\n<p></p>\n<p>ICSnormalization</p>\n<p>normalization</p>\n<h1 id=\"batchnorm\">batchnorm</h1>\n<p>normalizationbatchnormlayernorm</p>\n<h2 id=\"batchnorm\">batchnorm</h2>\n<p> <span class=\"math inline\">\\([B,C]\\)</span>\n <span class=\"math inline\">\\(B\\)</span> batch size<span class=\"math inline\">\\(C\\)</span> </p>\n<p> <span class=\"math inline\">\\(C\\)</span>\nnormalization\n<span class=\"math inline\">\\(C\\)</span> </p>\n<p> <span class=\"math inline\">\\(i\\)</span>\nbatch</p>\n<p><span class=\"math display\">\\[\n\\mu_{i}=\\frac{1}{B}\\sum_{j=1}^{B}x_{i,j}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\sigma_{i}^{2}=\\frac{1}{B}\\sum_{j=1}^{B}(x_{i,j}-\\mu_{i})^2\n\\]</span></p>\n<p>batchZ-score\nnormalization</p>\n<p><span class=\"math display\">\\[\nx_{i,j}&#39;=\\frac{x_{i,j}-\\mu_{i}}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\epsilon\\)</span>\n0</p>\n<p> <span class=\"math inline\">\\(C\\)</span>\n01</p>\n<p></p>\n<p>sigmoid</p>\n<p></p>\n<p>\n <span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p><span class=\"math display\">\\[\ny_{i,j} = \\gamma_{i} x_{i,j}&#39; + \\beta_{i}\n\\]</span></p>\n<p></p>\n<p>batchnorm\n<span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span>\n</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/340856414\">Batch\nNormalization: Accelerating Deep Network</a></p>\n<img src=\"/6a40bfa5/bn_algo.png\" class title=\"batch norm\">\n<h2 id=\"cnnbatchnorm\">CNNbatchnorm</h2>\n<p>batchnormCNN</p>\n<p>CNNfeature mapsize <span class=\"math inline\">\\([B,C,H,W]\\)</span>  <span class=\"math inline\">\\(B\\)</span> batch size<span class=\"math inline\">\\(C\\)</span> channel<span class=\"math inline\">\\(H\\)</span>  <span class=\"math inline\">\\(W\\)</span> </p>\n<p>batchnorm <span class=\"math inline\">\\(C\\times H\\times W\\)</span> \n<span class=\"math inline\">\\(B\\)</span>  <span class=\"math inline\">\\(B\\)</span> </p>\n<p>CNNchannel <span class=\"math inline\">\\(H\\times W\\)</span>\n <span class=\"math inline\">\\(H\\times W\\)</span>\nbatch</p>\n<p>batch <span class=\"math inline\">\\(B\\times H\\times W\\)</span>  <span class=\"math inline\">\\(C\\)</span>  <span class=\"math inline\">\\(C\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p>batchnormbatchnormrelufcbatchnormfcbias</p>\n<p>btwbatchnorm <span class=\"math inline\">\\(\\gamma\\)</span> 1 <span class=\"math inline\">\\(\\beta\\)</span>\n0batchnorm</p>\n<h2 id=\"\"></h2>\n<p>batchnormmini-batch</p>\n<p><span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span>\n</p>\n<p>samplesamplebatch</p>\n<p>i.i.d.</p>\n<p></p>\n<p>moving_mean = momentum  moving_mean + (1.0  momentum)  mean</p>\n<p>moving_var = momentum  moving_var + (1.0  momentum)  var</p>\n<p>batch</p>\n<p>momentum TF/Keras 0.99 Pytorch\n0.9</p>\n<p>momentum</p>\n<p>momentum</p>\n<p>batch sizemini\nbatchmomentum</p>\n<p>batch</p>\n<p>batch sizemini\nbatchbatchnormbatch\nsize</p>\n<img src=\"/6a40bfa5/bs_bn.png\" class title=\"batch size\">\n<p>batchnorm</p>\n<p>batch</p>\n<p>momentumbatch\nsizebatch size</p>\n<h2 id=\"batchnorm\">batchnorm</h2>\n<p>batchnormbatchnormICS2018How\nDoes Batch Normalization Help Optimization?</p>\n<p>batchnormICSICSbatchnormcovariate\nshiftbatchnorm</p>\n<img src=\"/6a40bfa5/bn_ics.png\" class title=\"ICS\">\n<p>ICSbatchnormICS</p>\n<p>ICSbatchnorm</p>\n<p>batchnormICS</p>\n<p>ICSICS</p>\n<p>ICS</p>\n<img src=\"/6a40bfa5/ics_define.png\" class title=\"ICS \">\n<p>iL2</p>\n<p><span class=\"math inline\">\\(G_{t,i}\\)</span>\nt</p>\n<p><span class=\"math inline\">\\(G_{t,i}&#39;\\)</span>\nt</p>\n<p></p>\n<p>ICS</p>\n<img src=\"/6a40bfa5/ics_measure.png\" class title=\"ICS measure\">\n<p>batchnorm</p>\n<p>batchnormICS</p>\n<p>batchnorm</p>\n<p>batchnorm</p>\n<h1 id=\"layernorm\">layernorm</h1>\n<p>batchnormlayernorm</p>\n<h2 id=\"layernorm\">layernorm</h2>\n<p>layernormlayerlayer</p>\n<p>batchnormbatchlayernorm</p>\n<img src=\"/6a40bfa5/bn_and_ln.png\" class title=\"bnln\">\n<p>NLPbatchnormlayernormbatchnormlayernorm</p>\n<p>layernormNLPRNNtransfomrer</p>\n<p>transformer <span class=\"math inline\">\\([B,S,H]\\)</span> <span class=\"math inline\">\\(S\\)</span>\npaddingzero-paddingbatch</p>\n<p></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">  </span><br><span class=\"line\">    </span><br></pre></td></tr></table></figure>\n<p>pad</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">    [P] [P]</span><br><span class=\"line\">      [P]</span><br></pre></td></tr></table></figure>\n<p>paddingbatchnorm</p>\n<p>[P] [P]\nbatch size2 [P]\nnormalization</p>\n<p>batch</p>\n<p> [P]\ntoken</p>\n<p>layernorm <span class=\"math inline\">\\(H\\)</span>\nnormalization <span class=\"math inline\">\\(H\\)</span>\n<span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p>token</p>\n<h2 id=\"transformerlayernorm\">transformerlayernorm</h2>\n<p>batchnormlayernormbatch</p>\n<p>layernormbatchbatch size</p>\n<p>nlplayernormbatchnorm</p>\n<p>PowerNorm: Rethinking Batch Normalization in\nTransformerstransformerBN</p>\n<p>batchbatchrunning\nstatisticsNLPIWSLT14batchrunning\nstatisticsCV</p>\n<p>magnitudeCV</p>\n<p>transformerBNCVNLPNLPbatch</p>\n<p>layernormNLPbatchnorm</p>\n<h2 id=\"rmsnorm\">RMSnorm</h2>\n<p>19Root Mean Square Layer\nNormalizationnormalizationRMSnormlayernorm</p>\n<p>RMSnormlayernorm</p>\n<img src=\"/6a40bfa5/rmsnorm.png\" class title=\"RMSnorm\">\n<p>layernorm</p>\n<img src=\"/6a40bfa5/rmsnorm_eff.png\" class title=\"RMSnorm\">\n<p>GRUlayernormlayernormlayernorm</p>\n<p>layernorm</p>\n<p>pRMSnormp%</p>\n<img src=\"/6a40bfa5/prmsnorm.png\" class title=\"prmsnorm\">\n<p>RMSnorm</p>\n<h1 id=\"post-norm-pre-norm\">post-norm &amp; pre-norm</h1>\n<h2 id=\"\"></h2>\n<p>layernorm</p>\n<p>transformerpost-normOn Layer Normalization in\nthe Transformer Architecturepre-norm</p>\n<p>post-normpre-norm</p>\n<img src=\"/6a40bfa5/postnorm_prenorm.png\" class title=\"postnorm and prenorm\">\n<p>post-normpre-norm</p>\n<p>post-normpre-normpre-normpost-normpre-norm</p>\n<p> <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(l+1\\)</span> post-norm</p>\n<p><span class=\"math display\">\\[\nx_{l+1}=\\mathrm{Norm}(x_l+\\mathrm{F}_t(x_l))\n\\]</span></p>\n<p>pre-norm</p>\n<p><span class=\"math display\">\\[\nx_{l+1}=x_l+\\mathrm{F}_l(\\mathrm{Norm}(x_l))\n\\]</span></p>\n<p>Pre NormPost Norm\n<span class=\"math inline\">\\(\\mathrm{F}_l(\\mathrm{Norm}(x_l))\\)</span>\nnorm</p>\n<p> <span class=\"math inline\">\\(l\\)</span> <span class=\"math inline\">\\(x_{l}x_{l+1}\\)</span>  <span class=\"math inline\">\\(\\mathrm{F}_l(\\mathrm{Norm}(x_l))\\)</span>  <span class=\"math inline\">\\(\\mathrm{F}_{l+1}(\\mathrm{Norm}(x_{l+1}))\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\mathrm{F}_l(\\operatorname{Norm}(x_l))+\\operatorname{F}_{l+1}(\\operatorname{Norm}(x_{l+1}))\n\\\\\n&amp;{\\approx}\\mathrm{F}_l(\\mathrm{Norm}(x_l))+\\mathrm{F}_{l+1}\\left(\\mathrm{Norm}(x_l)\\right)\n\\\\\n&amp;=(\\mathrm{F}_l\\oplus\\mathrm{F}_{l+1})(\\mathrm{Norm}(\\pmb{x_l}))\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(l+1\\)</span>  <span class=\"math inline\">\\(l\\)</span> </p>\n<p>pre-norm <span class=\"math inline\">\\(l\\)</span>\npost-norm</p>\n<p>normalizationpre-norm--</p>\n<p></p>\n<p>post-normlossnormpost-normpre-norm</p>\n<h2 id=\"warmup\">warmup</h2>\n<p>On Layer Normalization in the Transformer\nArchitecturepre-normpost-normtransformerwarmup</p>\n<p>warmup</p>\n<p>pre-normtransformerwarmuppost-norm+warmuppost-normwarmup</p>\n<img src=\"/6a40bfa5/warmup_effect.png\" class title=\"warmup\">\n<h2 id=\"deepnorm\">Deepnorm</h2>\n<p>2022DeepNet: Scaling Transformers to 1,000\nLayerstransformer</p>\n<p>Deepnorm</p>\n<img src=\"/6a40bfa5/deepnorm.png\" class title=\"deepnorm\">\n<p> <span class=\"math inline\">\\(\\alpha&gt;1\\)</span>\npost-norm</p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span> \n<span class=\"math inline\">\\(G_{l}\\)</span>\n</p>\n<p>deepnormpre-normpost-norm</p>\n<img src=\"/6a40bfa5/deepnorm_result.png\" class title=\"deepnorm result\">\n<p>post-norm</p>\n<h2 id=\"realformer--residual-attention\">Realformer--residual\nattention</h2>\n<p>post-normpre-normRealFormer:\nTransformer Likes Residual Attention</p>\n<img src=\"/6a40bfa5/realformer.png\" class title=\"realformer\">\n<p>RealFormerTransformerSoftmax</p>\n<p></p>\n<img src=\"/6a40bfa5/realformer_attention.png\" class title=\"realformer attention\">\n<p> <span class=\"math inline\">\\(Prev&#39;\\)</span>\nsoftmax\n<span class=\"math inline\">\\(\\frac{Q^{\\prime}K^{\\prime\nT}}{\\sqrt{d_k}}+Prev&#39;\\)</span> attention</p>\n<h1 id=\"\"></h1>\n<p>normalizationbatchnormlayernormtransformer</p>\n<p>rmsnorm + prenorm</p>\n<p>normalization</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1https://www.zhihu.com/question/487766088<br>\n2Towards Stabilizing Batch Statistics in Backward Propagation of\nBatch Normalization https://arxiv.org/abs/2001.06838<br>\n3Transformer()&amp;\nhttps://zhuanlan.zhihu.com/p/476102712<br>\n4Batch Normalization: Accelerating Deep Network Training by Reducing\nInternal Covariate Shift https://arxiv.org/abs/1502.03167<br>\n5How Does Batch Normalization Help Optimization?\nhttps://arxiv.org/abs/1805.11604<br>\n6Batch Normalization: Accelerating Deep Network\nhttps://zhuanlan.zhihu.com/p/340856414<br>\n7Layer Normalization https://arxiv.org/abs/1607.06450<br>\n8NormalizationBN/LN/WN\nhttps://zhuanlan.zhihu.com/p/33173246<br>\n9Transformer()BatchNormalization\nhttps://zhuanlan.zhihu.com/p/481277619<br>\n10Layer Normalization https://arxiv.org/abs/1607.06450<br>\n11PowerNorm: Rethinking Batch Normalization in Transformers\nhttps://arxiv.org/abs/2003.07845<br>\n12Root Mean Square Layer Normalization\nhttps://arxiv.org/abs/1910.07467<br>\n13On Layer Normalization in the Transformer Architecture\nhttps://arxiv.org/abs/2002.04745<br>\n14Pre NormPost Norm\nhttps://spaces.ac.cn/archives/9009<br>\n15Understanding the Difficulty of Training Transformers\nhttps://arxiv.org/abs/2004.08249<br>\n16RealFormer: Transformer Likes Residual Attention\nhttps://arxiv.org/abs/2012.11747<br>\n17DeepNet: Scaling Transformers to 1,000 Layers\nhttps://arxiv.org/abs/2203.00555</p>\n"},{"title":"(1)","abbrlink":"3345028a","date":"2024-03-17T02:46:09.000Z","_content":"\n![](/images/cover.png)  \n\n\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n\n***\n\n//  \n\nLLM~~\n\n# 1Transformerscaled dot-product attentionQKd  \n\nsoftmaxsoftmaxattentionscalingd softmax\n\n# 2TransformerQK  \n\n1QK  \n\n2QK  \n\n3  \n\n# 3TransformerFFNFFNFFN  \n\n1SVM kernel  \n\n2  \n\n3  \n\n# 4MQA(Multi-Query Attention)GQA(Grouped-Query Attention)MHA(Multi-Head Attention)  \n\n1MQAGQAMHAMHAKV  \n\n2Decoder-onlycausal attentionKVMQAGQAKVKV  \n\n# 5LLMDecoder-only  \n\n1AttentionDecoder-only  \n\n2  \n\n3Causal AttentionAttentiontoken  \n\n4KV Cache  \n\n5  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***\n\n\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  ","source":"_posts/cs/nlp/2024/03/-1.md","raw":"---\ntitle: (1)\ntags:\n  - NLP\n  - LLM\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 3345028a\ndate: 2024-03-17 10:46:09\n---\n\n![](/images/cover.png)  \n\n\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n\n***\n\n//  \n\nLLM~~\n\n# 1Transformerscaled dot-product attentionQKd  \n\nsoftmaxsoftmaxattentionscalingd softmax\n\n# 2TransformerQK  \n\n1QK  \n\n2QK  \n\n3  \n\n# 3TransformerFFNFFNFFN  \n\n1SVM kernel  \n\n2  \n\n3  \n\n# 4MQA(Multi-Query Attention)GQA(Grouped-Query Attention)MHA(Multi-Head Attention)  \n\n1MQAGQAMHAMHAKV  \n\n2Decoder-onlycausal attentionKVMQAGQAKVKV  \n\n# 5LLMDecoder-only  \n\n1AttentionDecoder-only  \n\n2  \n\n3Causal AttentionAttentiontoken  \n\n4KV Cache  \n\n5  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***\n\n\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  ","slug":"cs/nlp/2024/03/-1","published":1,"updated":"2024-03-17T14:16:49.511Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmfj000xam4kg9aaagpd","content":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<hr>\n<p>//</p>\n<p>LLM<sub></sub></p>\n<h1 id=\"transformerscaled-dot-product-attentionqkd\">1Transformerscaled\ndot-product attentionQKd</h1>\n<p>softmaxsoftmaxattentionscalingd\nsoftmax</p>\n<h1 id=\"transformerqk\">2TransformerQK</h1>\n<p>1QK</p>\n<p>2QK</p>\n<p>3</p>\n<h1 id=\"transformerffnffnffn\">3TransformerFFNFFNFFN</h1>\n<p>1SVM\nkernel</p>\n<p>2</p>\n<p>3</p>\n<h1 id=\"mqamulti-query-attentiongqagrouped-query-attentionmhamulti-head-attention\">4MQA(Multi-Query\nAttention)GQA(Grouped-Query Attention)MHA(Multi-Head\nAttention)</h1>\n<p>1MQAGQAMHAMHAKV</p>\n<p>2Decoder-onlycausal\nattentionKVMQAGQAKVKV</p>\n<h1 id=\"llmdecoder-only\">5LLMDecoder-only</h1>\n<p>1AttentionDecoder-only</p>\n<p>2</p>\n<p>3Causal\nAttentionAttentiontoken</p>\n<p>4KV Cache</p>\n<p>5</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n","length":1412,"excerpt":"","more":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<hr>\n<p>//</p>\n<p>LLM<sub></sub></p>\n<h1 id=\"transformerscaled-dot-product-attentionqkd\">1Transformerscaled\ndot-product attentionQKd</h1>\n<p>softmaxsoftmaxattentionscalingd\nsoftmax</p>\n<h1 id=\"transformerqk\">2TransformerQK</h1>\n<p>1QK</p>\n<p>2QK</p>\n<p>3</p>\n<h1 id=\"transformerffnffnffn\">3TransformerFFNFFNFFN</h1>\n<p>1SVM\nkernel</p>\n<p>2</p>\n<p>3</p>\n<h1 id=\"mqamulti-query-attentiongqagrouped-query-attentionmhamulti-head-attention\">4MQA(Multi-Query\nAttention)GQA(Grouped-Query Attention)MHA(Multi-Head\nAttention)</h1>\n<p>1MQAGQAMHAMHAKV</p>\n<p>2Decoder-onlycausal\nattentionKVMQAGQAKVKV</p>\n<h1 id=\"llmdecoder-only\">5LLMDecoder-only</h1>\n<p>1AttentionDecoder-only</p>\n<p>2</p>\n<p>3Causal\nAttentionAttentiontoken</p>\n<p>4KV Cache</p>\n<p>5</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n"},{"title":"ChatGPT","abbrlink":"14e576c","date":"2023-03-17T02:36:53.000Z","_content":"\n{% asset_img 1.png page_1 %}  \n\n{% asset_img 2.png page_2 %}  \n\n{% asset_img 3.png page_3 %}  \n\n{% asset_img 4.png page_4 %}  \n\n{% asset_img 5.png page_5 %}  \n\n{% asset_img 6.png page_6 %}  \n\n{% asset_img 7.png page_7 %}  \n\n{% asset_img 8.png page_8 %}  \n\n{% asset_img 9.png page_9 %}  \n\n{% asset_img 10.png page_10 %}  \n\n{% asset_img 11.png page_11 %}  \n\n{% asset_img 12.png page_12 %}  \n\n{% asset_img 13.png page_13 %}  \n\n{% asset_img 14.png page_14 %}  \n\n{% asset_img 15.png page_15 %}  \n\n{% asset_img 16.png page_16 %}  \n\n{% asset_img 17.png page_17 %}  \n\n{% asset_img 18.png page_18 %}  \n\n{% asset_img 19.png page_19 %}  \n\n{% asset_img 20.png page_20 %}  \n\n{% asset_img 21.png page_21 %}  \n\n{% asset_img 22.png page_22 %}  \n\n{% asset_img 23.png page_23 %}  \n\n{% asset_img 24.png page_24 %}  \n\n{% asset_img 25.png page_25 %}  \n\n{% asset_img 26.png page_26 %}  \n\n{% asset_img 27.png page_27 %}  \n\n{% asset_img 28.png page_28 %}  ","source":"_posts/cs/nlp/2024/03/ChatGPT.md","raw":"---\ntitle: ChatGPT\ntags:\n  - NLP\n  - LLM\n  - ChatGPT\n  - Sparrow\n  - LaMDA\n  - GopherCite\n  - WebGPT\n  - InstructGPT\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 14e576c\ndate: 2023-03-17 10:36:53\n---\n\n{% asset_img 1.png page_1 %}  \n\n{% asset_img 2.png page_2 %}  \n\n{% asset_img 3.png page_3 %}  \n\n{% asset_img 4.png page_4 %}  \n\n{% asset_img 5.png page_5 %}  \n\n{% asset_img 6.png page_6 %}  \n\n{% asset_img 7.png page_7 %}  \n\n{% asset_img 8.png page_8 %}  \n\n{% asset_img 9.png page_9 %}  \n\n{% asset_img 10.png page_10 %}  \n\n{% asset_img 11.png page_11 %}  \n\n{% asset_img 12.png page_12 %}  \n\n{% asset_img 13.png page_13 %}  \n\n{% asset_img 14.png page_14 %}  \n\n{% asset_img 15.png page_15 %}  \n\n{% asset_img 16.png page_16 %}  \n\n{% asset_img 17.png page_17 %}  \n\n{% asset_img 18.png page_18 %}  \n\n{% asset_img 19.png page_19 %}  \n\n{% asset_img 20.png page_20 %}  \n\n{% asset_img 21.png page_21 %}  \n\n{% asset_img 22.png page_22 %}  \n\n{% asset_img 23.png page_23 %}  \n\n{% asset_img 24.png page_24 %}  \n\n{% asset_img 25.png page_25 %}  \n\n{% asset_img 26.png page_26 %}  \n\n{% asset_img 27.png page_27 %}  \n\n{% asset_img 28.png page_28 %}  ","slug":"cs/nlp/2024/03/ChatGPT","published":1,"updated":"2024-03-17T03:28:29.054Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmfj000zam4k6s3665ed","content":"<img src=\"/14e576c/1.png\" class title=\"page_1\">\n<img src=\"/14e576c/2.png\" class title=\"page_2\">\n<img src=\"/14e576c/3.png\" class title=\"page_3\">\n<img src=\"/14e576c/4.png\" class title=\"page_4\">\n<img src=\"/14e576c/5.png\" class title=\"page_5\">\n<img src=\"/14e576c/6.png\" class title=\"page_6\">\n<img src=\"/14e576c/7.png\" class title=\"page_7\">\n<img src=\"/14e576c/8.png\" class title=\"page_8\">\n<img src=\"/14e576c/9.png\" class title=\"page_9\">\n<img src=\"/14e576c/10.png\" class title=\"page_10\">\n<img src=\"/14e576c/11.png\" class title=\"page_11\">\n<img src=\"/14e576c/12.png\" class title=\"page_12\">\n<img src=\"/14e576c/13.png\" class title=\"page_13\">\n<img src=\"/14e576c/14.png\" class title=\"page_14\">\n<img src=\"/14e576c/15.png\" class title=\"page_15\">\n<img src=\"/14e576c/16.png\" class title=\"page_16\">\n<img src=\"/14e576c/17.png\" class title=\"page_17\">\n<img src=\"/14e576c/18.png\" class title=\"page_18\">\n<img src=\"/14e576c/19.png\" class title=\"page_19\">\n<img src=\"/14e576c/20.png\" class title=\"page_20\">\n<img src=\"/14e576c/21.png\" class title=\"page_21\">\n<img src=\"/14e576c/22.png\" class title=\"page_22\">\n<img src=\"/14e576c/23.png\" class title=\"page_23\">\n<img src=\"/14e576c/24.png\" class title=\"page_24\">\n<img src=\"/14e576c/25.png\" class title=\"page_25\">\n<img src=\"/14e576c/26.png\" class title=\"page_26\">\n<img src=\"/14e576c/27.png\" class title=\"page_27\">\n<img src=\"/14e576c/28.png\" class title=\"page_28\">\n","length":0,"excerpt":"","more":"<img src=\"/14e576c/1.png\" class title=\"page_1\">\n<img src=\"/14e576c/2.png\" class title=\"page_2\">\n<img src=\"/14e576c/3.png\" class title=\"page_3\">\n<img src=\"/14e576c/4.png\" class title=\"page_4\">\n<img src=\"/14e576c/5.png\" class title=\"page_5\">\n<img src=\"/14e576c/6.png\" class title=\"page_6\">\n<img src=\"/14e576c/7.png\" class title=\"page_7\">\n<img src=\"/14e576c/8.png\" class title=\"page_8\">\n<img src=\"/14e576c/9.png\" class title=\"page_9\">\n<img src=\"/14e576c/10.png\" class title=\"page_10\">\n<img src=\"/14e576c/11.png\" class title=\"page_11\">\n<img src=\"/14e576c/12.png\" class title=\"page_12\">\n<img src=\"/14e576c/13.png\" class title=\"page_13\">\n<img src=\"/14e576c/14.png\" class title=\"page_14\">\n<img src=\"/14e576c/15.png\" class title=\"page_15\">\n<img src=\"/14e576c/16.png\" class title=\"page_16\">\n<img src=\"/14e576c/17.png\" class title=\"page_17\">\n<img src=\"/14e576c/18.png\" class title=\"page_18\">\n<img src=\"/14e576c/19.png\" class title=\"page_19\">\n<img src=\"/14e576c/20.png\" class title=\"page_20\">\n<img src=\"/14e576c/21.png\" class title=\"page_21\">\n<img src=\"/14e576c/22.png\" class title=\"page_22\">\n<img src=\"/14e576c/23.png\" class title=\"page_23\">\n<img src=\"/14e576c/24.png\" class title=\"page_24\">\n<img src=\"/14e576c/25.png\" class title=\"page_25\">\n<img src=\"/14e576c/26.png\" class title=\"page_26\">\n<img src=\"/14e576c/27.png\" class title=\"page_27\">\n<img src=\"/14e576c/28.png\" class title=\"page_28\">\n"},{"title":"(2)","abbrlink":"ad0bba9d","date":"2024-03-24T03:24:47.000Z","_content":"\n![](/images/cover.png)  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~~\n\n***  \n\n# 1Berttoken embedding()position encoding  \n\n1concat+  \n\n2(768)250076810007681-1  \n\n3one-hotone-hot  \n\n4  \n\n# 2LoRALoRA  \n\n1LoRALoRA  \n\n2//LoRA  \n\n3LoRA1batch23optionalint8/int4  \n\n# 3normalizationbatchnorm/layernorm  \n\n118018normalization/  \n\n2batchnormICSinternal covariate shifti.i.d.normalization  \n\n3.How Does Batch Normalization Help Optimization?batchnormICSbatchnormICSbatchnorm  \n\n# 4Transformerpre-normpost-norm?  \n\n1.Transformerpost-normadd & normpost-norm  \n\n2.Pre-normpost-normL+1L  \n\n3.post-normpre-normpost-normpre-norm  \n\n# 5Multi-Head Attentionhidden size=DhdD=dhsbatch size=1self-attentionFloat Operations  \n\n1.QKV6  s  D^22D  \n\n2.QKh  2  d  s^2h  \n\n3.scalingh  s^2  \n\n4.softmaxh  3  s^2softmaxsexpsexps  \n\n5.reductionVh  2  d  s^2  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)  \n","source":"_posts/cs/nlp/2024/03/-2.md","raw":"---\ntitle: (2)\ntags:\n  - NLP\n  - LLM\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: ad0bba9d\ndate: 2024-03-24 11:24:47\n---\n\n![](/images/cover.png)  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~~\n\n***  \n\n# 1Berttoken embedding()position encoding  \n\n1concat+  \n\n2(768)250076810007681-1  \n\n3one-hotone-hot  \n\n4  \n\n# 2LoRALoRA  \n\n1LoRALoRA  \n\n2//LoRA  \n\n3LoRA1batch23optionalint8/int4  \n\n# 3normalizationbatchnorm/layernorm  \n\n118018normalization/  \n\n2batchnormICSinternal covariate shifti.i.d.normalization  \n\n3.How Does Batch Normalization Help Optimization?batchnormICSbatchnormICSbatchnorm  \n\n# 4Transformerpre-normpost-norm?  \n\n1.Transformerpost-normadd & normpost-norm  \n\n2.Pre-normpost-normL+1L  \n\n3.post-normpre-normpost-normpre-norm  \n\n# 5Multi-Head Attentionhidden size=DhdD=dhsbatch size=1self-attentionFloat Operations  \n\n1.QKV6  s  D^22D  \n\n2.QKh  2  d  s^2h  \n\n3.scalingh  s^2  \n\n4.softmaxh  3  s^2softmaxsexpsexps  \n\n5.reductionVh  2  d  s^2  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)  \n","slug":"cs/nlp/2024/03/-2","published":1,"updated":"2024-03-24T04:16:09.176Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmfj0011am4kfc91agmw","content":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM<sub></sub></p>\n<hr>\n<h1 id=\"berttoken-embeddingposition-encoding\">1Berttoken\nembedding()position encoding</h1>\n<p>1concat+</p>\n<p>2(768)250076810007681-1</p>\n<p>3one-hotone-hot</p>\n<p>4</p>\n<h1 id=\"loralora\">2LoRALoRA</h1>\n<p>1LoRALoRA</p>\n<p>2//LoRA</p>\n<p>3LoRA1batch23optionalint8/int4</p>\n<h1 id=\"normalizationbatchnormlayernorm\">3normalizationbatchnorm/layernorm</h1>\n<p>118018normalization/</p>\n<p>2batchnormICSinternal\ncovariate\nshifti.i.d.normalization</p>\n<p>3.How Does Batch Normalization Help\nOptimization?batchnormICSbatchnormICSbatchnorm</p>\n<h1 id=\"transformerpre-normpost-norm\">4Transformerpre-normpost-norm?</h1>\n<p>1.Transformerpost-normadd &amp;\nnormpost-norm</p>\n<p>2.Pre-normpost-normL+1L</p>\n<p>3.post-normpre-normpost-normpre-norm</p>\n<h1 id=\"multi-head-attentionhidden-sizedhdddhsbatch-size1self-attentionfloat-operations\">5Multi-Head\nAttentionhidden\nsize=DhdD=dhsbatch\nsize=1self-attentionFloat\nOperations</h1>\n<p>1.QKV6  s \nD^22D</p>\n<p>2.QKh  2  d  s^2h</p>\n<p>3.scalingh  s^2</p>\n<p>4.softmaxh  3 \ns^2softmaxsexpsexps</p>\n<p>5.reductionVh  2  d  s^2</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n","length":2220,"excerpt":"","more":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM<sub></sub></p>\n<hr>\n<h1 id=\"berttoken-embeddingposition-encoding\">1Berttoken\nembedding()position encoding</h1>\n<p>1concat+</p>\n<p>2(768)250076810007681-1</p>\n<p>3one-hotone-hot</p>\n<p>4</p>\n<h1 id=\"loralora\">2LoRALoRA</h1>\n<p>1LoRALoRA</p>\n<p>2//LoRA</p>\n<p>3LoRA1batch23optionalint8/int4</p>\n<h1 id=\"normalizationbatchnormlayernorm\">3normalizationbatchnorm/layernorm</h1>\n<p>118018normalization/</p>\n<p>2batchnormICSinternal\ncovariate\nshifti.i.d.normalization</p>\n<p>3.How Does Batch Normalization Help\nOptimization?batchnormICSbatchnormICSbatchnorm</p>\n<h1 id=\"transformerpre-normpost-norm\">4Transformerpre-normpost-norm?</h1>\n<p>1.Transformerpost-normadd &amp;\nnormpost-norm</p>\n<p>2.Pre-normpost-normL+1L</p>\n<p>3.post-normpre-normpost-normpre-norm</p>\n<h1 id=\"multi-head-attentionhidden-sizedhdddhsbatch-size1self-attentionfloat-operations\">5Multi-Head\nAttentionhidden\nsize=DhdD=dhsbatch\nsize=1self-attentionFloat\nOperations</h1>\n<p>1.QKV6  s \nD^22D</p>\n<p>2.QKh  2  d  s^2h</p>\n<p>3.scalingh  s^2</p>\n<p>4.softmaxh  3 \ns^2softmaxsexpsexps</p>\n<p>5.reductionVh  2  d  s^2</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n"},{"title":"MoE","abbrlink":"44e38c1b","date":"2024-03-30T01:56:05.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n202434MoEQwen1.5-MoEDBRXJambaMistral  \n\nMoE  \n\n<center>\n\n|  |  |  |\n| :----: | :----: | :----: |\n| GPT4 | 20233 | 236George HotzGPT48220B |\n| Mistral-87B | 202312 | Mistral AI |\n| LLAMA-MoE | 202312 | github |\n| DeepSeek-MoE | 20241 | MoE |\n| abab6 |20241 | MiniMaxMoE |\n| 2.0 | 20242 |  |\n| Step-2 | 20243 |  |\n| MM1 | 20243 | MoE |\n| Grok-1 | 20243 | X |\n| Qwen1.5-MoE-A2.7B| 20243 |  |\n| DBRX | 20243 | Databricks |\n| Jamba | 20243 | AI21 |\n| Mistral-822B | 20244 | Mistral AI |\n| WizardLM-2-822B | 20244 |  |\n| 3.0 | 20244 | 400BMoE |\n| Arctic | 20244 | Snowflake480BDense-MoE Hybrid |\n\n</center>  \n\nMoEMoE  \n\n{% asset_img xiaomi_moe.jpg MoE %}  \n\nMoE  \n\nMoEMoEMoE  \n\n20244DeepSeek-MoEQwen1.5-MoE\n\n#   \n\nMoE  \n\nMoEGoogle\n\n##   \n\nMoE1991[Adaptive Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)Geoffrey HintonMichael I. JordanMoE1988  \n\n>This idea was first presented by Jacobs and Hinton at the Connectionist Summer School in Pittsburg in 1988.  \n\nMoEMoE  \n\n## RNN  \n\nGoogle20171[Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)MoELSTM137B128kLSTM  \n\n## Transformer  \n\n1. 20206Google[GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)MoEencoder-decodertransformerFFNMoE12.5B600BMoE2048  \n\n2. 20211Google[Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) T5encoder-decoderFFNMoErouting1.6Tswitch transformerSwitch Transformersscaling  \n\n3. 20222Google[ST-MoE: Designing Stable and Transferable Sparse Expert Models](https://arxiv.org/abs/2202.08906)encoder-decoderMoE269B32BST-MoEMoESwitch Transformer  \n\n## GPT  \n\n1. 202112GoogleGLaM[GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://arxiv.org/abs/2112.06905)1.2Tdecoder-onlyencoder-decoderdecoder-onlyGoogle  \n\n2. 20241[DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/abs/2401.06066)2312DeepSeekMoE  \n\n3. 2024DatabricksDBRXQwen1.5-MoE-A2.7BMistral AIMistral-8x22B  \n\n#   \n\nGeoffrey HintonMichael I. Jordan[Adaptive Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)MoE  \n\n1.   \n\n  \n\n  \n\nMoEexpert  \n\nMoEvowel discrimination taskMoEaeiou  \n\n2.   \n\nMoEexpert networkgating networkexpertgating networkexpertexpertstochastictruefalse  \n\n{% asset_img vanilla_moe.png Vanilla MoE %}  \n\n3.   \n\nMoEideaJacobsHinton1988lossensembleexpertexpertexpertresidual  \n\ncase $c$ $d^c$ ground truth $i$ expert $o_{i}^c$$p_{i}^c$ gating network $i$ expert $E^{c}$ \n\n$$E^{c}=\\left|\\left|d^{c}-\\sum_{i}p_{i}^{c}o_{i}^{c}\\right|\\right|^{2}$$\n\nexpert  \n\nexpertexpertexpert  \n\nexpertexpertexpertexpertgating network  \n\n  \n\nHintonJordanlossexpert  \n\ngating networkexpert  \n\n$$E^{c}=\\langle\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2\\rangle=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}$$\n\nexpertexpertexpert  \n\nlosslocalizationcasegating networkexpertgating networkexpert  \n\nlocalizationexpert  \n\nexpertexpertgating networkexpert error+-  \n\nexpert0  \n\n4. \n\nlosslossloss  \n\n$$\\text{loss}E^{c}=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}$$  \n\n$$\\text{loss}E^c=-log\\sum_ip_i^ce^{-\\frac12\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}$$  \n\nlossloss  \n\n$$\\text{loss}\\frac{\\partial E^c}{\\partial\\mathbf{o}_i^c}=-2p_i^c(\\mathbf{d}^c-\\mathbf{o}_i^c)$$  \n\n$$\\text{loss}\\frac{\\partial E^c}{\\partial\\mathbf{o}_i^c}=-\\left[\\frac{p_i^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}}{\\sum_jp_j^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_j^c\\|^2}}\\right](\\mathbf{d}^c-\\mathbf{o}_i^c)$$  \n\nlossloss $i$ expertexpertexpert $i$ casegating networklosscaseexpertlosslossexpertlocalizationexpert  \n\nBTWloss  \n\nMoE  \n\n# LSTM MoE  \n\nGoogle20171\n[OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER](https://arxiv.org/abs/1701.06538)MoELSTM137BLSTM7  \n\n1991  \n\n## \n\nTransformer  \n\nconditional computationconditional computationMoE  \n\n  \n\n- MoEexpertbatch sizebatch size  \nbatch size3216expertexpert2batch sizebatch sizebatch size  \n-   \nNLP  \n-   \n  \n-   \nGPU  \n- GPU  \nGPUbranchingif/elseMoEgating network  \n\n  \n\n## \n\n1.   \n\n  \n\nLSTMMoEembedding  \n\n{% asset_img rnn_moe.png LSTM MoE %}  \n\nexpertfeed-forward neural networknexpertgating networkn  \n\n$$\\begin{aligned}y=\\sum_{i=1}^nG(x)_iE_i(x)\\end{aligned}$$  \n\n$E_{i}(x)$  $i$ expert$G(x)_{i}$ gating network $i$ expert  \n\n $G(x)_{i}$ 0expert  \n\nexperttwo-level hierarchical MoEgating networkgating networkexpertgating networkexpertword2vechierarchical softmax  \n\n2. gating network  \n\ngating network  \n\nsoftmaxgating function  \n\n$$\\begin{aligned}G_\\sigma(x)=Softmax(x\\cdot W_g)\\end{aligned}$$  \n\ntopkksoftmax0expert  \n\nsparsitytopkgating function  \n\nGaussian noisenoise  \n\n$$\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}$$  \n\n$$KeepTopK(v,k)_i=\\begin{cases}v_i&\\text{if }v_i\\text{ is in the top }k\\text{ elements of }v.\\\\-\\infty&\\text{otherwise.}\\end{cases}$$  \n\n$$\\begin{aligned}H(x)_i=(x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\end{aligned}$$  \n\nnoisesoftplusReLU  \n\n{% asset_img softplus.png softplus %}  \n\n  \n\n##   \n\nMoEgating networkexpertexpert  \n\n  \n\nhard constraintexperthard constraintsoft constraint  \n\nexpert  \n\n$$Importance(X)=\\sum_{x\\in X}G(x)$$  \n\n$G(x)$ gating networkexpert  \n\n $L_{importance}$$L_{importance}$   \n\n$$L_{importance}(X)=w_{importance}\\cdot CV(Importance(X))^2$$  \n\n $w_{importance}$ 0.1CVcoefficient of variation  \n\ncoefficient of variation $\\sigma$   $\\mu$   \n\nMoEexpertexpertgating $L_{importance}$   \n\n $L_{importance}$  $L_{importance}$  $L_{importance}$   \n\nexpertexpertgating  \n\n $L_{load}$ expert  \n\nexpertback propagation $L_{load}$ expert  \n\nMoE $H(x)$ KeepTopK  \n\n$$\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}$$  \n\n$$\\begin{aligned}H(x)_i=(x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\end{aligned}$$  \n\n $kth\\_excluding(H(x),k,i)$ $H(x)$  $i$  $k$  $P(x,i)$ noise $i$ noise $kth\\_excluding(H(x),k,i)$   \n\n$$\\begin{aligned}P(x,i)=Pr\\Big((x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\\\>kth\\_excluding(H(x),k,i)\\Big)\\end{aligned}$$  \n\nnoise $i$  $i$ $P(x,i)$   \n\n$$\\begin{aligned}P(x,i)&=\\Phi\\Big(\\frac{(x\\cdot W_g)_i-kth\\_excluding(H(x),k,i)}{Softplus((x\\cdot W_{noise})_i)}\\Big)\\end{aligned}$$  \n\n $\\Phi$ CDF  \n\n $i$ expert  \n\n$$\\begin{aligned}Load(X)_i=\\sum_{x\\in X}P(x,i)\\end{aligned}$$  \n\nexpert  \n\n$$L_{load}(X)=w_{load}\\cdot CV(Load(X))^2$$  \n\n$w_{load}$ 0.1  \n\n $L_{importance}(X)$$Load(X)$   \n\nexpert $W_g$   $W_{noise}$ 0  \n\n  \n\n{% asset_img rnn_moe_load_function.png  %}  \n\n  \n\n##   \n\n1.   \n\n  \n\n1batch size  \n\nexpertbatch sizenexpertkbatch sizebexpertbatch sizekb/nexpertbatch size  \n- batchbatchMoEexpertdexpertkbd/nbatch size\n- LSTMbatch size\n\n2  \n\n  \n\nexpertinputoutput[input_size, hidden_size][hidden_size, output_size]GPU1000hidden_sizeexpert1000  \n\n2.  &   \n\nMoEdense4/32/256expertflat MoE256/1024/4096experthierarchical MoEexpert1Mflat4experthierarchical MoEgating2  \n\nppldenseMoEMoE\n\n{% asset_img rnn_moe_perf.png  %}  \n\n3.   \n\n4Bdiminishing returns  \n\n + 100B token32, 256, 10244096, 16384, 65536, 131072expertMoE137B  \n\n  \n\n{% asset_img rnn_moe_137b.png 137 %}  \n\n  \n\n4. Expert Specialization  \n\nMoE\n\ntokenspecialization  \n\n{% asset_img rnn_moe_specilized.png RNN MoE  %}  \n\n# GShard\n\n1. \n\n2018Berttransformer20206GoogleGShard: Scaling Giant Models with Conditional Computation and Automatic ShardingMoEencoder-decodertransformerMoE  \n\nGShardMoE600B  \n\n{% asset_img gshard_moe_family.png GShard MoE family %}  \n\nexpertLSMT MoE -- expert24expertChatGPTBertGPT  \n\nGShardMoE  \n\n2. \n\n  \n\nGoogleencoder-decoder transfomerGShardencoder-decoder transfomer  \n\nGShardencoderdecoderFFNMoENN/2MoE  \n\n{% asset_img gshard_model.png GShard %}  \n\ntop-2 expert  \n\nGShardLSTM MoEgating functionauxiliary loss function  \n\nMoE\n\n$$\\begin{aligned}\n\\mathcal{G}_{s,E}& =\\mathrm{GATE}(x_s)  \\\\\n\\mathrm{FFN}_e(x_s)& =wo_e\\cdot\\text{ReLU}(wi_e\\cdot x_s)  \\\\\ny_{s}& =\\sum_{e=1}^E\\mathcal{G}_{s,e}\\cdot\\mathrm{FFN}_e(x_s) \n\\end{aligned}$$\n\n $x_s$ MoEtoken$w_i$  $w_o$ $\\mathcal{G}_{s}$ gating function\n\nGShardgating function12  \n\nNtokenEexpertNEgating function  \n\ngating function  \n\n1 expert capacity  \n\nexperttokenexperttoken2N/E  \n\nexpert capacityGATE()expert $c_e$ tokentokenexpert  \n\n2 Local group dispatching  \n\ntokenG2N/EG  \n\nbatchbatchbatchgroupall2allgroup  \n\ngroupgradient accumulation  \n\n3 Auxiliary loss  \n\ngatingLSTM MoE  \n\n$$\\ell_{aux}=\\frac1E\\sum_{e=1}^E\\frac{c_e}S\\cdot m_e$$  \n\n$S$ token$E$ $c_e$  $e$ token$m_e$  $e$ expert $S$ token  \n\n $\\frac{c_e}S$  $\\frac{c_e}S$  $m_e$  $m_e$  $e$ expert $S$ tokenloss  \n\nloss  \n\ngating  \n\n{% asset_img gshard_algo_1.png GShard gating  %}  \n\n4 Random routing  \n\ntop-2 experttop-1  \n\ntop-1g2  \n\n3.   \n\n\n\n{% asset_img gshard_perf.png GShard %}  \n\n# Switch Transformer\n\n20224ChatGPTGoogleSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity2021GoogleSwitch Transformer  \n\nSwitch TransformerGShardencoder-decoderT51.6T2048expert  \n\nSwitch Transformer  \n\nSwitch TransformerSwitch TransformerFLOPS/token  \n\nSwitch Transformer  \n\n1TransformerMoESwitch Transformer  \n\n2MoE to denseMoEdenseMoE99%dense  \n\n3  \n- bf16MoE  \n- MoE  \n-   \n\n41TMoE  \n\n5101  \n\n6FLOPS/tokenSwitch Transformer  \n\n##   \n\nSwitch TransformerGShardtransformerFFNMoE  \n\n{% asset_img switch_transformer_structure.png Switch Transformer  %}  \n\nSwitch Transformergating functionSwitch Transformerrouting  \n\nkexpertSwitch Transformergating1expertk=1MoESwitch layer  \n\nroutingrouter  \n\n##   \n\nGShardSwitch Transformerexpert capacityexpertbatchtoken  \n\ntokenexpertoverflowtokenGShard  \n\nSwitch Transformercapacity factor  \n\n$$\\text{expert capacity}=\\left(\\frac{\\text{tokens per batch}}{\\text{number of experts}}\\right)\\times\\text{capacity factor}.$$  \n\ncapacity factorexperttokenoverflow\n\ncapacity factor  \n\n{% asset_img switch_transformer_diff_expert_capacity.png expert capacity %}  \n\nexpert capacity\n\ncapacity factor1overflowcapacity factor  \n\nexpertoverflowMoESwitch Transformer128  \n\ncapacity factoroverflow  \n\n{% asset_img switch_transformer_capacity_effect.png expert capacity %}  \n\ntokenscalingoverflow  \n\n  \n\n $N$ expert $T$ tokenbatch $\\mathcal{B}$ \n\n$$\\begin{aligned}\\text{loss}&=\\alpha\\cdot N\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}$$  \n\n$f_{i}$  $i$ experttoken  \n\n$$\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax }p(x)=i\\}\\end{aligned}$$  \n\n$P_i$ batchtoken$i$ expert  \n\n$$\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}$$  \n\nGShard  \n\n$f$  $P$  $1/N$  \n\n$\\alpha$ 1e-51e-11e-2  \n\n $\\sum_{i=1}^N(f_i\\cdot P_i)=\\sum_{i=1}^N(\\frac1N\\cdot\\frac1N)=\\frac1N$loss $N$expertloss  \n\n## \n\n1. trick  \n\n1bf16  \n\nbf16routing function  \n\nroutingsoftmaxexponentialrounding errorrouting  \n\n2  \n\n $\\mu=0$$\\sigma=\\sqrt{s}/n$sne.g. fan-in  \n\nTransformers=1.010  \n\n{% asset_img switch_transformer_init.png  %}  \n\n3dropout  \n\nSwitch Transformerdropout  \n\n{% asset_img switch_transformer_dropout.png dropout %}  \n\ndropoutdense0.1expertdropout  \n\n2. scaling  \n\nSwitch Transformerscaling  \n\n1Step-Basis  \n\nstepexpert  \n\nstepstep  \n\n{% asset_img switch_transformer_scaling_step.png step scaling %}  \n\n2Time-Basis  \n\nSwitch TransformerstepSwitch Transformerdense  \n\nSwitch TransformerdenseSwitch Transformerdensedense1/7  \n\n{% asset_img switch_transformer_scaling_time.png time scaling %}  \n\n3dense\n\nSwitch TransformerdenseSwitch Transformerdense  \n\nStep-BasisTime-Basis64Switch TransformerT5-LargestepSwitch Transformer  \n\n{% asset_img switch_transformer_scaling_dense.png dense %}  \n\n3. SFT  \n\nGLUESuperGLUEdense  \n\neval  \n\n{% asset_img switch_transformer_sft_result.png sft %}  \n\n4.   \n\nSwitch TransformerBTdense  \n\n  \n- Switch Transformerdense  \n- label25%75%ground truth  \n\ndensedensedenseSwitch Transformer30%  \n\n{% asset_img switch_transformer_distill.png  %}  \n\n99%  \n\n{% asset_img switch_transformer_distill_diff_model.png  %}  \n\nSuperGLUE  \n\n{% asset_img switch_transformer_distill_sft.png sft %}  \n\n# GLaM\n\n1. \n\n202112GoogleGLaM: Efficient Scaling of Language Models with Mixture-of-Experts1.2T64token96.6BMoE  \n\nSwitch TransformerGLaM1.6T token  \n\n  \n\n{% asset_img glam_related_model.png glam %}  \n\nGPT-3175BGPT-3NLPGPT-3  \n\n{% asset_img glam_compare_gpt3.png glamgpt3 %}  \n\n{% asset_img glam_compare_gpt3_2.png glamgpt3 %}  \n\n2. \n\nSwitch TransformerFFNMoESwitch TransformerGLaMexpert  \n\n{% asset_img glam_model.png glam %}  \n\n  \n\n1  \n\nXLNET  \n\n2\n\n> In the non-MoE Transformer feed-forward sub-layers, we replace the first linear projection and the activation function with the Gated Linear Unitwhich computes the component-wise product of two linear transformation of the input, followed by a Gaussian Error Linear Unit.  \n\n3. \n\ntrick  \n\n1Lingvo: a modular and scalable framework for sequence-to-sequence modelingNaNInf  \n\n2BPNaNInfcheckpointNaNInf  \n\nMoE  \n\n{% asset_img glam_family.png glam %}  \n\nGLaMdense  \n\n{% asset_img glam_perf.png glam %}  \n\nGLaM MoEdense  \n\n# ST-MoE  \n\n20222GoogleST-MOE: DESIGNING STABLE AND TRANSFERABLE SPARSE EXPERT MODELSST-MoEMoEMoE  \n\nST-MoE269B32B denseStable Transferable Mixture-of-ExpertsST-MoE-32B  \n\nMoEST-MoESwitch Transformer1MoE  \n\nST-MoE4B269BST-MoE  \n\n{% asset_img st_moe_models.png ST-MoE %}  \n\n##   \n\n  \n\n1.   \n\n  \n\n  \n\n> Some architectural improvements involve more multiplications than additions or do not sum many items at once\n\n1GELU Gated Linear Units (GEGLU)  \n\nGLUcomponent-wiseGELU-Linear FFNtransformerReLU FFN  \n\n$$\\begin{aligned}FFN_{GEGLU}(x,W,V,b,c)=GELU(xW+b)\\odot(xV+c)\\end{aligned}$$  \n\n  \n\n2RMSNorm  \n\nRMSNorm $g$  \n\n$$y_i=\\frac{x_i}{\\sqrt{\\frac1d\\sum_{i=1}^dx_i^2}}\\cdot g_i$$  \n\nST-MoEGEGLURMSNorm  \n\n{% asset_img st_moe_remove_multiplications.png  %}  \n\n  \n\n3dense  \n\nST-MoEexpertdensedense\n\n{% asset_img st_moe_more_dense_layer.png dense %}  \n\n4bias\n\nFFNbias B  \n\n$$\\text{FFN}_{\\text{GEGLU}}+\\text{Add Bias}(x)=[(\\text{GELU}(xW_{11})\\odot xW_{12})+B]W_2$$  \n\n$$\\mathrm{FFN}_{\\mathrm{GEGLU}}+\\mathrm{Mult~Bias}(x)=[(\\mathrm{GELU}(xW_{11})\\odot xW_{12})\\odot B]W_2$$  \n\n  \n\n  \n\n2. noise  \n\nST-MoE  \n\ninput-jitterrouterlogits[1e-2, 1e2]  \n\n{% asset_img st_moe_more_add_noise.png noise %}  \n\nnoise  \n\n  \n\n3.   \n\nactivationgradient  \n\nST-MoE269B  \n\nST-MoErouter z-loss  \n\n$$L_z(x)=\\frac{1}{B}\\sum_{i=1}^B\\left(\\log\\sum_{j=1}^Ne^{x_j^{(i)}}\\right)^2$$  \n\n$B$ token$N$ $x\\in\\mathcal{R}^{B\\times N}$ router  \n\nz-lossrouterlogitsz-loss  \n\n{% asset_img st_moe_z_loss_result.png z-loss %}  \n\nST-MoEz-loss  \n\nz-loss $c_z$   \n\n$$L_{tot}=L_{CE}+c_BL_B+c_zL_Z$$  \n\nST-MoE$c_z=0.001$  \n\n$L_B$  auxiliary load balance lossST-MoEGShard/Switch Transformer  \n\n$$\\begin{aligned}\\text{loss}&=\\alpha\\cdot N\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}$$  \n\n$$\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax }p(x)=i\\}\\end{aligned}$$  \n\n$$\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}$$  \n\n$N$  $\\mathcal{B}$ $T$ tokenbatch$f_{i}$  $i$ experttoken$P_i$ batchtoken$i$ expert  \n\n4. \n\nfloat32bfloat16bfloat16allreducebfloat16float32  \n\nST-MoE-32BallreduceST-MoEallreducefloat32  \n\nbfloat16float32  \n\n{% asset_img st_moe_round_error.png bf16 %}  \n\nz-loss  \n\nMoErouter  \n\nST-MoE1/5token  \n\nsoftmaxMoE  \n\n## \n\ndensescaling lawMoEdense \n \n1expert  \n\n2routing  \n\n3  \n\n4  \n\nMoEscaling lawUnified scaling laws for routed language models  \n\n1. expert  \n\nST-MoE8/16/32<1%>256  \n\n>1<=1  \n\n2. routingcapacity factor  \n\ncapacity factor  \n\n{% asset_img st_moe_capacity_factor.png capacity factor %}  \n\n  \n\n1capacity factor  \n\n2capacity facotr  \n \n3expertcapacity factor  \n\ncapacity factorcapacity factor  \n\ncapacity factor  \n\n{% asset_img st_moe_capacity_factor_speed.png capacity factor %}  \n\n##   \n\n1. ST-MoE  \n\nST-MoE-32BST-MoE-32B  \n\n{% asset_img st_moe_perf.png capacity ST-MoE-32B %}  \n\n2. Expert Specialization  \n\ndecodertokenencoder  \n\n{% asset_img st_moe_encoder_specialization.png encoder %}  \n\nencodertoken  \n\n{% asset_img st_moe_multiling_specialization.png  %}  \n\n# DeepseekMoE\n\n20241DeepseekMoEMoEDeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language ModelsDeepSeekMoE  \n\nDeepSeekMoEMoE2  \n\n1expertexpert  \n\n2expertexpertshared expert  \n\nexpert(specialization)  \n\nDeepSeekMoE2BMoE16BMoEDeepSeekMoE-16B40GB  \n\nDeepSeekMoE-2B2BDeepSeekMoE-16B7B40%  \n\nDeepSeekMoE-16B  \n\n{% asset_img ds_moe_perf.png deepseek moe %}  \n\nDeepSeekMoE-2B16B  \n\nDeepSeekMoE-145BMoEDeepSeek-67B  \n\n##   \n\nMoEmixture of expertmotivationexpert  \n\n1991expert  \n\nMoEknowledge hybridityknowledge redundancy  \n\n1  \n\nexpertexpert  \n\n2  \n\nexpertexpertexpertexpert8expertexpert  \n\n(expert specialization)MoE  \n\nexpertnon-overlap & foucusd knowledge  \n\nDeepSeekMoE2  \n\n1Fine-Grained Expert Segmentation  \n\nexpertexpertexpertspecialization16expert2120expert1/464expert8 $\\binom{64}8=4,426,165,368$    \n\n2Shared Expert Isolation  \n\nexpertcommon knowledgeexpertexpertexpertexpert  \n\nMoEFine-Grained Expert SegmentationShared Expert Isolation  \n\n{% asset_img ds_moe_structure.png deepseek moe  %}  \n\nexpert isolation20221DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale  \n\nMoEexpert $N$expert $K$DeepSeekMoEexpert $1/m$DeepSeekMoE $mN$ expertexpert $mK$ $T$ $L$ $e_i^l$  $i$ expertDeepSeekMoElayernorm  \n\n$$\\mathbf{u}_{1:T}^l=\\text{Self-Att}\\left(\\mathbf{h}_{1:T}^{l-1}\\right)+\\mathbf{h}_{1:T}^{l-1}$$  \n\n$$\\mathbf{h}_t^l=\\sum_{i=1}^{mN}\\left(g_{i,t}\\text{ FFN}_i\\left(\\mathbf{u}_t^l\\right)\\right)+\\mathbf{u}_t^l$$  \n\n$$g_{i,t}=\\begin{cases}s_{i,t},&s_{i,t}\\in\\text{Topk}(\\{s_{j,t}|1\\leqslant j\\leqslant mN\\},mK)\\\\0,&\\text{otherwise,}\\end{cases}$$  \n\n$$s_{i,t}=\\mathrm{Softmax}_i\\left({\\mathbf{u}_t^l}^T\\mathbf{e}_i^l\\right)$$  \n\n## \n\nMoEgating  \n\n1routing collapsegatingexpert  \n\n2  \n\nrouting collapseDeepSeekMoEexpert-level balance loss\n\n$$\\begin{aligned}\n\\mathcal{L}_{\\mathrm{ExpBal}}& =\\alpha_1\\sum_{i=1}^{N'}f_iP_i\n\\end{aligned}$$  \n\n$$\\begin{aligned}\nf_{i}& =\\frac{N^{\\prime}}{K^{\\prime}T}\\sum_{t=1}^T\\mathbb{1}(\\text{Token }t\\text{ selects Expert }i)\n\\end{aligned}$$  \n\n$$\\begin{aligned}\nP_{i}& =\\frac1T\\sum_{t=1}^Ts_{i,t} \n\\end{aligned}$$  \n\n$\\alpha_1$ expert-level balance factor  \n\n $f_i$  $P_i$ Switch Transformer  \n\nSwitch Transformer $f_i$  $i$ experttokenDeepSeekMoE $N'/K'$  $N'=mN-K_s$$K'=mK-K_s$$K_s$ expertDeepSeekMoE $f_i$ Switch Transformer  \n\n$N'/K'$ expertloss  \n\n$P_i$ token $i$ expertSwitch Transformer  \n\n $f_i$ $P_i$   \n\nDeepSeekMoEdevice-level balance loss\n\n$$\\begin{aligned}\n\\mathcal{L}_{\\mathrm{DevBal}}& =\\alpha_2\\sum_{i=1}^Df_i'P_i'\n\\end{aligned}$$\n\n$$\\begin{aligned}\nf_i^{\\prime}& =\\frac1{|\\mathcal{E}_i|}\\sum_{j\\in\\mathcal{E}_i}f_j\n\\end{aligned}$$\n\n$$\\begin{aligned}\nP_{i}^{\\prime}& =\\sum_{j\\in\\mathcal{E}_i}P_j\n\\end{aligned}$$\n\n$\\alpha_2$ device-level balance factor  \n\n$\\mathcal{E}_i$  $i$ \n\ndevice-level balance lossexpert-level balance loss $f_i$  $P_i$ expert  \n\nexpert64expert8tokentokenexpert  \n\nexpert\n\n## \n\n1. \n\n100B tokenDeepSeekMoE-2BBPE8k\n\nDeepSeekMoE-2B0.006multi-head attention0.3B\n\n{% asset_img ds_model_param.png  %}  \n\nrelative expert sizeDeepSeekMoEexpertFFN\n\n  \n\n<center>\n\n|  |  |\n| :----: | :----: |\n| optimizer | AdamW |\n| adam_beta_1 | 0.9 |\n| adam_beta_2 | 0.95 |\n| adam_weight_decay | 0.1 |\n| warmup schedule | linear |\n| warmup step | 2000 |\n| max lr | 1.08e-3 |\n| dropout | 0 |\n| sequence length | 2k |\n| batch size | 2k |\n| total step | 25,000 |\n\n\n</center>  \n\n  \n- expertGPUdevice-level balance loss  \n- expert-level balance factor0.01  \n- 80%0.31690%0.316  \n\n100BDeepSeekMoE-2Bbenchmark4densehash layermoeHash layers for large sparse modelsSwitch TransformerGShard\n\n{% asset_img ds_moe_comparison.png deepseek moe %}  \n\n  \n- Hash LayerSwitch Transformerdense  \n- GSshardHash LayerSwitch Transformer  \n- DeepSeekMoEGShard  \n\nDeepSeekMoEdenseGShardDeepSeekMoE-2B\n\ndenseGShard161.5DeepSeekMoE-2B  \n\n{% asset_img ds_moe_upper_bound_2b.png deepseek moe upper bound %}  \n\nDeepSeekMoEDeepSeekMoE-13B, 1.21.5GShardDeepSeekMoE-13Bmatch  \n\n{% asset_img ds_moe_upper_bound_13b.png deepseek moe upper bound %}  \n\n2. DeepSeekMoE\n\nDeepSeekMoEshared expertfine-grained expertexpert\n\n{% asset_img ds_moe_ablation.png deepseek moe upper bound  %}  \n\n1\n\n2\n\n364expert1/2/4pileloss1.808,1.806,1.8111:32+6\n\n3. expert specialization\n\nDeepSeekMoEexpert specialization\n\n1DeepSeekMoE-2B1.5GShardtop  \n\n\n\n{% asset_img ds_moe_expert_specialization.png  %}  \n\nDeepSeekMoEDeepSeekMoE  \n\n2DeepSeekMoEloss\n\n3GShardDeepSeekMoE  \n\n{% asset_img ds_moe_less_activated_expert.png  %}  \n\n132b2+6GShardDeepSeekMoE\n\n{% asset_img ds_2b_less_expert.png 2B %}  \n\n1. DeepSeekMoE-16B  \n\nDeepSeekMoE-16B2TLLAMA2-7B100k  \n\n{% asset_img ds_model_param.png  %}  \n\nMoE  \n\nMoEloss  \n\nDeepSeekMoE-16B6426gating functiontoken8token16.4B2.8B  \n\ndimension  \n\n  \n- lr = 4.2e-4  \n- 80%90%lr0.316  \n- batch size = 4.5k4kbatch18M token2T10.6w  \n- pipeline parallelism\n\nexpert level balance loss0.001  \n\nDeepSeekMoE-16BDeepSeek-7B  \n\n{% asset_img ds_16b_perf_1.png DeepSeek-7B %}  \n\nDeepSeekMoE-16BLLAMA2-7B  \n\n{% asset_img ds_16b_perf_2.png LLAMA2-7B %}  \n\n5. DeepSeekMoE-145B  \n\n245BtokenDeepSeekMoE-145BDeepSeek-67B  \n\n{% asset_img ds_moe_145b.png 145b %}  \n\n# DBRX\n\n2024327DatabricksDBRX132B36BMoE\n\nDBRXRoPEGLUGQAfine-grained expert16token4MixtralGrok-182DBRX  \n\nDBRX32k12TtokenDBRX3072H100post-trainingred-team3  \n\nDBRXGPT-3.5Gemini 1.0 ProCodeLLaMA-70B  \n\n{% asset_img dbrx_perf.png DBRX %}  \n\nDBRX  \n\n{% asset_img dbrx_infer_efficiency.png  %}  \n\n# Qwen1.5-MoE \n\n2024328Qwen1.5-MoE-A2.7B2.7BQwen1.5-7B  \n\nQwen1.5-MoE-A2.7BDeepSeekMoEDBRXfine-grained expert64token84  \n\nQwen1.5-MoE-A2.7BQwen-1.8B  \n\nQwen1.5-MoE-A2.7B  \n\n{% asset_img qwen1.5_moe_perf.png Qwen1.5-MoE-A2.7B %}  \n\nQwen1.5-MoE-A2.7Bnon-embedding7B  \n\n{% asset_img qwen1.5_moe_params.png Qwen1.5-MoE-A2.7B %}  \n\nQwen1.5-MoE-A2.7BQwen1.5-7B75%  \n\nA100-80GvLLMQwen1.5-7BQwen1.5-MoE-A2.7B  \n\n/token1000token1000TPSthroughput  \n\n{% asset_img qwen1.5_moe_tps.png Qwen1.5-MoE-A2.7B TPS %}  \n\nMoEdenseQwen1.5-MoE-A2.7BQwen1.5-7B1.74  \n\n# Mistral\n\n## Mistral 8x7B\n\n20231211Mistral AIMistral-8x7Btoken82  \n\nMistral-8x7B32kLLAM2-70BGPT-3.5  \n\n{% asset_img mistral_8_7b_perf.png Mistral 8x7B %}  \n\nMistral-8x7BLLAM2-70B6  \n\nLLAM2-13B  \n\n{% asset_img mistral_8_7b_active_perf.png Mistral 8x7B %}  \n\n## Mistral 8x22B\n\n2024417Mistral AIMistral-8x22B141B39BMoE  \n\nMistral-8x22BMistral-8x7B32k64kMistral-8x22Bfunction call  \n\n  \n\n{% asset_img mistral_8_22b_reasoning.png Mistral 8x22B reasoning %}  \n\n{% asset_img mistral_8_22b_multiling.png Mistral 8x22B  %}  \n\n{% asset_img mistral_8_22b_code.png Mistral 8x22B  %}  \n\n#   \n\n- MoEdenseMoE  \n- MoEMoE  \n- denseMoE  \n-   \n- GShardSwitch Transformer  \n- MoEMoE  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n# Reference  \n1Adaptive Mixtures of Local Experts https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf  \n2Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer https://arxiv.org/abs/1701.06538  \n3GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding https://arxiv.org/abs/2006.16668  \n4Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity https://arxiv.org/abs/2101.03961  \n5GLaM: Efficient Scaling of Language Models with Mixture-of-Experts https://arxiv.org/abs/2112.06905  \n6ST-MoE: Designing Stable and Transferable Sparse Expert Models https://arxiv.org/abs/2202.08906  \n7DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models https://arxiv.org/abs/2401.06066  \n8Introducing DBRX: A New State-of-the-Art Open LLM https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm  \n9Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters https://qwenlm.github.io/zh/blog/qwen-moe/  \n","source":"_posts/cs/nlp/2024/03/MoE-.md","raw":"---\ntitle: MoE\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - MoE\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 44e38c1b\ndate: 2024-03-30 09:56:05\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n202434MoEQwen1.5-MoEDBRXJambaMistral  \n\nMoE  \n\n<center>\n\n|  |  |  |\n| :----: | :----: | :----: |\n| GPT4 | 20233 | 236George HotzGPT48220B |\n| Mistral-87B | 202312 | Mistral AI |\n| LLAMA-MoE | 202312 | github |\n| DeepSeek-MoE | 20241 | MoE |\n| abab6 |20241 | MiniMaxMoE |\n| 2.0 | 20242 |  |\n| Step-2 | 20243 |  |\n| MM1 | 20243 | MoE |\n| Grok-1 | 20243 | X |\n| Qwen1.5-MoE-A2.7B| 20243 |  |\n| DBRX | 20243 | Databricks |\n| Jamba | 20243 | AI21 |\n| Mistral-822B | 20244 | Mistral AI |\n| WizardLM-2-822B | 20244 |  |\n| 3.0 | 20244 | 400BMoE |\n| Arctic | 20244 | Snowflake480BDense-MoE Hybrid |\n\n</center>  \n\nMoEMoE  \n\n{% asset_img xiaomi_moe.jpg MoE %}  \n\nMoE  \n\nMoEMoEMoE  \n\n20244DeepSeek-MoEQwen1.5-MoE\n\n#   \n\nMoE  \n\nMoEGoogle\n\n##   \n\nMoE1991[Adaptive Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)Geoffrey HintonMichael I. JordanMoE1988  \n\n>This idea was first presented by Jacobs and Hinton at the Connectionist Summer School in Pittsburg in 1988.  \n\nMoEMoE  \n\n## RNN  \n\nGoogle20171[Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)MoELSTM137B128kLSTM  \n\n## Transformer  \n\n1. 20206Google[GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)MoEencoder-decodertransformerFFNMoE12.5B600BMoE2048  \n\n2. 20211Google[Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) T5encoder-decoderFFNMoErouting1.6Tswitch transformerSwitch Transformersscaling  \n\n3. 20222Google[ST-MoE: Designing Stable and Transferable Sparse Expert Models](https://arxiv.org/abs/2202.08906)encoder-decoderMoE269B32BST-MoEMoESwitch Transformer  \n\n## GPT  \n\n1. 202112GoogleGLaM[GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://arxiv.org/abs/2112.06905)1.2Tdecoder-onlyencoder-decoderdecoder-onlyGoogle  \n\n2. 20241[DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/abs/2401.06066)2312DeepSeekMoE  \n\n3. 2024DatabricksDBRXQwen1.5-MoE-A2.7BMistral AIMistral-8x22B  \n\n#   \n\nGeoffrey HintonMichael I. Jordan[Adaptive Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)MoE  \n\n1.   \n\n  \n\n  \n\nMoEexpert  \n\nMoEvowel discrimination taskMoEaeiou  \n\n2.   \n\nMoEexpert networkgating networkexpertgating networkexpertexpertstochastictruefalse  \n\n{% asset_img vanilla_moe.png Vanilla MoE %}  \n\n3.   \n\nMoEideaJacobsHinton1988lossensembleexpertexpertexpertresidual  \n\ncase $c$ $d^c$ ground truth $i$ expert $o_{i}^c$$p_{i}^c$ gating network $i$ expert $E^{c}$ \n\n$$E^{c}=\\left|\\left|d^{c}-\\sum_{i}p_{i}^{c}o_{i}^{c}\\right|\\right|^{2}$$\n\nexpert  \n\nexpertexpertexpert  \n\nexpertexpertexpertexpertgating network  \n\n  \n\nHintonJordanlossexpert  \n\ngating networkexpert  \n\n$$E^{c}=\\langle\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2\\rangle=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}$$\n\nexpertexpertexpert  \n\nlosslocalizationcasegating networkexpertgating networkexpert  \n\nlocalizationexpert  \n\nexpertexpertgating networkexpert error+-  \n\nexpert0  \n\n4. \n\nlosslossloss  \n\n$$\\text{loss}E^{c}=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}$$  \n\n$$\\text{loss}E^c=-log\\sum_ip_i^ce^{-\\frac12\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}$$  \n\nlossloss  \n\n$$\\text{loss}\\frac{\\partial E^c}{\\partial\\mathbf{o}_i^c}=-2p_i^c(\\mathbf{d}^c-\\mathbf{o}_i^c)$$  \n\n$$\\text{loss}\\frac{\\partial E^c}{\\partial\\mathbf{o}_i^c}=-\\left[\\frac{p_i^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}}{\\sum_jp_j^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_j^c\\|^2}}\\right](\\mathbf{d}^c-\\mathbf{o}_i^c)$$  \n\nlossloss $i$ expertexpertexpert $i$ casegating networklosscaseexpertlosslossexpertlocalizationexpert  \n\nBTWloss  \n\nMoE  \n\n# LSTM MoE  \n\nGoogle20171\n[OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER](https://arxiv.org/abs/1701.06538)MoELSTM137BLSTM7  \n\n1991  \n\n## \n\nTransformer  \n\nconditional computationconditional computationMoE  \n\n  \n\n- MoEexpertbatch sizebatch size  \nbatch size3216expertexpert2batch sizebatch sizebatch size  \n-   \nNLP  \n-   \n  \n-   \nGPU  \n- GPU  \nGPUbranchingif/elseMoEgating network  \n\n  \n\n## \n\n1.   \n\n  \n\nLSTMMoEembedding  \n\n{% asset_img rnn_moe.png LSTM MoE %}  \n\nexpertfeed-forward neural networknexpertgating networkn  \n\n$$\\begin{aligned}y=\\sum_{i=1}^nG(x)_iE_i(x)\\end{aligned}$$  \n\n$E_{i}(x)$  $i$ expert$G(x)_{i}$ gating network $i$ expert  \n\n $G(x)_{i}$ 0expert  \n\nexperttwo-level hierarchical MoEgating networkgating networkexpertgating networkexpertword2vechierarchical softmax  \n\n2. gating network  \n\ngating network  \n\nsoftmaxgating function  \n\n$$\\begin{aligned}G_\\sigma(x)=Softmax(x\\cdot W_g)\\end{aligned}$$  \n\ntopkksoftmax0expert  \n\nsparsitytopkgating function  \n\nGaussian noisenoise  \n\n$$\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}$$  \n\n$$KeepTopK(v,k)_i=\\begin{cases}v_i&\\text{if }v_i\\text{ is in the top }k\\text{ elements of }v.\\\\-\\infty&\\text{otherwise.}\\end{cases}$$  \n\n$$\\begin{aligned}H(x)_i=(x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\end{aligned}$$  \n\nnoisesoftplusReLU  \n\n{% asset_img softplus.png softplus %}  \n\n  \n\n##   \n\nMoEgating networkexpertexpert  \n\n  \n\nhard constraintexperthard constraintsoft constraint  \n\nexpert  \n\n$$Importance(X)=\\sum_{x\\in X}G(x)$$  \n\n$G(x)$ gating networkexpert  \n\n $L_{importance}$$L_{importance}$   \n\n$$L_{importance}(X)=w_{importance}\\cdot CV(Importance(X))^2$$  \n\n $w_{importance}$ 0.1CVcoefficient of variation  \n\ncoefficient of variation $\\sigma$   $\\mu$   \n\nMoEexpertexpertgating $L_{importance}$   \n\n $L_{importance}$  $L_{importance}$  $L_{importance}$   \n\nexpertexpertgating  \n\n $L_{load}$ expert  \n\nexpertback propagation $L_{load}$ expert  \n\nMoE $H(x)$ KeepTopK  \n\n$$\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}$$  \n\n$$\\begin{aligned}H(x)_i=(x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\end{aligned}$$  \n\n $kth\\_excluding(H(x),k,i)$ $H(x)$  $i$  $k$  $P(x,i)$ noise $i$ noise $kth\\_excluding(H(x),k,i)$   \n\n$$\\begin{aligned}P(x,i)=Pr\\Big((x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\\\>kth\\_excluding(H(x),k,i)\\Big)\\end{aligned}$$  \n\nnoise $i$  $i$ $P(x,i)$   \n\n$$\\begin{aligned}P(x,i)&=\\Phi\\Big(\\frac{(x\\cdot W_g)_i-kth\\_excluding(H(x),k,i)}{Softplus((x\\cdot W_{noise})_i)}\\Big)\\end{aligned}$$  \n\n $\\Phi$ CDF  \n\n $i$ expert  \n\n$$\\begin{aligned}Load(X)_i=\\sum_{x\\in X}P(x,i)\\end{aligned}$$  \n\nexpert  \n\n$$L_{load}(X)=w_{load}\\cdot CV(Load(X))^2$$  \n\n$w_{load}$ 0.1  \n\n $L_{importance}(X)$$Load(X)$   \n\nexpert $W_g$   $W_{noise}$ 0  \n\n  \n\n{% asset_img rnn_moe_load_function.png  %}  \n\n  \n\n##   \n\n1.   \n\n  \n\n1batch size  \n\nexpertbatch sizenexpertkbatch sizebexpertbatch sizekb/nexpertbatch size  \n- batchbatchMoEexpertdexpertkbd/nbatch size\n- LSTMbatch size\n\n2  \n\n  \n\nexpertinputoutput[input_size, hidden_size][hidden_size, output_size]GPU1000hidden_sizeexpert1000  \n\n2.  &   \n\nMoEdense4/32/256expertflat MoE256/1024/4096experthierarchical MoEexpert1Mflat4experthierarchical MoEgating2  \n\nppldenseMoEMoE\n\n{% asset_img rnn_moe_perf.png  %}  \n\n3.   \n\n4Bdiminishing returns  \n\n + 100B token32, 256, 10244096, 16384, 65536, 131072expertMoE137B  \n\n  \n\n{% asset_img rnn_moe_137b.png 137 %}  \n\n  \n\n4. Expert Specialization  \n\nMoE\n\ntokenspecialization  \n\n{% asset_img rnn_moe_specilized.png RNN MoE  %}  \n\n# GShard\n\n1. \n\n2018Berttransformer20206GoogleGShard: Scaling Giant Models with Conditional Computation and Automatic ShardingMoEencoder-decodertransformerMoE  \n\nGShardMoE600B  \n\n{% asset_img gshard_moe_family.png GShard MoE family %}  \n\nexpertLSMT MoE -- expert24expertChatGPTBertGPT  \n\nGShardMoE  \n\n2. \n\n  \n\nGoogleencoder-decoder transfomerGShardencoder-decoder transfomer  \n\nGShardencoderdecoderFFNMoENN/2MoE  \n\n{% asset_img gshard_model.png GShard %}  \n\ntop-2 expert  \n\nGShardLSTM MoEgating functionauxiliary loss function  \n\nMoE\n\n$$\\begin{aligned}\n\\mathcal{G}_{s,E}& =\\mathrm{GATE}(x_s)  \\\\\n\\mathrm{FFN}_e(x_s)& =wo_e\\cdot\\text{ReLU}(wi_e\\cdot x_s)  \\\\\ny_{s}& =\\sum_{e=1}^E\\mathcal{G}_{s,e}\\cdot\\mathrm{FFN}_e(x_s) \n\\end{aligned}$$\n\n $x_s$ MoEtoken$w_i$  $w_o$ $\\mathcal{G}_{s}$ gating function\n\nGShardgating function12  \n\nNtokenEexpertNEgating function  \n\ngating function  \n\n1 expert capacity  \n\nexperttokenexperttoken2N/E  \n\nexpert capacityGATE()expert $c_e$ tokentokenexpert  \n\n2 Local group dispatching  \n\ntokenG2N/EG  \n\nbatchbatchbatchgroupall2allgroup  \n\ngroupgradient accumulation  \n\n3 Auxiliary loss  \n\ngatingLSTM MoE  \n\n$$\\ell_{aux}=\\frac1E\\sum_{e=1}^E\\frac{c_e}S\\cdot m_e$$  \n\n$S$ token$E$ $c_e$  $e$ token$m_e$  $e$ expert $S$ token  \n\n $\\frac{c_e}S$  $\\frac{c_e}S$  $m_e$  $m_e$  $e$ expert $S$ tokenloss  \n\nloss  \n\ngating  \n\n{% asset_img gshard_algo_1.png GShard gating  %}  \n\n4 Random routing  \n\ntop-2 experttop-1  \n\ntop-1g2  \n\n3.   \n\n\n\n{% asset_img gshard_perf.png GShard %}  \n\n# Switch Transformer\n\n20224ChatGPTGoogleSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity2021GoogleSwitch Transformer  \n\nSwitch TransformerGShardencoder-decoderT51.6T2048expert  \n\nSwitch Transformer  \n\nSwitch TransformerSwitch TransformerFLOPS/token  \n\nSwitch Transformer  \n\n1TransformerMoESwitch Transformer  \n\n2MoE to denseMoEdenseMoE99%dense  \n\n3  \n- bf16MoE  \n- MoE  \n-   \n\n41TMoE  \n\n5101  \n\n6FLOPS/tokenSwitch Transformer  \n\n##   \n\nSwitch TransformerGShardtransformerFFNMoE  \n\n{% asset_img switch_transformer_structure.png Switch Transformer  %}  \n\nSwitch Transformergating functionSwitch Transformerrouting  \n\nkexpertSwitch Transformergating1expertk=1MoESwitch layer  \n\nroutingrouter  \n\n##   \n\nGShardSwitch Transformerexpert capacityexpertbatchtoken  \n\ntokenexpertoverflowtokenGShard  \n\nSwitch Transformercapacity factor  \n\n$$\\text{expert capacity}=\\left(\\frac{\\text{tokens per batch}}{\\text{number of experts}}\\right)\\times\\text{capacity factor}.$$  \n\ncapacity factorexperttokenoverflow\n\ncapacity factor  \n\n{% asset_img switch_transformer_diff_expert_capacity.png expert capacity %}  \n\nexpert capacity\n\ncapacity factor1overflowcapacity factor  \n\nexpertoverflowMoESwitch Transformer128  \n\ncapacity factoroverflow  \n\n{% asset_img switch_transformer_capacity_effect.png expert capacity %}  \n\ntokenscalingoverflow  \n\n  \n\n $N$ expert $T$ tokenbatch $\\mathcal{B}$ \n\n$$\\begin{aligned}\\text{loss}&=\\alpha\\cdot N\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}$$  \n\n$f_{i}$  $i$ experttoken  \n\n$$\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax }p(x)=i\\}\\end{aligned}$$  \n\n$P_i$ batchtoken$i$ expert  \n\n$$\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}$$  \n\nGShard  \n\n$f$  $P$  $1/N$  \n\n$\\alpha$ 1e-51e-11e-2  \n\n $\\sum_{i=1}^N(f_i\\cdot P_i)=\\sum_{i=1}^N(\\frac1N\\cdot\\frac1N)=\\frac1N$loss $N$expertloss  \n\n## \n\n1. trick  \n\n1bf16  \n\nbf16routing function  \n\nroutingsoftmaxexponentialrounding errorrouting  \n\n2  \n\n $\\mu=0$$\\sigma=\\sqrt{s}/n$sne.g. fan-in  \n\nTransformers=1.010  \n\n{% asset_img switch_transformer_init.png  %}  \n\n3dropout  \n\nSwitch Transformerdropout  \n\n{% asset_img switch_transformer_dropout.png dropout %}  \n\ndropoutdense0.1expertdropout  \n\n2. scaling  \n\nSwitch Transformerscaling  \n\n1Step-Basis  \n\nstepexpert  \n\nstepstep  \n\n{% asset_img switch_transformer_scaling_step.png step scaling %}  \n\n2Time-Basis  \n\nSwitch TransformerstepSwitch Transformerdense  \n\nSwitch TransformerdenseSwitch Transformerdensedense1/7  \n\n{% asset_img switch_transformer_scaling_time.png time scaling %}  \n\n3dense\n\nSwitch TransformerdenseSwitch Transformerdense  \n\nStep-BasisTime-Basis64Switch TransformerT5-LargestepSwitch Transformer  \n\n{% asset_img switch_transformer_scaling_dense.png dense %}  \n\n3. SFT  \n\nGLUESuperGLUEdense  \n\neval  \n\n{% asset_img switch_transformer_sft_result.png sft %}  \n\n4.   \n\nSwitch TransformerBTdense  \n\n  \n- Switch Transformerdense  \n- label25%75%ground truth  \n\ndensedensedenseSwitch Transformer30%  \n\n{% asset_img switch_transformer_distill.png  %}  \n\n99%  \n\n{% asset_img switch_transformer_distill_diff_model.png  %}  \n\nSuperGLUE  \n\n{% asset_img switch_transformer_distill_sft.png sft %}  \n\n# GLaM\n\n1. \n\n202112GoogleGLaM: Efficient Scaling of Language Models with Mixture-of-Experts1.2T64token96.6BMoE  \n\nSwitch TransformerGLaM1.6T token  \n\n  \n\n{% asset_img glam_related_model.png glam %}  \n\nGPT-3175BGPT-3NLPGPT-3  \n\n{% asset_img glam_compare_gpt3.png glamgpt3 %}  \n\n{% asset_img glam_compare_gpt3_2.png glamgpt3 %}  \n\n2. \n\nSwitch TransformerFFNMoESwitch TransformerGLaMexpert  \n\n{% asset_img glam_model.png glam %}  \n\n  \n\n1  \n\nXLNET  \n\n2\n\n> In the non-MoE Transformer feed-forward sub-layers, we replace the first linear projection and the activation function with the Gated Linear Unitwhich computes the component-wise product of two linear transformation of the input, followed by a Gaussian Error Linear Unit.  \n\n3. \n\ntrick  \n\n1Lingvo: a modular and scalable framework for sequence-to-sequence modelingNaNInf  \n\n2BPNaNInfcheckpointNaNInf  \n\nMoE  \n\n{% asset_img glam_family.png glam %}  \n\nGLaMdense  \n\n{% asset_img glam_perf.png glam %}  \n\nGLaM MoEdense  \n\n# ST-MoE  \n\n20222GoogleST-MOE: DESIGNING STABLE AND TRANSFERABLE SPARSE EXPERT MODELSST-MoEMoEMoE  \n\nST-MoE269B32B denseStable Transferable Mixture-of-ExpertsST-MoE-32B  \n\nMoEST-MoESwitch Transformer1MoE  \n\nST-MoE4B269BST-MoE  \n\n{% asset_img st_moe_models.png ST-MoE %}  \n\n##   \n\n  \n\n1.   \n\n  \n\n  \n\n> Some architectural improvements involve more multiplications than additions or do not sum many items at once\n\n1GELU Gated Linear Units (GEGLU)  \n\nGLUcomponent-wiseGELU-Linear FFNtransformerReLU FFN  \n\n$$\\begin{aligned}FFN_{GEGLU}(x,W,V,b,c)=GELU(xW+b)\\odot(xV+c)\\end{aligned}$$  \n\n  \n\n2RMSNorm  \n\nRMSNorm $g$  \n\n$$y_i=\\frac{x_i}{\\sqrt{\\frac1d\\sum_{i=1}^dx_i^2}}\\cdot g_i$$  \n\nST-MoEGEGLURMSNorm  \n\n{% asset_img st_moe_remove_multiplications.png  %}  \n\n  \n\n3dense  \n\nST-MoEexpertdensedense\n\n{% asset_img st_moe_more_dense_layer.png dense %}  \n\n4bias\n\nFFNbias B  \n\n$$\\text{FFN}_{\\text{GEGLU}}+\\text{Add Bias}(x)=[(\\text{GELU}(xW_{11})\\odot xW_{12})+B]W_2$$  \n\n$$\\mathrm{FFN}_{\\mathrm{GEGLU}}+\\mathrm{Mult~Bias}(x)=[(\\mathrm{GELU}(xW_{11})\\odot xW_{12})\\odot B]W_2$$  \n\n  \n\n  \n\n2. noise  \n\nST-MoE  \n\ninput-jitterrouterlogits[1e-2, 1e2]  \n\n{% asset_img st_moe_more_add_noise.png noise %}  \n\nnoise  \n\n  \n\n3.   \n\nactivationgradient  \n\nST-MoE269B  \n\nST-MoErouter z-loss  \n\n$$L_z(x)=\\frac{1}{B}\\sum_{i=1}^B\\left(\\log\\sum_{j=1}^Ne^{x_j^{(i)}}\\right)^2$$  \n\n$B$ token$N$ $x\\in\\mathcal{R}^{B\\times N}$ router  \n\nz-lossrouterlogitsz-loss  \n\n{% asset_img st_moe_z_loss_result.png z-loss %}  \n\nST-MoEz-loss  \n\nz-loss $c_z$   \n\n$$L_{tot}=L_{CE}+c_BL_B+c_zL_Z$$  \n\nST-MoE$c_z=0.001$  \n\n$L_B$  auxiliary load balance lossST-MoEGShard/Switch Transformer  \n\n$$\\begin{aligned}\\text{loss}&=\\alpha\\cdot N\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}$$  \n\n$$\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax }p(x)=i\\}\\end{aligned}$$  \n\n$$\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}$$  \n\n$N$  $\\mathcal{B}$ $T$ tokenbatch$f_{i}$  $i$ experttoken$P_i$ batchtoken$i$ expert  \n\n4. \n\nfloat32bfloat16bfloat16allreducebfloat16float32  \n\nST-MoE-32BallreduceST-MoEallreducefloat32  \n\nbfloat16float32  \n\n{% asset_img st_moe_round_error.png bf16 %}  \n\nz-loss  \n\nMoErouter  \n\nST-MoE1/5token  \n\nsoftmaxMoE  \n\n## \n\ndensescaling lawMoEdense \n \n1expert  \n\n2routing  \n\n3  \n\n4  \n\nMoEscaling lawUnified scaling laws for routed language models  \n\n1. expert  \n\nST-MoE8/16/32<1%>256  \n\n>1<=1  \n\n2. routingcapacity factor  \n\ncapacity factor  \n\n{% asset_img st_moe_capacity_factor.png capacity factor %}  \n\n  \n\n1capacity factor  \n\n2capacity facotr  \n \n3expertcapacity factor  \n\ncapacity factorcapacity factor  \n\ncapacity factor  \n\n{% asset_img st_moe_capacity_factor_speed.png capacity factor %}  \n\n##   \n\n1. ST-MoE  \n\nST-MoE-32BST-MoE-32B  \n\n{% asset_img st_moe_perf.png capacity ST-MoE-32B %}  \n\n2. Expert Specialization  \n\ndecodertokenencoder  \n\n{% asset_img st_moe_encoder_specialization.png encoder %}  \n\nencodertoken  \n\n{% asset_img st_moe_multiling_specialization.png  %}  \n\n# DeepseekMoE\n\n20241DeepseekMoEMoEDeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language ModelsDeepSeekMoE  \n\nDeepSeekMoEMoE2  \n\n1expertexpert  \n\n2expertexpertshared expert  \n\nexpert(specialization)  \n\nDeepSeekMoE2BMoE16BMoEDeepSeekMoE-16B40GB  \n\nDeepSeekMoE-2B2BDeepSeekMoE-16B7B40%  \n\nDeepSeekMoE-16B  \n\n{% asset_img ds_moe_perf.png deepseek moe %}  \n\nDeepSeekMoE-2B16B  \n\nDeepSeekMoE-145BMoEDeepSeek-67B  \n\n##   \n\nMoEmixture of expertmotivationexpert  \n\n1991expert  \n\nMoEknowledge hybridityknowledge redundancy  \n\n1  \n\nexpertexpert  \n\n2  \n\nexpertexpertexpertexpert8expertexpert  \n\n(expert specialization)MoE  \n\nexpertnon-overlap & foucusd knowledge  \n\nDeepSeekMoE2  \n\n1Fine-Grained Expert Segmentation  \n\nexpertexpertexpertspecialization16expert2120expert1/464expert8 $\\binom{64}8=4,426,165,368$    \n\n2Shared Expert Isolation  \n\nexpertcommon knowledgeexpertexpertexpertexpert  \n\nMoEFine-Grained Expert SegmentationShared Expert Isolation  \n\n{% asset_img ds_moe_structure.png deepseek moe  %}  \n\nexpert isolation20221DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale  \n\nMoEexpert $N$expert $K$DeepSeekMoEexpert $1/m$DeepSeekMoE $mN$ expertexpert $mK$ $T$ $L$ $e_i^l$  $i$ expertDeepSeekMoElayernorm  \n\n$$\\mathbf{u}_{1:T}^l=\\text{Self-Att}\\left(\\mathbf{h}_{1:T}^{l-1}\\right)+\\mathbf{h}_{1:T}^{l-1}$$  \n\n$$\\mathbf{h}_t^l=\\sum_{i=1}^{mN}\\left(g_{i,t}\\text{ FFN}_i\\left(\\mathbf{u}_t^l\\right)\\right)+\\mathbf{u}_t^l$$  \n\n$$g_{i,t}=\\begin{cases}s_{i,t},&s_{i,t}\\in\\text{Topk}(\\{s_{j,t}|1\\leqslant j\\leqslant mN\\},mK)\\\\0,&\\text{otherwise,}\\end{cases}$$  \n\n$$s_{i,t}=\\mathrm{Softmax}_i\\left({\\mathbf{u}_t^l}^T\\mathbf{e}_i^l\\right)$$  \n\n## \n\nMoEgating  \n\n1routing collapsegatingexpert  \n\n2  \n\nrouting collapseDeepSeekMoEexpert-level balance loss\n\n$$\\begin{aligned}\n\\mathcal{L}_{\\mathrm{ExpBal}}& =\\alpha_1\\sum_{i=1}^{N'}f_iP_i\n\\end{aligned}$$  \n\n$$\\begin{aligned}\nf_{i}& =\\frac{N^{\\prime}}{K^{\\prime}T}\\sum_{t=1}^T\\mathbb{1}(\\text{Token }t\\text{ selects Expert }i)\n\\end{aligned}$$  \n\n$$\\begin{aligned}\nP_{i}& =\\frac1T\\sum_{t=1}^Ts_{i,t} \n\\end{aligned}$$  \n\n$\\alpha_1$ expert-level balance factor  \n\n $f_i$  $P_i$ Switch Transformer  \n\nSwitch Transformer $f_i$  $i$ experttokenDeepSeekMoE $N'/K'$  $N'=mN-K_s$$K'=mK-K_s$$K_s$ expertDeepSeekMoE $f_i$ Switch Transformer  \n\n$N'/K'$ expertloss  \n\n$P_i$ token $i$ expertSwitch Transformer  \n\n $f_i$ $P_i$   \n\nDeepSeekMoEdevice-level balance loss\n\n$$\\begin{aligned}\n\\mathcal{L}_{\\mathrm{DevBal}}& =\\alpha_2\\sum_{i=1}^Df_i'P_i'\n\\end{aligned}$$\n\n$$\\begin{aligned}\nf_i^{\\prime}& =\\frac1{|\\mathcal{E}_i|}\\sum_{j\\in\\mathcal{E}_i}f_j\n\\end{aligned}$$\n\n$$\\begin{aligned}\nP_{i}^{\\prime}& =\\sum_{j\\in\\mathcal{E}_i}P_j\n\\end{aligned}$$\n\n$\\alpha_2$ device-level balance factor  \n\n$\\mathcal{E}_i$  $i$ \n\ndevice-level balance lossexpert-level balance loss $f_i$  $P_i$ expert  \n\nexpert64expert8tokentokenexpert  \n\nexpert\n\n## \n\n1. \n\n100B tokenDeepSeekMoE-2BBPE8k\n\nDeepSeekMoE-2B0.006multi-head attention0.3B\n\n{% asset_img ds_model_param.png  %}  \n\nrelative expert sizeDeepSeekMoEexpertFFN\n\n  \n\n<center>\n\n|  |  |\n| :----: | :----: |\n| optimizer | AdamW |\n| adam_beta_1 | 0.9 |\n| adam_beta_2 | 0.95 |\n| adam_weight_decay | 0.1 |\n| warmup schedule | linear |\n| warmup step | 2000 |\n| max lr | 1.08e-3 |\n| dropout | 0 |\n| sequence length | 2k |\n| batch size | 2k |\n| total step | 25,000 |\n\n\n</center>  \n\n  \n- expertGPUdevice-level balance loss  \n- expert-level balance factor0.01  \n- 80%0.31690%0.316  \n\n100BDeepSeekMoE-2Bbenchmark4densehash layermoeHash layers for large sparse modelsSwitch TransformerGShard\n\n{% asset_img ds_moe_comparison.png deepseek moe %}  \n\n  \n- Hash LayerSwitch Transformerdense  \n- GSshardHash LayerSwitch Transformer  \n- DeepSeekMoEGShard  \n\nDeepSeekMoEdenseGShardDeepSeekMoE-2B\n\ndenseGShard161.5DeepSeekMoE-2B  \n\n{% asset_img ds_moe_upper_bound_2b.png deepseek moe upper bound %}  \n\nDeepSeekMoEDeepSeekMoE-13B, 1.21.5GShardDeepSeekMoE-13Bmatch  \n\n{% asset_img ds_moe_upper_bound_13b.png deepseek moe upper bound %}  \n\n2. DeepSeekMoE\n\nDeepSeekMoEshared expertfine-grained expertexpert\n\n{% asset_img ds_moe_ablation.png deepseek moe upper bound  %}  \n\n1\n\n2\n\n364expert1/2/4pileloss1.808,1.806,1.8111:32+6\n\n3. expert specialization\n\nDeepSeekMoEexpert specialization\n\n1DeepSeekMoE-2B1.5GShardtop  \n\n\n\n{% asset_img ds_moe_expert_specialization.png  %}  \n\nDeepSeekMoEDeepSeekMoE  \n\n2DeepSeekMoEloss\n\n3GShardDeepSeekMoE  \n\n{% asset_img ds_moe_less_activated_expert.png  %}  \n\n132b2+6GShardDeepSeekMoE\n\n{% asset_img ds_2b_less_expert.png 2B %}  \n\n1. DeepSeekMoE-16B  \n\nDeepSeekMoE-16B2TLLAMA2-7B100k  \n\n{% asset_img ds_model_param.png  %}  \n\nMoE  \n\nMoEloss  \n\nDeepSeekMoE-16B6426gating functiontoken8token16.4B2.8B  \n\ndimension  \n\n  \n- lr = 4.2e-4  \n- 80%90%lr0.316  \n- batch size = 4.5k4kbatch18M token2T10.6w  \n- pipeline parallelism\n\nexpert level balance loss0.001  \n\nDeepSeekMoE-16BDeepSeek-7B  \n\n{% asset_img ds_16b_perf_1.png DeepSeek-7B %}  \n\nDeepSeekMoE-16BLLAMA2-7B  \n\n{% asset_img ds_16b_perf_2.png LLAMA2-7B %}  \n\n5. DeepSeekMoE-145B  \n\n245BtokenDeepSeekMoE-145BDeepSeek-67B  \n\n{% asset_img ds_moe_145b.png 145b %}  \n\n# DBRX\n\n2024327DatabricksDBRX132B36BMoE\n\nDBRXRoPEGLUGQAfine-grained expert16token4MixtralGrok-182DBRX  \n\nDBRX32k12TtokenDBRX3072H100post-trainingred-team3  \n\nDBRXGPT-3.5Gemini 1.0 ProCodeLLaMA-70B  \n\n{% asset_img dbrx_perf.png DBRX %}  \n\nDBRX  \n\n{% asset_img dbrx_infer_efficiency.png  %}  \n\n# Qwen1.5-MoE \n\n2024328Qwen1.5-MoE-A2.7B2.7BQwen1.5-7B  \n\nQwen1.5-MoE-A2.7BDeepSeekMoEDBRXfine-grained expert64token84  \n\nQwen1.5-MoE-A2.7BQwen-1.8B  \n\nQwen1.5-MoE-A2.7B  \n\n{% asset_img qwen1.5_moe_perf.png Qwen1.5-MoE-A2.7B %}  \n\nQwen1.5-MoE-A2.7Bnon-embedding7B  \n\n{% asset_img qwen1.5_moe_params.png Qwen1.5-MoE-A2.7B %}  \n\nQwen1.5-MoE-A2.7BQwen1.5-7B75%  \n\nA100-80GvLLMQwen1.5-7BQwen1.5-MoE-A2.7B  \n\n/token1000token1000TPSthroughput  \n\n{% asset_img qwen1.5_moe_tps.png Qwen1.5-MoE-A2.7B TPS %}  \n\nMoEdenseQwen1.5-MoE-A2.7BQwen1.5-7B1.74  \n\n# Mistral\n\n## Mistral 8x7B\n\n20231211Mistral AIMistral-8x7Btoken82  \n\nMistral-8x7B32kLLAM2-70BGPT-3.5  \n\n{% asset_img mistral_8_7b_perf.png Mistral 8x7B %}  \n\nMistral-8x7BLLAM2-70B6  \n\nLLAM2-13B  \n\n{% asset_img mistral_8_7b_active_perf.png Mistral 8x7B %}  \n\n## Mistral 8x22B\n\n2024417Mistral AIMistral-8x22B141B39BMoE  \n\nMistral-8x22BMistral-8x7B32k64kMistral-8x22Bfunction call  \n\n  \n\n{% asset_img mistral_8_22b_reasoning.png Mistral 8x22B reasoning %}  \n\n{% asset_img mistral_8_22b_multiling.png Mistral 8x22B  %}  \n\n{% asset_img mistral_8_22b_code.png Mistral 8x22B  %}  \n\n#   \n\n- MoEdenseMoE  \n- MoEMoE  \n- denseMoE  \n-   \n- GShardSwitch Transformer  \n- MoEMoE  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n# Reference  \n1Adaptive Mixtures of Local Experts https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf  \n2Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer https://arxiv.org/abs/1701.06538  \n3GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding https://arxiv.org/abs/2006.16668  \n4Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity https://arxiv.org/abs/2101.03961  \n5GLaM: Efficient Scaling of Language Models with Mixture-of-Experts https://arxiv.org/abs/2112.06905  \n6ST-MoE: Designing Stable and Transferable Sparse Expert Models https://arxiv.org/abs/2202.08906  \n7DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models https://arxiv.org/abs/2401.06066  \n8Introducing DBRX: A New State-of-the-Art Open LLM https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm  \n9Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters https://qwenlm.github.io/zh/blog/qwen-moe/  \n","slug":"cs/nlp/2024/03/MoE-","published":1,"updated":"2024-05-10T06:51:36.425Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmfs007pam4kgy84cg3k","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>202434MoEQwen1.5-MoEDBRXJambaMistral</p>\n<p>MoE</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">GPT4</td>\n<td style=\"text-align: center;\">20233</td>\n<td style=\"text-align: center;\">236George\nHotzGPT48220B</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Mistral-87B</td>\n<td style=\"text-align: center;\">202312</td>\n<td style=\"text-align: center;\">Mistral AI</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">LLAMA-MoE</td>\n<td style=\"text-align: center;\">202312</td>\n<td style=\"text-align: center;\">github</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">DeepSeek-MoE</td>\n<td style=\"text-align: center;\">20241</td>\n<td style=\"text-align: center;\">MoE</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">abab6</td>\n<td style=\"text-align: center;\">20241</td>\n<td style=\"text-align: center;\">MiniMaxMoE</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">2.0</td>\n<td style=\"text-align: center;\">20242</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Step-2</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">MM1</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">MoE</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Grok-1</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">X</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Qwen1.5-MoE-A2.7B</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">DBRX</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">Databricks</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Jamba</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">AI21</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Mistral-822B</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\">Mistral AI</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">WizardLM-2-822B</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">3.0</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\">400BMoE</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Arctic</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\">Snowflake480BDense-MoE\nHybrid</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>MoEMoE</p>\n<img src=\"/44e38c1b/xiaomi_moe.jpg\" class title=\"MoE\">\n<p>MoE</p>\n<p>MoEMoEMoE</p>\n<p>20244DeepSeek-MoEQwen1.5-MoE</p>\n<h1 id=\"\"></h1>\n<p>MoE</p>\n<p>MoEGoogle</p>\n<h2 id=\"\"></h2>\n<p>MoE1991<a href=\"https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf\">Adaptive\nMixtures of Local Experts</a>Geoffrey HintonMichael I.\nJordanMoE1988</p>\n<blockquote>\n<p>This idea was first presented by Jacobs and Hinton at the\nConnectionist Summer School in Pittsburg in 1988.</p>\n</blockquote>\n<p>MoEMoE</p>\n<h2 id=\"rnn\">RNN</h2>\n<p>Google20171<a href=\"https://arxiv.org/abs/1701.06538\">Outrageously Large Neural\nNetworks: The Sparsely-Gated Mixture-of-Experts\nLayer</a>MoELSTM137B128kLSTM</p>\n<h2 id=\"transformer\">Transformer</h2>\n<ol type=\"1\">\n<li><p>20206Google<a href=\"https://arxiv.org/abs/2006.16668\">GShard: Scaling Giant Models\nwith Conditional Computation and Automatic\nSharding</a>MoEencoder-decodertransformerFFNMoE12.5B600BMoE2048</p></li>\n<li><p>20211Google<a href=\"https://arxiv.org/abs/2101.03961\">Switch Transformers: Scaling\nto Trillion Parameter Models with Simple and Efficient Sparsity</a>\nT5encoder-decoderFFNMoErouting1.6Tswitch\ntransformerSwitch\nTransformersscaling</p></li>\n<li><p>20222Google<a href=\"https://arxiv.org/abs/2202.08906\">ST-MoE: Designing Stable and\nTransferable Sparse Expert\nModels</a>encoder-decoderMoE269B32BST-MoEMoESwitch\nTransformer</p></li>\n</ol>\n<h2 id=\"gpt\">GPT</h2>\n<ol type=\"1\">\n<li><p>202112GoogleGLaM<a href=\"https://arxiv.org/abs/2112.06905\">GLaM: Efficient Scaling of\nLanguage Models with\nMixture-of-Experts</a>1.2Tdecoder-onlyencoder-decoderdecoder-onlyGoogle</p></li>\n<li><p>20241<a href=\"https://arxiv.org/abs/2401.06066\">DeepSeekMoE: Towards Ultimate\nExpert Specialization in Mixture-of-Experts Language\nModels</a>2312DeepSeekMoE</p></li>\n<li><p>2024DatabricksDBRXQwen1.5-MoE-A2.7BMistral\nAIMistral-8x22B</p></li>\n</ol>\n<h1 id=\"\"></h1>\n<p>Geoffrey HintonMichael I. Jordan<a href=\"https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf\">Adaptive\nMixtures of Local Experts</a>MoE</p>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<p></p>\n<p>MoEexpert</p>\n<p>MoEvowel discrimination\ntaskMoEaeiou</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p>MoEexpert networkgating\nnetworkexpertgating\nnetworkexpertexpertstochastictruefalse</p>\n<img src=\"/44e38c1b/vanilla_moe.png\" class title=\"Vanilla MoE\">\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>MoEideaJacobsHinton1988lossensembleexpertexpertexpertresidual</p>\n<p>case <span class=\"math inline\">\\(c\\)</span>\n<span class=\"math inline\">\\(d^c\\)</span> ground truth <span class=\"math inline\">\\(i\\)</span> expert <span class=\"math inline\">\\(o_{i}^c\\)</span><span class=\"math inline\">\\(p_{i}^c\\)</span> gating network <span class=\"math inline\">\\(i\\)</span>\nexpert <span class=\"math inline\">\\(E^{c}\\)</span> </p>\n<p><span class=\"math display\">\\[E^{c}=\\left|\\left|d^{c}-\\sum_{i}p_{i}^{c}o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p>expert</p>\n<p>expertexpertexpert</p>\n<p>expertexpertexpertexpertgating\nnetwork</p>\n<p></p>\n<p>HintonJordanlossexpert</p>\n<p>gating networkexpert</p>\n<p><span class=\"math display\">\\[E^{c}=\\langle\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2\\rangle=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p>expertexpertexpert</p>\n<p>losslocalizationcasegating\nnetworkexpertgating\nnetworkexpert</p>\n<p>localizationexpert</p>\n<p>expertexpertgating\nnetworkexpert\nerror+-</p>\n<p>expert0</p>\n<ol start=\"4\" type=\"1\">\n<li></li>\n</ol>\n<p>losslossloss</p>\n<p><span class=\"math display\">\\[\\text{loss}E^{c}=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p><span class=\"math display\">\\[\\text{loss}E^c=-log\\sum_ip_i^ce^{-\\frac12\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}\\]</span></p>\n<p>lossloss</p>\n<p><span class=\"math display\">\\[\\text{loss}\\frac{\\partial\nE^c}{\\partial\\mathbf{o}_i^c}=-2p_i^c(\\mathbf{d}^c-\\mathbf{o}_i^c)\\]</span></p>\n<p><span class=\"math display\">\\[\\text{loss}\\frac{\\partial\nE^c}{\\partial\\mathbf{o}_i^c}=-\\left[\\frac{p_i^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}}{\\sum_jp_j^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_j^c\\|^2}}\\right](\\mathbf{d}^c-\\mathbf{o}_i^c)\\]</span></p>\n<p>lossloss <span class=\"math inline\">\\(i\\)</span>\nexpertexpertexpert <span class=\"math inline\">\\(i\\)</span>\ncasegating\nnetworklosscaseexpertlosslossexpertlocalizationexpert</p>\n<p>BTWloss</p>\n<p>MoE</p>\n<h1 id=\"lstm-moe\">LSTM MoE</h1>\n<p>Google20171 <a href=\"https://arxiv.org/abs/1701.06538\">OUTRAGEOUSLY LARGE NEURAL\nNETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS\nLAYER</a>MoELSTM137BLSTM7</p>\n<p>1991</p>\n<h2 id=\"\"></h2>\n<p>Transformer</p>\n<p>conditional\ncomputationconditional\ncomputationMoE</p>\n<p></p>\n<ul>\n<li>MoEexpertbatch sizebatch\nsize<br>\nbatch\nsize3216expertexpert2batch\nsizebatch\nsizebatch\nsize<br>\n</li>\n<li><br>\nNLP<br>\n</li>\n<li><br>\n<br>\n</li>\n<li><br>\nGPU<br>\n</li>\n<li>GPU<br>\nGPUbranchingif/elseMoEgating\nnetwork</li>\n</ul>\n<p></p>\n<h2 id=\"\"></h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<p>LSTMMoEembedding</p>\n<img src=\"/44e38c1b/rnn_moe.png\" class title=\"LSTM MoE\">\n<p>expertfeed-forward neural\nnetworknexpertgating networkn</p>\n<p><span class=\"math display\">\\[\\begin{aligned}y=\\sum_{i=1}^nG(x)_iE_i(x)\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(E_{i}(x)\\)</span>  <span class=\"math inline\">\\(i\\)</span> expert<span class=\"math inline\">\\(G(x)_{i}\\)</span> gating network <span class=\"math inline\">\\(i\\)</span> expert</p>\n<p> <span class=\"math inline\">\\(G(x)_{i}\\)</span>\n0expert</p>\n<p>experttwo-level hierarchical\nMoEgating networkgating\nnetworkexpertgating\nnetworkexpertword2vechierarchical\nsoftmax</p>\n<ol start=\"2\" type=\"1\">\n<li>gating network</li>\n</ol>\n<p>gating network</p>\n<p>softmaxgating\nfunction</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G_\\sigma(x)=Softmax(x\\cdot\nW_g)\\end{aligned}\\]</span></p>\n<p>topkksoftmax0expert</p>\n<p>sparsitytopkgating\nfunction</p>\n<p>Gaussian\nnoisenoise</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[KeepTopK(v,k)_i=\\begin{cases}v_i&amp;\\text{if\n}v_i\\text{ is in the top }k\\text{ elements of\n}v.\\\\-\\infty&amp;\\text{otherwise.}\\end{cases}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}H(x)_i=(x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\end{aligned}\\]</span></p>\n<p>noisesoftplusReLU</p>\n<img src=\"/44e38c1b/softplus.png\" class title=\"softplus\">\n<p></p>\n<h2 id=\"\"></h2>\n<p>MoEgating\nnetworkexpertexpert</p>\n<p></p>\n<p>hard\nconstraintexperthard\nconstraintsoft\nconstraint</p>\n<p>expert</p>\n<p><span class=\"math display\">\\[Importance(X)=\\sum_{x\\in\nX}G(x)\\]</span></p>\n<p><span class=\"math inline\">\\(G(x)\\)</span> gating\nnetworkexpert</p>\n<p> <span class=\"math inline\">\\(L_{importance}\\)</span><span class=\"math inline\">\\(L_{importance}\\)</span> </p>\n<p><span class=\"math display\">\\[L_{importance}(X)=w_{importance}\\cdot\nCV(Importance(X))^2\\]</span></p>\n<p> <span class=\"math inline\">\\(w_{importance}\\)</span>\n0.1CVcoefficient of variation</p>\n<p>coefficient of\nvariation\n<span class=\"math inline\">\\(\\sigma\\)</span>   <span class=\"math inline\">\\(\\mu\\)</span> </p>\n<p>MoEexpertexpertgating\n<span class=\"math inline\">\\(L_{importance}\\)</span> </p>\n<p> <span class=\"math inline\">\\(L_{importance}\\)</span>  <span class=\"math inline\">\\(L_{importance}\\)</span>\n <span class=\"math inline\">\\(L_{importance}\\)</span>\n</p>\n<p>expertexpertgating</p>\n<p> <span class=\"math inline\">\\(L_{load}\\)</span>\nexpert</p>\n<p>expertback\npropagation <span class=\"math inline\">\\(L_{load}\\)</span>\nexpert</p>\n<p>MoE <span class=\"math inline\">\\(H(x)\\)</span> KeepTopK</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}H(x)_i=(x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(kth\\_excluding(H(x),k,i)\\)</span>\n<span class=\"math inline\">\\(H(x)\\)</span>  <span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(P(x,i)\\)</span>\nnoise <span class=\"math inline\">\\(i\\)</span> noise <span class=\"math inline\">\\(kth\\_excluding(H(x),k,i)\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{aligned}P(x,i)=Pr\\Big((x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\\\&gt;kth\\_excluding(H(x),k,i)\\Big)\\end{aligned}\\]</span></p>\n<p>noise <span class=\"math inline\">\\(i\\)</span>\n <span class=\"math inline\">\\(i\\)</span>\n<span class=\"math inline\">\\(P(x,i)\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{aligned}P(x,i)&amp;=\\Phi\\Big(\\frac{(x\\cdot\nW_g)_i-kth\\_excluding(H(x),k,i)}{Softplus((x\\cdot\nW_{noise})_i)}\\Big)\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\Phi\\)</span>\nCDF</p>\n<p> <span class=\"math inline\">\\(i\\)</span>\nexpert</p>\n<p><span class=\"math display\">\\[\\begin{aligned}Load(X)_i=\\sum_{x\\in\nX}P(x,i)\\end{aligned}\\]</span></p>\n<p>expert</p>\n<p><span class=\"math display\">\\[L_{load}(X)=w_{load}\\cdot\nCV(Load(X))^2\\]</span></p>\n<p><span class=\"math inline\">\\(w_{load}\\)</span>\n0.1</p>\n<p> <span class=\"math inline\">\\(L_{importance}(X)\\)</span><span class=\"math inline\">\\(Load(X)\\)</span>\n</p>\n<p>expert\n<span class=\"math inline\">\\(W_g\\)</span>  <span class=\"math inline\">\\(W_{noise}\\)</span>\n0</p>\n<p></p>\n<img src=\"/44e38c1b/rnn_moe_load_function.png\" class title=\"\">\n<p></p>\n<h2 id=\"\"></h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<p>1batch size</p>\n<p>expertbatch\nsizenexpertkbatch\nsizebexpertbatch\nsizekb/nexpertbatch size<br>\n-\nbatchbatchMoEexpertdexpertkbd/nbatch\nsize - LSTMbatch\nsize</p>\n<p>2</p>\n<p></p>\n<p>expertinputoutput[input_size,\nhidden_size][hidden_size,\noutput_size]GPU1000hidden_sizeexpert1000</p>\n<ol start=\"2\" type=\"1\">\n<li> &amp; </li>\n</ol>\n<p>MoEdense4/32/256expertflat\nMoE256/1024/4096experthierarchical\nMoEexpert1Mflat4experthierarchical\nMoEgating2</p>\n<p>ppldenseMoEMoE</p>\n<img src=\"/44e38c1b/rnn_moe_perf.png\" class title=\"\">\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>4Bdiminishing\nreturns</p>\n<p> + 100B\ntoken32, 256, 10244096, 16384, 65536,\n131072expertMoE137B</p>\n<p></p>\n<img src=\"/44e38c1b/rnn_moe_137b.png\" class title=\"137\">\n<p></p>\n<ol start=\"4\" type=\"1\">\n<li>Expert Specialization</li>\n</ol>\n<p>MoE</p>\n<p>tokenspecialization</p>\n<img src=\"/44e38c1b/rnn_moe_specilized.png\" class title=\"RNN MoE \">\n<h1 id=\"gshard\">GShard</h1>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>2018Berttransformer20206GoogleGShard:\nScaling Giant Models with Conditional Computation and Automatic\nShardingMoEencoder-decodertransformerMoE</p>\n<p>GShardMoE600B</p>\n<img src=\"/44e38c1b/gshard_moe_family.png\" class title=\"GShard MoE family\">\n<p>expertLSMT MoE --\nexpert24expertChatGPTBertGPT</p>\n<p>GShardMoE</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<p>Googleencoder-decoder\ntransfomerGShardencoder-decoder\ntransfomer</p>\n<p>GShardencoderdecoderFFNMoENN/2MoE</p>\n<img src=\"/44e38c1b/gshard_model.png\" class title=\"GShard\">\n<p>top-2 expert</p>\n<p>GShardLSTM MoEgating\nfunctionauxiliary loss function</p>\n<p>MoE</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{G}_{s,E}&amp; =\\mathrm{GATE}(x_s)  \\\\\n\\mathrm{FFN}_e(x_s)&amp; =wo_e\\cdot\\text{ReLU}(wi_e\\cdot x_s)  \\\\\ny_{s}&amp; =\\sum_{e=1}^E\\mathcal{G}_{s,e}\\cdot\\mathrm{FFN}_e(x_s)\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(x_s\\)</span> MoEtoken<span class=\"math inline\">\\(w_i\\)</span>  <span class=\"math inline\">\\(w_o\\)</span>\n<span class=\"math inline\">\\(\\mathcal{G}_{s}\\)</span> gating\nfunction</p>\n<p>GShardgating\nfunction12</p>\n<p>NtokenEexpertNEgating\nfunction</p>\n<p>gating function</p>\n<p>1 expert capacity</p>\n<p>experttokenexperttoken2N/E</p>\n<p>expert capacityGATE()expert <span class=\"math inline\">\\(c_e\\)</span>\ntokentokenexpert</p>\n<p>2 Local group dispatching</p>\n<p>tokenG2N/EG</p>\n<p>batchbatchbatchgroupall2allgroup</p>\n<p>groupgradient\naccumulation</p>\n<p>3 Auxiliary loss</p>\n<p>gatingLSTM\nMoE</p>\n<p><span class=\"math display\">\\[\\ell_{aux}=\\frac1E\\sum_{e=1}^E\\frac{c_e}S\\cdot\nm_e\\]</span></p>\n<p><span class=\"math inline\">\\(S\\)</span> token<span class=\"math inline\">\\(E\\)</span> <span class=\"math inline\">\\(c_e\\)</span>  <span class=\"math inline\">\\(e\\)</span> token<span class=\"math inline\">\\(m_e\\)</span>  <span class=\"math inline\">\\(e\\)</span> expert <span class=\"math inline\">\\(S\\)</span> token</p>\n<p> <span class=\"math inline\">\\(\\frac{c_e}S\\)</span>\n <span class=\"math inline\">\\(\\frac{c_e}S\\)</span>  <span class=\"math inline\">\\(m_e\\)</span>  <span class=\"math inline\">\\(m_e\\)</span>  <span class=\"math inline\">\\(e\\)</span> expert <span class=\"math inline\">\\(S\\)</span>\ntokenloss</p>\n<p>loss</p>\n<p>gating</p>\n<img src=\"/44e38c1b/gshard_algo_1.png\" class title=\"GShard gating \">\n<p>4 Random routing</p>\n<p>top-2\nexperttop-1</p>\n<p>top-1g2</p>\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<img src=\"/44e38c1b/gshard_perf.png\" class title=\"GShard\">\n<h1 id=\"switch-transformer\">Switch Transformer</h1>\n<p>20224ChatGPTGoogleSwitch\nTransformers: Scaling to Trillion Parameter Models with Simple and\nEfficient Sparsity2021GoogleSwitch\nTransformer</p>\n<p>Switch\nTransformerGShardencoder-decoderT51.6T2048expert</p>\n<p>Switch\nTransformer</p>\n<p>Switch\nTransformerSwitch\nTransformerFLOPS/token</p>\n<p>Switch Transformer</p>\n<p>1TransformerMoESwitch\nTransformer</p>\n<p>2MoE to\ndenseMoEdenseMoE99%dense</p>\n<p>3<br>\n- bf16MoE<br>\n- MoE<br>\n- </p>\n<p>41TMoE</p>\n<p>5101</p>\n<p>6FLOPS/tokenSwitch\nTransformer</p>\n<h2 id=\"-1\"></h2>\n<p>Switch\nTransformerGShardtransformerFFNMoE</p>\n<img src=\"/44e38c1b/switch_transformer_structure.png\" class title=\"Switch Transformer \">\n<p>Switch Transformergating\nfunctionSwitch Transformerrouting</p>\n<p>kexpertSwitch\nTransformergating1expertk=1MoESwitch\nlayer</p>\n<p>routingrouter</p>\n<h2 id=\"-1\"></h2>\n<p>GShardSwitch Transformerexpert\ncapacityexpertbatchtoken</p>\n<p>tokenexpertoverflowtokenGShard</p>\n<p>Switch Transformercapacity factor</p>\n<p><span class=\"math display\">\\[\\text{expert\ncapacity}=\\left(\\frac{\\text{tokens per batch}}{\\text{number of\nexperts}}\\right)\\times\\text{capacity factor}.\\]</span></p>\n<p>capacity\nfactorexperttokenoverflow</p>\n<p>capacity factor</p>\n<img src=\"/44e38c1b/switch_transformer_diff_expert_capacity.png\" class title=\"expert capacity\">\n<p>expert capacity</p>\n<p>capacity\nfactor1overflowcapacity\nfactor</p>\n<p>expertoverflowMoESwitch\nTransformer128</p>\n<p>capacity\nfactoroverflow</p>\n<img src=\"/44e38c1b/switch_transformer_capacity_effect.png\" class title=\"expert capacity\">\n<p>tokenscalingoverflow</p>\n<p></p>\n<p> <span class=\"math inline\">\\(N\\)</span> expert <span class=\"math inline\">\\(T\\)</span> tokenbatch <span class=\"math inline\">\\(\\mathcal{B}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\text{loss}&amp;=\\alpha\\cdot\nN\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(f_{i}\\)</span>  <span class=\"math inline\">\\(i\\)</span> experttoken</p>\n<p><span class=\"math display\">\\[\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax\n}p(x)=i\\}\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(P_i\\)</span>\nbatchtoken<span class=\"math inline\">\\(i\\)</span>\nexpert</p>\n<p><span class=\"math display\">\\[\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}\\]</span></p>\n<p>GShard</p>\n<p><span class=\"math inline\">\\(f\\)</span> \n<span class=\"math inline\">\\(P\\)</span>  <span class=\"math inline\">\\(1/N\\)</span></p>\n<p><span class=\"math inline\">\\(\\alpha\\)</span>\n1e-51e-11e-2</p>\n<p> <span class=\"math inline\">\\(\\sum_{i=1}^N(f_i\\cdot\nP_i)=\\sum_{i=1}^N(\\frac1N\\cdot\\frac1N)=\\frac1N\\)</span>loss\n<span class=\"math inline\">\\(N\\)</span>expertloss</p>\n<h2 id=\"-1\"></h2>\n<ol type=\"1\">\n<li>trick</li>\n</ol>\n<p>1bf16</p>\n<p>bf16routing\nfunction</p>\n<p>routingsoftmaxexponentialrounding\nerrorrouting</p>\n<p>2</p>\n<p> <span class=\"math inline\">\\(\\mu=0\\)</span><span class=\"math inline\">\\(\\sigma=\\sqrt{s}/n\\)</span>sne.g.\nfan-in</p>\n<p>Transformers=1.010</p>\n<img src=\"/44e38c1b/switch_transformer_init.png\" class title=\"\">\n<p>3dropout</p>\n<p>Switch\nTransformerdropout</p>\n<img src=\"/44e38c1b/switch_transformer_dropout.png\" class title=\"dropout\">\n<p>dropoutdense0.1expertdropout</p>\n<ol start=\"2\" type=\"1\">\n<li>scaling</li>\n</ol>\n<p>Switch Transformerscaling</p>\n<p>1Step-Basis</p>\n<p>stepexpert</p>\n<p>stepstep</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_step.png\" class title=\"step scaling\">\n<p>2Time-Basis</p>\n<p>Switch\nTransformerstepSwitch\nTransformerdense</p>\n<p>Switch\nTransformerdenseSwitch\nTransformerdensedense1/7</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_time.png\" class title=\"time scaling\">\n<p>3dense</p>\n<p>Switch\nTransformerdenseSwitch\nTransformerdense</p>\n<p>Step-BasisTime-Basis64Switch\nTransformerT5-LargestepSwitch\nTransformer</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_dense.png\" class title=\"dense\">\n<ol start=\"3\" type=\"1\">\n<li>SFT</li>\n</ol>\n<p>GLUESuperGLUEdense</p>\n<p>eval</p>\n<img src=\"/44e38c1b/switch_transformer_sft_result.png\" class title=\"sft\">\n<ol start=\"4\" type=\"1\">\n<li></li>\n</ol>\n<p>Switch\nTransformerBTdense</p>\n<p><br>\n- Switch\nTransformerdense<br>\n- label25%75%ground truth</p>\n<p>densedensedenseSwitch\nTransformer30%</p>\n<img src=\"/44e38c1b/switch_transformer_distill.png\" class title=\"\">\n<p>99%</p>\n<img src=\"/44e38c1b/switch_transformer_distill_diff_model.png\" class title=\"\">\n<p>SuperGLUE</p>\n<img src=\"/44e38c1b/switch_transformer_distill_sft.png\" class title=\"sft\">\n<h1 id=\"glam\">GLaM</h1>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>202112GoogleGLaM: Efficient Scaling of Language Models\nwith\nMixture-of-Experts1.2T64token96.6BMoE</p>\n<p>Switch TransformerGLaM1.6T\ntoken</p>\n<p></p>\n<img src=\"/44e38c1b/glam_related_model.png\" class title=\"glam\">\n<p>GPT-3175BGPT-3NLPGPT-3</p>\n<img src=\"/44e38c1b/glam_compare_gpt3.png\" class title=\"glamgpt3\">\n<img src=\"/44e38c1b/glam_compare_gpt3_2.png\" class title=\"glamgpt3\">\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p>Switch\nTransformerFFNMoESwitch\nTransformerGLaMexpert</p>\n<img src=\"/44e38c1b/glam_model.png\" class title=\"glam\">\n<p></p>\n<p>1</p>\n<p>XLNET</p>\n<p>2</p>\n<blockquote>\n<p>In the non-MoE Transformer feed-forward sub-layers, we replace the\nfirst linear projection and the activation function with the Gated\nLinear Unitwhich computes the component-wise product of two linear\ntransformation of the input, followed by a Gaussian Error Linear\nUnit.</p>\n</blockquote>\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>trick</p>\n<p>1Lingvo: a modular and scalable framework for\nsequence-to-sequence\nmodelingNaNInf</p>\n<p>2BPNaNInfcheckpointNaNInf</p>\n<p>MoE</p>\n<img src=\"/44e38c1b/glam_family.png\" class title=\"glam\">\n<p>GLaMdense</p>\n<img src=\"/44e38c1b/glam_perf.png\" class title=\"glam\">\n<p>GLaM MoEdense</p>\n<h1 id=\"st-moe\">ST-MoE</h1>\n<p>20222GoogleST-MOE: DESIGNING STABLE AND TRANSFERABLE\nSPARSE EXPERT\nMODELSST-MoEMoEMoE</p>\n<p>ST-MoE269B32B\ndenseStable Transferable\nMixture-of-ExpertsST-MoE-32B</p>\n<p>MoEST-MoESwitch\nTransformer1MoE</p>\n<p>ST-MoE4B269BST-MoE</p>\n<img src=\"/44e38c1b/st_moe_models.png\" class title=\"ST-MoE\">\n<h2 id=\"\"></h2>\n<p></p>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<p></p>\n<blockquote>\n<p>Some architectural improvements involve more multiplications than\nadditions or do not sum many items at once</p>\n</blockquote>\n<p>1GELU Gated Linear Units (GEGLU)</p>\n<p>GLUcomponent-wiseGELU-Linear\nFFNtransformerReLU FFN</p>\n<p><span class=\"math display\">\\[\\begin{aligned}FFN_{GEGLU}(x,W,V,b,c)=GELU(xW+b)\\odot(xV+c)\\end{aligned}\\]</span></p>\n<p></p>\n<p>2RMSNorm</p>\n<p>RMSNorm <span class=\"math inline\">\\(g\\)</span></p>\n<p><span class=\"math display\">\\[y_i=\\frac{x_i}{\\sqrt{\\frac1d\\sum_{i=1}^dx_i^2}}\\cdot\ng_i\\]</span></p>\n<p>ST-MoEGEGLURMSNorm</p>\n<img src=\"/44e38c1b/st_moe_remove_multiplications.png\" class title=\"\">\n<p></p>\n<p>3dense</p>\n<p>ST-MoEexpertdensedense</p>\n<img src=\"/44e38c1b/st_moe_more_dense_layer.png\" class title=\"dense\">\n<p>4bias</p>\n<p>FFNbias\nB</p>\n<p><span class=\"math display\">\\[\\text{FFN}_{\\text{GEGLU}}+\\text{Add\nBias}(x)=[(\\text{GELU}(xW_{11})\\odot xW_{12})+B]W_2\\]</span></p>\n<p><span class=\"math display\">\\[\\mathrm{FFN}_{\\mathrm{GEGLU}}+\\mathrm{Mult~Bias}(x)=[(\\mathrm{GELU}(xW_{11})\\odot\nxW_{12})\\odot B]W_2\\]</span></p>\n<p></p>\n<p></p>\n<ol start=\"2\" type=\"1\">\n<li>noise</li>\n</ol>\n<p>ST-MoE</p>\n<p>input-jitterrouterlogits[1e-2,\n1e2]</p>\n<img src=\"/44e38c1b/st_moe_more_add_noise.png\" class title=\"noise\">\n<p>noise</p>\n<p></p>\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>activationgradient</p>\n<p>ST-MoE269B</p>\n<p>ST-MoErouter z-loss</p>\n<p><span class=\"math display\">\\[L_z(x)=\\frac{1}{B}\\sum_{i=1}^B\\left(\\log\\sum_{j=1}^Ne^{x_j^{(i)}}\\right)^2\\]</span></p>\n<p><span class=\"math inline\">\\(B\\)</span> token<span class=\"math inline\">\\(N\\)</span> <span class=\"math inline\">\\(x\\in\\mathcal{R}^{B\\times N}\\)</span>\nrouter</p>\n<p>z-lossrouterlogitsz-loss</p>\n<img src=\"/44e38c1b/st_moe_z_loss_result.png\" class title=\"z-loss\">\n<p>ST-MoEz-loss</p>\n<p>z-loss <span class=\"math inline\">\\(c_z\\)</span>\n</p>\n<p><span class=\"math display\">\\[L_{tot}=L_{CE}+c_BL_B+c_zL_Z\\]</span></p>\n<p>ST-MoE<span class=\"math inline\">\\(c_z=0.001\\)</span></p>\n<p><span class=\"math inline\">\\(L_B\\)</span>  auxiliary load balance\nlossST-MoEGShard/Switch\nTransformer</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\text{loss}&amp;=\\alpha\\cdot\nN\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax\n}p(x)=i\\}\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(N\\)</span>  <span class=\"math inline\">\\(\\mathcal{B}\\)</span> <span class=\"math inline\">\\(T\\)</span> tokenbatch<span class=\"math inline\">\\(f_{i}\\)</span>  <span class=\"math inline\">\\(i\\)</span> experttoken<span class=\"math inline\">\\(P_i\\)</span> batchtoken<span class=\"math inline\">\\(i\\)</span> expert</p>\n<ol start=\"4\" type=\"1\">\n<li></li>\n</ol>\n<p>float32bfloat16bfloat16allreducebfloat16float32</p>\n<p>ST-MoE-32BallreduceST-MoEallreducefloat32</p>\n<p>bfloat16float32</p>\n<img src=\"/44e38c1b/st_moe_round_error.png\" class title=\"bf16\">\n<p>z-loss</p>\n<p>MoErouter</p>\n<p>ST-MoE1/5token</p>\n<p>softmaxMoE</p>\n<h2 id=\"-2\"></h2>\n<p>densescaling\nlawMoEdense</p>\n<p>1expert</p>\n<p>2routing</p>\n<p>3</p>\n<p>4</p>\n<p>MoEscaling lawUnified scaling laws for routed\nlanguage models</p>\n<ol type=\"1\">\n<li>expert</li>\n</ol>\n<p>ST-MoE8/16/32&lt;1%&gt;256</p>\n<p>&gt;1&lt;=1</p>\n<ol start=\"2\" type=\"1\">\n<li>routingcapacity factor</li>\n</ol>\n<p>capacity factor</p>\n<img src=\"/44e38c1b/st_moe_capacity_factor.png\" class title=\"capacity factor\">\n<p></p>\n<p>1capacity factor</p>\n<p>2capacity\nfacotr</p>\n<p>3expertcapacity\nfactor</p>\n<p>capacity\nfactorcapacity\nfactor</p>\n<p>capacity factor</p>\n<img src=\"/44e38c1b/st_moe_capacity_factor_speed.png\" class title=\"capacity factor\">\n<h2 id=\"-2\"></h2>\n<ol type=\"1\">\n<li>ST-MoE</li>\n</ol>\n<p>ST-MoE-32BST-MoE-32B</p>\n<img src=\"/44e38c1b/st_moe_perf.png\" class title=\"capacity ST-MoE-32B\">\n<ol start=\"2\" type=\"1\">\n<li>Expert Specialization</li>\n</ol>\n<p>decodertokenencoder</p>\n<img src=\"/44e38c1b/st_moe_encoder_specialization.png\" class title=\"encoder\">\n<p>encodertoken</p>\n<img src=\"/44e38c1b/st_moe_multiling_specialization.png\" class title=\"\">\n<h1 id=\"deepseekmoe\">DeepseekMoE</h1>\n<p>20241DeepseekMoEMoEDeepSeekMoE:\nTowards Ultimate Expert Specialization in Mixture-of-Experts Language\nModelsDeepSeekMoE</p>\n<p>DeepSeekMoEMoE2</p>\n<p>1expertexpert</p>\n<p>2expertexpertshared\nexpert</p>\n<p>expert(specialization)</p>\n<p>DeepSeekMoE2BMoE16BMoEDeepSeekMoE-16B40GB</p>\n<p>DeepSeekMoE-2B2BDeepSeekMoE-16B7B40%</p>\n<p>DeepSeekMoE-16B</p>\n<img src=\"/44e38c1b/ds_moe_perf.png\" class title=\"deepseek moe\">\n<p>DeepSeekMoE-2B16B</p>\n<p>DeepSeekMoE-145BMoEDeepSeek-67B</p>\n<h2 id=\"-3\"></h2>\n<p>MoEmixture of\nexpertmotivationexpert</p>\n<p>1991expert</p>\n<p>MoEknowledge hybridityknowledge\nredundancy</p>\n<p>1</p>\n<p>expertexpert</p>\n<p>2</p>\n<p>expertexpertexpertexpert8expertexpert</p>\n<p>(expert\nspecialization)MoE</p>\n<p>expertnon-overlap &amp; foucusd\nknowledge</p>\n<p>DeepSeekMoE2</p>\n<p>1Fine-Grained Expert Segmentation</p>\n<p>expertexpertexpertspecialization16expert2120expert1/464expert8\n<span class=\"math inline\">\\(\\binom{64}8=4,426,165,368\\)</span>\n</p>\n<p>2Shared Expert Isolation</p>\n<p>expertcommon\nknowledgeexpertexpertexpertexpert</p>\n<p>MoEFine-Grained Expert SegmentationShared\nExpert Isolation</p>\n<img src=\"/44e38c1b/ds_moe_structure.png\" class title=\"deepseek moe \">\n<p>expert isolation20221DeepSpeed-MoE:\nAdvancing Mixture-of-Experts Inference and Training to Power\nNext-Generation AI Scale</p>\n<p>MoEexpert <span class=\"math inline\">\\(N\\)</span>expert <span class=\"math inline\">\\(K\\)</span>DeepSeekMoEexpert\n<span class=\"math inline\">\\(1/m\\)</span>DeepSeekMoE <span class=\"math inline\">\\(mN\\)</span> expertexpert <span class=\"math inline\">\\(mK\\)</span> <span class=\"math inline\">\\(T\\)</span> <span class=\"math inline\">\\(L\\)</span> <span class=\"math inline\">\\(e_i^l\\)</span>  <span class=\"math inline\">\\(i\\)</span>\nexpertDeepSeekMoElayernorm</p>\n<p><span class=\"math display\">\\[\\mathbf{u}_{1:T}^l=\\text{Self-Att}\\left(\\mathbf{h}_{1:T}^{l-1}\\right)+\\mathbf{h}_{1:T}^{l-1}\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{h}_t^l=\\sum_{i=1}^{mN}\\left(g_{i,t}\\text{\nFFN}_i\\left(\\mathbf{u}_t^l\\right)\\right)+\\mathbf{u}_t^l\\]</span></p>\n<p><span class=\"math display\">\\[g_{i,t}=\\begin{cases}s_{i,t},&amp;s_{i,t}\\in\\text{Topk}(\\{s_{j,t}|1\\leqslant\nj\\leqslant mN\\},mK)\\\\0,&amp;\\text{otherwise,}\\end{cases}\\]</span></p>\n<p><span class=\"math display\">\\[s_{i,t}=\\mathrm{Softmax}_i\\left({\\mathbf{u}_t^l}^T\\mathbf{e}_i^l\\right)\\]</span></p>\n<h2 id=\"-2\"></h2>\n<p>MoEgating</p>\n<p>1routing\ncollapsegatingexpert</p>\n<p>2</p>\n<p>routing collapseDeepSeekMoEexpert-level balance\nloss</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}_{\\mathrm{ExpBal}}&amp; =\\alpha_1\\sum_{i=1}^{N&#39;}f_iP_i\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nf_{i}&amp;\n=\\frac{N^{\\prime}}{K^{\\prime}T}\\sum_{t=1}^T\\mathbb{1}(\\text{Token\n}t\\text{ selects Expert }i)\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nP_{i}&amp; =\\frac1T\\sum_{t=1}^Ts_{i,t}\n\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(\\alpha_1\\)</span> expert-level\nbalance factor</p>\n<p> <span class=\"math inline\">\\(f_i\\)</span>  <span class=\"math inline\">\\(P_i\\)</span> Switch\nTransformer</p>\n<p>Switch Transformer <span class=\"math inline\">\\(f_i\\)</span>\n <span class=\"math inline\">\\(i\\)</span>\nexperttokenDeepSeekMoE\n<span class=\"math inline\">\\(N&#39;/K&#39;\\)</span>  <span class=\"math inline\">\\(N&#39;=mN-K_s\\)</span><span class=\"math inline\">\\(K&#39;=mK-K_s\\)</span><span class=\"math inline\">\\(K_s\\)</span>\nexpertDeepSeekMoE\n<span class=\"math inline\">\\(f_i\\)</span> Switch\nTransformer</p>\n<p><span class=\"math inline\">\\(N&#39;/K&#39;\\)</span>\nexpertloss</p>\n<p><span class=\"math inline\">\\(P_i\\)</span> token\n<span class=\"math inline\">\\(i\\)</span> expertSwitch\nTransformer</p>\n<p> <span class=\"math inline\">\\(f_i\\)</span> <span class=\"math inline\">\\(P_i\\)</span> </p>\n<p>DeepSeekMoEdevice-level balance\nloss</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}_{\\mathrm{DevBal}}&amp; =\\alpha_2\\sum_{i=1}^Df_i&#39;P_i&#39;\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nf_i^{\\prime}&amp; =\\frac1{|\\mathcal{E}_i|}\\sum_{j\\in\\mathcal{E}_i}f_j\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nP_{i}^{\\prime}&amp; =\\sum_{j\\in\\mathcal{E}_i}P_j\n\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(\\alpha_2\\)</span> device-level\nbalance factor</p>\n<p><span class=\"math inline\">\\(\\mathcal{E}_i\\)</span>  <span class=\"math inline\">\\(i\\)</span> </p>\n<p>device-level balance lossexpert-level balance loss\n<span class=\"math inline\">\\(f_i\\)</span>  <span class=\"math inline\">\\(P_i\\)</span>\nexpert</p>\n<p>expert64expert8tokentokenexpert</p>\n<p>expert</p>\n<h2 id=\"-3\"></h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>100B\ntokenDeepSeekMoE-2BBPE8k</p>\n<p>DeepSeekMoE-2B0.006multi-head\nattention0.3B</p>\n<img src=\"/44e38c1b/ds_model_param.png\" class title=\"\">\n<p>relative expert\nsizeDeepSeekMoEexpertFFN</p>\n<p></p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">optimizer</td>\n<td style=\"text-align: center;\">AdamW</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">adam_beta_1</td>\n<td style=\"text-align: center;\">0.9</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">adam_beta_2</td>\n<td style=\"text-align: center;\">0.95</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">adam_weight_decay</td>\n<td style=\"text-align: center;\">0.1</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">warmup schedule</td>\n<td style=\"text-align: center;\">linear</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">warmup step</td>\n<td style=\"text-align: center;\">2000</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">max lr</td>\n<td style=\"text-align: center;\">1.08e-3</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">dropout</td>\n<td style=\"text-align: center;\">0</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">sequence length</td>\n<td style=\"text-align: center;\">2k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">batch size</td>\n<td style=\"text-align: center;\">2k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">total step</td>\n<td style=\"text-align: center;\">25,000</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p><br>\n- expertGPUdevice-level balance loss<br>\n- expert-level balance factor0.01<br>\n- 80%0.31690%0.316</p>\n<p>100BDeepSeekMoE-2Bbenchmark4densehash\nlayermoeHash layers for large sparse modelsSwitch\nTransformerGShard</p>\n<img src=\"/44e38c1b/ds_moe_comparison.png\" class title=\"deepseek moe \">\n<p><br>\n- Hash LayerSwitch\nTransformerdense<br>\n- GSshardHash LayerSwitch\nTransformer<br>\n- DeepSeekMoEGShard</p>\n<p>DeepSeekMoEdenseGShardDeepSeekMoE-2B</p>\n<p>denseGShard161.5DeepSeekMoE-2B</p>\n<img src=\"/44e38c1b/ds_moe_upper_bound_2b.png\" class title=\"deepseek moe upper bound\">\n<p>DeepSeekMoEDeepSeekMoE-13B,\n1.21.5GShardDeepSeekMoE-13Bmatch</p>\n<img src=\"/44e38c1b/ds_moe_upper_bound_13b.png\" class title=\"deepseek moe upper bound\">\n<ol start=\"2\" type=\"1\">\n<li>DeepSeekMoE</li>\n</ol>\n<p>DeepSeekMoEshared expertfine-grained\nexpertexpert</p>\n<img src=\"/44e38c1b/ds_moe_ablation.png\" class title=\"deepseek moe upper bound \">\n<p>1</p>\n<p>2</p>\n<p>364expert1/2/4pileloss1.808,1.806,1.8111:32+6</p>\n<ol start=\"3\" type=\"1\">\n<li>expert specialization</li>\n</ol>\n<p>DeepSeekMoEexpert specialization</p>\n<p>1DeepSeekMoE-2B1.5GShardtop</p>\n<p></p>\n<img src=\"/44e38c1b/ds_moe_expert_specialization.png\" class title=\"\">\n<p>DeepSeekMoEDeepSeekMoE</p>\n<p>2DeepSeekMoEloss</p>\n<p>3GShardDeepSeekMoE</p>\n<img src=\"/44e38c1b/ds_moe_less_activated_expert.png\" class title=\"\">\n<p>132b2+6GShardDeepSeekMoE</p>\n<img src=\"/44e38c1b/ds_2b_less_expert.png\" class title=\"2B\">\n<ol type=\"1\">\n<li>DeepSeekMoE-16B</li>\n</ol>\n<p>DeepSeekMoE-16B2TLLAMA2-7B100k</p>\n<img src=\"/44e38c1b/ds_model_param.png\" class title=\"\">\n<p>MoE</p>\n<p>MoEloss</p>\n<p>DeepSeekMoE-16B6426gating\nfunctiontoken8token16.4B2.8B</p>\n<p>dimension</p>\n<p><br>\n- lr = 4.2e-4<br>\n- 80%90%lr0.316<br>\n- batch size = 4.5k4kbatch18M\ntoken2T10.6w<br>\n- pipeline parallelism</p>\n<p>expert level balance\nloss0.001</p>\n<p>DeepSeekMoE-16BDeepSeek-7B</p>\n<img src=\"/44e38c1b/ds_16b_perf_1.png\" class title=\"DeepSeek-7B\">\n<p>DeepSeekMoE-16BLLAMA2-7B</p>\n<img src=\"/44e38c1b/ds_16b_perf_2.png\" class title=\"LLAMA2-7B\">\n<ol start=\"5\" type=\"1\">\n<li>DeepSeekMoE-145B</li>\n</ol>\n<p>245BtokenDeepSeekMoE-145BDeepSeek-67B</p>\n<img src=\"/44e38c1b/ds_moe_145b.png\" class title=\"145b\">\n<h1 id=\"dbrx\">DBRX</h1>\n<p>2024327DatabricksDBRX132B36BMoE</p>\n<p>DBRXRoPEGLUGQAfine-grained\nexpert16token4MixtralGrok-182DBRX</p>\n<p>DBRX32k12TtokenDBRX3072H100post-trainingred-team3</p>\n<p>DBRXGPT-3.5Gemini 1.0\nProCodeLLaMA-70B</p>\n<img src=\"/44e38c1b/dbrx_perf.png\" class title=\"DBRX\">\n<p>DBRX</p>\n<img src=\"/44e38c1b/dbrx_infer_efficiency.png\" class title=\"\">\n<h1 id=\"qwen1.5-moe\">Qwen1.5-MoE</h1>\n<p>2024328Qwen1.5-MoE-A2.7B2.7BQwen1.5-7B</p>\n<p>Qwen1.5-MoE-A2.7BDeepSeekMoEDBRXfine-grained\nexpert64token84</p>\n<p>Qwen1.5-MoE-A2.7BQwen-1.8B</p>\n<p>Qwen1.5-MoE-A2.7B</p>\n<img src=\"/44e38c1b/qwen1.5_moe_perf.png\" class title=\"Qwen1.5-MoE-A2.7B\">\n<p>Qwen1.5-MoE-A2.7Bnon-embedding7B</p>\n<img src=\"/44e38c1b/qwen1.5_moe_params.png\" class title=\"Qwen1.5-MoE-A2.7B\">\n<p>Qwen1.5-MoE-A2.7BQwen1.5-7B75%</p>\n<p>A100-80GvLLMQwen1.5-7BQwen1.5-MoE-A2.7B</p>\n<p>/token1000token1000TPSthroughput</p>\n<img src=\"/44e38c1b/qwen1.5_moe_tps.png\" class title=\"Qwen1.5-MoE-A2.7B TPS\">\n<p>MoEdenseQwen1.5-MoE-A2.7BQwen1.5-7B1.74</p>\n<h1 id=\"mistral\">Mistral</h1>\n<h2 id=\"mistral-8x7b\">Mistral 8x7B</h2>\n<p>20231211Mistral\nAIMistral-8x7Btoken82</p>\n<p>Mistral-8x7B32kLLAM2-70BGPT-3.5</p>\n<img src=\"/44e38c1b/mistral_8_7b_perf.png\" class title=\"Mistral 8x7B\">\n<p>Mistral-8x7BLLAM2-70B6</p>\n<p>LLAM2-13B</p>\n<img src=\"/44e38c1b/mistral_8_7b_active_perf.png\" class title=\"Mistral 8x7B\">\n<h2 id=\"mistral-8x22b\">Mistral 8x22B</h2>\n<p>2024417Mistral\nAIMistral-8x22B141B39BMoE</p>\n<p>Mistral-8x22BMistral-8x7B32k64kMistral-8x22Bfunction\ncall</p>\n<p></p>\n<img src=\"/44e38c1b/mistral_8_22b_reasoning.png\" class title=\"Mistral 8x22B reasoning\">\n<img src=\"/44e38c1b/mistral_8_22b_multiling.png\" class title=\"Mistral 8x22B \">\n<img src=\"/44e38c1b/mistral_8_22b_code.png\" class title=\"Mistral 8x22B \">\n<h1 id=\"\"></h1>\n<ul>\n<li>MoEdenseMoE<br>\n</li>\n<li>MoEMoE<br>\n</li>\n<li>denseMoE<br>\n</li>\n<li><br>\n</li>\n<li>GShardSwitch Transformer<br>\n</li>\n<li>MoEMoE</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Adaptive Mixtures of Local Experts\nhttps://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf<br>\n2Outrageously Large Neural Networks: The Sparsely-Gated\nMixture-of-Experts Layer https://arxiv.org/abs/1701.06538<br>\n3GShard: Scaling Giant Models with Conditional Computation and\nAutomatic Sharding https://arxiv.org/abs/2006.16668<br>\n4Switch Transformers: Scaling to Trillion Parameter Models with\nSimple and Efficient Sparsity https://arxiv.org/abs/2101.03961<br>\n5GLaM: Efficient Scaling of Language Models with Mixture-of-Experts\nhttps://arxiv.org/abs/2112.06905<br>\n6ST-MoE: Designing Stable and Transferable Sparse Expert Models\nhttps://arxiv.org/abs/2202.08906<br>\n7DeepSeekMoE: Towards Ultimate Expert Specialization in\nMixture-of-Experts Language Models\nhttps://arxiv.org/abs/2401.06066<br>\n8Introducing DBRX: A New State-of-the-Art Open LLM\nhttps://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm<br>\n9Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated\nParameters https://qwenlm.github.io/zh/blog/qwen-moe/</p>\n","length":33074,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>202434MoEQwen1.5-MoEDBRXJambaMistral</p>\n<p>MoE</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">GPT4</td>\n<td style=\"text-align: center;\">20233</td>\n<td style=\"text-align: center;\">236George\nHotzGPT48220B</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Mistral-87B</td>\n<td style=\"text-align: center;\">202312</td>\n<td style=\"text-align: center;\">Mistral AI</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">LLAMA-MoE</td>\n<td style=\"text-align: center;\">202312</td>\n<td style=\"text-align: center;\">github</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">DeepSeek-MoE</td>\n<td style=\"text-align: center;\">20241</td>\n<td style=\"text-align: center;\">MoE</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">abab6</td>\n<td style=\"text-align: center;\">20241</td>\n<td style=\"text-align: center;\">MiniMaxMoE</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">2.0</td>\n<td style=\"text-align: center;\">20242</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Step-2</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">MM1</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">MoE</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Grok-1</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">X</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Qwen1.5-MoE-A2.7B</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">DBRX</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">Databricks</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Jamba</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">AI21</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Mistral-822B</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\">Mistral AI</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">WizardLM-2-822B</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">3.0</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\">400BMoE</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Arctic</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\">Snowflake480BDense-MoE\nHybrid</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>MoEMoE</p>\n<img src=\"/44e38c1b/xiaomi_moe.jpg\" class title=\"MoE\">\n<p>MoE</p>\n<p>MoEMoEMoE</p>\n<p>20244DeepSeek-MoEQwen1.5-MoE</p>\n<h1 id=\"\"></h1>\n<p>MoE</p>\n<p>MoEGoogle</p>\n<h2 id=\"\"></h2>\n<p>MoE1991<a href=\"https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf\">Adaptive\nMixtures of Local Experts</a>Geoffrey HintonMichael I.\nJordanMoE1988</p>\n<blockquote>\n<p>This idea was first presented by Jacobs and Hinton at the\nConnectionist Summer School in Pittsburg in 1988.</p>\n</blockquote>\n<p>MoEMoE</p>\n<h2 id=\"rnn\">RNN</h2>\n<p>Google20171<a href=\"https://arxiv.org/abs/1701.06538\">Outrageously Large Neural\nNetworks: The Sparsely-Gated Mixture-of-Experts\nLayer</a>MoELSTM137B128kLSTM</p>\n<h2 id=\"transformer\">Transformer</h2>\n<ol type=\"1\">\n<li><p>20206Google<a href=\"https://arxiv.org/abs/2006.16668\">GShard: Scaling Giant Models\nwith Conditional Computation and Automatic\nSharding</a>MoEencoder-decodertransformerFFNMoE12.5B600BMoE2048</p></li>\n<li><p>20211Google<a href=\"https://arxiv.org/abs/2101.03961\">Switch Transformers: Scaling\nto Trillion Parameter Models with Simple and Efficient Sparsity</a>\nT5encoder-decoderFFNMoErouting1.6Tswitch\ntransformerSwitch\nTransformersscaling</p></li>\n<li><p>20222Google<a href=\"https://arxiv.org/abs/2202.08906\">ST-MoE: Designing Stable and\nTransferable Sparse Expert\nModels</a>encoder-decoderMoE269B32BST-MoEMoESwitch\nTransformer</p></li>\n</ol>\n<h2 id=\"gpt\">GPT</h2>\n<ol type=\"1\">\n<li><p>202112GoogleGLaM<a href=\"https://arxiv.org/abs/2112.06905\">GLaM: Efficient Scaling of\nLanguage Models with\nMixture-of-Experts</a>1.2Tdecoder-onlyencoder-decoderdecoder-onlyGoogle</p></li>\n<li><p>20241<a href=\"https://arxiv.org/abs/2401.06066\">DeepSeekMoE: Towards Ultimate\nExpert Specialization in Mixture-of-Experts Language\nModels</a>2312DeepSeekMoE</p></li>\n<li><p>2024DatabricksDBRXQwen1.5-MoE-A2.7BMistral\nAIMistral-8x22B</p></li>\n</ol>\n<h1 id=\"\"></h1>\n<p>Geoffrey HintonMichael I. Jordan<a href=\"https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf\">Adaptive\nMixtures of Local Experts</a>MoE</p>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<p></p>\n<p>MoEexpert</p>\n<p>MoEvowel discrimination\ntaskMoEaeiou</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p>MoEexpert networkgating\nnetworkexpertgating\nnetworkexpertexpertstochastictruefalse</p>\n<img src=\"/44e38c1b/vanilla_moe.png\" class title=\"Vanilla MoE\">\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>MoEideaJacobsHinton1988lossensembleexpertexpertexpertresidual</p>\n<p>case <span class=\"math inline\">\\(c\\)</span>\n<span class=\"math inline\">\\(d^c\\)</span> ground truth <span class=\"math inline\">\\(i\\)</span> expert <span class=\"math inline\">\\(o_{i}^c\\)</span><span class=\"math inline\">\\(p_{i}^c\\)</span> gating network <span class=\"math inline\">\\(i\\)</span>\nexpert <span class=\"math inline\">\\(E^{c}\\)</span> </p>\n<p><span class=\"math display\">\\[E^{c}=\\left|\\left|d^{c}-\\sum_{i}p_{i}^{c}o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p>expert</p>\n<p>expertexpertexpert</p>\n<p>expertexpertexpertexpertgating\nnetwork</p>\n<p></p>\n<p>HintonJordanlossexpert</p>\n<p>gating networkexpert</p>\n<p><span class=\"math display\">\\[E^{c}=\\langle\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2\\rangle=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p>expertexpertexpert</p>\n<p>losslocalizationcasegating\nnetworkexpertgating\nnetworkexpert</p>\n<p>localizationexpert</p>\n<p>expertexpertgating\nnetworkexpert\nerror+-</p>\n<p>expert0</p>\n<ol start=\"4\" type=\"1\">\n<li></li>\n</ol>\n<p>losslossloss</p>\n<p><span class=\"math display\">\\[\\text{loss}E^{c}=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p><span class=\"math display\">\\[\\text{loss}E^c=-log\\sum_ip_i^ce^{-\\frac12\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}\\]</span></p>\n<p>lossloss</p>\n<p><span class=\"math display\">\\[\\text{loss}\\frac{\\partial\nE^c}{\\partial\\mathbf{o}_i^c}=-2p_i^c(\\mathbf{d}^c-\\mathbf{o}_i^c)\\]</span></p>\n<p><span class=\"math display\">\\[\\text{loss}\\frac{\\partial\nE^c}{\\partial\\mathbf{o}_i^c}=-\\left[\\frac{p_i^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}}{\\sum_jp_j^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_j^c\\|^2}}\\right](\\mathbf{d}^c-\\mathbf{o}_i^c)\\]</span></p>\n<p>lossloss <span class=\"math inline\">\\(i\\)</span>\nexpertexpertexpert <span class=\"math inline\">\\(i\\)</span>\ncasegating\nnetworklosscaseexpertlosslossexpertlocalizationexpert</p>\n<p>BTWloss</p>\n<p>MoE</p>\n<h1 id=\"lstm-moe\">LSTM MoE</h1>\n<p>Google20171 <a href=\"https://arxiv.org/abs/1701.06538\">OUTRAGEOUSLY LARGE NEURAL\nNETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS\nLAYER</a>MoELSTM137BLSTM7</p>\n<p>1991</p>\n<h2 id=\"\"></h2>\n<p>Transformer</p>\n<p>conditional\ncomputationconditional\ncomputationMoE</p>\n<p></p>\n<ul>\n<li>MoEexpertbatch sizebatch\nsize<br>\nbatch\nsize3216expertexpert2batch\nsizebatch\nsizebatch\nsize<br>\n</li>\n<li><br>\nNLP<br>\n</li>\n<li><br>\n<br>\n</li>\n<li><br>\nGPU<br>\n</li>\n<li>GPU<br>\nGPUbranchingif/elseMoEgating\nnetwork</li>\n</ul>\n<p></p>\n<h2 id=\"\"></h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<p>LSTMMoEembedding</p>\n<img src=\"/44e38c1b/rnn_moe.png\" class title=\"LSTM MoE\">\n<p>expertfeed-forward neural\nnetworknexpertgating networkn</p>\n<p><span class=\"math display\">\\[\\begin{aligned}y=\\sum_{i=1}^nG(x)_iE_i(x)\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(E_{i}(x)\\)</span>  <span class=\"math inline\">\\(i\\)</span> expert<span class=\"math inline\">\\(G(x)_{i}\\)</span> gating network <span class=\"math inline\">\\(i\\)</span> expert</p>\n<p> <span class=\"math inline\">\\(G(x)_{i}\\)</span>\n0expert</p>\n<p>experttwo-level hierarchical\nMoEgating networkgating\nnetworkexpertgating\nnetworkexpertword2vechierarchical\nsoftmax</p>\n<ol start=\"2\" type=\"1\">\n<li>gating network</li>\n</ol>\n<p>gating network</p>\n<p>softmaxgating\nfunction</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G_\\sigma(x)=Softmax(x\\cdot\nW_g)\\end{aligned}\\]</span></p>\n<p>topkksoftmax0expert</p>\n<p>sparsitytopkgating\nfunction</p>\n<p>Gaussian\nnoisenoise</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[KeepTopK(v,k)_i=\\begin{cases}v_i&amp;\\text{if\n}v_i\\text{ is in the top }k\\text{ elements of\n}v.\\\\-\\infty&amp;\\text{otherwise.}\\end{cases}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}H(x)_i=(x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\end{aligned}\\]</span></p>\n<p>noisesoftplusReLU</p>\n<img src=\"/44e38c1b/softplus.png\" class title=\"softplus\">\n<p></p>\n<h2 id=\"\"></h2>\n<p>MoEgating\nnetworkexpertexpert</p>\n<p></p>\n<p>hard\nconstraintexperthard\nconstraintsoft\nconstraint</p>\n<p>expert</p>\n<p><span class=\"math display\">\\[Importance(X)=\\sum_{x\\in\nX}G(x)\\]</span></p>\n<p><span class=\"math inline\">\\(G(x)\\)</span> gating\nnetworkexpert</p>\n<p> <span class=\"math inline\">\\(L_{importance}\\)</span><span class=\"math inline\">\\(L_{importance}\\)</span> </p>\n<p><span class=\"math display\">\\[L_{importance}(X)=w_{importance}\\cdot\nCV(Importance(X))^2\\]</span></p>\n<p> <span class=\"math inline\">\\(w_{importance}\\)</span>\n0.1CVcoefficient of variation</p>\n<p>coefficient of\nvariation\n<span class=\"math inline\">\\(\\sigma\\)</span>   <span class=\"math inline\">\\(\\mu\\)</span> </p>\n<p>MoEexpertexpertgating\n<span class=\"math inline\">\\(L_{importance}\\)</span> </p>\n<p> <span class=\"math inline\">\\(L_{importance}\\)</span>  <span class=\"math inline\">\\(L_{importance}\\)</span>\n <span class=\"math inline\">\\(L_{importance}\\)</span>\n</p>\n<p>expertexpertgating</p>\n<p> <span class=\"math inline\">\\(L_{load}\\)</span>\nexpert</p>\n<p>expertback\npropagation <span class=\"math inline\">\\(L_{load}\\)</span>\nexpert</p>\n<p>MoE <span class=\"math inline\">\\(H(x)\\)</span> KeepTopK</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}H(x)_i=(x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(kth\\_excluding(H(x),k,i)\\)</span>\n<span class=\"math inline\">\\(H(x)\\)</span>  <span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(P(x,i)\\)</span>\nnoise <span class=\"math inline\">\\(i\\)</span> noise <span class=\"math inline\">\\(kth\\_excluding(H(x),k,i)\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{aligned}P(x,i)=Pr\\Big((x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\\\&gt;kth\\_excluding(H(x),k,i)\\Big)\\end{aligned}\\]</span></p>\n<p>noise <span class=\"math inline\">\\(i\\)</span>\n <span class=\"math inline\">\\(i\\)</span>\n<span class=\"math inline\">\\(P(x,i)\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{aligned}P(x,i)&amp;=\\Phi\\Big(\\frac{(x\\cdot\nW_g)_i-kth\\_excluding(H(x),k,i)}{Softplus((x\\cdot\nW_{noise})_i)}\\Big)\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\Phi\\)</span>\nCDF</p>\n<p> <span class=\"math inline\">\\(i\\)</span>\nexpert</p>\n<p><span class=\"math display\">\\[\\begin{aligned}Load(X)_i=\\sum_{x\\in\nX}P(x,i)\\end{aligned}\\]</span></p>\n<p>expert</p>\n<p><span class=\"math display\">\\[L_{load}(X)=w_{load}\\cdot\nCV(Load(X))^2\\]</span></p>\n<p><span class=\"math inline\">\\(w_{load}\\)</span>\n0.1</p>\n<p> <span class=\"math inline\">\\(L_{importance}(X)\\)</span><span class=\"math inline\">\\(Load(X)\\)</span>\n</p>\n<p>expert\n<span class=\"math inline\">\\(W_g\\)</span>  <span class=\"math inline\">\\(W_{noise}\\)</span>\n0</p>\n<p></p>\n<img src=\"/44e38c1b/rnn_moe_load_function.png\" class title=\"\">\n<p></p>\n<h2 id=\"\"></h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<p>1batch size</p>\n<p>expertbatch\nsizenexpertkbatch\nsizebexpertbatch\nsizekb/nexpertbatch size<br>\n-\nbatchbatchMoEexpertdexpertkbd/nbatch\nsize - LSTMbatch\nsize</p>\n<p>2</p>\n<p></p>\n<p>expertinputoutput[input_size,\nhidden_size][hidden_size,\noutput_size]GPU1000hidden_sizeexpert1000</p>\n<ol start=\"2\" type=\"1\">\n<li> &amp; </li>\n</ol>\n<p>MoEdense4/32/256expertflat\nMoE256/1024/4096experthierarchical\nMoEexpert1Mflat4experthierarchical\nMoEgating2</p>\n<p>ppldenseMoEMoE</p>\n<img src=\"/44e38c1b/rnn_moe_perf.png\" class title=\"\">\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>4Bdiminishing\nreturns</p>\n<p> + 100B\ntoken32, 256, 10244096, 16384, 65536,\n131072expertMoE137B</p>\n<p></p>\n<img src=\"/44e38c1b/rnn_moe_137b.png\" class title=\"137\">\n<p></p>\n<ol start=\"4\" type=\"1\">\n<li>Expert Specialization</li>\n</ol>\n<p>MoE</p>\n<p>tokenspecialization</p>\n<img src=\"/44e38c1b/rnn_moe_specilized.png\" class title=\"RNN MoE \">\n<h1 id=\"gshard\">GShard</h1>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>2018Berttransformer20206GoogleGShard:\nScaling Giant Models with Conditional Computation and Automatic\nShardingMoEencoder-decodertransformerMoE</p>\n<p>GShardMoE600B</p>\n<img src=\"/44e38c1b/gshard_moe_family.png\" class title=\"GShard MoE family\">\n<p>expertLSMT MoE --\nexpert24expertChatGPTBertGPT</p>\n<p>GShardMoE</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<p>Googleencoder-decoder\ntransfomerGShardencoder-decoder\ntransfomer</p>\n<p>GShardencoderdecoderFFNMoENN/2MoE</p>\n<img src=\"/44e38c1b/gshard_model.png\" class title=\"GShard\">\n<p>top-2 expert</p>\n<p>GShardLSTM MoEgating\nfunctionauxiliary loss function</p>\n<p>MoE</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{G}_{s,E}&amp; =\\mathrm{GATE}(x_s)  \\\\\n\\mathrm{FFN}_e(x_s)&amp; =wo_e\\cdot\\text{ReLU}(wi_e\\cdot x_s)  \\\\\ny_{s}&amp; =\\sum_{e=1}^E\\mathcal{G}_{s,e}\\cdot\\mathrm{FFN}_e(x_s)\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(x_s\\)</span> MoEtoken<span class=\"math inline\">\\(w_i\\)</span>  <span class=\"math inline\">\\(w_o\\)</span>\n<span class=\"math inline\">\\(\\mathcal{G}_{s}\\)</span> gating\nfunction</p>\n<p>GShardgating\nfunction12</p>\n<p>NtokenEexpertNEgating\nfunction</p>\n<p>gating function</p>\n<p>1 expert capacity</p>\n<p>experttokenexperttoken2N/E</p>\n<p>expert capacityGATE()expert <span class=\"math inline\">\\(c_e\\)</span>\ntokentokenexpert</p>\n<p>2 Local group dispatching</p>\n<p>tokenG2N/EG</p>\n<p>batchbatchbatchgroupall2allgroup</p>\n<p>groupgradient\naccumulation</p>\n<p>3 Auxiliary loss</p>\n<p>gatingLSTM\nMoE</p>\n<p><span class=\"math display\">\\[\\ell_{aux}=\\frac1E\\sum_{e=1}^E\\frac{c_e}S\\cdot\nm_e\\]</span></p>\n<p><span class=\"math inline\">\\(S\\)</span> token<span class=\"math inline\">\\(E\\)</span> <span class=\"math inline\">\\(c_e\\)</span>  <span class=\"math inline\">\\(e\\)</span> token<span class=\"math inline\">\\(m_e\\)</span>  <span class=\"math inline\">\\(e\\)</span> expert <span class=\"math inline\">\\(S\\)</span> token</p>\n<p> <span class=\"math inline\">\\(\\frac{c_e}S\\)</span>\n <span class=\"math inline\">\\(\\frac{c_e}S\\)</span>  <span class=\"math inline\">\\(m_e\\)</span>  <span class=\"math inline\">\\(m_e\\)</span>  <span class=\"math inline\">\\(e\\)</span> expert <span class=\"math inline\">\\(S\\)</span>\ntokenloss</p>\n<p>loss</p>\n<p>gating</p>\n<img src=\"/44e38c1b/gshard_algo_1.png\" class title=\"GShard gating \">\n<p>4 Random routing</p>\n<p>top-2\nexperttop-1</p>\n<p>top-1g2</p>\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<img src=\"/44e38c1b/gshard_perf.png\" class title=\"GShard\">\n<h1 id=\"switch-transformer\">Switch Transformer</h1>\n<p>20224ChatGPTGoogleSwitch\nTransformers: Scaling to Trillion Parameter Models with Simple and\nEfficient Sparsity2021GoogleSwitch\nTransformer</p>\n<p>Switch\nTransformerGShardencoder-decoderT51.6T2048expert</p>\n<p>Switch\nTransformer</p>\n<p>Switch\nTransformerSwitch\nTransformerFLOPS/token</p>\n<p>Switch Transformer</p>\n<p>1TransformerMoESwitch\nTransformer</p>\n<p>2MoE to\ndenseMoEdenseMoE99%dense</p>\n<p>3<br>\n- bf16MoE<br>\n- MoE<br>\n- </p>\n<p>41TMoE</p>\n<p>5101</p>\n<p>6FLOPS/tokenSwitch\nTransformer</p>\n<h2 id=\"-1\"></h2>\n<p>Switch\nTransformerGShardtransformerFFNMoE</p>\n<img src=\"/44e38c1b/switch_transformer_structure.png\" class title=\"Switch Transformer \">\n<p>Switch Transformergating\nfunctionSwitch Transformerrouting</p>\n<p>kexpertSwitch\nTransformergating1expertk=1MoESwitch\nlayer</p>\n<p>routingrouter</p>\n<h2 id=\"-1\"></h2>\n<p>GShardSwitch Transformerexpert\ncapacityexpertbatchtoken</p>\n<p>tokenexpertoverflowtokenGShard</p>\n<p>Switch Transformercapacity factor</p>\n<p><span class=\"math display\">\\[\\text{expert\ncapacity}=\\left(\\frac{\\text{tokens per batch}}{\\text{number of\nexperts}}\\right)\\times\\text{capacity factor}.\\]</span></p>\n<p>capacity\nfactorexperttokenoverflow</p>\n<p>capacity factor</p>\n<img src=\"/44e38c1b/switch_transformer_diff_expert_capacity.png\" class title=\"expert capacity\">\n<p>expert capacity</p>\n<p>capacity\nfactor1overflowcapacity\nfactor</p>\n<p>expertoverflowMoESwitch\nTransformer128</p>\n<p>capacity\nfactoroverflow</p>\n<img src=\"/44e38c1b/switch_transformer_capacity_effect.png\" class title=\"expert capacity\">\n<p>tokenscalingoverflow</p>\n<p></p>\n<p> <span class=\"math inline\">\\(N\\)</span> expert <span class=\"math inline\">\\(T\\)</span> tokenbatch <span class=\"math inline\">\\(\\mathcal{B}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\text{loss}&amp;=\\alpha\\cdot\nN\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(f_{i}\\)</span>  <span class=\"math inline\">\\(i\\)</span> experttoken</p>\n<p><span class=\"math display\">\\[\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax\n}p(x)=i\\}\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(P_i\\)</span>\nbatchtoken<span class=\"math inline\">\\(i\\)</span>\nexpert</p>\n<p><span class=\"math display\">\\[\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}\\]</span></p>\n<p>GShard</p>\n<p><span class=\"math inline\">\\(f\\)</span> \n<span class=\"math inline\">\\(P\\)</span>  <span class=\"math inline\">\\(1/N\\)</span></p>\n<p><span class=\"math inline\">\\(\\alpha\\)</span>\n1e-51e-11e-2</p>\n<p> <span class=\"math inline\">\\(\\sum_{i=1}^N(f_i\\cdot\nP_i)=\\sum_{i=1}^N(\\frac1N\\cdot\\frac1N)=\\frac1N\\)</span>loss\n<span class=\"math inline\">\\(N\\)</span>expertloss</p>\n<h2 id=\"-1\"></h2>\n<ol type=\"1\">\n<li>trick</li>\n</ol>\n<p>1bf16</p>\n<p>bf16routing\nfunction</p>\n<p>routingsoftmaxexponentialrounding\nerrorrouting</p>\n<p>2</p>\n<p> <span class=\"math inline\">\\(\\mu=0\\)</span><span class=\"math inline\">\\(\\sigma=\\sqrt{s}/n\\)</span>sne.g.\nfan-in</p>\n<p>Transformers=1.010</p>\n<img src=\"/44e38c1b/switch_transformer_init.png\" class title=\"\">\n<p>3dropout</p>\n<p>Switch\nTransformerdropout</p>\n<img src=\"/44e38c1b/switch_transformer_dropout.png\" class title=\"dropout\">\n<p>dropoutdense0.1expertdropout</p>\n<ol start=\"2\" type=\"1\">\n<li>scaling</li>\n</ol>\n<p>Switch Transformerscaling</p>\n<p>1Step-Basis</p>\n<p>stepexpert</p>\n<p>stepstep</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_step.png\" class title=\"step scaling\">\n<p>2Time-Basis</p>\n<p>Switch\nTransformerstepSwitch\nTransformerdense</p>\n<p>Switch\nTransformerdenseSwitch\nTransformerdensedense1/7</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_time.png\" class title=\"time scaling\">\n<p>3dense</p>\n<p>Switch\nTransformerdenseSwitch\nTransformerdense</p>\n<p>Step-BasisTime-Basis64Switch\nTransformerT5-LargestepSwitch\nTransformer</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_dense.png\" class title=\"dense\">\n<ol start=\"3\" type=\"1\">\n<li>SFT</li>\n</ol>\n<p>GLUESuperGLUEdense</p>\n<p>eval</p>\n<img src=\"/44e38c1b/switch_transformer_sft_result.png\" class title=\"sft\">\n<ol start=\"4\" type=\"1\">\n<li></li>\n</ol>\n<p>Switch\nTransformerBTdense</p>\n<p><br>\n- Switch\nTransformerdense<br>\n- label25%75%ground truth</p>\n<p>densedensedenseSwitch\nTransformer30%</p>\n<img src=\"/44e38c1b/switch_transformer_distill.png\" class title=\"\">\n<p>99%</p>\n<img src=\"/44e38c1b/switch_transformer_distill_diff_model.png\" class title=\"\">\n<p>SuperGLUE</p>\n<img src=\"/44e38c1b/switch_transformer_distill_sft.png\" class title=\"sft\">\n<h1 id=\"glam\">GLaM</h1>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>202112GoogleGLaM: Efficient Scaling of Language Models\nwith\nMixture-of-Experts1.2T64token96.6BMoE</p>\n<p>Switch TransformerGLaM1.6T\ntoken</p>\n<p></p>\n<img src=\"/44e38c1b/glam_related_model.png\" class title=\"glam\">\n<p>GPT-3175BGPT-3NLPGPT-3</p>\n<img src=\"/44e38c1b/glam_compare_gpt3.png\" class title=\"glamgpt3\">\n<img src=\"/44e38c1b/glam_compare_gpt3_2.png\" class title=\"glamgpt3\">\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p>Switch\nTransformerFFNMoESwitch\nTransformerGLaMexpert</p>\n<img src=\"/44e38c1b/glam_model.png\" class title=\"glam\">\n<p></p>\n<p>1</p>\n<p>XLNET</p>\n<p>2</p>\n<blockquote>\n<p>In the non-MoE Transformer feed-forward sub-layers, we replace the\nfirst linear projection and the activation function with the Gated\nLinear Unitwhich computes the component-wise product of two linear\ntransformation of the input, followed by a Gaussian Error Linear\nUnit.</p>\n</blockquote>\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>trick</p>\n<p>1Lingvo: a modular and scalable framework for\nsequence-to-sequence\nmodelingNaNInf</p>\n<p>2BPNaNInfcheckpointNaNInf</p>\n<p>MoE</p>\n<img src=\"/44e38c1b/glam_family.png\" class title=\"glam\">\n<p>GLaMdense</p>\n<img src=\"/44e38c1b/glam_perf.png\" class title=\"glam\">\n<p>GLaM MoEdense</p>\n<h1 id=\"st-moe\">ST-MoE</h1>\n<p>20222GoogleST-MOE: DESIGNING STABLE AND TRANSFERABLE\nSPARSE EXPERT\nMODELSST-MoEMoEMoE</p>\n<p>ST-MoE269B32B\ndenseStable Transferable\nMixture-of-ExpertsST-MoE-32B</p>\n<p>MoEST-MoESwitch\nTransformer1MoE</p>\n<p>ST-MoE4B269BST-MoE</p>\n<img src=\"/44e38c1b/st_moe_models.png\" class title=\"ST-MoE\">\n<h2 id=\"\"></h2>\n<p></p>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<p></p>\n<blockquote>\n<p>Some architectural improvements involve more multiplications than\nadditions or do not sum many items at once</p>\n</blockquote>\n<p>1GELU Gated Linear Units (GEGLU)</p>\n<p>GLUcomponent-wiseGELU-Linear\nFFNtransformerReLU FFN</p>\n<p><span class=\"math display\">\\[\\begin{aligned}FFN_{GEGLU}(x,W,V,b,c)=GELU(xW+b)\\odot(xV+c)\\end{aligned}\\]</span></p>\n<p></p>\n<p>2RMSNorm</p>\n<p>RMSNorm <span class=\"math inline\">\\(g\\)</span></p>\n<p><span class=\"math display\">\\[y_i=\\frac{x_i}{\\sqrt{\\frac1d\\sum_{i=1}^dx_i^2}}\\cdot\ng_i\\]</span></p>\n<p>ST-MoEGEGLURMSNorm</p>\n<img src=\"/44e38c1b/st_moe_remove_multiplications.png\" class title=\"\">\n<p></p>\n<p>3dense</p>\n<p>ST-MoEexpertdensedense</p>\n<img src=\"/44e38c1b/st_moe_more_dense_layer.png\" class title=\"dense\">\n<p>4bias</p>\n<p>FFNbias\nB</p>\n<p><span class=\"math display\">\\[\\text{FFN}_{\\text{GEGLU}}+\\text{Add\nBias}(x)=[(\\text{GELU}(xW_{11})\\odot xW_{12})+B]W_2\\]</span></p>\n<p><span class=\"math display\">\\[\\mathrm{FFN}_{\\mathrm{GEGLU}}+\\mathrm{Mult~Bias}(x)=[(\\mathrm{GELU}(xW_{11})\\odot\nxW_{12})\\odot B]W_2\\]</span></p>\n<p></p>\n<p></p>\n<ol start=\"2\" type=\"1\">\n<li>noise</li>\n</ol>\n<p>ST-MoE</p>\n<p>input-jitterrouterlogits[1e-2,\n1e2]</p>\n<img src=\"/44e38c1b/st_moe_more_add_noise.png\" class title=\"noise\">\n<p>noise</p>\n<p></p>\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>activationgradient</p>\n<p>ST-MoE269B</p>\n<p>ST-MoErouter z-loss</p>\n<p><span class=\"math display\">\\[L_z(x)=\\frac{1}{B}\\sum_{i=1}^B\\left(\\log\\sum_{j=1}^Ne^{x_j^{(i)}}\\right)^2\\]</span></p>\n<p><span class=\"math inline\">\\(B\\)</span> token<span class=\"math inline\">\\(N\\)</span> <span class=\"math inline\">\\(x\\in\\mathcal{R}^{B\\times N}\\)</span>\nrouter</p>\n<p>z-lossrouterlogitsz-loss</p>\n<img src=\"/44e38c1b/st_moe_z_loss_result.png\" class title=\"z-loss\">\n<p>ST-MoEz-loss</p>\n<p>z-loss <span class=\"math inline\">\\(c_z\\)</span>\n</p>\n<p><span class=\"math display\">\\[L_{tot}=L_{CE}+c_BL_B+c_zL_Z\\]</span></p>\n<p>ST-MoE<span class=\"math inline\">\\(c_z=0.001\\)</span></p>\n<p><span class=\"math inline\">\\(L_B\\)</span>  auxiliary load balance\nlossST-MoEGShard/Switch\nTransformer</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\text{loss}&amp;=\\alpha\\cdot\nN\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax\n}p(x)=i\\}\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(N\\)</span>  <span class=\"math inline\">\\(\\mathcal{B}\\)</span> <span class=\"math inline\">\\(T\\)</span> tokenbatch<span class=\"math inline\">\\(f_{i}\\)</span>  <span class=\"math inline\">\\(i\\)</span> experttoken<span class=\"math inline\">\\(P_i\\)</span> batchtoken<span class=\"math inline\">\\(i\\)</span> expert</p>\n<ol start=\"4\" type=\"1\">\n<li></li>\n</ol>\n<p>float32bfloat16bfloat16allreducebfloat16float32</p>\n<p>ST-MoE-32BallreduceST-MoEallreducefloat32</p>\n<p>bfloat16float32</p>\n<img src=\"/44e38c1b/st_moe_round_error.png\" class title=\"bf16\">\n<p>z-loss</p>\n<p>MoErouter</p>\n<p>ST-MoE1/5token</p>\n<p>softmaxMoE</p>\n<h2 id=\"-2\"></h2>\n<p>densescaling\nlawMoEdense</p>\n<p>1expert</p>\n<p>2routing</p>\n<p>3</p>\n<p>4</p>\n<p>MoEscaling lawUnified scaling laws for routed\nlanguage models</p>\n<ol type=\"1\">\n<li>expert</li>\n</ol>\n<p>ST-MoE8/16/32&lt;1%&gt;256</p>\n<p>&gt;1&lt;=1</p>\n<ol start=\"2\" type=\"1\">\n<li>routingcapacity factor</li>\n</ol>\n<p>capacity factor</p>\n<img src=\"/44e38c1b/st_moe_capacity_factor.png\" class title=\"capacity factor\">\n<p></p>\n<p>1capacity factor</p>\n<p>2capacity\nfacotr</p>\n<p>3expertcapacity\nfactor</p>\n<p>capacity\nfactorcapacity\nfactor</p>\n<p>capacity factor</p>\n<img src=\"/44e38c1b/st_moe_capacity_factor_speed.png\" class title=\"capacity factor\">\n<h2 id=\"-2\"></h2>\n<ol type=\"1\">\n<li>ST-MoE</li>\n</ol>\n<p>ST-MoE-32BST-MoE-32B</p>\n<img src=\"/44e38c1b/st_moe_perf.png\" class title=\"capacity ST-MoE-32B\">\n<ol start=\"2\" type=\"1\">\n<li>Expert Specialization</li>\n</ol>\n<p>decodertokenencoder</p>\n<img src=\"/44e38c1b/st_moe_encoder_specialization.png\" class title=\"encoder\">\n<p>encodertoken</p>\n<img src=\"/44e38c1b/st_moe_multiling_specialization.png\" class title=\"\">\n<h1 id=\"deepseekmoe\">DeepseekMoE</h1>\n<p>20241DeepseekMoEMoEDeepSeekMoE:\nTowards Ultimate Expert Specialization in Mixture-of-Experts Language\nModelsDeepSeekMoE</p>\n<p>DeepSeekMoEMoE2</p>\n<p>1expertexpert</p>\n<p>2expertexpertshared\nexpert</p>\n<p>expert(specialization)</p>\n<p>DeepSeekMoE2BMoE16BMoEDeepSeekMoE-16B40GB</p>\n<p>DeepSeekMoE-2B2BDeepSeekMoE-16B7B40%</p>\n<p>DeepSeekMoE-16B</p>\n<img src=\"/44e38c1b/ds_moe_perf.png\" class title=\"deepseek moe\">\n<p>DeepSeekMoE-2B16B</p>\n<p>DeepSeekMoE-145BMoEDeepSeek-67B</p>\n<h2 id=\"-3\"></h2>\n<p>MoEmixture of\nexpertmotivationexpert</p>\n<p>1991expert</p>\n<p>MoEknowledge hybridityknowledge\nredundancy</p>\n<p>1</p>\n<p>expertexpert</p>\n<p>2</p>\n<p>expertexpertexpertexpert8expertexpert</p>\n<p>(expert\nspecialization)MoE</p>\n<p>expertnon-overlap &amp; foucusd\nknowledge</p>\n<p>DeepSeekMoE2</p>\n<p>1Fine-Grained Expert Segmentation</p>\n<p>expertexpertexpertspecialization16expert2120expert1/464expert8\n<span class=\"math inline\">\\(\\binom{64}8=4,426,165,368\\)</span>\n</p>\n<p>2Shared Expert Isolation</p>\n<p>expertcommon\nknowledgeexpertexpertexpertexpert</p>\n<p>MoEFine-Grained Expert SegmentationShared\nExpert Isolation</p>\n<img src=\"/44e38c1b/ds_moe_structure.png\" class title=\"deepseek moe \">\n<p>expert isolation20221DeepSpeed-MoE:\nAdvancing Mixture-of-Experts Inference and Training to Power\nNext-Generation AI Scale</p>\n<p>MoEexpert <span class=\"math inline\">\\(N\\)</span>expert <span class=\"math inline\">\\(K\\)</span>DeepSeekMoEexpert\n<span class=\"math inline\">\\(1/m\\)</span>DeepSeekMoE <span class=\"math inline\">\\(mN\\)</span> expertexpert <span class=\"math inline\">\\(mK\\)</span> <span class=\"math inline\">\\(T\\)</span> <span class=\"math inline\">\\(L\\)</span> <span class=\"math inline\">\\(e_i^l\\)</span>  <span class=\"math inline\">\\(i\\)</span>\nexpertDeepSeekMoElayernorm</p>\n<p><span class=\"math display\">\\[\\mathbf{u}_{1:T}^l=\\text{Self-Att}\\left(\\mathbf{h}_{1:T}^{l-1}\\right)+\\mathbf{h}_{1:T}^{l-1}\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{h}_t^l=\\sum_{i=1}^{mN}\\left(g_{i,t}\\text{\nFFN}_i\\left(\\mathbf{u}_t^l\\right)\\right)+\\mathbf{u}_t^l\\]</span></p>\n<p><span class=\"math display\">\\[g_{i,t}=\\begin{cases}s_{i,t},&amp;s_{i,t}\\in\\text{Topk}(\\{s_{j,t}|1\\leqslant\nj\\leqslant mN\\},mK)\\\\0,&amp;\\text{otherwise,}\\end{cases}\\]</span></p>\n<p><span class=\"math display\">\\[s_{i,t}=\\mathrm{Softmax}_i\\left({\\mathbf{u}_t^l}^T\\mathbf{e}_i^l\\right)\\]</span></p>\n<h2 id=\"-2\"></h2>\n<p>MoEgating</p>\n<p>1routing\ncollapsegatingexpert</p>\n<p>2</p>\n<p>routing collapseDeepSeekMoEexpert-level balance\nloss</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}_{\\mathrm{ExpBal}}&amp; =\\alpha_1\\sum_{i=1}^{N&#39;}f_iP_i\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nf_{i}&amp;\n=\\frac{N^{\\prime}}{K^{\\prime}T}\\sum_{t=1}^T\\mathbb{1}(\\text{Token\n}t\\text{ selects Expert }i)\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nP_{i}&amp; =\\frac1T\\sum_{t=1}^Ts_{i,t}\n\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(\\alpha_1\\)</span> expert-level\nbalance factor</p>\n<p> <span class=\"math inline\">\\(f_i\\)</span>  <span class=\"math inline\">\\(P_i\\)</span> Switch\nTransformer</p>\n<p>Switch Transformer <span class=\"math inline\">\\(f_i\\)</span>\n <span class=\"math inline\">\\(i\\)</span>\nexperttokenDeepSeekMoE\n<span class=\"math inline\">\\(N&#39;/K&#39;\\)</span>  <span class=\"math inline\">\\(N&#39;=mN-K_s\\)</span><span class=\"math inline\">\\(K&#39;=mK-K_s\\)</span><span class=\"math inline\">\\(K_s\\)</span>\nexpertDeepSeekMoE\n<span class=\"math inline\">\\(f_i\\)</span> Switch\nTransformer</p>\n<p><span class=\"math inline\">\\(N&#39;/K&#39;\\)</span>\nexpertloss</p>\n<p><span class=\"math inline\">\\(P_i\\)</span> token\n<span class=\"math inline\">\\(i\\)</span> expertSwitch\nTransformer</p>\n<p> <span class=\"math inline\">\\(f_i\\)</span> <span class=\"math inline\">\\(P_i\\)</span> </p>\n<p>DeepSeekMoEdevice-level balance\nloss</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}_{\\mathrm{DevBal}}&amp; =\\alpha_2\\sum_{i=1}^Df_i&#39;P_i&#39;\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nf_i^{\\prime}&amp; =\\frac1{|\\mathcal{E}_i|}\\sum_{j\\in\\mathcal{E}_i}f_j\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nP_{i}^{\\prime}&amp; =\\sum_{j\\in\\mathcal{E}_i}P_j\n\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(\\alpha_2\\)</span> device-level\nbalance factor</p>\n<p><span class=\"math inline\">\\(\\mathcal{E}_i\\)</span>  <span class=\"math inline\">\\(i\\)</span> </p>\n<p>device-level balance lossexpert-level balance loss\n<span class=\"math inline\">\\(f_i\\)</span>  <span class=\"math inline\">\\(P_i\\)</span>\nexpert</p>\n<p>expert64expert8tokentokenexpert</p>\n<p>expert</p>\n<h2 id=\"-3\"></h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>100B\ntokenDeepSeekMoE-2BBPE8k</p>\n<p>DeepSeekMoE-2B0.006multi-head\nattention0.3B</p>\n<img src=\"/44e38c1b/ds_model_param.png\" class title=\"\">\n<p>relative expert\nsizeDeepSeekMoEexpertFFN</p>\n<p></p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">optimizer</td>\n<td style=\"text-align: center;\">AdamW</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">adam_beta_1</td>\n<td style=\"text-align: center;\">0.9</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">adam_beta_2</td>\n<td style=\"text-align: center;\">0.95</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">adam_weight_decay</td>\n<td style=\"text-align: center;\">0.1</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">warmup schedule</td>\n<td style=\"text-align: center;\">linear</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">warmup step</td>\n<td style=\"text-align: center;\">2000</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">max lr</td>\n<td style=\"text-align: center;\">1.08e-3</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">dropout</td>\n<td style=\"text-align: center;\">0</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">sequence length</td>\n<td style=\"text-align: center;\">2k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">batch size</td>\n<td style=\"text-align: center;\">2k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">total step</td>\n<td style=\"text-align: center;\">25,000</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p><br>\n- expertGPUdevice-level balance loss<br>\n- expert-level balance factor0.01<br>\n- 80%0.31690%0.316</p>\n<p>100BDeepSeekMoE-2Bbenchmark4densehash\nlayermoeHash layers for large sparse modelsSwitch\nTransformerGShard</p>\n<img src=\"/44e38c1b/ds_moe_comparison.png\" class title=\"deepseek moe \">\n<p><br>\n- Hash LayerSwitch\nTransformerdense<br>\n- GSshardHash LayerSwitch\nTransformer<br>\n- DeepSeekMoEGShard</p>\n<p>DeepSeekMoEdenseGShardDeepSeekMoE-2B</p>\n<p>denseGShard161.5DeepSeekMoE-2B</p>\n<img src=\"/44e38c1b/ds_moe_upper_bound_2b.png\" class title=\"deepseek moe upper bound\">\n<p>DeepSeekMoEDeepSeekMoE-13B,\n1.21.5GShardDeepSeekMoE-13Bmatch</p>\n<img src=\"/44e38c1b/ds_moe_upper_bound_13b.png\" class title=\"deepseek moe upper bound\">\n<ol start=\"2\" type=\"1\">\n<li>DeepSeekMoE</li>\n</ol>\n<p>DeepSeekMoEshared expertfine-grained\nexpertexpert</p>\n<img src=\"/44e38c1b/ds_moe_ablation.png\" class title=\"deepseek moe upper bound \">\n<p>1</p>\n<p>2</p>\n<p>364expert1/2/4pileloss1.808,1.806,1.8111:32+6</p>\n<ol start=\"3\" type=\"1\">\n<li>expert specialization</li>\n</ol>\n<p>DeepSeekMoEexpert specialization</p>\n<p>1DeepSeekMoE-2B1.5GShardtop</p>\n<p></p>\n<img src=\"/44e38c1b/ds_moe_expert_specialization.png\" class title=\"\">\n<p>DeepSeekMoEDeepSeekMoE</p>\n<p>2DeepSeekMoEloss</p>\n<p>3GShardDeepSeekMoE</p>\n<img src=\"/44e38c1b/ds_moe_less_activated_expert.png\" class title=\"\">\n<p>132b2+6GShardDeepSeekMoE</p>\n<img src=\"/44e38c1b/ds_2b_less_expert.png\" class title=\"2B\">\n<ol type=\"1\">\n<li>DeepSeekMoE-16B</li>\n</ol>\n<p>DeepSeekMoE-16B2TLLAMA2-7B100k</p>\n<img src=\"/44e38c1b/ds_model_param.png\" class title=\"\">\n<p>MoE</p>\n<p>MoEloss</p>\n<p>DeepSeekMoE-16B6426gating\nfunctiontoken8token16.4B2.8B</p>\n<p>dimension</p>\n<p><br>\n- lr = 4.2e-4<br>\n- 80%90%lr0.316<br>\n- batch size = 4.5k4kbatch18M\ntoken2T10.6w<br>\n- pipeline parallelism</p>\n<p>expert level balance\nloss0.001</p>\n<p>DeepSeekMoE-16BDeepSeek-7B</p>\n<img src=\"/44e38c1b/ds_16b_perf_1.png\" class title=\"DeepSeek-7B\">\n<p>DeepSeekMoE-16BLLAMA2-7B</p>\n<img src=\"/44e38c1b/ds_16b_perf_2.png\" class title=\"LLAMA2-7B\">\n<ol start=\"5\" type=\"1\">\n<li>DeepSeekMoE-145B</li>\n</ol>\n<p>245BtokenDeepSeekMoE-145BDeepSeek-67B</p>\n<img src=\"/44e38c1b/ds_moe_145b.png\" class title=\"145b\">\n<h1 id=\"dbrx\">DBRX</h1>\n<p>2024327DatabricksDBRX132B36BMoE</p>\n<p>DBRXRoPEGLUGQAfine-grained\nexpert16token4MixtralGrok-182DBRX</p>\n<p>DBRX32k12TtokenDBRX3072H100post-trainingred-team3</p>\n<p>DBRXGPT-3.5Gemini 1.0\nProCodeLLaMA-70B</p>\n<img src=\"/44e38c1b/dbrx_perf.png\" class title=\"DBRX\">\n<p>DBRX</p>\n<img src=\"/44e38c1b/dbrx_infer_efficiency.png\" class title=\"\">\n<h1 id=\"qwen1.5-moe\">Qwen1.5-MoE</h1>\n<p>2024328Qwen1.5-MoE-A2.7B2.7BQwen1.5-7B</p>\n<p>Qwen1.5-MoE-A2.7BDeepSeekMoEDBRXfine-grained\nexpert64token84</p>\n<p>Qwen1.5-MoE-A2.7BQwen-1.8B</p>\n<p>Qwen1.5-MoE-A2.7B</p>\n<img src=\"/44e38c1b/qwen1.5_moe_perf.png\" class title=\"Qwen1.5-MoE-A2.7B\">\n<p>Qwen1.5-MoE-A2.7Bnon-embedding7B</p>\n<img src=\"/44e38c1b/qwen1.5_moe_params.png\" class title=\"Qwen1.5-MoE-A2.7B\">\n<p>Qwen1.5-MoE-A2.7BQwen1.5-7B75%</p>\n<p>A100-80GvLLMQwen1.5-7BQwen1.5-MoE-A2.7B</p>\n<p>/token1000token1000TPSthroughput</p>\n<img src=\"/44e38c1b/qwen1.5_moe_tps.png\" class title=\"Qwen1.5-MoE-A2.7B TPS\">\n<p>MoEdenseQwen1.5-MoE-A2.7BQwen1.5-7B1.74</p>\n<h1 id=\"mistral\">Mistral</h1>\n<h2 id=\"mistral-8x7b\">Mistral 8x7B</h2>\n<p>20231211Mistral\nAIMistral-8x7Btoken82</p>\n<p>Mistral-8x7B32kLLAM2-70BGPT-3.5</p>\n<img src=\"/44e38c1b/mistral_8_7b_perf.png\" class title=\"Mistral 8x7B\">\n<p>Mistral-8x7BLLAM2-70B6</p>\n<p>LLAM2-13B</p>\n<img src=\"/44e38c1b/mistral_8_7b_active_perf.png\" class title=\"Mistral 8x7B\">\n<h2 id=\"mistral-8x22b\">Mistral 8x22B</h2>\n<p>2024417Mistral\nAIMistral-8x22B141B39BMoE</p>\n<p>Mistral-8x22BMistral-8x7B32k64kMistral-8x22Bfunction\ncall</p>\n<p></p>\n<img src=\"/44e38c1b/mistral_8_22b_reasoning.png\" class title=\"Mistral 8x22B reasoning\">\n<img src=\"/44e38c1b/mistral_8_22b_multiling.png\" class title=\"Mistral 8x22B \">\n<img src=\"/44e38c1b/mistral_8_22b_code.png\" class title=\"Mistral 8x22B \">\n<h1 id=\"\"></h1>\n<ul>\n<li>MoEdenseMoE<br>\n</li>\n<li>MoEMoE<br>\n</li>\n<li>denseMoE<br>\n</li>\n<li><br>\n</li>\n<li>GShardSwitch Transformer<br>\n</li>\n<li>MoEMoE</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Adaptive Mixtures of Local Experts\nhttps://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf<br>\n2Outrageously Large Neural Networks: The Sparsely-Gated\nMixture-of-Experts Layer https://arxiv.org/abs/1701.06538<br>\n3GShard: Scaling Giant Models with Conditional Computation and\nAutomatic Sharding https://arxiv.org/abs/2006.16668<br>\n4Switch Transformers: Scaling to Trillion Parameter Models with\nSimple and Efficient Sparsity https://arxiv.org/abs/2101.03961<br>\n5GLaM: Efficient Scaling of Language Models with Mixture-of-Experts\nhttps://arxiv.org/abs/2112.06905<br>\n6ST-MoE: Designing Stable and Transferable Sparse Expert Models\nhttps://arxiv.org/abs/2202.08906<br>\n7DeepSeekMoE: Towards Ultimate Expert Specialization in\nMixture-of-Experts Language Models\nhttps://arxiv.org/abs/2401.06066<br>\n8Introducing DBRX: A New State-of-the-Art Open LLM\nhttps://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm<br>\n9Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated\nParameters https://qwenlm.github.io/zh/blog/qwen-moe/</p>\n"},{"title":"Attention:MHA,MQAGQA","abbrlink":"3dc22f96","date":"2024-03-05T10:49:38.000Z","_content":"\n//  \n\nAttentionMHAMulti-Head AttentionMQAMulti-Query AttentionGQAGrouped-Query AttentionKV Cache  \n\nAttentionFlashAttentionSliding Window Attention  \n\nLLM\n\n# AttentionRNNAttention\n\nattention\n\nattention\n\n## RNN\n\n> Memory is attention through time. ~ Alex Graves 2020\n\nTransformerRNNSeq2Seq\n\n{% asset_img seq2seq.png seq2seq %}  \n\n{% asset_img encoder.png encoder %}  \n\n{% asset_img decoder.png decoder %}  \n\n[AI Summer](https://theaisummer.com/attention/)  \n\nRNN cellhidden stateRNN encodercontext $z$ RNN decoder $z$ decodertoken[start]  \n\n $z$   \n\n  \n\nLSMTGRU  \n\n $z$  $z$   \n\n $z$   \n\n  \n\nCNNheatmap  \n\n{% asset_img cnn_heatmap.png heatmap %}  \n\nCNNimplicitly\n\nSeq2Seqimplicitexplicit  \n\nRNN $i$ $h_i$  $h_i$  $h_i$ hidden state  \n\n --   \n\n $i$ decoder $y_{i-1}$ encoder $\\mathbf{h}$ score\n\n$$\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in R^n$$  \n\n\n\n$$e_{ij}=\\text{attentiom}_{\\text{net }(\\mathbf{y}_{i-1},h_j)}$$  \n\n $\\mathbf{y}_{i-1}$  $h_j$  $e_{ij}$fc  \n \n  $e_{ij}$ attention netencoder hidden statesoftmax  \n\n$$\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}$$  \n\ndecoder  \n\n$$z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j$$\n\n{% asset_img seq2seq_attention.png seq2seq attention %}  \n\nattention netdecoderhidden stateencoder hidden state\n\nattentionattention  \n\n{% asset_img attention_calculation.png attention calculation %}  \n\nattention $\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot h$  $s$ decoderhidden state $y$ $h$ encoderhidden state  \n\nscaled dot-product attention  \n\n## Transformerattention\n\nRNN attentiontransformer attentionAttention Is All You NeedRNNtime stepattentionhidden stateattention  \n\n{% asset_img transformer_structure.png transformer structure.png %}  \n\nencoderdecoderencoderattentionself-attentiondecoderself-attentioncross-attention  \n\ntransformerattention $\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V$ $Q=W_{Q}YK=W_{K}XV=W_{V}X$ cross-attention $X$ encoderhidden states$Y$ decoderhidden statesself-attention $X=Y$  \n\nscaled dot-product attentionsoftmax\n\n$$\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V$$  \n\nattention  \n\n\n\n{% asset_img Scaled-dot-product-self-attention.pbm self-attention %}  \n\nquerykeyvalueattention  \n\n+  \n\n-30keyvalue  \n\n30querykey5  \n\ntop5 $[8,4,4,2,2]$  $[5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]$  \n \n1 $[0.4,0.2,0.2,0.1,0.1]$ 30 $0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}$ \n\ntransformer attention $QK^T$ softmax/  \n\nself-attention $QKV$  $X$sequencetokencross-attentiondecodersequence  \n\nself-attention $QKV$  $X$  $QK^T$  $QK^T$ MHA  \n\nattentionMHAMQAGQA\n\n[pytorch forcasting](https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention)\n\n```python\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self, dropout: float = None, scale: bool = True):\n        super(ScaledDotProductAttention, self).__init__()\n        if dropout is not None:\n            self.dropout = nn.Dropout(p=dropout)\n        else:\n            self.dropout = dropout\n        self.softmax = nn.Softmax(dim=2)\n        self.scale = scale\n\ndef forward(self, q, k, v, mask=None):\n        attn = torch.bmm(q, k.permute(0, 2, 1))  # query-key overlap\n\n        if self.scale:\n            dimension = torch.as_tensor(k.size(-1), dtype=attn.dtype, device=attn.device).sqrt()\n            attn = attn / dimension\n\n        if mask is not None:\n            attn = attn.masked_fill(mask, -1e9)\n        attn = self.softmax(attn)\n\n        if self.dropout is not None:\n            attn = self.dropout(attn)\n        output = torch.bmm(attn, v)\n        return output, attn\n```\n\n## scaling\n\nBTW $QK^T$  $\\sqrt{d}$   \n\nsoftmaxsoftmax  \n\n{% asset_img softmax.png softmax %}  \n\n[](https://spaces.ac.cn/archives/8620)attentionscaling $\\sqrt{d}$ softmaxnormalizationattentionscaling  \n\n# MHA\n\nattentionMHAmulti-head attention\n\nMHA2017Attention Is All You Needattentionattention  \n\n$$\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)$$  \n\n$$head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)$$  \n\nhidden size $d$ MHA $QKV$ hidden state $head_{num}$  $d_{head}$  $head_{num}$  $QKV$ attention  $head_{num}$  $d_{head}$ concat  \n\namazing  \n\n{% asset_img multihead_attention.png MHA %}  \n\n  \n\nAttention Is All You Need\n\n>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.  \n\nattention headattention head  \n\nCNN $3\\times3\\times128$ 128 $3\\times3$  $3\\times3$   \n\n$$\\left.\\left[\\begin{matrix}1&0&-1\\\\1&0&-1\\\\1&0&-1\\end{matrix}\\right.\\right]$$  \n\n  \n\n$$\\left.\\left[\\begin{matrix}1&1&1\\\\0&0&0\\\\-1&-1&-1\\end{matrix}\\right.\\right]$$  \n\n  \n\n128 $3\\times3$ 128MHA  \n\nexpect  \n\n[](https://zhuanlan.zhihu.com/p/626820422)12 $QK^T$   \n\nMHAattentionattention\n\n  \n\n[Are Sixteen Heads Really Better than One?](https://arxiv.org/pdf/1905.10650.pdf)MHA  \n\nhidden sizeLLM1216244896  \n\n[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)MHA  \n\n```python\ndef attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n             / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim = -1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\n\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        '''\n        h: head number\n        '''\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d\n        self.d = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n        \n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n        \n        # 1) Do all the linear projections in batch from d_model => h x d \n        query, key, value = \\\n            [l(x).view(nbatches, -1, self.h, self.d).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key, value))]\n        \n        # 2) Apply attention on all the projected vectors in batch. \n        x, self.attn = attention(query, key, value, mask=mask, \n                                 dropout=self.dropout)\n        \n        # 3) \"Concat\" using a view and apply a final linear. \n        x = x.transpose(1, 2).contiguous() \\\n             .view(nbatches, -1, self.h * self.d)\n        return self.linears[-1](x)\n```\n\n[transformers](https://github.com/huggingface/transformers)\n\n# KV Cache\n\nMQAGQAKV Cache  \n\nencoder-decoderAGIdecoder-onlyLLMauto-regressive  \n\n $\\text{input}_{i-1}$  $\\text{token}_{i}$  $\\text{token}_{i}$  $\\text{input}_{i-1}$  $\\text{input}_{i}$  $\\text{input}_{i}$  $\\text{token}_{i+1}$   \n\ntokentoken  \n\n```\nstep0: =[BOS]=\nstep1: =[BOS]=\nstep2: =[BOS]=\nstep3: =[BOS]=\nstep4: =[BOS]=\nstep5: =[BOS]=[EOS]\n```\n\n[BOS][EOS]  \n\nhidden state \n\nstepsteptokenstepstep\n\n\n\nattention  \n\n$$\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n$$\n\ndecodermask attention\n\n34attention\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n$$\n\n45attention\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n$$\n\n5 $o_{0}$  $o_{2}$   \n\n  \n\n  \n\nstep0101step515instruction800stepstep0800step1799...\n\nstep  \n\nKV Cache  \n\n $k$  $v$   \n\n34 $k$  $v$ \n\n\n\n$$\n\\text{cache}_l=\\text{None}\\\\\n$$\n\n<center></center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$  \n\nkv_cache $l$ \n\n5 $l$ <u>****</u> $k$  $v$  $o_{3}$  $o_{0}o_{1}o_{2}$   \n\n $l$  $o_{0}o_{1}o_{2}$ FNN $l+1$  $l+1$  $k$  $v$  $l+1$  $k$  $v$   \n\n $k$  $v$ \n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$\n\n<center></center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n$$  \n\nattentionFFN  \n\ntransformersuse_cache=TrueKV Cache  \n\nGPT2  \n\n```python\nClass GPT2Attention(nn.Module):\n    ...\n    ...\n    def forward(\n        self,\n        hidden_states: Optional[Tuple[torch.FloatTensor]],\n        layer_past: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n        if encoder_hidden_states is not None:\n            if not hasattr(self, \"q_attn\"):\n                raise ValueError(\n                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n                )\n\n            query = self.q_attn(hidden_states)\n            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n            attention_mask = encoder_attention_mask\n        else:\n            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n\n        query = self._split_heads(query, self.num_heads, self.head_dim)\n        key = self._split_heads(key, self.num_heads, self.head_dim)\n        value = self._split_heads(value, self.num_heads, self.head_dim)\n\n        # \n        if layer_past is not None:\n            past_key, past_value = layer_past\n            key = torch.cat((past_key, key), dim=-2)  # key\n            value = torch.cat((past_value, value), dim=-2)  # value\n\n        if use_cache is True:\n            present = (key, value)  # \n        else:\n            present = None\n\n        if self.reorder_and_upcast_attn:\n            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n        else:\n            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n\n        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n        attn_output = self.c_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output)\n\n        outputs = (attn_output, present)\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs  # a, present, (attentions)\n```\n\nKV Cachedecodermask attentiontokentoken  \n\nKV Cache  \n\n $s$  $L$ hidden size $d$   \n\n$$\n2\\times L\\times s\\times d\n$$  \n\n\n\n$$\n2\\times 2\\times L\\times s\\times d\n$$  \n\nLlama2 7B $L=32$  $L=4096$ token524,288 bytes52K $s=1024$ 536,870,912 bytes500M  \n\nbatch size=1batch size1G  \n\nMHA $qkv$ \n\n\n\n{% asset_img gpu_cache.png gpu cache %}  \n\nH10050ML2 CacheL1 CacheLlama2 7B100token  \n\nLLM34B/70B\n\nL2 CacheHBML2 Cache  \n\n{% asset_img sram_dram.png  %}  \n\n  \n\nCache  \n\n  \n\n# MQA\n\nMQA\n\nGoogle2019Fast Transformer Decoding: One Write-Head is All You NeedMQABert  \n\nMQAMHA $W_{Q}W_{K}W_{V}$ nn= $d_{model}$  $d_{head}$ attentionMQA $Q$ MHA $KV$  $d_{head}$ nQuery $KV$ attention  \n\nMHA $KV$ MQA $KV$ MHA  \n\n{% asset_img MQA.webp MQA %}  \n\n $KV$   \n\nLlama2 7B32MQA1024token1/32536,870,912 bytes / 32 = 16,777,216 bytes16M\n\n $KV$   \n\nMQAMHAhidden sizehead num  \n\n{% asset_img mqa_result_1.png MQA results 1 %}  \n\n{% asset_img mqa_result_3.png MQA results 3 %}  \n\n# GQA  \n\nMQAMHAGQAGrouped-Query AttentionMQAMHA  \n\nGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints2023\n\nGQA $Q$ MHA/MQA $KV$  $Q$  $Q$ groupgroup $Q$  $KV$ group $Q$  $KV$   \n\nMHA $KV$ GQAMQA $KV$ GQA  \n\n\n\n{% asset_img GQA.png GQA %}  \n\n  \n\n{% asset_img GQA_result_1.png GQA result %}  \n\n2/3/4GQAMHAMQAMHAMQAGQAaverage poolingMHAMHAGQA  \n\nLlama2GQAtech reportMHAMQAGQA  \n\n{% asset_img llama2_qga.png llama2 GQA result %}  \n\n#   \n\nMHAMQAGQA\n\nGQALLM  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n1The Annotated Transformer \n https://nlp.seas.harvard.edu/2018/04/03/attention.html  \n2Attention Is All You Need https://arxiv.org/pdf/1706.03762.pdf  \n3Fast Transformer Decoding: One Write-Head is All You Need https://arxiv.org/pdf/1911.02150.pdf  \n4https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096  \n5GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf  \n6How Attention works in Deep Learning: understanding the attention mechanism in sequence models https://theaisummer.com/attention/  \n7A simple overview of RNN, LSTM and Attention Mechanism \nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b  \n8https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention  \n9Transformer https://spaces.ac.cn/archives/8620  \n10https://theaisummer.com/self-attention/  https://theaisummer.com/self-attention/  \n11https://zhuanlan.zhihu.com/p/626820422 https://zhuanlan.zhihu.com/p/626820422  \n12Are Sixteen Heads Really Better than One? \nhttps://arxiv.org/pdf/1905.10650.pdf  \n13This post is all you needTransformer \nhttps://zhuanlan.zhihu.com/p/420820453  \n14The Illustrated Transformer https://jalammar.github.io/illustrated-transformer/  \n15Multi-Query Attention is All You Need https://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055  \n\n","source":"_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA.md","raw":"---\ntitle: 'Attention:MHA,MQAGQA'\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - attention\n  - KV Cache\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 3dc22f96\ndate: 2024-03-05 18:49:38\n---\n\n//  \n\nAttentionMHAMulti-Head AttentionMQAMulti-Query AttentionGQAGrouped-Query AttentionKV Cache  \n\nAttentionFlashAttentionSliding Window Attention  \n\nLLM\n\n# AttentionRNNAttention\n\nattention\n\nattention\n\n## RNN\n\n> Memory is attention through time. ~ Alex Graves 2020\n\nTransformerRNNSeq2Seq\n\n{% asset_img seq2seq.png seq2seq %}  \n\n{% asset_img encoder.png encoder %}  \n\n{% asset_img decoder.png decoder %}  \n\n[AI Summer](https://theaisummer.com/attention/)  \n\nRNN cellhidden stateRNN encodercontext $z$ RNN decoder $z$ decodertoken[start]  \n\n $z$   \n\n  \n\nLSMTGRU  \n\n $z$  $z$   \n\n $z$   \n\n  \n\nCNNheatmap  \n\n{% asset_img cnn_heatmap.png heatmap %}  \n\nCNNimplicitly\n\nSeq2Seqimplicitexplicit  \n\nRNN $i$ $h_i$  $h_i$  $h_i$ hidden state  \n\n --   \n\n $i$ decoder $y_{i-1}$ encoder $\\mathbf{h}$ score\n\n$$\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in R^n$$  \n\n\n\n$$e_{ij}=\\text{attentiom}_{\\text{net }(\\mathbf{y}_{i-1},h_j)}$$  \n\n $\\mathbf{y}_{i-1}$  $h_j$  $e_{ij}$fc  \n \n  $e_{ij}$ attention netencoder hidden statesoftmax  \n\n$$\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}$$  \n\ndecoder  \n\n$$z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j$$\n\n{% asset_img seq2seq_attention.png seq2seq attention %}  \n\nattention netdecoderhidden stateencoder hidden state\n\nattentionattention  \n\n{% asset_img attention_calculation.png attention calculation %}  \n\nattention $\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot h$  $s$ decoderhidden state $y$ $h$ encoderhidden state  \n\nscaled dot-product attention  \n\n## Transformerattention\n\nRNN attentiontransformer attentionAttention Is All You NeedRNNtime stepattentionhidden stateattention  \n\n{% asset_img transformer_structure.png transformer structure.png %}  \n\nencoderdecoderencoderattentionself-attentiondecoderself-attentioncross-attention  \n\ntransformerattention $\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V$ $Q=W_{Q}YK=W_{K}XV=W_{V}X$ cross-attention $X$ encoderhidden states$Y$ decoderhidden statesself-attention $X=Y$  \n\nscaled dot-product attentionsoftmax\n\n$$\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V$$  \n\nattention  \n\n\n\n{% asset_img Scaled-dot-product-self-attention.pbm self-attention %}  \n\nquerykeyvalueattention  \n\n+  \n\n-30keyvalue  \n\n30querykey5  \n\ntop5 $[8,4,4,2,2]$  $[5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]$  \n \n1 $[0.4,0.2,0.2,0.1,0.1]$ 30 $0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}$ \n\ntransformer attention $QK^T$ softmax/  \n\nself-attention $QKV$  $X$sequencetokencross-attentiondecodersequence  \n\nself-attention $QKV$  $X$  $QK^T$  $QK^T$ MHA  \n\nattentionMHAMQAGQA\n\n[pytorch forcasting](https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention)\n\n```python\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self, dropout: float = None, scale: bool = True):\n        super(ScaledDotProductAttention, self).__init__()\n        if dropout is not None:\n            self.dropout = nn.Dropout(p=dropout)\n        else:\n            self.dropout = dropout\n        self.softmax = nn.Softmax(dim=2)\n        self.scale = scale\n\ndef forward(self, q, k, v, mask=None):\n        attn = torch.bmm(q, k.permute(0, 2, 1))  # query-key overlap\n\n        if self.scale:\n            dimension = torch.as_tensor(k.size(-1), dtype=attn.dtype, device=attn.device).sqrt()\n            attn = attn / dimension\n\n        if mask is not None:\n            attn = attn.masked_fill(mask, -1e9)\n        attn = self.softmax(attn)\n\n        if self.dropout is not None:\n            attn = self.dropout(attn)\n        output = torch.bmm(attn, v)\n        return output, attn\n```\n\n## scaling\n\nBTW $QK^T$  $\\sqrt{d}$   \n\nsoftmaxsoftmax  \n\n{% asset_img softmax.png softmax %}  \n\n[](https://spaces.ac.cn/archives/8620)attentionscaling $\\sqrt{d}$ softmaxnormalizationattentionscaling  \n\n# MHA\n\nattentionMHAmulti-head attention\n\nMHA2017Attention Is All You Needattentionattention  \n\n$$\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)$$  \n\n$$head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)$$  \n\nhidden size $d$ MHA $QKV$ hidden state $head_{num}$  $d_{head}$  $head_{num}$  $QKV$ attention  $head_{num}$  $d_{head}$ concat  \n\namazing  \n\n{% asset_img multihead_attention.png MHA %}  \n\n  \n\nAttention Is All You Need\n\n>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.  \n\nattention headattention head  \n\nCNN $3\\times3\\times128$ 128 $3\\times3$  $3\\times3$   \n\n$$\\left.\\left[\\begin{matrix}1&0&-1\\\\1&0&-1\\\\1&0&-1\\end{matrix}\\right.\\right]$$  \n\n  \n\n$$\\left.\\left[\\begin{matrix}1&1&1\\\\0&0&0\\\\-1&-1&-1\\end{matrix}\\right.\\right]$$  \n\n  \n\n128 $3\\times3$ 128MHA  \n\nexpect  \n\n[](https://zhuanlan.zhihu.com/p/626820422)12 $QK^T$   \n\nMHAattentionattention\n\n  \n\n[Are Sixteen Heads Really Better than One?](https://arxiv.org/pdf/1905.10650.pdf)MHA  \n\nhidden sizeLLM1216244896  \n\n[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)MHA  \n\n```python\ndef attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n             / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim = -1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\n\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        '''\n        h: head number\n        '''\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d\n        self.d = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n        \n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n        \n        # 1) Do all the linear projections in batch from d_model => h x d \n        query, key, value = \\\n            [l(x).view(nbatches, -1, self.h, self.d).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key, value))]\n        \n        # 2) Apply attention on all the projected vectors in batch. \n        x, self.attn = attention(query, key, value, mask=mask, \n                                 dropout=self.dropout)\n        \n        # 3) \"Concat\" using a view and apply a final linear. \n        x = x.transpose(1, 2).contiguous() \\\n             .view(nbatches, -1, self.h * self.d)\n        return self.linears[-1](x)\n```\n\n[transformers](https://github.com/huggingface/transformers)\n\n# KV Cache\n\nMQAGQAKV Cache  \n\nencoder-decoderAGIdecoder-onlyLLMauto-regressive  \n\n $\\text{input}_{i-1}$  $\\text{token}_{i}$  $\\text{token}_{i}$  $\\text{input}_{i-1}$  $\\text{input}_{i}$  $\\text{input}_{i}$  $\\text{token}_{i+1}$   \n\ntokentoken  \n\n```\nstep0: =[BOS]=\nstep1: =[BOS]=\nstep2: =[BOS]=\nstep3: =[BOS]=\nstep4: =[BOS]=\nstep5: =[BOS]=[EOS]\n```\n\n[BOS][EOS]  \n\nhidden state \n\nstepsteptokenstepstep\n\n\n\nattention  \n\n$$\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n$$\n\ndecodermask attention\n\n34attention\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n$$\n\n45attention\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n$$\n\n5 $o_{0}$  $o_{2}$   \n\n  \n\n  \n\nstep0101step515instruction800stepstep0800step1799...\n\nstep  \n\nKV Cache  \n\n $k$  $v$   \n\n34 $k$  $v$ \n\n\n\n$$\n\\text{cache}_l=\\text{None}\\\\\n$$\n\n<center></center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$  \n\nkv_cache $l$ \n\n5 $l$ <u>****</u> $k$  $v$  $o_{3}$  $o_{0}o_{1}o_{2}$   \n\n $l$  $o_{0}o_{1}o_{2}$ FNN $l+1$  $l+1$  $k$  $v$  $l+1$  $k$  $v$   \n\n $k$  $v$ \n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$\n\n<center></center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n$$  \n\nattentionFFN  \n\ntransformersuse_cache=TrueKV Cache  \n\nGPT2  \n\n```python\nClass GPT2Attention(nn.Module):\n    ...\n    ...\n    def forward(\n        self,\n        hidden_states: Optional[Tuple[torch.FloatTensor]],\n        layer_past: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n        if encoder_hidden_states is not None:\n            if not hasattr(self, \"q_attn\"):\n                raise ValueError(\n                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n                )\n\n            query = self.q_attn(hidden_states)\n            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n            attention_mask = encoder_attention_mask\n        else:\n            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n\n        query = self._split_heads(query, self.num_heads, self.head_dim)\n        key = self._split_heads(key, self.num_heads, self.head_dim)\n        value = self._split_heads(value, self.num_heads, self.head_dim)\n\n        # \n        if layer_past is not None:\n            past_key, past_value = layer_past\n            key = torch.cat((past_key, key), dim=-2)  # key\n            value = torch.cat((past_value, value), dim=-2)  # value\n\n        if use_cache is True:\n            present = (key, value)  # \n        else:\n            present = None\n\n        if self.reorder_and_upcast_attn:\n            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n        else:\n            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n\n        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n        attn_output = self.c_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output)\n\n        outputs = (attn_output, present)\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs  # a, present, (attentions)\n```\n\nKV Cachedecodermask attentiontokentoken  \n\nKV Cache  \n\n $s$  $L$ hidden size $d$   \n\n$$\n2\\times L\\times s\\times d\n$$  \n\n\n\n$$\n2\\times 2\\times L\\times s\\times d\n$$  \n\nLlama2 7B $L=32$  $L=4096$ token524,288 bytes52K $s=1024$ 536,870,912 bytes500M  \n\nbatch size=1batch size1G  \n\nMHA $qkv$ \n\n\n\n{% asset_img gpu_cache.png gpu cache %}  \n\nH10050ML2 CacheL1 CacheLlama2 7B100token  \n\nLLM34B/70B\n\nL2 CacheHBML2 Cache  \n\n{% asset_img sram_dram.png  %}  \n\n  \n\nCache  \n\n  \n\n# MQA\n\nMQA\n\nGoogle2019Fast Transformer Decoding: One Write-Head is All You NeedMQABert  \n\nMQAMHA $W_{Q}W_{K}W_{V}$ nn= $d_{model}$  $d_{head}$ attentionMQA $Q$ MHA $KV$  $d_{head}$ nQuery $KV$ attention  \n\nMHA $KV$ MQA $KV$ MHA  \n\n{% asset_img MQA.webp MQA %}  \n\n $KV$   \n\nLlama2 7B32MQA1024token1/32536,870,912 bytes / 32 = 16,777,216 bytes16M\n\n $KV$   \n\nMQAMHAhidden sizehead num  \n\n{% asset_img mqa_result_1.png MQA results 1 %}  \n\n{% asset_img mqa_result_3.png MQA results 3 %}  \n\n# GQA  \n\nMQAMHAGQAGrouped-Query AttentionMQAMHA  \n\nGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints2023\n\nGQA $Q$ MHA/MQA $KV$  $Q$  $Q$ groupgroup $Q$  $KV$ group $Q$  $KV$   \n\nMHA $KV$ GQAMQA $KV$ GQA  \n\n\n\n{% asset_img GQA.png GQA %}  \n\n  \n\n{% asset_img GQA_result_1.png GQA result %}  \n\n2/3/4GQAMHAMQAMHAMQAGQAaverage poolingMHAMHAGQA  \n\nLlama2GQAtech reportMHAMQAGQA  \n\n{% asset_img llama2_qga.png llama2 GQA result %}  \n\n#   \n\nMHAMQAGQA\n\nGQALLM  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n1The Annotated Transformer \n https://nlp.seas.harvard.edu/2018/04/03/attention.html  \n2Attention Is All You Need https://arxiv.org/pdf/1706.03762.pdf  \n3Fast Transformer Decoding: One Write-Head is All You Need https://arxiv.org/pdf/1911.02150.pdf  \n4https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096  \n5GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf  \n6How Attention works in Deep Learning: understanding the attention mechanism in sequence models https://theaisummer.com/attention/  \n7A simple overview of RNN, LSTM and Attention Mechanism \nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b  \n8https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention  \n9Transformer https://spaces.ac.cn/archives/8620  \n10https://theaisummer.com/self-attention/  https://theaisummer.com/self-attention/  \n11https://zhuanlan.zhihu.com/p/626820422 https://zhuanlan.zhihu.com/p/626820422  \n12Are Sixteen Heads Really Better than One? \nhttps://arxiv.org/pdf/1905.10650.pdf  \n13This post is all you needTransformer \nhttps://zhuanlan.zhihu.com/p/420820453  \n14The Illustrated Transformer https://jalammar.github.io/illustrated-transformer/  \n15Multi-Query Attention is All You Need https://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055  \n\n","slug":"cs/nlp/2024/03/Attention-MHA-MQAGQA","published":1,"updated":"2024-04-27T15:23:22.122Z","comments":1,"layout":"post","photos":[],"_id":"clwwvjmfs007qam4k2deo4dy2","content":"<p>//</p>\n<p>AttentionMHAMulti-Head\nAttentionMQAMulti-Query AttentionGQAGrouped-Query\nAttentionKV\nCache</p>\n<p>AttentionFlashAttentionSliding\nWindow Attention</p>\n<p>LLM</p>\n<h1 id=\"attentionrnnattention\">AttentionRNNAttention</h1>\n<p>attention</p>\n<p>attention</p>\n<h2 id=\"rnn\">RNN</h2>\n<blockquote>\n<p>Memory is attention through time. ~ Alex Graves 2020</p>\n</blockquote>\n<p>TransformerRNNSeq2Seq</p>\n<img src=\"/3dc22f96/seq2seq.png\" class title=\"seq2seq\">\n<img src=\"/3dc22f96/encoder.png\" class title=\"encoder\">\n<img src=\"/3dc22f96/decoder.png\" class title=\"decoder\">\n<p><a href=\"https://theaisummer.com/attention/\">AI\nSummer</a></p>\n<p>RNN cellhidden stateRNN\nencodercontext <span class=\"math inline\">\\(z\\)</span> RNN decoder <span class=\"math inline\">\\(z\\)</span>\ndecodertoken[start]</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p></p>\n<p>LSMTGRU</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p></p>\n<p>CNNheatmap</p>\n<img src=\"/3dc22f96/cnn_heatmap.png\" class title=\"heatmap\">\n<p>CNNimplicitly</p>\n<p>Seq2Seqimplicitexplicit</p>\n<p>RNN <span class=\"math inline\">\\(i\\)</span> <span class=\"math inline\">\\(h_i\\)</span>  <span class=\"math inline\">\\(h_i\\)</span>\n\n<span class=\"math inline\">\\(h_i\\)</span>\nhidden\nstate</p>\n<p>\n--\n</p>\n<p> <span class=\"math inline\">\\(i\\)</span>\ndecoder <span class=\"math inline\">\\(y_{i-1}\\)</span>\nencoder <span class=\"math inline\">\\(\\mathbf{h}\\)</span>\nscore</p>\n<p><span class=\"math display\">\\[\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in\nR^n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[e_{ij}=\\text{attentiom}_{\\text{net\n}(\\mathbf{y}_{i-1},h_j)}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\mathbf{y}_{i-1}\\)</span>\n <span class=\"math inline\">\\(h_j\\)</span>  <span class=\"math inline\">\\(e_{ij}\\)</span>fc</p>\n<p> <span class=\"math inline\">\\(e_{ij}\\)</span>\nattention\nnetencoder hidden statesoftmax</p>\n<p><span class=\"math display\">\\[\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}\\]</span></p>\n<p>decoder</p>\n<p><span class=\"math display\">\\[z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j\\]</span></p>\n<img src=\"/3dc22f96/seq2seq_attention.png\" class title=\"seq2seq attention\">\n<p>attention netdecoderhidden\nstateencoder hidden\nstate</p>\n<p>attentionattention</p>\n<img src=\"/3dc22f96/attention_calculation.png\" class title=\"attention calculation\">\n<p>attention <span class=\"math inline\">\\(\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot\nh\\)</span>  <span class=\"math inline\">\\(s\\)</span>\ndecoderhidden state <span class=\"math inline\">\\(y\\)</span> <span class=\"math inline\">\\(h\\)</span> encoderhidden state</p>\n<p>scaled dot-product\nattention</p>\n<h2 id=\"transformerattention\">Transformerattention</h2>\n<p>RNN attentiontransformer\nattentionAttention Is All You\nNeedRNNtime\nstepattentionhidden\nstateattention</p>\n<img src=\"/3dc22f96/transformer_structure.png\" class title=\"transformer structure.png\">\n<p>encoderdecoderencoderattentionself-attentiondecoderself-attentioncross-attention</p>\n<p>transformerattention <span class=\"math inline\">\\(\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V\\)</span>\n<span class=\"math inline\">\\(Q=W_{Q}YK=W_{K}XV=W_{V}X\\)</span>\ncross-attention <span class=\"math inline\">\\(X\\)</span>\nencoderhidden states<span class=\"math inline\">\\(Y\\)</span>\ndecoderhidden statesself-attention <span class=\"math inline\">\\(X=Y\\)</span></p>\n<p>scaled dot-product attentionsoftmax</p>\n<p><span class=\"math display\">\\[\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V\\]</span></p>\n<p>attention</p>\n<p></p>\n<img src=\"/3dc22f96/Scaled-dot-product-self-attention.pbm\" class title=\"self-attention\">\n<p>querykeyvalueattention</p>\n<p>+</p>\n<p>-30keyvalue</p>\n<p>30querykey5</p>\n<p>top5 <span class=\"math inline\">\\([8,4,4,2,2]\\)</span>  <span class=\"math inline\">\\([5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]\\)</span>\n</p>\n<p>1 <span class=\"math inline\">\\([0.4,0.2,0.2,0.1,0.1]\\)</span>\n30 <span class=\"math inline\">\\(0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}\\)</span>\n</p>\n<p>transformer attention <span class=\"math inline\">\\(QK^T\\)</span>\nsoftmax/</p>\n<p>self-attention <span class=\"math inline\">\\(QKV\\)</span>\n <span class=\"math inline\">\\(X\\)</span>sequencetokencross-attentiondecodersequence</p>\n<p>self-attention <span class=\"math inline\">\\(QKV\\)</span>\n <span class=\"math inline\">\\(X\\)</span>  <span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(QK^T\\)</span>\nMHA</p>\n<p>attentionMHAMQAGQA</p>\n<p><a href=\"https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention\">pytorch\nforcasting</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ScaledDotProductAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, dropout: <span class=\"built_in\">float</span> = <span class=\"literal\">None</span>, scale: <span class=\"built_in\">bool</span> = <span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.dropout = dropout</span><br><span class=\"line\">        self.softmax = nn.Softmax(dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">        self.scale = scale</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, q, k, v, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        attn = torch.bmm(q, k.permute(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))  <span class=\"comment\"># query-key overlap</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.scale:</span><br><span class=\"line\">            dimension = torch.as_tensor(k.size(-<span class=\"number\">1</span>), dtype=attn.dtype, device=attn.device).sqrt()</span><br><span class=\"line\">            attn = attn / dimension</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = attn.masked_fill(mask, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">        attn = self.softmax(attn)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = self.dropout(attn)</span><br><span class=\"line\">        output = torch.bmm(attn, v)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, attn</span><br></pre></td></tr></table></figure>\n<h2 id=\"scaling\">scaling</h2>\n<p>BTW <span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(\\sqrt{d}\\)</span> </p>\n<p>softmaxsoftmax</p>\n<img src=\"/3dc22f96/softmax.png\" class title=\"softmax\">\n<p><a href=\"https://spaces.ac.cn/archives/8620\"></a>attentionscaling\n<span class=\"math inline\">\\(\\sqrt{d}\\)</span>\nsoftmaxnormalizationattentionscaling</p>\n<h1 id=\"mha\">MHA</h1>\n<p>attentionMHAmulti-head\nattention</p>\n<p>MHA2017Attention Is All You\nNeedattentionattention</p>\n<p><span class=\"math display\">\\[\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)\\]</span></p>\n<p><span class=\"math display\">\\[head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)\\]</span></p>\n<p>hidden size <span class=\"math inline\">\\(d\\)</span>\nMHA <span class=\"math inline\">\\(QKV\\)</span>\nhidden state <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span>  <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(QKV\\)</span>\nattention <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span> concat</p>\n<p>amazing</p>\n<img src=\"/3dc22f96/multihead_attention.png\" class title=\"MHA\">\n<p></p>\n<p>Attention Is All You Need</p>\n<blockquote>\n<p>Multi-head attention allows the model to jointly attend to\ninformation from different representation subspaces at different\npositions.</p>\n</blockquote>\n<p>attention\nheadattention\nhead</p>\n<p>CNN <span class=\"math inline\">\\(3\\times3\\times128\\)</span> 128 <span class=\"math inline\">\\(3\\times3\\)</span>\n <span class=\"math inline\">\\(3\\times3\\)</span> </p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;1&amp;1\\\\0&amp;0&amp;0\\\\-1&amp;-1&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p></p>\n<p>128 <span class=\"math inline\">\\(3\\times3\\)</span>\n128MHA</p>\n<p>expect</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/626820422\"></a>12\n<span class=\"math inline\">\\(QK^T\\)</span> </p>\n<p>MHAattentionattention</p>\n<p></p>\n<p><a href=\"https://arxiv.org/pdf/1905.10650.pdf\">Are Sixteen Heads Really\nBetter than One?</a>MHA</p>\n<p>hidden\nsizeLLM1216244896</p>\n<p><a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\">The\nAnnotated Transformer</a>MHA</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">attention</span>(<span class=\"params\">query, key, value, mask=<span class=\"literal\">None</span>, dropout=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class=\"line\">    d_k = query.size(-<span class=\"number\">1</span>)</span><br><span class=\"line\">    scores = torch.matmul(query, key.transpose(-<span class=\"number\">2</span>, -<span class=\"number\">1</span>)) \\</span><br><span class=\"line\">             / math.sqrt(d_k)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        scores = scores.masked_fill(mask == <span class=\"number\">0</span>, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">    p_attn = F.softmax(scores, dim = -<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        p_attn = dropout(p_attn)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultiHeadedAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, h, d_model, dropout=<span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        h: head number</span></span><br><span class=\"line\"><span class=\"string\">        &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> d_model % h == <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"comment\"># We assume d_v always equals d</span></span><br><span class=\"line\">        self.d = d_model // h</span><br><span class=\"line\">        self.h = h</span><br><span class=\"line\">        self.linears = clones(nn.Linear(d_model, d_model), <span class=\"number\">4</span>)</span><br><span class=\"line\">        self.attn = <span class=\"literal\">None</span></span><br><span class=\"line\">        self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, query, key, value, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"comment\"># Same mask applied to all h heads.</span></span><br><span class=\"line\">            mask = mask.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">        nbatches = query.size(<span class=\"number\">0</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 1) Do all the linear projections in batch from d_model =&gt; h x d </span></span><br><span class=\"line\">        query, key, value = \\</span><br><span class=\"line\">            [l(x).view(nbatches, -<span class=\"number\">1</span>, self.h, self.d).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">             <span class=\"keyword\">for</span> l, x <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(self.linears, (query, key, value))]</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class=\"line\">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class=\"line\">                                 dropout=self.dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class=\"line\">        x = x.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous() \\</span><br><span class=\"line\">             .view(nbatches, -<span class=\"number\">1</span>, self.h * self.d)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.linears[-<span class=\"number\">1</span>](x)</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://github.com/huggingface/transformers\">transformers</a></p>\n<h1 id=\"kv-cache\">KV Cache</h1>\n<p>MQAGQAKV\nCache</p>\n<p>encoder-decoderAGIdecoder-onlyLLMauto-regressive</p>\n<p> <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span> \n<span class=\"math inline\">\\(\\text{input}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{input}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i+1}\\)</span>\n</p>\n<p>tokentoken</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">step0: =[BOS]=</span><br><span class=\"line\">step1: =[BOS]=</span><br><span class=\"line\">step2: =[BOS]=</span><br><span class=\"line\">step3: =[BOS]=</span><br><span class=\"line\">step4: =[BOS]=</span><br><span class=\"line\">step5: =[BOS]=[EOS]</span><br></pre></td></tr></table></figure>\n<p>[BOS][EOS]</p>\n<p>hidden\nstate</p>\n<p>stepsteptokenstepstep</p>\n<p></p>\n<p>attention</p>\n<p><span class=\"math display\">\\[\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n\\]</span></p>\n<p>decodermask\nattention</p>\n<p>34attention</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>45attention</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&amp;=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>5 <span class=\"math inline\">\\(o_{0}\\)</span>  <span class=\"math inline\">\\(o_{2}\\)</span> </p>\n<p></p>\n<p></p>\n<p>step0101step515instruction800stepstep0800step1799...</p>\n<p>step</p>\n<p>KV\nCache</p>\n<p> <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p>34\n<span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=\\text{None}\\\\\n\\]</span></p>\n<center>\n\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<p>kv_cache <span class=\"math inline\">\\(l\\)</span>\n</p>\n<p>5 <span class=\"math inline\">\\(l\\)</span>\n<u><strong></strong></u> <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span>  <span class=\"math inline\">\\(o_{3}\\)</span>  <span class=\"math inline\">\\(o_{0}o_{1}o_{2}\\)</span> </p>\n<p> <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(o_{0}o_{1}o_{2}\\)</span>\nFNN <span class=\"math inline\">\\(l+1\\)</span>\n <span class=\"math inline\">\\(l+1\\)</span>\n <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span>  <span class=\"math inline\">\\(l+1\\)</span>  <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p> <span class=\"math inline\">\\(k\\)</span> \n<span class=\"math inline\">\\(v\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<center>\n\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n\\]</span></p>\n<p>attentionFFN</p>\n<p>transformersuse_cache=TrueKV Cache</p>\n<p>GPT2</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Class GPT2Attention(nn.Module):</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">        self,</span></span><br><span class=\"line\"><span class=\"params\">        hidden_states: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.FloatTensor]],</span></span><br><span class=\"line\"><span class=\"params\">        layer_past: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.Tensor]] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        head_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_hidden_states: <span class=\"type\">Optional</span>[torch.Tensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        use_cache: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">        output_attentions: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">    </span>) -&gt; <span class=\"type\">Tuple</span>[<span class=\"type\">Union</span>[torch.Tensor, <span class=\"type\">Tuple</span>[torch.Tensor]], ...]:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> encoder_hidden_states <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">hasattr</span>(self, <span class=\"string\">&quot;q_attn&quot;</span>):</span><br><span class=\"line\">                <span class=\"keyword\">raise</span> ValueError(</span><br><span class=\"line\">                    <span class=\"string\">&quot;If class is used as cross attention, the weights `q_attn` have to be defined. &quot;</span></span><br><span class=\"line\">                    <span class=\"string\">&quot;Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.&quot;</span></span><br><span class=\"line\">                )</span><br><span class=\"line\"></span><br><span class=\"line\">            query = self.q_attn(hidden_states)</span><br><span class=\"line\">            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">            attention_mask = encoder_attention_mask</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        query = self._split_heads(query, self.num_heads, self.head_dim)</span><br><span class=\"line\">        key = self._split_heads(key, self.num_heads, self.head_dim)</span><br><span class=\"line\">        value = self._split_heads(value, self.num_heads, self.head_dim)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># </span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> layer_past <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            past_key, past_value = layer_past</span><br><span class=\"line\">            key = torch.cat((past_key, key), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># key</span></span><br><span class=\"line\">            value = torch.cat((past_value, value), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># value</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> use_cache <span class=\"keyword\">is</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">            present = (key, value)  <span class=\"comment\"># </span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            present = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.reorder_and_upcast_attn:</span><br><span class=\"line\">            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)</span><br><span class=\"line\">        attn_output = self.c_proj(attn_output)</span><br><span class=\"line\">        attn_output = self.resid_dropout(attn_output)</span><br><span class=\"line\"></span><br><span class=\"line\">        outputs = (attn_output, present)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> output_attentions:</span><br><span class=\"line\">            outputs += (attn_weights,)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> outputs  <span class=\"comment\"># a, present, (attentions)</span></span><br></pre></td></tr></table></figure>\n<p>KV\nCachedecodermask\nattentiontokentoken</p>\n<p>KV Cache</p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n <span class=\"math inline\">\\(L\\)</span> hidden size <span class=\"math inline\">\\(d\\)</span> </p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d\n\\]</span></p>\n<p>Llama2 7B <span class=\"math inline\">\\(L=32\\)</span> \n<span class=\"math inline\">\\(L=4096\\)</span>\ntoken524,288 bytes52K <span class=\"math inline\">\\(s=1024\\)</span> 536,870,912\nbytes500M</p>\n<p>batch size=1batch\nsize1G</p>\n<p>MHA <span class=\"math inline\">\\(qkv\\)</span>\n</p>\n<p></p>\n<img src=\"/3dc22f96/gpu_cache.png\" class title=\"gpu cache\">\n<p>H10050ML2 CacheL1\nCacheLlama2\n7B100token</p>\n<p>LLM34B/70B</p>\n<p>L2 CacheHBML2\nCache</p>\n<img src=\"/3dc22f96/sram_dram.png\" class title=\"\">\n<p></p>\n<p>Cache</p>\n<p></p>\n<h1 id=\"mqa\">MQA</h1>\n<p>MQA</p>\n<p>Google2019Fast Transformer Decoding: One Write-Head is All\nYou\nNeedMQABert</p>\n<p>MQAMHA <span class=\"math inline\">\\(W_{Q}W_{K}W_{V}\\)</span>\nnn= <span class=\"math inline\">\\(d_{model}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span>\nattentionMQA <span class=\"math inline\">\\(Q\\)</span> MHA <span class=\"math inline\">\\(KV\\)</span> \n<span class=\"math inline\">\\(d_{head}\\)</span>\nnQuery <span class=\"math inline\">\\(KV\\)</span>\nattention</p>\n<p>MHA <span class=\"math inline\">\\(KV\\)</span>\nMQA <span class=\"math inline\">\\(KV\\)</span>\nMHA</p>\n<img src=\"/3dc22f96/MQA.webp\" class title=\"MQA\">\n<p> <span class=\"math inline\">\\(KV\\)</span>\n</p>\n<p>Llama2\n7B32MQA1024token1/32536,870,912\nbytes / 32 = 16,777,216 bytes16M</p>\n<p> <span class=\"math inline\">\\(KV\\)</span>\n</p>\n<p>MQAMHAhidden\nsizehead num</p>\n<img src=\"/3dc22f96/mqa_result_1.png\" class title=\"MQA results 1\">\n<img src=\"/3dc22f96/mqa_result_3.png\" class title=\"MQA results 3\">\n<h1 id=\"gqa\">GQA</h1>\n<p>MQAMHAGQAGrouped-Query\nAttentionMQAMHA</p>\n<p>GQA: Training Generalized Multi-Query Transformer Models\nfrom Multi-Head Checkpoints2023</p>\n<p>GQA <span class=\"math inline\">\\(Q\\)</span>\nMHA/MQA <span class=\"math inline\">\\(KV\\)</span>\n <span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(Q\\)</span> groupgroup\n<span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(KV\\)</span> group <span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(KV\\)</span> </p>\n<p>MHA <span class=\"math inline\">\\(KV\\)</span>\nGQAMQA <span class=\"math inline\">\\(KV\\)</span> GQA</p>\n<p></p>\n<img src=\"/3dc22f96/GQA.png\" class title=\"GQA\">\n<p></p>\n<img src=\"/3dc22f96/GQA_result_1.png\" class title=\"GQA result\">\n<p>2/3/4GQAMHAMQAMHAMQAGQAaverage\npoolingMHAMHAGQA</p>\n<p>Llama2GQAtech\nreportMHAMQAGQA</p>\n<img src=\"/3dc22f96/llama2_qga.png\" class title=\"llama2 GQA result\">\n<h1 id=\"\"></h1>\n<p>MHAMQAGQA</p>\n<p>GQALLM</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1The Annotated Transformer\nhttps://nlp.seas.harvard.edu/2018/04/03/attention.html<br>\n2Attention Is All You Need\nhttps://arxiv.org/pdf/1706.03762.pdf<br>\n3Fast Transformer Decoding: One Write-Head is All You Need\nhttps://arxiv.org/pdf/1911.02150.pdf<br>\n4https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096<br>\n5GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf<br>\n6How Attention works in Deep Learning: understanding the attention\nmechanism in sequence models https://theaisummer.com/attention/<br>\n7A simple overview of RNN, LSTM and Attention Mechanism\nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b<br>\n8https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention<br>\n9Transformer\nhttps://spaces.ac.cn/archives/8620<br>\n10https://theaisummer.com/self-attention/\nhttps://theaisummer.com/self-attention/<br>\n11https://zhuanlan.zhihu.com/p/626820422\nhttps://zhuanlan.zhihu.com/p/626820422<br>\n12Are Sixteen Heads Really Better than One?\nhttps://arxiv.org/pdf/1905.10650.pdf<br>\n13This post is all you needTransformer\nhttps://zhuanlan.zhihu.com/p/420820453<br>\n14The Illustrated Transformer\nhttps://jalammar.github.io/illustrated-transformer/<br>\n15Multi-Query Attention is All You Need\nhttps://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055</p>\n","length":16733,"excerpt":"","more":"<p>//</p>\n<p>AttentionMHAMulti-Head\nAttentionMQAMulti-Query AttentionGQAGrouped-Query\nAttentionKV\nCache</p>\n<p>AttentionFlashAttentionSliding\nWindow Attention</p>\n<p>LLM</p>\n<h1 id=\"attentionrnnattention\">AttentionRNNAttention</h1>\n<p>attention</p>\n<p>attention</p>\n<h2 id=\"rnn\">RNN</h2>\n<blockquote>\n<p>Memory is attention through time. ~ Alex Graves 2020</p>\n</blockquote>\n<p>TransformerRNNSeq2Seq</p>\n<img src=\"/3dc22f96/seq2seq.png\" class title=\"seq2seq\">\n<img src=\"/3dc22f96/encoder.png\" class title=\"encoder\">\n<img src=\"/3dc22f96/decoder.png\" class title=\"decoder\">\n<p><a href=\"https://theaisummer.com/attention/\">AI\nSummer</a></p>\n<p>RNN cellhidden stateRNN\nencodercontext <span class=\"math inline\">\\(z\\)</span> RNN decoder <span class=\"math inline\">\\(z\\)</span>\ndecodertoken[start]</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p></p>\n<p>LSMTGRU</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p></p>\n<p>CNNheatmap</p>\n<img src=\"/3dc22f96/cnn_heatmap.png\" class title=\"heatmap\">\n<p>CNNimplicitly</p>\n<p>Seq2Seqimplicitexplicit</p>\n<p>RNN <span class=\"math inline\">\\(i\\)</span> <span class=\"math inline\">\\(h_i\\)</span>  <span class=\"math inline\">\\(h_i\\)</span>\n\n<span class=\"math inline\">\\(h_i\\)</span>\nhidden\nstate</p>\n<p>\n--\n</p>\n<p> <span class=\"math inline\">\\(i\\)</span>\ndecoder <span class=\"math inline\">\\(y_{i-1}\\)</span>\nencoder <span class=\"math inline\">\\(\\mathbf{h}\\)</span>\nscore</p>\n<p><span class=\"math display\">\\[\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in\nR^n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[e_{ij}=\\text{attentiom}_{\\text{net\n}(\\mathbf{y}_{i-1},h_j)}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\mathbf{y}_{i-1}\\)</span>\n <span class=\"math inline\">\\(h_j\\)</span>  <span class=\"math inline\">\\(e_{ij}\\)</span>fc</p>\n<p> <span class=\"math inline\">\\(e_{ij}\\)</span>\nattention\nnetencoder hidden statesoftmax</p>\n<p><span class=\"math display\">\\[\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}\\]</span></p>\n<p>decoder</p>\n<p><span class=\"math display\">\\[z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j\\]</span></p>\n<img src=\"/3dc22f96/seq2seq_attention.png\" class title=\"seq2seq attention\">\n<p>attention netdecoderhidden\nstateencoder hidden\nstate</p>\n<p>attentionattention</p>\n<img src=\"/3dc22f96/attention_calculation.png\" class title=\"attention calculation\">\n<p>attention <span class=\"math inline\">\\(\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot\nh\\)</span>  <span class=\"math inline\">\\(s\\)</span>\ndecoderhidden state <span class=\"math inline\">\\(y\\)</span> <span class=\"math inline\">\\(h\\)</span> encoderhidden state</p>\n<p>scaled dot-product\nattention</p>\n<h2 id=\"transformerattention\">Transformerattention</h2>\n<p>RNN attentiontransformer\nattentionAttention Is All You\nNeedRNNtime\nstepattentionhidden\nstateattention</p>\n<img src=\"/3dc22f96/transformer_structure.png\" class title=\"transformer structure.png\">\n<p>encoderdecoderencoderattentionself-attentiondecoderself-attentioncross-attention</p>\n<p>transformerattention <span class=\"math inline\">\\(\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V\\)</span>\n<span class=\"math inline\">\\(Q=W_{Q}YK=W_{K}XV=W_{V}X\\)</span>\ncross-attention <span class=\"math inline\">\\(X\\)</span>\nencoderhidden states<span class=\"math inline\">\\(Y\\)</span>\ndecoderhidden statesself-attention <span class=\"math inline\">\\(X=Y\\)</span></p>\n<p>scaled dot-product attentionsoftmax</p>\n<p><span class=\"math display\">\\[\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V\\]</span></p>\n<p>attention</p>\n<p></p>\n<img src=\"/3dc22f96/Scaled-dot-product-self-attention.pbm\" class title=\"self-attention\">\n<p>querykeyvalueattention</p>\n<p>+</p>\n<p>-30keyvalue</p>\n<p>30querykey5</p>\n<p>top5 <span class=\"math inline\">\\([8,4,4,2,2]\\)</span>  <span class=\"math inline\">\\([5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]\\)</span>\n</p>\n<p>1 <span class=\"math inline\">\\([0.4,0.2,0.2,0.1,0.1]\\)</span>\n30 <span class=\"math inline\">\\(0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}\\)</span>\n</p>\n<p>transformer attention <span class=\"math inline\">\\(QK^T\\)</span>\nsoftmax/</p>\n<p>self-attention <span class=\"math inline\">\\(QKV\\)</span>\n <span class=\"math inline\">\\(X\\)</span>sequencetokencross-attentiondecodersequence</p>\n<p>self-attention <span class=\"math inline\">\\(QKV\\)</span>\n <span class=\"math inline\">\\(X\\)</span>  <span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(QK^T\\)</span>\nMHA</p>\n<p>attentionMHAMQAGQA</p>\n<p><a href=\"https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention\">pytorch\nforcasting</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ScaledDotProductAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, dropout: <span class=\"built_in\">float</span> = <span class=\"literal\">None</span>, scale: <span class=\"built_in\">bool</span> = <span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.dropout = dropout</span><br><span class=\"line\">        self.softmax = nn.Softmax(dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">        self.scale = scale</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, q, k, v, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        attn = torch.bmm(q, k.permute(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))  <span class=\"comment\"># query-key overlap</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.scale:</span><br><span class=\"line\">            dimension = torch.as_tensor(k.size(-<span class=\"number\">1</span>), dtype=attn.dtype, device=attn.device).sqrt()</span><br><span class=\"line\">            attn = attn / dimension</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = attn.masked_fill(mask, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">        attn = self.softmax(attn)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = self.dropout(attn)</span><br><span class=\"line\">        output = torch.bmm(attn, v)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, attn</span><br></pre></td></tr></table></figure>\n<h2 id=\"scaling\">scaling</h2>\n<p>BTW <span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(\\sqrt{d}\\)</span> </p>\n<p>softmaxsoftmax</p>\n<img src=\"/3dc22f96/softmax.png\" class title=\"softmax\">\n<p><a href=\"https://spaces.ac.cn/archives/8620\"></a>attentionscaling\n<span class=\"math inline\">\\(\\sqrt{d}\\)</span>\nsoftmaxnormalizationattentionscaling</p>\n<h1 id=\"mha\">MHA</h1>\n<p>attentionMHAmulti-head\nattention</p>\n<p>MHA2017Attention Is All You\nNeedattentionattention</p>\n<p><span class=\"math display\">\\[\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)\\]</span></p>\n<p><span class=\"math display\">\\[head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)\\]</span></p>\n<p>hidden size <span class=\"math inline\">\\(d\\)</span>\nMHA <span class=\"math inline\">\\(QKV\\)</span>\nhidden state <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span>  <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(QKV\\)</span>\nattention <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span> concat</p>\n<p>amazing</p>\n<img src=\"/3dc22f96/multihead_attention.png\" class title=\"MHA\">\n<p></p>\n<p>Attention Is All You Need</p>\n<blockquote>\n<p>Multi-head attention allows the model to jointly attend to\ninformation from different representation subspaces at different\npositions.</p>\n</blockquote>\n<p>attention\nheadattention\nhead</p>\n<p>CNN <span class=\"math inline\">\\(3\\times3\\times128\\)</span> 128 <span class=\"math inline\">\\(3\\times3\\)</span>\n <span class=\"math inline\">\\(3\\times3\\)</span> </p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;1&amp;1\\\\0&amp;0&amp;0\\\\-1&amp;-1&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p></p>\n<p>128 <span class=\"math inline\">\\(3\\times3\\)</span>\n128MHA</p>\n<p>expect</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/626820422\"></a>12\n<span class=\"math inline\">\\(QK^T\\)</span> </p>\n<p>MHAattentionattention</p>\n<p></p>\n<p><a href=\"https://arxiv.org/pdf/1905.10650.pdf\">Are Sixteen Heads Really\nBetter than One?</a>MHA</p>\n<p>hidden\nsizeLLM1216244896</p>\n<p><a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\">The\nAnnotated Transformer</a>MHA</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">attention</span>(<span class=\"params\">query, key, value, mask=<span class=\"literal\">None</span>, dropout=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class=\"line\">    d_k = query.size(-<span class=\"number\">1</span>)</span><br><span class=\"line\">    scores = torch.matmul(query, key.transpose(-<span class=\"number\">2</span>, -<span class=\"number\">1</span>)) \\</span><br><span class=\"line\">             / math.sqrt(d_k)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        scores = scores.masked_fill(mask == <span class=\"number\">0</span>, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">    p_attn = F.softmax(scores, dim = -<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        p_attn = dropout(p_attn)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultiHeadedAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, h, d_model, dropout=<span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        h: head number</span></span><br><span class=\"line\"><span class=\"string\">        &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> d_model % h == <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"comment\"># We assume d_v always equals d</span></span><br><span class=\"line\">        self.d = d_model // h</span><br><span class=\"line\">        self.h = h</span><br><span class=\"line\">        self.linears = clones(nn.Linear(d_model, d_model), <span class=\"number\">4</span>)</span><br><span class=\"line\">        self.attn = <span class=\"literal\">None</span></span><br><span class=\"line\">        self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, query, key, value, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"comment\"># Same mask applied to all h heads.</span></span><br><span class=\"line\">            mask = mask.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">        nbatches = query.size(<span class=\"number\">0</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 1) Do all the linear projections in batch from d_model =&gt; h x d </span></span><br><span class=\"line\">        query, key, value = \\</span><br><span class=\"line\">            [l(x).view(nbatches, -<span class=\"number\">1</span>, self.h, self.d).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">             <span class=\"keyword\">for</span> l, x <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(self.linears, (query, key, value))]</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class=\"line\">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class=\"line\">                                 dropout=self.dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class=\"line\">        x = x.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous() \\</span><br><span class=\"line\">             .view(nbatches, -<span class=\"number\">1</span>, self.h * self.d)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.linears[-<span class=\"number\">1</span>](x)</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://github.com/huggingface/transformers\">transformers</a></p>\n<h1 id=\"kv-cache\">KV Cache</h1>\n<p>MQAGQAKV\nCache</p>\n<p>encoder-decoderAGIdecoder-onlyLLMauto-regressive</p>\n<p> <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span> \n<span class=\"math inline\">\\(\\text{input}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{input}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i+1}\\)</span>\n</p>\n<p>tokentoken</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">step0: =[BOS]=</span><br><span class=\"line\">step1: =[BOS]=</span><br><span class=\"line\">step2: =[BOS]=</span><br><span class=\"line\">step3: =[BOS]=</span><br><span class=\"line\">step4: =[BOS]=</span><br><span class=\"line\">step5: =[BOS]=[EOS]</span><br></pre></td></tr></table></figure>\n<p>[BOS][EOS]</p>\n<p>hidden\nstate</p>\n<p>stepsteptokenstepstep</p>\n<p></p>\n<p>attention</p>\n<p><span class=\"math display\">\\[\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n\\]</span></p>\n<p>decodermask\nattention</p>\n<p>34attention</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>45attention</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&amp;=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>5 <span class=\"math inline\">\\(o_{0}\\)</span>  <span class=\"math inline\">\\(o_{2}\\)</span> </p>\n<p></p>\n<p></p>\n<p>step0101step515instruction800stepstep0800step1799...</p>\n<p>step</p>\n<p>KV\nCache</p>\n<p> <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p>34\n<span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=\\text{None}\\\\\n\\]</span></p>\n<center>\n\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<p>kv_cache <span class=\"math inline\">\\(l\\)</span>\n</p>\n<p>5 <span class=\"math inline\">\\(l\\)</span>\n<u><strong></strong></u> <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span>  <span class=\"math inline\">\\(o_{3}\\)</span>  <span class=\"math inline\">\\(o_{0}o_{1}o_{2}\\)</span> </p>\n<p> <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(o_{0}o_{1}o_{2}\\)</span>\nFNN <span class=\"math inline\">\\(l+1\\)</span>\n <span class=\"math inline\">\\(l+1\\)</span>\n <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span>  <span class=\"math inline\">\\(l+1\\)</span>  <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p> <span class=\"math inline\">\\(k\\)</span> \n<span class=\"math inline\">\\(v\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<center>\n\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n\\]</span></p>\n<p>attentionFFN</p>\n<p>transformersuse_cache=TrueKV Cache</p>\n<p>GPT2</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Class GPT2Attention(nn.Module):</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">        self,</span></span><br><span class=\"line\"><span class=\"params\">        hidden_states: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.FloatTensor]],</span></span><br><span class=\"line\"><span class=\"params\">        layer_past: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.Tensor]] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        head_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_hidden_states: <span class=\"type\">Optional</span>[torch.Tensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        use_cache: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">        output_attentions: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">    </span>) -&gt; <span class=\"type\">Tuple</span>[<span class=\"type\">Union</span>[torch.Tensor, <span class=\"type\">Tuple</span>[torch.Tensor]], ...]:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> encoder_hidden_states <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">hasattr</span>(self, <span class=\"string\">&quot;q_attn&quot;</span>):</span><br><span class=\"line\">                <span class=\"keyword\">raise</span> ValueError(</span><br><span class=\"line\">                    <span class=\"string\">&quot;If class is used as cross attention, the weights `q_attn` have to be defined. &quot;</span></span><br><span class=\"line\">                    <span class=\"string\">&quot;Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.&quot;</span></span><br><span class=\"line\">                )</span><br><span class=\"line\"></span><br><span class=\"line\">            query = self.q_attn(hidden_states)</span><br><span class=\"line\">            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">            attention_mask = encoder_attention_mask</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        query = self._split_heads(query, self.num_heads, self.head_dim)</span><br><span class=\"line\">        key = self._split_heads(key, self.num_heads, self.head_dim)</span><br><span class=\"line\">        value = self._split_heads(value, self.num_heads, self.head_dim)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># </span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> layer_past <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            past_key, past_value = layer_past</span><br><span class=\"line\">            key = torch.cat((past_key, key), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># key</span></span><br><span class=\"line\">            value = torch.cat((past_value, value), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># value</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> use_cache <span class=\"keyword\">is</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">            present = (key, value)  <span class=\"comment\"># </span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            present = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.reorder_and_upcast_attn:</span><br><span class=\"line\">            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)</span><br><span class=\"line\">        attn_output = self.c_proj(attn_output)</span><br><span class=\"line\">        attn_output = self.resid_dropout(attn_output)</span><br><span class=\"line\"></span><br><span class=\"line\">        outputs = (attn_output, present)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> output_attentions:</span><br><span class=\"line\">            outputs += (attn_weights,)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> outputs  <span class=\"comment\"># a, present, (attentions)</span></span><br></pre></td></tr></table></figure>\n<p>KV\nCachedecodermask\nattentiontokentoken</p>\n<p>KV Cache</p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n <span class=\"math inline\">\\(L\\)</span> hidden size <span class=\"math inline\">\\(d\\)</span> </p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d\n\\]</span></p>\n<p>Llama2 7B <span class=\"math inline\">\\(L=32\\)</span> \n<span class=\"math inline\">\\(L=4096\\)</span>\ntoken524,288 bytes52K <span class=\"math inline\">\\(s=1024\\)</span> 536,870,912\nbytes500M</p>\n<p>batch size=1batch\nsize1G</p>\n<p>MHA <span class=\"math inline\">\\(qkv\\)</span>\n</p>\n<p></p>\n<img src=\"/3dc22f96/gpu_cache.png\" class title=\"gpu cache\">\n<p>H10050ML2 CacheL1\nCacheLlama2\n7B100token</p>\n<p>LLM34B/70B</p>\n<p>L2 CacheHBML2\nCache</p>\n<img src=\"/3dc22f96/sram_dram.png\" class title=\"\">\n<p></p>\n<p>Cache</p>\n<p></p>\n<h1 id=\"mqa\">MQA</h1>\n<p>MQA</p>\n<p>Google2019Fast Transformer Decoding: One Write-Head is All\nYou\nNeedMQABert</p>\n<p>MQAMHA <span class=\"math inline\">\\(W_{Q}W_{K}W_{V}\\)</span>\nnn= <span class=\"math inline\">\\(d_{model}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span>\nattentionMQA <span class=\"math inline\">\\(Q\\)</span> MHA <span class=\"math inline\">\\(KV\\)</span> \n<span class=\"math inline\">\\(d_{head}\\)</span>\nnQuery <span class=\"math inline\">\\(KV\\)</span>\nattention</p>\n<p>MHA <span class=\"math inline\">\\(KV\\)</span>\nMQA <span class=\"math inline\">\\(KV\\)</span>\nMHA</p>\n<img src=\"/3dc22f96/MQA.webp\" class title=\"MQA\">\n<p> <span class=\"math inline\">\\(KV\\)</span>\n</p>\n<p>Llama2\n7B32MQA1024token1/32536,870,912\nbytes / 32 = 16,777,216 bytes16M</p>\n<p> <span class=\"math inline\">\\(KV\\)</span>\n</p>\n<p>MQAMHAhidden\nsizehead num</p>\n<img src=\"/3dc22f96/mqa_result_1.png\" class title=\"MQA results 1\">\n<img src=\"/3dc22f96/mqa_result_3.png\" class title=\"MQA results 3\">\n<h1 id=\"gqa\">GQA</h1>\n<p>MQAMHAGQAGrouped-Query\nAttentionMQAMHA</p>\n<p>GQA: Training Generalized Multi-Query Transformer Models\nfrom Multi-Head Checkpoints2023</p>\n<p>GQA <span class=\"math inline\">\\(Q\\)</span>\nMHA/MQA <span class=\"math inline\">\\(KV\\)</span>\n <span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(Q\\)</span> groupgroup\n<span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(KV\\)</span> group <span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(KV\\)</span> </p>\n<p>MHA <span class=\"math inline\">\\(KV\\)</span>\nGQAMQA <span class=\"math inline\">\\(KV\\)</span> GQA</p>\n<p></p>\n<img src=\"/3dc22f96/GQA.png\" class title=\"GQA\">\n<p></p>\n<img src=\"/3dc22f96/GQA_result_1.png\" class title=\"GQA result\">\n<p>2/3/4GQAMHAMQAMHAMQAGQAaverage\npoolingMHAMHAGQA</p>\n<p>Llama2GQAtech\nreportMHAMQAGQA</p>\n<img src=\"/3dc22f96/llama2_qga.png\" class title=\"llama2 GQA result\">\n<h1 id=\"\"></h1>\n<p>MHAMQAGQA</p>\n<p>GQALLM</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1The Annotated Transformer\nhttps://nlp.seas.harvard.edu/2018/04/03/attention.html<br>\n2Attention Is All You Need\nhttps://arxiv.org/pdf/1706.03762.pdf<br>\n3Fast Transformer Decoding: One Write-Head is All You Need\nhttps://arxiv.org/pdf/1911.02150.pdf<br>\n4https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096<br>\n5GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf<br>\n6How Attention works in Deep Learning: understanding the attention\nmechanism in sequence models https://theaisummer.com/attention/<br>\n7A simple overview of RNN, LSTM and Attention Mechanism\nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b<br>\n8https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention<br>\n9Transformer\nhttps://spaces.ac.cn/archives/8620<br>\n10https://theaisummer.com/self-attention/\nhttps://theaisummer.com/self-attention/<br>\n11https://zhuanlan.zhihu.com/p/626820422\nhttps://zhuanlan.zhihu.com/p/626820422<br>\n12Are Sixteen Heads Really Better than One?\nhttps://arxiv.org/pdf/1905.10650.pdf<br>\n13This post is all you needTransformer\nhttps://zhuanlan.zhihu.com/p/420820453<br>\n14The Illustrated Transformer\nhttps://jalammar.github.io/illustrated-transformer/<br>\n15Multi-Query Attention is All You Need\nhttps://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055</p>\n"}],"PostAsset":[{"_id":"source/_posts/cs/nlp/2024/05/-DPO/dpo_loss_code.png","post":"clwwvjmfa0001am4k00qx9rl7","slug":"dpo_loss_code.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/gradient.png","post":"clwwvjmfa0001am4k00qx9rl7","slug":"gradient.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/intro.png","post":"clwwvjmfa0001am4k00qx9rl7","slug":"intro.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/result_1.png","post":"clwwvjmfa0001am4k00qx9rl7","slug":"result_1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/result_2.png","post":"clwwvjmfa0001am4k00qx9rl7","slug":"result_2.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/result_3.png","post":"clwwvjmfa0001am4k00qx9rl7","slug":"result_3.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/result_4.png","post":"clwwvjmfa0001am4k00qx9rl7","slug":"result_4.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/acce_alog.png","post":"clwwvjmfe0008am4k2kd9fnjj","slug":"acce_alog.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/acce_draft_model_param.png","post":"clwwvjmfe0008am4k2kd9fnjj","slug":"acce_draft_model_param.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/acce_k.png","post":"clwwvjmfe0008am4k2kd9fnjj","slug":"acce_k.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_alpha.png","post":"clwwvjmfe0008am4k2kd9fnjj","slug":"fi_alpha.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_choose_gamma.png","post":"clwwvjmfe0008am4k2kd9fnjj","slug":"fi_choose_gamma.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_example.png","post":"clwwvjmfe0008am4k2kd9fnjj","slug":"fi_example.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_expected_token_num.png","post":"clwwvjmfe0008am4k2kd9fnjj","slug":"fi_expected_token_num.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_sd_algo.png","post":"clwwvjmfe0008am4k2kd9fnjj","slug":"fi_sd_algo.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_speed_and_op.png","post":"clwwvjmfe0008am4k2kd9fnjj","slug":"fi_speed_and_op.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_speed_and_op_table.png","post":"clwwvjmfe0008am4k2kd9fnjj","slug":"fi_speed_and_op_table.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_t5_result.png","post":"clwwvjmfe0008am4k2kd9fnjj","slug":"fi_t5_result.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_walltime.png","post":"clwwvjmfe0008am4k2kd9fnjj","slug":"fi_walltime.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/formula.png","post":"clwwvjmfe0008am4k2kd9fnjj","slug":"formula.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/speculative_decoding.png","post":"clwwvjmfe0008am4k2kd9fnjj","slug":"speculative_decoding.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/alpha.png","post":"clwwvjmfc0003am4kezrjgw6a","slug":"alpha.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/odpo_intro.png","post":"clwwvjmfc0003am4kezrjgw6a","slug":"odpo_intro.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/scaling_function.png","post":"clwwvjmfc0003am4kezrjgw6a","slug":"scaling_function.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/sentiment_control.png","post":"clwwvjmfc0003am4kezrjgw6a","slug":"sentiment_control.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/summarization.png","post":"clwwvjmfc0003am4kezrjgw6a","slug":"summarization.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/toxicity_control.png","post":"clwwvjmfc0003am4kezrjgw6a","slug":"toxicity_control.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-5/bfloat16.jpeg","post":"clwwvjmfe0009am4kbhfkdk91","slug":"bfloat16.jpeg","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-5/ntk_by_parts.png","post":"clwwvjmfe0009am4kbhfkdk91","slug":"ntk_by_parts.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-5/yarn.png","post":"clwwvjmfe0009am4kbhfkdk91","slug":"yarn.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/digimon.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"digimon.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_booksum.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"infini_attention_booksum.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_compare.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"infini_attention_compare.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_gating.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"infini_attention_gating.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_language_modeling.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"infini_attention_language_modeling.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_passkey.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"infini_attention_passkey.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_process.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"infini_attention_process.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_structure.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"infini_attention_structure.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_ablation.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"lm_infinite_ablation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_attention_entropy.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"lm_infinite_attention_entropy.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_attention_logits_explode.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"lm_infinite_attention_logits_explode.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_design.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"lm_infinite_design.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_downstream.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"lm_infinite_downstream.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_middle_k.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"lm_infinite_middle_k.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_ppl_200m.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"lm_infinite_ppl_200m.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_ppl_figure.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"lm_infinite_ppl_figure.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_starting_tokens.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"lm_infinite_starting_tokens.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_starting_tokens_num.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"lm_infinite_starting_tokens_num.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/streamingllm_compare.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"streamingllm_compare.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/streamingllm_model_ppl.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"streamingllm_model_ppl.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/stremingllm_attention_sink.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"stremingllm_attention_sink.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/stremingllm_exp.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"stremingllm_exp.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/stremingllm_init_token_num.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"stremingllm_init_token_num.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/stremingllm_kv_cache.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"stremingllm_kv_cache.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/stremingllm_perf_4m.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"stremingllm_perf_4m.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/xl_attention.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"xl_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/xl_vanilla_sw.png","post":"clwwvjmfg000dam4k8l3m65oc","slug":"xl_vanilla_sw.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi.png","post":"clwwvjmfg000ham4k57oldeuh","slug":"meta_pi.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi_explanation.png","post":"clwwvjmfg000ham4k57oldeuh","slug":"meta_pi_explanation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi_nosft.png","post":"clwwvjmfg000ham4k57oldeuh","slug":"meta_pi_nosft.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_rope_ext.png","post":"clwwvjmfg000ham4k57oldeuh","slug":"meta_rope_ext.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/mix_precision_fp16.png","post":"clwwvjmfg000ham4k57oldeuh","slug":"mix_precision_fp16.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/rope_matrix.png","post":"clwwvjmfg000ham4k57oldeuh","slug":"rope_matrix.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/complex_number.png","post":"clwwvjmfh000kam4k8roy12jb","slug":"complex_number.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/remote_attenuation.png","post":"clwwvjmfh000kam4k8roy12jb","slug":"remote_attenuation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/rope.png","post":"clwwvjmfh000kam4k8roy12jb","slug":"rope.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//add_money.jpg","post":"clwwvjmfg000gam4k1bflb3wt","slug":"add_money.jpg","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//eng_config.png","post":"clwwvjmfg000gam4k1bflb3wt","slug":"eng_config.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//eng_data.png","post":"clwwvjmfg000gam4k1bflb3wt","slug":"eng_data.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//eng_data_dist.png","post":"clwwvjmfg000gam4k1bflb3wt","slug":"eng_data_dist.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//eng_needle_comp.png","post":"clwwvjmfg000gam4k1bflb3wt","slug":"eng_needle_comp.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//eng_ppl.png","post":"clwwvjmfg000gam4k1bflb3wt","slug":"eng_ppl.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//eng_sample.png","post":"clwwvjmfg000gam4k1bflb3wt","slug":"eng_sample.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//eng_tokens.png","post":"clwwvjmfg000gam4k1bflb3wt","slug":"eng_tokens.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_dataset.png","post":"clwwvjmfg000gam4k1bflb3wt","slug":"paraphrasing_dataset.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_dataset_dist.png","post":"clwwvjmfg000gam4k1bflb3wt","slug":"paraphrasing_dataset_dist.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_example.png","post":"clwwvjmfg000gam4k1bflb3wt","slug":"paraphrasing_example.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_intro.png","post":"clwwvjmfg000gam4k1bflb3wt","slug":"paraphrasing_intro.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_lost.png","post":"clwwvjmfg000gam4k1bflb3wt","slug":"paraphrasing_lost.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_perf.png","post":"clwwvjmfg000gam4k1bflb3wt","slug":"paraphrasing_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_quality.png","post":"clwwvjmfg000gam4k1bflb3wt","slug":"paraphrasing_quality.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//pose_method.png","post":"clwwvjmfg000gam4k1bflb3wt","slug":"pose_method.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//pose_passkey.png","post":"clwwvjmfg000gam4k1bflb3wt","slug":"pose_passkey.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//pose_ppl.png","post":"clwwvjmfg000gam4k1bflb3wt","slug":"pose_ppl.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/big_bird_attention.png","post":"clwwvjmfi000ram4k8x4jbo7m","slug":"big_bird_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/dilated_conv.png","post":"clwwvjmfi000ram4k8x4jbo7m","slug":"dilated_conv.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/longformer_attention.png","post":"clwwvjmfi000ram4k8x4jbo7m","slug":"longformer_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_architechture.png","post":"clwwvjmfi000ram4k8x4jbo7m","slug":"mistral_architechture.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_large_performance.jpeg","post":"clwwvjmfi000ram4k8x4jbo7m","slug":"mistral_large_performance.jpeg","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_perf.png","post":"clwwvjmfi000ram4k8x4jbo7m","slug":"mistral_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_swa.png","post":"clwwvjmfi000ram4k8x4jbo7m","slug":"mistral_swa.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/ms_invest_mistral.png","post":"clwwvjmfi000ram4k8x4jbo7m","slug":"ms_invest_mistral.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/prefill_and_chunking.png","post":"clwwvjmfi000ram4k8x4jbo7m","slug":"prefill_and_chunking.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/receptive_field_cnn.png","post":"clwwvjmfi000ram4k8x4jbo7m","slug":"receptive_field_cnn.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/rolling_buffer.png","post":"clwwvjmfi000ram4k8x4jbo7m","slug":"rolling_buffer.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/bn_and_ln.png","post":"clwwvjmfh000lam4k3yolgekg","slug":"bn_and_ln.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/cv_batchnorm.png","post":"clwwvjmfh000lam4k3yolgekg","slug":"cv_batchnorm.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/cv_layernorm.jpeg","post":"clwwvjmfh000lam4k3yolgekg","slug":"cv_layernorm.jpeg","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/norm_in_nlp.png","post":"clwwvjmfh000lam4k3yolgekg","slug":"norm_in_nlp.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/ablation.png","post":"clwwvjmfd0007am4kdqcmbpcw","slug":"ablation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/benchmark.png","post":"clwwvjmfd0007am4kdqcmbpcw","slug":"benchmark.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/contingency_table.png","post":"clwwvjmfd0007am4kdqcmbpcw","slug":"contingency_table.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/dpo_correlation.png","post":"clwwvjmfd0007am4kdqcmbpcw","slug":"dpo_correlation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/gradient.png","post":"clwwvjmfd0007am4kdqcmbpcw","slug":"gradient.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/hyperparameters.png","post":"clwwvjmfd0007am4kdqcmbpcw","slug":"hyperparameters.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/intro.png","post":"clwwvjmfd0007am4kdqcmbpcw","slug":"intro.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/ln.png","post":"clwwvjmfd0007am4kdqcmbpcw","slug":"ln.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/ln_effect.png","post":"clwwvjmfd0007am4kdqcmbpcw","slug":"ln_effect.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/main_results.png","post":"clwwvjmfd0007am4kdqcmbpcw","slug":"main_results.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/margin_dist.png","post":"clwwvjmfd0007am4kdqcmbpcw","slug":"margin_dist.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/reward_accuracy.png","post":"clwwvjmfd0007am4kdqcmbpcw","slug":"reward_accuracy.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/reward_accuracy_compare.png","post":"clwwvjmfd0007am4kdqcmbpcw","slug":"reward_accuracy_compare.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/simpo_contingency.png","post":"clwwvjmfd0007am4kdqcmbpcw","slug":"simpo_contingency.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/simpo_hyperparameters.png","post":"clwwvjmfd0007am4kdqcmbpcw","slug":"simpo_hyperparameters.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/04/-4/transformer.png","post":"clwwvjmfi000pam4kglhffgkz","slug":"transformer.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/9B.png","post":"clwwvjmfi000tam4kcvrsat47","slug":"9B.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/base_model_eval.png","post":"clwwvjmfi000tam4kcvrsat47","slug":"base_model_eval.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/cover.png","post":"clwwvjmfi000tam4kcvrsat47","slug":"cover.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/eval.png","post":"clwwvjmfi000tam4kcvrsat47","slug":"eval.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/ict.png","post":"clwwvjmfi000tam4kcvrsat47","slug":"ict.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/long_context_result.png","post":"clwwvjmfi000tam4kcvrsat47","slug":"long_context_result.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/model.png","post":"clwwvjmfi000tam4kcvrsat47","slug":"model.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/multimodal.png","post":"clwwvjmfi000tam4kcvrsat47","slug":"multimodal.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/perf.png","post":"clwwvjmfi000tam4kcvrsat47","slug":"perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/pretrain_data_dist.png","post":"clwwvjmfi000tam4kcvrsat47","slug":"pretrain_data_dist.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/pretrain_data_pipeline.png","post":"clwwvjmfi000tam4kcvrsat47","slug":"pretrain_data_pipeline.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/sft.png","post":"clwwvjmfi000tam4kcvrsat47","slug":"sft.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/third_party.png","post":"clwwvjmfi000tam4kcvrsat47","slug":"third_party.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/1.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/10.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"10.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/11.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"11.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/12.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"12.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/13.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"13.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/14.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"14.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/15.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"15.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/16.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"16.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/17.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"17.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/18.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"18.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/19.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"19.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/2.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"2.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/20.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"20.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/21.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"21.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/22.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"22.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/23.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"23.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/24.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"24.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/25.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"25.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/26.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"26.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/27.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"27.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/28.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"28.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/3.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"3.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/4.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"4.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/5.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"5.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/6.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"6.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/7.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"7.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/8.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"8.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/9.png","post":"clwwvjmfj000zam4k6s3665ed","slug":"9.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_algo.png","post":"clwwvjmfj000vam4k7ll2hgro","slug":"bn_algo.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_and_ln.png","post":"clwwvjmfj000vam4k7ll2hgro","slug":"bn_and_ln.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_ics.png","post":"clwwvjmfj000vam4k7ll2hgro","slug":"bn_ics.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_ln_gn_in.png","post":"clwwvjmfj000vam4k7ll2hgro","slug":"bn_ln_gn_in.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bs_bn.png","post":"clwwvjmfj000vam4k7ll2hgro","slug":"bs_bn.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/deepnorm.png","post":"clwwvjmfj000vam4k7ll2hgro","slug":"deepnorm.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/deepnorm_result.png","post":"clwwvjmfj000vam4k7ll2hgro","slug":"deepnorm_result.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ellipse_1.png","post":"clwwvjmfj000vam4k7ll2hgro","slug":"ellipse_1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ellipse_2.png","post":"clwwvjmfj000vam4k7ll2hgro","slug":"ellipse_2.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ics_define.png","post":"clwwvjmfj000vam4k7ll2hgro","slug":"ics_define.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ics_measure.png","post":"clwwvjmfj000vam4k7ll2hgro","slug":"ics_measure.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/lossfunc_surface.jpeg","post":"clwwvjmfj000vam4k7ll2hgro","slug":"lossfunc_surface.jpeg","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/postnorm_prenorm.png","post":"clwwvjmfj000vam4k7ll2hgro","slug":"postnorm_prenorm.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/prmsnorm.png","post":"clwwvjmfj000vam4k7ll2hgro","slug":"prmsnorm.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/realformer.png","post":"clwwvjmfj000vam4k7ll2hgro","slug":"realformer.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/realformer_attention.png","post":"clwwvjmfj000vam4k7ll2hgro","slug":"realformer_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/rmsnorm.png","post":"clwwvjmfj000vam4k7ll2hgro","slug":"rmsnorm.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/rmsnorm_eff.png","post":"clwwvjmfj000vam4k7ll2hgro","slug":"rmsnorm_eff.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/sigmoid.png","post":"clwwvjmfj000vam4k7ll2hgro","slug":"sigmoid.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/warmup_effect.png","post":"clwwvjmfj000vam4k7ll2hgro","slug":"warmup_effect.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/cover.jpeg","post":"clwwvjmfs007pam4kgy84cg3k","slug":"cover.jpeg","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_infer_efficiency.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"dbrx_infer_efficiency.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_long_perf_1.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"dbrx_long_perf_1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_long_perf_2.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"dbrx_long_perf_2.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_perf.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"dbrx_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_train_efficiency.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"dbrx_train_efficiency.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_vs_closed_models.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"dbrx_vs_closed_models.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_vs_open_models.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"dbrx_vs_open_models.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_16b_perf_1.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"ds_16b_perf_1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_16b_perf_2.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"ds_16b_perf_2.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_2b_less_expert.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"ds_2b_less_expert.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_model_param.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"ds_model_param.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_145b.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"ds_moe_145b.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_ablation.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"ds_moe_ablation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_comparison.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"ds_moe_comparison.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_expert_specialization.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"ds_moe_expert_specialization.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_less_activated_expert.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"ds_moe_less_activated_expert.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_perf.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"ds_moe_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_sft.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"ds_moe_sft.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_structure.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"ds_moe_structure.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_upper_bound_13b.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"ds_moe_upper_bound_13b.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_upper_bound_2b.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"ds_moe_upper_bound_2b.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_compare_gpt3.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"glam_compare_gpt3.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_compare_gpt3_2.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"glam_compare_gpt3_2.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_family.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"glam_family.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_model.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"glam_model.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_perf.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"glam_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_related_model.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"glam_related_model.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_algo_1.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"gshard_algo_1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_model.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"gshard_model.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_moe_family.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"gshard_moe_family.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_perf.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"gshard_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_result.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"gshard_result.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/mistral_8_22b_code.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"mistral_8_22b_code.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/mistral_8_22b_multiling.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"mistral_8_22b_multiling.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/mistral_8_22b_reasoning.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"mistral_8_22b_reasoning.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/mistral_8_7b_active_perf.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"mistral_8_7b_active_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/mistral_8_7b_perf.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"mistral_8_7b_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/modular_connectionist.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"modular_connectionist.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/qwen1.5_moe_params.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"qwen1.5_moe_params.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/qwen1.5_moe_perf.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"qwen1.5_moe_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/qwen1.5_moe_tps.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"qwen1.5_moe_tps.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"rnn_moe.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_137b.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"rnn_moe_137b.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_hierarchical_gating.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"rnn_moe_hierarchical_gating.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_load_function.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"rnn_moe_load_function.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_perf.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"rnn_moe_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_specilized.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"rnn_moe_specilized.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/softplus.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"softplus.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_capacity_factor.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"st_moe_capacity_factor.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_capacity_factor_speed.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"st_moe_capacity_factor_speed.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_encoder_specialization.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"st_moe_encoder_specialization.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_models.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"st_moe_models.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_more_add_bias.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"st_moe_more_add_bias.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_more_add_noise.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"st_moe_more_add_noise.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_more_dense_layer.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"st_moe_more_dense_layer.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_multiling_specialization.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"st_moe_multiling_specialization.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_perf.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"st_moe_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_remove_multiplications.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"st_moe_remove_multiplications.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_round_error.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"st_moe_round_error.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_z_loss_result.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"st_moe_z_loss_result.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_capacity_effect.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"switch_transformer_capacity_effect.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_diff_expert_capacity.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"switch_transformer_diff_expert_capacity.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_distill.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"switch_transformer_distill.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_distill_diff_model.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"switch_transformer_distill_diff_model.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_distill_sft.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"switch_transformer_distill_sft.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_dropout.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"switch_transformer_dropout.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_init.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"switch_transformer_init.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_pretrain_result.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"switch_transformer_pretrain_result.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_scaling_dense.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"switch_transformer_scaling_dense.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_scaling_step.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"switch_transformer_scaling_step.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_scaling_time.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"switch_transformer_scaling_time.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_sft_result.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"switch_transformer_sft_result.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_structure.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"switch_transformer_structure.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/vanilla_moe.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"vanilla_moe.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/vanilla_moe_result.png","post":"clwwvjmfs007pam4kgy84cg3k","slug":"vanilla_moe_result.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/xiaomi_moe.jpg","post":"clwwvjmfs007pam4kgy84cg3k","slug":"xiaomi_moe.jpg","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/GQA.png","post":"clwwvjmfs007qam4k2deo4dy2","slug":"GQA.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/GQA_result_1.png","post":"clwwvjmfs007qam4k2deo4dy2","slug":"GQA_result_1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/MQA.png","post":"clwwvjmfs007qam4k2deo4dy2","slug":"MQA.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/MQA.webp","post":"clwwvjmfs007qam4k2deo4dy2","slug":"MQA.webp","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Markdown _  Nice.html","post":"clwwvjmfs007qam4k2deo4dy2","slug":"Markdown _  Nice.html","modified":1,"renderable":1},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Scaled-dot-product-self-attention.pbm","post":"clwwvjmfs007qam4k2deo4dy2","slug":"Scaled-dot-product-self-attention.pbm","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Scaled-dot-product-self-attention.png","post":"clwwvjmfs007qam4k2deo4dy2","slug":"Scaled-dot-product-self-attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/attention_calculation.png","post":"clwwvjmfs007qam4k2deo4dy2","slug":"attention_calculation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/cnn_heatmap.png","post":"clwwvjmfs007qam4k2deo4dy2","slug":"cnn_heatmap.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/decoder.png","post":"clwwvjmfs007qam4k2deo4dy2","slug":"decoder.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/encoder.png","post":"clwwvjmfs007qam4k2deo4dy2","slug":"encoder.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/gpu_cache.png","post":"clwwvjmfs007qam4k2deo4dy2","slug":"gpu_cache.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/lihongyi_self_attention.png","post":"clwwvjmfs007qam4k2deo4dy2","slug":"lihongyi_self_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/llama2_qga.png","post":"clwwvjmfs007qam4k2deo4dy2","slug":"llama2_qga.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/mqa_result_1.png","post":"clwwvjmfs007qam4k2deo4dy2","slug":"mqa_result_1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/mqa_result_3.png","post":"clwwvjmfs007qam4k2deo4dy2","slug":"mqa_result_3.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/multihead_attention.png","post":"clwwvjmfs007qam4k2deo4dy2","slug":"multihead_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/seq2seq.png","post":"clwwvjmfs007qam4k2deo4dy2","slug":"seq2seq.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/seq2seq_attention.png","post":"clwwvjmfs007qam4k2deo4dy2","slug":"seq2seq_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/softmax.png","post":"clwwvjmfs007qam4k2deo4dy2","slug":"softmax.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/sram_dram.png","post":"clwwvjmfs007qam4k2deo4dy2","slug":"sram_dram.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/transformer_structure.png","post":"clwwvjmfs007qam4k2deo4dy2","slug":"transformer_structure.png","modified":1,"renderable":0}],"PostCategory":[{"post_id":"clwwvjmfh000kam4k8roy12jb","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfl001mam4k12iq8w45"},{"post_id":"clwwvjmfh000kam4k8roy12jb","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfl001nam4kcp4722qx"},{"post_id":"clwwvjmfh000kam4k8roy12jb","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfl001ram4k3b0u4ho3"},{"post_id":"clwwvjmfe0008am4k2kd9fnjj","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfl001tam4kazn11e6t"},{"post_id":"clwwvjmfe0008am4k2kd9fnjj","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfl001xam4k18atha86"},{"post_id":"clwwvjmfe0008am4k2kd9fnjj","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfl001zam4kbjjjawl2"},{"post_id":"clwwvjmfh000lam4k3yolgekg","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfl0023am4k1g511y9g"},{"post_id":"clwwvjmfh000lam4k3yolgekg","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfl0025am4k6x4l8oxc"},{"post_id":"clwwvjmfh000lam4k3yolgekg","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfl0029am4kanlx0xjq"},{"post_id":"clwwvjmfi000oam4kdgkle53r","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfl002aam4k0lej9la5"},{"post_id":"clwwvjmfi000oam4kdgkle53r","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfl002dam4kae9j54qp"},{"post_id":"clwwvjmfi000oam4kdgkle53r","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfl002eam4k87sr3w4o"},{"post_id":"clwwvjmfa0001am4k00qx9rl7","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfm002ham4k1nvq39tw"},{"post_id":"clwwvjmfa0001am4k00qx9rl7","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfm002iam4kbuzv3zun"},{"post_id":"clwwvjmfa0001am4k00qx9rl7","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfm002lam4kh107f5v0"},{"post_id":"clwwvjmfi000pam4kglhffgkz","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfm002mam4k738m6r9b"},{"post_id":"clwwvjmfi000pam4kglhffgkz","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfm002qam4k2map4kck"},{"post_id":"clwwvjmfi000pam4kglhffgkz","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfm002sam4kd7kk3ch5"},{"post_id":"clwwvjmfi000ram4k8x4jbo7m","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfm002wam4k4gjwdq4e"},{"post_id":"clwwvjmfi000ram4k8x4jbo7m","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfm002yam4k9a083eaf"},{"post_id":"clwwvjmfi000ram4k8x4jbo7m","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfm0032am4kfpn2bp2y"},{"post_id":"clwwvjmfe0009am4kbhfkdk91","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfm0034am4k1dqgcalm"},{"post_id":"clwwvjmfe0009am4kbhfkdk91","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfm0038am4k20od2996"},{"post_id":"clwwvjmfe0009am4kbhfkdk91","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfm003aam4kf56la0kf"},{"post_id":"clwwvjmfi000tam4kcvrsat47","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfn003eam4k9kgp0jc6"},{"post_id":"clwwvjmfi000tam4kcvrsat47","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfn003gam4k7zvp9z40"},{"post_id":"clwwvjmfi000tam4kcvrsat47","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfn003kam4k8nkt7nlv"},{"post_id":"clwwvjmfj000vam4k7ll2hgro","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfn003mam4k2gvifn8f"},{"post_id":"clwwvjmfj000vam4k7ll2hgro","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfn003qam4kgag8aoko"},{"post_id":"clwwvjmfj000vam4k7ll2hgro","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfn003sam4kcbh1g1ga"},{"post_id":"clwwvjmff000cam4k0xy63bue","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfn003wam4khzugdp10"},{"post_id":"clwwvjmff000cam4k0xy63bue","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfn003yam4kc6in2vku"},{"post_id":"clwwvjmff000cam4k0xy63bue","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfn0042am4k8se35jvg"},{"post_id":"clwwvjmfj000xam4kg9aaagpd","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfn0043am4k1k3tfrad"},{"post_id":"clwwvjmfj000xam4kg9aaagpd","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfn0044am4k9vvzhmj5"},{"post_id":"clwwvjmfj000xam4kg9aaagpd","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfn0046am4k8iy2hfnn"},{"post_id":"clwwvjmfj000zam4k6s3665ed","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfn0047am4k8nq045qh"},{"post_id":"clwwvjmfj000zam4k6s3665ed","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfn0049am4k0c4yempf"},{"post_id":"clwwvjmfj000zam4k6s3665ed","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfn004aam4kfynm08qg"},{"post_id":"clwwvjmfc0003am4kezrjgw6a","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfo004cam4kdztgc3fk"},{"post_id":"clwwvjmfc0003am4kezrjgw6a","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfo004dam4k34mp62lx"},{"post_id":"clwwvjmfc0003am4kezrjgw6a","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfo004fam4ke0az86y8"},{"post_id":"clwwvjmfj0011am4kfc91agmw","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfo004ham4khutm2hjh"},{"post_id":"clwwvjmfj0011am4kfc91agmw","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfo004kam4k1a4i12jo"},{"post_id":"clwwvjmfj0011am4kfc91agmw","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfo004mam4k7k8t1pyb"},{"post_id":"clwwvjmfg000dam4k8l3m65oc","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfo004pam4k3pvs0g0k"},{"post_id":"clwwvjmfg000dam4k8l3m65oc","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfo004ram4kczi4hkj2"},{"post_id":"clwwvjmfg000dam4k8l3m65oc","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfo004uam4kez4y1ooh"},{"post_id":"clwwvjmfg000gam4k1bflb3wt","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfo004wam4k3492f19y"},{"post_id":"clwwvjmfg000gam4k1bflb3wt","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfo004zam4k9b7bhpu0"},{"post_id":"clwwvjmfg000gam4k1bflb3wt","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfo0051am4kb8vah5nr"},{"post_id":"clwwvjmfd0007am4kdqcmbpcw","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfo0054am4kf3d5bw1d"},{"post_id":"clwwvjmfd0007am4kdqcmbpcw","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfo0056am4kdiw02k5x"},{"post_id":"clwwvjmfd0007am4kdqcmbpcw","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfo0059am4k5wo0g8cc"},{"post_id":"clwwvjmfg000ham4k57oldeuh","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfo005bam4kbsiq2x6x"},{"post_id":"clwwvjmfg000ham4k57oldeuh","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfo005dam4k7eh67ggx"},{"post_id":"clwwvjmfg000ham4k57oldeuh","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfo005gam4k6l5849q3"},{"post_id":"clwwvjmfs007pam4kgy84cg3k","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfs007sam4kbudoavro"},{"post_id":"clwwvjmfs007pam4kgy84cg3k","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfs007uam4kfyfscija"},{"post_id":"clwwvjmfs007pam4kgy84cg3k","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfs007wam4k3minan5y"},{"post_id":"clwwvjmfs007qam4k2deo4dy2","category_id":"clwwvjmfc0004am4kbjnn53l9","_id":"clwwvjmfs007yam4kg88qbog6"},{"post_id":"clwwvjmfs007qam4k2deo4dy2","category_id":"clwwvjmfh000iam4k1o5i46zb","_id":"clwwvjmfs0080am4k51x4bx7m"},{"post_id":"clwwvjmfs007qam4k2deo4dy2","category_id":"clwwvjmfk001ham4k1r6t008k","_id":"clwwvjmfs0082am4k5zah5lg7"}],"PostTag":[{"post_id":"clwwvjmfa0001am4k00qx9rl7","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfk0013am4k61zpewat"},{"post_id":"clwwvjmfa0001am4k00qx9rl7","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfk0014am4k2swif2s6"},{"post_id":"clwwvjmfa0001am4k00qx9rl7","tag_id":"clwwvjmfg000fam4k440i97fj","_id":"clwwvjmfk0017am4kepafgq3w"},{"post_id":"clwwvjmfa0001am4k00qx9rl7","tag_id":"clwwvjmfh000jam4k959r5w2f","_id":"clwwvjmfk0018am4ked8e7son"},{"post_id":"clwwvjmfa0001am4k00qx9rl7","tag_id":"clwwvjmfi000nam4k7d0qfdzh","_id":"clwwvjmfk001bam4k4zi822va"},{"post_id":"clwwvjmfa0001am4k00qx9rl7","tag_id":"clwwvjmfi000sam4k8ygihaz7","_id":"clwwvjmfk001cam4kdahp8hyb"},{"post_id":"clwwvjmfa0001am4k00qx9rl7","tag_id":"clwwvjmfj000wam4k3t2m3vr2","_id":"clwwvjmfk001fam4k47rj37c1"},{"post_id":"clwwvjmfc0003am4kezrjgw6a","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfl001pam4kh6grev9k"},{"post_id":"clwwvjmfc0003am4kezrjgw6a","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfl001sam4kbim26tep"},{"post_id":"clwwvjmfc0003am4kezrjgw6a","tag_id":"clwwvjmfg000fam4k440i97fj","_id":"clwwvjmfl001vam4kfi8j2eb4"},{"post_id":"clwwvjmfc0003am4kezrjgw6a","tag_id":"clwwvjmfh000jam4k959r5w2f","_id":"clwwvjmfl001yam4k7wjh1x29"},{"post_id":"clwwvjmfc0003am4kezrjgw6a","tag_id":"clwwvjmfi000nam4k7d0qfdzh","_id":"clwwvjmfl0021am4k52j53091"},{"post_id":"clwwvjmfc0003am4kezrjgw6a","tag_id":"clwwvjmfi000sam4k8ygihaz7","_id":"clwwvjmfl0024am4kgetj4gl3"},{"post_id":"clwwvjmfc0003am4kezrjgw6a","tag_id":"clwwvjmfj000wam4k3t2m3vr2","_id":"clwwvjmfl0027am4kfwc231lb"},{"post_id":"clwwvjmfd0007am4kdqcmbpcw","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfm002oam4kb3oi36na"},{"post_id":"clwwvjmfd0007am4kdqcmbpcw","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfm002ram4keuiq63uu"},{"post_id":"clwwvjmfd0007am4kdqcmbpcw","tag_id":"clwwvjmfg000fam4k440i97fj","_id":"clwwvjmfm002uam4k6epz61sm"},{"post_id":"clwwvjmfd0007am4kdqcmbpcw","tag_id":"clwwvjmfh000jam4k959r5w2f","_id":"clwwvjmfm002xam4kg8cg3mfy"},{"post_id":"clwwvjmfd0007am4kdqcmbpcw","tag_id":"clwwvjmfi000nam4k7d0qfdzh","_id":"clwwvjmfm0030am4k167j3ptu"},{"post_id":"clwwvjmfd0007am4kdqcmbpcw","tag_id":"clwwvjmfi000sam4k8ygihaz7","_id":"clwwvjmfm0033am4kent01vzo"},{"post_id":"clwwvjmfd0007am4kdqcmbpcw","tag_id":"clwwvjmfj000wam4k3t2m3vr2","_id":"clwwvjmfm0036am4kfm72f50t"},{"post_id":"clwwvjmfe0008am4k2kd9fnjj","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfm0039am4k1w4e0zpg"},{"post_id":"clwwvjmfe0008am4k2kd9fnjj","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfm003cam4k2c2159tj"},{"post_id":"clwwvjmfe0008am4k2kd9fnjj","tag_id":"clwwvjmfg000fam4k440i97fj","_id":"clwwvjmfn003fam4kgh9jf483"},{"post_id":"clwwvjmfe0008am4k2kd9fnjj","tag_id":"clwwvjmfm002zam4k88bqhw2j","_id":"clwwvjmfn003iam4k2iakbmqs"},{"post_id":"clwwvjmfe0009am4kbhfkdk91","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfn003lam4k6wb38cau"},{"post_id":"clwwvjmfe0009am4kbhfkdk91","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfn003oam4k8alj6jgc"},{"post_id":"clwwvjmfe0009am4kbhfkdk91","tag_id":"clwwvjmfm003bam4k1c9b4l06","_id":"clwwvjmfn003ram4k5fms6req"},{"post_id":"clwwvjmff000cam4k0xy63bue","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfn003uam4k65s4g316"},{"post_id":"clwwvjmff000cam4k0xy63bue","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfn003xam4k2oome025"},{"post_id":"clwwvjmff000cam4k0xy63bue","tag_id":"clwwvjmfm003bam4k1c9b4l06","_id":"clwwvjmfn0040am4k7jxm6p9u"},{"post_id":"clwwvjmfg000dam4k8l3m65oc","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfo004gam4kdsxgarta"},{"post_id":"clwwvjmfg000dam4k8l3m65oc","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfo004iam4kceh0f7ej"},{"post_id":"clwwvjmfg000dam4k8l3m65oc","tag_id":"clwwvjmfg000fam4k440i97fj","_id":"clwwvjmfo004lam4k2mgi8r94"},{"post_id":"clwwvjmfg000dam4k8l3m65oc","tag_id":"clwwvjmfn003tam4khqtvcem7","_id":"clwwvjmfo004nam4kgng126ch"},{"post_id":"clwwvjmfg000dam4k8l3m65oc","tag_id":"clwwvjmfn003zam4k0s5s7ap7","_id":"clwwvjmfo004qam4k460t6seu"},{"post_id":"clwwvjmfg000dam4k8l3m65oc","tag_id":"clwwvjmfn0045am4k7k5r7fyz","_id":"clwwvjmfo004sam4kdwolh8pb"},{"post_id":"clwwvjmfg000dam4k8l3m65oc","tag_id":"clwwvjmfi000nam4k7d0qfdzh","_id":"clwwvjmfo004vam4k7v7c7kst"},{"post_id":"clwwvjmfg000dam4k8l3m65oc","tag_id":"clwwvjmfn004bam4k2fz20y4k","_id":"clwwvjmfo004xam4k279r8lry"},{"post_id":"clwwvjmfg000gam4k1bflb3wt","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfo0050am4k29tg5jbd"},{"post_id":"clwwvjmfg000gam4k1bflb3wt","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfo0052am4kemvn2rze"},{"post_id":"clwwvjmfg000gam4k1bflb3wt","tag_id":"clwwvjmfg000fam4k440i97fj","_id":"clwwvjmfo0055am4k45eq1y5u"},{"post_id":"clwwvjmfg000gam4k1bflb3wt","tag_id":"clwwvjmfn003zam4k0s5s7ap7","_id":"clwwvjmfo0057am4kbconbdc8"},{"post_id":"clwwvjmfg000gam4k1bflb3wt","tag_id":"clwwvjmfn0045am4k7k5r7fyz","_id":"clwwvjmfo005aam4k1qen0vpm"},{"post_id":"clwwvjmfg000gam4k1bflb3wt","tag_id":"clwwvjmfi000nam4k7d0qfdzh","_id":"clwwvjmfo005cam4k7ve42grm"},{"post_id":"clwwvjmfg000gam4k1bflb3wt","tag_id":"clwwvjmfn004bam4k2fz20y4k","_id":"clwwvjmfo005fam4kamfw96u5"},{"post_id":"clwwvjmfg000ham4k57oldeuh","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfo005ham4ke0vmahv4"},{"post_id":"clwwvjmfg000ham4k57oldeuh","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfp005jam4kbk4m5n7v"},{"post_id":"clwwvjmfg000ham4k57oldeuh","tag_id":"clwwvjmfg000fam4k440i97fj","_id":"clwwvjmfp005kam4k57arghrq"},{"post_id":"clwwvjmfg000ham4k57oldeuh","tag_id":"clwwvjmfn003zam4k0s5s7ap7","_id":"clwwvjmfp005lam4ka0pgc6v6"},{"post_id":"clwwvjmfg000ham4k57oldeuh","tag_id":"clwwvjmfo0053am4kccwi6g9x","_id":"clwwvjmfp005nam4kec18drut"},{"post_id":"clwwvjmfh000kam4k8roy12jb","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfp005oam4kgli6fvw7"},{"post_id":"clwwvjmfh000kam4k8roy12jb","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfp005qam4kdq4dgi0a"},{"post_id":"clwwvjmfh000kam4k8roy12jb","tag_id":"clwwvjmfg000fam4k440i97fj","_id":"clwwvjmfp005ram4k6ico9hk9"},{"post_id":"clwwvjmfh000kam4k8roy12jb","tag_id":"clwwvjmfo0058am4k9lsh864g","_id":"clwwvjmfp005tam4kcwrd30cb"},{"post_id":"clwwvjmfh000kam4k8roy12jb","tag_id":"clwwvjmfo005eam4k3n7x08zg","_id":"clwwvjmfp005uam4k5bpv0faj"},{"post_id":"clwwvjmfh000lam4k3yolgekg","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfp005wam4k1muzcypr"},{"post_id":"clwwvjmfh000lam4k3yolgekg","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfp005xam4kfoc2c5h4"},{"post_id":"clwwvjmfh000lam4k3yolgekg","tag_id":"clwwvjmfg000fam4k440i97fj","_id":"clwwvjmfp005zam4khrf02x5v"},{"post_id":"clwwvjmfh000lam4k3yolgekg","tag_id":"clwwvjmfo005iam4kghgahceq","_id":"clwwvjmfp0060am4k6l0t9p3k"},{"post_id":"clwwvjmfh000lam4k3yolgekg","tag_id":"clwwvjmfp005mam4k3te9en2z","_id":"clwwvjmfp0062am4kf9oz3slm"},{"post_id":"clwwvjmfh000lam4k3yolgekg","tag_id":"clwwvjmfp005pam4k5gc4hsrc","_id":"clwwvjmfp0063am4kh2my47kv"},{"post_id":"clwwvjmfi000oam4kdgkle53r","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfp0065am4k96iyb0vv"},{"post_id":"clwwvjmfi000oam4kdgkle53r","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfp0066am4kb44z3vhu"},{"post_id":"clwwvjmfi000oam4kdgkle53r","tag_id":"clwwvjmfm003bam4k1c9b4l06","_id":"clwwvjmfp0067am4k3qb8hj3v"},{"post_id":"clwwvjmfi000pam4kglhffgkz","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfp0069am4k2im76o3k"},{"post_id":"clwwvjmfi000pam4kglhffgkz","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfp006aam4k54c8a0a4"},{"post_id":"clwwvjmfi000pam4kglhffgkz","tag_id":"clwwvjmfm003bam4k1c9b4l06","_id":"clwwvjmfq006cam4kf5r73ycq"},{"post_id":"clwwvjmfi000ram4k8x4jbo7m","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfq006dam4k2m5w9efm"},{"post_id":"clwwvjmfi000ram4k8x4jbo7m","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfq006fam4kht9ehdkq"},{"post_id":"clwwvjmfi000ram4k8x4jbo7m","tag_id":"clwwvjmfg000fam4k440i97fj","_id":"clwwvjmfq006gam4ka059cxs3"},{"post_id":"clwwvjmfi000ram4k8x4jbo7m","tag_id":"clwwvjmfn004bam4k2fz20y4k","_id":"clwwvjmfq006iam4kcnpp0hhm"},{"post_id":"clwwvjmfi000ram4k8x4jbo7m","tag_id":"clwwvjmfp0061am4k2rn5h3t8","_id":"clwwvjmfq006jam4k5x6ba973"},{"post_id":"clwwvjmfi000ram4k8x4jbo7m","tag_id":"clwwvjmfp0064am4k085xbz6x","_id":"clwwvjmfq006lam4kgqc2agdo"},{"post_id":"clwwvjmfi000tam4kcvrsat47","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfq006mam4kdhvket3e"},{"post_id":"clwwvjmfi000tam4kcvrsat47","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfq006oam4k3umf9f08"},{"post_id":"clwwvjmfi000tam4kcvrsat47","tag_id":"clwwvjmfg000fam4k440i97fj","_id":"clwwvjmfq006pam4k8mpxdt8s"},{"post_id":"clwwvjmfi000tam4kcvrsat47","tag_id":"clwwvjmfp0068am4k28bm3vxe","_id":"clwwvjmfq006qam4kfxo68rf9"},{"post_id":"clwwvjmfi000tam4kcvrsat47","tag_id":"clwwvjmfp006bam4kha3801b9","_id":"clwwvjmfq006sam4kdjqiasoy"},{"post_id":"clwwvjmfi000tam4kcvrsat47","tag_id":"clwwvjmfn003zam4k0s5s7ap7","_id":"clwwvjmfq006tam4k8xb7g1c1"},{"post_id":"clwwvjmfj000vam4k7ll2hgro","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfq006wam4k8xkw6rr9"},{"post_id":"clwwvjmfj000vam4k7ll2hgro","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfq006xam4k3agjchoc"},{"post_id":"clwwvjmfj000vam4k7ll2hgro","tag_id":"clwwvjmfg000fam4k440i97fj","_id":"clwwvjmfq006zam4kbpq56m2p"},{"post_id":"clwwvjmfj000vam4k7ll2hgro","tag_id":"clwwvjmfo005iam4kghgahceq","_id":"clwwvjmfq0070am4k75soaxsb"},{"post_id":"clwwvjmfj000vam4k7ll2hgro","tag_id":"clwwvjmfq006kam4kdole13l3","_id":"clwwvjmfq0072am4k8gykci28"},{"post_id":"clwwvjmfj000vam4k7ll2hgro","tag_id":"clwwvjmfq006nam4k63mh9wxp","_id":"clwwvjmfq0073am4k848aa3oy"},{"post_id":"clwwvjmfj000vam4k7ll2hgro","tag_id":"clwwvjmfp005mam4k3te9en2z","_id":"clwwvjmfq0075am4k19kd0es8"},{"post_id":"clwwvjmfj000vam4k7ll2hgro","tag_id":"clwwvjmfp005pam4k5gc4hsrc","_id":"clwwvjmfq0076am4k42w8dmk3"},{"post_id":"clwwvjmfj000xam4kg9aaagpd","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfq0078am4k9hu0adze"},{"post_id":"clwwvjmfj000xam4kg9aaagpd","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfq0079am4kb9eph22x"},{"post_id":"clwwvjmfj000xam4kg9aaagpd","tag_id":"clwwvjmfm003bam4k1c9b4l06","_id":"clwwvjmfq007aam4kenf23v64"},{"post_id":"clwwvjmfj000zam4k6s3665ed","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfr007eam4k2cvs7gyj"},{"post_id":"clwwvjmfj000zam4k6s3665ed","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfr007fam4kaabk9fxj"},{"post_id":"clwwvjmfj000zam4k6s3665ed","tag_id":"clwwvjmfq006yam4k8bsq4r4p","_id":"clwwvjmfr007gam4ke49whgm6"},{"post_id":"clwwvjmfj000zam4k6s3665ed","tag_id":"clwwvjmfq0071am4k60jqek6e","_id":"clwwvjmfr007ham4k3sr710zq"},{"post_id":"clwwvjmfj000zam4k6s3665ed","tag_id":"clwwvjmfq0074am4kf1rtafsi","_id":"clwwvjmfr007iam4kbmyf9rkq"},{"post_id":"clwwvjmfj000zam4k6s3665ed","tag_id":"clwwvjmfq0077am4k2ibta3ck","_id":"clwwvjmfr007jam4k0uj03g21"},{"post_id":"clwwvjmfj000zam4k6s3665ed","tag_id":"clwwvjmfq007bam4k89qm1izm","_id":"clwwvjmfr007kam4k0i6jew8s"},{"post_id":"clwwvjmfj000zam4k6s3665ed","tag_id":"clwwvjmfr007cam4keckc4688","_id":"clwwvjmfr007lam4k17f7h963"},{"post_id":"clwwvjmfj0011am4kfc91agmw","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfr007mam4k9x5rbehg"},{"post_id":"clwwvjmfj0011am4kfc91agmw","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfr007nam4k160adxe8"},{"post_id":"clwwvjmfj0011am4kfc91agmw","tag_id":"clwwvjmfm003bam4k1c9b4l06","_id":"clwwvjmfr007oam4kccfn43pb"},{"post_id":"clwwvjmfs007pam4kgy84cg3k","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfs007vam4k044b9ycy"},{"post_id":"clwwvjmfs007pam4kgy84cg3k","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfs007xam4kesvehu5d"},{"post_id":"clwwvjmfs007pam4kgy84cg3k","tag_id":"clwwvjmfg000fam4k440i97fj","_id":"clwwvjmfs007zam4k7w2i2cy3"},{"post_id":"clwwvjmfs007pam4kgy84cg3k","tag_id":"clwwvjmfs007ram4k4hxx9hr8","_id":"clwwvjmfs0081am4keofd50im"},{"post_id":"clwwvjmfs007qam4k2deo4dy2","tag_id":"clwwvjmfd0005am4k243tgp4z","_id":"clwwvjmfs0083am4kb8pu6ew3"},{"post_id":"clwwvjmfs007qam4k2deo4dy2","tag_id":"clwwvjmfe000bam4k7l2z6g3x","_id":"clwwvjmfs0084am4k1pde4v2s"},{"post_id":"clwwvjmfs007qam4k2deo4dy2","tag_id":"clwwvjmfg000fam4k440i97fj","_id":"clwwvjmfs0085am4kbwovfpf0"},{"post_id":"clwwvjmfs007qam4k2deo4dy2","tag_id":"clwwvjmfn004bam4k2fz20y4k","_id":"clwwvjmfs0086am4khgn7h64w"},{"post_id":"clwwvjmfs007qam4k2deo4dy2","tag_id":"clwwvjmfs007tam4kevv53ch3","_id":"clwwvjmfs0087am4k6axt6vx3"}],"Tag":[{"name":"NLP","_id":"clwwvjmfd0005am4k243tgp4z"},{"name":"LLM","_id":"clwwvjmfe000bam4k7l2z6g3x"},{"name":"transformer","_id":"clwwvjmfg000fam4k440i97fj"},{"name":"","_id":"clwwvjmfh000jam4k959r5w2f"},{"name":"","_id":"clwwvjmfi000nam4k7d0qfdzh"},{"name":"SFT","_id":"clwwvjmfi000sam4k8ygihaz7"},{"name":"","_id":"clwwvjmfj000wam4k3t2m3vr2"},{"name":"","_id":"clwwvjmfm002zam4k88bqhw2j"},{"name":"","_id":"clwwvjmfm003bam4k1c9b4l06"},{"name":"","_id":"clwwvjmfn003tam4khqtvcem7"},{"name":"","_id":"clwwvjmfn003zam4k0s5s7ap7"},{"name":"","_id":"clwwvjmfn0045am4k7k5r7fyz"},{"name":"attention","_id":"clwwvjmfn004bam4k2fz20y4k"},{"name":"","_id":"clwwvjmfo0053am4kccwi6g9x"},{"name":"positional encoding","_id":"clwwvjmfo0058am4k9lsh864g"},{"name":"RoPE","_id":"clwwvjmfo005eam4k3n7x08zg"},{"name":"layernorm","_id":"clwwvjmfo005iam4kghgahceq"},{"name":"normalization","_id":"clwwvjmfp005mam4k3te9en2z"},{"name":"batchnorm","_id":"clwwvjmfp005pam4k5gc4hsrc"},{"name":"sliding window attention","_id":"clwwvjmfp0061am4k2rn5h3t8"},{"name":"sparse attention","_id":"clwwvjmfp0064am4k085xbz6x"},{"name":"","_id":"clwwvjmfp0068am4k28bm3vxe"},{"name":"","_id":"clwwvjmfp006bam4kha3801b9"},{"name":"post-norm","_id":"clwwvjmfq006kam4kdole13l3"},{"name":"pre-norm","_id":"clwwvjmfq006nam4k63mh9wxp"},{"name":"ChatGPT","_id":"clwwvjmfq006yam4k8bsq4r4p"},{"name":"Sparrow","_id":"clwwvjmfq0071am4k60jqek6e"},{"name":"LaMDA","_id":"clwwvjmfq0074am4kf1rtafsi"},{"name":"GopherCite","_id":"clwwvjmfq0077am4k2ibta3ck"},{"name":"WebGPT","_id":"clwwvjmfq007bam4k89qm1izm"},{"name":"InstructGPT","_id":"clwwvjmfr007cam4keckc4688"},{"name":"MoE","_id":"clwwvjmfs007ram4k4hxx9hr8"},{"name":"KV Cache","_id":"clwwvjmfs007tam4kevv53ch3"}]}}