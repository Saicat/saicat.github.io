{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","path":"images/logo-algolia-nebula-blue-full.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","path":"js/bookmark.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","path":"js/comments-buttons.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","path":"js/comments.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/config.js","path":"js/config.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","path":"js/motion.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","path":"js/next-boot.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","path":"js/pjax.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","path":"js/schedule.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","path":"css/noscript.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/addtoany.js","path":"js/third-party/addtoany.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/fancybox.js","path":"js/third-party/fancybox.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/pace.js","path":"js/third-party/pace.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","path":"js/third-party/quicklink.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","path":"js/third-party/analytics/baidu-analytics.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","path":"js/third-party/analytics/growingio.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","path":"js/third-party/analytics/google-analytics.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/matomo.js","path":"js/third-party/analytics/matomo.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","path":"js/third-party/chat/chatra.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","path":"js/third-party/chat/tidio.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","path":"js/third-party/comments/changyan.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","path":"js/third-party/comments/disqusjs.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","path":"js/third-party/comments/disqus.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","path":"js/third-party/comments/gitalk.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","path":"js/third-party/comments/isso.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","path":"js/third-party/comments/livere.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","path":"js/third-party/comments/utterances.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","path":"js/third-party/math/katex.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","path":"js/third-party/math/mathjax.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","path":"js/third-party/search/algolia-search.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","path":"js/third-party/search/local-search.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","path":"js/third-party/statistics/firestore.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","path":"js/third-party/statistics/lean-analytics.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","path":"js/third-party/tags/mermaid.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","path":"js/third-party/tags/pdf.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/wavedrom.js","path":"js/third-party/tags/wavedrom.js","modified":0,"renderable":1},{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"source/images/cover.png","path":"images/cover.png","modified":0,"renderable":0},{"_id":"source/images/qrcode.jpg","path":"images/qrcode.jpg","modified":0,"renderable":0},{"_id":"source/images/avatar/20180303210737_XsJVr.jpeg","path":"images/avatar/20180303210737_XsJVr.jpeg","modified":0,"renderable":0},{"_id":"source/images/avatar/Picasso_Elephant.png","path":"images/avatar/Picasso_Elephant.png","modified":0,"renderable":0},{"_id":"source/images/avatar/shadow.png","path":"images/avatar/shadow.png","modified":0,"renderable":0},{"_id":"source/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","path":"images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","modified":0,"renderable":0},{"_id":"source/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","path":"images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","modified":0,"renderable":0},{"_id":"source/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","path":"images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","modified":0,"renderable":0},{"_id":"source/images/background/wallhaven-2ywymm.png","path":"images/background/wallhaven-2ywymm.png","modified":0,"renderable":0},{"_id":"source/images/background/wallhaven-gpxpg3.png","path":"images/background/wallhaven-gpxpg3.png","modified":0,"renderable":0},{"_id":"source/images/background/wallhaven-p97q73.png","path":"images/background/wallhaven-p97q73.png","modified":0,"renderable":0},{"_id":"source/images/background/wallhaven-x636oz.png","path":"images/background/wallhaven-x636oz.png","modified":0,"renderable":0},{"_id":"source/images/favicon/favicon_io/about.txt","path":"images/favicon/favicon_io/about.txt","modified":0,"renderable":0},{"_id":"source/images/favicon/favicon_io/android-chrome-192x192.png","path":"images/favicon/favicon_io/android-chrome-192x192.png","modified":0,"renderable":0},{"_id":"source/images/favicon/favicon_io/android-chrome-512x512.png","path":"images/favicon/favicon_io/android-chrome-512x512.png","modified":0,"renderable":0},{"_id":"source/images/favicon/favicon_io/apple-touch-icon.png","path":"images/favicon/favicon_io/apple-touch-icon.png","modified":0,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon-16x16.png","path":"images/favicon/favicon_io/favicon-16x16.png","modified":0,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon-32x32.png","path":"images/favicon/favicon_io/favicon-32x32.png","modified":0,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon.ico","path":"images/favicon/favicon_io/favicon.ico","modified":0,"renderable":0},{"_id":"source/images/favicon/favicon_io/site.webmanifest","path":"images/favicon/favicon_io/site.webmanifest","modified":0,"renderable":0}],"Cache":[{"_id":"node_modules/hexo-theme-next/LICENSE.md","hash":"68fc9a03d50fd4b5ea97092b05967d1819dea2c4","modified":1706697684353},{"_id":"node_modules/hexo-theme-next/.DS_Store","hash":"de144412354354ba6acf87862ba4f4b6f025ccfc","modified":1712377768129},{"_id":"node_modules/hexo-theme-next/_vendors.yml","hash":"4f6046ceb1470be9ff334ede20b73871c951d845","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/README.md","hash":"d6820f46d03a93bd6dc8b10f49f58aec82ad2b06","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/_config.yml","hash":"255c963c680da5da34c259c560dd8211b75188ca","modified":1708604632809},{"_id":"node_modules/hexo-theme-next/source/.DS_Store","hash":"20f7a9b9a682cc55305492b2e240489f6bf832e6","modified":1709026838425},{"_id":"node_modules/hexo-theme-next/package.json","hash":"4b48877b223ec717e708540a2df03d64983c02ab","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/docs/AUTHORS.md","hash":"a648823121563c34a177ae91f5a774b5e29f01a0","modified":1706697684352},{"_id":"node_modules/hexo-theme-next/languages/README.md","hash":"b2567e32805dda79601157351a07e5ca9fe01315","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1706697684352},{"_id":"node_modules/hexo-theme-next/docs/LICENSE.txt","hash":"f5b14f791b7cfa1d16da981d929152e088a5d1b8","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/languages/ar.yml","hash":"7d0f39e8684284a04bb9808521c87fecda8bd131","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/languages/de.yml","hash":"79b37df731c29665dee6cd7c90d278e1edfb6e24","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/en.yml","hash":"ba0fd79a2b1d8db01a034180556061745965ff05","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/es.yml","hash":"dffc63ef42e1266b88e0acf08994fd17a9908d53","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/eo.yml","hash":"e34bb33ae827bf2f0727088599a73bc64bdad1b0","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/id.yml","hash":"929df147f4f17d638b07de5fe52ca13e2549ab1c","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/fa.yml","hash":"f3ffc444599f4ac92d62e9ed00a1490ebc277d70","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/fr.yml","hash":"8ac44e58f71a38b7697a2f7f98a6971ed818cb5b","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/bn.yml","hash":"564bed75da6e05b11dce6164508f97a15e2fb6c2","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/ja.yml","hash":"543222bfc516aab6c33e8534f807972ecb8943a9","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/ko.yml","hash":"d345a303310c8a5f4836c3683f3580f861ebd1b4","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/nl.yml","hash":"3cb3687696635ec71b4ca40c5fc43b56acc8843e","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/pt-BR.yml","hash":"76b8576ce228d540a16b1f0af5af2cce20923194","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/pt.yml","hash":"70de366e10ea584ba039d40d6b35ac97f93454ad","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/ru.yml","hash":"c6d8de0ff7d8148d09993257cfd3b7aca755696c","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/si.yml","hash":"2d712eedf3f60d04d36c3108cf5a12e2a52e875c","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/th.yml","hash":"6829e998b39f8f143e20b276bb1f62d95a29de58","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/tr.yml","hash":"a57e4ed089b893a95f5e1ecff17ce625165f4d46","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/tk.yml","hash":"511726054873f6f8d7ce0d2e803f6731de0ddbe7","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/uk.yml","hash":"ff537047b4b4c3ca9a7b64fa7f428a9942751eeb","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/vi.yml","hash":"7ebcba5e1128784195e4681dffc9d34c4e873fec","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/zh-CN.yml","hash":"741d7efe0262c9cdc2c648014b55599665d90f6b","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/it.yml","hash":"16d716ecfd748def2f6486ef5a82d0ab7ceb4890","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/zh-HK.yml","hash":"88ea50eeb9097ab4a87a44981a102d8594feb064","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/layout/_layout.njk","hash":"fc0a45112f2dcfc2642404e8934ea32a793c3bd7","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/category.njk","hash":"c68b7343d0f8145010f93351908cc36ef6212ec1","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/index.njk","hash":"dd63e488ae8cc144335a5958acedf6a16edd7a92","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/archive.njk","hash":"d759f4d2cf5ddc6875ea250113a00662c1caf6d1","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/post.njk","hash":"0bfce9f133f501a9a4837257e3b862b3bbca15be","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/page.njk","hash":"b0660b2af0ac7d3fda14ca4d9f2c9e79ef06c6f9","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/.DS_Store","hash":"8870fa1130b0178d74f0bce24d0b2f4569d4f933","modified":1712377610248},{"_id":"node_modules/hexo-theme-next/scripts/filters/default-injects.js","hash":"872f01cb10e422a648ea505436532e776e92926b","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/languages/zh-TW.yml","hash":"4695c87d6b81b3a23d16ad6513d9eaa925f8d8ad","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/scripts/events/index.js","hash":"bd9ea82376cd87df611ea3ae077875c7c595a3df","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/filters/locals.js","hash":"9eb5310664759931287dd28ea39165dfb67f12ed","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/scripts/filters/minify.js","hash":"447db39d17775b2bd18d8af9c9d65b7b8449f751","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/scripts/helpers/engine.js","hash":"d292b78485e8e8055712b0ed6de7cf559c5fbdcd","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/scripts/filters/post.js","hash":"fdc8a0af90035e89c3fcb754a0eb189b8951a2bc","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/scripts/helpers/font.js","hash":"3394185a7f0393c16ce52c8028f90da3e9239c55","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/scripts/helpers/navigation.js","hash":"78107021101553c3d23e89290f7530b60cf4aa86","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/layout/tag.njk","hash":"9e16ba20c28a7f2c6bc75aa427f48122301a30aa","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-url.js","hash":"6281d47c1de98eb38f3aa0f6df29bbb19d412173","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-paginator.js","hash":"e86c764b546e4fbb87970cabc4135a56f9ef9fe1","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-config.js","hash":"ead37e9167b682f1fa34b5401c3050e18c7ee4a3","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-vendors.js","hash":"957241c28796ff352de7f4cffba7bb289b043586","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/button.js","hash":"c6ad2ed544fbb25ecb5d820c36e76302504271b7","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/scripts/tags/caniuse.js","hash":"935a311142a409c1896b3ae3f01fe7a9e2db1134","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/scripts/tags/center-quote.js","hash":"92c19d796bdb3320df9caea59bf52df7a95d9da9","modified":1706697684337},{"_id":"node_modules/hexo-theme-next/scripts/tags/group-pictures.js","hash":"9ed799c329abf830f623689d7e136991256a24ca","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/scripts/tags/index.js","hash":"1f6aba7820f1fb58b61969485148db21846e1aa9","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/tags/label.js","hash":"8a73348186113bae0a51ea2f891c1bb882fab05a","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/scripts/tags/note.js","hash":"7b94ddb46b7d4b0fe815f2fbe4bd375f07f55363","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/mermaid.js","hash":"4fb01ca650fa8b256b8d48f50dc1b18350bd3d6d","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/scripts/tags/link-grid.js","hash":"18a483c2d5afd701f6080ffdddf2d1321370336c","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/scripts/tags/pdf.js","hash":"344636b6fd7e27e8831c1e194039afc0d61931cd","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/tabs.js","hash":"0eabe51da40b4b13e16419c8fe02452d9a4fef73","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/scripts/tags/video.js","hash":"2ee926448583be8f95af1f2884ae2c9c4830151d","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/scripts/tags/wavedrom.js","hash":"b44dfeeb58b41945d469141787f3dbce4b117d08","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1706697684330},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","hash":"770d63c26f22705311028a36b52e999cc8a2da82","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","hash":"745bd828205da7e5fbd3f860cc3697097630f5f3","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","hash":"b03ba78c6916ad2f390d55bc1bc18fafb64b0ebf","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/docs/ru/README.md","hash":"29c89a41b371f893e56c87ea61adabc444ec58cc","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","hash":"d775148c2bf20c028622af5609a788167352bf1e","modified":1706697684350},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/README.md","hash":"12a3e96581964a22b474cc739675d52ef93ff932","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"12a6631617695504d5cf2a94b57d87bd331bef6f","modified":1706697684353},{"_id":"node_modules/hexo-theme-next/source/css/_colors.styl","hash":"3c6798c10cc220d83481cb3f3782e78558cee789","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_mixins.styl","hash":"83647a6207333b9609ba90b0946b3fa9548e6381","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/main.styl","hash":"921a58577f411cf4eb5cfd66db0a241f8f88578c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","hash":"dadc81256afb127b77eac6763d5ee0ec9c77f0a3","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CONTRIBUTING.md","hash":"a089f7a8368ab0b7d7b9b7ec0ac3767a453435df","modified":1706697684353},{"_id":"node_modules/hexo-theme-next/layout/_third-party/addtoany.njk","hash":"ef64c6bfb8540cd874701236b9be47db2496e98e","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_third-party/fancybox.njk","hash":"844559f46e2ff1c8be234d5763703106e2072a7b","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/pace.njk","hash":"d7ad5714079f7f65446f880baf14722435ca9061","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/index.njk","hash":"dfd7cdd6ba89f8c3deabc27726c7a350cadafd11","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/quicklink.njk","hash":"0efed71ed530447718c4ea5bbd5fc8695b0b0d5f","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/comments.njk","hash":"d0c470b0f6690aa217e9ada848c5e2e73fb27c6f","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_partials/pagination.njk","hash":"bc719473ed5948ab6859449d60b8d36cfc1542b4","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/widgets.njk","hash":"e7f988ecddb2159313699a00827a45eca5622bd4","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_partials/languages.njk","hash":"e43f22198cccb5f6e306b1ce0d28d12a4fb891f8","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/source/css/.DS_Store","hash":"6b12ac4edf32b2194ccd6b95c3f5930b07c7d56b","modified":1709026838423},{"_id":"node_modules/hexo-theme-next/layout/_macro/post-collapse.njk","hash":"abda600685ee972e1f6b7a2dcc56f13e2daa6263","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_macro/sidebar.njk","hash":"547c62ab14d9e05d2d9116db9048a677fbe1fb6d","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/footer.njk","hash":"d77ec95cfee58b17807763dc2adb7946829cb316","modified":1706757600094},{"_id":"node_modules/hexo-theme-next/layout/_scripts/index.njk","hash":"6668878a0f9a1166c6a879755f54a08d942da870","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/config.js","hash":"9ec51eb61f7fee612ffc5252f489003a0fa301fc","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/highlight.js","hash":"6aec7b2c38c50989a23bfaa0d560e75c7f553e12","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/injects.js","hash":"d987709267a1bc6e5014411e9983d7c49c102c16","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/utils.js","hash":"6853e5433e3eaa19ea43fa20b08d956ba4cec4ac","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/vendors.js","hash":"464db1e7182e5b9cdbd32e8b5368d5e683b1d9c7","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/navigation.js","hash":"dd3562686d95a50375e6fd32e717ccb0d99c1e3d","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/changyan.js","hash":"5798cfc8f63665031dd3e01debed051628cec319","modified":1706697684338},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/common.js","hash":"19a402a225c31edffc50f202a14e0d582d3db23e","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/layout/_scripts/vendors.njk","hash":"be80b9fe415a9a09d74c28e230995fd292dfc123","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/default-config.js","hash":"93ee5f9109dad885dc38c49bcee630c10f9dce6e","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqus.js","hash":"7f71d6b271ba65ff333d5682e7575711d368c0d2","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqusjs.js","hash":"a600a98e7436edeb31e291abca359885567df3c9","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/gitalk.js","hash":"7bb7dafdd7f6bca8464b54e17e552ce7f1714195","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/isso.js","hash":"ff8b5b5145220a17d0ecd9508ba9bd2d3b2da47d","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/livere.js","hash":"5a07d8bb52bc1d51a624ca8db54be144566c306b","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/utterances.js","hash":"d3bded697bc32dace689d2a6dfb6eb7514169d15","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","hash":"ba7ba2c129d1f240c6a22cec3e53f3f22af64b6b","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/source/js/third-party/fancybox.js","hash":"819f382c561fe5ec23c67cc5fabd63dd1cc22dc1","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/layout/_macro/post.njk","hash":"cbe208445e4d1df82ebd1761e1eaced3eab77fb3","modified":1706698899947},{"_id":"node_modules/hexo-theme-next/layout/_partials/.DS_Store","hash":"e2dd935d50913be894bedacfbaa77854f0aeba95","modified":1712377615269},{"_id":"node_modules/hexo-theme-next/source/js/third-party/addtoany.js","hash":"5276c8f78ee562a8965216dc67d762e59cb4a9f2","modified":1706697684332},{"_id":"node_modules/hexo-theme-next/source/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/baidu-analytics.njk","hash":"6215309aee028dcb734452beec448c5afb6c63fc","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/google-analytics.njk","hash":"d89066ff53879693f023e540d59c86137172c529","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/cloudflare.njk","hash":"a5b8297c2c383124dd6a56e256ecc0c0dcf489be","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/growingio.njk","hash":"8afaa772c390bd9d53a5cff9645ac3168334eb98","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/index.njk","hash":"f900306497b133e8b098bd9f4b96b93d1d96c185","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/matomo.njk","hash":"4e89648a8ec8194c5823064cbca39c938a799006","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/microsoft-clarity.njk","hash":"9dc00fcb0a05899f048eace9f9160b78956655d5","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/plausible.njk","hash":"ef9f2bb7110507f1c4336800af9157d5fa9765bd","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/umami.njk","hash":"3343750682fbd8535e50f8129be3003ad26015b4","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/css/_common/.DS_Store","hash":"71c6bca6ae43dd79b3d75183550713e9ad0f9f8e","modified":1709197037676},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Gemini.styl","hash":"96e0a7c2a65ce68215e17e369085b2ea2f1334f2","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Mist.styl","hash":"a1418c9dc8c0f1a0ad4ded0f4627c45bf0db1a10","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Muse.styl","hash":"e3be898f5ebcf435a26542653a9297ff2c71aeb0","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/chatra.njk","hash":"d7263fca16d0278ccf1f6aa1c6df6902a6344a09","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Pisces.styl","hash":"48f4f277946a168d0db1ea02804e85c22ca2c7db","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/tidio.njk","hash":"02aab857c27fc103216029be991688b12a73a525","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/changyan.njk","hash":"d1c950f8fbdf85e7a3eae5463767a89e858e8220","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqus.njk","hash":"9375b19a89b7fa9474e558d085af5448d4c5c50c","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqusjs.njk","hash":"0749cb6902baecdfd01f779a2a2513f6d2f6a823","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/isso.njk","hash":"64cc3bdaf644fd32c0d0a247f29f5b6904da9af3","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/livere.njk","hash":"3b13b09fba84ec6000886890a6710736a2b8fafe","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/source/css/_variables/base.styl","hash":"c4fc4e862d09221265ab1466085f057be2ad2e4d","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/gitalk.njk","hash":"b63b7e2ede0d3e66e732fa1a06bda9b19e1e85d4","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/utterances.njk","hash":"5a94032bc3512a10ad4328fc19ec07b819a1d687","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/index.njk","hash":"abf37fc55aa86702118e8fdf5bf2d389dd589aa0","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/katex.njk","hash":"1ebf658690468ea197bdd0416eb7cfa4bd0b083a","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/mathjax.njk","hash":"3677017fd4572b158311f5f5d870590ab25184e0","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/algolia-search.njk","hash":"24ed76e0c72a25ac152820c750a05826a706b6f4","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/localsearch.njk","hash":"e45ea3542cdc9ed7ec8447b5e6f35df4c5e82758","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/busuanzi-counter.njk","hash":"a4bc501da0f22f7e420f0ca47e83988ce90b1368","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/firestore.njk","hash":"d32ebe94560fa95824478ebbff531bffc47b194d","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/index.njk","hash":"568ddf7955d11d93fb5e842b403a7ac8b1b7fdb1","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/lean-analytics.njk","hash":"2446e748cdc102c78492216319ac02148db7daf6","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/mermaid.njk","hash":"099e031f52fb8e47b3af5b2684737efc9e643ee7","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/pdf.njk","hash":"2c81984cc4f5123103460442f6e046f5b6c97127","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/wavedrom.njk","hash":"02202bf563fb5eedde2ccad4d6c5b9109d30a703","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head-unique.njk","hash":"8da52a144060db1a0a088ccb2e6cc8376d1fce70","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head.njk","hash":"5388b157bba4a40b9312f4a45c6678974ccf0837","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/brand.njk","hash":"dd9c4c03e99dfde0dfb8edefcb2c933f2f560efc","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/index.njk","hash":"650de421a8ce4cf685428ffbe0087ff84cbd1356","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu-item.njk","hash":"41a8b0cc16f60fa085cb719d07216d86b6bc4bf8","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu.njk","hash":"ee6fc2f111572d3eeab0a2fecbb2d6b3e37ab26b","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/sub-menu.njk","hash":"06480d8ec5f0b87eafd47f082f07968d7282dd5c","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/breadcrumb.njk","hash":"89825e75cc45e9709fa6ba89883669eedaff6f46","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/categories.njk","hash":"17156d99941f28a225951ffdcfa9a115e20dc2d2","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/page-header.njk","hash":"7ed4f102a1825195cff8d7995bf9219f323a9034","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/schedule.njk","hash":"0f4bc8e257da60f77c0c1738607b2bde55810684","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/tags.njk","hash":"a18d1598e36cc72f2b0b24c3cc3c5990dfaa3254","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-copyright.njk","hash":"bfff923526d6800218f08dba6ce0bbf5c17755fd","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-meta.njk","hash":"9fa47e4fb342811da590ee4adc91cf81118c0a39","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-followme.njk","hash":"c1e33b4889f75acc490af3c8bde0ec56c518ff41","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-related.njk","hash":"e0986db00a0201dd3c60570f964829c84ba5bc68","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-reward.njk","hash":"e8b8a7c41e9ec612d0c0c73419529d55d1c16256","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-share.njk","hash":"16696990e4ce65fc8db18c4635082a5d5d06ff07","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/algolia-search.njk","hash":"efb2b6f19df02ba5ae623a1f274fff52aed21e6f","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/index.njk","hash":"8f6f256ab3b351ffc80f1f3f1d9834e9a7cfac31","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/localsearch.njk","hash":"661f7acae43f0be694266323320f977d84119abe","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1706697684335},{"_id":"node_modules/hexo-theme-next/layout/_partials/sidebar/site-overview.njk","hash":"78a1a8cac44de7e963ab4cd51c988442eb3e789a","modified":1707031409664},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","hash":"def07bcc7c17d8a0caad177fb1dd2f3a5e5b3536","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","hash":"1e826dea3f684c0515f362dc1352447a1f0eae71","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","hash":"da361917d65e5dca8362f8cdeb6c8cc0e8316cec","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","hash":"fdb7b7cef1a147d897e7f7cd8903b58368ec2062","modified":1706697684333},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","hash":"4536cb6d0a9bbaaa86fab3fa0101f6a3a3ec5a76","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","hash":"6e0682bb42170d61b13b786295f45f9c785f8b73","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","hash":"835cbf54c49ef1327f47df70ff2636ad36b6f57d","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","hash":"6bf821310342c5b87a631873e7650a475a0765f1","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/wavedrom.js","hash":"40dcd10df6edf124088c329346e0cc0bdac74ef1","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_header.styl","hash":"dafc6d23c80d6fe3e55a7711e94210d2479b629a","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_layout.styl","hash":"fa4fd8f76464e214fb7318f325b13c2b62f4b478","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_menu.styl","hash":"fb550935d374e0bdf1097fce187337dc05cad3e1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_posts-expand.styl","hash":"485d23ccb42c0d0c8ead7ea8930dd3e06d79a285","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/index.styl","hash":"ab16a3dcdc0393b9b582ef59dcc13db9320e917c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_header.styl","hash":"3fbfab591f280e2e7f3b0265901c93bc4bd137ed","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_layout.styl","hash":"6569a6640f79d247a8235b3914772c0e2f99ead2","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_menu.styl","hash":"82cda756f5b7092df2eee6641b9786df71623bdb","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sidebar.styl","hash":"547c0b5cd5e7ea10d21863d13a6b16579a49396c","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sub-menu.styl","hash":"c48ccd8d6651fe1a01faff8f01179456d39ba9b1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_header.styl","hash":"ac2dc0ce9c775a83ef7132ae957b54539366ac9c","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_layout.styl","hash":"26a0cba1eee5de45a45a5e14e17707f905390512","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sidebar.styl","hash":"91dbf3ca5c3a613d4e30618c120da535bf2d0336","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/index.styl","hash":"8000075b227749a7495eaf417cac6ccfbe441580","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"778ed2ad5643b93970c95626b325defeb586733f","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_menu.styl","hash":"72dc825c50357402c342d62ab60fc0c478ab6bc1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Gemini/index.styl","hash":"9dfe853c901bdc52fc950bacdf15484dbb9bf140","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/back-to-top.styl","hash":"7664491542046df9a3887cf40a06e00c0b4086a9","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/index.styl","hash":"2298e521253b3bf376a2412271bc2a7d305051f3","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/reading-progress.styl","hash":"90a86045a33c1bae49fc2f6fa1e1b53170c7f77b","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/base.styl","hash":"d0a7c99095f490b0d2ed6b1be43d435960798cec","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/buttons.styl","hash":"a042571d85ff7265f799004239a45f36b716b8a6","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/comments.styl","hash":"e4fecc889ba3317a64e9abba5842c79dff9b7827","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/index.styl","hash":"523fb7b653b87ae37fc91fc8813e4ffad87b0d7e","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/pagination.styl","hash":"f4228c759db4a650c8d38745c2edd1dc83c45687","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/normalize.styl","hash":"b56367ea676ea8e8783ea89cd4ab150c7da7a060","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/index.styl","hash":"8e34df131830d4fa3725e4590a672ba1cf1903e5","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/mobile.styl","hash":"1dbf2c339adcd27026c3a2ded32ee91ce08cea26","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/breadcrumb.styl","hash":"8afdc311c6b8db121758371f95cf1c5e77354f42","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/categories.styl","hash":"b6e2eb1550a7845cb2adf86081a4ab6c7bde1e68","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/index.styl","hash":"7504dbc5c70262b048143b2c37d2b5aa2809afa2","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/schedule.styl","hash":"6b816c2511242ee503fb5f34cd3e4dcdafc06b85","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/disqusjs.styl","hash":"877a537d5b95beb048142e4fdee6f17e6ef9c7bb","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/toggles.styl","hash":"782ee1fc5e669d3ddbfeb82b73ad7fe561f1a4fb","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/tag-cloud.styl","hash":"1a81d1a71fcf0699629ce6e72dfd0a15f3a2dd0a","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tables.styl","hash":"e840b23d33023e6d45e018f6e84b683dd56efd8d","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/index.styl","hash":"54d12e2c5d9982f7b9e5b23be5133954a8514e9d","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/math.styl","hash":"9d995eb4871a6c273d9d51558676a1fdabf69e72","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/search.styl","hash":"e72799ce3f9b79753e365b2f8c8ef6c310668d4a","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/index.styl","hash":"098d4bd034e986fcf7e443eac4fc2193935461b7","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/utterances.styl","hash":"56d90ae0559caa55b75f3c300ff2711f9ed65fc4","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-body.styl","hash":"56d5b7ff73f466c9ae54f7204ae899281295d749","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-collapse.styl","hash":"7369928305330c73ae0b3f063a681a8384d8fde4","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-footer.styl","hash":"11497388f124bfbb4001495a67d3629a9f618405","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/gitalk.styl","hash":"8f094c4ac17e2ab45569b12d157747f9c7333c12","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-followme.styl","hash":"1ecfd64507954810b07a9d21fb5305b5378feda0","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-gallery.styl","hash":"aa366d37389760c8595529b850f461569577a1c5","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-nav.styl","hash":"9ac6f477177264c26a46e8333b8456720a0444dc","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-header.styl","hash":"1191f1bfa5c43e54be8e5b3cc0d802984e161747","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-widgets.styl","hash":"ebfba158a0a4af3d1dabcacbc58986664de52140","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-reward.styl","hash":"04cf4a69537fc14d3b8904f965d283356853847f","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"f634f94828620e88c3f5a8db56f7944f6ba232b0","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/fold.styl","hash":"42a0b65491ad85438596b3fe0b7f23973e4cef34","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"393ff96234e4196b569d4b11496774eb78e147de","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/index.styl","hash":"138f78147bc6bd6005f329ada34dc79b7625542d","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/index.styl","hash":"22cd37bd5df9972d5074710896aba4424ad5161c","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/label.styl","hash":"debee14539272fbe3835a7d3853af2230baa3501","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/link-grid.styl","hash":"7f8a7345e6537a62cd9e9a94c8f7065b541d9b04","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/mermaid.styl","hash":"48d35dba575a7c9e8845b16652e76b7d4a4646de","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/pdf.styl","hash":"b6654a1d7cf82577d8263faffee8af3ad4a5c0e8","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/footer/index.styl","hash":"4e967702cf4c637132346bc74ec8854426f1a68c","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"d6418fd2bbfba7b73ddf11ec62db9637fdf5d8af","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/wavedrom.styl","hash":"af113411ad9cca7674177be36af8dd399680834d","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/tabs.styl","hash":"33dd6ad015dde65fd46f34961655442e8e82b52e","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/bookmark.styl","hash":"e74f4bb47a101b014ee2a1783c87f3b87323f9a0","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/note.styl","hash":"98d4c20aff0f0fcfe1824017fb06ab21ef0d218e","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/menu.styl","hash":"bbbc40b03cb299d2a6a568f329b2ce98e1cdc430","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-meta.styl","hash":"a851e9d5aefcd027c95eeb323860b6da70f202d1","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/github-banner.styl","hash":"38c64c2d04e46848382bfa246a0e9c508294767b","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/index.styl","hash":"6e0d0796ef7fbbb62ffdfb448753a850de82c74f","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/index.styl","hash":"da5e88f8debd5ac8d7af5c6ba6240df66104955f","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-nav.styl","hash":"bf3ad8b4268f763a1e26377681644887694bc009","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"0847400d8579b0a2dd1bf662c78954c10adf2680","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/related-posts.styl","hash":"b05908f04ef95f2d91e6eba89b12411c378d050f","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"46eece42510c2c89bb9209afb0262ad76a4b0b36","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"5b38ac4a0f1ade0e681aff0e3366c481d9cf3dcd","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"c6a27beb3f741211a14576026f3b4cfc44cc6407","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"24752d145c6fb8f5344dca9c7b9640839c02e009","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"c2e354a565c8c1b32bd0ceacc972b17982758b67","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/site-state.styl","hash":"26dd0adfcb1db6df29c6090c8d7e9b5a43583fb0","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"9a7c71560fbdc936ad4e736fe15063ea3e8a644b","modified":1706697684374},{"_id":"source/_posts/.DS_Store","hash":"9537ad1c18c9fd6035ad6454d9fbe73e921c7bf7","modified":1710643828009},{"_id":"source/.DS_Store","hash":"14806b28f39163f96211abc3c45d4c1cee37c964","modified":1711530520631},{"_id":"source/images/.DS_Store","hash":"60fbc9bc8c2d88510803a394d229a0442cae1cb2","modified":1709016856181},{"_id":"source/CNAME","hash":"07ad8fed7f1b810bf3715a0e68324e5cbb1d47ac","modified":1706872557451},{"_id":"source/tags/index.md","hash":"e995ed2b8452b1906600b3853b920f13423098b7","modified":1706698644396},{"_id":"source/categories/index.md","hash":"f5c920fbc09ea3d8edf250de7e31bcc6b3e765ae","modified":1706698717077},{"_id":"source/images/qrcode.jpg","hash":"eab6c8893d0c79cf705cf64d738f560563365b0c","modified":1707548301740},{"_id":"source/about/index.md","hash":"9294d008cc673abc2eaf740f101ebac560029267","modified":1706698701349},{"_id":"source/_posts/cs/.DS_Store","hash":"db1ba195a60412ec7a7d63e3f3ffaa7a874fe079","modified":1710516092319},{"_id":"source/_data/styles.styl","hash":"f4bb55ef0972c829e3382d1bae1786b3ab5d54ef","modified":1707045638288},{"_id":"source/images/cover.png","hash":"2f2aa6173619dd38425673ba110b50b9156d4d10","modified":1710684380714},{"_id":"source/images/avatar/20180303210737_XsJVr.jpeg","hash":"2ebbbcead4446a69b0ac0698ab227a1af49735ce","modified":1707030615190},{"_id":"source/images/avatar/.DS_Store","hash":"c3fa37607ceb3f7ba411cf4203d2a333f773d921","modified":1707118756919},{"_id":"source/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","hash":"3e7db51e59a8261de04f55814475556494d153f6","modified":1707048498957},{"_id":"source/images/favicon/.DS_Store","hash":"83ddccadffca5384db3dfc167728b7c7cacd9a87","modified":1707796842439},{"_id":"source/images/background/.DS_Store","hash":"88f5d31d0db89adcf679f2a7fefc8947139a1c1f","modified":1709026807046},{"_id":"source/_posts/cs/nlp/.DS_Store","hash":"191b630f3a0ab6b51ade0203a88ec2a591a4c630","modified":1712491157998},{"_id":"source/images/favicon/favicon_io/about.txt","hash":"d1f084751c81e98b1675b8c0745f6f306a1eaf66","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/android-chrome-192x192.png","hash":"2164889376f2bd6f9dd2066ebdfe4abd547f49d9","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/android-chrome-512x512.png","hash":"419f0b0317ad2a31acfa7dc19e1502731e92e05f","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/favicon-16x16.png","hash":"fb63ee2fc88b9ce42dec5bf68088da14be9350f5","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/apple-touch-icon.png","hash":"5e36b0bfe67bcf96f4f0e92366a5de509fe9a78a","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/favicon-32x32.png","hash":"f42c950146a171afca4ad70ff425b5b4ff02f926","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/favicon.ico","hash":"fceaa7158ae89a98fe5b93152dd7dec2d36c1ff4","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/site.webmanifest","hash":"bf31baf91bdd2fcde24a45e3f2a1be33733c6f69","modified":1706844814000},{"_id":"source/_posts/cs/nlp/2024/.DS_Store","hash":"e1ff4e987cacd29d0eccd08d7b9c94e2c1fdaea1","modified":1712491161398},{"_id":"source/_posts/cs/nlp/2024/04/.DS_Store","hash":"d164c5eb50b8fb0eb749d500a32ce6f1127b6576","modified":1712576047078},{"_id":"source/_posts/cs/nlp/2024/04/normalization-.md","hash":"6f8406c5b4ebfd9c9352673ea6029560549853c6","modified":1712575921659},{"_id":"source/_posts/cs/nlp/2024/02/LLM.md","hash":"dd73e47b1cdb864f26d5780ac4fe08603bcc9b3c","modified":1710314618942},{"_id":"source/_posts/cs/nlp/2024/02/.DS_Store","hash":"055b036ca6d94a43ddf5cc44ff50734997452711","modified":1709897296542},{"_id":"source/_posts/cs/nlp/2024/04/LLM.md","hash":"b42759e5c570dfd0d4521190facbac87ceeb2a65","modified":1713366809878},{"_id":"source/_posts/cs/nlp/2024/04/-3.md","hash":"8698b25360b8d695243cf4501595dcf9dd4dc63d","modified":1712377872156},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE.md","hash":"dbe2c4a5a96fe27437a53c4fa9054bb19e05f28a","modified":1712299467271},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention.md","hash":"31d3d5cbbde092ba1a855319647b24e32ffda8ef","modified":1710934710908},{"_id":"source/_posts/cs/nlp/2024/04/-4.md","hash":"29e98d2f05db69db5b17e76ae67969077665e0b2","modified":1713605340638},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization.md","hash":"39dbb9473a0afe9654512f34be923e0210d5e1a0","modified":1712471774847},{"_id":"source/_posts/cs/nlp/2024/03/Yi-.md","hash":"1871881f3afdaf9b2930d31a03d62079ca4ff9db","modified":1711713217115},{"_id":"source/_posts/cs/nlp/2024/03/-1.md","hash":"f25140aae947e215b4bf597973bf28a52bce575a","modified":1710685009511},{"_id":"source/_posts/cs/nlp/2024/03/.DS_Store","hash":"b4d7093fd8cccd2f9383673530a69adfd371e489","modified":1712491157997},{"_id":"source/_posts/cs/nlp/2024/03/-2.md","hash":"d8b936ccac17c2d991f3894335231adb87aaabc9","modified":1711253769176},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT.md","hash":"9af08ff8d7274b005502eba5fb0461ea1a0729d9","modified":1710646109054},{"_id":"source/_posts/cs/nlp/2024/03/MoE-.md","hash":"27d7179f587d01094d94db453a629a974e8c3a27","modified":1713603377731},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA.md","hash":"c5802e9f2c1eb743e74e4b89a0f4005a5c427b07","modified":1710425874411},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/.DS_Store","hash":"2e33b8d145af72ec31cbad8c19fc2528fc2a909f","modified":1709014663934},{"_id":"source/_posts/cs/nlp/2024/02/LLM/mix_precision_fp16.png","hash":"4da22fa80039f8271ffd839aba70f6b1aca7afe4","modified":1709188879237},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/.DS_Store","hash":"96243fa0e625d8d7395157516c6299723b4ce769","modified":1712575638552},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/bn_and_ln.png","hash":"ff10e75b9d2c95aa713f013b104ed307f2121e57","modified":1711118594120},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/complex_number.png","hash":"255aa16a44913d53784b09d0a08762df6d21dc16","modified":1708758408339},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/.DS_Store","hash":"f036d46924a246fd06315b601bca6cc759d95300","modified":1710600789234},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_large_performance.jpeg","hash":"54e3ed874802ac9465580d6b5fcc5d6c1de96244","modified":1710250364698},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/dilated_conv.png","hash":"bbc2ff2e9f891da4bfaf6d535ab8545acc18e8a6","modified":1710560488146},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/receptive_field_cnn.png","hash":"46515aa3bce1eb0fc244f62ceee7b899c28183e8","modified":1710321816411},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/.DS_Store","hash":"5d08a407b858db4f8a5bad5168cbb23224622856","modified":1711667651040},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1711247018739},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_and_ln.png","hash":"ff10e75b9d2c95aa713f013b104ed307f2121e57","modified":1711118594120},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_ln_gn_in.png","hash":"9783c818f5e0eaea33d169718476bbe8874cf945","modified":1711120826525},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/lossfunc_surface.jpeg","hash":"c78a8df335da0d507963fb73a62fe2c3d145c91f","modified":1711005649925},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/realformer_attention.png","hash":"e9a92e5c07c8ca6873ea70671ad54eb2f1a13332","modified":1711206279560},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/rmsnorm.png","hash":"55bbcb42145011f7b5adf90cc613e22e2b94f060","modified":1711165884464},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/.DS_Store","hash":"81b22bc167ab6b1514c4698d3c49fcf0836924dc","modified":1710670459475},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/.DS_Store","hash":"9d409e9dac238eb07cc6841f8e8d05eed83df842","modified":1710056957220},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/MQA.webp","hash":"456a8ab19cc1564912034c375e8c3c5a42be6837","modified":1709973970557},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/attention_calculation.png","hash":"1f020c39c78221e41c5c6953ef97239e9f42aa3c","modified":1709780575011},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/cnn_heatmap.png","hash":"cb0bde73c9c4d0646133947ebaab16c44c753667","modified":1709723125449},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/decoder.png","hash":"28ee3d1ab68bd325ecb9d2066bc264a63d7de081","modified":1709716894560},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/encoder.png","hash":"d6a3a39c420d90e50f02f8b34f127bfe34177331","modified":1709716888116},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/seq2seq.png","hash":"9baa57cc8000a918d0adca6dceaac3ea54791ea8","modified":1709716876496},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/softmax.png","hash":"de80ba20e55abf7457cac958aa87627d0a7e5d77","modified":1709821278308},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/seq2seq_attention.png","hash":"b95046eee028b45dd9734639ecde8189e93b2374","modified":1709781776387},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/.DS_Store","hash":"27b3f84a2ddc198e99ebf6d13ed505831a10240e","modified":1713585612426},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/softplus.png","hash":"bdc66c39227441390f2241b4f26c0b1fbab331d9","modified":1713279943448},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/xiaomi_moe.jpg","hash":"d898ba33f1ee70efa136dbff3cb38983b461524f","modified":1711814228139},{"_id":"source/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","hash":"0629f15db217c8b7037a5b46ef187d79b9a66a09","modified":1707048511782},{"_id":"source/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","hash":"6a3863489ed0982c7fe19d575eae28ba8c286ccf","modified":1707048415396},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi_nosft.png","hash":"d3b46ed19e458973c3586f3bf2ef906deb4080a8","modified":1709199016415},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/remote_attenuation.png","hash":"90aa3d8f76c9b5c18de8b7740649b499241cab0c","modified":1708957811757},{"_id":"source/_posts/cs/nlp/2024/02/LLM/rope_matrix.png","hash":"84372b91204098bab2ff09d2d865ad869f93bdbd","modified":1709262766062},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/cv_layernorm.jpeg","hash":"f0874ecc4b9d8da8bf3bff0e13a6313ed19a7b15","modified":1712494304517},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/big_bird_attention.png","hash":"ed6c76b9bb77b98d34c429333498d04dac8e3ed9","modified":1710561804292},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_architechture.png","hash":"5e4c347dc41d7f070f54b386fdccf675cfeb8f10","modified":1710255091449},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/model.png","hash":"631efc6d4e92da128d7a10a4dc6af307ee4ddcbf","modified":1711459921302},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/third_party.png","hash":"673fe2b2cad3b1f40c0fcfd190c0034d8dbe7f31","modified":1711615706510},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ellipse_2.png","hash":"ee20ecce8c3470d17b1a4bd43df811d16269ffd3","modified":1711006599335},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/prmsnorm.png","hash":"d5826342f665f4cb04fdbb2e3d83e0b2607355c9","modified":1711166590963},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/postnorm_prenorm.png","hash":"d8830735e89c73ca416baabf4a195d7891d9f0ed","modified":1711167201092},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/sigmoid.png","hash":"f1ced5f06861a2e0296050aff17eebfe3d023a6f","modified":1711246985230},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/multihead_attention.png","hash":"6f8ee285f2646dc163b6b3164a0639aa9ddd7f27","modified":1709637863252},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/llama2_qga.png","hash":"5e0dea7d03de9144eb524a0a9adb102e91b52aaa","modified":1709983486925},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/sram_dram.png","hash":"ae7a9296b67c02608460a334fbbad3781b890302","modified":1709971938995},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_load_function.png","hash":"644684f21f85d565328d98334d41bbe019acbbfc","modified":1712050094734},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/modular_connectionist.png","hash":"b5865cf34faba075b4f2316c2cae0559dac2d883","modified":1711981604894},{"_id":"source/images/avatar/Picasso_Elephant.png","hash":"6d38d4b24c13a3fd506ee37a562126b76a24de83","modified":1707118741657},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/rope.png","hash":"5535c87dbcfee3ed646db93662a0b6a1ccd3b31b","modified":1708958930811},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/cv_batchnorm.png","hash":"d9e8d897c36125fddcf1cbcfa5c237a37158a939","modified":1712503708413},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/longformer_attention.png","hash":"64860379955872ecac5835b3f9d8c6d130c7e485","modified":1710560038203},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/rolling_buffer.png","hash":"34d4db9f4855926db561faa80e934dd971c0974e","modified":1710516051198},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/9B.png","hash":"7de19972e48b1f43c501d60a1c43a28c46b198e8","modified":1711618166808},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/long_context_result.png","hash":"daea6f734d64bdf5d24c6e17a640f23b1bd35b5f","modified":1711531324567},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/deepnorm_result.png","hash":"138cafc159f1e2e02455c540b4754f7cbb7f521d","modified":1711208592246},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_algo.png","hash":"56f1ab55c0e94814e6e37c30421012ed82098d62","modified":1711028736211},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/deepnorm.png","hash":"4726d8a40d1d0db397005408295e1ba54809a7e4","modified":1711207769375},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ics_define.png","hash":"6bf3240ef78bad2cf76897a29c05428f4c195fba","modified":1711116459766},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/mqa_result_1.png","hash":"a052b57f71eb78e3158ed2ee06ff0e5597607a2f","modified":1709975039443},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_less_activated_expert.png","hash":"ac7ad86dabe94564bdb17399231c6ddc7da83962","modified":1712806182926},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_expert_specialization.png","hash":"64947872486dd083a7076bfdfe67cb0626208579","modified":1712805838590},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_compare_gpt3.png","hash":"311b21079599473054378e339885e0b87719e63e","modified":1712848303496},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_137b.png","hash":"fc11cfc87b2994956a9adbee76621fe5d964dc30","modified":1713447317172},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe.png","hash":"2a2ee095b8cc0727daa2cdd0c63891e4a470eae8","modified":1712042970123},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/vanilla_moe.png","hash":"7da0a6e9d529256107b5f6b287737ac47513a797","modified":1711962655018},{"_id":"source/_posts/cs/nlp/2024/04/-4/transformer.png","hash":"9dddf171ca51f2ed1218baa9b84f4b98e9b911cf","modified":1713605151549},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/prefill_and_chunking.png","hash":"0c706e0728ea462b2b00c59a97c79ccf5f05b598","modified":1710516401027},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/base_model_eval.png","hash":"9b4e65d246865683f8e3348d74bfad03c937b65f","modified":1711616539832},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/perf.png","hash":"3c068a423dcd32cac7f1630bd69fbe5a4c6789af","modified":1711614561095},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/sft.png","hash":"ea9aea143af836012f44d21956ab5455487e9bfb","modified":1711615463552},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bs_bn.png","hash":"aa28241d75f914603b9f7f67cc54db4e61bac668","modified":1711113379148},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/rmsnorm_eff.png","hash":"350a7a2703eef1ee9357609ae5820bfc30835681","modified":1711166117196},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/GQA.png","hash":"d6b0cd20ae5f68a96f0865b3d0b6b129969caa4b","modified":1709982190361},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/GQA_result_1.png","hash":"87f2c3632fdf83828e8bd07a95cd8e7bf277fc88","modified":1709982952107},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Scaled-dot-product-self-attention.pbm","hash":"03da711b1547c944deea60d9bf345eb30e7c566f","modified":1709638849226},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/gpu_cache.png","hash":"edb6b1abdecd3099f2d68c2a729c0ca9b1fb0db7","modified":1709970717456},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/transformer_structure.png","hash":"87f0258e43922eface0277e13167a4ba8c1402bd","modified":1709802508932},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_ablation.png","hash":"108dc8d66ae0e370e7969403efd85178b9a8523a","modified":1712805107241},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_family.png","hash":"9d813f12f82d8702886f7ede72c5a25151390ba9","modified":1712932424673},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_moe_family.png","hash":"21a6c80f1dac39eb364fb417ed83afde2b212675","modified":1713449413788},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_distill_sft.png","hash":"7a59660f367634d68aa392698042f3d9cfe190da","modified":1712979135172},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_init.png","hash":"f9fb36a0defac7a5990b5c58a614f3165af0ae3e","modified":1712934593938},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_dropout.png","hash":"63f36ca61aaa71e88ddf23339e63a9ccd898a6ce","modified":1712934811239},{"_id":"source/images/background/wallhaven-2ywymm.png","hash":"aeaf089b64472a1c72cdd9778c5deac4086eb38f","modified":1707045207190},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi_explanation.png","hash":"df22d646e5a47e6223b7722007cfb7dde7c27280","modified":1709198077742},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/eval.png","hash":"e78d1d820de4c455b9301124d6016a19762eae1f","modified":1711446016442},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/multimodal.png","hash":"b14a4eb4d377101acf7b50904b9ee0f1d473aacc","modified":1711614412507},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/pretrain_data_dist.png","hash":"8c66a625723cb87ee67a9ff60d3614b369f50592","modified":1711463822846},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/warmup_effect.png","hash":"3e936786065e1ab9cbad17f5b86a5b8129720270","modified":1711204451931},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_perf.png","hash":"ca3c95d4b1c1c8f986d7adcacbf04da444e91610","modified":1712049369889},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_specilized.png","hash":"138ad5a388c77cc02202de794ec4cb734d633065","modified":1712043657680},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_structure.png","hash":"d906a9148e035025683e8da1eee5fa3d87164aa5","modified":1713585604066},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi.png","hash":"193df6c392d71dd4b90202d9be97f000f37b68ec","modified":1709197025669},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_perf.png","hash":"c9d7ce0a301920c4e722e341200f311995923735","modified":1710558943189},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_swa.png","hash":"59037b91ba8f256fd89b3d60b8ce477e4c8f4b3a","modified":1710252446580},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ics_measure.png","hash":"e9fe87cfea7dcef7cb66e1d76c17d883cbbc3cbd","modified":1711115709830},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/28.png","hash":"d438e857378575809c880b78ca715dc69e50b364","modified":1710643355076},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/mqa_result_3.png","hash":"12e310102ace1f9e89c0e9a352cf4a3462335a60","modified":1709986493059},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_upper_bound_13b.png","hash":"cec21f0cf3809011ce210dffda35da3671147008","modified":1712803976611},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_perf.png","hash":"b6ed751d2f171dac971bcf1d7f12e8a2d7fad388","modified":1712672178340},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_structure.png","hash":"b0564913ecc1f78e5052dbc07eb65b3f048846e3","modified":1712752326556},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_compare_gpt3_2.png","hash":"89172dda5ac569780ea47e85bc43eaef1d6918ae","modified":1712848396056},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_model.png","hash":"63c95ea5ae77528f7d94a94b21cf12ed63e0bfdb","modified":1712849528728},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_distill.png","hash":"be5cae81fafbda6b638ac185421bd04e00b7a60d","modified":1712978653926},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_pretrain_result.png","hash":"880f70f371b8392e3021bf56d282be5640c232f7","modified":1712135713134},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_scaling_dense.png","hash":"b8de8c427de51d62e69915da7a97bf4a9b505317","modified":1712976996880},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_scaling_time.png","hash":"b56568665d2680cc0723269ae39dc4b60de1b01c","modified":1712976813025},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/realformer.png","hash":"3bc805db3177c7e6521362b063543941da8d2bd3","modified":1711206089667},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/21.png","hash":"fb2577b5fa73b06b786484b3723f7aa3819638a0","modified":1710643325115},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_sft.png","hash":"8bc26fcfef1aab448d0db0b28666852f62961a3b","modified":1712821943440},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_perf.png","hash":"01fb9d4f232e9f0b86abb91dbe5d8fb9fac456ce","modified":1712933127383},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_algo_1.png","hash":"aa8bcd982c78e4304c63f81e44b20519dc04f18f","modified":1712070648688},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_result.png","hash":"32ce9bba1b65ceb2e69c079e113d8b4c524bc479","modified":1712067349025},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_diff_expert_capacity.png","hash":"23f2234b45a2a05ff8b098e68a361a71f46816e9","modified":1712133650991},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_hierarchical_gating.png","hash":"067160c735e9c0b8cff777df60d52dfca21ea783","modified":1712045417559},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_distill_diff_model.png","hash":"05c4d885f5563e3dbd56c055a52df5c1531677a7","modified":1712978973713},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_rope_ext.png","hash":"43e1131d146b8104e6936d73aca51d8ace4aff61","modified":1709206562025},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/cover.png","hash":"0493fd58fd2dad33394399d960924dbff6b386b1","modified":1711459395465},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/20.png","hash":"5d42628c8dac91c9671a58535b730e91966c0cbc","modified":1710643321378},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_comparison.png","hash":"20c7e90a390604d2146b4984678524046ad941b8","modified":1712803733203},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_model.png","hash":"5179df7e42bb7dc49fb6f92f4ec68ed820aeaae2","modified":1712068764148},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_scaling_step.png","hash":"dbfa55ab1c94e266344315fd215f2d646eaefda1","modified":1712976642477},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/lihongyi_self_attention.png","hash":"39db6256143fd9a494e848240a8daa434aaddea5","modified":1709965340148},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_145b.png","hash":"502a377d8625d78d2e3ba7281bfb11732c14a61c","modified":1712822142201},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_upper_bound_2b.png","hash":"4338539fe123371c5512c730fc8a35864236323b","modified":1712803847753},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_capacity_effect.png","hash":"16268929d0ec965d3771fccbb595b75d82f05912","modified":1712134554223},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_sft_result.png","hash":"0dd7cfaac788c5d112dd10fe98f0052b369420bf","modified":1712978414761},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/vanilla_moe_result.png","hash":"e491e44cfff384422f3d9cf87cd5e52fd976aed7","modified":1711963546083},{"_id":"source/images/avatar/shadow.png","hash":"da67a14e556fb3223c1c4ec8b9f33eb52c612f50","modified":1706779539112},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/ict.png","hash":"2445ecbbf5ec6a96695f21c03a5fcbf67640b9f0","modified":1711615224209},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_ics.png","hash":"f92751ea20430f25caa3d6bb892c5894bf7509d6","modified":1711115173958},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/17.png","hash":"8ea1c5d90f3da5c469eb17aea50f377cb9c28ba0","modified":1710643306201},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/18.png","hash":"fe0a8e7005110abca19bc7ae506f3e35042b70ec","modified":1710643312126},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/26.png","hash":"f74d03dae65109740f48924f30b52a742b5e4273","modified":1710643346427},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/pretrain_data_pipeline.png","hash":"91218a2272eab9284904c91bacd8d8a40e3c1580","modified":1711462239524},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/10.png","hash":"2c52d4f90dd9356eeb4c9a39f1df1038ccec4693","modified":1710643262247},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/3.png","hash":"4c2c2a30d9ac8db03bab56da5d16ce2042ef73bc","modified":1710643220743},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/6.png","hash":"4b32a49bfead98f5238871b81076176e38168333","modified":1710643241274},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/1.png","hash":"a8898b3f3b7c64fabc5fad9bf8ef5524501d2aeb","modified":1710643881936},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/19.png","hash":"1d7b929e709657c9b7d7ca4da8eadc8c4ca4b3ca","modified":1710643317015},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/24.png","hash":"b89b0a0ca774a4efc1ece628fb20379b5f6a0b69","modified":1710643338042},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Scaled-dot-product-self-attention.png","hash":"983eae2b767df413ef3211ddaf31f1b833d7c86f","modified":1709986303475},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/8.png","hash":"92b0e3b75ce97bbaf4aa69e484216702293589ee","modified":1710643252580},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/27.png","hash":"557c04a2134b6ae147e076cdb80de1730e937d9b","modified":1710643350772},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/9.png","hash":"2a0fb56563b13411035ed41a3ad882f66f948b26","modified":1710643257459},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/11.png","hash":"9b964f3aa6f82a09eb2f2f944508bf0a9d29efb3","modified":1710643267156},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/22.png","hash":"3b27321ef8d76844f6720e1a27d65d1946d48ea7","modified":1710643330423},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/23.png","hash":"9a9af6308620b59f2ee00a3d0da4e942d953e406","modified":1710643334116},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/25.png","hash":"aa259b58be90eed6af0c4ba800a991b9464453d0","modified":1710643342514},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/15.png","hash":"15abcbcf7340941e98dac7a0ab42d922e7fea1b4","modified":1710643288058},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/2.png","hash":"768421239ad7c838dd86714fd9f17b3c73cbb887","modified":1710643214870},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/4.png","hash":"4dacbfb89079d528da1208773961e1366debde9b","modified":1710643230331},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/12.png","hash":"c6c493b14e0a1cc4863a912c4ccc998de194bfc0","modified":1710643275198},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/14.png","hash":"71f75960246f7528b3b83b84f8f91775f9e2fb45","modified":1710643284450},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/16.png","hash":"ba7b2bc65e10389cf9a87ddef69f462e806304f5","modified":1710643293307},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/MQA.png","hash":"7e3f3037311be60e79a7b5388338febc9f3b6d7c","modified":1709986434286},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/7.png","hash":"95640525b0706d3118eeb88c6c4c6217a96c39d0","modified":1710643246812},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Markdown _  Nice.html","hash":"c905c942579a520c7b3c788a00cdb9ae359d4a32","modified":1709897264700},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/13.png","hash":"68b7da3e3074e4d6995eaf96c7d8cf622eadffb7","modified":1710643279584},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/5.png","hash":"7ae786b309a757c5f61a713c7ceef4d2824b024e","modified":1710643235878},{"_id":"source/images/background/wallhaven-p97q73.png","hash":"e83875b9cd1dcfa87f0e76dc1a327cbe2921c5da","modified":1707045618160},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/ms_invest_mistral.png","hash":"faf324c0b57843516a0b256750e6475ec0c2ce93","modified":1710316114714},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ellipse_1.png","hash":"ef2470f6bf1511dc9aac9f1c6489b9d2ffdcb45f","modified":1711006814272},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/norm_in_nlp.png","hash":"7be79b0e55d7d00ff6c16c247d0e506771453380","modified":1712575607757},{"_id":"source/images/background/wallhaven-gpxpg3.png","hash":"d798eaa0d679ed8a14022b0741098711c3402ca8","modified":1707045245660},{"_id":"source/images/background/wallhaven-x636oz.png","hash":"d419359164da44691e2bc8641256ca7c8111e227","modified":1706875075740},{"_id":"public/baidusitemap.xml","hash":"d7e82c24faf2eacd12a4fe5b97ebf15c6660ab65","modified":1713605339409},{"_id":"public/search.xml","hash":"292935e62566bbb51b928a3140ae1d8334cd75e3","modified":1713605339409},{"_id":"public/sitemap.xml","hash":"ad36e756eb3ebfd360ec34fdfc8a38c8c0751d32","modified":1713605339409},{"_id":"public/sitemap.txt","hash":"31bcb834de2e23fee94a870aa18ee88ec55e3590","modified":1713605339409},{"_id":"public/tags/index.html","hash":"f996857fc676d3b3787c891e9f8c25c00e0264f7","modified":1713605339409},{"_id":"public/categories/index.html","hash":"ddda03deb184cb08fcac6d03777f665c35b50875","modified":1713605339409},{"_id":"public/about/index.html","hash":"3428f3b2207950173516eaf6a4455dda136ecd78","modified":1713605339409},{"_id":"public/1736008.html","hash":"eb8c16361d5d8b43790edd6cb7bb2e60edf7056f","modified":1713605339409},{"_id":"public/b70b4a2d.html","hash":"7a75c0b025e58f4042c82426b8599fba9492d13c","modified":1713605339409},{"_id":"public/812c93f3.html","hash":"14be21ee9dba8644b19bdaf8c034309c37510499","modified":1713605339409},{"_id":"public/44e38c1b.html","hash":"43c828aac7f7200d219aa44045a3dffb64abbddd","modified":1713605339409},{"_id":"public/41b6a819.html","hash":"2e4a5dfbed61b6a472d9fcf5c6637befec6250c9","modified":1713605339409},{"_id":"public/ad0bba9d.html","hash":"82e5c5fc5400f1915bb8b9c8b98c46738ff5c441","modified":1713605339409},{"_id":"public/3345028a.html","hash":"3de29b016a37489cb83fd23c57089fca133271de","modified":1713605339409},{"_id":"public/6a40bfa5.html","hash":"e14b4d80e9cb8432308f6a149360a117a9faa97d","modified":1713605339409},{"_id":"public/c61d17e3.html","hash":"d94c6e70b3688075878ebd42e6da5dea181b4ec8","modified":1713605339409},{"_id":"public/3dc22f96.html","hash":"f225ef70e7a7420021c671a8c78765f5790dcea6","modified":1713605339409},{"_id":"public/c4da56c0.html","hash":"64ee224775d60d336751d283d7882761e4605fc7","modified":1713605339409},{"_id":"public/a051710f.html","hash":"4a3ff3650f56c44248cad7f014ec907cec9e2144","modified":1713605339409},{"_id":"public/14e576c.html","hash":"5138e302a51908db25f3a2a012446ecf7b3e41bd","modified":1713605339409},{"_id":"public/archives/index.html","hash":"fbdc7601617e78c45407f4197bb0467643b76a38","modified":1713605339409},{"_id":"public/archives/page/2/index.html","hash":"b976b18ce4c3871b79fee68210a83ffb14f607f1","modified":1713605339409},{"_id":"public/archives/2023/index.html","hash":"7331ca2d9664554eebd79c67910bfdfe563b4fd4","modified":1713605339409},{"_id":"public/archives/2023/03/index.html","hash":"a15a71b818c1c96fdef562917f7614dd8dc7290f","modified":1713605339409},{"_id":"public/archives/2024/index.html","hash":"02dfbac405f65153b126f26ce9f7737859f6d6ca","modified":1713605339409},{"_id":"public/archives/2024/page/2/index.html","hash":"8c0c686ffd5a68f595a7cc45c5835904527d99b9","modified":1713605339409},{"_id":"public/archives/2024/02/index.html","hash":"ea4335fa18af9712a082dca56e0f8c7d434e17ad","modified":1713605339409},{"_id":"public/archives/2024/03/index.html","hash":"e3d2f26927ea82d681c274c0f8c3d9146e255fac","modified":1713605339409},{"_id":"public/archives/2024/04/index.html","hash":"2c2e04edc2a11d7e89dbff4b3e1722e2f726e96d","modified":1713605339409},{"_id":"public/categories/CS/index.html","hash":"32a9b326f96520ed084422ef9429e30b98c371d7","modified":1713605339409},{"_id":"public/categories/CS/page/2/index.html","hash":"c1adabe34e44f4b8522eaad3cb8acd8d739a18e6","modified":1713605339409},{"_id":"public/categories/CS/NLP/index.html","hash":"48038fd29ac3af86550d9cb41d4d287884761d92","modified":1713605339409},{"_id":"public/categories/CS/NLP/page/2/index.html","hash":"402b9d545dd09f5f0e5cb3f64f091813cf4f52c3","modified":1713605339409},{"_id":"public/categories/CS/NLP/LLM/index.html","hash":"d232d280bd23f60ddf61206c2910f718d6fd0d81","modified":1713605339409},{"_id":"public/categories/CS/NLP/LLM/page/2/index.html","hash":"3d3068bb7f5f424bc58c4f8d6d7a07425ae57212","modified":1713605339409},{"_id":"public/index.html","hash":"52f1aef221f9442a0369f1246d7b0ef5ea400eeb","modified":1713605339409},{"_id":"public/tags/NLP/index.html","hash":"6e8cbf8b72fe0783fc254bdc84c9abef7aac6341","modified":1713605339409},{"_id":"public/tags/NLP/page/2/index.html","hash":"ca78158542cb4444580fd683733c0d364d730f08","modified":1713605339409},{"_id":"public/tags/LLM/index.html","hash":"f1769f703d06a3e50e28419068b5027606fec379","modified":1713605339409},{"_id":"public/tags/LLM/page/2/index.html","hash":"4ce9d7a847fe46596eb94931eb1c8eb02ac49d48","modified":1713605339409},{"_id":"public/tags/transformer/index.html","hash":"f5b1079b6ebe277e034709ed2fc644a125705612","modified":1713605339409},{"_id":"public/tags/positional-encoding/index.html","hash":"ebcbf68009707c75338bbec40305b46e59bf1058","modified":1713605339409},{"_id":"public/tags/RoPE/index.html","hash":"7b479448dc88942c7c830a45e6fab04a38a8a925","modified":1713605339409},{"_id":"public/tags//index.html","hash":"27899e5601919f3874fd1ff88dbce5a73d6877dd","modified":1713605339409},{"_id":"public/tags//index.html","hash":"b7d6b55bb72e136c82d9947328316ccf4628a86d","modified":1713605339409},{"_id":"public/tags/layernorm/index.html","hash":"aff48392c85be7e597ff60400d7b518bd5f19a6a","modified":1713605339409},{"_id":"public/tags/normalization/index.html","hash":"45966d7d9cfcbc2386eba051cfb4dcda8f3a2daa","modified":1713605339409},{"_id":"public/tags/batchnorm/index.html","hash":"cfb762d981f8fc9f78d1d69d0414185636bade5b","modified":1713605339409},{"_id":"public/tags//index.html","hash":"944fd8d3cbe2d18519a795e1738def998e06967a","modified":1713605339409},{"_id":"public/tags/attention/index.html","hash":"4a571270c855f9f29f45546b769a057d2ed730d7","modified":1713605339409},{"_id":"public/tags//index.html","hash":"cc8327a2eee15109765105a469570be4fd16cde2","modified":1713605339409},{"_id":"public/tags/sliding-window-attention/index.html","hash":"4eae7a4264851dafe9a97a29401e9cd3cba80bd3","modified":1713605339409},{"_id":"public/tags/sparse-attention/index.html","hash":"678601a25e46805b9293ddf7fb68a5e72e78ec6e","modified":1713605339409},{"_id":"public/tags//index.html","hash":"0fa479a26583927405c7a3f9c4d90556968988f5","modified":1713605339409},{"_id":"public/tags//index.html","hash":"19c9f6a04df14d44fbd2df500a682c33a36f5638","modified":1713605339409},{"_id":"public/tags/post-norm/index.html","hash":"a2032b1e48a770f792fbabe9e83b69cf0f0f5d94","modified":1713605339409},{"_id":"public/tags/pre-norm/index.html","hash":"b41fe6ee5e2d8b18c6827904fc3a33963c93aee7","modified":1713605339409},{"_id":"public/tags/ChatGPT/index.html","hash":"bad547f09ea0ca9012604be0b5d8bfae4f38700c","modified":1713605339409},{"_id":"public/tags/Sparrow/index.html","hash":"c2dfd67732b1a5d88ed31d598bbe3058c9b340a7","modified":1713605339409},{"_id":"public/tags/LaMDA/index.html","hash":"cbeeadc52f8c3f7c7e8df96dcc57be798b5c8fd7","modified":1713605339409},{"_id":"public/tags/GopherCite/index.html","hash":"66270b413d60b0f27a070bcf4351ac452303e86c","modified":1713605339409},{"_id":"public/tags/WebGPT/index.html","hash":"a9dad2ee87d5a28b90db4025ce141d12fe1017e9","modified":1713605339409},{"_id":"public/tags/InstructGPT/index.html","hash":"effc6aeabe2fe05a882f20ed1cd407c097ee3952","modified":1713605339409},{"_id":"public/tags/MoE/index.html","hash":"7ad3e7f4937305739a7d9339ef86beef83d96177","modified":1713605339409},{"_id":"public/tags/KV-Cache/index.html","hash":"d06c2e1f4e53b5a421115b115a02ee694b0a37d7","modified":1713605339409},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1713605339409},{"_id":"public/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1713605339409},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1713605339409},{"_id":"public/CNAME","hash":"07ad8fed7f1b810bf3715a0e68324e5cbb1d47ac","modified":1713605339409},{"_id":"public/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1713605339409},{"_id":"public/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1713605339409},{"_id":"public/images/qrcode.jpg","hash":"eab6c8893d0c79cf705cf64d738f560563365b0c","modified":1713605339409},{"_id":"public/images/avatar/20180303210737_XsJVr.jpeg","hash":"2ebbbcead4446a69b0ac0698ab227a1af49735ce","modified":1713605339409},{"_id":"public/images/cover.png","hash":"2f2aa6173619dd38425673ba110b50b9156d4d10","modified":1713605339409},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1713605339409},{"_id":"public/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","hash":"3e7db51e59a8261de04f55814475556494d153f6","modified":1713605339409},{"_id":"public/images/favicon/favicon_io/about.txt","hash":"d1f084751c81e98b1675b8c0745f6f306a1eaf66","modified":1713605339409},{"_id":"public/images/favicon/favicon_io/android-chrome-512x512.png","hash":"419f0b0317ad2a31acfa7dc19e1502731e92e05f","modified":1713605339409},{"_id":"public/images/favicon/favicon_io/apple-touch-icon.png","hash":"5e36b0bfe67bcf96f4f0e92366a5de509fe9a78a","modified":1713605339409},{"_id":"public/images/favicon/favicon_io/favicon-16x16.png","hash":"fb63ee2fc88b9ce42dec5bf68088da14be9350f5","modified":1713605339409},{"_id":"public/images/favicon/favicon_io/favicon.ico","hash":"fceaa7158ae89a98fe5b93152dd7dec2d36c1ff4","modified":1713605339409},{"_id":"public/images/favicon/favicon_io/android-chrome-192x192.png","hash":"2164889376f2bd6f9dd2066ebdfe4abd547f49d9","modified":1713605339409},{"_id":"public/images/favicon/favicon_io/favicon-32x32.png","hash":"f42c950146a171afca4ad70ff425b5b4ff02f926","modified":1713605339409},{"_id":"public/images/favicon/favicon_io/site.webmanifest","hash":"bf31baf91bdd2fcde24a45e3f2a1be33733c6f69","modified":1713605339409},{"_id":"public/c4da56c0/mix_precision_fp16.png","hash":"4da22fa80039f8271ffd839aba70f6b1aca7afe4","modified":1713605339409},{"_id":"public/a051710f/complex_number.png","hash":"255aa16a44913d53784b09d0a08762df6d21dc16","modified":1713605339409},{"_id":"public/b70b4a2d/bn_and_ln.png","hash":"ff10e75b9d2c95aa713f013b104ed307f2121e57","modified":1713605339409},{"_id":"public/c61d17e3/dilated_conv.png","hash":"bbc2ff2e9f891da4bfaf6d535ab8545acc18e8a6","modified":1713605339409},{"_id":"public/c61d17e3/mistral_large_performance.jpeg","hash":"54e3ed874802ac9465580d6b5fcc5d6c1de96244","modified":1713605339409},{"_id":"public/c61d17e3/receptive_field_cnn.png","hash":"46515aa3bce1eb0fc244f62ceee7b899c28183e8","modified":1713605339409},{"_id":"public/6a40bfa5/bn_and_ln.png","hash":"ff10e75b9d2c95aa713f013b104ed307f2121e57","modified":1713605339409},{"_id":"public/6a40bfa5/bn_ln_gn_in.png","hash":"9783c818f5e0eaea33d169718476bbe8874cf945","modified":1713605339409},{"_id":"public/6a40bfa5/lossfunc_surface.jpeg","hash":"c78a8df335da0d507963fb73a62fe2c3d145c91f","modified":1713605339409},{"_id":"public/6a40bfa5/realformer_attention.png","hash":"e9a92e5c07c8ca6873ea70671ad54eb2f1a13332","modified":1713605339409},{"_id":"public/6a40bfa5/rmsnorm.png","hash":"55bbcb42145011f7b5adf90cc613e22e2b94f060","modified":1713605339409},{"_id":"public/3dc22f96/MQA.webp","hash":"456a8ab19cc1564912034c375e8c3c5a42be6837","modified":1713605339409},{"_id":"public/3dc22f96/cnn_heatmap.png","hash":"cb0bde73c9c4d0646133947ebaab16c44c753667","modified":1713605339409},{"_id":"public/3dc22f96/decoder.png","hash":"28ee3d1ab68bd325ecb9d2066bc264a63d7de081","modified":1713605339409},{"_id":"public/3dc22f96/encoder.png","hash":"d6a3a39c420d90e50f02f8b34f127bfe34177331","modified":1713605339409},{"_id":"public/3dc22f96/attention_calculation.png","hash":"1f020c39c78221e41c5c6953ef97239e9f42aa3c","modified":1713605339409},{"_id":"public/3dc22f96/seq2seq.png","hash":"9baa57cc8000a918d0adca6dceaac3ea54791ea8","modified":1713605339409},{"_id":"public/3dc22f96/seq2seq_attention.png","hash":"b95046eee028b45dd9734639ecde8189e93b2374","modified":1713605339409},{"_id":"public/3dc22f96/softmax.png","hash":"de80ba20e55abf7457cac958aa87627d0a7e5d77","modified":1713605339409},{"_id":"public/44e38c1b/softplus.png","hash":"bdc66c39227441390f2241b4f26c0b1fbab331d9","modified":1713605339409},{"_id":"public/44e38c1b/xiaomi_moe.jpg","hash":"d898ba33f1ee70efa136dbff3cb38983b461524f","modified":1713605339409},{"_id":"public/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","hash":"6a3863489ed0982c7fe19d575eae28ba8c286ccf","modified":1713605339409},{"_id":"public/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","hash":"0629f15db217c8b7037a5b46ef187d79b9a66a09","modified":1713605339409},{"_id":"public/c4da56c0/meta_pi_nosft.png","hash":"d3b46ed19e458973c3586f3bf2ef906deb4080a8","modified":1713605339409},{"_id":"public/c4da56c0/rope_matrix.png","hash":"84372b91204098bab2ff09d2d865ad869f93bdbd","modified":1713605339409},{"_id":"public/a051710f/remote_attenuation.png","hash":"90aa3d8f76c9b5c18de8b7740649b499241cab0c","modified":1713605339409},{"_id":"public/b70b4a2d/cv_layernorm.jpeg","hash":"f0874ecc4b9d8da8bf3bff0e13a6313ed19a7b15","modified":1713605339409},{"_id":"public/41b6a819/model.png","hash":"631efc6d4e92da128d7a10a4dc6af307ee4ddcbf","modified":1713605339409},{"_id":"public/c61d17e3/big_bird_attention.png","hash":"ed6c76b9bb77b98d34c429333498d04dac8e3ed9","modified":1713605339409},{"_id":"public/41b6a819/third_party.png","hash":"673fe2b2cad3b1f40c0fcfd190c0034d8dbe7f31","modified":1713605339409},{"_id":"public/c61d17e3/mistral_architechture.png","hash":"5e4c347dc41d7f070f54b386fdccf675cfeb8f10","modified":1713605339409},{"_id":"public/6a40bfa5/ellipse_2.png","hash":"ee20ecce8c3470d17b1a4bd43df811d16269ffd3","modified":1713605339409},{"_id":"public/6a40bfa5/postnorm_prenorm.png","hash":"d8830735e89c73ca416baabf4a195d7891d9f0ed","modified":1713605339409},{"_id":"public/6a40bfa5/prmsnorm.png","hash":"d5826342f665f4cb04fdbb2e3d83e0b2607355c9","modified":1713605339409},{"_id":"public/6a40bfa5/sigmoid.png","hash":"f1ced5f06861a2e0296050aff17eebfe3d023a6f","modified":1713605339409},{"_id":"public/3dc22f96/llama2_qga.png","hash":"5e0dea7d03de9144eb524a0a9adb102e91b52aaa","modified":1713605339409},{"_id":"public/3dc22f96/multihead_attention.png","hash":"6f8ee285f2646dc163b6b3164a0639aa9ddd7f27","modified":1713605339409},{"_id":"public/3dc22f96/sram_dram.png","hash":"ae7a9296b67c02608460a334fbbad3781b890302","modified":1713605339409},{"_id":"public/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1713605339409},{"_id":"public/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1713605339409},{"_id":"public/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1713605339409},{"_id":"public/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1713605339409},{"_id":"public/js/motion.js","hash":"770d63c26f22705311028a36b52e999cc8a2da82","modified":1713605339409},{"_id":"public/js/next-boot.js","hash":"745bd828205da7e5fbd3f860cc3697097630f5f3","modified":1713605339409},{"_id":"public/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1713605339409},{"_id":"public/js/pjax.js","hash":"b03ba78c6916ad2f390d55bc1bc18fafb64b0ebf","modified":1713605339409},{"_id":"public/js/utils.js","hash":"d775148c2bf20c028622af5609a788167352bf1e","modified":1713605339409},{"_id":"public/css/main.css","hash":"6aea217c0462e6970606601a9fe39183cf15614c","modified":1713605339409},{"_id":"public/css/noscript.css","hash":"4cd5301e478e0e0d4b176740ec314087ec5cb707","modified":1713605339409},{"_id":"public/js/third-party/addtoany.js","hash":"5276c8f78ee562a8965216dc67d762e59cb4a9f2","modified":1713605339409},{"_id":"public/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1713605339409},{"_id":"public/js/schemes/muse.js","hash":"ba7ba2c129d1f240c6a22cec3e53f3f22af64b6b","modified":1713605339409},{"_id":"public/js/third-party/fancybox.js","hash":"819f382c561fe5ec23c67cc5fabd63dd1cc22dc1","modified":1713605339409},{"_id":"public/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1713605339409},{"_id":"public/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1713605339409},{"_id":"public/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1713605339409},{"_id":"public/js/third-party/analytics/google-analytics.js","hash":"def07bcc7c17d8a0caad177fb1dd2f3a5e5b3536","modified":1713605339409},{"_id":"public/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1713605339409},{"_id":"public/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1713605339409},{"_id":"public/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1713605339409},{"_id":"public/js/third-party/comments/disqus.js","hash":"da361917d65e5dca8362f8cdeb6c8cc0e8316cec","modified":1713605339409},{"_id":"public/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1713605339409},{"_id":"public/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1713605339409},{"_id":"public/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1713605339409},{"_id":"public/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1713605339409},{"_id":"public/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1713605339409},{"_id":"public/js/third-party/search/algolia-search.js","hash":"fdb7b7cef1a147d897e7f7cd8903b58368ec2062","modified":1713605339409},{"_id":"public/js/third-party/search/local-search.js","hash":"4536cb6d0a9bbaaa86fab3fa0101f6a3a3ec5a76","modified":1713605339409},{"_id":"public/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1713605339409},{"_id":"public/js/third-party/statistics/firestore.js","hash":"6e0682bb42170d61b13b786295f45f9c785f8b73","modified":1713605339409},{"_id":"public/js/third-party/statistics/lean-analytics.js","hash":"835cbf54c49ef1327f47df70ff2636ad36b6f57d","modified":1713605339409},{"_id":"public/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1713605339409},{"_id":"public/js/third-party/tags/mermaid.js","hash":"6bf821310342c5b87a631873e7650a475a0765f1","modified":1713605339409},{"_id":"public/js/third-party/tags/wavedrom.js","hash":"40dcd10df6edf124088c329346e0cc0bdac74ef1","modified":1713605339409},{"_id":"public/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1713605339409},{"_id":"public/js/third-party/comments/disqusjs.js","hash":"1e826dea3f684c0515f362dc1352447a1f0eae71","modified":1713605339409},{"_id":"public/44e38c1b/modular_connectionist.png","hash":"b5865cf34faba075b4f2316c2cae0559dac2d883","modified":1713605339409},{"_id":"public/44e38c1b/rnn_moe_load_function.png","hash":"644684f21f85d565328d98334d41bbe019acbbfc","modified":1713605339409},{"_id":"public/images/avatar/Picasso_Elephant.png","hash":"6d38d4b24c13a3fd506ee37a562126b76a24de83","modified":1713605339409},{"_id":"public/a051710f/rope.png","hash":"5535c87dbcfee3ed646db93662a0b6a1ccd3b31b","modified":1713605339409},{"_id":"public/b70b4a2d/cv_batchnorm.png","hash":"d9e8d897c36125fddcf1cbcfa5c237a37158a939","modified":1713605339409},{"_id":"public/41b6a819/9B.png","hash":"7de19972e48b1f43c501d60a1c43a28c46b198e8","modified":1713605339409},{"_id":"public/41b6a819/long_context_result.png","hash":"daea6f734d64bdf5d24c6e17a640f23b1bd35b5f","modified":1713605339409},{"_id":"public/c61d17e3/longformer_attention.png","hash":"64860379955872ecac5835b3f9d8c6d130c7e485","modified":1713605339409},{"_id":"public/c61d17e3/rolling_buffer.png","hash":"34d4db9f4855926db561faa80e934dd971c0974e","modified":1713605339409},{"_id":"public/6a40bfa5/bn_algo.png","hash":"56f1ab55c0e94814e6e37c30421012ed82098d62","modified":1713605339409},{"_id":"public/6a40bfa5/deepnorm.png","hash":"4726d8a40d1d0db397005408295e1ba54809a7e4","modified":1713605339409},{"_id":"public/6a40bfa5/deepnorm_result.png","hash":"138cafc159f1e2e02455c540b4754f7cbb7f521d","modified":1713605339409},{"_id":"public/6a40bfa5/ics_define.png","hash":"6bf3240ef78bad2cf76897a29c05428f4c195fba","modified":1713605339409},{"_id":"public/3dc22f96/mqa_result_1.png","hash":"a052b57f71eb78e3158ed2ee06ff0e5597607a2f","modified":1713605339409},{"_id":"public/44e38c1b/ds_moe_expert_specialization.png","hash":"64947872486dd083a7076bfdfe67cb0626208579","modified":1713605339409},{"_id":"public/44e38c1b/ds_moe_less_activated_expert.png","hash":"ac7ad86dabe94564bdb17399231c6ddc7da83962","modified":1713605339409},{"_id":"public/44e38c1b/glam_compare_gpt3.png","hash":"311b21079599473054378e339885e0b87719e63e","modified":1713605339409},{"_id":"public/44e38c1b/rnn_moe_137b.png","hash":"fc11cfc87b2994956a9adbee76621fe5d964dc30","modified":1713605339409},{"_id":"public/44e38c1b/rnn_moe.png","hash":"2a2ee095b8cc0727daa2cdd0c63891e4a470eae8","modified":1713605339409},{"_id":"public/44e38c1b/vanilla_moe.png","hash":"7da0a6e9d529256107b5f6b287737ac47513a797","modified":1713605339409},{"_id":"public/1736008/transformer.png","hash":"9dddf171ca51f2ed1218baa9b84f4b98e9b911cf","modified":1713605339409},{"_id":"public/41b6a819/base_model_eval.png","hash":"9b4e65d246865683f8e3348d74bfad03c937b65f","modified":1713605339409},{"_id":"public/41b6a819/perf.png","hash":"3c068a423dcd32cac7f1630bd69fbe5a4c6789af","modified":1713605339409},{"_id":"public/41b6a819/sft.png","hash":"ea9aea143af836012f44d21956ab5455487e9bfb","modified":1713605339409},{"_id":"public/c61d17e3/prefill_and_chunking.png","hash":"0c706e0728ea462b2b00c59a97c79ccf5f05b598","modified":1713605339409},{"_id":"public/6a40bfa5/bs_bn.png","hash":"aa28241d75f914603b9f7f67cc54db4e61bac668","modified":1713605339409},{"_id":"public/6a40bfa5/rmsnorm_eff.png","hash":"350a7a2703eef1ee9357609ae5820bfc30835681","modified":1713605339409},{"_id":"public/3dc22f96/GQA.png","hash":"d6b0cd20ae5f68a96f0865b3d0b6b129969caa4b","modified":1713605339409},{"_id":"public/3dc22f96/GQA_result_1.png","hash":"87f2c3632fdf83828e8bd07a95cd8e7bf277fc88","modified":1713605339409},{"_id":"public/3dc22f96/Scaled-dot-product-self-attention.pbm","hash":"03da711b1547c944deea60d9bf345eb30e7c566f","modified":1713605339409},{"_id":"public/3dc22f96/gpu_cache.png","hash":"edb6b1abdecd3099f2d68c2a729c0ca9b1fb0db7","modified":1713605339409},{"_id":"public/3dc22f96/transformer_structure.png","hash":"87f0258e43922eface0277e13167a4ba8c1402bd","modified":1713605339409},{"_id":"public/44e38c1b/ds_moe_ablation.png","hash":"108dc8d66ae0e370e7969403efd85178b9a8523a","modified":1713605339409},{"_id":"public/3dc22f96/Markdown _  Nice.html","hash":"c905c942579a520c7b3c788a00cdb9ae359d4a32","modified":1713605339409},{"_id":"public/44e38c1b/glam_family.png","hash":"9d813f12f82d8702886f7ede72c5a25151390ba9","modified":1713605339409},{"_id":"public/44e38c1b/gshard_moe_family.png","hash":"21a6c80f1dac39eb364fb417ed83afde2b212675","modified":1713605339409},{"_id":"public/44e38c1b/switch_transformer_distill_sft.png","hash":"7a59660f367634d68aa392698042f3d9cfe190da","modified":1713605339409},{"_id":"public/44e38c1b/switch_transformer_init.png","hash":"f9fb36a0defac7a5990b5c58a614f3165af0ae3e","modified":1713605339409},{"_id":"public/44e38c1b/switch_transformer_dropout.png","hash":"63f36ca61aaa71e88ddf23339e63a9ccd898a6ce","modified":1713605339409},{"_id":"public/images/background/wallhaven-2ywymm.png","hash":"aeaf089b64472a1c72cdd9778c5deac4086eb38f","modified":1713605339409},{"_id":"public/c4da56c0/meta_pi_explanation.png","hash":"df22d646e5a47e6223b7722007cfb7dde7c27280","modified":1713605339409},{"_id":"public/41b6a819/eval.png","hash":"e78d1d820de4c455b9301124d6016a19762eae1f","modified":1713605339409},{"_id":"public/41b6a819/pretrain_data_dist.png","hash":"8c66a625723cb87ee67a9ff60d3614b369f50592","modified":1713605339409},{"_id":"public/41b6a819/multimodal.png","hash":"b14a4eb4d377101acf7b50904b9ee0f1d473aacc","modified":1713605339409},{"_id":"public/6a40bfa5/warmup_effect.png","hash":"3e936786065e1ab9cbad17f5b86a5b8129720270","modified":1713605339409},{"_id":"public/44e38c1b/rnn_moe_perf.png","hash":"ca3c95d4b1c1c8f986d7adcacbf04da444e91610","modified":1713605339409},{"_id":"public/44e38c1b/rnn_moe_specilized.png","hash":"138ad5a388c77cc02202de794ec4cb734d633065","modified":1713605339409},{"_id":"public/44e38c1b/switch_transformer_structure.png","hash":"d906a9148e035025683e8da1eee5fa3d87164aa5","modified":1713605339409},{"_id":"public/c4da56c0/meta_pi.png","hash":"193df6c392d71dd4b90202d9be97f000f37b68ec","modified":1713605339409},{"_id":"public/c61d17e3/mistral_perf.png","hash":"c9d7ce0a301920c4e722e341200f311995923735","modified":1713605339409},{"_id":"public/c61d17e3/mistral_swa.png","hash":"59037b91ba8f256fd89b3d60b8ce477e4c8f4b3a","modified":1713605339409},{"_id":"public/14e576c/28.png","hash":"d438e857378575809c880b78ca715dc69e50b364","modified":1713605339409},{"_id":"public/6a40bfa5/ics_measure.png","hash":"e9fe87cfea7dcef7cb66e1d76c17d883cbbc3cbd","modified":1713605339409},{"_id":"public/3dc22f96/mqa_result_3.png","hash":"12e310102ace1f9e89c0e9a352cf4a3462335a60","modified":1713605339409},{"_id":"public/44e38c1b/ds_moe_upper_bound_13b.png","hash":"cec21f0cf3809011ce210dffda35da3671147008","modified":1713605339409},{"_id":"public/44e38c1b/ds_moe_perf.png","hash":"b6ed751d2f171dac971bcf1d7f12e8a2d7fad388","modified":1713605339409},{"_id":"public/44e38c1b/ds_moe_structure.png","hash":"b0564913ecc1f78e5052dbc07eb65b3f048846e3","modified":1713605339409},{"_id":"public/44e38c1b/glam_compare_gpt3_2.png","hash":"89172dda5ac569780ea47e85bc43eaef1d6918ae","modified":1713605339409},{"_id":"public/44e38c1b/glam_model.png","hash":"63c95ea5ae77528f7d94a94b21cf12ed63e0bfdb","modified":1713605339409},{"_id":"public/44e38c1b/switch_transformer_distill.png","hash":"be5cae81fafbda6b638ac185421bd04e00b7a60d","modified":1713605339409},{"_id":"public/44e38c1b/switch_transformer_pretrain_result.png","hash":"880f70f371b8392e3021bf56d282be5640c232f7","modified":1713605339409},{"_id":"public/44e38c1b/switch_transformer_scaling_dense.png","hash":"b8de8c427de51d62e69915da7a97bf4a9b505317","modified":1713605339409},{"_id":"public/44e38c1b/switch_transformer_scaling_time.png","hash":"b56568665d2680cc0723269ae39dc4b60de1b01c","modified":1713605339409},{"_id":"public/14e576c/21.png","hash":"fb2577b5fa73b06b786484b3723f7aa3819638a0","modified":1713605339409},{"_id":"public/6a40bfa5/realformer.png","hash":"3bc805db3177c7e6521362b063543941da8d2bd3","modified":1713605339409},{"_id":"public/44e38c1b/ds_moe_sft.png","hash":"8bc26fcfef1aab448d0db0b28666852f62961a3b","modified":1713605339409},{"_id":"public/44e38c1b/gshard_algo_1.png","hash":"aa8bcd982c78e4304c63f81e44b20519dc04f18f","modified":1713605339409},{"_id":"public/44e38c1b/glam_perf.png","hash":"01fb9d4f232e9f0b86abb91dbe5d8fb9fac456ce","modified":1713605339409},{"_id":"public/44e38c1b/gshard_result.png","hash":"32ce9bba1b65ceb2e69c079e113d8b4c524bc479","modified":1713605339409},{"_id":"public/44e38c1b/rnn_moe_hierarchical_gating.png","hash":"067160c735e9c0b8cff777df60d52dfca21ea783","modified":1713605339409},{"_id":"public/44e38c1b/switch_transformer_diff_expert_capacity.png","hash":"23f2234b45a2a05ff8b098e68a361a71f46816e9","modified":1713605339409},{"_id":"public/44e38c1b/switch_transformer_distill_diff_model.png","hash":"05c4d885f5563e3dbd56c055a52df5c1531677a7","modified":1713605339409},{"_id":"public/c4da56c0/meta_rope_ext.png","hash":"43e1131d146b8104e6936d73aca51d8ace4aff61","modified":1713605339409},{"_id":"public/41b6a819/cover.png","hash":"0493fd58fd2dad33394399d960924dbff6b386b1","modified":1713605339409},{"_id":"public/14e576c/20.png","hash":"5d42628c8dac91c9671a58535b730e91966c0cbc","modified":1713605339409},{"_id":"public/44e38c1b/ds_moe_comparison.png","hash":"20c7e90a390604d2146b4984678524046ad941b8","modified":1713605339409},{"_id":"public/44e38c1b/gshard_model.png","hash":"5179df7e42bb7dc49fb6f92f4ec68ed820aeaae2","modified":1713605339409},{"_id":"public/44e38c1b/switch_transformer_scaling_step.png","hash":"dbfa55ab1c94e266344315fd215f2d646eaefda1","modified":1713605339409},{"_id":"public/3dc22f96/lihongyi_self_attention.png","hash":"39db6256143fd9a494e848240a8daa434aaddea5","modified":1713605339409},{"_id":"public/44e38c1b/ds_moe_145b.png","hash":"502a377d8625d78d2e3ba7281bfb11732c14a61c","modified":1713605339409},{"_id":"public/44e38c1b/ds_moe_upper_bound_2b.png","hash":"4338539fe123371c5512c730fc8a35864236323b","modified":1713605339409},{"_id":"public/44e38c1b/switch_transformer_capacity_effect.png","hash":"16268929d0ec965d3771fccbb595b75d82f05912","modified":1713605339409},{"_id":"public/44e38c1b/switch_transformer_sft_result.png","hash":"0dd7cfaac788c5d112dd10fe98f0052b369420bf","modified":1713605339409},{"_id":"public/44e38c1b/vanilla_moe_result.png","hash":"e491e44cfff384422f3d9cf87cd5e52fd976aed7","modified":1713605339409},{"_id":"public/images/avatar/shadow.png","hash":"da67a14e556fb3223c1c4ec8b9f33eb52c612f50","modified":1713605339409},{"_id":"public/41b6a819/ict.png","hash":"2445ecbbf5ec6a96695f21c03a5fcbf67640b9f0","modified":1713605339409},{"_id":"public/14e576c/18.png","hash":"fe0a8e7005110abca19bc7ae506f3e35042b70ec","modified":1713605339409},{"_id":"public/14e576c/17.png","hash":"8ea1c5d90f3da5c469eb17aea50f377cb9c28ba0","modified":1713605339409},{"_id":"public/14e576c/26.png","hash":"f74d03dae65109740f48924f30b52a742b5e4273","modified":1713605339409},{"_id":"public/6a40bfa5/bn_ics.png","hash":"f92751ea20430f25caa3d6bb892c5894bf7509d6","modified":1713605339409},{"_id":"public/41b6a819/pretrain_data_pipeline.png","hash":"91218a2272eab9284904c91bacd8d8a40e3c1580","modified":1713605339409},{"_id":"public/14e576c/10.png","hash":"2c52d4f90dd9356eeb4c9a39f1df1038ccec4693","modified":1713605339409},{"_id":"public/14e576c/3.png","hash":"4c2c2a30d9ac8db03bab56da5d16ce2042ef73bc","modified":1713605339409},{"_id":"public/14e576c/6.png","hash":"4b32a49bfead98f5238871b81076176e38168333","modified":1713605339409},{"_id":"public/14e576c/1.png","hash":"a8898b3f3b7c64fabc5fad9bf8ef5524501d2aeb","modified":1713605339409},{"_id":"public/14e576c/19.png","hash":"1d7b929e709657c9b7d7ca4da8eadc8c4ca4b3ca","modified":1713605339409},{"_id":"public/14e576c/24.png","hash":"b89b0a0ca774a4efc1ece628fb20379b5f6a0b69","modified":1713605339409},{"_id":"public/3dc22f96/Scaled-dot-product-self-attention.png","hash":"983eae2b767df413ef3211ddaf31f1b833d7c86f","modified":1713605339409},{"_id":"public/14e576c/8.png","hash":"92b0e3b75ce97bbaf4aa69e484216702293589ee","modified":1713605339409},{"_id":"public/14e576c/27.png","hash":"557c04a2134b6ae147e076cdb80de1730e937d9b","modified":1713605339409},{"_id":"public/14e576c/9.png","hash":"2a0fb56563b13411035ed41a3ad882f66f948b26","modified":1713605339409},{"_id":"public/14e576c/11.png","hash":"9b964f3aa6f82a09eb2f2f944508bf0a9d29efb3","modified":1713605339409},{"_id":"public/14e576c/22.png","hash":"3b27321ef8d76844f6720e1a27d65d1946d48ea7","modified":1713605339409},{"_id":"public/14e576c/23.png","hash":"9a9af6308620b59f2ee00a3d0da4e942d953e406","modified":1713605339409},{"_id":"public/14e576c/25.png","hash":"aa259b58be90eed6af0c4ba800a991b9464453d0","modified":1713605339409},{"_id":"public/14e576c/15.png","hash":"15abcbcf7340941e98dac7a0ab42d922e7fea1b4","modified":1713605339409},{"_id":"public/14e576c/2.png","hash":"768421239ad7c838dd86714fd9f17b3c73cbb887","modified":1713605339409},{"_id":"public/14e576c/4.png","hash":"4dacbfb89079d528da1208773961e1366debde9b","modified":1713605339409},{"_id":"public/14e576c/12.png","hash":"c6c493b14e0a1cc4863a912c4ccc998de194bfc0","modified":1713605339409},{"_id":"public/14e576c/14.png","hash":"71f75960246f7528b3b83b84f8f91775f9e2fb45","modified":1713605339409},{"_id":"public/14e576c/16.png","hash":"ba7b2bc65e10389cf9a87ddef69f462e806304f5","modified":1713605339409},{"_id":"public/3dc22f96/MQA.png","hash":"7e3f3037311be60e79a7b5388338febc9f3b6d7c","modified":1713605339409},{"_id":"public/14e576c/7.png","hash":"95640525b0706d3118eeb88c6c4c6217a96c39d0","modified":1713605339409},{"_id":"public/14e576c/13.png","hash":"68b7da3e3074e4d6995eaf96c7d8cf622eadffb7","modified":1713605339409},{"_id":"public/14e576c/5.png","hash":"7ae786b309a757c5f61a713c7ceef4d2824b024e","modified":1713605339409},{"_id":"public/images/background/wallhaven-p97q73.png","hash":"e83875b9cd1dcfa87f0e76dc1a327cbe2921c5da","modified":1713605339409},{"_id":"public/c61d17e3/ms_invest_mistral.png","hash":"faf324c0b57843516a0b256750e6475ec0c2ce93","modified":1713605339409},{"_id":"public/6a40bfa5/ellipse_1.png","hash":"ef2470f6bf1511dc9aac9f1c6489b9d2ffdcb45f","modified":1713605339409},{"_id":"public/b70b4a2d/norm_in_nlp.png","hash":"7be79b0e55d7d00ff6c16c247d0e506771453380","modified":1713605339409},{"_id":"public/images/background/wallhaven-gpxpg3.png","hash":"d798eaa0d679ed8a14022b0741098711c3402ca8","modified":1713605339409},{"_id":"public/images/background/wallhaven-x636oz.png","hash":"d419359164da44691e2bc8641256ca7c8111e227","modified":1713605339409}],"Category":[{"name":"CS","_id":"clv7wdj0k0004794kh75tf46r"},{"name":"NLP","parent":"clv7wdj0k0004794kh75tf46r","_id":"clv7wdj0q000i794k1vl9636s"},{"name":"LLM","parent":"clv7wdj0q000i794k1vl9636s","_id":"clv7wdj0u0016794kbxxmbbg6"}],"Data":[{"_id":"styles","data":".post-toc .nav .nav-child {\n  display: block;\n}\n.post-toc ol {\n  font-size: 13px;\n}\nbody {\n  background: url(\"/images/background/wallhaven-p97q73.png\");\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-size: cover;\n  background-position: 50% 50%;\n}\n:root {\n  --content-bg-color: rgba(32,32,32,0.816);\n}\n"}],"Page":[{"title":"tags","date":"2024-01-31T10:50:02.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2024-01-31 18:50:02\ntype: \"tags\"\ncomments: false\n---\n","updated":"2024-01-31T10:57:24.396Z","path":"tags/index.html","layout":"page","_id":"clv7wdj0f0000794k5tsr2on5","content":"\n","length":0,"excerpt":"","more":"\n"},{"title":"categories","date":"2024-01-31T10:57:57.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2024-01-31 18:57:57\ntype: \"categories\"\ncomments: false\n---\n","updated":"2024-01-31T10:58:37.077Z","path":"categories/index.html","layout":"page","_id":"clv7wdj0j0002794kg9pqfy2h","content":"\n","length":0,"excerpt":"","more":"\n"},{"title":"about","date":"2024-01-31T10:57:44.000Z","type":"about","comments":0,"_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2024-01-31 18:57:44\ntype: \"about\"\ncomments: false\n---\n","updated":"2024-01-31T10:58:21.349Z","path":"about/index.html","layout":"page","_id":"clv7wdj0m0006794k9dtjcmqz","content":"\n","length":0,"excerpt":"","more":"\n"}],"Post":[{"title":"LLM:RoPE","abbrlink":"a051710f","date":"2024-02-21T13:18:13.000Z","mathjax":true,"_content":"\nLLMRoPE\n\n# RoPE\n\nRoPERotary Position Embedding2021TransformerRoPE<big><u>****</u></big>\n\n2023AlibiRoPE20232024RoPEAlibiAlibi\n\nRoPERoPE  \n\n# \n\nRoPE\n\n  \n\nBert256/512token  \n  \n<u>****</u>token-2token-1token-10002token-10001  \n<u>****</u><u>****</u>self-attention<u>****</u>  \n<u>****</u><u>****</u><u>****</u><u>****</u>  \n\n  \n\n3  \n\n## \n\nself-attention  \n\n$$\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}$$  \n\n\n$x_i$  $x_j$  $i$  $j$ $p$   \n\n$p$ $x$  $p$ attentionsoftmaxelement-wise addition\n\n $x + p$   $x * p$ \n\n## \n\n $x$  $p$   \n\n1 $e_1 = x_ + p_1$ 18 $e_8 = x_ + p_8$  $e_1$  $e_8$ <u>****</u>  \n\n15121512=512handle\n\n1 $q_{i}k_{j}^{T}$  \n\n$$\\begin{align*}q_ik_j^\\top&=\\left(x_i+p_i\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{\\color{red}W_K^\\top p_j^\\top}\\right)\\end{align*}\\tag{2}$$  \n\n $p_iW_\\mathbb{Q}$  $W_K^\\top p_j^\\top$   \n\n### Google\n\nGoogleSelf-Attention with Relative Position Representations $p_iW_\\mathbb{Q}$  $j$ $W_K^\\top p_j^\\top$  $i$$j$  $R_{ij}^K$attention<u>**input projection**</u>  \n\n$$\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n$$  \n\n$R_{ij}^K$ clip  \n\n$$\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n$$\n\n $p_\\mathrm{K}$  \n\nclip****tokentoken256>256\n\nGoogleinput\n\n$$\\begin{align*}\no_\\mathrm{i}&=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} + {\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}$$\n\nGoogle $p_{j}W_{\\mathrm{V}}$ \n\n$$\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}$$\n\n$R_{\\mathrm{i,j}}^{\\mathrm{V}}$  $R_{ij}^K$  + clip\n\n### XLNET\n\nXLNETGoogle  \n\n2  \n\n$$\\begin{align*}\nq_ik_j^T\n&= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&= \nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n$$  \n\n\n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n$$    \n\n $p_i$  $u$  $\\nu$  $p_j$  $R_{i-j}^\\top$   \n\n $u$  $\\nu$  $u$  $\\nu$ \n\n$$\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n$$   \n\nXLNET  \n\n$$\\begin{align*}\no_\\mathrm{i}\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}$$\n\nGoogleXLNET $\\mathrm{a_{i,j}}$ 2 $i$  $j$ clip\n\nT5  \n\n### T5\n\n6 $i$  $j$   \n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n$$    \n\nXLNETDeBertaT5\n\n## \n\nattention  \n\n1softmax33\n\n8433  \n\n\n\n\n\nself-attentionlinear attention  \n\n# RoPE\n\n## attention\n\nRoPE\n\n  \n\nself-attention1 =  + softmaxsoftmax  \n\n  \n\n$$\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n$$  \n\n $q_m$  $m$ query$k_n$  $n$ key$f_q$  $f_k$ querykey  \n\n $f_q$  $f_k$  $g$ 11  \n\nRoPE  \n\n## \n\n11 $g$ \n\n2  \n\n{% asset_img complex_number.png 282 401  %}\n\nquerykey2  \n hidden size = 2   \n\n211Roformer  \n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta} \\\\\nf_k(\\boldsymbol{k}_n,n)&=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta} \\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n$$  \n\n $\\boldsymbol{k}_n^*$  $\\boldsymbol{k}_n$   \n\n\n\n  \n\n$$\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib \n\\end{gathered}\n\\tag{13}\n$$\n\n11  \n\n  \n\n  \n\n$$\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n$$\n\n22 $q_m$   \n\n$$\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n$$\n\n\n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m) \n&= \\boldsymbol{q}_me^{im\\theta} \\\\\n&= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&= (q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n$$  \n\n16  \n\n$$\nf_q(\\boldsymbol{q}_m,m) = \\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n$$  \n\n1transpose  \n\n\n\n$$\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n) \n&= (k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n$$  \n\n\n\n$$\nf_k(\\boldsymbol{k}_n,n) = \\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n$$  \n\n  \n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)) \\\\&+ (q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\\\\\n=&q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right) \\\\\n&+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right) \\\\\n&+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta)) \\\\\n&+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta)) \\\\\n=&q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta) \\\\\n&-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta) +q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n$$\n\n\n$$\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n$$\n\n $g$  \n\n$$\n\\begin{aligned}\n&g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n= &\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right] \\\\\n= &\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) - i(q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})](\\cos((m - n)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n$$\n\n\n\n## \n\n17\n\n$$\nf_q(\\boldsymbol{q}_m,m)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n$$\n\n  \n\n$$\nf_k(\\boldsymbol{k}_n,n)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n$$\n\n  \n\n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta) + \\sin(m\\theta)\\sin(n\\theta)}&\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n$$  \n\n2223  \n\n$$\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n$$  \n\n $f_q$  $f_k$   \n\n\n\n## 2\n\n2 $f_q$  $f_k$  $g$ 11  \n\n  \n\n$$\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\\\0&0&0&0&\\cdots&\\sin m\\theta_{d/2-1}&\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n$$  \n\n$d$  $d/2$  $d/2$  $\\theta$\n\n$$\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n$$  \n\n $m$  $n$  $R_m$  $R_n$self-attention  \n\n $\\theta$ GoogleAttention is All You Need\n\n## \n\n25  \n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n$$\n\nelement-wise  \n\nLLAMAdecoder\n\n## \n\n  \n\n\n\n $\\theta$   \n\n[Roformer](https://arxiv.org/abs/2104.09864)[](https://spaces.ac.cn/archives/8265)  \n\n $d = 128$ \n\n{% asset_img remote_attenuation.png 775 457  %}  \n\n#   \n\nRoPEtransformer\n\n# Reference\n1Transformerhttps://spaces.ac.cn/archives/8130  \n2Transformer2https://spaces.ac.cn/archives/8265  \n3RoFormer: Enhanced Transformer with Rotary Position Embedding https://arxiv.org/abs/2104.09864  \n4RoPE https://zhuanlan.zhihu.com/p/647109286  \n\n***\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)\n","source":"_posts/cs/nlp/2024/02/LLMRoPE.md","raw":"---\ntitle: LLM:RoPE\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - positional encoding\n  - RoPE\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: a051710f\ndate: 2024-02-21 21:18:13\nmathjax: true\n---\n\nLLMRoPE\n\n# RoPE\n\nRoPERotary Position Embedding2021TransformerRoPE<big><u>****</u></big>\n\n2023AlibiRoPE20232024RoPEAlibiAlibi\n\nRoPERoPE  \n\n# \n\nRoPE\n\n  \n\nBert256/512token  \n  \n<u>****</u>token-2token-1token-10002token-10001  \n<u>****</u><u>****</u>self-attention<u>****</u>  \n<u>****</u><u>****</u><u>****</u><u>****</u>  \n\n  \n\n3  \n\n## \n\nself-attention  \n\n$$\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}$$  \n\n\n$x_i$  $x_j$  $i$  $j$ $p$   \n\n$p$ $x$  $p$ attentionsoftmaxelement-wise addition\n\n $x + p$   $x * p$ \n\n## \n\n $x$  $p$   \n\n1 $e_1 = x_ + p_1$ 18 $e_8 = x_ + p_8$  $e_1$  $e_8$ <u>****</u>  \n\n15121512=512handle\n\n1 $q_{i}k_{j}^{T}$  \n\n$$\\begin{align*}q_ik_j^\\top&=\\left(x_i+p_i\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{\\color{red}W_K^\\top p_j^\\top}\\right)\\end{align*}\\tag{2}$$  \n\n $p_iW_\\mathbb{Q}$  $W_K^\\top p_j^\\top$   \n\n### Google\n\nGoogleSelf-Attention with Relative Position Representations $p_iW_\\mathbb{Q}$  $j$ $W_K^\\top p_j^\\top$  $i$$j$  $R_{ij}^K$attention<u>**input projection**</u>  \n\n$$\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n$$  \n\n$R_{ij}^K$ clip  \n\n$$\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n$$\n\n $p_\\mathrm{K}$  \n\nclip****tokentoken256>256\n\nGoogleinput\n\n$$\\begin{align*}\no_\\mathrm{i}&=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} + {\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}$$\n\nGoogle $p_{j}W_{\\mathrm{V}}$ \n\n$$\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}$$\n\n$R_{\\mathrm{i,j}}^{\\mathrm{V}}$  $R_{ij}^K$  + clip\n\n### XLNET\n\nXLNETGoogle  \n\n2  \n\n$$\\begin{align*}\nq_ik_j^T\n&= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&= \nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n$$  \n\n\n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n$$    \n\n $p_i$  $u$  $\\nu$  $p_j$  $R_{i-j}^\\top$   \n\n $u$  $\\nu$  $u$  $\\nu$ \n\n$$\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n$$   \n\nXLNET  \n\n$$\\begin{align*}\no_\\mathrm{i}\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}$$\n\nGoogleXLNET $\\mathrm{a_{i,j}}$ 2 $i$  $j$ clip\n\nT5  \n\n### T5\n\n6 $i$  $j$   \n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n$$    \n\nXLNETDeBertaT5\n\n## \n\nattention  \n\n1softmax33\n\n8433  \n\n\n\n\n\nself-attentionlinear attention  \n\n# RoPE\n\n## attention\n\nRoPE\n\n  \n\nself-attention1 =  + softmaxsoftmax  \n\n  \n\n$$\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n$$  \n\n $q_m$  $m$ query$k_n$  $n$ key$f_q$  $f_k$ querykey  \n\n $f_q$  $f_k$  $g$ 11  \n\nRoPE  \n\n## \n\n11 $g$ \n\n2  \n\n{% asset_img complex_number.png 282 401  %}\n\nquerykey2  \n hidden size = 2   \n\n211Roformer  \n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta} \\\\\nf_k(\\boldsymbol{k}_n,n)&=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta} \\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n$$  \n\n $\\boldsymbol{k}_n^*$  $\\boldsymbol{k}_n$   \n\n\n\n  \n\n$$\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib \n\\end{gathered}\n\\tag{13}\n$$\n\n11  \n\n  \n\n  \n\n$$\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n$$\n\n22 $q_m$   \n\n$$\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n$$\n\n\n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m) \n&= \\boldsymbol{q}_me^{im\\theta} \\\\\n&= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&= (q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n$$  \n\n16  \n\n$$\nf_q(\\boldsymbol{q}_m,m) = \\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n$$  \n\n1transpose  \n\n\n\n$$\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n) \n&= (k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n$$  \n\n\n\n$$\nf_k(\\boldsymbol{k}_n,n) = \\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n$$  \n\n  \n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)) \\\\&+ (q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\\\\\n=&q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right) \\\\\n&+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right) \\\\\n&+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta)) \\\\\n&+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta)) \\\\\n=&q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta) \\\\\n&-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta) +q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n$$\n\n\n$$\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n$$\n\n $g$  \n\n$$\n\\begin{aligned}\n&g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n= &\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right] \\\\\n= &\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) - i(q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})](\\cos((m - n)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n$$\n\n\n\n## \n\n17\n\n$$\nf_q(\\boldsymbol{q}_m,m)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n$$\n\n  \n\n$$\nf_k(\\boldsymbol{k}_n,n)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n$$\n\n  \n\n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta) + \\sin(m\\theta)\\sin(n\\theta)}&\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n$$  \n\n2223  \n\n$$\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n$$  \n\n $f_q$  $f_k$   \n\n\n\n## 2\n\n2 $f_q$  $f_k$  $g$ 11  \n\n  \n\n$$\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\\\0&0&0&0&\\cdots&\\sin m\\theta_{d/2-1}&\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n$$  \n\n$d$  $d/2$  $d/2$  $\\theta$\n\n$$\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n$$  \n\n $m$  $n$  $R_m$  $R_n$self-attention  \n\n $\\theta$ GoogleAttention is All You Need\n\n## \n\n25  \n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n$$\n\nelement-wise  \n\nLLAMAdecoder\n\n## \n\n  \n\n\n\n $\\theta$   \n\n[Roformer](https://arxiv.org/abs/2104.09864)[](https://spaces.ac.cn/archives/8265)  \n\n $d = 128$ \n\n{% asset_img remote_attenuation.png 775 457  %}  \n\n#   \n\nRoPEtransformer\n\n# Reference\n1Transformerhttps://spaces.ac.cn/archives/8130  \n2Transformer2https://spaces.ac.cn/archives/8265  \n3RoFormer: Enhanced Transformer with Rotary Position Embedding https://arxiv.org/abs/2104.09864  \n4RoPE https://zhuanlan.zhihu.com/p/647109286  \n\n***\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)\n","slug":"cs/nlp/2024/02/LLMRoPE","published":1,"updated":"2024-04-05T06:44:27.271Z","comments":1,"layout":"post","photos":[],"_id":"clv7wdj0h0001794k723j4p9n","content":"<p>LLMRoPE</p>\n<h1 id=\"rope\">RoPE</h1>\n<p>RoPERotary Position\nEmbedding2021TransformerRoPE<big><u><strong></strong></u></big></p>\n<p>2023AlibiRoPE20232024RoPEAlibiAlibi</p>\n<p>RoPERoPE</p>\n<h1 id=\"\"></h1>\n<p>RoPE</p>\n<p></p>\n<p>Bert256/512token<br>\n<br>\n<u><strong></strong></u>token-2token-1token-10002token-10001<br>\n<u><strong></strong></u><u><strong></strong></u>self-attention<u><strong></strong></u><br>\n<u><strong></strong></u><u><strong></strong></u><u><strong></strong></u><u><strong></strong></u></p>\n<p></p>\n<p>3</p>\n<h2 id=\"\"></h2>\n<p>self-attention</p>\n<p><span class=\"math display\">\\[\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}\\]</span></p>\n<p><span class=\"math inline\">\\(x_i\\)</span>  <span class=\"math inline\">\\(x_j\\)</span>  <span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(p\\)</span> </p>\n<p><span class=\"math inline\">\\(p\\)</span>\n<span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(p\\)</span>\nattentionsoftmaxelement-wise\naddition</p>\n<p>\n<span class=\"math inline\">\\(x + p\\)</span>  <span class=\"math inline\">\\(x * p\\)</span> </p>\n<h2 id=\"\"></h2>\n<p> <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(p\\)</span>\n</p>\n<p>1 <span class=\"math inline\">\\(e_1 =\nx_ + p_1\\)</span>\n18\n<span class=\"math inline\">\\(e_8 = x_ + p_8\\)</span>  <span class=\"math inline\">\\(e_1\\)</span>  <span class=\"math inline\">\\(e_8\\)</span>\n<u><strong></strong></u></p>\n<p>15121512=512handle</p>\n<p>1 <span class=\"math inline\">\\(q_{i}k_{j}^{T}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{align*}q_ik_j^\\top&amp;=\\left(x_i+p_i\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&amp;=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{\\color{red}W_K^\\top\np_j^\\top}\\right)\\end{align*}\\tag{2}\\]</span></p>\n<p> <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span> </p>\n<h3 id=\"google\">Google</h3>\n<p>GoogleSelf-Attention with Relative\nPosition Representations <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n\n<span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span>  <span class=\"math inline\">\\(i\\)</span><span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(R_{ij}^K\\)</span>attention<u><strong>input\nprojection</strong></u></p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n\\]</span></p>\n<p><span class=\"math inline\">\\(R_{ij}^K\\)</span>\nclip</p>\n<p><span class=\"math display\">\\[\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n\\]</span></p>\n<p> <span class=\"math inline\">\\(p_\\mathrm{K}\\)</span>\n</p>\n<p>clip<strong></strong>tokentoken256&gt;256</p>\n<p>Googleinput</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}&amp;=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} +\n{\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}\\]</span></p>\n<p>Google <span class=\"math inline\">\\(p_{j}W_{\\mathrm{V}}\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}\\]</span></p>\n<p><span class=\"math inline\">\\(R_{\\mathrm{i,j}}^{\\mathrm{V}}\\)</span> \n<span class=\"math inline\">\\(R_{ij}^K\\)</span> \n+ clip</p>\n<h3 id=\"xlnet\">XLNET</h3>\n<p>XLNETGoogle</p>\n<p>2</p>\n<p><span class=\"math display\">\\[\\begin{align*}\nq_ik_j^T\n&amp;= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&amp;=\nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu}\nW_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(p_i\\)</span> \n<span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span>  <span class=\"math inline\">\\(p_j\\)</span>  <span class=\"math inline\">\\(R_{i-j}^\\top\\)</span> </p>\n<p> <span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span>\n <span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span> </p>\n<p><span class=\"math display\">\\[\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n\\]</span></p>\n<p>XLNET</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}\\]</span></p>\n<p>GoogleXLNET <span class=\"math inline\">\\(\\mathrm{a_{i,j}}\\)</span>\n2\n<span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span>\nclip</p>\n<p>T5</p>\n<h3 id=\"t5\">T5</h3>\n<p>6\n<span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span>\n</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n\\]</span></p>\n<p>XLNETDeBertaT5</p>\n<h2 id=\"\"></h2>\n<p>attention</p>\n<p>1softmax33</p>\n<p>8433</p>\n<p></p>\n<p></p>\n<p>self-attentionlinear\nattention</p>\n<h1 id=\"rope\">RoPE</h1>\n<h2 id=\"attention\">attention</h2>\n<p>RoPE</p>\n<p></p>\n<p>self-attention1\n=  +\nsoftmaxsoftmax</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(q_m\\)</span>  <span class=\"math inline\">\\(m\\)</span> query<span class=\"math inline\">\\(k_n\\)</span>  <span class=\"math inline\">\\(n\\)</span> key<span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>\nquerykey</p>\n<p> <span class=\"math inline\">\\(f_q\\)</span> \n<span class=\"math inline\">\\(f_k\\)</span>  <span class=\"math inline\">\\(g\\)</span> 11</p>\n<p>RoPE</p>\n<h2 id=\"\"></h2>\n<p>11 <span class=\"math inline\">\\(g\\)</span>\n</p>\n<p>2</p>\n<img src=\"/a051710f/complex_number.png\" class width=\"282\" height=\"401\" title=\"\">\n<p>querykey2<br>\n hidden size = 2 </p>\n<p>211Roformer</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&amp;=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta}\n\\\\\nf_k(\\boldsymbol{k}_n,n)&amp;=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta}\n\\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&amp;=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\boldsymbol{k}_n^*\\)</span>  <span class=\"math inline\">\\(\\boldsymbol{k}_n\\)</span> </p>\n<p></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib\n\\end{gathered}\n\\tag{13}\n\\]</span></p>\n<p>11</p>\n<p></p>\n<p></p>\n<p><span class=\"math display\">\\[\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n\\]</span></p>\n<p>22 <span class=\"math inline\">\\(q_m\\)</span> </p>\n<p><span class=\"math display\">\\[\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)\n&amp;= \\boldsymbol{q}_me^{im\\theta} \\\\\n&amp;= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&amp;=\n(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n\\]</span></p>\n<p>16</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m) =\n\\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n\\]</span></p>\n<p>1transpose</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n)\n&amp;=\n(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n) =\n\\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n\\]</span></p>\n<p><br>\n<span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&amp;(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))\n\\\\&amp;+ (q_m^{(1)}\\sin(m\\theta) +\nq_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) +\nk_n^{(2)}\\cos(n\\theta))\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta))\n\\\\\n&amp;+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta))\n\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta)\n\\\\\n&amp;-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta)\n+q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n\\]</span></p>\n<p> <span class=\"math display\">\\[\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(g\\)</span></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n=\n&amp;\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\\\\n= &amp;\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) -\ni(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})](\\cos((m -\nn)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 -\nq_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n\\]</span></p>\n<p></p>\n<h2 id=\"\"></h2>\n<p>17</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta)\n+ \\sin(m\\theta)\\sin(n\\theta)}&amp;\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&amp;\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&amp;{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&amp;{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n\\]</span></p>\n<p>2223</p>\n<p><span class=\"math display\">\\[\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n\\]</span></p>\n<p> <span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>\n</p>\n<p></p>\n<h2 id=\"2\">2</h2>\n<p>2 <span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>  <span class=\"math inline\">\\(g\\)</span>\n11</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&amp;-\\sin\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\\\sin m\\theta_0&amp;\\cos\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\cos\nm\\theta_1&amp;-\\sin m\\theta_1&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\sin\nm\\theta_1&amp;\\cos\nm\\theta_1&amp;\\cdots&amp;0&amp;0\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\cos\nm\\theta_{d/2-1}&amp;-\\sin\nm\\theta_{d/2-1}\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\sin\nm\\theta_{d/2-1}&amp;\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n\\]</span></p>\n<p><span class=\"math inline\">\\(d\\)</span>\n <span class=\"math inline\">\\(d/2\\)</span>  <span class=\"math inline\">\\(d/2\\)</span> \n<span class=\"math inline\">\\(\\theta\\)</span></p>\n<p><span class=\"math display\">\\[\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(m\\)</span>\n <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(R_m\\)</span>  <span class=\"math inline\">\\(R_n\\)</span>self-attention</p>\n<p> <span class=\"math inline\">\\(\\theta\\)</span>\nGoogleAttention is All You\nNeed</p>\n<h2 id=\"\"></h2>\n<p>25</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n\\]</span></p>\n<p>element-wise</p>\n<p>LLAMAdecoder</p>\n<h2 id=\"\"></h2>\n<p></p>\n<p></p>\n<p>\n<span class=\"math inline\">\\(\\theta\\)</span>\n</p>\n<p><a href=\"https://arxiv.org/abs/2104.09864\">Roformer</a><a href=\"https://spaces.ac.cn/archives/8265\"></a></p>\n<p> <span class=\"math inline\">\\(d = 128\\)</span>\n</p>\n<img src=\"/a051710f/remote_attenuation.png\" class width=\"775\" height=\"457\" title=\"\">\n<h1 id=\"\"></h1>\n<p>RoPEtransformer</p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Transformerhttps://spaces.ac.cn/archives/8130<br>\n2Transformer2https://spaces.ac.cn/archives/8265<br>\n3RoFormer: Enhanced Transformer with Rotary Position Embedding\nhttps://arxiv.org/abs/2104.09864<br>\n4RoPE\nhttps://zhuanlan.zhihu.com/p/647109286</p>\n<hr>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n","length":14264,"excerpt":"","more":"<p>LLMRoPE</p>\n<h1 id=\"rope\">RoPE</h1>\n<p>RoPERotary Position\nEmbedding2021TransformerRoPE<big><u><strong></strong></u></big></p>\n<p>2023AlibiRoPE20232024RoPEAlibiAlibi</p>\n<p>RoPERoPE</p>\n<h1 id=\"\"></h1>\n<p>RoPE</p>\n<p></p>\n<p>Bert256/512token<br>\n<br>\n<u><strong></strong></u>token-2token-1token-10002token-10001<br>\n<u><strong></strong></u><u><strong></strong></u>self-attention<u><strong></strong></u><br>\n<u><strong></strong></u><u><strong></strong></u><u><strong></strong></u><u><strong></strong></u></p>\n<p></p>\n<p>3</p>\n<h2 id=\"\"></h2>\n<p>self-attention</p>\n<p><span class=\"math display\">\\[\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}\\]</span></p>\n<p><span class=\"math inline\">\\(x_i\\)</span>  <span class=\"math inline\">\\(x_j\\)</span>  <span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(p\\)</span> </p>\n<p><span class=\"math inline\">\\(p\\)</span>\n<span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(p\\)</span>\nattentionsoftmaxelement-wise\naddition</p>\n<p>\n<span class=\"math inline\">\\(x + p\\)</span>  <span class=\"math inline\">\\(x * p\\)</span> </p>\n<h2 id=\"\"></h2>\n<p> <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(p\\)</span>\n</p>\n<p>1 <span class=\"math inline\">\\(e_1 =\nx_ + p_1\\)</span>\n18\n<span class=\"math inline\">\\(e_8 = x_ + p_8\\)</span>  <span class=\"math inline\">\\(e_1\\)</span>  <span class=\"math inline\">\\(e_8\\)</span>\n<u><strong></strong></u></p>\n<p>15121512=512handle</p>\n<p>1 <span class=\"math inline\">\\(q_{i}k_{j}^{T}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{align*}q_ik_j^\\top&amp;=\\left(x_i+p_i\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&amp;=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{\\color{red}W_K^\\top\np_j^\\top}\\right)\\end{align*}\\tag{2}\\]</span></p>\n<p> <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span> </p>\n<h3 id=\"google\">Google</h3>\n<p>GoogleSelf-Attention with Relative\nPosition Representations <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n\n<span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span>  <span class=\"math inline\">\\(i\\)</span><span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(R_{ij}^K\\)</span>attention<u><strong>input\nprojection</strong></u></p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n\\]</span></p>\n<p><span class=\"math inline\">\\(R_{ij}^K\\)</span>\nclip</p>\n<p><span class=\"math display\">\\[\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n\\]</span></p>\n<p> <span class=\"math inline\">\\(p_\\mathrm{K}\\)</span>\n</p>\n<p>clip<strong></strong>tokentoken256&gt;256</p>\n<p>Googleinput</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}&amp;=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} +\n{\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}\\]</span></p>\n<p>Google <span class=\"math inline\">\\(p_{j}W_{\\mathrm{V}}\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}\\]</span></p>\n<p><span class=\"math inline\">\\(R_{\\mathrm{i,j}}^{\\mathrm{V}}\\)</span> \n<span class=\"math inline\">\\(R_{ij}^K\\)</span> \n+ clip</p>\n<h3 id=\"xlnet\">XLNET</h3>\n<p>XLNETGoogle</p>\n<p>2</p>\n<p><span class=\"math display\">\\[\\begin{align*}\nq_ik_j^T\n&amp;= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&amp;=\nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu}\nW_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(p_i\\)</span> \n<span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span>  <span class=\"math inline\">\\(p_j\\)</span>  <span class=\"math inline\">\\(R_{i-j}^\\top\\)</span> </p>\n<p> <span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span>\n <span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span> </p>\n<p><span class=\"math display\">\\[\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n\\]</span></p>\n<p>XLNET</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}\\]</span></p>\n<p>GoogleXLNET <span class=\"math inline\">\\(\\mathrm{a_{i,j}}\\)</span>\n2\n<span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span>\nclip</p>\n<p>T5</p>\n<h3 id=\"t5\">T5</h3>\n<p>6\n<span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span>\n</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n\\]</span></p>\n<p>XLNETDeBertaT5</p>\n<h2 id=\"\"></h2>\n<p>attention</p>\n<p>1softmax33</p>\n<p>8433</p>\n<p></p>\n<p></p>\n<p>self-attentionlinear\nattention</p>\n<h1 id=\"rope\">RoPE</h1>\n<h2 id=\"attention\">attention</h2>\n<p>RoPE</p>\n<p></p>\n<p>self-attention1\n=  +\nsoftmaxsoftmax</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(q_m\\)</span>  <span class=\"math inline\">\\(m\\)</span> query<span class=\"math inline\">\\(k_n\\)</span>  <span class=\"math inline\">\\(n\\)</span> key<span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>\nquerykey</p>\n<p> <span class=\"math inline\">\\(f_q\\)</span> \n<span class=\"math inline\">\\(f_k\\)</span>  <span class=\"math inline\">\\(g\\)</span> 11</p>\n<p>RoPE</p>\n<h2 id=\"\"></h2>\n<p>11 <span class=\"math inline\">\\(g\\)</span>\n</p>\n<p>2</p>\n<img src=\"/a051710f/complex_number.png\" class width=\"282\" height=\"401\" title=\"\">\n<p>querykey2<br>\n hidden size = 2 </p>\n<p>211Roformer</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&amp;=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta}\n\\\\\nf_k(\\boldsymbol{k}_n,n)&amp;=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta}\n\\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&amp;=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\boldsymbol{k}_n^*\\)</span>  <span class=\"math inline\">\\(\\boldsymbol{k}_n\\)</span> </p>\n<p></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib\n\\end{gathered}\n\\tag{13}\n\\]</span></p>\n<p>11</p>\n<p></p>\n<p></p>\n<p><span class=\"math display\">\\[\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n\\]</span></p>\n<p>22 <span class=\"math inline\">\\(q_m\\)</span> </p>\n<p><span class=\"math display\">\\[\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)\n&amp;= \\boldsymbol{q}_me^{im\\theta} \\\\\n&amp;= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&amp;=\n(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n\\]</span></p>\n<p>16</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m) =\n\\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n\\]</span></p>\n<p>1transpose</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n)\n&amp;=\n(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n) =\n\\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n\\]</span></p>\n<p><br>\n<span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&amp;(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))\n\\\\&amp;+ (q_m^{(1)}\\sin(m\\theta) +\nq_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) +\nk_n^{(2)}\\cos(n\\theta))\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta))\n\\\\\n&amp;+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta))\n\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta)\n\\\\\n&amp;-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta)\n+q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n\\]</span></p>\n<p> <span class=\"math display\">\\[\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(g\\)</span></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n=\n&amp;\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\\\\n= &amp;\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) -\ni(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})](\\cos((m -\nn)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 -\nq_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n\\]</span></p>\n<p></p>\n<h2 id=\"\"></h2>\n<p>17</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta)\n+ \\sin(m\\theta)\\sin(n\\theta)}&amp;\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&amp;\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&amp;{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&amp;{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n\\]</span></p>\n<p>2223</p>\n<p><span class=\"math display\">\\[\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n\\]</span></p>\n<p> <span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>\n</p>\n<p></p>\n<h2 id=\"2\">2</h2>\n<p>2 <span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>  <span class=\"math inline\">\\(g\\)</span>\n11</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&amp;-\\sin\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\\\sin m\\theta_0&amp;\\cos\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\cos\nm\\theta_1&amp;-\\sin m\\theta_1&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\sin\nm\\theta_1&amp;\\cos\nm\\theta_1&amp;\\cdots&amp;0&amp;0\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\cos\nm\\theta_{d/2-1}&amp;-\\sin\nm\\theta_{d/2-1}\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\sin\nm\\theta_{d/2-1}&amp;\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n\\]</span></p>\n<p><span class=\"math inline\">\\(d\\)</span>\n <span class=\"math inline\">\\(d/2\\)</span>  <span class=\"math inline\">\\(d/2\\)</span> \n<span class=\"math inline\">\\(\\theta\\)</span></p>\n<p><span class=\"math display\">\\[\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(m\\)</span>\n <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(R_m\\)</span>  <span class=\"math inline\">\\(R_n\\)</span>self-attention</p>\n<p> <span class=\"math inline\">\\(\\theta\\)</span>\nGoogleAttention is All You\nNeed</p>\n<h2 id=\"\"></h2>\n<p>25</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n\\]</span></p>\n<p>element-wise</p>\n<p>LLAMAdecoder</p>\n<h2 id=\"\"></h2>\n<p></p>\n<p></p>\n<p>\n<span class=\"math inline\">\\(\\theta\\)</span>\n</p>\n<p><a href=\"https://arxiv.org/abs/2104.09864\">Roformer</a><a href=\"https://spaces.ac.cn/archives/8265\"></a></p>\n<p> <span class=\"math inline\">\\(d = 128\\)</span>\n</p>\n<img src=\"/a051710f/remote_attenuation.png\" class width=\"775\" height=\"457\" title=\"\">\n<h1 id=\"\"></h1>\n<p>RoPEtransformer</p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Transformerhttps://spaces.ac.cn/archives/8130<br>\n2Transformer2https://spaces.ac.cn/archives/8265<br>\n3RoFormer: Enhanced Transformer with Rotary Position Embedding\nhttps://arxiv.org/abs/2104.09864<br>\n4RoPE\nhttps://zhuanlan.zhihu.com/p/647109286</p>\n<hr>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n"},{"title":"LLM","abbrlink":"c4da56c0","date":"2024-02-28T07:19:28.000Z","_content":"\n  \n\nRoPERoPE[](http://www.linsight.cn/a051710f.html) [](https://zhuanlan.zhihu.com/p/684072868) [](https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&mid=2247483759&idx=1&sn=f7b59b879476b8687a340606b5568eae&chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&token=88551061&lang=zh_CN#rd)\n\n#   \n\n2023LLM20235Claude100k tokens67ChatGPT3.516kChatGLM2-B32k  \n\nChatGLMAgentChatGLM3ChatGLM4  \n\nLM-SYSLongChatMosaicLMMPT16k\n\nQwen-1.532k  \n\n<center>\n\n|  |  |\n| :----: | :----: |\n| Baichuan2 | 192k |\n| GPT4-turbo  | 128k |\n| Yi | 200k |\n| Kimi Chat | 128k(20) |\n| Claude2 | 200k |  \n\n</center>\n\n  \n\n  \n\n#   \n\ntokenizertokentoken>1.5tokenizer2200ktoken30w  \n\n27  \n\n<big><u>****</u></big>  \n\nRAGRetrieval-augmented generationRAG  \n\n<big><u>****</u></big>prompt  \n\nprompt  \n\n1ppl2attention\n\n# \n\n  \n\n2k/4k8k16kPPLRoPE<u>****</u>  \n\n## \n\n2k/4k8k/16k/32k+  \n\n  \n\n1.  \n\n32k  \n\n4k8attention maskattention mask  \n\n>  \n\n2.  \n\ntransformer  \n\n $l$  $V$ hidden size $h$ batch size $b$  $s$ Adam1  \n\n(1) \n\n $\\Phi$  =  + $l$ * decoder = $Vh + l(12h^2 + 13h)$  \n\n $s$   \n\n(2)   \n\n = logits + $l$ *  $\\approx2bshV + l*(24bsh^2+4bs^2h)$\n\nsoftmaxsoftmax $s$ \n\n\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\frac{}{}\n&=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n$$\n\n $s << h$  $h$ 1k1w $s$  $sh$   \n\n(3)   \n\noptimizer\n\n$\\Phi$$\\Phi$$2\\Phi$ $(\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi + 2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]$ \n\n{% asset_img mix_precision_fp16.png  %}  \n\n\n\n  \n\nsoftmaxdropout  \n\nattention $x$  $QKV$  $x$  $QK$  $QK$ softmax $QK^T$  $11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2$  $l$  $l$   \n\n $s$ 4k32k64GPUbatch sizegradient accumulation<big><u>****</u></big>  \n\n2B7B16k32k200k34B70B+  \n\n2k4k  \n\n  \n\n##  Position Interpolation\n\n236Meta[EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION](https://arxiv.org/pdf/2306.15595.pdf)RoPEPIPosition Interpolation2k32k1kstep\n\n{% asset_img meta_pi.png PI %}  \n{% asset_img LLM/meta_pi.png PI %}  \n\n>In contrast, LLaMA models that are extended via direct fine-tuning only saw a minimal increase of the effective context window size kmax from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear indication of an acceleration in the increase of window size.  \n\n1w20482560\n\nRoPE  \n\nRoPERoPE $\\left|m-n \\right|$ <2048attention $\\left|m-n \\right|$ \n\n{% asset_img meta_rope_ext.png RoPE %}  \n\n3000attention score\n\n\n\nPI  \n\n{% asset_img meta_pi_nosft.png PI %}  \n\n2k2k2k4k\n\n{% asset_img meta_pi_explanation.png PI %}  \n\n123...11.522.5...0.5  \n\nattention score $\\tilde{a}(s)=a(Ls/L^{\\prime})$ $L$ 2048$L^{\\prime}$ 8k/16k/32k\n\nRoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}'(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L'}\\right)\n\\end{aligned}\n\\end{equation}\n$$\n\n $m$ 1 ${L}/{L'}$  \n\n  \n\n\n\n\n\n## NTK-Aware Interpolation \n\ncosNTK-Aware InterpolationRoPE<u>****</u>NTK-Aware Scaled RoPECodeLlama1M  \n\nNTKNeural Tangent KernelGLM4  \n\n>Neural Tangent Kernel (NTK) NTK   \n Neural Tangent Kernel  \nNTK   \nNTK \n\nNTK  \n\n  \n\nRoPE $m$   \n\n{% asset_img rope_matrix.png RoPE %}  \n\n22 $d/2$  $\\theta_j=10000^{-2j/d}$  $j$ $j$  $base=10000$  $base$   \n\n  \n\n  \n\n[RoPE](https://www.zhihu.com/people/us4ever)2  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle= \\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n$$  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n $h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*$ $s=m-n$   \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n $\\alpha=L'/L>1$  $s$   \n\nNTK-Aware Scaled RoPE $\\theta_j$ baseRoPE10000  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n $\\theta$  $\\alpha^{\\frac{-2j}{d-2}}$  $j$  $\\alpha^{\\frac{-2j}{d-2}}$ 1 $j$  $j$ 0 $d/2 - 1$$\\alpha^{\\frac{-2j}{d-2}}$  $\\alpha^{-1}$ \n\n[](https://zhuanlan.zhihu.com/p/645770522)NTK-Aware Interpolation  \n\n>RoPE 12 3 60  RoPE 1/60  1/60 4 RoPE NTK-Aware RoPE  1.5  2  90  24  129.6k  43.2k   \n\nRoPE[](https://kexue.fm/archives/9675)  \n\nYaRN[](https://arxiv.org/pdf/2309.00071.pdf)NTK  \n\n>Given the results from [6], this method performs much better at extending the context size of non-finetuned models compared to PI [9]. However, one major disadvantage of this method is that given it is not just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\" interpolation [6] yields inferior results to PI [9]. Furthermore, due to the \"out-of-bound\" values, the theoretical scale factor s does not accurately describe the true context extension scale. In practice, the scale value s has to be set higher than the expected scale for a given context length extension.  \n\nNTK4k32k $\\alpha=L'/L$ 816\n\n## NTK-by-parts\n\nNTK-by-partsNTKNTK-awareRoPENTK-by-parts  \n\n $j$ RoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n$\\lambda_j$  $j$   \n\n $j$  $L$ RoPE $sin$ 1/40~1-1~0 $j$   \n\nNTK-by-parts  \n\n-  $j$  $\\lambda_j$    \n-  $\\lambda_j\\geq$   \n- NTK-aware interpolation  \n\n $r(j)=\\frac{L}{\\lambda_j}$  $\\beta_1\\beta_2$  $r(j)<\\beta_1$  $r(j)\\geq \\beta_2$   \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&if&r(j)<\\beta_1\\\\1&if&r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n$$  \n\nNTK-by-parts $\\theta_j$   \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n$$\n\n $\\beta_1\\beta_2$  $\\beta_1=1\\beta_2=32$ 1/32   \n\n## Dynamically NTK Scaled RoPE  \n\nNTK-Aware InterpolationRoPEattention score $l$  $L$  $\\alpha$ baseDynamically NTK Scaled RoPENTK  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n$$  \n\n $l$  $l>L$  $\\alpha$ 1 $l\\leq L$   \n\nkv-cacheRoPE  \n\n## YaRN  \n\ntokensoftmaxRoPEtoken  \n\nRoPEsoftmaxlogitsoftmax $t>1$ RoPE $\\sqrt{t}$ RoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n$$  \n\nLlama 1Llama 2$\\begin{aligned}\\sqrt{\\frac1t}&=0.1\\ln(\\alpha)+1.\\end{aligned}$Llama  \n\nYaRNNTK-by-partsattention score  \n\nYaRN\n\n## logn  \n\nlognattention $\\sqrt{d}$ logn[](https://zhuanlan.zhihu.com/p/678755776)YaRN  \n\ntokentokentokenattention score  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L'}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n$$  \n\n $L'>L$ YaRN\n\n## \n\nwindow attentionstreaming LLMLongLoRAFocus Transformer\n\n#   \n\n2k4k  \n\n-   \n- token  \n\nattention score  \n\nPINTKNTKlognYaRN  \n\n# Reference  \n1transformerKV cache https://zhuanlan.zhihu.com/p/624740065  \n2EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION https://arxiv.org/pdf/2306.15595.pdf  \n3Transformer10RoPE https://kexue.fm/archives/9675  \n4YaRN: Efficient Context Window Extension of Large\nLanguage Models https://arxiv.org/pdf/2309.00071.pdf  \n5RoPE https://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ  \n6LLM https://zhuanlan.zhihu.com/p/645770522\n7prompt https://cloud.tencent.com/developer/article/2330611  \n8Transformer8 https://spaces.ac.cn/archives/9444  \n9RoPE192K https://zhuanlan.zhihu.com/p/678755776\n***\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)\n","source":"_posts/cs/nlp/2024/02/LLM.md","raw":"---\ntitle: LLM\nabbrlink: c4da56c0\ndate: 2024-02-28 15:19:28\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  \n\nRoPERoPE[](http://www.linsight.cn/a051710f.html) [](https://zhuanlan.zhihu.com/p/684072868) [](https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&mid=2247483759&idx=1&sn=f7b59b879476b8687a340606b5568eae&chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&token=88551061&lang=zh_CN#rd)\n\n#   \n\n2023LLM20235Claude100k tokens67ChatGPT3.516kChatGLM2-B32k  \n\nChatGLMAgentChatGLM3ChatGLM4  \n\nLM-SYSLongChatMosaicLMMPT16k\n\nQwen-1.532k  \n\n<center>\n\n|  |  |\n| :----: | :----: |\n| Baichuan2 | 192k |\n| GPT4-turbo  | 128k |\n| Yi | 200k |\n| Kimi Chat | 128k(20) |\n| Claude2 | 200k |  \n\n</center>\n\n  \n\n  \n\n#   \n\ntokenizertokentoken>1.5tokenizer2200ktoken30w  \n\n27  \n\n<big><u>****</u></big>  \n\nRAGRetrieval-augmented generationRAG  \n\n<big><u>****</u></big>prompt  \n\nprompt  \n\n1ppl2attention\n\n# \n\n  \n\n2k/4k8k16kPPLRoPE<u>****</u>  \n\n## \n\n2k/4k8k/16k/32k+  \n\n  \n\n1.  \n\n32k  \n\n4k8attention maskattention mask  \n\n>  \n\n2.  \n\ntransformer  \n\n $l$  $V$ hidden size $h$ batch size $b$  $s$ Adam1  \n\n(1) \n\n $\\Phi$  =  + $l$ * decoder = $Vh + l(12h^2 + 13h)$  \n\n $s$   \n\n(2)   \n\n = logits + $l$ *  $\\approx2bshV + l*(24bsh^2+4bs^2h)$\n\nsoftmaxsoftmax $s$ \n\n\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\frac{}{}\n&=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n$$\n\n $s << h$  $h$ 1k1w $s$  $sh$   \n\n(3)   \n\noptimizer\n\n$\\Phi$$\\Phi$$2\\Phi$ $(\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi + 2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]$ \n\n{% asset_img mix_precision_fp16.png  %}  \n\n\n\n  \n\nsoftmaxdropout  \n\nattention $x$  $QKV$  $x$  $QK$  $QK$ softmax $QK^T$  $11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2$  $l$  $l$   \n\n $s$ 4k32k64GPUbatch sizegradient accumulation<big><u>****</u></big>  \n\n2B7B16k32k200k34B70B+  \n\n2k4k  \n\n  \n\n##  Position Interpolation\n\n236Meta[EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION](https://arxiv.org/pdf/2306.15595.pdf)RoPEPIPosition Interpolation2k32k1kstep\n\n{% asset_img meta_pi.png PI %}  \n{% asset_img LLM/meta_pi.png PI %}  \n\n>In contrast, LLaMA models that are extended via direct fine-tuning only saw a minimal increase of the effective context window size kmax from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear indication of an acceleration in the increase of window size.  \n\n1w20482560\n\nRoPE  \n\nRoPERoPE $\\left|m-n \\right|$ <2048attention $\\left|m-n \\right|$ \n\n{% asset_img meta_rope_ext.png RoPE %}  \n\n3000attention score\n\n\n\nPI  \n\n{% asset_img meta_pi_nosft.png PI %}  \n\n2k2k2k4k\n\n{% asset_img meta_pi_explanation.png PI %}  \n\n123...11.522.5...0.5  \n\nattention score $\\tilde{a}(s)=a(Ls/L^{\\prime})$ $L$ 2048$L^{\\prime}$ 8k/16k/32k\n\nRoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}'(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L'}\\right)\n\\end{aligned}\n\\end{equation}\n$$\n\n $m$ 1 ${L}/{L'}$  \n\n  \n\n\n\n\n\n## NTK-Aware Interpolation \n\ncosNTK-Aware InterpolationRoPE<u>****</u>NTK-Aware Scaled RoPECodeLlama1M  \n\nNTKNeural Tangent KernelGLM4  \n\n>Neural Tangent Kernel (NTK) NTK   \n Neural Tangent Kernel  \nNTK   \nNTK \n\nNTK  \n\n  \n\nRoPE $m$   \n\n{% asset_img rope_matrix.png RoPE %}  \n\n22 $d/2$  $\\theta_j=10000^{-2j/d}$  $j$ $j$  $base=10000$  $base$   \n\n  \n\n  \n\n[RoPE](https://www.zhihu.com/people/us4ever)2  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle= \\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n$$  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n $h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*$ $s=m-n$   \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n $\\alpha=L'/L>1$  $s$   \n\nNTK-Aware Scaled RoPE $\\theta_j$ baseRoPE10000  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n $\\theta$  $\\alpha^{\\frac{-2j}{d-2}}$  $j$  $\\alpha^{\\frac{-2j}{d-2}}$ 1 $j$  $j$ 0 $d/2 - 1$$\\alpha^{\\frac{-2j}{d-2}}$  $\\alpha^{-1}$ \n\n[](https://zhuanlan.zhihu.com/p/645770522)NTK-Aware Interpolation  \n\n>RoPE 12 3 60  RoPE 1/60  1/60 4 RoPE NTK-Aware RoPE  1.5  2  90  24  129.6k  43.2k   \n\nRoPE[](https://kexue.fm/archives/9675)  \n\nYaRN[](https://arxiv.org/pdf/2309.00071.pdf)NTK  \n\n>Given the results from [6], this method performs much better at extending the context size of non-finetuned models compared to PI [9]. However, one major disadvantage of this method is that given it is not just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\" interpolation [6] yields inferior results to PI [9]. Furthermore, due to the \"out-of-bound\" values, the theoretical scale factor s does not accurately describe the true context extension scale. In practice, the scale value s has to be set higher than the expected scale for a given context length extension.  \n\nNTK4k32k $\\alpha=L'/L$ 816\n\n## NTK-by-parts\n\nNTK-by-partsNTKNTK-awareRoPENTK-by-parts  \n\n $j$ RoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n$\\lambda_j$  $j$   \n\n $j$  $L$ RoPE $sin$ 1/40~1-1~0 $j$   \n\nNTK-by-parts  \n\n-  $j$  $\\lambda_j$    \n-  $\\lambda_j\\geq$   \n- NTK-aware interpolation  \n\n $r(j)=\\frac{L}{\\lambda_j}$  $\\beta_1\\beta_2$  $r(j)<\\beta_1$  $r(j)\\geq \\beta_2$   \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&if&r(j)<\\beta_1\\\\1&if&r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n$$  \n\nNTK-by-parts $\\theta_j$   \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n$$\n\n $\\beta_1\\beta_2$  $\\beta_1=1\\beta_2=32$ 1/32   \n\n## Dynamically NTK Scaled RoPE  \n\nNTK-Aware InterpolationRoPEattention score $l$  $L$  $\\alpha$ baseDynamically NTK Scaled RoPENTK  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n$$  \n\n $l$  $l>L$  $\\alpha$ 1 $l\\leq L$   \n\nkv-cacheRoPE  \n\n## YaRN  \n\ntokensoftmaxRoPEtoken  \n\nRoPEsoftmaxlogitsoftmax $t>1$ RoPE $\\sqrt{t}$ RoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n$$  \n\nLlama 1Llama 2$\\begin{aligned}\\sqrt{\\frac1t}&=0.1\\ln(\\alpha)+1.\\end{aligned}$Llama  \n\nYaRNNTK-by-partsattention score  \n\nYaRN\n\n## logn  \n\nlognattention $\\sqrt{d}$ logn[](https://zhuanlan.zhihu.com/p/678755776)YaRN  \n\ntokentokentokenattention score  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L'}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n$$  \n\n $L'>L$ YaRN\n\n## \n\nwindow attentionstreaming LLMLongLoRAFocus Transformer\n\n#   \n\n2k4k  \n\n-   \n- token  \n\nattention score  \n\nPINTKNTKlognYaRN  \n\n# Reference  \n1transformerKV cache https://zhuanlan.zhihu.com/p/624740065  \n2EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION https://arxiv.org/pdf/2306.15595.pdf  \n3Transformer10RoPE https://kexue.fm/archives/9675  \n4YaRN: Efficient Context Window Extension of Large\nLanguage Models https://arxiv.org/pdf/2309.00071.pdf  \n5RoPE https://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ  \n6LLM https://zhuanlan.zhihu.com/p/645770522\n7prompt https://cloud.tencent.com/developer/article/2330611  \n8Transformer8 https://spaces.ac.cn/archives/9444  \n9RoPE192K https://zhuanlan.zhihu.com/p/678755776\n***\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)\n","slug":"cs/nlp/2024/02/LLM","published":1,"updated":"2024-03-13T07:23:38.942Z","comments":1,"layout":"post","photos":[],"_id":"clv7wdj0j0003794kdn9qe4rt","content":"<p></p>\n<p>RoPERoPE<a href=\"http://www.linsight.cn/a051710f.html\"></a> <a href=\"https://zhuanlan.zhihu.com/p/684072868\"></a> <a href=\"https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&amp;mid=2247483759&amp;idx=1&amp;sn=f7b59b879476b8687a340606b5568eae&amp;chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&amp;token=88551061&amp;lang=zh_CN#rd\"></a></p>\n<h1 id=\"\"></h1>\n<p>2023LLM20235Claude100k\ntokens67ChatGPT3.516kChatGLM2-B32k</p>\n<p>ChatGLMAgentChatGLM3ChatGLM4</p>\n<p>LM-SYSLongChatMosaicLMMPT16k</p>\n<p>Qwen-1.532k</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Baichuan2</td>\n<td style=\"text-align: center;\">192k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">GPT4-turbo</td>\n<td style=\"text-align: center;\">128k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Yi</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Kimi Chat</td>\n<td style=\"text-align: center;\">128k(20)</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Claude2</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p></p>\n<p></p>\n<h1 id=\"\"></h1>\n<p>tokenizertokentoken&gt;1.5tokenizer2200ktoken30w</p>\n<p>27</p>\n<p><big><u><strong></strong></u></big></p>\n<p>RAGRetrieval-augmented\ngenerationRAG</p>\n<p><big><u><strong></strong></u></big>prompt</p>\n<p>prompt</p>\n<p>1ppl2attention</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>2k/4k8k16kPPLRoPE<u><strong></strong></u></p>\n<h2 id=\"\"></h2>\n<p>2k/4k8k/16k/32k+</p>\n<p></p>\n<p>1.</p>\n<p>32k</p>\n<p>4k8attention\nmaskattention\nmask</p>\n<p>&gt;</p>\n<p>2.</p>\n<p>transformer</p>\n<p> <span class=\"math inline\">\\(l\\)</span> \n<span class=\"math inline\">\\(V\\)</span> hidden size <span class=\"math inline\">\\(h\\)</span> batch size <span class=\"math inline\">\\(b\\)</span>  <span class=\"math inline\">\\(s\\)</span>\nAdam1</p>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p> <span class=\"math inline\">\\(\\Phi\\)</span> = \n+ <span class=\"math inline\">\\(l\\)</span> * decoder = <span class=\"math inline\">\\(Vh + l(12h^2 + 13h)\\)</span></p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p> = logits + <span class=\"math inline\">\\(l\\)</span> *  <span class=\"math inline\">\\(\\approx2bshV + l*(24bsh^2+4bs^2h)\\)</span></p>\n<p>softmaxsoftmax <span class=\"math inline\">\\(s\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{}{}\n&amp;=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&amp;\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(s &lt;&lt; h\\)</span>\n <span class=\"math inline\">\\(h\\)</span> 1k1w\n<span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(sh\\)</span>\n</p>\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>optimizer</p>\n<p><span class=\"math inline\">\\(\\Phi\\)</span><span class=\"math inline\">\\(\\Phi\\)</span><span class=\"math inline\">\\(2\\Phi\\)</span>\n<span class=\"math inline\">\\((\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi +\n2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]\\)</span>\n</p>\n<img src=\"/c4da56c0/mix_precision_fp16.png\" class title=\"\">\n<p></p>\n<p></p>\n<p>softmaxdropout</p>\n<p>attention <span class=\"math inline\">\\(x\\)</span>\n <span class=\"math inline\">\\(QKV\\)</span>  <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(QK\\)</span>  <span class=\"math inline\">\\(QK\\)</span> softmax\n<span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2\\)</span> \n<span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(l\\)</span> </p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n4k32k64GPUbatch\nsizegradient\naccumulation<big><u><strong></strong></u></big></p>\n<p>2B7B16k32k200k34B70B+</p>\n<p>2k4k</p>\n<p></p>\n<h2 id=\"-position-interpolation\"> Position\nInterpolation</h2>\n<p>236Meta<a href=\"https://arxiv.org/pdf/2306.15595.pdf\">EXTENDING CONTEXT WINDOW\nOF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION</a>RoPEPIPosition\nInterpolation2k32k1kstep</p>\n<img src=\"/c4da56c0/meta_pi.png\" class title=\"PI\">\n\n<blockquote>\n<p>In contrast, LLaMA models that are extended via direct fine-tuning\nonly saw a minimal increase of the effective context window size kmax\nfrom 2048 to 2560, even after fine-tuning for more than 10000 steps,\nwith no clear indication of an acceleration in the increase of window\nsize.</p>\n</blockquote>\n<p>1w20482560</p>\n<p>RoPE</p>\n<p>RoPERoPE\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n&lt;2048attention\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n</p>\n<img src=\"/c4da56c0/meta_rope_ext.png\" class title=\"RoPE\">\n<p>3000attention\nscore</p>\n<p></p>\n<p>PI</p>\n<img src=\"/c4da56c0/meta_pi_nosft.png\" class title=\"PI\">\n<p>2k2k2k4k</p>\n<img src=\"/c4da56c0/meta_pi_explanation.png\" class title=\"PI\">\n<p>123...11.522.5...0.5</p>\n<p>attention\nscore <span class=\"math inline\">\\(\\tilde{a}(s)=a(Ls/L^{\\prime})\\)</span> <span class=\"math inline\">\\(L\\)</span> 2048<span class=\"math inline\">\\(L^{\\prime}\\)</span>\n8k/16k/32k</p>\n<p>RoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}&#39;(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L&#39;}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(m\\)</span> 1\n<span class=\"math inline\">\\({L}/{L&#39;}\\)</span></p>\n<p></p>\n<p></p>\n<p></p>\n<h2 id=\"ntk-aware-interpolation\">NTK-Aware Interpolation</h2>\n<p>cosNTK-Aware\nInterpolationRoPE<u><strong></strong></u>NTK-Aware\nScaled RoPECodeLlama1M</p>\n<p>NTKNeural Tangent\nKernelGLM4</p>\n<blockquote>\n<p>Neural Tangent Kernel (NTK)\nNTK\n<br>\n\nNeural Tangent\nKernel<br>\nNTK\n<br>\nNTK\n</p>\n</blockquote>\n<p>NTK</p>\n<p></p>\n<p>RoPE <span class=\"math inline\">\\(m\\)</span>\n</p>\n<img src=\"/c4da56c0/rope_matrix.png\" class title=\"RoPE\">\n<p>22 <span class=\"math inline\">\\(d/2\\)</span>\n\n<span class=\"math inline\">\\(\\theta_j=10000^{-2j/d}\\)</span> \n<span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(base=10000\\)</span>  <span class=\"math inline\">\\(base\\)</span>\n</p>\n<p></p>\n<p></p>\n<p><a href=\"https://www.zhihu.com/people/us4ever\">RoPE</a>2</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=\n\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&amp;\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*\\)</span>\n<span class=\"math inline\">\\(s=m-n\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha=L&#39;/L&gt;1\\)</span>\n <span class=\"math inline\">\\(s\\)</span> </p>\n<p>NTK-Aware Scaled RoPE <span class=\"math inline\">\\(\\theta_j\\)</span>\nbaseRoPE10000</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\theta\\)</span>  <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>  <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>\n1 <span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(j\\)</span> 0\n<span class=\"math inline\">\\(d/2 - 1\\)</span><span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>  <span class=\"math inline\">\\(\\alpha^{-1}\\)</span> </p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/645770522\"></a>NTK-Aware\nInterpolation</p>\n<blockquote>\n<p>RoPE\n12 3 60 \nRoPE 1/60 \n1/60 4 RoPE\nNTK-Aware\nRoPE  1.5\n 2  90  24\n 129.6k  43.2k\n</p>\n</blockquote>\n<p>RoPE<a href=\"https://kexue.fm/archives/9675\"></a></p>\n<p>YaRN<a href=\"https://arxiv.org/pdf/2309.00071.pdf\"></a>NTK</p>\n<blockquote>\n<p>Given the results from [6], this method performs much better at\nextending the context size of non-finetuned models compared to PI [9].\nHowever, one major disadvantage of this method is that given it is not\njust an interpolation scheme, some dimensions are slightly extrapolated\nto \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\"\ninterpolation [6] yields inferior results to PI [9]. Furthermore, due to\nthe \"out-of-bound\" values, the theoretical scale factor s does not\naccurately describe the true context extension scale. In practice, the\nscale value s has to be set higher than the expected scale for a given\ncontext length extension.</p>\n</blockquote>\n<p>NTK4k32k\n<span class=\"math inline\">\\(\\alpha=L&#39;/L\\)</span>\n816</p>\n<h2 id=\"ntk-by-parts\">NTK-by-parts</h2>\n<p>NTK-by-partsNTKNTK-awareRoPENTK-by-parts</p>\n<p> <span class=\"math inline\">\\(j\\)</span> RoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p><span class=\"math inline\">\\(\\lambda_j\\)</span>\n <span class=\"math inline\">\\(j\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(L\\)</span>\nRoPE\n<span class=\"math inline\">\\(sin\\)</span>\n1/40<sub>1-1</sub>0\n<span class=\"math inline\">\\(j\\)</span>\n</p>\n<p>NTK-by-parts</p>\n<ul>\n<li> <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(\\lambda_j\\)</span> \n<br>\n</li>\n<li> <span class=\"math inline\">\\(\\lambda_j\\geq\\)</span>\n<br>\n</li>\n<li>NTK-aware interpolation</li>\n</ul>\n<p> <span class=\"math inline\">\\(r(j)=\\frac{L}{\\lambda_j}\\)</span>\n <span class=\"math inline\">\\(\\beta_1\\beta_2\\)</span> \n<span class=\"math inline\">\\(r(j)&lt;\\beta_1\\)</span>\n <span class=\"math inline\">\\(r(j)\\geq\n\\beta_2\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&amp;if&amp;r(j)&lt;\\beta_1\\\\1&amp;if&amp;r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&amp;otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>NTK-by-parts <span class=\"math inline\">\\(\\theta_j\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\beta_1\\beta_2\\)</span>\n <span class=\"math inline\">\\(\\beta_1=1\\beta_2=32\\)</span>\n1/32</p>\n<h2 id=\"dynamically-ntk-scaled-rope\">Dynamically NTK Scaled RoPE</h2>\n<p>NTK-Aware\nInterpolationRoPEattention\nscore <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(L\\)</span>  <span class=\"math inline\">\\(\\alpha\\)</span>\nbaseDynamically NTK Scaled\nRoPENTK</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(l\\)</span> \n<span class=\"math inline\">\\(l&gt;L\\)</span>  <span class=\"math inline\">\\(\\alpha\\)</span> 1 <span class=\"math inline\">\\(l\\leq L\\)</span> </p>\n<p>kv-cacheRoPE</p>\n<h2 id=\"yarn\">YaRN</h2>\n<p>tokensoftmaxRoPEtoken</p>\n<p>RoPEsoftmaxlogitsoftmax\n<span class=\"math inline\">\\(t&gt;1\\)</span>\nRoPE <span class=\"math inline\">\\(\\sqrt{t}\\)</span>\nRoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>Llama 1Llama 2<span class=\"math inline\">\\(\\begin{aligned}\\sqrt{\\frac1t}&amp;=0.1\\ln(\\alpha)+1.\\end{aligned}\\)</span>Llama</p>\n<p>YaRNNTK-by-partsattention\nscore</p>\n<p>YaRN</p>\n<h2 id=\"logn\">logn</h2>\n<p>lognattention <span class=\"math inline\">\\(\\sqrt{d}\\)</span>\nlogn<a href=\"https://zhuanlan.zhihu.com/p/678755776\"></a>YaRN</p>\n<p>tokentokentokenattention\nscore</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L&#39;}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(L&#39;&gt;L\\)</span>\nYaRN</p>\n<h2 id=\"\"></h2>\n<p>window\nattentionstreaming LLMLongLoRAFocus\nTransformer</p>\n<h1 id=\"\"></h1>\n<p>2k4k</p>\n<ul>\n<li><br>\n</li>\n<li>token</li>\n</ul>\n<p>attention score</p>\n<p>PINTKNTKlognYaRN</p>\n<h1 id=\"reference\">Reference</h1>\n<p>1transformerKV cache\nhttps://zhuanlan.zhihu.com/p/624740065<br>\n2EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION https://arxiv.org/pdf/2306.15595.pdf<br>\n3Transformer10RoPE\nhttps://kexue.fm/archives/9675<br>\n4YaRN: Efficient Context Window Extension of Large Language Models\nhttps://arxiv.org/pdf/2309.00071.pdf<br>\n5RoPE\nhttps://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ<br>\n6LLM https://zhuanlan.zhihu.com/p/645770522\n7prompt\nhttps://cloud.tencent.com/developer/article/2330611<br>\n8Transformer8\nhttps://spaces.ac.cn/archives/9444<br>\n9RoPE192K\nhttps://zhuanlan.zhihu.com/p/678755776 ***</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n","length":12689,"excerpt":"","more":"<p></p>\n<p>RoPERoPE<a href=\"http://www.linsight.cn/a051710f.html\"></a> <a href=\"https://zhuanlan.zhihu.com/p/684072868\"></a> <a href=\"https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&amp;mid=2247483759&amp;idx=1&amp;sn=f7b59b879476b8687a340606b5568eae&amp;chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&amp;token=88551061&amp;lang=zh_CN#rd\"></a></p>\n<h1 id=\"\"></h1>\n<p>2023LLM20235Claude100k\ntokens67ChatGPT3.516kChatGLM2-B32k</p>\n<p>ChatGLMAgentChatGLM3ChatGLM4</p>\n<p>LM-SYSLongChatMosaicLMMPT16k</p>\n<p>Qwen-1.532k</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Baichuan2</td>\n<td style=\"text-align: center;\">192k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">GPT4-turbo</td>\n<td style=\"text-align: center;\">128k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Yi</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Kimi Chat</td>\n<td style=\"text-align: center;\">128k(20)</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Claude2</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p></p>\n<p></p>\n<h1 id=\"\"></h1>\n<p>tokenizertokentoken&gt;1.5tokenizer2200ktoken30w</p>\n<p>27</p>\n<p><big><u><strong></strong></u></big></p>\n<p>RAGRetrieval-augmented\ngenerationRAG</p>\n<p><big><u><strong></strong></u></big>prompt</p>\n<p>prompt</p>\n<p>1ppl2attention</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>2k/4k8k16kPPLRoPE<u><strong></strong></u></p>\n<h2 id=\"\"></h2>\n<p>2k/4k8k/16k/32k+</p>\n<p></p>\n<p>1.</p>\n<p>32k</p>\n<p>4k8attention\nmaskattention\nmask</p>\n<p>&gt;</p>\n<p>2.</p>\n<p>transformer</p>\n<p> <span class=\"math inline\">\\(l\\)</span> \n<span class=\"math inline\">\\(V\\)</span> hidden size <span class=\"math inline\">\\(h\\)</span> batch size <span class=\"math inline\">\\(b\\)</span>  <span class=\"math inline\">\\(s\\)</span>\nAdam1</p>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p> <span class=\"math inline\">\\(\\Phi\\)</span> = \n+ <span class=\"math inline\">\\(l\\)</span> * decoder = <span class=\"math inline\">\\(Vh + l(12h^2 + 13h)\\)</span></p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p> = logits + <span class=\"math inline\">\\(l\\)</span> *  <span class=\"math inline\">\\(\\approx2bshV + l*(24bsh^2+4bs^2h)\\)</span></p>\n<p>softmaxsoftmax <span class=\"math inline\">\\(s\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{}{}\n&amp;=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&amp;\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(s &lt;&lt; h\\)</span>\n <span class=\"math inline\">\\(h\\)</span> 1k1w\n<span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(sh\\)</span>\n</p>\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>optimizer</p>\n<p><span class=\"math inline\">\\(\\Phi\\)</span><span class=\"math inline\">\\(\\Phi\\)</span><span class=\"math inline\">\\(2\\Phi\\)</span>\n<span class=\"math inline\">\\((\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi +\n2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]\\)</span>\n</p>\n<img src=\"/c4da56c0/mix_precision_fp16.png\" class title=\"\">\n<p></p>\n<p></p>\n<p>softmaxdropout</p>\n<p>attention <span class=\"math inline\">\\(x\\)</span>\n <span class=\"math inline\">\\(QKV\\)</span>  <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(QK\\)</span>  <span class=\"math inline\">\\(QK\\)</span> softmax\n<span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2\\)</span> \n<span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(l\\)</span> </p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n4k32k64GPUbatch\nsizegradient\naccumulation<big><u><strong></strong></u></big></p>\n<p>2B7B16k32k200k34B70B+</p>\n<p>2k4k</p>\n<p></p>\n<h2 id=\"-position-interpolation\"> Position\nInterpolation</h2>\n<p>236Meta<a href=\"https://arxiv.org/pdf/2306.15595.pdf\">EXTENDING CONTEXT WINDOW\nOF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION</a>RoPEPIPosition\nInterpolation2k32k1kstep</p>\n<img src=\"/c4da56c0/meta_pi.png\" class title=\"PI\">\n\n<blockquote>\n<p>In contrast, LLaMA models that are extended via direct fine-tuning\nonly saw a minimal increase of the effective context window size kmax\nfrom 2048 to 2560, even after fine-tuning for more than 10000 steps,\nwith no clear indication of an acceleration in the increase of window\nsize.</p>\n</blockquote>\n<p>1w20482560</p>\n<p>RoPE</p>\n<p>RoPERoPE\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n&lt;2048attention\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n</p>\n<img src=\"/c4da56c0/meta_rope_ext.png\" class title=\"RoPE\">\n<p>3000attention\nscore</p>\n<p></p>\n<p>PI</p>\n<img src=\"/c4da56c0/meta_pi_nosft.png\" class title=\"PI\">\n<p>2k2k2k4k</p>\n<img src=\"/c4da56c0/meta_pi_explanation.png\" class title=\"PI\">\n<p>123...11.522.5...0.5</p>\n<p>attention\nscore <span class=\"math inline\">\\(\\tilde{a}(s)=a(Ls/L^{\\prime})\\)</span> <span class=\"math inline\">\\(L\\)</span> 2048<span class=\"math inline\">\\(L^{\\prime}\\)</span>\n8k/16k/32k</p>\n<p>RoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}&#39;(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L&#39;}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(m\\)</span> 1\n<span class=\"math inline\">\\({L}/{L&#39;}\\)</span></p>\n<p></p>\n<p></p>\n<p></p>\n<h2 id=\"ntk-aware-interpolation\">NTK-Aware Interpolation</h2>\n<p>cosNTK-Aware\nInterpolationRoPE<u><strong></strong></u>NTK-Aware\nScaled RoPECodeLlama1M</p>\n<p>NTKNeural Tangent\nKernelGLM4</p>\n<blockquote>\n<p>Neural Tangent Kernel (NTK)\nNTK\n<br>\n\nNeural Tangent\nKernel<br>\nNTK\n<br>\nNTK\n</p>\n</blockquote>\n<p>NTK</p>\n<p></p>\n<p>RoPE <span class=\"math inline\">\\(m\\)</span>\n</p>\n<img src=\"/c4da56c0/rope_matrix.png\" class title=\"RoPE\">\n<p>22 <span class=\"math inline\">\\(d/2\\)</span>\n\n<span class=\"math inline\">\\(\\theta_j=10000^{-2j/d}\\)</span> \n<span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(base=10000\\)</span>  <span class=\"math inline\">\\(base\\)</span>\n</p>\n<p></p>\n<p></p>\n<p><a href=\"https://www.zhihu.com/people/us4ever\">RoPE</a>2</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=\n\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&amp;\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*\\)</span>\n<span class=\"math inline\">\\(s=m-n\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha=L&#39;/L&gt;1\\)</span>\n <span class=\"math inline\">\\(s\\)</span> </p>\n<p>NTK-Aware Scaled RoPE <span class=\"math inline\">\\(\\theta_j\\)</span>\nbaseRoPE10000</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\theta\\)</span>  <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>  <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>\n1 <span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(j\\)</span> 0\n<span class=\"math inline\">\\(d/2 - 1\\)</span><span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>  <span class=\"math inline\">\\(\\alpha^{-1}\\)</span> </p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/645770522\"></a>NTK-Aware\nInterpolation</p>\n<blockquote>\n<p>RoPE\n12 3 60 \nRoPE 1/60 \n1/60 4 RoPE\nNTK-Aware\nRoPE  1.5\n 2  90  24\n 129.6k  43.2k\n</p>\n</blockquote>\n<p>RoPE<a href=\"https://kexue.fm/archives/9675\"></a></p>\n<p>YaRN<a href=\"https://arxiv.org/pdf/2309.00071.pdf\"></a>NTK</p>\n<blockquote>\n<p>Given the results from [6], this method performs much better at\nextending the context size of non-finetuned models compared to PI [9].\nHowever, one major disadvantage of this method is that given it is not\njust an interpolation scheme, some dimensions are slightly extrapolated\nto \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\"\ninterpolation [6] yields inferior results to PI [9]. Furthermore, due to\nthe \"out-of-bound\" values, the theoretical scale factor s does not\naccurately describe the true context extension scale. In practice, the\nscale value s has to be set higher than the expected scale for a given\ncontext length extension.</p>\n</blockquote>\n<p>NTK4k32k\n<span class=\"math inline\">\\(\\alpha=L&#39;/L\\)</span>\n816</p>\n<h2 id=\"ntk-by-parts\">NTK-by-parts</h2>\n<p>NTK-by-partsNTKNTK-awareRoPENTK-by-parts</p>\n<p> <span class=\"math inline\">\\(j\\)</span> RoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p><span class=\"math inline\">\\(\\lambda_j\\)</span>\n <span class=\"math inline\">\\(j\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(L\\)</span>\nRoPE\n<span class=\"math inline\">\\(sin\\)</span>\n1/40<sub>1-1</sub>0\n<span class=\"math inline\">\\(j\\)</span>\n</p>\n<p>NTK-by-parts</p>\n<ul>\n<li> <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(\\lambda_j\\)</span> \n<br>\n</li>\n<li> <span class=\"math inline\">\\(\\lambda_j\\geq\\)</span>\n<br>\n</li>\n<li>NTK-aware interpolation</li>\n</ul>\n<p> <span class=\"math inline\">\\(r(j)=\\frac{L}{\\lambda_j}\\)</span>\n <span class=\"math inline\">\\(\\beta_1\\beta_2\\)</span> \n<span class=\"math inline\">\\(r(j)&lt;\\beta_1\\)</span>\n <span class=\"math inline\">\\(r(j)\\geq\n\\beta_2\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&amp;if&amp;r(j)&lt;\\beta_1\\\\1&amp;if&amp;r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&amp;otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>NTK-by-parts <span class=\"math inline\">\\(\\theta_j\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\beta_1\\beta_2\\)</span>\n <span class=\"math inline\">\\(\\beta_1=1\\beta_2=32\\)</span>\n1/32</p>\n<h2 id=\"dynamically-ntk-scaled-rope\">Dynamically NTK Scaled RoPE</h2>\n<p>NTK-Aware\nInterpolationRoPEattention\nscore <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(L\\)</span>  <span class=\"math inline\">\\(\\alpha\\)</span>\nbaseDynamically NTK Scaled\nRoPENTK</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(l\\)</span> \n<span class=\"math inline\">\\(l&gt;L\\)</span>  <span class=\"math inline\">\\(\\alpha\\)</span> 1 <span class=\"math inline\">\\(l\\leq L\\)</span> </p>\n<p>kv-cacheRoPE</p>\n<h2 id=\"yarn\">YaRN</h2>\n<p>tokensoftmaxRoPEtoken</p>\n<p>RoPEsoftmaxlogitsoftmax\n<span class=\"math inline\">\\(t&gt;1\\)</span>\nRoPE <span class=\"math inline\">\\(\\sqrt{t}\\)</span>\nRoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>Llama 1Llama 2<span class=\"math inline\">\\(\\begin{aligned}\\sqrt{\\frac1t}&amp;=0.1\\ln(\\alpha)+1.\\end{aligned}\\)</span>Llama</p>\n<p>YaRNNTK-by-partsattention\nscore</p>\n<p>YaRN</p>\n<h2 id=\"logn\">logn</h2>\n<p>lognattention <span class=\"math inline\">\\(\\sqrt{d}\\)</span>\nlogn<a href=\"https://zhuanlan.zhihu.com/p/678755776\"></a>YaRN</p>\n<p>tokentokentokenattention\nscore</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L&#39;}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(L&#39;&gt;L\\)</span>\nYaRN</p>\n<h2 id=\"\"></h2>\n<p>window\nattentionstreaming LLMLongLoRAFocus\nTransformer</p>\n<h1 id=\"\"></h1>\n<p>2k4k</p>\n<ul>\n<li><br>\n</li>\n<li>token</li>\n</ul>\n<p>attention score</p>\n<p>PINTKNTKlognYaRN</p>\n<h1 id=\"reference\">Reference</h1>\n<p>1transformerKV cache\nhttps://zhuanlan.zhihu.com/p/624740065<br>\n2EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION https://arxiv.org/pdf/2306.15595.pdf<br>\n3Transformer10RoPE\nhttps://kexue.fm/archives/9675<br>\n4YaRN: Efficient Context Window Extension of Large Language Models\nhttps://arxiv.org/pdf/2309.00071.pdf<br>\n5RoPE\nhttps://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ<br>\n6LLM https://zhuanlan.zhihu.com/p/645770522\n7prompt\nhttps://cloud.tencent.com/developer/article/2330611<br>\n8Transformer8\nhttps://spaces.ac.cn/archives/9444<br>\n9RoPE192K\nhttps://zhuanlan.zhihu.com/p/678755776 ***</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n"},{"title":"normalization-","abbrlink":"b70b4a2d","date":"2024-04-06T04:24:25.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)normalizationnorm  \n\nnorm  \n\n[https://github.com/Saicat/normalization_exp](https://github.com/Saicat/normalization_exp)\n\n# \n\nnormalizationCNN  \n\n```python\nimport torch\nfrom torch import nn\n\n# epsilon\neps = 1e-8\n\n# \nbatch_size = 3\nfeature_num = 4\ntorch.manual_seed(0)  # \ninputs = torch.randn(batch_size, feature_num)\nprint(':\\n', inputs)\n```\n\n34batch size=34  \n\n```python\n:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\n```\n\n## batchnorm  \n\npytorchBatchNorm1d  \n\n```python\n# torchbatchnorm\ntorch_bn = nn.BatchNorm1d(num_features=feature_num, affine=True)  # batchnorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(1)  # \ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(feature_num))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(feature_num))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_bn(inputs)\nprint('torch bn:\\n', torch_normed)\n```\n\nbatchnorm/layernorm+BatchNorm\"affine\"\"affine\"False  \n\npytorchnorm1.00  \n\n```python\nweight:\n Parameter containing:\ntensor([0.6614, 0.2669, 0.0617, 0.6213], requires_grad=True)\nbias:\n Parameter containing:\ntensor([-0.4519, -0.1661, -1.5228,  0.3817], requires_grad=True) \n```\n\nweightbias4  \n\nbatchnorm  \n\n```python\ntorch bn:\n tensor([[ 0.4756,  0.0513, -1.6033,  0.4715],\n        [-1.0197, -0.5421, -1.4535,  1.0937],\n        [-0.8117, -0.0077, -1.5115, -0.4202]],\n       grad_fn=<NativeBatchNormBackward0>)\n```\n\nbatchnorm  \n\n```python\n# bn\n\n# \nmean = torch.mean(inputs, dim=0, keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=0, keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias\nprint('bn:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(isclose)\n```\n\ndim=0batchsamplefeature    \n\n```python\n:\n tensor([[-0.0876, -0.6985, -0.7907,  0.5295]])\n:\n tensor([[1.1612, 0.4971, 1.0630, 0.2692]]) \n```\n\nmeanstdkeepdimTrueFalsebroadcast  \n\nstdunbiasedFalsetorchbatchnorm  \n\nbatchnorm  \n\n```python\nbn:\n tensor([[ 0.4756,  0.0514, -1.6033,  0.4715],\n        [-1.0197, -0.5421, -1.4535,  1.0937],\n        [-0.8117, -0.0077, -1.5115, -0.4202]], grad_fn=<AddBackward0>)\n```\n\ntorch.isclosebatchnormbatchnorm  \n\n```python\ntensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\nequal1e-5~1e-4eps  \n\n## layernorm  \n\nlayernorm34\n\ntorch\n\n```python\n# torchlayernorm\ntorch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=True)  # layernorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(2)  # \ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(feature_num))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(feature_num))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n```\n\nlayernorm  \n\n```python\nweight:\n Parameter containing:\ntensor([ 0.3923, -0.2236, -0.3195, -1.2050], requires_grad=True)\nbias:\n Parameter containing:\ntensor([ 1.0445, -0.6332,  0.5731,  0.5409], requires_grad=True) \n```\n  \n\nlayernorm  \n\n```python\ntorch ln:\n tensor([[ 1.5120, -0.6001,  1.0604, -0.0392],\n        [ 0.7249, -0.3772,  0.3331, -0.9155],\n        [ 0.6645, -0.6209,  0.7693, -1.4324]],\n       grad_fn=<NativeLayerNormBackward0>)\n```\n\n  \n\ndim=1  \n\n```python\n# ln\n\n# \nmean = torch.mean(inputs, dim=1, keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=1, keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\nprint('ln:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(isclose)\n```\n\n  \n\n```python\n:\n tensor([[-0.0907],\n        [-0.3104],\n        [-0.3843]])\n:\n tensor([[1.3691],\n        [0.9502],\n        [0.3458]]) \n```\n\n  \n\n```python\nln:\n tensor([[ 1.5120, -0.6001,  1.0604, -0.0392],\n        [ 0.7249, -0.3772,  0.3331, -0.9155],\n        [ 0.6645, -0.6209,  0.7693, -1.4325]], grad_fn=<AddBackward0>)\n:\n tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\n##   \n\nbatchnormlayernorm  \n\n{% asset_img bn_and_ln.png bnln %}  \n\nbatchnormdim=0batchlayernormdim=1  \n\nbatchnormlayernorm  \n\n# CV\n\nCV  \n\nCV[N,C,H,W]Nbatch sizeCchannelHWfeature mapCV  \n\n```python\n# [N,C,H,W]\nbatch_size = 2\nchannel = 2\nheight = 2\nwidth = 3\ntorch.manual_seed(3)  # \ninputs = torch.randn(batch_size, channel, height, width)\nprint(':\\n', inputs)\n```\n\n  \n\n```python\n:\n tensor([[[[-0.0766,  0.3599, -0.7820],\n          [ 0.0715,  0.6648, -0.2868]],\n\n         [[ 1.6206, -1.5967,  0.4046],\n          [ 0.6113,  0.7604, -0.0336]]],\n\n\n        [[[-0.3448,  0.4937, -0.0776],\n          [-1.8054,  0.4851,  0.2052]],\n\n         [[ 0.3384,  1.3528,  0.3736],\n          [ 0.0134,  0.7737, -0.1092]]]])\n```\n\n## batchnorm  \n\nBatchNorm2dchannel  \n\n```python\n# torchbatchnorm\ntorch_bn = nn.BatchNorm2d(num_features=channel, affine=True)  # batchnorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(4)  # \ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(channel))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(channel))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_bn(inputs)\nprint('torch bn:\\n', torch_normed)\n```\n\nchannel  \n\n```python\nweight:\n Parameter containing:\ntensor([-1.6053,  0.2325], requires_grad=True)\nbias:\n Parameter containing:\ntensor([2.2399, 0.8473], requires_grad=True) \n```\n\ntorchbatchnorm2d  \n\n```python\ntorch bn:\n tensor([[[[2.2043, 1.1275, 3.9442],\n          [1.8388, 0.3753, 2.7226]],\n\n         [[1.2185, 0.2591, 0.8559],\n          [0.9175, 0.9620, 0.7252]]],\n\n\n        [[[2.8658, 0.7975, 2.2066],\n          [6.4684, 0.8186, 1.5090]],\n\n         [[0.8362, 1.1387, 0.8467],\n          [0.7392, 0.9660, 0.7027]]]], grad_fn=<NativeBatchNormBackward0>)\n```\n\nbatchnorm2d  \n\n```python\n# bn\n\nmanual_normed = []\n# channel\nfor c in range(channel):\n    # \n    mean = torch.mean(inputs[:, c, :, :])\n    std = torch.std(inputs[:, c, :, :], unbiased=False)\n    normed = (inputs[:, c, :, :] - mean) / (std + eps) * torch_bn.weight[c] + torch_bn.bias[c]\n    normed = normed.unsqueeze(1)\n    manual_normed.append(normed)\nmanual_normed = torch.cat(manual_normed, 1)\nprint('bn:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\nCVchannel\"batch\"  \n\nNHW  \n\n{% asset_img cv_batchnorm.png CVbatchnorm %}  \n\n  \n\n```python\nbn:\n tensor([[[[2.2043, 1.1275, 3.9442],\n          [1.8388, 0.3752, 2.7226]],\n\n         [[1.2185, 0.2591, 0.8559],\n          [0.9175, 0.9620, 0.7252]]],\n\n\n        [[[2.8658, 0.7975, 2.2066],\n          [6.4685, 0.8186, 1.5089]],\n\n         [[0.8362, 1.1387, 0.8467],\n          [0.7392, 0.9660, 0.7027]]]], grad_fn=<CatBackward0>)\n:\n tensor([[[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]],\n\n\n        [[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]]])\n```\n\n## layernorm  \n\n[torchlayernorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)layernorm  \n\n```python\n# torchlayernorm\ntorch_ln = nn.LayerNorm(\n    normalized_shape=[channel, height, width], \n    elementwise_affine=True\n)  # layernorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(5)  # \ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(channel, height, width))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(channel, height, width))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n```\n\n\n\n{% asset_img cv_layernorm.jpeg CVlayernorm %}  \n\n[channel, height, width]  \n\n```python\nweight:\n Parameter containing:\ntensor([[[-0.4868, -0.6038, -0.5581],\n         [ 0.6675, -0.1974,  1.9428]],\n\n        [[-1.4017, -0.7626,  0.6312],\n         [-0.8991, -0.5578,  0.6907]]], requires_grad=True)\nbias:\n Parameter containing:\ntensor([[[ 0.2225, -0.6662,  0.6846],\n         [ 0.5740, -0.5829,  0.7679]],\n\n        [[ 0.0571, -1.1894, -0.5659],\n         [-0.8327,  0.9014,  0.2116]]], requires_grad=True) \n```\n\nchannel  \n\nlayernorm  \n\n```python\ntorch ln:\n tensor([[[[ 0.3594, -0.8338,  1.3456],\n          [ 0.5128, -0.7147, -0.3012]],\n\n         [[-2.5939,  0.5089, -0.3546],\n          [-1.3715,  0.4607,  0.0553]]],\n\n\n        [[[ 0.5477, -0.9583,  0.8526],\n          [-1.2112, -0.6760,  0.9378]],\n\n         [[-0.3219, -2.4580, -0.3647],\n          [-0.6744,  0.4171, -0.0264]]]], grad_fn=<NativeLayerNormBackward0>)\n```\n\nlayernorm  \n\n```python\n# ln\n\nmanual_normed = []\n# channel\nfor b in range(batch_size):\n    # \n    mean = torch.mean(inputs[b, :, :, :])\n    std = torch.std(inputs[b, :, :, :], unbiased=False)\n    normed = (inputs[b, :, :, :] - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\n    normed = normed.unsqueeze(0)\n    manual_normed.append(normed)\nmanual_normed = torch.cat(manual_normed, 0)\nprint('ln:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\nchannel  \n\n  \n\n```python\nln:\n tensor([[[[ 0.3594, -0.8338,  1.3456],\n          [ 0.5128, -0.7147, -0.3012]],\n\n         [[-2.5939,  0.5090, -0.3546],\n          [-1.3715,  0.4607,  0.0553]]],\n\n\n        [[[ 0.5477, -0.9583,  0.8527],\n          [-1.2112, -0.6760,  0.9378]],\n\n         [[-0.3219, -2.4581, -0.3647],\n          [-0.6744,  0.4171, -0.0264]]]], grad_fn=<CatBackward0>)\n:\n tensor([[[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]],\n\n\n        [[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]]])\n```\n\n# NLP  \n\nNLP  \n\nNbatch sizeSsequence lengthHhidden size  \n\n```python\n# [N,S,H]\nbatch_size = 2\nseq_len = 3\nhidden_size = 4\ntorch.manual_seed(6)  # \ninputs = torch.randn(batch_size, seq_len, hidden_size)\nprint(':\\n', inputs)\n```\n\n## batchnorm  \n\n  \n\n```python\n# torchbatchnorm\ntorch_bn = nn.BatchNorm1d(num_features=hidden_size, affine=True)  # batchnorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(7)  # \ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(hidden_size))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(hidden_size))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# # \ntorch_normed = torch_bn(inputs.transpose(1, 2)).transpose(1, 2)\nprint('torch bn:\\n', torch_normed)\n```\n\ntransposetranspose  \n\n  \n\n```python\nweight:\n Parameter containing:\ntensor([-0.1468,  0.7861,  0.9468, -1.1143], requires_grad=True)\nbias:\n Parameter containing:\ntensor([ 1.6908, -0.8948, -0.3556,  1.2324], requires_grad=True) \n\ntorch bn:\n tensor([[[ 1.8740, -0.7037, -1.8222,  2.3385],\n         [ 1.7413, -1.8119,  0.3641,  0.0200],\n         [ 1.4615, -0.2676,  0.1081,  1.3450]],\n\n        [[ 1.7084, -1.9653,  1.0169,  0.5785],\n         [ 1.8213, -0.8614, -0.8056,  2.9892],\n         [ 1.5383,  0.2409, -0.9949,  0.1231]]], grad_fn=<TransposeBackward0>)\n```\n\nbatchnorm  \n\n  \n\n```python\n# bn\n\n# \nmean = torch.mean(inputs, dim=(0, 1) , keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=(0, 1), keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias\nprint('bn:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\ndim=(0,1)[N, S, H][NS, H]batchnorm  \n\n  \n\n```python\n:\n tensor([[[-0.2151,  0.5444, -0.2633, -0.5424]]])\n:\n tensor([[[0.7984, 0.3537, 0.7799, 0.7986]]]) \n\nbn:\n tensor([[[ 1.8740, -0.7037, -1.8222,  2.3385],\n         [ 1.7413, -1.8119,  0.3641,  0.0200],\n         [ 1.4615, -0.2676,  0.1081,  1.3450]],\n\n        [[ 1.7084, -1.9653,  1.0169,  0.5785],\n         [ 1.8213, -0.8614, -0.8056,  2.9892],\n         [ 1.5383,  0.2409, -0.9950,  0.1231]]], grad_fn=<AddBackward0>)\n:\n tensor([[[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]],\n\n        [[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]]])\n```\n\n## layernorm  \n\nNLPlayernormhuggingfacebertlayernorm  \n\n```python\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states: torch.normTensor, input_tensor: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n```\n\n  \n\n```python\n# torchlayernorm\ntorch_ln = nn.LayerNorm(normalized_shape=hidden_size, elementwise_affine=True)  # layernorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(8)  # \ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(hidden_size))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(hidden_size))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n```\n\nhidden size  \n\n```python\nweight:\n Parameter containing:\ntensor([ 0.2713, -1.2729,  0.5027,  0.4181], requires_grad=True)\nbias:\n Parameter containing:\ntensor([-0.6394, -0.6608, -0.1433, -0.1043], requires_grad=True) \n\ntorch ln:\n tensor([[[-0.7547, -2.8528, -0.5092, -0.3423],\n         [-1.0957, -0.8780,  0.2388,  0.2097],\n         [-0.3502, -1.6158, -0.3133, -0.7224]],\n\n        [[-0.9134, -0.4490,  0.6868, -0.3029],\n         [-0.7116, -2.5589, -0.1039, -0.6493],\n         [-0.5076, -2.1031, -0.9346, -0.1230]]],\n       grad_fn=<NativeLayerNormBackward0>)\n```\n\n  \n\n```python\n# ln\n\n# \nmean = torch.mean(inputs, dim=2, keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=2, keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\nprint('ln:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\n  \n\n```python\n:\n tensor([[[-0.8469],\n         [ 0.0745],\n         [ 0.3386]],\n\n        [[ 0.1364],\n         [-0.7003],\n         [ 0.2831]]])\n:\n tensor([[[0.8578],\n         [0.3354],\n         [0.6505]],\n\n        [[0.4426],\n         [0.8448],\n         [0.6816]]]) \n```\n\nsampletoken  \n\n  \n\n```python\nln:\n tensor([[[-0.7547, -2.8528, -0.5092, -0.3423],\n         [-1.0957, -0.8780,  0.2388,  0.2097],\n         [-0.3502, -1.6158, -0.3133, -0.7224]],\n\n        [[-0.9134, -0.4490,  0.6868, -0.3029],\n         [-0.7116, -2.5590, -0.1039, -0.6493],\n         [-0.5076, -2.1031, -0.9347, -0.1230]]], grad_fn=<AddBackward0>)\n:\n tensor([[[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]],\n\n        [[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]]])\n```\n\n#   \n\n\n\nbatchnorm  \n\n```python\n# \nbatch_size = 3\nfeature_num = 4\ntorch.manual_seed(0)  # \ninputs = torch.randn(batch_size, feature_num)\nprint(':\\n', inputs)\n\n# \nmean = torch.mean(inputs, dim=0, keepdim=True)\n# print(':\\n', mean)\nstd = torch.std(inputs, dim=0, keepdim=True, unbiased=False)\n# print(':\\n', std, '\\n')\n\n# torchbatchnorm\ntorch_bn = nn.BatchNorm1d(num_features=feature_num, affine=True)\n\n# \ntorch_bn.weight = nn.Parameter(std)\ntorch_bn.bias =  nn.Parameter(mean)\n# print('weight:\\n', torch_bn.weight)\n# print('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_bn(inputs)\nprint('torch bn:\\n', torch_normed)\n\nisclose = torch.isclose(torch_normed, inputs, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\n  \n\n```python\n:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\ntorch bn:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1821]],\n       grad_fn=<NativeBatchNormBackward0>)\n:\n tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\nbatchnorm  \n\nlayernorm  \n\n```python\nprint(':\\n', inputs)\n\n# \nmean = torch.mean(inputs, dim=1, keepdim=True)\n# print(':\\n', mean)\nstd = torch.std(inputs, dim=1, keepdim=True, unbiased=False)\n# print(':\\n', std, '\\n')\n\n# torchlayernorm\ntorch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=True)  # layernorm\n\n# \ntorch_bn.weight = nn.Parameter(std)\ntorch_bn.bias =  nn.Parameter(mean)\n# print('weight:\\n', torch_bn.weight)\n# print('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n\nisclose = torch.isclose(torch_normed, inputs, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\n\n\n```python\n:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\ntorch ln:\n tensor([[ 1.1918, -0.1481, -1.5251,  0.4814],\n        [-0.8146, -1.1451,  0.7512,  1.2086],\n        [-0.9685, -0.0551, -0.6140,  1.6376]],\n       grad_fn=<NativeLayerNormBackward0>)\n:\n tensor([[False, False, False, False],\n        [False, False, False, False],\n        [False, False, False, False]])\n```\n\nlayernormlayernorm  \n\nCVNLP\n\nbatchnormlayernorm  \n\n# \n\nbatchnormlayernorm  \n\nbatchnormlayernorm  \n\nbatchnorm\"\"\"\"batchbatchbatchfeature map  \n\nlayernorm  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***\n\n# Reference  \n1LAYERNORM https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html  \n2BATCHNORM1D https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html  \n3BATCHNORM2D https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html  \n","source":"_posts/cs/nlp/2024/04/normalization-.md","raw":"---\ntitle: normalization-\nabbrlink: b70b4a2d\ndate: 2024-04-06 12:24:25\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - layernorm\n  - normalization\n  - batchnorm\ncategories:\n  - CS\n  - NLP\n  - LLM\n\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)normalizationnorm  \n\nnorm  \n\n[https://github.com/Saicat/normalization_exp](https://github.com/Saicat/normalization_exp)\n\n# \n\nnormalizationCNN  \n\n```python\nimport torch\nfrom torch import nn\n\n# epsilon\neps = 1e-8\n\n# \nbatch_size = 3\nfeature_num = 4\ntorch.manual_seed(0)  # \ninputs = torch.randn(batch_size, feature_num)\nprint(':\\n', inputs)\n```\n\n34batch size=34  \n\n```python\n:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\n```\n\n## batchnorm  \n\npytorchBatchNorm1d  \n\n```python\n# torchbatchnorm\ntorch_bn = nn.BatchNorm1d(num_features=feature_num, affine=True)  # batchnorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(1)  # \ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(feature_num))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(feature_num))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_bn(inputs)\nprint('torch bn:\\n', torch_normed)\n```\n\nbatchnorm/layernorm+BatchNorm\"affine\"\"affine\"False  \n\npytorchnorm1.00  \n\n```python\nweight:\n Parameter containing:\ntensor([0.6614, 0.2669, 0.0617, 0.6213], requires_grad=True)\nbias:\n Parameter containing:\ntensor([-0.4519, -0.1661, -1.5228,  0.3817], requires_grad=True) \n```\n\nweightbias4  \n\nbatchnorm  \n\n```python\ntorch bn:\n tensor([[ 0.4756,  0.0513, -1.6033,  0.4715],\n        [-1.0197, -0.5421, -1.4535,  1.0937],\n        [-0.8117, -0.0077, -1.5115, -0.4202]],\n       grad_fn=<NativeBatchNormBackward0>)\n```\n\nbatchnorm  \n\n```python\n# bn\n\n# \nmean = torch.mean(inputs, dim=0, keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=0, keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias\nprint('bn:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(isclose)\n```\n\ndim=0batchsamplefeature    \n\n```python\n:\n tensor([[-0.0876, -0.6985, -0.7907,  0.5295]])\n:\n tensor([[1.1612, 0.4971, 1.0630, 0.2692]]) \n```\n\nmeanstdkeepdimTrueFalsebroadcast  \n\nstdunbiasedFalsetorchbatchnorm  \n\nbatchnorm  \n\n```python\nbn:\n tensor([[ 0.4756,  0.0514, -1.6033,  0.4715],\n        [-1.0197, -0.5421, -1.4535,  1.0937],\n        [-0.8117, -0.0077, -1.5115, -0.4202]], grad_fn=<AddBackward0>)\n```\n\ntorch.isclosebatchnormbatchnorm  \n\n```python\ntensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\nequal1e-5~1e-4eps  \n\n## layernorm  \n\nlayernorm34\n\ntorch\n\n```python\n# torchlayernorm\ntorch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=True)  # layernorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(2)  # \ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(feature_num))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(feature_num))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n```\n\nlayernorm  \n\n```python\nweight:\n Parameter containing:\ntensor([ 0.3923, -0.2236, -0.3195, -1.2050], requires_grad=True)\nbias:\n Parameter containing:\ntensor([ 1.0445, -0.6332,  0.5731,  0.5409], requires_grad=True) \n```\n  \n\nlayernorm  \n\n```python\ntorch ln:\n tensor([[ 1.5120, -0.6001,  1.0604, -0.0392],\n        [ 0.7249, -0.3772,  0.3331, -0.9155],\n        [ 0.6645, -0.6209,  0.7693, -1.4324]],\n       grad_fn=<NativeLayerNormBackward0>)\n```\n\n  \n\ndim=1  \n\n```python\n# ln\n\n# \nmean = torch.mean(inputs, dim=1, keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=1, keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\nprint('ln:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(isclose)\n```\n\n  \n\n```python\n:\n tensor([[-0.0907],\n        [-0.3104],\n        [-0.3843]])\n:\n tensor([[1.3691],\n        [0.9502],\n        [0.3458]]) \n```\n\n  \n\n```python\nln:\n tensor([[ 1.5120, -0.6001,  1.0604, -0.0392],\n        [ 0.7249, -0.3772,  0.3331, -0.9155],\n        [ 0.6645, -0.6209,  0.7693, -1.4325]], grad_fn=<AddBackward0>)\n:\n tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\n##   \n\nbatchnormlayernorm  \n\n{% asset_img bn_and_ln.png bnln %}  \n\nbatchnormdim=0batchlayernormdim=1  \n\nbatchnormlayernorm  \n\n# CV\n\nCV  \n\nCV[N,C,H,W]Nbatch sizeCchannelHWfeature mapCV  \n\n```python\n# [N,C,H,W]\nbatch_size = 2\nchannel = 2\nheight = 2\nwidth = 3\ntorch.manual_seed(3)  # \ninputs = torch.randn(batch_size, channel, height, width)\nprint(':\\n', inputs)\n```\n\n  \n\n```python\n:\n tensor([[[[-0.0766,  0.3599, -0.7820],\n          [ 0.0715,  0.6648, -0.2868]],\n\n         [[ 1.6206, -1.5967,  0.4046],\n          [ 0.6113,  0.7604, -0.0336]]],\n\n\n        [[[-0.3448,  0.4937, -0.0776],\n          [-1.8054,  0.4851,  0.2052]],\n\n         [[ 0.3384,  1.3528,  0.3736],\n          [ 0.0134,  0.7737, -0.1092]]]])\n```\n\n## batchnorm  \n\nBatchNorm2dchannel  \n\n```python\n# torchbatchnorm\ntorch_bn = nn.BatchNorm2d(num_features=channel, affine=True)  # batchnorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(4)  # \ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(channel))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(channel))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_bn(inputs)\nprint('torch bn:\\n', torch_normed)\n```\n\nchannel  \n\n```python\nweight:\n Parameter containing:\ntensor([-1.6053,  0.2325], requires_grad=True)\nbias:\n Parameter containing:\ntensor([2.2399, 0.8473], requires_grad=True) \n```\n\ntorchbatchnorm2d  \n\n```python\ntorch bn:\n tensor([[[[2.2043, 1.1275, 3.9442],\n          [1.8388, 0.3753, 2.7226]],\n\n         [[1.2185, 0.2591, 0.8559],\n          [0.9175, 0.9620, 0.7252]]],\n\n\n        [[[2.8658, 0.7975, 2.2066],\n          [6.4684, 0.8186, 1.5090]],\n\n         [[0.8362, 1.1387, 0.8467],\n          [0.7392, 0.9660, 0.7027]]]], grad_fn=<NativeBatchNormBackward0>)\n```\n\nbatchnorm2d  \n\n```python\n# bn\n\nmanual_normed = []\n# channel\nfor c in range(channel):\n    # \n    mean = torch.mean(inputs[:, c, :, :])\n    std = torch.std(inputs[:, c, :, :], unbiased=False)\n    normed = (inputs[:, c, :, :] - mean) / (std + eps) * torch_bn.weight[c] + torch_bn.bias[c]\n    normed = normed.unsqueeze(1)\n    manual_normed.append(normed)\nmanual_normed = torch.cat(manual_normed, 1)\nprint('bn:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\nCVchannel\"batch\"  \n\nNHW  \n\n{% asset_img cv_batchnorm.png CVbatchnorm %}  \n\n  \n\n```python\nbn:\n tensor([[[[2.2043, 1.1275, 3.9442],\n          [1.8388, 0.3752, 2.7226]],\n\n         [[1.2185, 0.2591, 0.8559],\n          [0.9175, 0.9620, 0.7252]]],\n\n\n        [[[2.8658, 0.7975, 2.2066],\n          [6.4685, 0.8186, 1.5089]],\n\n         [[0.8362, 1.1387, 0.8467],\n          [0.7392, 0.9660, 0.7027]]]], grad_fn=<CatBackward0>)\n:\n tensor([[[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]],\n\n\n        [[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]]])\n```\n\n## layernorm  \n\n[torchlayernorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)layernorm  \n\n```python\n# torchlayernorm\ntorch_ln = nn.LayerNorm(\n    normalized_shape=[channel, height, width], \n    elementwise_affine=True\n)  # layernorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(5)  # \ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(channel, height, width))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(channel, height, width))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n```\n\n\n\n{% asset_img cv_layernorm.jpeg CVlayernorm %}  \n\n[channel, height, width]  \n\n```python\nweight:\n Parameter containing:\ntensor([[[-0.4868, -0.6038, -0.5581],\n         [ 0.6675, -0.1974,  1.9428]],\n\n        [[-1.4017, -0.7626,  0.6312],\n         [-0.8991, -0.5578,  0.6907]]], requires_grad=True)\nbias:\n Parameter containing:\ntensor([[[ 0.2225, -0.6662,  0.6846],\n         [ 0.5740, -0.5829,  0.7679]],\n\n        [[ 0.0571, -1.1894, -0.5659],\n         [-0.8327,  0.9014,  0.2116]]], requires_grad=True) \n```\n\nchannel  \n\nlayernorm  \n\n```python\ntorch ln:\n tensor([[[[ 0.3594, -0.8338,  1.3456],\n          [ 0.5128, -0.7147, -0.3012]],\n\n         [[-2.5939,  0.5089, -0.3546],\n          [-1.3715,  0.4607,  0.0553]]],\n\n\n        [[[ 0.5477, -0.9583,  0.8526],\n          [-1.2112, -0.6760,  0.9378]],\n\n         [[-0.3219, -2.4580, -0.3647],\n          [-0.6744,  0.4171, -0.0264]]]], grad_fn=<NativeLayerNormBackward0>)\n```\n\nlayernorm  \n\n```python\n# ln\n\nmanual_normed = []\n# channel\nfor b in range(batch_size):\n    # \n    mean = torch.mean(inputs[b, :, :, :])\n    std = torch.std(inputs[b, :, :, :], unbiased=False)\n    normed = (inputs[b, :, :, :] - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\n    normed = normed.unsqueeze(0)\n    manual_normed.append(normed)\nmanual_normed = torch.cat(manual_normed, 0)\nprint('ln:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\nchannel  \n\n  \n\n```python\nln:\n tensor([[[[ 0.3594, -0.8338,  1.3456],\n          [ 0.5128, -0.7147, -0.3012]],\n\n         [[-2.5939,  0.5090, -0.3546],\n          [-1.3715,  0.4607,  0.0553]]],\n\n\n        [[[ 0.5477, -0.9583,  0.8527],\n          [-1.2112, -0.6760,  0.9378]],\n\n         [[-0.3219, -2.4581, -0.3647],\n          [-0.6744,  0.4171, -0.0264]]]], grad_fn=<CatBackward0>)\n:\n tensor([[[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]],\n\n\n        [[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]]])\n```\n\n# NLP  \n\nNLP  \n\nNbatch sizeSsequence lengthHhidden size  \n\n```python\n# [N,S,H]\nbatch_size = 2\nseq_len = 3\nhidden_size = 4\ntorch.manual_seed(6)  # \ninputs = torch.randn(batch_size, seq_len, hidden_size)\nprint(':\\n', inputs)\n```\n\n## batchnorm  \n\n  \n\n```python\n# torchbatchnorm\ntorch_bn = nn.BatchNorm1d(num_features=hidden_size, affine=True)  # batchnorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(7)  # \ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(hidden_size))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(hidden_size))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# # \ntorch_normed = torch_bn(inputs.transpose(1, 2)).transpose(1, 2)\nprint('torch bn:\\n', torch_normed)\n```\n\ntransposetranspose  \n\n  \n\n```python\nweight:\n Parameter containing:\ntensor([-0.1468,  0.7861,  0.9468, -1.1143], requires_grad=True)\nbias:\n Parameter containing:\ntensor([ 1.6908, -0.8948, -0.3556,  1.2324], requires_grad=True) \n\ntorch bn:\n tensor([[[ 1.8740, -0.7037, -1.8222,  2.3385],\n         [ 1.7413, -1.8119,  0.3641,  0.0200],\n         [ 1.4615, -0.2676,  0.1081,  1.3450]],\n\n        [[ 1.7084, -1.9653,  1.0169,  0.5785],\n         [ 1.8213, -0.8614, -0.8056,  2.9892],\n         [ 1.5383,  0.2409, -0.9949,  0.1231]]], grad_fn=<TransposeBackward0>)\n```\n\nbatchnorm  \n\n  \n\n```python\n# bn\n\n# \nmean = torch.mean(inputs, dim=(0, 1) , keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=(0, 1), keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias\nprint('bn:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\ndim=(0,1)[N, S, H][NS, H]batchnorm  \n\n  \n\n```python\n:\n tensor([[[-0.2151,  0.5444, -0.2633, -0.5424]]])\n:\n tensor([[[0.7984, 0.3537, 0.7799, 0.7986]]]) \n\nbn:\n tensor([[[ 1.8740, -0.7037, -1.8222,  2.3385],\n         [ 1.7413, -1.8119,  0.3641,  0.0200],\n         [ 1.4615, -0.2676,  0.1081,  1.3450]],\n\n        [[ 1.7084, -1.9653,  1.0169,  0.5785],\n         [ 1.8213, -0.8614, -0.8056,  2.9892],\n         [ 1.5383,  0.2409, -0.9950,  0.1231]]], grad_fn=<AddBackward0>)\n:\n tensor([[[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]],\n\n        [[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]]])\n```\n\n## layernorm  \n\nNLPlayernormhuggingfacebertlayernorm  \n\n```python\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states: torch.normTensor, input_tensor: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n```\n\n  \n\n```python\n# torchlayernorm\ntorch_ln = nn.LayerNorm(normalized_shape=hidden_size, elementwise_affine=True)  # layernorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(8)  # \ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(hidden_size))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(hidden_size))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n```\n\nhidden size  \n\n```python\nweight:\n Parameter containing:\ntensor([ 0.2713, -1.2729,  0.5027,  0.4181], requires_grad=True)\nbias:\n Parameter containing:\ntensor([-0.6394, -0.6608, -0.1433, -0.1043], requires_grad=True) \n\ntorch ln:\n tensor([[[-0.7547, -2.8528, -0.5092, -0.3423],\n         [-1.0957, -0.8780,  0.2388,  0.2097],\n         [-0.3502, -1.6158, -0.3133, -0.7224]],\n\n        [[-0.9134, -0.4490,  0.6868, -0.3029],\n         [-0.7116, -2.5589, -0.1039, -0.6493],\n         [-0.5076, -2.1031, -0.9346, -0.1230]]],\n       grad_fn=<NativeLayerNormBackward0>)\n```\n\n  \n\n```python\n# ln\n\n# \nmean = torch.mean(inputs, dim=2, keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=2, keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\nprint('ln:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\n  \n\n```python\n:\n tensor([[[-0.8469],\n         [ 0.0745],\n         [ 0.3386]],\n\n        [[ 0.1364],\n         [-0.7003],\n         [ 0.2831]]])\n:\n tensor([[[0.8578],\n         [0.3354],\n         [0.6505]],\n\n        [[0.4426],\n         [0.8448],\n         [0.6816]]]) \n```\n\nsampletoken  \n\n  \n\n```python\nln:\n tensor([[[-0.7547, -2.8528, -0.5092, -0.3423],\n         [-1.0957, -0.8780,  0.2388,  0.2097],\n         [-0.3502, -1.6158, -0.3133, -0.7224]],\n\n        [[-0.9134, -0.4490,  0.6868, -0.3029],\n         [-0.7116, -2.5590, -0.1039, -0.6493],\n         [-0.5076, -2.1031, -0.9347, -0.1230]]], grad_fn=<AddBackward0>)\n:\n tensor([[[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]],\n\n        [[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]]])\n```\n\n#   \n\n\n\nbatchnorm  \n\n```python\n# \nbatch_size = 3\nfeature_num = 4\ntorch.manual_seed(0)  # \ninputs = torch.randn(batch_size, feature_num)\nprint(':\\n', inputs)\n\n# \nmean = torch.mean(inputs, dim=0, keepdim=True)\n# print(':\\n', mean)\nstd = torch.std(inputs, dim=0, keepdim=True, unbiased=False)\n# print(':\\n', std, '\\n')\n\n# torchbatchnorm\ntorch_bn = nn.BatchNorm1d(num_features=feature_num, affine=True)\n\n# \ntorch_bn.weight = nn.Parameter(std)\ntorch_bn.bias =  nn.Parameter(mean)\n# print('weight:\\n', torch_bn.weight)\n# print('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_bn(inputs)\nprint('torch bn:\\n', torch_normed)\n\nisclose = torch.isclose(torch_normed, inputs, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\n  \n\n```python\n:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\ntorch bn:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1821]],\n       grad_fn=<NativeBatchNormBackward0>)\n:\n tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\nbatchnorm  \n\nlayernorm  \n\n```python\nprint(':\\n', inputs)\n\n# \nmean = torch.mean(inputs, dim=1, keepdim=True)\n# print(':\\n', mean)\nstd = torch.std(inputs, dim=1, keepdim=True, unbiased=False)\n# print(':\\n', std, '\\n')\n\n# torchlayernorm\ntorch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=True)  # layernorm\n\n# \ntorch_bn.weight = nn.Parameter(std)\ntorch_bn.bias =  nn.Parameter(mean)\n# print('weight:\\n', torch_bn.weight)\n# print('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n\nisclose = torch.isclose(torch_normed, inputs, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\n\n\n```python\n:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\ntorch ln:\n tensor([[ 1.1918, -0.1481, -1.5251,  0.4814],\n        [-0.8146, -1.1451,  0.7512,  1.2086],\n        [-0.9685, -0.0551, -0.6140,  1.6376]],\n       grad_fn=<NativeLayerNormBackward0>)\n:\n tensor([[False, False, False, False],\n        [False, False, False, False],\n        [False, False, False, False]])\n```\n\nlayernormlayernorm  \n\nCVNLP\n\nbatchnormlayernorm  \n\n# \n\nbatchnormlayernorm  \n\nbatchnormlayernorm  \n\nbatchnorm\"\"\"\"batchbatchbatchfeature map  \n\nlayernorm  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***\n\n# Reference  \n1LAYERNORM https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html  \n2BATCHNORM1D https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html  \n3BATCHNORM2D https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html  \n","slug":"cs/nlp/2024/04/normalization-","published":1,"updated":"2024-04-08T11:32:01.659Z","comments":1,"layout":"post","photos":[],"_id":"clv7wdj0m0007794k8auaeubi","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a>normalizationnorm</p>\n<p>norm</p>\n<p><a href=\"https://github.com/Saicat/normalization_exp\">https://github.com/Saicat/normalization_exp</a></p>\n<h1 id=\"\"></h1>\n<p>normalizationCNN</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># epsilon</span></span><br><span class=\"line\">eps = <span class=\"number\">1e-8</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">batch_size = <span class=\"number\">3</span></span><br><span class=\"line\">feature_num = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">0</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, feature_num)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<p>34batch\nsize=34</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm\">batchnorm</h2>\n<p>pytorchBatchNorm1d</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=feature_num, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># batchnorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">1</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(feature_num))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(feature_num))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>batchnorm/layernorm+BatchNorm\"affine\"\"affine\"False</p>\n<p>pytorchnorm1.00</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([<span class=\"number\">0.6614</span>, <span class=\"number\">0.2669</span>, <span class=\"number\">0.0617</span>, <span class=\"number\">0.6213</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.4519</span>, -<span class=\"number\">0.1661</span>, -<span class=\"number\">1.5228</span>,  <span class=\"number\">0.3817</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>weightbias4</p>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">0.4756</span>,  <span class=\"number\">0.0513</span>, -<span class=\"number\">1.6033</span>,  <span class=\"number\">0.4715</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0197</span>, -<span class=\"number\">0.5421</span>, -<span class=\"number\">1.4535</span>,  <span class=\"number\">1.0937</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8117</span>, -<span class=\"number\">0.0077</span>, -<span class=\"number\">1.5115</span>, -<span class=\"number\">0.4202</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># bn</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bn:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(isclose)</span><br></pre></td></tr></table></figure>\n<p>dim=0batchsamplefeature</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[-<span class=\"number\">0.0876</span>, -<span class=\"number\">0.6985</span>, -<span class=\"number\">0.7907</span>,  <span class=\"number\">0.5295</span>]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"number\">1.1612</span>, <span class=\"number\">0.4971</span>, <span class=\"number\">1.0630</span>, <span class=\"number\">0.2692</span>]]) </span><br></pre></td></tr></table></figure>\n<p>meanstdkeepdimTrueFalsebroadcast</p>\n<p>stdunbiasedFalsetorchbatchnorm</p>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bn:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">0.4756</span>,  <span class=\"number\">0.0514</span>, -<span class=\"number\">1.6033</span>,  <span class=\"number\">0.4715</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0197</span>, -<span class=\"number\">0.5421</span>, -<span class=\"number\">1.4535</span>,  <span class=\"number\">1.0937</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8117</span>, -<span class=\"number\">0.0077</span>, -<span class=\"number\">1.5115</span>, -<span class=\"number\">0.4202</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>torch.isclosebatchnormbatchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<p>equal1e-5~1e-4eps</p>\n<h2 id=\"layernorm\">layernorm</h2>\n<p>layernorm34</p>\n<p>torch</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">2</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(feature_num))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(feature_num))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">0.3923</span>, -<span class=\"number\">0.2236</span>, -<span class=\"number\">0.3195</span>, -<span class=\"number\">1.2050</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">1.0445</span>, -<span class=\"number\">0.6332</span>,  <span class=\"number\">0.5731</span>,  <span class=\"number\">0.5409</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p></p>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5120</span>, -<span class=\"number\">0.6001</span>,  <span class=\"number\">1.0604</span>, -<span class=\"number\">0.0392</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.7249</span>, -<span class=\"number\">0.3772</span>,  <span class=\"number\">0.3331</span>, -<span class=\"number\">0.9155</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.6645</span>, -<span class=\"number\">0.6209</span>,  <span class=\"number\">0.7693</span>, -<span class=\"number\">1.4324</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p></p>\n<p>dim=1</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ln</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;ln:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[-<span class=\"number\">0.0907</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.3104</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.3843</span>]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"number\">1.3691</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.9502</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.3458</span>]]) </span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5120</span>, -<span class=\"number\">0.6001</span>,  <span class=\"number\">1.0604</span>, -<span class=\"number\">0.0392</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.7249</span>, -<span class=\"number\">0.3772</span>,  <span class=\"number\">0.3331</span>, -<span class=\"number\">0.9155</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.6645</span>, -<span class=\"number\">0.6209</span>,  <span class=\"number\">0.7693</span>, -<span class=\"number\">1.4325</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"\"></h2>\n<p>batchnormlayernorm</p>\n<img src=\"/b70b4a2d/bn_and_ln.png\" class title=\"bnln\">\n<p>batchnormdim=0batchlayernormdim=1</p>\n<p>batchnormlayernorm</p>\n<h1 id=\"cv\">CV</h1>\n<p>CV</p>\n<p>CV[N,C,H,W]Nbatch\nsizeCchannelHWfeature\nmapCV</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># [N,C,H,W]</span></span><br><span class=\"line\">batch_size = <span class=\"number\">2</span></span><br><span class=\"line\">channel = <span class=\"number\">2</span></span><br><span class=\"line\">height = <span class=\"number\">2</span></span><br><span class=\"line\">width = <span class=\"number\">3</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">3</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, channel, height, width)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[[-<span class=\"number\">0.0766</span>,  <span class=\"number\">0.3599</span>, -<span class=\"number\">0.7820</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.0715</span>,  <span class=\"number\">0.6648</span>, -<span class=\"number\">0.2868</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[ <span class=\"number\">1.6206</span>, -<span class=\"number\">1.5967</span>,  <span class=\"number\">0.4046</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.6113</span>,  <span class=\"number\">0.7604</span>, -<span class=\"number\">0.0336</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[-<span class=\"number\">0.3448</span>,  <span class=\"number\">0.4937</span>, -<span class=\"number\">0.0776</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.8054</span>,  <span class=\"number\">0.4851</span>,  <span class=\"number\">0.2052</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[ <span class=\"number\">0.3384</span>,  <span class=\"number\">1.3528</span>,  <span class=\"number\">0.3736</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.0134</span>,  <span class=\"number\">0.7737</span>, -<span class=\"number\">0.1092</span>]]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm-1\">batchnorm</h2>\n<p>BatchNorm2dchannel</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm2d(num_features=channel, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># batchnorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">4</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(channel))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(channel))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>channel</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">1.6053</span>,  <span class=\"number\">0.2325</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([<span class=\"number\">2.2399</span>, <span class=\"number\">0.8473</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>torchbatchnorm2d</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[[[<span class=\"number\">2.2043</span>, <span class=\"number\">1.1275</span>, <span class=\"number\">3.9442</span>],</span><br><span class=\"line\">          [<span class=\"number\">1.8388</span>, <span class=\"number\">0.3753</span>, <span class=\"number\">2.7226</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">1.2185</span>, <span class=\"number\">0.2591</span>, <span class=\"number\">0.8559</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.9175</span>, <span class=\"number\">0.9620</span>, <span class=\"number\">0.7252</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"number\">2.8658</span>, <span class=\"number\">0.7975</span>, <span class=\"number\">2.2066</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.4684</span>, <span class=\"number\">0.8186</span>, <span class=\"number\">1.5090</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">0.8362</span>, <span class=\"number\">1.1387</span>, <span class=\"number\">0.8467</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.7392</span>, <span class=\"number\">0.9660</span>, <span class=\"number\">0.7027</span>]]]], grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>batchnorm2d</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># bn</span></span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = []</span><br><span class=\"line\"><span class=\"comment\"># channel</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(channel):</span><br><span class=\"line\">    <span class=\"comment\"># </span></span><br><span class=\"line\">    mean = torch.mean(inputs[:, c, :, :])</span><br><span class=\"line\">    std = torch.std(inputs[:, c, :, :], unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    normed = (inputs[:, c, :, :] - mean) / (std + eps) * torch_bn.weight[c] + torch_bn.bias[c]</span><br><span class=\"line\">    normed = normed.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">    manual_normed.append(normed)</span><br><span class=\"line\">manual_normed = torch.cat(manual_normed, <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bn:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>CVchannel\"batch\"</p>\n<p>NHW</p>\n<img src=\"/b70b4a2d/cv_batchnorm.png\" class title=\"CVbatchnorm\">\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bn:</span><br><span class=\"line\"> tensor([[[[<span class=\"number\">2.2043</span>, <span class=\"number\">1.1275</span>, <span class=\"number\">3.9442</span>],</span><br><span class=\"line\">          [<span class=\"number\">1.8388</span>, <span class=\"number\">0.3752</span>, <span class=\"number\">2.7226</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">1.2185</span>, <span class=\"number\">0.2591</span>, <span class=\"number\">0.8559</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.9175</span>, <span class=\"number\">0.9620</span>, <span class=\"number\">0.7252</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"number\">2.8658</span>, <span class=\"number\">0.7975</span>, <span class=\"number\">2.2066</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.4685</span>, <span class=\"number\">0.8186</span>, <span class=\"number\">1.5089</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">0.8362</span>, <span class=\"number\">1.1387</span>, <span class=\"number\">0.8467</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.7392</span>, <span class=\"number\">0.9660</span>, <span class=\"number\">0.7027</span>]]]], grad_fn=&lt;CatBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"layernorm-1\">layernorm</h2>\n<p><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\">torchlayernorm</a>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(</span><br><span class=\"line\">    normalized_shape=[channel, height, width], </span><br><span class=\"line\">    elementwise_affine=<span class=\"literal\">True</span></span><br><span class=\"line\">)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">5</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(channel, height, width))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(channel, height, width))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p></p>\n<img src=\"/b70b4a2d/cv_layernorm.jpeg\" class title=\"CVlayernorm\">\n<p>[channel, height, width]</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([[[-<span class=\"number\">0.4868</span>, -<span class=\"number\">0.6038</span>, -<span class=\"number\">0.5581</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.6675</span>, -<span class=\"number\">0.1974</span>,  <span class=\"number\">1.9428</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">1.4017</span>, -<span class=\"number\">0.7626</span>,  <span class=\"number\">0.6312</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.8991</span>, -<span class=\"number\">0.5578</span>,  <span class=\"number\">0.6907</span>]]], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([[[ <span class=\"number\">0.2225</span>, -<span class=\"number\">0.6662</span>,  <span class=\"number\">0.6846</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.5740</span>, -<span class=\"number\">0.5829</span>,  <span class=\"number\">0.7679</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">0.0571</span>, -<span class=\"number\">1.1894</span>, -<span class=\"number\">0.5659</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.8327</span>,  <span class=\"number\">0.9014</span>,  <span class=\"number\">0.2116</span>]]], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>channel</p>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[[[ <span class=\"number\">0.3594</span>, -<span class=\"number\">0.8338</span>,  <span class=\"number\">1.3456</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.5128</span>, -<span class=\"number\">0.7147</span>, -<span class=\"number\">0.3012</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">2.5939</span>,  <span class=\"number\">0.5089</span>, -<span class=\"number\">0.3546</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.3715</span>,  <span class=\"number\">0.4607</span>,  <span class=\"number\">0.0553</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[ <span class=\"number\">0.5477</span>, -<span class=\"number\">0.9583</span>,  <span class=\"number\">0.8526</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.2112</span>, -<span class=\"number\">0.6760</span>,  <span class=\"number\">0.9378</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">0.3219</span>, -<span class=\"number\">2.4580</span>, -<span class=\"number\">0.3647</span>],</span><br><span class=\"line\">          [-<span class=\"number\">0.6744</span>,  <span class=\"number\">0.4171</span>, -<span class=\"number\">0.0264</span>]]]], grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ln</span></span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = []</span><br><span class=\"line\"><span class=\"comment\"># channel</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> b <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(batch_size):</span><br><span class=\"line\">    <span class=\"comment\"># </span></span><br><span class=\"line\">    mean = torch.mean(inputs[b, :, :, :])</span><br><span class=\"line\">    std = torch.std(inputs[b, :, :, :], unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    normed = (inputs[b, :, :, :] - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\">    normed = normed.unsqueeze(<span class=\"number\">0</span>)</span><br><span class=\"line\">    manual_normed.append(normed)</span><br><span class=\"line\">manual_normed = torch.cat(manual_normed, <span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;ln:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>channel</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln:</span><br><span class=\"line\"> tensor([[[[ <span class=\"number\">0.3594</span>, -<span class=\"number\">0.8338</span>,  <span class=\"number\">1.3456</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.5128</span>, -<span class=\"number\">0.7147</span>, -<span class=\"number\">0.3012</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">2.5939</span>,  <span class=\"number\">0.5090</span>, -<span class=\"number\">0.3546</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.3715</span>,  <span class=\"number\">0.4607</span>,  <span class=\"number\">0.0553</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[ <span class=\"number\">0.5477</span>, -<span class=\"number\">0.9583</span>,  <span class=\"number\">0.8527</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.2112</span>, -<span class=\"number\">0.6760</span>,  <span class=\"number\">0.9378</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">0.3219</span>, -<span class=\"number\">2.4581</span>, -<span class=\"number\">0.3647</span>],</span><br><span class=\"line\">          [-<span class=\"number\">0.6744</span>,  <span class=\"number\">0.4171</span>, -<span class=\"number\">0.0264</span>]]]], grad_fn=&lt;CatBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]]])</span><br></pre></td></tr></table></figure>\n<h1 id=\"nlp\">NLP</h1>\n<p>NLP</p>\n<p>Nbatch sizeSsequence lengthHhidden size</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># [N,S,H]</span></span><br><span class=\"line\">batch_size = <span class=\"number\">2</span></span><br><span class=\"line\">seq_len = <span class=\"number\">3</span></span><br><span class=\"line\">hidden_size = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">6</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, seq_len, hidden_size)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm-2\">batchnorm</h2>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=hidden_size, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># batchnorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">7</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(hidden_size))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(hidden_size))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># # </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>transposetranspose</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.1468</span>,  <span class=\"number\">0.7861</span>,  <span class=\"number\">0.9468</span>, -<span class=\"number\">1.1143</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">1.6908</span>, -<span class=\"number\">0.8948</span>, -<span class=\"number\">0.3556</span>,  <span class=\"number\">1.2324</span>], requires_grad=<span class=\"literal\">True</span>) </span><br><span class=\"line\"></span><br><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[[ <span class=\"number\">1.8740</span>, -<span class=\"number\">0.7037</span>, -<span class=\"number\">1.8222</span>,  <span class=\"number\">2.3385</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.7413</span>, -<span class=\"number\">1.8119</span>,  <span class=\"number\">0.3641</span>,  <span class=\"number\">0.0200</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.4615</span>, -<span class=\"number\">0.2676</span>,  <span class=\"number\">0.1081</span>,  <span class=\"number\">1.3450</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">1.7084</span>, -<span class=\"number\">1.9653</span>,  <span class=\"number\">1.0169</span>,  <span class=\"number\">0.5785</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.8213</span>, -<span class=\"number\">0.8614</span>, -<span class=\"number\">0.8056</span>,  <span class=\"number\">2.9892</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.5383</span>,  <span class=\"number\">0.2409</span>, -<span class=\"number\">0.9949</span>,  <span class=\"number\">0.1231</span>]]], grad_fn=&lt;TransposeBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>batchnorm</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># bn</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=(<span class=\"number\">0</span>, <span class=\"number\">1</span>) , keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=(<span class=\"number\">0</span>, <span class=\"number\">1</span>), keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bn:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>dim=(0,1)[N,\nS, H][NS,\nH]batchnorm</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.2151</span>,  <span class=\"number\">0.5444</span>, -<span class=\"number\">0.2633</span>, -<span class=\"number\">0.5424</span>]]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"number\">0.7984</span>, <span class=\"number\">0.3537</span>, <span class=\"number\">0.7799</span>, <span class=\"number\">0.7986</span>]]]) </span><br><span class=\"line\"></span><br><span class=\"line\">bn:</span><br><span class=\"line\"> tensor([[[ <span class=\"number\">1.8740</span>, -<span class=\"number\">0.7037</span>, -<span class=\"number\">1.8222</span>,  <span class=\"number\">2.3385</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.7413</span>, -<span class=\"number\">1.8119</span>,  <span class=\"number\">0.3641</span>,  <span class=\"number\">0.0200</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.4615</span>, -<span class=\"number\">0.2676</span>,  <span class=\"number\">0.1081</span>,  <span class=\"number\">1.3450</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">1.7084</span>, -<span class=\"number\">1.9653</span>,  <span class=\"number\">1.0169</span>,  <span class=\"number\">0.5785</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.8213</span>, -<span class=\"number\">0.8614</span>, -<span class=\"number\">0.8056</span>,  <span class=\"number\">2.9892</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.5383</span>,  <span class=\"number\">0.2409</span>, -<span class=\"number\">0.9950</span>,  <span class=\"number\">0.1231</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"layernorm-2\">layernorm</h2>\n<p>NLPlayernormhuggingfacebertlayernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BertSelfOutput</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, config</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class=\"line\">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class=\"line\">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, hidden_states: torch.normTensor, input_tensor: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class=\"line\">        hidden_states = self.dense(hidden_states)</span><br><span class=\"line\">        hidden_states = self.dropout(hidden_states)</span><br><span class=\"line\">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> hidden_states</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=hidden_size, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">8</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(hidden_size))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(hidden_size))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>hidden size</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">0.2713</span>, -<span class=\"number\">1.2729</span>,  <span class=\"number\">0.5027</span>,  <span class=\"number\">0.4181</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.6394</span>, -<span class=\"number\">0.6608</span>, -<span class=\"number\">0.1433</span>, -<span class=\"number\">0.1043</span>], requires_grad=<span class=\"literal\">True</span>) </span><br><span class=\"line\"></span><br><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.7547</span>, -<span class=\"number\">2.8528</span>, -<span class=\"number\">0.5092</span>, -<span class=\"number\">0.3423</span>],</span><br><span class=\"line\">         [-<span class=\"number\">1.0957</span>, -<span class=\"number\">0.8780</span>,  <span class=\"number\">0.2388</span>,  <span class=\"number\">0.2097</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.3502</span>, -<span class=\"number\">1.6158</span>, -<span class=\"number\">0.3133</span>, -<span class=\"number\">0.7224</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">0.9134</span>, -<span class=\"number\">0.4490</span>,  <span class=\"number\">0.6868</span>, -<span class=\"number\">0.3029</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7116</span>, -<span class=\"number\">2.5589</span>, -<span class=\"number\">0.1039</span>, -<span class=\"number\">0.6493</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.5076</span>, -<span class=\"number\">2.1031</span>, -<span class=\"number\">0.9346</span>, -<span class=\"number\">0.1230</span>]]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ln</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">2</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">2</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;ln:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.8469</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.0745</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.3386</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">0.1364</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7003</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.2831</span>]]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"number\">0.8578</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.3354</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.6505</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"number\">0.4426</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.8448</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.6816</span>]]]) </span><br></pre></td></tr></table></figure>\n<p>sampletoken</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.7547</span>, -<span class=\"number\">2.8528</span>, -<span class=\"number\">0.5092</span>, -<span class=\"number\">0.3423</span>],</span><br><span class=\"line\">         [-<span class=\"number\">1.0957</span>, -<span class=\"number\">0.8780</span>,  <span class=\"number\">0.2388</span>,  <span class=\"number\">0.2097</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.3502</span>, -<span class=\"number\">1.6158</span>, -<span class=\"number\">0.3133</span>, -<span class=\"number\">0.7224</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">0.9134</span>, -<span class=\"number\">0.4490</span>,  <span class=\"number\">0.6868</span>, -<span class=\"number\">0.3029</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7116</span>, -<span class=\"number\">2.5590</span>, -<span class=\"number\">0.1039</span>, -<span class=\"number\">0.6493</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.5076</span>, -<span class=\"number\">2.1031</span>, -<span class=\"number\">0.9347</span>, -<span class=\"number\">0.1230</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]])</span><br></pre></td></tr></table></figure>\n<h1 id=\"\"></h1>\n<p></p>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">batch_size = <span class=\"number\">3</span></span><br><span class=\"line\">feature_num = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">0</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, feature_num)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, mean)</span></span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, std, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=feature_num, affine=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(std)</span><br><span class=\"line\">torch_bn.bias =  nn.Parameter(mean)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;weight:\\n&#x27;, torch_bn.weight)</span></span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;bias:\\n&#x27;, torch_bn.bias, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br><span class=\"line\"></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, inputs, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1821</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<p>batchnorm</p>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, mean)</span></span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, std, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(std)</span><br><span class=\"line\">torch_bn.bias =  nn.Parameter(mean)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;weight:\\n&#x27;, torch_bn.weight)</span></span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;bias:\\n&#x27;, torch_bn.bias, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br><span class=\"line\"></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, inputs, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.1918</span>, -<span class=\"number\">0.1481</span>, -<span class=\"number\">1.5251</span>,  <span class=\"number\">0.4814</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8146</span>, -<span class=\"number\">1.1451</span>,  <span class=\"number\">0.7512</span>,  <span class=\"number\">1.2086</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.9685</span>, -<span class=\"number\">0.0551</span>, -<span class=\"number\">0.6140</span>,  <span class=\"number\">1.6376</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>],</span><br><span class=\"line\">        [<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>],</span><br><span class=\"line\">        [<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>]])</span><br></pre></td></tr></table></figure>\n<p>layernormlayernorm</p>\n<p>CVNLP</p>\n<p>batchnormlayernorm</p>\n<h1 id=\"\"></h1>\n<p>batchnormlayernorm</p>\n<p>batchnormlayernorm</p>\n<p>batchnorm\"\"\"\"batchbatchbatchfeature\nmap</p>\n<p>layernorm</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1LAYERNORM\nhttps://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html<br>\n2BATCHNORM1D\nhttps://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html<br>\n3BATCHNORM2D\nhttps://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html</p>\n","length":18594,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a>normalizationnorm</p>\n<p>norm</p>\n<p><a href=\"https://github.com/Saicat/normalization_exp\">https://github.com/Saicat/normalization_exp</a></p>\n<h1 id=\"\"></h1>\n<p>normalizationCNN</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># epsilon</span></span><br><span class=\"line\">eps = <span class=\"number\">1e-8</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">batch_size = <span class=\"number\">3</span></span><br><span class=\"line\">feature_num = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">0</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, feature_num)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<p>34batch\nsize=34</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm\">batchnorm</h2>\n<p>pytorchBatchNorm1d</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=feature_num, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># batchnorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">1</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(feature_num))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(feature_num))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>batchnorm/layernorm+BatchNorm\"affine\"\"affine\"False</p>\n<p>pytorchnorm1.00</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([<span class=\"number\">0.6614</span>, <span class=\"number\">0.2669</span>, <span class=\"number\">0.0617</span>, <span class=\"number\">0.6213</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.4519</span>, -<span class=\"number\">0.1661</span>, -<span class=\"number\">1.5228</span>,  <span class=\"number\">0.3817</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>weightbias4</p>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">0.4756</span>,  <span class=\"number\">0.0513</span>, -<span class=\"number\">1.6033</span>,  <span class=\"number\">0.4715</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0197</span>, -<span class=\"number\">0.5421</span>, -<span class=\"number\">1.4535</span>,  <span class=\"number\">1.0937</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8117</span>, -<span class=\"number\">0.0077</span>, -<span class=\"number\">1.5115</span>, -<span class=\"number\">0.4202</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># bn</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bn:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(isclose)</span><br></pre></td></tr></table></figure>\n<p>dim=0batchsamplefeature</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[-<span class=\"number\">0.0876</span>, -<span class=\"number\">0.6985</span>, -<span class=\"number\">0.7907</span>,  <span class=\"number\">0.5295</span>]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"number\">1.1612</span>, <span class=\"number\">0.4971</span>, <span class=\"number\">1.0630</span>, <span class=\"number\">0.2692</span>]]) </span><br></pre></td></tr></table></figure>\n<p>meanstdkeepdimTrueFalsebroadcast</p>\n<p>stdunbiasedFalsetorchbatchnorm</p>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bn:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">0.4756</span>,  <span class=\"number\">0.0514</span>, -<span class=\"number\">1.6033</span>,  <span class=\"number\">0.4715</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0197</span>, -<span class=\"number\">0.5421</span>, -<span class=\"number\">1.4535</span>,  <span class=\"number\">1.0937</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8117</span>, -<span class=\"number\">0.0077</span>, -<span class=\"number\">1.5115</span>, -<span class=\"number\">0.4202</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>torch.isclosebatchnormbatchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<p>equal1e-5~1e-4eps</p>\n<h2 id=\"layernorm\">layernorm</h2>\n<p>layernorm34</p>\n<p>torch</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">2</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(feature_num))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(feature_num))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">0.3923</span>, -<span class=\"number\">0.2236</span>, -<span class=\"number\">0.3195</span>, -<span class=\"number\">1.2050</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">1.0445</span>, -<span class=\"number\">0.6332</span>,  <span class=\"number\">0.5731</span>,  <span class=\"number\">0.5409</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p></p>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5120</span>, -<span class=\"number\">0.6001</span>,  <span class=\"number\">1.0604</span>, -<span class=\"number\">0.0392</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.7249</span>, -<span class=\"number\">0.3772</span>,  <span class=\"number\">0.3331</span>, -<span class=\"number\">0.9155</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.6645</span>, -<span class=\"number\">0.6209</span>,  <span class=\"number\">0.7693</span>, -<span class=\"number\">1.4324</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p></p>\n<p>dim=1</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ln</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;ln:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[-<span class=\"number\">0.0907</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.3104</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.3843</span>]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"number\">1.3691</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.9502</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.3458</span>]]) </span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5120</span>, -<span class=\"number\">0.6001</span>,  <span class=\"number\">1.0604</span>, -<span class=\"number\">0.0392</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.7249</span>, -<span class=\"number\">0.3772</span>,  <span class=\"number\">0.3331</span>, -<span class=\"number\">0.9155</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.6645</span>, -<span class=\"number\">0.6209</span>,  <span class=\"number\">0.7693</span>, -<span class=\"number\">1.4325</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"\"></h2>\n<p>batchnormlayernorm</p>\n<img src=\"/b70b4a2d/bn_and_ln.png\" class title=\"bnln\">\n<p>batchnormdim=0batchlayernormdim=1</p>\n<p>batchnormlayernorm</p>\n<h1 id=\"cv\">CV</h1>\n<p>CV</p>\n<p>CV[N,C,H,W]Nbatch\nsizeCchannelHWfeature\nmapCV</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># [N,C,H,W]</span></span><br><span class=\"line\">batch_size = <span class=\"number\">2</span></span><br><span class=\"line\">channel = <span class=\"number\">2</span></span><br><span class=\"line\">height = <span class=\"number\">2</span></span><br><span class=\"line\">width = <span class=\"number\">3</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">3</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, channel, height, width)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[[-<span class=\"number\">0.0766</span>,  <span class=\"number\">0.3599</span>, -<span class=\"number\">0.7820</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.0715</span>,  <span class=\"number\">0.6648</span>, -<span class=\"number\">0.2868</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[ <span class=\"number\">1.6206</span>, -<span class=\"number\">1.5967</span>,  <span class=\"number\">0.4046</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.6113</span>,  <span class=\"number\">0.7604</span>, -<span class=\"number\">0.0336</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[-<span class=\"number\">0.3448</span>,  <span class=\"number\">0.4937</span>, -<span class=\"number\">0.0776</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.8054</span>,  <span class=\"number\">0.4851</span>,  <span class=\"number\">0.2052</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[ <span class=\"number\">0.3384</span>,  <span class=\"number\">1.3528</span>,  <span class=\"number\">0.3736</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.0134</span>,  <span class=\"number\">0.7737</span>, -<span class=\"number\">0.1092</span>]]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm-1\">batchnorm</h2>\n<p>BatchNorm2dchannel</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm2d(num_features=channel, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># batchnorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">4</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(channel))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(channel))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>channel</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">1.6053</span>,  <span class=\"number\">0.2325</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([<span class=\"number\">2.2399</span>, <span class=\"number\">0.8473</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>torchbatchnorm2d</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[[[<span class=\"number\">2.2043</span>, <span class=\"number\">1.1275</span>, <span class=\"number\">3.9442</span>],</span><br><span class=\"line\">          [<span class=\"number\">1.8388</span>, <span class=\"number\">0.3753</span>, <span class=\"number\">2.7226</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">1.2185</span>, <span class=\"number\">0.2591</span>, <span class=\"number\">0.8559</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.9175</span>, <span class=\"number\">0.9620</span>, <span class=\"number\">0.7252</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"number\">2.8658</span>, <span class=\"number\">0.7975</span>, <span class=\"number\">2.2066</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.4684</span>, <span class=\"number\">0.8186</span>, <span class=\"number\">1.5090</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">0.8362</span>, <span class=\"number\">1.1387</span>, <span class=\"number\">0.8467</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.7392</span>, <span class=\"number\">0.9660</span>, <span class=\"number\">0.7027</span>]]]], grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>batchnorm2d</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># bn</span></span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = []</span><br><span class=\"line\"><span class=\"comment\"># channel</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(channel):</span><br><span class=\"line\">    <span class=\"comment\"># </span></span><br><span class=\"line\">    mean = torch.mean(inputs[:, c, :, :])</span><br><span class=\"line\">    std = torch.std(inputs[:, c, :, :], unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    normed = (inputs[:, c, :, :] - mean) / (std + eps) * torch_bn.weight[c] + torch_bn.bias[c]</span><br><span class=\"line\">    normed = normed.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">    manual_normed.append(normed)</span><br><span class=\"line\">manual_normed = torch.cat(manual_normed, <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bn:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>CVchannel\"batch\"</p>\n<p>NHW</p>\n<img src=\"/b70b4a2d/cv_batchnorm.png\" class title=\"CVbatchnorm\">\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bn:</span><br><span class=\"line\"> tensor([[[[<span class=\"number\">2.2043</span>, <span class=\"number\">1.1275</span>, <span class=\"number\">3.9442</span>],</span><br><span class=\"line\">          [<span class=\"number\">1.8388</span>, <span class=\"number\">0.3752</span>, <span class=\"number\">2.7226</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">1.2185</span>, <span class=\"number\">0.2591</span>, <span class=\"number\">0.8559</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.9175</span>, <span class=\"number\">0.9620</span>, <span class=\"number\">0.7252</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"number\">2.8658</span>, <span class=\"number\">0.7975</span>, <span class=\"number\">2.2066</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.4685</span>, <span class=\"number\">0.8186</span>, <span class=\"number\">1.5089</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">0.8362</span>, <span class=\"number\">1.1387</span>, <span class=\"number\">0.8467</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.7392</span>, <span class=\"number\">0.9660</span>, <span class=\"number\">0.7027</span>]]]], grad_fn=&lt;CatBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"layernorm-1\">layernorm</h2>\n<p><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\">torchlayernorm</a>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(</span><br><span class=\"line\">    normalized_shape=[channel, height, width], </span><br><span class=\"line\">    elementwise_affine=<span class=\"literal\">True</span></span><br><span class=\"line\">)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">5</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(channel, height, width))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(channel, height, width))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p></p>\n<img src=\"/b70b4a2d/cv_layernorm.jpeg\" class title=\"CVlayernorm\">\n<p>[channel, height, width]</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([[[-<span class=\"number\">0.4868</span>, -<span class=\"number\">0.6038</span>, -<span class=\"number\">0.5581</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.6675</span>, -<span class=\"number\">0.1974</span>,  <span class=\"number\">1.9428</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">1.4017</span>, -<span class=\"number\">0.7626</span>,  <span class=\"number\">0.6312</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.8991</span>, -<span class=\"number\">0.5578</span>,  <span class=\"number\">0.6907</span>]]], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([[[ <span class=\"number\">0.2225</span>, -<span class=\"number\">0.6662</span>,  <span class=\"number\">0.6846</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.5740</span>, -<span class=\"number\">0.5829</span>,  <span class=\"number\">0.7679</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">0.0571</span>, -<span class=\"number\">1.1894</span>, -<span class=\"number\">0.5659</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.8327</span>,  <span class=\"number\">0.9014</span>,  <span class=\"number\">0.2116</span>]]], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>channel</p>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[[[ <span class=\"number\">0.3594</span>, -<span class=\"number\">0.8338</span>,  <span class=\"number\">1.3456</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.5128</span>, -<span class=\"number\">0.7147</span>, -<span class=\"number\">0.3012</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">2.5939</span>,  <span class=\"number\">0.5089</span>, -<span class=\"number\">0.3546</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.3715</span>,  <span class=\"number\">0.4607</span>,  <span class=\"number\">0.0553</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[ <span class=\"number\">0.5477</span>, -<span class=\"number\">0.9583</span>,  <span class=\"number\">0.8526</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.2112</span>, -<span class=\"number\">0.6760</span>,  <span class=\"number\">0.9378</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">0.3219</span>, -<span class=\"number\">2.4580</span>, -<span class=\"number\">0.3647</span>],</span><br><span class=\"line\">          [-<span class=\"number\">0.6744</span>,  <span class=\"number\">0.4171</span>, -<span class=\"number\">0.0264</span>]]]], grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ln</span></span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = []</span><br><span class=\"line\"><span class=\"comment\"># channel</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> b <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(batch_size):</span><br><span class=\"line\">    <span class=\"comment\"># </span></span><br><span class=\"line\">    mean = torch.mean(inputs[b, :, :, :])</span><br><span class=\"line\">    std = torch.std(inputs[b, :, :, :], unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    normed = (inputs[b, :, :, :] - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\">    normed = normed.unsqueeze(<span class=\"number\">0</span>)</span><br><span class=\"line\">    manual_normed.append(normed)</span><br><span class=\"line\">manual_normed = torch.cat(manual_normed, <span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;ln:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>channel</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln:</span><br><span class=\"line\"> tensor([[[[ <span class=\"number\">0.3594</span>, -<span class=\"number\">0.8338</span>,  <span class=\"number\">1.3456</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.5128</span>, -<span class=\"number\">0.7147</span>, -<span class=\"number\">0.3012</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">2.5939</span>,  <span class=\"number\">0.5090</span>, -<span class=\"number\">0.3546</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.3715</span>,  <span class=\"number\">0.4607</span>,  <span class=\"number\">0.0553</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[ <span class=\"number\">0.5477</span>, -<span class=\"number\">0.9583</span>,  <span class=\"number\">0.8527</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.2112</span>, -<span class=\"number\">0.6760</span>,  <span class=\"number\">0.9378</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">0.3219</span>, -<span class=\"number\">2.4581</span>, -<span class=\"number\">0.3647</span>],</span><br><span class=\"line\">          [-<span class=\"number\">0.6744</span>,  <span class=\"number\">0.4171</span>, -<span class=\"number\">0.0264</span>]]]], grad_fn=&lt;CatBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]]])</span><br></pre></td></tr></table></figure>\n<h1 id=\"nlp\">NLP</h1>\n<p>NLP</p>\n<p>Nbatch sizeSsequence lengthHhidden size</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># [N,S,H]</span></span><br><span class=\"line\">batch_size = <span class=\"number\">2</span></span><br><span class=\"line\">seq_len = <span class=\"number\">3</span></span><br><span class=\"line\">hidden_size = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">6</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, seq_len, hidden_size)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm-2\">batchnorm</h2>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=hidden_size, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># batchnorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">7</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(hidden_size))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(hidden_size))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># # </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>transposetranspose</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.1468</span>,  <span class=\"number\">0.7861</span>,  <span class=\"number\">0.9468</span>, -<span class=\"number\">1.1143</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">1.6908</span>, -<span class=\"number\">0.8948</span>, -<span class=\"number\">0.3556</span>,  <span class=\"number\">1.2324</span>], requires_grad=<span class=\"literal\">True</span>) </span><br><span class=\"line\"></span><br><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[[ <span class=\"number\">1.8740</span>, -<span class=\"number\">0.7037</span>, -<span class=\"number\">1.8222</span>,  <span class=\"number\">2.3385</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.7413</span>, -<span class=\"number\">1.8119</span>,  <span class=\"number\">0.3641</span>,  <span class=\"number\">0.0200</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.4615</span>, -<span class=\"number\">0.2676</span>,  <span class=\"number\">0.1081</span>,  <span class=\"number\">1.3450</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">1.7084</span>, -<span class=\"number\">1.9653</span>,  <span class=\"number\">1.0169</span>,  <span class=\"number\">0.5785</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.8213</span>, -<span class=\"number\">0.8614</span>, -<span class=\"number\">0.8056</span>,  <span class=\"number\">2.9892</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.5383</span>,  <span class=\"number\">0.2409</span>, -<span class=\"number\">0.9949</span>,  <span class=\"number\">0.1231</span>]]], grad_fn=&lt;TransposeBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>batchnorm</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># bn</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=(<span class=\"number\">0</span>, <span class=\"number\">1</span>) , keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=(<span class=\"number\">0</span>, <span class=\"number\">1</span>), keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bn:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>dim=(0,1)[N,\nS, H][NS,\nH]batchnorm</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.2151</span>,  <span class=\"number\">0.5444</span>, -<span class=\"number\">0.2633</span>, -<span class=\"number\">0.5424</span>]]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"number\">0.7984</span>, <span class=\"number\">0.3537</span>, <span class=\"number\">0.7799</span>, <span class=\"number\">0.7986</span>]]]) </span><br><span class=\"line\"></span><br><span class=\"line\">bn:</span><br><span class=\"line\"> tensor([[[ <span class=\"number\">1.8740</span>, -<span class=\"number\">0.7037</span>, -<span class=\"number\">1.8222</span>,  <span class=\"number\">2.3385</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.7413</span>, -<span class=\"number\">1.8119</span>,  <span class=\"number\">0.3641</span>,  <span class=\"number\">0.0200</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.4615</span>, -<span class=\"number\">0.2676</span>,  <span class=\"number\">0.1081</span>,  <span class=\"number\">1.3450</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">1.7084</span>, -<span class=\"number\">1.9653</span>,  <span class=\"number\">1.0169</span>,  <span class=\"number\">0.5785</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.8213</span>, -<span class=\"number\">0.8614</span>, -<span class=\"number\">0.8056</span>,  <span class=\"number\">2.9892</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.5383</span>,  <span class=\"number\">0.2409</span>, -<span class=\"number\">0.9950</span>,  <span class=\"number\">0.1231</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"layernorm-2\">layernorm</h2>\n<p>NLPlayernormhuggingfacebertlayernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BertSelfOutput</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, config</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class=\"line\">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class=\"line\">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, hidden_states: torch.normTensor, input_tensor: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class=\"line\">        hidden_states = self.dense(hidden_states)</span><br><span class=\"line\">        hidden_states = self.dropout(hidden_states)</span><br><span class=\"line\">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> hidden_states</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=hidden_size, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">8</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(hidden_size))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(hidden_size))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>hidden size</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">0.2713</span>, -<span class=\"number\">1.2729</span>,  <span class=\"number\">0.5027</span>,  <span class=\"number\">0.4181</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.6394</span>, -<span class=\"number\">0.6608</span>, -<span class=\"number\">0.1433</span>, -<span class=\"number\">0.1043</span>], requires_grad=<span class=\"literal\">True</span>) </span><br><span class=\"line\"></span><br><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.7547</span>, -<span class=\"number\">2.8528</span>, -<span class=\"number\">0.5092</span>, -<span class=\"number\">0.3423</span>],</span><br><span class=\"line\">         [-<span class=\"number\">1.0957</span>, -<span class=\"number\">0.8780</span>,  <span class=\"number\">0.2388</span>,  <span class=\"number\">0.2097</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.3502</span>, -<span class=\"number\">1.6158</span>, -<span class=\"number\">0.3133</span>, -<span class=\"number\">0.7224</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">0.9134</span>, -<span class=\"number\">0.4490</span>,  <span class=\"number\">0.6868</span>, -<span class=\"number\">0.3029</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7116</span>, -<span class=\"number\">2.5589</span>, -<span class=\"number\">0.1039</span>, -<span class=\"number\">0.6493</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.5076</span>, -<span class=\"number\">2.1031</span>, -<span class=\"number\">0.9346</span>, -<span class=\"number\">0.1230</span>]]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ln</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">2</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">2</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;ln:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.8469</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.0745</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.3386</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">0.1364</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7003</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.2831</span>]]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"number\">0.8578</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.3354</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.6505</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"number\">0.4426</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.8448</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.6816</span>]]]) </span><br></pre></td></tr></table></figure>\n<p>sampletoken</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.7547</span>, -<span class=\"number\">2.8528</span>, -<span class=\"number\">0.5092</span>, -<span class=\"number\">0.3423</span>],</span><br><span class=\"line\">         [-<span class=\"number\">1.0957</span>, -<span class=\"number\">0.8780</span>,  <span class=\"number\">0.2388</span>,  <span class=\"number\">0.2097</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.3502</span>, -<span class=\"number\">1.6158</span>, -<span class=\"number\">0.3133</span>, -<span class=\"number\">0.7224</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">0.9134</span>, -<span class=\"number\">0.4490</span>,  <span class=\"number\">0.6868</span>, -<span class=\"number\">0.3029</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7116</span>, -<span class=\"number\">2.5590</span>, -<span class=\"number\">0.1039</span>, -<span class=\"number\">0.6493</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.5076</span>, -<span class=\"number\">2.1031</span>, -<span class=\"number\">0.9347</span>, -<span class=\"number\">0.1230</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]])</span><br></pre></td></tr></table></figure>\n<h1 id=\"\"></h1>\n<p></p>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">batch_size = <span class=\"number\">3</span></span><br><span class=\"line\">feature_num = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">0</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, feature_num)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, mean)</span></span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, std, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=feature_num, affine=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(std)</span><br><span class=\"line\">torch_bn.bias =  nn.Parameter(mean)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;weight:\\n&#x27;, torch_bn.weight)</span></span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;bias:\\n&#x27;, torch_bn.bias, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br><span class=\"line\"></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, inputs, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1821</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<p>batchnorm</p>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, mean)</span></span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, std, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(std)</span><br><span class=\"line\">torch_bn.bias =  nn.Parameter(mean)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;weight:\\n&#x27;, torch_bn.weight)</span></span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;bias:\\n&#x27;, torch_bn.bias, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br><span class=\"line\"></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, inputs, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.1918</span>, -<span class=\"number\">0.1481</span>, -<span class=\"number\">1.5251</span>,  <span class=\"number\">0.4814</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8146</span>, -<span class=\"number\">1.1451</span>,  <span class=\"number\">0.7512</span>,  <span class=\"number\">1.2086</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.9685</span>, -<span class=\"number\">0.0551</span>, -<span class=\"number\">0.6140</span>,  <span class=\"number\">1.6376</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>],</span><br><span class=\"line\">        [<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>],</span><br><span class=\"line\">        [<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>]])</span><br></pre></td></tr></table></figure>\n<p>layernormlayernorm</p>\n<p>CVNLP</p>\n<p>batchnormlayernorm</p>\n<h1 id=\"\"></h1>\n<p>batchnormlayernorm</p>\n<p>batchnormlayernorm</p>\n<p>batchnorm\"\"\"\"batchbatchbatchfeature\nmap</p>\n<p>layernorm</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1LAYERNORM\nhttps://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html<br>\n2BATCHNORM1D\nhttps://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html<br>\n3BATCHNORM2D\nhttps://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html</p>\n"},{"title":"","abbrlink":"812c93f3","date":"2024-04-14T06:41:31.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n2024Q2RAGAgent  \n\n  \n\n# \n\n -->  -->  --> &kv cache\n\nMHA-->GQAMQAswaring attention\n\nPoSE\n\nntklognyarnrope abf\n\n\n\nStreamingLLM\n\nLeave No Context Behind:Efficient Infinite Context Transformers with Infini-attention\n\n#   \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n# Reference  \n1  \n2","source":"_posts/cs/nlp/2024/04/LLM.md","raw":"---\ntitle: \nabbrlink: 812c93f3\ndate: 2024-04-14 14:41:31\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \n  - attention\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n2024Q2RAGAgent  \n\n  \n\n# \n\n -->  -->  --> &kv cache\n\nMHA-->GQAMQAswaring attention\n\nPoSE\n\nntklognyarnrope abf\n\n\n\nStreamingLLM\n\nLeave No Context Behind:Efficient Infinite Context Transformers with Infini-attention\n\n#   \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n# Reference  \n1  \n2","slug":"cs/nlp/2024/04/LLM","published":1,"updated":"2024-04-17T15:13:29.878Z","comments":1,"layout":"post","photos":[],"_id":"clv7wdj0m0008794kcrdcgj7d","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>2024Q2RAGAgent</p>\n<p></p>\n<h1 id=\"section\"></h1>\n<p> --&gt;  --&gt;  --&gt; &amp;kv\ncache</p>\n<p>MHA--&gt;GQAMQAswaring attention</p>\n<p>PoSE</p>\n<p>ntklognyarnrope abf</p>\n<p></p>\n<p>StreamingLLM</p>\n<p>Leave No Context Behind:Efficient Infinite Context Transformers with\nInfini-attention</p>\n<h1 id=\"\"></h1>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1<br>\n2</p>\n","length":584,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>2024Q2RAGAgent</p>\n<p></p>\n<h1 id=\"section\"></h1>\n<p> --&gt;  --&gt;  --&gt; &amp;kv\ncache</p>\n<p>MHA--&gt;GQAMQAswaring attention</p>\n<p>PoSE</p>\n<p>ntklognyarnrope abf</p>\n<p></p>\n<p>StreamingLLM</p>\n<p>Leave No Context Behind:Efficient Infinite Context Transformers with\nInfini-attention</p>\n<h1 id=\"\"></h1>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1<br>\n2</p>\n"},{"title":"(3)","abbrlink":"1736008","date":"2024-04-05T06:08:31.000Z","_content":"\n![](/images/cover.png)  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~~\n\n***  \n\n# 1.RoPE  \n\nRoPERoPE  \n\nRoPE2k2kAlibiNTKYaRN  \n\n# 2.batchnormmomentum  \n\nbatchnormbatchnormalization  \n\nmoving_mean = momentum  moving_mean + (1.0  momentum)  mean  \n\nmoving_var = momentum  moving_var + (1.0  momentum)  var  \n\nmomentumbatch sizemini batchmomentum  \n\n# 3.  \n\n  \n\n  \n\n  \n\n  \n\n# 4.kv cache  \n\nGPT  \n\ntoken  \n\ntokenkv  \n\nkvL2kv  \n\n# 5.ReLU  \n\n1max(0, x)21/  \n\n1020  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)  \n","source":"_posts/cs/nlp/2024/04/-3.md","raw":"---\ntitle: (3)\ntags:\n  - NLP\n  - LLM\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: '1736008'\ndate: 2024-04-05 14:08:31\n---\n\n![](/images/cover.png)  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~~\n\n***  \n\n# 1.RoPE  \n\nRoPERoPE  \n\nRoPE2k2kAlibiNTKYaRN  \n\n# 2.batchnormmomentum  \n\nbatchnormbatchnormalization  \n\nmoving_mean = momentum  moving_mean + (1.0  momentum)  mean  \n\nmoving_var = momentum  moving_var + (1.0  momentum)  var  \n\nmomentumbatch sizemini batchmomentum  \n\n# 3.  \n\n  \n\n  \n\n  \n\n  \n\n# 4.kv cache  \n\nGPT  \n\ntoken  \n\ntokenkv  \n\nkvL2kv  \n\n# 5.ReLU  \n\n1max(0, x)21/  \n\n1020  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)  \n","slug":"cs/nlp/2024/04/-3","published":1,"updated":"2024-04-06T04:31:12.156Z","comments":1,"layout":"post","photos":[],"_id":"clv7wdj0n0009794kh92k0fxj","content":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM<sub></sub></p>\n<hr>\n<h1 id=\"rope\">1.RoPE</h1>\n<p>RoPERoPE</p>\n<p>RoPE2k2kAlibiNTKYaRN</p>\n<h1 id=\"batchnormmomentum\">2.batchnormmomentum</h1>\n<p>batchnormbatchnormalization</p>\n<p>moving_mean = momentum  moving_mean + (1.0  momentum)  mean</p>\n<p>moving_var = momentum  moving_var + (1.0  momentum)  var</p>\n<p>momentumbatch\nsizemini batchmomentum</p>\n<h1 id=\"\">3.</h1>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<h1 id=\"kv-cache\">4.kv cache</h1>\n<p>GPT</p>\n<p>token</p>\n<p>tokenkv</p>\n<p>kvL2kv</p>\n<h1 id=\"relu\">5.ReLU</h1>\n<p>1max(0,\nx)21/</p>\n<p>1020</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n","length":1558,"excerpt":"","more":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM<sub></sub></p>\n<hr>\n<h1 id=\"rope\">1.RoPE</h1>\n<p>RoPERoPE</p>\n<p>RoPE2k2kAlibiNTKYaRN</p>\n<h1 id=\"batchnormmomentum\">2.batchnormmomentum</h1>\n<p>batchnormbatchnormalization</p>\n<p>moving_mean = momentum  moving_mean + (1.0  momentum)  mean</p>\n<p>moving_var = momentum  moving_var + (1.0  momentum)  var</p>\n<p>momentumbatch\nsizemini batchmomentum</p>\n<h1 id=\"\">3.</h1>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<h1 id=\"kv-cache\">4.kv cache</h1>\n<p>GPT</p>\n<p>token</p>\n<p>tokenkv</p>\n<p>kvL2kv</p>\n<h1 id=\"relu\">5.ReLU</h1>\n<p>1max(0,\nx)21/</p>\n<p>1020</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n"},{"title":"(3)","abbrlink":"1736008","date":"2024-04-20T08:56:45.000Z","_content":"\n![](/images/cover.png)  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~~\n\n***  \n\n# 1.Transformerlayernormbatchnorm  \n\nNLPpaddingpaddingnormalizationbatchnormbatchtokentokenPowerNorm: Rethinking Batch Normalization in TransformersNLPbatchnormlayernorm  \n\n# 2.transformerencdoerdecoder  \n\ndecoderself-attentionencoderattentionKVdecoderQcross-attention  \n\n{% asset_img transformer.png transformer %}  \n\n# 3.PyTorchTensorview()reshape()  \n\n1.view()reshape()tensorview()reshape  \n\n2.view()tensor  \n\n3.reshape()tensortensorview()  \n\n4.is_contiguous()contiguous()  \n\n# 4.RLHFPPO  \n\nPPO4  \n\n1.ActorSFTactionCriticloss  \n\n2.ReferenceSFTRLHFReferenceActorActorReferenceKL penaltyActor  \n\n3.RewardSFTRLHF  \n\n4.CriticRewardActortoken  \n\n# 5.GPTLVhidden sizeHbatch sizeBSAdamN  \n\noptimizer  \n\n1.VH12H^2+13H=VH+L(12H^2+13H)2  \n\n2.2  \n\n3.optimizer(+)*2+(++2)*4=2016  \n\n4.softmaxdropout34BSH+5BNS^22  \n\n\nGPT3175BH=12288L=96N=96350GB=1S=102490GS=81923420G  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)  \n","source":"_posts/cs/nlp/2024/04/-4.md","raw":"---\ntitle: (3)\nabbrlink: '1736008'\ndate: 2024-04-20 16:56:45\ntags:\ncategories:\n---\n\n![](/images/cover.png)  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~~\n\n***  \n\n# 1.Transformerlayernormbatchnorm  \n\nNLPpaddingpaddingnormalizationbatchnormbatchtokentokenPowerNorm: Rethinking Batch Normalization in TransformersNLPbatchnormlayernorm  \n\n# 2.transformerencdoerdecoder  \n\ndecoderself-attentionencoderattentionKVdecoderQcross-attention  \n\n{% asset_img transformer.png transformer %}  \n\n# 3.PyTorchTensorview()reshape()  \n\n1.view()reshape()tensorview()reshape  \n\n2.view()tensor  \n\n3.reshape()tensortensorview()  \n\n4.is_contiguous()contiguous()  \n\n# 4.RLHFPPO  \n\nPPO4  \n\n1.ActorSFTactionCriticloss  \n\n2.ReferenceSFTRLHFReferenceActorActorReferenceKL penaltyActor  \n\n3.RewardSFTRLHF  \n\n4.CriticRewardActortoken  \n\n# 5.GPTLVhidden sizeHbatch sizeBSAdamN  \n\noptimizer  \n\n1.VH12H^2+13H=VH+L(12H^2+13H)2  \n\n2.2  \n\n3.optimizer(+)*2+(++2)*4=2016  \n\n4.softmaxdropout34BSH+5BNS^22  \n\n\nGPT3175BH=12288L=96N=96350GB=1S=102490GS=81923420G  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)  \n","slug":"cs/nlp/2024/04/-4","published":1,"updated":"2024-04-20T09:29:00.638Z","_id":"clv7wdj0p000c794k57zc8pi2","comments":1,"layout":"post","photos":[],"content":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM<sub></sub></p>\n<hr>\n<h1 id=\"transformerlayernormbatchnorm\">1.Transformerlayernormbatchnorm</h1>\n<p>NLPpaddingpaddingnormalizationbatchnormbatchtokentokenPowerNorm:\nRethinking Batch Normalization in\nTransformersNLPbatchnormlayernorm</p>\n<h1 id=\"transformerencdoerdecoder\">2.transformerencdoerdecoder</h1>\n<p>decoderself-attentionencoderattentionKVdecoderQcross-attention</p>\n<img src=\"/1736008/transformer.png\" class title=\"transformer\">\n<h1 id=\"pytorchtensorviewreshape\">3.PyTorchTensorview()reshape()</h1>\n<p>1.view()reshape()tensorview()reshape</p>\n<p>2.view()tensor</p>\n<p>3.reshape()tensortensorview()</p>\n<p>4.is_contiguous()contiguous()</p>\n<h1 id=\"rlhfppo\">4.RLHFPPO</h1>\n<p>PPO4</p>\n<p>1.ActorSFTactionCriticloss</p>\n<p>2.ReferenceSFTRLHFReferenceActorActorReferenceKL\npenaltyActor</p>\n<p>3.RewardSFTRLHF</p>\n<p>4.CriticRewardActortoken</p>\n<h1 id=\"gptlvhidden-sizehbatch-sizebsadamn\">5.GPTLVhidden\nsizeHbatch\nsizeBSAdamN</h1>\n<p>optimizer</p>\n<p>1.VH12H<sup>2+13H=VH+L(12H</sup>2+13H)2</p>\n<p>2.2</p>\n<p>3.optimizer(+)<em>2+(++2)</em>4=2016</p>\n<p>4.softmaxdropout34BSH+5BNS^22</p>\n<p>\nGPT3175BH=12288L=96N=96350GB=1S=102490GS=81923420G</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n","length":2201,"excerpt":"","more":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM<sub></sub></p>\n<hr>\n<h1 id=\"transformerlayernormbatchnorm\">1.Transformerlayernormbatchnorm</h1>\n<p>NLPpaddingpaddingnormalizationbatchnormbatchtokentokenPowerNorm:\nRethinking Batch Normalization in\nTransformersNLPbatchnormlayernorm</p>\n<h1 id=\"transformerencdoerdecoder\">2.transformerencdoerdecoder</h1>\n<p>decoderself-attentionencoderattentionKVdecoderQcross-attention</p>\n<img src=\"/1736008/transformer.png\" class title=\"transformer\">\n<h1 id=\"pytorchtensorviewreshape\">3.PyTorchTensorview()reshape()</h1>\n<p>1.view()reshape()tensorview()reshape</p>\n<p>2.view()tensor</p>\n<p>3.reshape()tensortensorview()</p>\n<p>4.is_contiguous()contiguous()</p>\n<h1 id=\"rlhfppo\">4.RLHFPPO</h1>\n<p>PPO4</p>\n<p>1.ActorSFTactionCriticloss</p>\n<p>2.ReferenceSFTRLHFReferenceActorActorReferenceKL\npenaltyActor</p>\n<p>3.RewardSFTRLHF</p>\n<p>4.CriticRewardActortoken</p>\n<h1 id=\"gptlvhidden-sizehbatch-sizebsadamn\">5.GPTLVhidden\nsizeHbatch\nsizeBSAdamN</h1>\n<p>optimizer</p>\n<p>1.VH12H<sup>2+13H=VH+L(12H</sup>2+13H)2</p>\n<p>2.2</p>\n<p>3.optimizer(+)<em>2+(++2)</em>4=2016</p>\n<p>4.softmaxdropout34BSH+5BNS^22</p>\n<p>\nGPT3175BH=12288L=96N=96350GB=1S=102490GS=81923420G</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n"},{"title":":sliding window attention","abbrlink":"c61d17e3","date":"2024-03-12T09:26:00.000Z","_content":"\n//  \n\nLLM  \n\n[LLM](http://www.linsight.cn/c4da56c0.html)32k+/128k+[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)MQAGQAKV  \n\nSWAsliding window attention  \n\nQwenMistralSWA  \n\nMistral  \n\nMistral AIAI202352023912Mistral 7BMoEMistral 8x7B  \n\n20242  \n\n{% asset_img ms_invest_mistral.png MS %}  \n\n20242Mistral Large & 32kMMLUGPT4  \n\n{% asset_img mistral_large_performance.jpeg Mistral Large MMLU Performance %}  \n\nMistralOPENAIMETA  \n\n# SWA\n\nSWAMistralMistral 7BSWA  \n\n## Mistral 7B\n\n202310MistralMistral 7B[](https://arxiv.org/pdf/2310.06825.pdf)LlamaMistralGQASWA  \n\nMistral 7B  \n\n{% asset_img mistral_architechture.png Mistral Architechture %}  \n\nMistralkv=8GQAintermediate sizeLlama211008  \n\n## \n\ncausal attentiontokentoken  \n\n $s$ 1 $s^2$ KV Cache  \n\n/ $s$ \n\n1\n\n $[m,n]\\times[n,p]$  $[m,p]$ $m\\times p$  $n$  $n$  $2mpn$ floating point operationsFLOPs  \n\n[Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)  \n\nMHA $s$  $L$ hidden size $d_{model}$  $d_{q}$   $n_{q}$ $d_{model} = n_{q}\\times d_{q}$ operationFLOPs  \n\n<center>\n\n| Operation | FLOPsMHA |\n| :---- | :----: |\n| Attention: QKV | $6\\times s\\times h_{model}^{2}$  |\n| Attention: QK logits ( $QK^T$ ) | $n_{q}\\times 2\\times s^2\\times h_{q}$ |\n| Attention: Softmax | $n_{q}\\times 3\\times s^2$ |\n| Attention: Reduction (apply to $V$) | $n_{q}\\times 2\\times s^2\\times h_{q}$ |\n| Attention: Outupt Linear Project | $2\\times s\\times h_{model}^{2}$ |\n\n</center>\n\nSoftmax $[1,s]$ softmax $3s$  $s$ exp $s$  $s$  $[s,s]$ softmax  $3s^2$  $n_{q}$ \n\noperationscalingdropout\n\nMistral 7BGQA  \n\nKVkv $n_{kv}$\n\n<center>\n\n| Operation | FLOPsGQA |\n| :---- | :----: |\n| Attention: QKV | $2\\times s\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times n_{kv})$  |\n\n</center>\n\nQK logitsSoftmaxReduction $s$   \n\n2\n\nKV Cache  \n\n$$\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\n\n\n$$\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\nMistral 7B16kKV_Cache2G  \n\nGQA16k+\n\n## SWA\n\nattention $s$  $s$ \n\nCNN  \n\n{% asset_img receptive_field_cnn.png CNN Receptive Field %}  \n\n3 $3\\times 3$ CNNsliding window  \n\nlayer 3layer 2 $3\\times 3$ layer 2  \n\nlayer 2layer 1 $3\\times 3$ layer 2 $3\\times 3$ layer 1 $5\\times 5$ layer 3<u>****</u> $5\\times 5$   \n\nlayer 4layer 4layer 1  $7\\times 7$   \n\n  \n\n  \n\nCNN  \n\n  \n\n  \n\n\n\nMistralSWA  \n\n{% asset_img mistral_swa.png Mistral SWA %}  \n\ncausal attentionattention mask  \n\nSWAattention mask33  \n\nCNNLLM  \n\nMistral 7B409632 $4096\\times 32=131,072$ 131k  \n\nattentionQK logitsSoftmaxReduction $s$ SWAoperation4k131k131k $32\\times 32=1024$   \n\n $s$ 131k $31/32$   \n\nSWA4kcausal attention4k\n\n>In practice, for a sequence length of 16K and W = 4096, changes made to FlashAttention [11] and xFormers [18] yield a 2x speed improvement over a vanilla attention baseline.\n\nMistralSWAFlashAttentionxFormers16k2  \n\n## KV Cache\n\nsliding windowKV Cache  \n\nSWAkv  \n\n $W=4$ 5token1tokenkv  \n\n{% asset_img rolling_buffer.png swa rolling buffer %}  \n\nthroughputcase  \n\n## Prompt\n\nRAGfunciton callprompt  \n\nGPT4system promptOPENAI  \n\n>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\" and the user's locale is \"en-US\"\nYour knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07.\nImage input capabilities: Enabled\n>\n>Tools\n>\n>python\n>\n>When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 60.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is disabled. Do not make external web requests or API calls as they will fail.\n>\n>dalle\n>\n>Whenever a description of an image is given, create a prompt that dalle can use to generate the image and abide to the following policy:\n>1. The prompt must be in English. Translate to English if needed.\n>2. DO NOT ask for permission to generate the image, just do it!\n>3. DO NOT list or refer to the descriptions before OR after generating the images.\n>4. Do not create more than 1 image, even if the user requests more.\n>5. Do not create images in the style of artists, creative professionals or studios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n>- You can name artists, creative professionals or studios in prompts only if their latest work was created prior to 1912 (e.g. Van Gogh, Goya)\n>- If asked to generate an image that would violate this policy, instead apply the following procedure: (a) substitute the artist's name with three adjectives that capture key aspects of the style; (b) include an associated artistic movement or era to provide context; and (c) mention the primary medium used by the artist\n>6. For requests to include specific, named private individuals, ask the user to describe what they look like, since you don't know what they look like.\n>7. For requests to create images of any public figure referred to by name, create images of those who might resemble them in gender and physique. But they shouldn't look like them. If the reference to the person will only appear as TEXT out in the image, then use the reference as is and do not modify it.\n>8. Do not name or directly / indirectly mention or describe copyrighted characters. Rewrite prompts to describe in detail a specific different character with a different specific color, hair style, or other defining visual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around 100 words long.\nExample dalle invocation:\n>{\n>\"prompt\": \"<insert prompt here>\"\n>}\n>namespace dalle {\n>\n>Create images from a text-only prompt.\ntype text2im = (_: {\nThe size of the requested image. Use 1024x1024 (square) as the default, 1792x1024 if the user requests a wide image, and 1024x1792 for full-body portraits. Always include this parameter in the request.\nn?: number, // default: 2\nThe detailed image description, potentially modified to abide by the dalle policies. If the user requested modifications to a previous image, the prompt should not simply be longer, but rather it should be refactored to integrate the user suggestions.\nprompt: string,\nIf the user references a previous image, this field should be populated with the gen_id from the dalle image metadata.\nreferenced_image_ids?: string[],\n}) => any;\n} // namespace dalle\n>\n>voice_mode\n>Voice mode functions are not available in text conversations.\n>namespace voice_mode {   } // namespace voice_mode\n>\n>browser\n>\n>You have the tool `browser`. Use `browser` in the following circumstances:\n>    - User is asking about current events or something that requires real-time information (weather, sports scores, etc.)\n>    - User is asking about some term you are totally unfamiliar with (it might be new)\n>    - User explicitly asks you to browse or provide links to references\n>\n>Given a query that requires retrieval, your turn will consist of three steps:\n>1. Call the search function to get a list of results.\n>2. Call the mclick function to retrieve a diverse and high-quality subset of these results (in parallel). Remember to SELECT AT LEAST 3 sources when using `mclick`.\n>3. Write a response to the user based on these results. In your response, cite sources using the citation format below.\n>\n>In some cases, you should repeat step 1 twice, if the initial results are unsatisfactory, and you believe that you can refine the query to get better results.\n>\n>You can also open a url directly if one is provided by the user. Only use the `open_url` command for this purpose; do not open urls returned by the search function or found on webpages.\n>\n>The `browser` tool has the following commands:\n\t`search(query: str, recency_days: int)` Issues a query to a search engine and displays the results.\n         `mclick(ids: list[str])`. Retrieves the contents of the webpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST 3 and at most 10 pages. Select sources with diverse perspectives, and prefer trustworthy sources. Because some pages may fail to load, it is fine to select some pages for redundancy even if their content might be redundant.\n\t`open_url(url: str)` Opens the given URL and displays it.\n>\n>For citing quotes from the 'browser' tool: please render in this format: {message idx}{link text}.\nFor long citations: please render in this format: [link text](message idx).\nOtherwise do not render links.\n\nsystem promptkvsystem promptsliding windowsystem promptkv\n\n $W=4$system prompt9system promptkv [4,4,1]   \n\nwindowattention mask0  \n\nattention mask  \n\n  \n\n  \n\nprompt  \n\n{% asset_img prefill_and_chunking.png prefill and chunking %}  \n\nFlashAttention/PagedAttention\n\nMistral 7BLlamaLlama 34B  \n\n{% asset_img mistral_perf.png mistral performance %}  \n\nMistral7B  \n\n# Sparse Attention\n\nSWAsparse attentionsparse attention  \n\nsparse attention  \n\n## Longformer\n\nMistralSWA  \n\n2020[Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150.pdf)SWAsparse attention  \n\nLongformer  \n\n{% asset_img longformer_attention.png longformer %}  \n\nbSWABert  \n\nSWAdilated sliding window    \n\n{% asset_img dilated_conv.png dilated convolution %}  \n\nattentionSWAdilated sliding window  \n\n  \n\nBert[CLS] tokentoken  \n\nGPTinstructionprompt  \n\ntokenglobal attentionsliding windowd  \n\n## Big Bird\n\n2020Longformersparse attention[Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)  \n\nsliding windowglobal attentionLongformerBig Birdrandom attention  \n\n{% asset_img big_bird_attention.png big bird attention %}  \n\n $r=2$ 2\n\n#   \n\nSWA  \n\nSWAsparse attention<big>****</big>global + local attentionflash attentionrandom attention\n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n1Mistral 7B https://arxiv.org/pdf/2310.06825.pdf  \n2Longformer: The Long-Document Transformer \nhttps://arxiv.org/pdf/2004.05150.pdf  \n3Training Compute-Optimal Large Language Models https://arxiv.org/pdf/2203.15556.pdf  \n4GPT-4 System Prompt Revealed https://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed  \n5Big Bird: Transformers for Longer Sequences https://arxiv.org/abs/2007.14062  ","source":"_posts/cs/nlp/2024/03/LLM-sliding-window-attention.md","raw":"---\ntitle: ':sliding window attention'\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - attention\n  - sliding window attention\n  - sparse attention\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: c61d17e3\ndate: 2024-03-12 17:26:00\n---\n\n//  \n\nLLM  \n\n[LLM](http://www.linsight.cn/c4da56c0.html)32k+/128k+[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)MQAGQAKV  \n\nSWAsliding window attention  \n\nQwenMistralSWA  \n\nMistral  \n\nMistral AIAI202352023912Mistral 7BMoEMistral 8x7B  \n\n20242  \n\n{% asset_img ms_invest_mistral.png MS %}  \n\n20242Mistral Large & 32kMMLUGPT4  \n\n{% asset_img mistral_large_performance.jpeg Mistral Large MMLU Performance %}  \n\nMistralOPENAIMETA  \n\n# SWA\n\nSWAMistralMistral 7BSWA  \n\n## Mistral 7B\n\n202310MistralMistral 7B[](https://arxiv.org/pdf/2310.06825.pdf)LlamaMistralGQASWA  \n\nMistral 7B  \n\n{% asset_img mistral_architechture.png Mistral Architechture %}  \n\nMistralkv=8GQAintermediate sizeLlama211008  \n\n## \n\ncausal attentiontokentoken  \n\n $s$ 1 $s^2$ KV Cache  \n\n/ $s$ \n\n1\n\n $[m,n]\\times[n,p]$  $[m,p]$ $m\\times p$  $n$  $n$  $2mpn$ floating point operationsFLOPs  \n\n[Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)  \n\nMHA $s$  $L$ hidden size $d_{model}$  $d_{q}$   $n_{q}$ $d_{model} = n_{q}\\times d_{q}$ operationFLOPs  \n\n<center>\n\n| Operation | FLOPsMHA |\n| :---- | :----: |\n| Attention: QKV | $6\\times s\\times h_{model}^{2}$  |\n| Attention: QK logits ( $QK^T$ ) | $n_{q}\\times 2\\times s^2\\times h_{q}$ |\n| Attention: Softmax | $n_{q}\\times 3\\times s^2$ |\n| Attention: Reduction (apply to $V$) | $n_{q}\\times 2\\times s^2\\times h_{q}$ |\n| Attention: Outupt Linear Project | $2\\times s\\times h_{model}^{2}$ |\n\n</center>\n\nSoftmax $[1,s]$ softmax $3s$  $s$ exp $s$  $s$  $[s,s]$ softmax  $3s^2$  $n_{q}$ \n\noperationscalingdropout\n\nMistral 7BGQA  \n\nKVkv $n_{kv}$\n\n<center>\n\n| Operation | FLOPsGQA |\n| :---- | :----: |\n| Attention: QKV | $2\\times s\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times n_{kv})$  |\n\n</center>\n\nQK logitsSoftmaxReduction $s$   \n\n2\n\nKV Cache  \n\n$$\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\n\n\n$$\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\nMistral 7B16kKV_Cache2G  \n\nGQA16k+\n\n## SWA\n\nattention $s$  $s$ \n\nCNN  \n\n{% asset_img receptive_field_cnn.png CNN Receptive Field %}  \n\n3 $3\\times 3$ CNNsliding window  \n\nlayer 3layer 2 $3\\times 3$ layer 2  \n\nlayer 2layer 1 $3\\times 3$ layer 2 $3\\times 3$ layer 1 $5\\times 5$ layer 3<u>****</u> $5\\times 5$   \n\nlayer 4layer 4layer 1  $7\\times 7$   \n\n  \n\n  \n\nCNN  \n\n  \n\n  \n\n\n\nMistralSWA  \n\n{% asset_img mistral_swa.png Mistral SWA %}  \n\ncausal attentionattention mask  \n\nSWAattention mask33  \n\nCNNLLM  \n\nMistral 7B409632 $4096\\times 32=131,072$ 131k  \n\nattentionQK logitsSoftmaxReduction $s$ SWAoperation4k131k131k $32\\times 32=1024$   \n\n $s$ 131k $31/32$   \n\nSWA4kcausal attention4k\n\n>In practice, for a sequence length of 16K and W = 4096, changes made to FlashAttention [11] and xFormers [18] yield a 2x speed improvement over a vanilla attention baseline.\n\nMistralSWAFlashAttentionxFormers16k2  \n\n## KV Cache\n\nsliding windowKV Cache  \n\nSWAkv  \n\n $W=4$ 5token1tokenkv  \n\n{% asset_img rolling_buffer.png swa rolling buffer %}  \n\nthroughputcase  \n\n## Prompt\n\nRAGfunciton callprompt  \n\nGPT4system promptOPENAI  \n\n>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\" and the user's locale is \"en-US\"\nYour knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07.\nImage input capabilities: Enabled\n>\n>Tools\n>\n>python\n>\n>When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 60.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is disabled. Do not make external web requests or API calls as they will fail.\n>\n>dalle\n>\n>Whenever a description of an image is given, create a prompt that dalle can use to generate the image and abide to the following policy:\n>1. The prompt must be in English. Translate to English if needed.\n>2. DO NOT ask for permission to generate the image, just do it!\n>3. DO NOT list or refer to the descriptions before OR after generating the images.\n>4. Do not create more than 1 image, even if the user requests more.\n>5. Do not create images in the style of artists, creative professionals or studios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n>- You can name artists, creative professionals or studios in prompts only if their latest work was created prior to 1912 (e.g. Van Gogh, Goya)\n>- If asked to generate an image that would violate this policy, instead apply the following procedure: (a) substitute the artist's name with three adjectives that capture key aspects of the style; (b) include an associated artistic movement or era to provide context; and (c) mention the primary medium used by the artist\n>6. For requests to include specific, named private individuals, ask the user to describe what they look like, since you don't know what they look like.\n>7. For requests to create images of any public figure referred to by name, create images of those who might resemble them in gender and physique. But they shouldn't look like them. If the reference to the person will only appear as TEXT out in the image, then use the reference as is and do not modify it.\n>8. Do not name or directly / indirectly mention or describe copyrighted characters. Rewrite prompts to describe in detail a specific different character with a different specific color, hair style, or other defining visual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around 100 words long.\nExample dalle invocation:\n>{\n>\"prompt\": \"<insert prompt here>\"\n>}\n>namespace dalle {\n>\n>Create images from a text-only prompt.\ntype text2im = (_: {\nThe size of the requested image. Use 1024x1024 (square) as the default, 1792x1024 if the user requests a wide image, and 1024x1792 for full-body portraits. Always include this parameter in the request.\nn?: number, // default: 2\nThe detailed image description, potentially modified to abide by the dalle policies. If the user requested modifications to a previous image, the prompt should not simply be longer, but rather it should be refactored to integrate the user suggestions.\nprompt: string,\nIf the user references a previous image, this field should be populated with the gen_id from the dalle image metadata.\nreferenced_image_ids?: string[],\n}) => any;\n} // namespace dalle\n>\n>voice_mode\n>Voice mode functions are not available in text conversations.\n>namespace voice_mode {   } // namespace voice_mode\n>\n>browser\n>\n>You have the tool `browser`. Use `browser` in the following circumstances:\n>    - User is asking about current events or something that requires real-time information (weather, sports scores, etc.)\n>    - User is asking about some term you are totally unfamiliar with (it might be new)\n>    - User explicitly asks you to browse or provide links to references\n>\n>Given a query that requires retrieval, your turn will consist of three steps:\n>1. Call the search function to get a list of results.\n>2. Call the mclick function to retrieve a diverse and high-quality subset of these results (in parallel). Remember to SELECT AT LEAST 3 sources when using `mclick`.\n>3. Write a response to the user based on these results. In your response, cite sources using the citation format below.\n>\n>In some cases, you should repeat step 1 twice, if the initial results are unsatisfactory, and you believe that you can refine the query to get better results.\n>\n>You can also open a url directly if one is provided by the user. Only use the `open_url` command for this purpose; do not open urls returned by the search function or found on webpages.\n>\n>The `browser` tool has the following commands:\n\t`search(query: str, recency_days: int)` Issues a query to a search engine and displays the results.\n         `mclick(ids: list[str])`. Retrieves the contents of the webpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST 3 and at most 10 pages. Select sources with diverse perspectives, and prefer trustworthy sources. Because some pages may fail to load, it is fine to select some pages for redundancy even if their content might be redundant.\n\t`open_url(url: str)` Opens the given URL and displays it.\n>\n>For citing quotes from the 'browser' tool: please render in this format: {message idx}{link text}.\nFor long citations: please render in this format: [link text](message idx).\nOtherwise do not render links.\n\nsystem promptkvsystem promptsliding windowsystem promptkv\n\n $W=4$system prompt9system promptkv [4,4,1]   \n\nwindowattention mask0  \n\nattention mask  \n\n  \n\n  \n\nprompt  \n\n{% asset_img prefill_and_chunking.png prefill and chunking %}  \n\nFlashAttention/PagedAttention\n\nMistral 7BLlamaLlama 34B  \n\n{% asset_img mistral_perf.png mistral performance %}  \n\nMistral7B  \n\n# Sparse Attention\n\nSWAsparse attentionsparse attention  \n\nsparse attention  \n\n## Longformer\n\nMistralSWA  \n\n2020[Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150.pdf)SWAsparse attention  \n\nLongformer  \n\n{% asset_img longformer_attention.png longformer %}  \n\nbSWABert  \n\nSWAdilated sliding window    \n\n{% asset_img dilated_conv.png dilated convolution %}  \n\nattentionSWAdilated sliding window  \n\n  \n\nBert[CLS] tokentoken  \n\nGPTinstructionprompt  \n\ntokenglobal attentionsliding windowd  \n\n## Big Bird\n\n2020Longformersparse attention[Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)  \n\nsliding windowglobal attentionLongformerBig Birdrandom attention  \n\n{% asset_img big_bird_attention.png big bird attention %}  \n\n $r=2$ 2\n\n#   \n\nSWA  \n\nSWAsparse attention<big>****</big>global + local attentionflash attentionrandom attention\n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n1Mistral 7B https://arxiv.org/pdf/2310.06825.pdf  \n2Longformer: The Long-Document Transformer \nhttps://arxiv.org/pdf/2004.05150.pdf  \n3Training Compute-Optimal Large Language Models https://arxiv.org/pdf/2203.15556.pdf  \n4GPT-4 System Prompt Revealed https://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed  \n5Big Bird: Transformers for Longer Sequences https://arxiv.org/abs/2007.14062  ","slug":"cs/nlp/2024/03/LLM-sliding-window-attention","published":1,"updated":"2024-03-20T11:38:30.908Z","comments":1,"layout":"post","photos":[],"_id":"clv7wdj0p000d794khco2341a","content":"<p>//</p>\n<p>LLM</p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a>32k+/128k+<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a>MQAGQAKV</p>\n<p>SWAsliding\nwindow attention</p>\n<p>QwenMistralSWA</p>\n<p>Mistral</p>\n<p>Mistral\nAIAI202352023912Mistral\n7BMoEMistral 8x7B</p>\n<p>20242</p>\n<img src=\"/c61d17e3/ms_invest_mistral.png\" class title=\"MS\">\n<p>20242Mistral Large &amp;\n32kMMLUGPT4</p>\n<img src=\"/c61d17e3/mistral_large_performance.jpeg\" class title=\"Mistral Large MMLU Performance\">\n<p>MistralOPENAIMETA</p>\n<h1 id=\"swa\">SWA</h1>\n<p>SWAMistralMistral\n7BSWA</p>\n<h2 id=\"mistral-7b\">Mistral 7B</h2>\n<p>202310MistralMistral 7B<a href=\"https://arxiv.org/pdf/2310.06825.pdf\"></a>LlamaMistralGQASWA</p>\n<p>Mistral 7B</p>\n<img src=\"/c61d17e3/mistral_architechture.png\" class title=\"Mistral Architechture\">\n<p>Mistralkv=8GQAintermediate\nsizeLlama211008</p>\n<h2 id=\"\"></h2>\n<p>causal\nattentiontokentoken</p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n1 <span class=\"math inline\">\\(s^2\\)</span> KV\nCache</p>\n<p>/ <span class=\"math inline\">\\(s\\)</span>\n</p>\n<p>1</p>\n<p> <span class=\"math inline\">\\([m,n]\\times[n,p]\\)</span>  <span class=\"math inline\">\\([m,p]\\)</span> <span class=\"math inline\">\\(m\\times p\\)</span>  <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(2mpn\\)</span> floating point\noperationsFLOPs</p>\n<p><a href=\"https://arxiv.org/pdf/2203.15556.pdf\">Training\nCompute-Optimal Large Language Models</a></p>\n<p>MHA <span class=\"math inline\">\\(s\\)</span>\n <span class=\"math inline\">\\(L\\)</span> hidden size\n<span class=\"math inline\">\\(d_{model}\\)</span>  <span class=\"math inline\">\\(d_{q}\\)</span>   <span class=\"math inline\">\\(n_{q}\\)</span> <span class=\"math inline\">\\(d_{model} = n_{q}\\times d_{q}\\)</span>\noperationFLOPs</p>\n<center>\n<table>\n<colgroup>\n<col style=\"width: 45%\">\n<col style=\"width: 54%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPsMHA</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(6\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: QK logits ( <span class=\"math inline\">\\(QK^T\\)</span> )</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n2\\times s^2\\times h_{q}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Softmax</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n3\\times s^2\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: Reduction (apply to <span class=\"math inline\">\\(V\\)</span>)</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n2\\times s^2\\times h_{q}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Outupt Linear Project</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>Softmax <span class=\"math inline\">\\([1,s]\\)</span>\nsoftmax <span class=\"math inline\">\\(3s\\)</span> \n<span class=\"math inline\">\\(s\\)</span> exp <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\([s,s]\\)</span> softmax <span class=\"math inline\">\\(3s^2\\)</span> \n<span class=\"math inline\">\\(n_{q}\\)</span> </p>\n<p>operationscalingdropout</p>\n<p>Mistral 7BGQA</p>\n<p>KVkv <span class=\"math inline\">\\(n_{kv}\\)</span></p>\n<center>\n<table>\n<colgroup>\n<col style=\"width: 45%\">\n<col style=\"width: 54%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPsGQA</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times\nn_{kv})\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>QK logitsSoftmaxReduction <span class=\"math inline\">\\(s\\)</span> </p>\n<p>2</p>\n<p>KV Cache</p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p>Mistral 7B16kKV_Cache2G</p>\n<p>GQA16k+</p>\n<h2 id=\"swa\">SWA</h2>\n<p>attention <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(s\\)</span>\n</p>\n<p>CNN</p>\n<img src=\"/c61d17e3/receptive_field_cnn.png\" class title=\"CNN Receptive Field\">\n<p>3 <span class=\"math inline\">\\(3\\times 3\\)</span>\nCNNsliding window</p>\n<p>layer 3layer 2 <span class=\"math inline\">\\(3\\times 3\\)</span> layer\n2</p>\n<p>layer 2layer 1 <span class=\"math inline\">\\(3\\times 3\\)</span> layer 2 <span class=\"math inline\">\\(3\\times 3\\)</span> layer 1\n<span class=\"math inline\">\\(5\\times 5\\)</span> layer\n3<u><strong></strong></u> <span class=\"math inline\">\\(5\\times 5\\)</span> </p>\n<p>layer 4layer\n4layer 1  <span class=\"math inline\">\\(7\\times 7\\)</span> </p>\n<p></p>\n<p></p>\n<p>CNN</p>\n<p></p>\n<p></p>\n<p></p>\n<p>MistralSWA</p>\n<img src=\"/c61d17e3/mistral_swa.png\" class title=\"Mistral SWA\">\n<p>causal\nattentionattention\nmask</p>\n<p>SWAattention\nmask33</p>\n<p>CNNLLM</p>\n<p>Mistral 7B409632\n<span class=\"math inline\">\\(4096\\times 32=131,072\\)</span>\n131k</p>\n<p>attentionQK\nlogitsSoftmaxReduction <span class=\"math inline\">\\(s\\)</span>\nSWAoperation4k131k131k\n<span class=\"math inline\">\\(32\\times 32=1024\\)</span> </p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n131k <span class=\"math inline\">\\(31/32\\)</span> </p>\n<p>SWA4kcausal\nattention4k</p>\n<blockquote>\n<p>In practice, for a sequence length of 16K and W = 4096, changes made\nto FlashAttention [11] and xFormers [18] yield a 2x speed improvement\nover a vanilla attention baseline.</p>\n</blockquote>\n<p>MistralSWAFlashAttentionxFormers16k2</p>\n<h2 id=\"kv-cache\">KV Cache</h2>\n<p>sliding windowKV\nCache</p>\n<p>SWAkv</p>\n<p> <span class=\"math inline\">\\(W=4\\)</span>\n5token1tokenkv</p>\n<img src=\"/c61d17e3/rolling_buffer.png\" class title=\"swa rolling buffer\">\n<p>throughputcase</p>\n<h2 id=\"prompt\">Prompt</h2>\n<p>RAGfunciton\ncallprompt</p>\n<p>GPT4system\npromptOPENAI</p>\n<blockquote>\n<p>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\nAppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\nand the user's locale is \"en-US\" Your knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07. Image input capabilities: Enabled</p>\n<p>Tools</p>\n<p>python</p>\n<p>When you send a message containing Python code to python, it will be\nexecuted in a stateful Jupyter notebook environment. python will respond\nwith the output of the execution or time out after 60.0 seconds. The\ndrive at '/mnt/data' can be used to save and persist user files.\nInternet access for this session is disabled. Do not make external web\nrequests or API calls as they will fail.</p>\n<p>dalle</p>\n<p>Whenever a description of an image is given, create a prompt that\ndalle can use to generate the image and abide to the following policy:\n1. The prompt must be in English. Translate to English if needed. 2. DO\nNOT ask for permission to generate the image, just do it! 3. DO NOT list\nor refer to the descriptions before OR after generating the images. 4.\nDo not create more than 1 image, even if the user requests more. 5. Do\nnot create images in the style of artists, creative professionals or\nstudios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n- You can name artists, creative professionals or studios in prompts\nonly if their latest work was created prior to 1912 (e.g. Van Gogh,\nGoya) - If asked to generate an image that would violate this policy,\ninstead apply the following procedure: (a) substitute the artist's name\nwith three adjectives that capture key aspects of the style; (b) include\nan associated artistic movement or era to provide context; and (c)\nmention the primary medium used by the artist 6. For requests to include\nspecific, named private individuals, ask the user to describe what they\nlook like, since you don't know what they look like. 7. For requests to\ncreate images of any public figure referred to by name, create images of\nthose who might resemble them in gender and physique. But they shouldn't\nlook like them. If the reference to the person will only appear as TEXT\nout in the image, then use the reference as is and do not modify it. 8.\nDo not name or directly / indirectly mention or describe copyrighted\ncharacters. Rewrite prompts to describe in detail a specific different\ncharacter with a different specific color, hair style, or other defining\nvisual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around\n100 words long. Example dalle invocation: { \"prompt\":\n\"<insert prompt here>\" } namespace dalle {</insert></p>\n<p>Create images from a text-only prompt. type text2im = (_: { The size\nof the requested image. Use 1024x1024 (square) as the default, 1792x1024\nif the user requests a wide image, and 1024x1792 for full-body\nportraits. Always include this parameter in the request. n?: number, //\ndefault: 2 The detailed image description, potentially modified to abide\nby the dalle policies. If the user requested modifications to a previous\nimage, the prompt should not simply be longer, but rather it should be\nrefactored to integrate the user suggestions. prompt: string, If the\nuser references a previous image, this field should be populated with\nthe gen_id from the dalle image metadata. referenced_image_ids?:\nstring[], }) =&gt; any; } // namespace dalle</p>\n<p>voice_mode Voice mode functions are not available in text\nconversations. namespace voice_mode { } // namespace voice_mode</p>\n<p>browser</p>\n<p>You have the tool <code>browser</code>. Use <code>browser</code> in\nthe following circumstances: - User is asking about current events or\nsomething that requires real-time information (weather, sports scores,\netc.) - User is asking about some term you are totally unfamiliar with\n(it might be new) - User explicitly asks you to browse or provide links\nto references</p>\n<p>Given a query that requires retrieval, your turn will consist of\nthree steps: 1. Call the search function to get a list of results. 2.\nCall the mclick function to retrieve a diverse and high-quality subset\nof these results (in parallel). Remember to SELECT AT LEAST 3 sources\nwhen using <code>mclick</code>. 3. Write a response to the user based on\nthese results. In your response, cite sources using the citation format\nbelow.</p>\n<p>In some cases, you should repeat step 1 twice, if the initial results\nare unsatisfactory, and you believe that you can refine the query to get\nbetter results.</p>\n<p>You can also open a url directly if one is provided by the user. Only\nuse the <code>open_url</code> command for this purpose; do not open urls\nreturned by the search function or found on webpages.</p>\n<p>The <code>browser</code> tool has the following commands:\n<code>search(query: str, recency_days: int)</code> Issues a query to a\nsearch engine and displays the results.\n<code>mclick(ids: list[str])</code>. Retrieves the contents of the\nwebpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST\n3 and at most 10 pages. Select sources with diverse perspectives, and\nprefer trustworthy sources. Because some pages may fail to load, it is\nfine to select some pages for redundancy even if their content might be\nredundant. <code>open_url(url: str)</code> Opens the given URL and\ndisplays it.</p>\n<p>For citing quotes from the 'browser' tool: please render in this\nformat: {message idx}{link text}. For long citations: please render\nin this format: <a href=\"message%20idx\">link text</a>. Otherwise do not\nrender links.</p>\n</blockquote>\n<p>system\npromptkvsystem\npromptsliding windowsystem\npromptkv</p>\n<p> <span class=\"math inline\">\\(W=4\\)</span>system\nprompt9system promptkv [4,4,1] </p>\n<p>windowattention\nmask0</p>\n<p>attention\nmask</p>\n<p></p>\n<p></p>\n<p>prompt</p>\n<img src=\"/c61d17e3/prefill_and_chunking.png\" class title=\"prefill and chunking\">\n<p>FlashAttention/PagedAttention</p>\n<p>Mistral\n7BLlamaLlama\n34B</p>\n<img src=\"/c61d17e3/mistral_perf.png\" class title=\"mistral performance\">\n<p>Mistral7B</p>\n<h1 id=\"sparse-attention\">Sparse Attention</h1>\n<p>SWAsparse attentionsparse\nattention</p>\n<p>sparse\nattention</p>\n<h2 id=\"longformer\">Longformer</h2>\n<p>MistralSWA</p>\n<p>2020<a href=\"https://arxiv.org/pdf/2004.05150.pdf\">Longformer:\nThe Long-Document Transformer</a>SWAsparse\nattention</p>\n<p>Longformer</p>\n<img src=\"/c61d17e3/longformer_attention.png\" class title=\"longformer\">\n<p>bSWABert</p>\n<p>SWAdilated sliding\nwindow</p>\n<img src=\"/c61d17e3/dilated_conv.png\" class title=\"dilated convolution\">\n<p>attentionSWAdilated sliding\nwindow</p>\n<p></p>\n<p>Bert[CLS]\ntokentoken</p>\n<p>GPTinstructionprompt</p>\n<p>tokenglobal\nattentionsliding windowd</p>\n<h2 id=\"big-bird\">Big Bird</h2>\n<p>2020Longformersparse\nattention<a href=\"https://arxiv.org/abs/2007.14062\">Big Bird: Transformers for\nLonger Sequences</a></p>\n<p>sliding windowglobal attentionLongformerBig\nBirdrandom attention</p>\n<img src=\"/c61d17e3/big_bird_attention.png\" class title=\"big bird attention\">\n<p> <span class=\"math inline\">\\(r=2\\)</span>\n2</p>\n<h1 id=\"\"></h1>\n<p>SWA</p>\n<p>SWAsparse\nattention<big><strong></strong></big>global\n+ local attentionflash attentionrandom\nattention</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Mistral 7B https://arxiv.org/pdf/2310.06825.pdf<br>\n2Longformer: The Long-Document Transformer\nhttps://arxiv.org/pdf/2004.05150.pdf<br>\n3Training Compute-Optimal Large Language Models\nhttps://arxiv.org/pdf/2203.15556.pdf<br>\n4GPT-4 System Prompt Revealed\nhttps://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed<br>\n5Big Bird: Transformers for Longer Sequences\nhttps://arxiv.org/abs/2007.14062</p>\n","length":10783,"excerpt":"","more":"<p>//</p>\n<p>LLM</p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a>32k+/128k+<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a>MQAGQAKV</p>\n<p>SWAsliding\nwindow attention</p>\n<p>QwenMistralSWA</p>\n<p>Mistral</p>\n<p>Mistral\nAIAI202352023912Mistral\n7BMoEMistral 8x7B</p>\n<p>20242</p>\n<img src=\"/c61d17e3/ms_invest_mistral.png\" class title=\"MS\">\n<p>20242Mistral Large &amp;\n32kMMLUGPT4</p>\n<img src=\"/c61d17e3/mistral_large_performance.jpeg\" class title=\"Mistral Large MMLU Performance\">\n<p>MistralOPENAIMETA</p>\n<h1 id=\"swa\">SWA</h1>\n<p>SWAMistralMistral\n7BSWA</p>\n<h2 id=\"mistral-7b\">Mistral 7B</h2>\n<p>202310MistralMistral 7B<a href=\"https://arxiv.org/pdf/2310.06825.pdf\"></a>LlamaMistralGQASWA</p>\n<p>Mistral 7B</p>\n<img src=\"/c61d17e3/mistral_architechture.png\" class title=\"Mistral Architechture\">\n<p>Mistralkv=8GQAintermediate\nsizeLlama211008</p>\n<h2 id=\"\"></h2>\n<p>causal\nattentiontokentoken</p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n1 <span class=\"math inline\">\\(s^2\\)</span> KV\nCache</p>\n<p>/ <span class=\"math inline\">\\(s\\)</span>\n</p>\n<p>1</p>\n<p> <span class=\"math inline\">\\([m,n]\\times[n,p]\\)</span>  <span class=\"math inline\">\\([m,p]\\)</span> <span class=\"math inline\">\\(m\\times p\\)</span>  <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(2mpn\\)</span> floating point\noperationsFLOPs</p>\n<p><a href=\"https://arxiv.org/pdf/2203.15556.pdf\">Training\nCompute-Optimal Large Language Models</a></p>\n<p>MHA <span class=\"math inline\">\\(s\\)</span>\n <span class=\"math inline\">\\(L\\)</span> hidden size\n<span class=\"math inline\">\\(d_{model}\\)</span>  <span class=\"math inline\">\\(d_{q}\\)</span>   <span class=\"math inline\">\\(n_{q}\\)</span> <span class=\"math inline\">\\(d_{model} = n_{q}\\times d_{q}\\)</span>\noperationFLOPs</p>\n<center>\n<table>\n<colgroup>\n<col style=\"width: 45%\">\n<col style=\"width: 54%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPsMHA</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(6\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: QK logits ( <span class=\"math inline\">\\(QK^T\\)</span> )</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n2\\times s^2\\times h_{q}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Softmax</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n3\\times s^2\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: Reduction (apply to <span class=\"math inline\">\\(V\\)</span>)</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n2\\times s^2\\times h_{q}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Outupt Linear Project</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>Softmax <span class=\"math inline\">\\([1,s]\\)</span>\nsoftmax <span class=\"math inline\">\\(3s\\)</span> \n<span class=\"math inline\">\\(s\\)</span> exp <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\([s,s]\\)</span> softmax <span class=\"math inline\">\\(3s^2\\)</span> \n<span class=\"math inline\">\\(n_{q}\\)</span> </p>\n<p>operationscalingdropout</p>\n<p>Mistral 7BGQA</p>\n<p>KVkv <span class=\"math inline\">\\(n_{kv}\\)</span></p>\n<center>\n<table>\n<colgroup>\n<col style=\"width: 45%\">\n<col style=\"width: 54%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPsGQA</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times\nn_{kv})\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>QK logitsSoftmaxReduction <span class=\"math inline\">\\(s\\)</span> </p>\n<p>2</p>\n<p>KV Cache</p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p>Mistral 7B16kKV_Cache2G</p>\n<p>GQA16k+</p>\n<h2 id=\"swa\">SWA</h2>\n<p>attention <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(s\\)</span>\n</p>\n<p>CNN</p>\n<img src=\"/c61d17e3/receptive_field_cnn.png\" class title=\"CNN Receptive Field\">\n<p>3 <span class=\"math inline\">\\(3\\times 3\\)</span>\nCNNsliding window</p>\n<p>layer 3layer 2 <span class=\"math inline\">\\(3\\times 3\\)</span> layer\n2</p>\n<p>layer 2layer 1 <span class=\"math inline\">\\(3\\times 3\\)</span> layer 2 <span class=\"math inline\">\\(3\\times 3\\)</span> layer 1\n<span class=\"math inline\">\\(5\\times 5\\)</span> layer\n3<u><strong></strong></u> <span class=\"math inline\">\\(5\\times 5\\)</span> </p>\n<p>layer 4layer\n4layer 1  <span class=\"math inline\">\\(7\\times 7\\)</span> </p>\n<p></p>\n<p></p>\n<p>CNN</p>\n<p></p>\n<p></p>\n<p></p>\n<p>MistralSWA</p>\n<img src=\"/c61d17e3/mistral_swa.png\" class title=\"Mistral SWA\">\n<p>causal\nattentionattention\nmask</p>\n<p>SWAattention\nmask33</p>\n<p>CNNLLM</p>\n<p>Mistral 7B409632\n<span class=\"math inline\">\\(4096\\times 32=131,072\\)</span>\n131k</p>\n<p>attentionQK\nlogitsSoftmaxReduction <span class=\"math inline\">\\(s\\)</span>\nSWAoperation4k131k131k\n<span class=\"math inline\">\\(32\\times 32=1024\\)</span> </p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n131k <span class=\"math inline\">\\(31/32\\)</span> </p>\n<p>SWA4kcausal\nattention4k</p>\n<blockquote>\n<p>In practice, for a sequence length of 16K and W = 4096, changes made\nto FlashAttention [11] and xFormers [18] yield a 2x speed improvement\nover a vanilla attention baseline.</p>\n</blockquote>\n<p>MistralSWAFlashAttentionxFormers16k2</p>\n<h2 id=\"kv-cache\">KV Cache</h2>\n<p>sliding windowKV\nCache</p>\n<p>SWAkv</p>\n<p> <span class=\"math inline\">\\(W=4\\)</span>\n5token1tokenkv</p>\n<img src=\"/c61d17e3/rolling_buffer.png\" class title=\"swa rolling buffer\">\n<p>throughputcase</p>\n<h2 id=\"prompt\">Prompt</h2>\n<p>RAGfunciton\ncallprompt</p>\n<p>GPT4system\npromptOPENAI</p>\n<blockquote>\n<p>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\nAppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\nand the user's locale is \"en-US\" Your knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07. Image input capabilities: Enabled</p>\n<p>Tools</p>\n<p>python</p>\n<p>When you send a message containing Python code to python, it will be\nexecuted in a stateful Jupyter notebook environment. python will respond\nwith the output of the execution or time out after 60.0 seconds. The\ndrive at '/mnt/data' can be used to save and persist user files.\nInternet access for this session is disabled. Do not make external web\nrequests or API calls as they will fail.</p>\n<p>dalle</p>\n<p>Whenever a description of an image is given, create a prompt that\ndalle can use to generate the image and abide to the following policy:\n1. The prompt must be in English. Translate to English if needed. 2. DO\nNOT ask for permission to generate the image, just do it! 3. DO NOT list\nor refer to the descriptions before OR after generating the images. 4.\nDo not create more than 1 image, even if the user requests more. 5. Do\nnot create images in the style of artists, creative professionals or\nstudios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n- You can name artists, creative professionals or studios in prompts\nonly if their latest work was created prior to 1912 (e.g. Van Gogh,\nGoya) - If asked to generate an image that would violate this policy,\ninstead apply the following procedure: (a) substitute the artist's name\nwith three adjectives that capture key aspects of the style; (b) include\nan associated artistic movement or era to provide context; and (c)\nmention the primary medium used by the artist 6. For requests to include\nspecific, named private individuals, ask the user to describe what they\nlook like, since you don't know what they look like. 7. For requests to\ncreate images of any public figure referred to by name, create images of\nthose who might resemble them in gender and physique. But they shouldn't\nlook like them. If the reference to the person will only appear as TEXT\nout in the image, then use the reference as is and do not modify it. 8.\nDo not name or directly / indirectly mention or describe copyrighted\ncharacters. Rewrite prompts to describe in detail a specific different\ncharacter with a different specific color, hair style, or other defining\nvisual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around\n100 words long. Example dalle invocation: { \"prompt\":\n\"<insert prompt here>\" } namespace dalle {</insert></p>\n<p>Create images from a text-only prompt. type text2im = (_: { The size\nof the requested image. Use 1024x1024 (square) as the default, 1792x1024\nif the user requests a wide image, and 1024x1792 for full-body\nportraits. Always include this parameter in the request. n?: number, //\ndefault: 2 The detailed image description, potentially modified to abide\nby the dalle policies. If the user requested modifications to a previous\nimage, the prompt should not simply be longer, but rather it should be\nrefactored to integrate the user suggestions. prompt: string, If the\nuser references a previous image, this field should be populated with\nthe gen_id from the dalle image metadata. referenced_image_ids?:\nstring[], }) =&gt; any; } // namespace dalle</p>\n<p>voice_mode Voice mode functions are not available in text\nconversations. namespace voice_mode { } // namespace voice_mode</p>\n<p>browser</p>\n<p>You have the tool <code>browser</code>. Use <code>browser</code> in\nthe following circumstances: - User is asking about current events or\nsomething that requires real-time information (weather, sports scores,\netc.) - User is asking about some term you are totally unfamiliar with\n(it might be new) - User explicitly asks you to browse or provide links\nto references</p>\n<p>Given a query that requires retrieval, your turn will consist of\nthree steps: 1. Call the search function to get a list of results. 2.\nCall the mclick function to retrieve a diverse and high-quality subset\nof these results (in parallel). Remember to SELECT AT LEAST 3 sources\nwhen using <code>mclick</code>. 3. Write a response to the user based on\nthese results. In your response, cite sources using the citation format\nbelow.</p>\n<p>In some cases, you should repeat step 1 twice, if the initial results\nare unsatisfactory, and you believe that you can refine the query to get\nbetter results.</p>\n<p>You can also open a url directly if one is provided by the user. Only\nuse the <code>open_url</code> command for this purpose; do not open urls\nreturned by the search function or found on webpages.</p>\n<p>The <code>browser</code> tool has the following commands:\n<code>search(query: str, recency_days: int)</code> Issues a query to a\nsearch engine and displays the results.\n<code>mclick(ids: list[str])</code>. Retrieves the contents of the\nwebpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST\n3 and at most 10 pages. Select sources with diverse perspectives, and\nprefer trustworthy sources. Because some pages may fail to load, it is\nfine to select some pages for redundancy even if their content might be\nredundant. <code>open_url(url: str)</code> Opens the given URL and\ndisplays it.</p>\n<p>For citing quotes from the 'browser' tool: please render in this\nformat: {message idx}{link text}. For long citations: please render\nin this format: <a href=\"message%20idx\">link text</a>. Otherwise do not\nrender links.</p>\n</blockquote>\n<p>system\npromptkvsystem\npromptsliding windowsystem\npromptkv</p>\n<p> <span class=\"math inline\">\\(W=4\\)</span>system\nprompt9system promptkv [4,4,1] </p>\n<p>windowattention\nmask0</p>\n<p>attention\nmask</p>\n<p></p>\n<p></p>\n<p>prompt</p>\n<img src=\"/c61d17e3/prefill_and_chunking.png\" class title=\"prefill and chunking\">\n<p>FlashAttention/PagedAttention</p>\n<p>Mistral\n7BLlamaLlama\n34B</p>\n<img src=\"/c61d17e3/mistral_perf.png\" class title=\"mistral performance\">\n<p>Mistral7B</p>\n<h1 id=\"sparse-attention\">Sparse Attention</h1>\n<p>SWAsparse attentionsparse\nattention</p>\n<p>sparse\nattention</p>\n<h2 id=\"longformer\">Longformer</h2>\n<p>MistralSWA</p>\n<p>2020<a href=\"https://arxiv.org/pdf/2004.05150.pdf\">Longformer:\nThe Long-Document Transformer</a>SWAsparse\nattention</p>\n<p>Longformer</p>\n<img src=\"/c61d17e3/longformer_attention.png\" class title=\"longformer\">\n<p>bSWABert</p>\n<p>SWAdilated sliding\nwindow</p>\n<img src=\"/c61d17e3/dilated_conv.png\" class title=\"dilated convolution\">\n<p>attentionSWAdilated sliding\nwindow</p>\n<p></p>\n<p>Bert[CLS]\ntokentoken</p>\n<p>GPTinstructionprompt</p>\n<p>tokenglobal\nattentionsliding windowd</p>\n<h2 id=\"big-bird\">Big Bird</h2>\n<p>2020Longformersparse\nattention<a href=\"https://arxiv.org/abs/2007.14062\">Big Bird: Transformers for\nLonger Sequences</a></p>\n<p>sliding windowglobal attentionLongformerBig\nBirdrandom attention</p>\n<img src=\"/c61d17e3/big_bird_attention.png\" class title=\"big bird attention\">\n<p> <span class=\"math inline\">\\(r=2\\)</span>\n2</p>\n<h1 id=\"\"></h1>\n<p>SWA</p>\n<p>SWAsparse\nattention<big><strong></strong></big>global\n+ local attentionflash attentionrandom\nattention</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Mistral 7B https://arxiv.org/pdf/2310.06825.pdf<br>\n2Longformer: The Long-Document Transformer\nhttps://arxiv.org/pdf/2004.05150.pdf<br>\n3Training Compute-Optimal Large Language Models\nhttps://arxiv.org/pdf/2203.15556.pdf<br>\n4GPT-4 System Prompt Revealed\nhttps://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed<br>\n5Big Bird: Transformers for Longer Sequences\nhttps://arxiv.org/abs/2007.14062</p>\n"},{"title":"Yi-","abbrlink":"41b6a819","date":"2024-03-26T08:51:08.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n01.AIAI20231101.AIYi-6BYi-34B baseYi-6B-200KYi-34B-200K baseYi01.AIchatYi-9B  \n\n202311YiSuperCLUE/CMMLUYiYi  \n\n20243Yi  \n\n# TL;DR\n\n  \n\n- Yi-34Bint4float16<1%RTX409024G\n- LLAMA2\n- 3.1Tscaling law1T\n- <10k\n- 4k\n- \n\n# \n\n##   \n\nYi6B9B34B34B  \n\n34B24GRTX4090  \n\nint434B24GGPU  \n\n[Understanding INT4 Quantization for Language Models: Latency Speedup, Composability, and Failure Cases](https://arxiv.org/abs/2301.12017)Yi-34Bint8bf16<1%int4  \n\n{% asset_img eval.png Yi %}  \n\n3.1T tokenDeepMindscaling law1TB<2T  \n\nChinchilla[Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)scaling law  \n\nscaling lawYiovertrain  \n\n<big>**Yi+--34B+70B**</big>  \n\n##   \n\nLLAMA2  \n\n- LLAMA270BGQAYiGQA  \n- RoPERoPE ABFEffective long-context scaling of foundation modelsbase10M  \n- SwiGLUGLU Variants Improve Transformer  \n\nactivation size4h8/3hGQA  \n\n> We use SwiGLU as Yis post-attention layer, reducing its activation size from 4h to 8/3h (h denotes hidden size) to be consistent with the normal post-attention layer. This adjustment also compensates for the reduction in parameter resulted from GQA, making the overall parameter count comparible of existing 7B and 34B models.  \n\n{% asset_img model.png Yi %}  \n\n<big>****</big>  \n\n## tokenizer  \n\n- BPE64000  \n- digit  \n- OOVunicode  \n-   \n  \nLLMtokenizerYi  \n\n#   \n\nLLMYi  \n\n{% asset_img cover.png  %}  \n\n##   \n\n  \n\n{% asset_img pretrain_data_pipeline.png  %}  \n\n1.  &   \n\n  \n\nCCNeTCCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data  \n\n2.  Heuristic Rule Filters  \n\n  \n\n- URL  \n-   \n- n-gramScaling Language Models: Methods, Analysis & Insights from Training GopherCulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages\n- Personal Identifiable InformationPII\n\n3.  Learned Filters  \n\n4scorer  \n\n- Perplexity ScorerCCNet: Extracting High Quality Monolingual Datasets from Web Crawl Datakenlmperplexity\n- Quality Scorer\n- Document Coherence Scorer\n- Safety Scorer\n\n4.  Cluster-based Filters  \n\n  \n\n5. \n\nThe RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Onlyminhash  \n\n  \n\n{% asset_img pretrain_data_dist.png  %}  \n\ngarbage ingarbage out\n\n> we prefer 3T tokens over sophasticated engineering over 10T tokens without extensive filtering\n\n10T3  \n\n##  \n\nQuality is All You Need  \n\n<10kSFT  \n\nGemini: A family of highly capable multimodal models.Llama 2: Open Foundation and Fine-Tuned Chat ModelsLima: Less is more for alignmentFLANScaling instruction-finetuned language modelsUltraChatEnhancing chat language models by scaling high-quality instructional conversations  \n\n  \n\n- <big>**prompt distribution selection**</big>Wizardlm: Empowering large language models to follow complex instructionsSFT  \n- <big>**CoT data formatting**</big>Take a step back: Evoking reasoning via abstraction in large language modelsStep-Back  \n- <big>**response formatting**</big>Lima: Less is more for alignmentresponseintroduction-body-conclusionwhere the body is usually a list of bullet point  \n- <big>****</big>responseresponse  \n- <big>****</big>response  \n- <big>****</big>#instag: Instruction tagging for analyzing supervised fine-tuning of large language models  \n- <big>****</big>How abilities in large language models are affected by supervised fine-tuning data compositionapproximate grid search{1, 1/2, 1/4, 1/8, 1/16, 1/32, 1/64}  \n- <big>****</big>OPENAIChatML[https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md](https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md)system promptuser inputbot response\n\nSFT\n\n{% asset_img sft.png SFT %}  \n\n#   \n\n## infra\n\nYi  \n\n(1) \n(2) \n(3) DPOMegatronDeepSpeed\n(4) LLMcontinuous batching  paged attention\n\nUI  \n\n##   \n\n4k  \n\n##   \n\n  \n\n- AdamWbeta=[0.9,0.999]epsilon = 1e-8  \n- seq_len = 4096  \n- batch size = 64  \n- constant lr = 1e-5weight decay = 0.1  \n- gradient clip = 1.0  \n- max step = 300\n- Neftune: Noisy embeddings improve instruction finetuning6B noise scale = 534B noise scale = 45\n\n# \n\n## \n\n1.   \n\nYi  \n\n{% asset_img base_model_eval.png Base %}  \n\nGPT3.5GPT4Yi  \n\nYi  \n\n2.   \n\n- Yi-34BYi-6BYi-34BYi-6B  \n- Yi-34BQwen-14BFalcon-180B\n- GPT-4LLMLLMGPT-4GPT-3.5LLMQwen-14BYi-34BC-EvalCMMLUGaokaoGPT-4BBHHumanEvalMATH  \n\n\n3. In-Context Learning  \n\nYiin-context learning-underlying function  \n\n y = w1x1 + w2x2 + ... + wnxn  \n\n x1, x2, ..., xn, y x  y  \n\n w1, w2, ..., wn  \n\na y  y  |y  y| b y == y   \n\nunderlying function  \n\n[1,-1]Yi-34BLLAMA-70B  \n\n[11111]LLAMA-70BMistral 8*7B  \n\n{% asset_img ict.png ICT %}  \n\n## Chat  \n\n1. \n\nbasezero-shotfew-shot\n\n{% asset_img eval.png Yi %}  \n\nGoodharts principle  \n\nYi-34B-ChatYi-6B-ChatSFT  \n\n2. \n\n{% asset_img third_party.png  %}  \n\n#   \n\nbase3  \n\n##   \n\n4kbase200kSFT  \n\nattentionattentionsparse attention  \n\n12length-upsampled long-context data3  \n\nrecitation  \n\nData engineering for scaling language models to 128k contextParaphrasing the original text makes high accuracy long-context qa  \n\n5B tokenbatch size=4Mtoken100step1005B/4M=1250  \n\n> We continue pretrain the model on 5B tokens with 4M batch size, which translate to 100 optimization steps. Aligning with the concurrent work from Fu et al. [22], we observe that such light-weight continue pretraining is already able to enable a strong performance on Needle-in-a-Haystack test, as we will show in Figure 6.\n\nData engineering for scaling language models to 128k context  \n\nSFT  \n\n  \n\n  \n\n\n\n  \n\n{% asset_img long_context_result.png  %}  \n\n##   \n\nViTCLIP ViT-H/14 modeltransformerYi-Chat  \n\n{% asset_img multimodal.png  %}  \n\n3  \n\n1224^2ViTprojection1-LAION-400MViTViTLLM  \n\n2ViT448^2LAION-400M2000-480-CLLaVALLaVARFlickrVQAv2RefCOCOVisual7w  \n\n3100-GQAVizWiz VQATextCapsOCR-VQAVisual GenomeShareGPT4V50,000  \n\n128A1006B334B10  \n\n## Depth Upscaling \n\n326B489B  \n\nScaling large language models with simple yet effective depth up-scaling12-281648  \n\ncosine similarity  \n\n  \n\n{% asset_img 9B.png 9B %}  \n\n  \n\nDepth Upscaling  \n\n800B token  \n\n70%  \n\nconstant lr = 3e-54M tokenbatch size  \n\nbatch sizeYi-6B  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n","source":"_posts/cs/nlp/2024/03/Yi-.md","raw":"---\ntitle: Yi-\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 41b6a819\ndate: 2024-03-26 16:51:08\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n01.AIAI20231101.AIYi-6BYi-34B baseYi-6B-200KYi-34B-200K baseYi01.AIchatYi-9B  \n\n202311YiSuperCLUE/CMMLUYiYi  \n\n20243Yi  \n\n# TL;DR\n\n  \n\n- Yi-34Bint4float16<1%RTX409024G\n- LLAMA2\n- 3.1Tscaling law1T\n- <10k\n- 4k\n- \n\n# \n\n##   \n\nYi6B9B34B34B  \n\n34B24GRTX4090  \n\nint434B24GGPU  \n\n[Understanding INT4 Quantization for Language Models: Latency Speedup, Composability, and Failure Cases](https://arxiv.org/abs/2301.12017)Yi-34Bint8bf16<1%int4  \n\n{% asset_img eval.png Yi %}  \n\n3.1T tokenDeepMindscaling law1TB<2T  \n\nChinchilla[Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)scaling law  \n\nscaling lawYiovertrain  \n\n<big>**Yi+--34B+70B**</big>  \n\n##   \n\nLLAMA2  \n\n- LLAMA270BGQAYiGQA  \n- RoPERoPE ABFEffective long-context scaling of foundation modelsbase10M  \n- SwiGLUGLU Variants Improve Transformer  \n\nactivation size4h8/3hGQA  \n\n> We use SwiGLU as Yis post-attention layer, reducing its activation size from 4h to 8/3h (h denotes hidden size) to be consistent with the normal post-attention layer. This adjustment also compensates for the reduction in parameter resulted from GQA, making the overall parameter count comparible of existing 7B and 34B models.  \n\n{% asset_img model.png Yi %}  \n\n<big>****</big>  \n\n## tokenizer  \n\n- BPE64000  \n- digit  \n- OOVunicode  \n-   \n  \nLLMtokenizerYi  \n\n#   \n\nLLMYi  \n\n{% asset_img cover.png  %}  \n\n##   \n\n  \n\n{% asset_img pretrain_data_pipeline.png  %}  \n\n1.  &   \n\n  \n\nCCNeTCCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data  \n\n2.  Heuristic Rule Filters  \n\n  \n\n- URL  \n-   \n- n-gramScaling Language Models: Methods, Analysis & Insights from Training GopherCulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages\n- Personal Identifiable InformationPII\n\n3.  Learned Filters  \n\n4scorer  \n\n- Perplexity ScorerCCNet: Extracting High Quality Monolingual Datasets from Web Crawl Datakenlmperplexity\n- Quality Scorer\n- Document Coherence Scorer\n- Safety Scorer\n\n4.  Cluster-based Filters  \n\n  \n\n5. \n\nThe RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Onlyminhash  \n\n  \n\n{% asset_img pretrain_data_dist.png  %}  \n\ngarbage ingarbage out\n\n> we prefer 3T tokens over sophasticated engineering over 10T tokens without extensive filtering\n\n10T3  \n\n##  \n\nQuality is All You Need  \n\n<10kSFT  \n\nGemini: A family of highly capable multimodal models.Llama 2: Open Foundation and Fine-Tuned Chat ModelsLima: Less is more for alignmentFLANScaling instruction-finetuned language modelsUltraChatEnhancing chat language models by scaling high-quality instructional conversations  \n\n  \n\n- <big>**prompt distribution selection**</big>Wizardlm: Empowering large language models to follow complex instructionsSFT  \n- <big>**CoT data formatting**</big>Take a step back: Evoking reasoning via abstraction in large language modelsStep-Back  \n- <big>**response formatting**</big>Lima: Less is more for alignmentresponseintroduction-body-conclusionwhere the body is usually a list of bullet point  \n- <big>****</big>responseresponse  \n- <big>****</big>response  \n- <big>****</big>#instag: Instruction tagging for analyzing supervised fine-tuning of large language models  \n- <big>****</big>How abilities in large language models are affected by supervised fine-tuning data compositionapproximate grid search{1, 1/2, 1/4, 1/8, 1/16, 1/32, 1/64}  \n- <big>****</big>OPENAIChatML[https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md](https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md)system promptuser inputbot response\n\nSFT\n\n{% asset_img sft.png SFT %}  \n\n#   \n\n## infra\n\nYi  \n\n(1) \n(2) \n(3) DPOMegatronDeepSpeed\n(4) LLMcontinuous batching  paged attention\n\nUI  \n\n##   \n\n4k  \n\n##   \n\n  \n\n- AdamWbeta=[0.9,0.999]epsilon = 1e-8  \n- seq_len = 4096  \n- batch size = 64  \n- constant lr = 1e-5weight decay = 0.1  \n- gradient clip = 1.0  \n- max step = 300\n- Neftune: Noisy embeddings improve instruction finetuning6B noise scale = 534B noise scale = 45\n\n# \n\n## \n\n1.   \n\nYi  \n\n{% asset_img base_model_eval.png Base %}  \n\nGPT3.5GPT4Yi  \n\nYi  \n\n2.   \n\n- Yi-34BYi-6BYi-34BYi-6B  \n- Yi-34BQwen-14BFalcon-180B\n- GPT-4LLMLLMGPT-4GPT-3.5LLMQwen-14BYi-34BC-EvalCMMLUGaokaoGPT-4BBHHumanEvalMATH  \n\n\n3. In-Context Learning  \n\nYiin-context learning-underlying function  \n\n y = w1x1 + w2x2 + ... + wnxn  \n\n x1, x2, ..., xn, y x  y  \n\n w1, w2, ..., wn  \n\na y  y  |y  y| b y == y   \n\nunderlying function  \n\n[1,-1]Yi-34BLLAMA-70B  \n\n[11111]LLAMA-70BMistral 8*7B  \n\n{% asset_img ict.png ICT %}  \n\n## Chat  \n\n1. \n\nbasezero-shotfew-shot\n\n{% asset_img eval.png Yi %}  \n\nGoodharts principle  \n\nYi-34B-ChatYi-6B-ChatSFT  \n\n2. \n\n{% asset_img third_party.png  %}  \n\n#   \n\nbase3  \n\n##   \n\n4kbase200kSFT  \n\nattentionattentionsparse attention  \n\n12length-upsampled long-context data3  \n\nrecitation  \n\nData engineering for scaling language models to 128k contextParaphrasing the original text makes high accuracy long-context qa  \n\n5B tokenbatch size=4Mtoken100step1005B/4M=1250  \n\n> We continue pretrain the model on 5B tokens with 4M batch size, which translate to 100 optimization steps. Aligning with the concurrent work from Fu et al. [22], we observe that such light-weight continue pretraining is already able to enable a strong performance on Needle-in-a-Haystack test, as we will show in Figure 6.\n\nData engineering for scaling language models to 128k context  \n\nSFT  \n\n  \n\n  \n\n\n\n  \n\n{% asset_img long_context_result.png  %}  \n\n##   \n\nViTCLIP ViT-H/14 modeltransformerYi-Chat  \n\n{% asset_img multimodal.png  %}  \n\n3  \n\n1224^2ViTprojection1-LAION-400MViTViTLLM  \n\n2ViT448^2LAION-400M2000-480-CLLaVALLaVARFlickrVQAv2RefCOCOVisual7w  \n\n3100-GQAVizWiz VQATextCapsOCR-VQAVisual GenomeShareGPT4V50,000  \n\n128A1006B334B10  \n\n## Depth Upscaling \n\n326B489B  \n\nScaling large language models with simple yet effective depth up-scaling12-281648  \n\ncosine similarity  \n\n  \n\n{% asset_img 9B.png 9B %}  \n\n  \n\nDepth Upscaling  \n\n800B token  \n\n70%  \n\nconstant lr = 3e-54M tokenbatch size  \n\nbatch sizeYi-6B  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n","slug":"cs/nlp/2024/03/Yi-","published":1,"updated":"2024-03-29T11:53:37.115Z","comments":1,"layout":"post","photos":[],"_id":"clv7wdj0q000g794k9qwk2stf","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>01.AIAI20231101.AIYi-6BYi-34B\nbaseYi-6B-200KYi-34B-200K\nbaseYi01.AIchatYi-9B</p>\n<p>202311YiSuperCLUE/CMMLUYiYi</p>\n<p>20243Yi</p>\n<h1 id=\"tldr\">TL;DR</h1>\n<p></p>\n<ul>\n<li>Yi-34Bint4float16&lt;1%RTX409024G</li>\n<li>LLAMA2</li>\n<li>3.1Tscaling\nlaw1T</li>\n<li>&lt;10k</li>\n<li>4k</li>\n<li></li>\n</ul>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>Yi6B9B34B34B</p>\n<p>34B24GRTX4090</p>\n<p>int434B24GGPU</p>\n<p><a href=\"https://arxiv.org/abs/2301.12017\">Understanding INT4\nQuantization for Language Models: Latency Speedup, Composability, and\nFailure\nCases</a>Yi-34Bint8bf16&lt;1%int4</p>\n<img src=\"/41b6a819/eval.png\" class title=\"Yi\">\n<p>3.1T tokenDeepMindscaling\nlaw1TB&lt;2T</p>\n<p>Chinchilla<a href=\"https://arxiv.org/abs/2203.15556\">Training Compute-Optimal Large\nLanguage Models</a>scaling law</p>\n<p>scaling lawYiovertrain</p>\n<p><big><strong>Yi+--34B+70B</strong></big></p>\n<h2 id=\"\"></h2>\n<p>LLAMA2</p>\n<ul>\n<li>LLAMA270BGQAYiGQA<br>\n</li>\n<li>RoPERoPE ABFEffective long-context scaling of\nfoundation modelsbase10M<br>\n</li>\n<li>SwiGLUGLU Variants Improve Transformer</li>\n</ul>\n<p>activation\nsize4h8/3hGQA</p>\n<blockquote>\n<p>We use SwiGLU as Yis post-attention layer, reducing its activation\nsize from 4h to 8/3h (h denotes hidden size) to be consistent with the\nnormal post-attention layer. This adjustment also compensates for the\nreduction in parameter resulted from GQA, making the overall parameter\ncount comparible of existing 7B and 34B models.</p>\n</blockquote>\n<img src=\"/41b6a819/model.png\" class title=\"Yi\">\n<p><big><strong></strong></big></p>\n<h2 id=\"tokenizer\">tokenizer</h2>\n<ul>\n<li>BPE64000<br>\n</li>\n<li>digit<br>\n</li>\n<li>OOVunicode </li>\n<li></li>\n</ul>\n<p>LLMtokenizerYi</p>\n<h1 id=\"\"></h1>\n<p>LLMYi</p>\n<img src=\"/41b6a819/cover.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p></p>\n<img src=\"/41b6a819/pretrain_data_pipeline.png\" class title=\"\">\n<ol type=\"1\">\n<li> &amp; </li>\n</ol>\n<p></p>\n<p>CCNeTCCNet: Extracting High Quality Monolingual Datasets\nfrom Web Crawl Data</p>\n<ol start=\"2\" type=\"1\">\n<li> Heuristic Rule Filters</li>\n</ol>\n<p></p>\n<ul>\n<li>URL<br>\n</li>\n<li><br>\n</li>\n<li>n-gramScaling Language Models:\nMethods, Analysis &amp; Insights from Training\nGopherCulturaX: A Cleaned, Enormous, and\nMultilingual Dataset for Large Language Models in 167 Languages</li>\n<li>Personal Identifiable\nInformationPII</li>\n</ul>\n<ol start=\"3\" type=\"1\">\n<li> Learned Filters</li>\n</ol>\n<p>4scorer</p>\n<ul>\n<li>Perplexity ScorerCCNet: Extracting High Quality Monolingual\nDatasets from Web Crawl\nDatakenlmperplexity</li>\n<li>Quality\nScorer</li>\n<li>Document Coherence\nScorer</li>\n<li>Safety Scorer</li>\n</ul>\n<ol start=\"4\" type=\"1\">\n<li> Cluster-based Filters</li>\n</ol>\n<p></p>\n<ol start=\"5\" type=\"1\">\n<li></li>\n</ol>\n<p>The RefinedWeb Dataset for Falcon LLM: Outperforming Curated\nCorpora with Web Data, and Web Data\nOnlyminhash</p>\n<p></p>\n<img src=\"/41b6a819/pretrain_data_dist.png\" class title=\"\">\n<p>garbage\ningarbage out</p>\n<blockquote>\n<p>we prefer 3T tokens over sophasticated engineering over 10T tokens\nwithout extensive filtering</p>\n</blockquote>\n<p>10T3</p>\n<h2 id=\"\"></h2>\n<p>Quality is All You Need</p>\n<p>&lt;10kSFT</p>\n<p>Gemini: A family of highly capable multimodal\nmodels.Llama 2: Open Foundation and Fine-Tuned Chat\nModelsLima: Less is more for alignmentFLANScaling\ninstruction-finetuned language modelsUltraChatEnhancing chat\nlanguage models by scaling high-quality instructional\nconversations</p>\n<p></p>\n<ul>\n<li><big><strong>prompt distribution\nselection</strong></big>Wizardlm: Empowering large language\nmodels to follow complex\ninstructionsSFT<br>\n</li>\n<li><big><strong>CoT data formatting</strong></big>Take a\nstep back: Evoking reasoning via abstraction in large language\nmodelsStep-Back<br>\n</li>\n<li><big><strong>response formatting</strong></big>Lima:\nLess is more for\nalignmentresponseintroduction-body-conclusionwhere\nthe body is usually a list of bullet point<br>\n</li>\n<li><big><strong></strong></big>responseresponse<br>\n</li>\n<li><big><strong></strong></big>response<br>\n</li>\n<li><big><strong></strong></big>#instag:\nInstruction tagging for analyzing supervised fine-tuning of large\nlanguage\nmodels<br>\n</li>\n<li><big><strong></strong></big>How\nabilities in large language models are affected by supervised\nfine-tuning data compositionapproximate grid\nsearch{1, 1/2, 1/4, 1/8, 1/16, 1/32,\n1/64}<br>\n</li>\n<li><big><strong></strong></big>OPENAIChatML<a href=\"https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md\">https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md</a>system\npromptuser inputbot response</li>\n</ul>\n<p>SFT</p>\n<img src=\"/41b6a819/sft.png\" class title=\"SFT\">\n<h1 id=\"\"></h1>\n<h2 id=\"infra\">infra</h2>\n<p>Yi</p>\n<ol type=\"1\">\n<li></li>\n<li></li>\n<li>DPOMegatronDeepSpeed</li>\n<li>LLMcontinuous batching  paged\nattention</li>\n</ol>\n<p>UI</p>\n<h2 id=\"\"></h2>\n<p>4k</p>\n<h2 id=\"\"></h2>\n<p></p>\n<ul>\n<li>AdamWbeta=[0.9,0.999]epsilon = 1e-8<br>\n</li>\n<li>seq_len = 4096<br>\n</li>\n<li>batch size = 64<br>\n</li>\n<li>constant lr = 1e-5weight decay = 0.1<br>\n</li>\n<li>gradient clip = 1.0<br>\n</li>\n<li>max step = 300</li>\n<li>Neftune: Noisy embeddings improve instruction\nfinetuning6B noise scale = 534B noise scale =\n45</li>\n</ul>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>Yi</p>\n<img src=\"/41b6a819/base_model_eval.png\" class title=\"Base\">\n<p>GPT3.5GPT4Yi</p>\n<p>Yi</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<ul>\n<li>Yi-34BYi-6BYi-34BYi-6B<br>\n</li>\n<li>Yi-34BQwen-14BFalcon-180B</li>\n<li>GPT-4LLMLLMGPT-4GPT-3.5LLMQwen-14BYi-34BC-EvalCMMLUGaokaoGPT-4BBHHumanEvalMATH</li>\n</ul>\n<ol start=\"3\" type=\"1\">\n<li>In-Context Learning</li>\n</ol>\n<p>Yiin-context\nlearning-underlying\nfunction</p>\n<p> y = w1x1 + w2x2 +\n... + wnxn</p>\n<p> x1, x2, ..., xn, y x \ny</p>\n<p> w1, w2, ..., wn</p>\n<p>a y  y  |y  y|\nb y == y </p>\n<p>underlying\nfunction</p>\n<p>[1,-1]Yi-34BLLAMA-70B</p>\n<p>[11111]LLAMA-70BMistral\n8*7B</p>\n<img src=\"/41b6a819/ict.png\" class title=\"ICT\">\n<h2 id=\"chat\">Chat</h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>basezero-shotfew-shot</p>\n<img src=\"/41b6a819/eval.png\" class title=\"Yi\">\n<p>Goodharts\nprinciple</p>\n<p>Yi-34B-ChatYi-6B-ChatSFT</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<img src=\"/41b6a819/third_party.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<p>base3</p>\n<h2 id=\"\"></h2>\n<p>4kbase200kSFT</p>\n<p>attentionattentionsparse\nattention</p>\n<p>12length-upsampled\nlong-context\ndata3</p>\n<p>recitation</p>\n<p>Data engineering for scaling language\nmodels to 128k contextParaphrasing the original text makes high\naccuracy long-context qa</p>\n<p>5B tokenbatch\nsize=4Mtoken100step1005B/4M=1250</p>\n<blockquote>\n<p>We continue pretrain the model on 5B tokens with 4M batch size, which\ntranslate to 100 optimization steps. Aligning with the concurrent work\nfrom Fu et al. [22], we observe that such light-weight continue\npretraining is already able to enable a strong performance on\nNeedle-in-a-Haystack test, as we will show in Figure 6.</p>\n</blockquote>\n<p>Data engineering for scaling language models to 128k\ncontext</p>\n<p>SFT</p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<img src=\"/41b6a819/long_context_result.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p>ViTCLIP ViT-H/14\nmodeltransformerYi-Chat</p>\n<img src=\"/41b6a819/multimodal.png\" class title=\"\">\n<p>3</p>\n<p>1224^2ViTprojection1-LAION-400MViTViTLLM</p>\n<p>2ViT448^2LAION-400M2000-480-CLLaVALLaVARFlickrVQAv2RefCOCOVisual7w</p>\n<p>3100-GQAVizWiz\nVQATextCapsOCR-VQAVisual\nGenomeShareGPT4V50,000</p>\n<p>128A1006B334B10</p>\n<h2 id=\"depth-upscaling-\">Depth Upscaling </h2>\n<p>326B489B</p>\n<p>Scaling large language models with simple yet effective depth\nup-scaling12-281648</p>\n<p>cosine\nsimilarity</p>\n<p></p>\n<img src=\"/41b6a819/9B.png\" class title=\"9B\">\n<p></p>\n<p>Depth Upscaling</p>\n<p>800B token</p>\n<p>70%</p>\n<p>constant lr = 3e-54M\ntokenbatch size</p>\n<p>batch\nsizeYi-6B</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<p><a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n","length":8965,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>01.AIAI20231101.AIYi-6BYi-34B\nbaseYi-6B-200KYi-34B-200K\nbaseYi01.AIchatYi-9B</p>\n<p>202311YiSuperCLUE/CMMLUYiYi</p>\n<p>20243Yi</p>\n<h1 id=\"tldr\">TL;DR</h1>\n<p></p>\n<ul>\n<li>Yi-34Bint4float16&lt;1%RTX409024G</li>\n<li>LLAMA2</li>\n<li>3.1Tscaling\nlaw1T</li>\n<li>&lt;10k</li>\n<li>4k</li>\n<li></li>\n</ul>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>Yi6B9B34B34B</p>\n<p>34B24GRTX4090</p>\n<p>int434B24GGPU</p>\n<p><a href=\"https://arxiv.org/abs/2301.12017\">Understanding INT4\nQuantization for Language Models: Latency Speedup, Composability, and\nFailure\nCases</a>Yi-34Bint8bf16&lt;1%int4</p>\n<img src=\"/41b6a819/eval.png\" class title=\"Yi\">\n<p>3.1T tokenDeepMindscaling\nlaw1TB&lt;2T</p>\n<p>Chinchilla<a href=\"https://arxiv.org/abs/2203.15556\">Training Compute-Optimal Large\nLanguage Models</a>scaling law</p>\n<p>scaling lawYiovertrain</p>\n<p><big><strong>Yi+--34B+70B</strong></big></p>\n<h2 id=\"\"></h2>\n<p>LLAMA2</p>\n<ul>\n<li>LLAMA270BGQAYiGQA<br>\n</li>\n<li>RoPERoPE ABFEffective long-context scaling of\nfoundation modelsbase10M<br>\n</li>\n<li>SwiGLUGLU Variants Improve Transformer</li>\n</ul>\n<p>activation\nsize4h8/3hGQA</p>\n<blockquote>\n<p>We use SwiGLU as Yis post-attention layer, reducing its activation\nsize from 4h to 8/3h (h denotes hidden size) to be consistent with the\nnormal post-attention layer. This adjustment also compensates for the\nreduction in parameter resulted from GQA, making the overall parameter\ncount comparible of existing 7B and 34B models.</p>\n</blockquote>\n<img src=\"/41b6a819/model.png\" class title=\"Yi\">\n<p><big><strong></strong></big></p>\n<h2 id=\"tokenizer\">tokenizer</h2>\n<ul>\n<li>BPE64000<br>\n</li>\n<li>digit<br>\n</li>\n<li>OOVunicode </li>\n<li></li>\n</ul>\n<p>LLMtokenizerYi</p>\n<h1 id=\"\"></h1>\n<p>LLMYi</p>\n<img src=\"/41b6a819/cover.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p></p>\n<img src=\"/41b6a819/pretrain_data_pipeline.png\" class title=\"\">\n<ol type=\"1\">\n<li> &amp; </li>\n</ol>\n<p></p>\n<p>CCNeTCCNet: Extracting High Quality Monolingual Datasets\nfrom Web Crawl Data</p>\n<ol start=\"2\" type=\"1\">\n<li> Heuristic Rule Filters</li>\n</ol>\n<p></p>\n<ul>\n<li>URL<br>\n</li>\n<li><br>\n</li>\n<li>n-gramScaling Language Models:\nMethods, Analysis &amp; Insights from Training\nGopherCulturaX: A Cleaned, Enormous, and\nMultilingual Dataset for Large Language Models in 167 Languages</li>\n<li>Personal Identifiable\nInformationPII</li>\n</ul>\n<ol start=\"3\" type=\"1\">\n<li> Learned Filters</li>\n</ol>\n<p>4scorer</p>\n<ul>\n<li>Perplexity ScorerCCNet: Extracting High Quality Monolingual\nDatasets from Web Crawl\nDatakenlmperplexity</li>\n<li>Quality\nScorer</li>\n<li>Document Coherence\nScorer</li>\n<li>Safety Scorer</li>\n</ul>\n<ol start=\"4\" type=\"1\">\n<li> Cluster-based Filters</li>\n</ol>\n<p></p>\n<ol start=\"5\" type=\"1\">\n<li></li>\n</ol>\n<p>The RefinedWeb Dataset for Falcon LLM: Outperforming Curated\nCorpora with Web Data, and Web Data\nOnlyminhash</p>\n<p></p>\n<img src=\"/41b6a819/pretrain_data_dist.png\" class title=\"\">\n<p>garbage\ningarbage out</p>\n<blockquote>\n<p>we prefer 3T tokens over sophasticated engineering over 10T tokens\nwithout extensive filtering</p>\n</blockquote>\n<p>10T3</p>\n<h2 id=\"\"></h2>\n<p>Quality is All You Need</p>\n<p>&lt;10kSFT</p>\n<p>Gemini: A family of highly capable multimodal\nmodels.Llama 2: Open Foundation and Fine-Tuned Chat\nModelsLima: Less is more for alignmentFLANScaling\ninstruction-finetuned language modelsUltraChatEnhancing chat\nlanguage models by scaling high-quality instructional\nconversations</p>\n<p></p>\n<ul>\n<li><big><strong>prompt distribution\nselection</strong></big>Wizardlm: Empowering large language\nmodels to follow complex\ninstructionsSFT<br>\n</li>\n<li><big><strong>CoT data formatting</strong></big>Take a\nstep back: Evoking reasoning via abstraction in large language\nmodelsStep-Back<br>\n</li>\n<li><big><strong>response formatting</strong></big>Lima:\nLess is more for\nalignmentresponseintroduction-body-conclusionwhere\nthe body is usually a list of bullet point<br>\n</li>\n<li><big><strong></strong></big>responseresponse<br>\n</li>\n<li><big><strong></strong></big>response<br>\n</li>\n<li><big><strong></strong></big>#instag:\nInstruction tagging for analyzing supervised fine-tuning of large\nlanguage\nmodels<br>\n</li>\n<li><big><strong></strong></big>How\nabilities in large language models are affected by supervised\nfine-tuning data compositionapproximate grid\nsearch{1, 1/2, 1/4, 1/8, 1/16, 1/32,\n1/64}<br>\n</li>\n<li><big><strong></strong></big>OPENAIChatML<a href=\"https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md\">https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md</a>system\npromptuser inputbot response</li>\n</ul>\n<p>SFT</p>\n<img src=\"/41b6a819/sft.png\" class title=\"SFT\">\n<h1 id=\"\"></h1>\n<h2 id=\"infra\">infra</h2>\n<p>Yi</p>\n<ol type=\"1\">\n<li></li>\n<li></li>\n<li>DPOMegatronDeepSpeed</li>\n<li>LLMcontinuous batching  paged\nattention</li>\n</ol>\n<p>UI</p>\n<h2 id=\"\"></h2>\n<p>4k</p>\n<h2 id=\"\"></h2>\n<p></p>\n<ul>\n<li>AdamWbeta=[0.9,0.999]epsilon = 1e-8<br>\n</li>\n<li>seq_len = 4096<br>\n</li>\n<li>batch size = 64<br>\n</li>\n<li>constant lr = 1e-5weight decay = 0.1<br>\n</li>\n<li>gradient clip = 1.0<br>\n</li>\n<li>max step = 300</li>\n<li>Neftune: Noisy embeddings improve instruction\nfinetuning6B noise scale = 534B noise scale =\n45</li>\n</ul>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>Yi</p>\n<img src=\"/41b6a819/base_model_eval.png\" class title=\"Base\">\n<p>GPT3.5GPT4Yi</p>\n<p>Yi</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<ul>\n<li>Yi-34BYi-6BYi-34BYi-6B<br>\n</li>\n<li>Yi-34BQwen-14BFalcon-180B</li>\n<li>GPT-4LLMLLMGPT-4GPT-3.5LLMQwen-14BYi-34BC-EvalCMMLUGaokaoGPT-4BBHHumanEvalMATH</li>\n</ul>\n<ol start=\"3\" type=\"1\">\n<li>In-Context Learning</li>\n</ol>\n<p>Yiin-context\nlearning-underlying\nfunction</p>\n<p> y = w1x1 + w2x2 +\n... + wnxn</p>\n<p> x1, x2, ..., xn, y x \ny</p>\n<p> w1, w2, ..., wn</p>\n<p>a y  y  |y  y|\nb y == y </p>\n<p>underlying\nfunction</p>\n<p>[1,-1]Yi-34BLLAMA-70B</p>\n<p>[11111]LLAMA-70BMistral\n8*7B</p>\n<img src=\"/41b6a819/ict.png\" class title=\"ICT\">\n<h2 id=\"chat\">Chat</h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>basezero-shotfew-shot</p>\n<img src=\"/41b6a819/eval.png\" class title=\"Yi\">\n<p>Goodharts\nprinciple</p>\n<p>Yi-34B-ChatYi-6B-ChatSFT</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<img src=\"/41b6a819/third_party.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<p>base3</p>\n<h2 id=\"\"></h2>\n<p>4kbase200kSFT</p>\n<p>attentionattentionsparse\nattention</p>\n<p>12length-upsampled\nlong-context\ndata3</p>\n<p>recitation</p>\n<p>Data engineering for scaling language\nmodels to 128k contextParaphrasing the original text makes high\naccuracy long-context qa</p>\n<p>5B tokenbatch\nsize=4Mtoken100step1005B/4M=1250</p>\n<blockquote>\n<p>We continue pretrain the model on 5B tokens with 4M batch size, which\ntranslate to 100 optimization steps. Aligning with the concurrent work\nfrom Fu et al. [22], we observe that such light-weight continue\npretraining is already able to enable a strong performance on\nNeedle-in-a-Haystack test, as we will show in Figure 6.</p>\n</blockquote>\n<p>Data engineering for scaling language models to 128k\ncontext</p>\n<p>SFT</p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<img src=\"/41b6a819/long_context_result.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p>ViTCLIP ViT-H/14\nmodeltransformerYi-Chat</p>\n<img src=\"/41b6a819/multimodal.png\" class title=\"\">\n<p>3</p>\n<p>1224^2ViTprojection1-LAION-400MViTViTLLM</p>\n<p>2ViT448^2LAION-400M2000-480-CLLaVALLaVARFlickrVQAv2RefCOCOVisual7w</p>\n<p>3100-GQAVizWiz\nVQATextCapsOCR-VQAVisual\nGenomeShareGPT4V50,000</p>\n<p>128A1006B334B10</p>\n<h2 id=\"depth-upscaling-\">Depth Upscaling </h2>\n<p>326B489B</p>\n<p>Scaling large language models with simple yet effective depth\nup-scaling12-281648</p>\n<p>cosine\nsimilarity</p>\n<p></p>\n<img src=\"/41b6a819/9B.png\" class title=\"9B\">\n<p></p>\n<p>Depth Upscaling</p>\n<p>800B token</p>\n<p>70%</p>\n<p>constant lr = 3e-54M\ntokenbatch size</p>\n<p>batch\nsizeYi-6B</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<p><a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n"},{"title":"(1)","abbrlink":"3345028a","date":"2024-03-17T02:46:09.000Z","_content":"\n![](/images/cover.png)  \n\n\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n\n***\n\n//  \n\nLLM~~\n\n# 1Transformerscaled dot-product attentionQKd  \n\nsoftmaxsoftmaxattentionscalingd softmax\n\n# 2TransformerQK  \n\n1QK  \n\n2QK  \n\n3  \n\n# 3TransformerFFNFFNFFN  \n\n1SVM kernel  \n\n2  \n\n3  \n\n# 4MQA(Multi-Query Attention)GQA(Grouped-Query Attention)MHA(Multi-Head Attention)  \n\n1MQAGQAMHAMHAKV  \n\n2Decoder-onlycausal attentionKVMQAGQAKVKV  \n\n# 5LLMDecoder-only  \n\n1AttentionDecoder-only  \n\n2  \n\n3Causal AttentionAttentiontoken  \n\n4KV Cache  \n\n5  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***\n\n\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  ","source":"_posts/cs/nlp/2024/03/-1.md","raw":"---\ntitle: (1)\ntags:\n  - NLP\n  - LLM\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 3345028a\ndate: 2024-03-17 10:46:09\n---\n\n![](/images/cover.png)  \n\n\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n\n***\n\n//  \n\nLLM~~\n\n# 1Transformerscaled dot-product attentionQKd  \n\nsoftmaxsoftmaxattentionscalingd softmax\n\n# 2TransformerQK  \n\n1QK  \n\n2QK  \n\n3  \n\n# 3TransformerFFNFFNFFN  \n\n1SVM kernel  \n\n2  \n\n3  \n\n# 4MQA(Multi-Query Attention)GQA(Grouped-Query Attention)MHA(Multi-Head Attention)  \n\n1MQAGQAMHAMHAKV  \n\n2Decoder-onlycausal attentionKVMQAGQAKVKV  \n\n# 5LLMDecoder-only  \n\n1AttentionDecoder-only  \n\n2  \n\n3Causal AttentionAttentiontoken  \n\n4KV Cache  \n\n5  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***\n\n\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  ","slug":"cs/nlp/2024/03/-1","published":1,"updated":"2024-03-17T14:16:49.511Z","comments":1,"layout":"post","photos":[],"_id":"clv7wdj0q000h794k2fip10h4","content":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<hr>\n<p>//</p>\n<p>LLM<sub></sub></p>\n<h1 id=\"transformerscaled-dot-product-attentionqkd\">1Transformerscaled\ndot-product attentionQKd</h1>\n<p>softmaxsoftmaxattentionscalingd\nsoftmax</p>\n<h1 id=\"transformerqk\">2TransformerQK</h1>\n<p>1QK</p>\n<p>2QK</p>\n<p>3</p>\n<h1 id=\"transformerffnffnffn\">3TransformerFFNFFNFFN</h1>\n<p>1SVM\nkernel</p>\n<p>2</p>\n<p>3</p>\n<h1 id=\"mqamulti-query-attentiongqagrouped-query-attentionmhamulti-head-attention\">4MQA(Multi-Query\nAttention)GQA(Grouped-Query Attention)MHA(Multi-Head\nAttention)</h1>\n<p>1MQAGQAMHAMHAKV</p>\n<p>2Decoder-onlycausal\nattentionKVMQAGQAKVKV</p>\n<h1 id=\"llmdecoder-only\">5LLMDecoder-only</h1>\n<p>1AttentionDecoder-only</p>\n<p>2</p>\n<p>3Causal\nAttentionAttentiontoken</p>\n<p>4KV Cache</p>\n<p>5</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n","length":1412,"excerpt":"","more":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<hr>\n<p>//</p>\n<p>LLM<sub></sub></p>\n<h1 id=\"transformerscaled-dot-product-attentionqkd\">1Transformerscaled\ndot-product attentionQKd</h1>\n<p>softmaxsoftmaxattentionscalingd\nsoftmax</p>\n<h1 id=\"transformerqk\">2TransformerQK</h1>\n<p>1QK</p>\n<p>2QK</p>\n<p>3</p>\n<h1 id=\"transformerffnffnffn\">3TransformerFFNFFNFFN</h1>\n<p>1SVM\nkernel</p>\n<p>2</p>\n<p>3</p>\n<h1 id=\"mqamulti-query-attentiongqagrouped-query-attentionmhamulti-head-attention\">4MQA(Multi-Query\nAttention)GQA(Grouped-Query Attention)MHA(Multi-Head\nAttention)</h1>\n<p>1MQAGQAMHAMHAKV</p>\n<p>2Decoder-onlycausal\nattentionKVMQAGQAKVKV</p>\n<h1 id=\"llmdecoder-only\">5LLMDecoder-only</h1>\n<p>1AttentionDecoder-only</p>\n<p>2</p>\n<p>3Causal\nAttentionAttentiontoken</p>\n<p>4KV Cache</p>\n<p>5</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n"},{"title":"transformernormalization","abbrlink":"6a40bfa5","date":"2024-03-19T13:06:12.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nNormalizationattention  \n\nnormalizationtransformer  \n\n  \n\n# why normalization\n\nnormalization  \n\n  \n\n## normalization\n\n $Loss(x_1,x_2)=x_1^2+x_2^2+b$   \n\n{% asset_img lossfunc_surface.jpeg loss function surface %}  \n\n  \n\n  \n\nminimum  \n\n  \n\n{% asset_img ellipse_1.png ellipse %}  \n\n  \n\n  \n\n{% asset_img ellipse_2.png ellipse %}  \n\n  \n\n  \n\n $x_{1}$ $x_{2}$->  \n\n0.x~2.x0  \n\n  \n\n $x_{2}$   \n\n  \n\n  \n\nnormalization  \n\nnormalization  \n$$x^{\\prime}=\\frac{x-\\min(\\mathrm{x})}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}$$  \n\n$$x^{\\prime}=\\frac{x-\\mu}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}$$  \n\n$$x^{\\prime}=\\frac{x-\\mu}\\sigma$$  \n\n $\\mu$ $\\sigma$   \n\nZ-score normalization  \n\n  \n\nPCA  \n\n## ICS...\n\ni.i.d.independent and identical distribution  \n\ni.i.d.  \n\n  \n\n  \n\n  \n\ni.i.d.i.i.d.  \n\nPCAi.i.d.  \n\n  \n\ni.i.d.  \n\nICSinternal covariate shift  \n\n  \n\nnormalization  \n\nnormalizationbatchnormICSbatchnorm  \n\nICS  \n\n## \n\nsigmoid\n\n{% asset_img sigmoid.png sigmoid %}  \n\n > 6  < -6 sigmoid  \n\n  \n\nICSnormalization  \n\nnormalization  \n\n# batchnorm\n\nnormalizationbatchnormlayernorm  \n\n## batchnorm\n\n $[B,C]$  $B$ batch size$C$   \n\n $C$ normalization $C$   \n\n $i$ batch  \n\n$$\n\\mu_{i}=\\frac{1}{B}\\sum_{j=1}^{B}x_{i,j}\n$$  \n\n  \n\n$$\n\\sigma_{i}^{2}=\\frac{1}{B}\\sum_{j=1}^{B}(x_{i,j}-\\mu_{i})^2\n$$  \n\nbatchZ-score normalization  \n\n$$\nx_{i,j}'=\\frac{x_{i,j}-\\mu_{i}}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\n$$  \n\n $\\epsilon$ 0  \n\n $C$ 01  \n\n  \n\nsigmoid  \n\n  \n\n  $\\gamma$  $\\beta$   \n\n$$\ny_{i,j} = \\gamma_{i} x_{i,j}' + \\beta_{i}\n$$  \n\n  \n\nbatchnorm $\\gamma$  $\\beta$ \n\n[Batch Normalization: Accelerating Deep Network](https://zhuanlan.zhihu.com/p/340856414)  \n\n{% asset_img bn_algo.png batch norm %}  \n\n## CNNbatchnorm  \n\nbatchnormCNN  \n\nCNNfeature mapsize $[B,C,H,W]$  $B$ batch size$C$ channel$H$  $W$   \n\nbatchnorm $C\\times H\\times W$  $B$  $B$   \n\nCNNchannel $H\\times W$  $H\\times W$ batch  \n\nbatch $B\\times H\\times W$  $C$  $C$  $\\gamma$  $\\beta$   \n\nbatchnormbatchnormrelufcbatchnormfcbias  \n\nbtwbatchnorm $\\gamma$ 1 $\\beta$ 0batchnorm  \n\n##   \n\nbatchnormmini-batch  \n\n$\\gamma$  $\\beta$   \n\nsamplesamplebatch  \n\ni.i.d.  \n\n\n\nmoving_mean = momentum  moving_mean + (1.0  momentum)  mean  \n\nmoving_var = momentum  moving_var + (1.0  momentum)  var  \n\nbatch  \n\nmomentum TF/Keras 0.99 Pytorch 0.9  \n\nmomentum  \n \nmomentum  \n\nbatch sizemini batchmomentum  \n\nbatch\n\nbatch sizemini batchbatchnormbatch size  \n\n{% asset_img bs_bn.png batch size %}  \n\nbatchnorm    \n\nbatch\n\nmomentumbatch sizebatch size\n\n## batchnorm\n\nbatchnormbatchnormICS2018How Does Batch Normalization Help Optimization?  \n\nbatchnormICSICSbatchnormcovariate shiftbatchnorm  \n\n{% asset_img bn_ics.png ICS %}  \n\nICSbatchnormICS  \n\nICSbatchnorm  \n\nbatchnormICS  \n\nICSICS  \n\nICS\n\n{% asset_img ics_define.png ICS  %}  \n\niL2\n\n$G_{t,i}$ t  \n\n$G_{t,i}'$ t  \n\n  \n\nICS\n\n{% asset_img ics_measure.png ICS measure %}  \n\nbatchnorm  \n\nbatchnormICS  \n\nbatchnorm  \n\nbatchnorm  \n\n# layernorm\n\nbatchnormlayernorm  \n\n## layernorm\n\nlayernormlayerlayer  \n\nbatchnormbatchlayernorm  \n\n{% asset_img bn_and_ln.png bnln %}  \n\nNLPbatchnormlayernormbatchnormlayernorm  \n\nlayernormNLPRNNtransfomrer  \n\ntransformer $[B,S,H]$ $S$ paddingzero-paddingbatch  \n\n\n\n```\n      \n  \n    \n```\n\npad\n\n```\n      \n    [P] [P]\n      [P]\n```\n\npaddingbatchnorm  \n\n[P] [P] batch size2 [P] normalization  \n\nbatch  \n\n [P] token  \n\nlayernorm $H$ normalization $H$ $\\gamma$  $\\beta$   \n\ntoken  \n\n## transformerlayernorm  \n\nbatchnormlayernormbatch  \n\nlayernormbatchbatch size  \n\nnlplayernormbatchnorm  \n\nPowerNorm: Rethinking Batch Normalization in TransformerstransformerBN  \n\nbatchbatchrunning statisticsNLPIWSLT14batchrunning statisticsCV  \n\nmagnitudeCV  \n\ntransformerBNCVNLPNLPbatch  \n\nlayernormNLPbatchnorm  \n\n## RMSnorm\n\n19Root Mean Square Layer NormalizationnormalizationRMSnormlayernorm  \n\nRMSnormlayernorm  \n\n{% asset_img rmsnorm.png RMSnorm %}  \n\nlayernorm  \n\n{% asset_img rmsnorm_eff.png RMSnorm %}  \n\nGRUlayernormlayernormlayernorm  \n\nlayernorm  \n\npRMSnormp%  \n\n{% asset_img prmsnorm.png prmsnorm %}  \n\nRMSnorm  \n\n# post-norm & pre-norm\n\n## \nlayernorm  \n\ntransformerpost-normOn Layer Normalization in the Transformer Architecturepre-norm  \n\npost-normpre-norm\n\n{% asset_img postnorm_prenorm.png postnorm and prenorm %}  \n\npost-normpre-norm  \n\npost-normpre-normpre-normpost-normpre-norm  \n\n $l$  $l+1$ post-norm  \n\n$$\nx_{l+1}=\\mathrm{Norm}(x_l+\\mathrm{F}_t(x_l))\n$$\n\npre-norm  \n\n$$\nx_{l+1}=x_l+\\mathrm{F}_l(\\mathrm{Norm}(x_l))\n$$\n\nPre NormPost Norm $\\mathrm{F}_l(\\mathrm{Norm}(x_l))$ norm  \n\n $l$ $x_{l}x_{l+1}$  $\\mathrm{F}_l(\\mathrm{Norm}(x_l))$  $\\mathrm{F}_{l+1}(\\mathrm{Norm}(x_{l+1}))$   \n\n$$\\begin{aligned}\n&\\mathrm{F}_l(\\operatorname{Norm}(x_l))+\\operatorname{F}_{l+1}(\\operatorname{Norm}(x_{l+1})) \\\\\n&{\\approx}\\mathrm{F}_l(\\mathrm{Norm}(x_l))+\\mathrm{F}_{l+1}\\left(\\mathrm{Norm}(x_l)\\right) \\\\\n&=(\\mathrm{F}_l\\oplus\\mathrm{F}_{l+1})(\\mathrm{Norm}(\\pmb{x_l}))\n\\end{aligned}$$\n\n $l$  $l+1$  $l$   \n\npre-norm $l$ post-norm  \n\nnormalizationpre-norm--  \n\n  \n\npost-normlossnormpost-normpre-norm  \n\n## warmup  \n\nOn Layer Normalization in the Transformer Architecturepre-normpost-normtransformerwarmup  \n\nwarmup  \n\npre-normtransformerwarmuppost-norm+warmuppost-normwarmup  \n\n{% asset_img warmup_effect.png warmup %}  \n\n## Deepnorm  \n\n2022DeepNet: Scaling Transformers to 1,000 Layerstransformer  \n\nDeepnorm  \n\n{% asset_img deepnorm.png deepnorm %}  \n\n $\\alpha>1$ post-norm  \n\n $\\beta$  $G_{l}$   \n\ndeepnormpre-normpost-norm  \n\n{% asset_img deepnorm_result.png deepnorm result %}  \n\npost-norm  \n\n## Realformer--residual attention  \n\npost-normpre-normRealFormer: Transformer Likes Residual Attention  \n\n{% asset_img realformer.png realformer %}  \n\nRealFormerTransformerSoftmax\n\n\n\n{% asset_img realformer_attention.png realformer attention %}  \n\n $Prev'$ softmax $\\frac{Q^{\\prime}K^{\\prime T}}{\\sqrt{d_k}}+Prev'$ attention  \n\n# \n\nnormalizationbatchnormlayernormtransformer\n\nrmsnorm + prenorm  \n\nnormalization    \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n***\n\n# Reference  \n1https://www.zhihu.com/question/487766088  \n2Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization https://arxiv.org/abs/2001.06838  \n3Transformer()& https://zhuanlan.zhihu.com/p/476102712  \n4Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://arxiv.org/abs/1502.03167  \n5How Does Batch Normalization Help Optimization? https://arxiv.org/abs/1805.11604  \n6Batch Normalization: Accelerating Deep Network https://zhuanlan.zhihu.com/p/340856414  \n7Layer Normalization https://arxiv.org/abs/1607.06450  \n8NormalizationBN/LN/WN https://zhuanlan.zhihu.com/p/33173246  \n9Transformer()BatchNormalization https://zhuanlan.zhihu.com/p/481277619  \n10Layer Normalization https://arxiv.org/abs/1607.06450  \n11PowerNorm: Rethinking Batch Normalization in Transformers https://arxiv.org/abs/2003.07845  \n12Root Mean Square Layer Normalization https://arxiv.org/abs/1910.07467  \n13On Layer Normalization in the Transformer Architecture https://arxiv.org/abs/2002.04745  \n14Pre NormPost Norm https://spaces.ac.cn/archives/9009  \n15Understanding the Difficulty of Training Transformers https://arxiv.org/abs/2004.08249  \n16RealFormer: Transformer Likes Residual Attention https://arxiv.org/abs/2012.11747  \n17DeepNet: Scaling Transformers to 1,000 Layers https://arxiv.org/abs/2203.00555  \n\n","source":"_posts/cs/nlp/2024/03/Transformernormalization.md","raw":"---\ntitle: transformernormalization\nabbrlink: 6a40bfa5\ndate: 2024-03-19 21:06:12\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - layernorm\n  - post-norm\n  - pre-norm\n  - normalization\n  - batchnorm\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nNormalizationattention  \n\nnormalizationtransformer  \n\n  \n\n# why normalization\n\nnormalization  \n\n  \n\n## normalization\n\n $Loss(x_1,x_2)=x_1^2+x_2^2+b$   \n\n{% asset_img lossfunc_surface.jpeg loss function surface %}  \n\n  \n\n  \n\nminimum  \n\n  \n\n{% asset_img ellipse_1.png ellipse %}  \n\n  \n\n  \n\n{% asset_img ellipse_2.png ellipse %}  \n\n  \n\n  \n\n $x_{1}$ $x_{2}$->  \n\n0.x~2.x0  \n\n  \n\n $x_{2}$   \n\n  \n\n  \n\nnormalization  \n\nnormalization  \n$$x^{\\prime}=\\frac{x-\\min(\\mathrm{x})}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}$$  \n\n$$x^{\\prime}=\\frac{x-\\mu}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}$$  \n\n$$x^{\\prime}=\\frac{x-\\mu}\\sigma$$  \n\n $\\mu$ $\\sigma$   \n\nZ-score normalization  \n\n  \n\nPCA  \n\n## ICS...\n\ni.i.d.independent and identical distribution  \n\ni.i.d.  \n\n  \n\n  \n\n  \n\ni.i.d.i.i.d.  \n\nPCAi.i.d.  \n\n  \n\ni.i.d.  \n\nICSinternal covariate shift  \n\n  \n\nnormalization  \n\nnormalizationbatchnormICSbatchnorm  \n\nICS  \n\n## \n\nsigmoid\n\n{% asset_img sigmoid.png sigmoid %}  \n\n > 6  < -6 sigmoid  \n\n  \n\nICSnormalization  \n\nnormalization  \n\n# batchnorm\n\nnormalizationbatchnormlayernorm  \n\n## batchnorm\n\n $[B,C]$  $B$ batch size$C$   \n\n $C$ normalization $C$   \n\n $i$ batch  \n\n$$\n\\mu_{i}=\\frac{1}{B}\\sum_{j=1}^{B}x_{i,j}\n$$  \n\n  \n\n$$\n\\sigma_{i}^{2}=\\frac{1}{B}\\sum_{j=1}^{B}(x_{i,j}-\\mu_{i})^2\n$$  \n\nbatchZ-score normalization  \n\n$$\nx_{i,j}'=\\frac{x_{i,j}-\\mu_{i}}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\n$$  \n\n $\\epsilon$ 0  \n\n $C$ 01  \n\n  \n\nsigmoid  \n\n  \n\n  $\\gamma$  $\\beta$   \n\n$$\ny_{i,j} = \\gamma_{i} x_{i,j}' + \\beta_{i}\n$$  \n\n  \n\nbatchnorm $\\gamma$  $\\beta$ \n\n[Batch Normalization: Accelerating Deep Network](https://zhuanlan.zhihu.com/p/340856414)  \n\n{% asset_img bn_algo.png batch norm %}  \n\n## CNNbatchnorm  \n\nbatchnormCNN  \n\nCNNfeature mapsize $[B,C,H,W]$  $B$ batch size$C$ channel$H$  $W$   \n\nbatchnorm $C\\times H\\times W$  $B$  $B$   \n\nCNNchannel $H\\times W$  $H\\times W$ batch  \n\nbatch $B\\times H\\times W$  $C$  $C$  $\\gamma$  $\\beta$   \n\nbatchnormbatchnormrelufcbatchnormfcbias  \n\nbtwbatchnorm $\\gamma$ 1 $\\beta$ 0batchnorm  \n\n##   \n\nbatchnormmini-batch  \n\n$\\gamma$  $\\beta$   \n\nsamplesamplebatch  \n\ni.i.d.  \n\n\n\nmoving_mean = momentum  moving_mean + (1.0  momentum)  mean  \n\nmoving_var = momentum  moving_var + (1.0  momentum)  var  \n\nbatch  \n\nmomentum TF/Keras 0.99 Pytorch 0.9  \n\nmomentum  \n \nmomentum  \n\nbatch sizemini batchmomentum  \n\nbatch\n\nbatch sizemini batchbatchnormbatch size  \n\n{% asset_img bs_bn.png batch size %}  \n\nbatchnorm    \n\nbatch\n\nmomentumbatch sizebatch size\n\n## batchnorm\n\nbatchnormbatchnormICS2018How Does Batch Normalization Help Optimization?  \n\nbatchnormICSICSbatchnormcovariate shiftbatchnorm  \n\n{% asset_img bn_ics.png ICS %}  \n\nICSbatchnormICS  \n\nICSbatchnorm  \n\nbatchnormICS  \n\nICSICS  \n\nICS\n\n{% asset_img ics_define.png ICS  %}  \n\niL2\n\n$G_{t,i}$ t  \n\n$G_{t,i}'$ t  \n\n  \n\nICS\n\n{% asset_img ics_measure.png ICS measure %}  \n\nbatchnorm  \n\nbatchnormICS  \n\nbatchnorm  \n\nbatchnorm  \n\n# layernorm\n\nbatchnormlayernorm  \n\n## layernorm\n\nlayernormlayerlayer  \n\nbatchnormbatchlayernorm  \n\n{% asset_img bn_and_ln.png bnln %}  \n\nNLPbatchnormlayernormbatchnormlayernorm  \n\nlayernormNLPRNNtransfomrer  \n\ntransformer $[B,S,H]$ $S$ paddingzero-paddingbatch  \n\n\n\n```\n      \n  \n    \n```\n\npad\n\n```\n      \n    [P] [P]\n      [P]\n```\n\npaddingbatchnorm  \n\n[P] [P] batch size2 [P] normalization  \n\nbatch  \n\n [P] token  \n\nlayernorm $H$ normalization $H$ $\\gamma$  $\\beta$   \n\ntoken  \n\n## transformerlayernorm  \n\nbatchnormlayernormbatch  \n\nlayernormbatchbatch size  \n\nnlplayernormbatchnorm  \n\nPowerNorm: Rethinking Batch Normalization in TransformerstransformerBN  \n\nbatchbatchrunning statisticsNLPIWSLT14batchrunning statisticsCV  \n\nmagnitudeCV  \n\ntransformerBNCVNLPNLPbatch  \n\nlayernormNLPbatchnorm  \n\n## RMSnorm\n\n19Root Mean Square Layer NormalizationnormalizationRMSnormlayernorm  \n\nRMSnormlayernorm  \n\n{% asset_img rmsnorm.png RMSnorm %}  \n\nlayernorm  \n\n{% asset_img rmsnorm_eff.png RMSnorm %}  \n\nGRUlayernormlayernormlayernorm  \n\nlayernorm  \n\npRMSnormp%  \n\n{% asset_img prmsnorm.png prmsnorm %}  \n\nRMSnorm  \n\n# post-norm & pre-norm\n\n## \nlayernorm  \n\ntransformerpost-normOn Layer Normalization in the Transformer Architecturepre-norm  \n\npost-normpre-norm\n\n{% asset_img postnorm_prenorm.png postnorm and prenorm %}  \n\npost-normpre-norm  \n\npost-normpre-normpre-normpost-normpre-norm  \n\n $l$  $l+1$ post-norm  \n\n$$\nx_{l+1}=\\mathrm{Norm}(x_l+\\mathrm{F}_t(x_l))\n$$\n\npre-norm  \n\n$$\nx_{l+1}=x_l+\\mathrm{F}_l(\\mathrm{Norm}(x_l))\n$$\n\nPre NormPost Norm $\\mathrm{F}_l(\\mathrm{Norm}(x_l))$ norm  \n\n $l$ $x_{l}x_{l+1}$  $\\mathrm{F}_l(\\mathrm{Norm}(x_l))$  $\\mathrm{F}_{l+1}(\\mathrm{Norm}(x_{l+1}))$   \n\n$$\\begin{aligned}\n&\\mathrm{F}_l(\\operatorname{Norm}(x_l))+\\operatorname{F}_{l+1}(\\operatorname{Norm}(x_{l+1})) \\\\\n&{\\approx}\\mathrm{F}_l(\\mathrm{Norm}(x_l))+\\mathrm{F}_{l+1}\\left(\\mathrm{Norm}(x_l)\\right) \\\\\n&=(\\mathrm{F}_l\\oplus\\mathrm{F}_{l+1})(\\mathrm{Norm}(\\pmb{x_l}))\n\\end{aligned}$$\n\n $l$  $l+1$  $l$   \n\npre-norm $l$ post-norm  \n\nnormalizationpre-norm--  \n\n  \n\npost-normlossnormpost-normpre-norm  \n\n## warmup  \n\nOn Layer Normalization in the Transformer Architecturepre-normpost-normtransformerwarmup  \n\nwarmup  \n\npre-normtransformerwarmuppost-norm+warmuppost-normwarmup  \n\n{% asset_img warmup_effect.png warmup %}  \n\n## Deepnorm  \n\n2022DeepNet: Scaling Transformers to 1,000 Layerstransformer  \n\nDeepnorm  \n\n{% asset_img deepnorm.png deepnorm %}  \n\n $\\alpha>1$ post-norm  \n\n $\\beta$  $G_{l}$   \n\ndeepnormpre-normpost-norm  \n\n{% asset_img deepnorm_result.png deepnorm result %}  \n\npost-norm  \n\n## Realformer--residual attention  \n\npost-normpre-normRealFormer: Transformer Likes Residual Attention  \n\n{% asset_img realformer.png realformer %}  \n\nRealFormerTransformerSoftmax\n\n\n\n{% asset_img realformer_attention.png realformer attention %}  \n\n $Prev'$ softmax $\\frac{Q^{\\prime}K^{\\prime T}}{\\sqrt{d_k}}+Prev'$ attention  \n\n# \n\nnormalizationbatchnormlayernormtransformer\n\nrmsnorm + prenorm  \n\nnormalization    \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n***\n\n# Reference  \n1https://www.zhihu.com/question/487766088  \n2Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization https://arxiv.org/abs/2001.06838  \n3Transformer()& https://zhuanlan.zhihu.com/p/476102712  \n4Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://arxiv.org/abs/1502.03167  \n5How Does Batch Normalization Help Optimization? https://arxiv.org/abs/1805.11604  \n6Batch Normalization: Accelerating Deep Network https://zhuanlan.zhihu.com/p/340856414  \n7Layer Normalization https://arxiv.org/abs/1607.06450  \n8NormalizationBN/LN/WN https://zhuanlan.zhihu.com/p/33173246  \n9Transformer()BatchNormalization https://zhuanlan.zhihu.com/p/481277619  \n10Layer Normalization https://arxiv.org/abs/1607.06450  \n11PowerNorm: Rethinking Batch Normalization in Transformers https://arxiv.org/abs/2003.07845  \n12Root Mean Square Layer Normalization https://arxiv.org/abs/1910.07467  \n13On Layer Normalization in the Transformer Architecture https://arxiv.org/abs/2002.04745  \n14Pre NormPost Norm https://spaces.ac.cn/archives/9009  \n15Understanding the Difficulty of Training Transformers https://arxiv.org/abs/2004.08249  \n16RealFormer: Transformer Likes Residual Attention https://arxiv.org/abs/2012.11747  \n17DeepNet: Scaling Transformers to 1,000 Layers https://arxiv.org/abs/2203.00555  \n\n","slug":"cs/nlp/2024/03/Transformernormalization","published":1,"updated":"2024-04-07T06:36:14.847Z","comments":1,"layout":"post","photos":[],"_id":"clv7wdj0r000k794k5owmantc","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>Normalizationattention</p>\n<p>normalizationtransformer</p>\n<p></p>\n<h1 id=\"why-normalization\">why normalization</h1>\n<p>normalization</p>\n<p></p>\n<h2 id=\"normalization\">normalization</h2>\n<p> <span class=\"math inline\">\\(Loss(x_1,x_2)=x_1^2+x_2^2+b\\)</span>\n</p>\n<img src=\"/6a40bfa5/lossfunc_surface.jpeg\" class title=\"loss function surface\">\n<p></p>\n<p></p>\n<p>minimum</p>\n<p></p>\n<img src=\"/6a40bfa5/ellipse_1.png\" class title=\"ellipse\">\n<p></p>\n<p></p>\n<img src=\"/6a40bfa5/ellipse_2.png\" class title=\"ellipse\">\n<p></p>\n<p></p>\n<p> <span class=\"math inline\">\\(x_{1}\\)</span> <span class=\"math inline\">\\(x_{2}\\)</span>-&gt;</p>\n<p>0.x~2.x0</p>\n<p></p>\n<p> <span class=\"math inline\">\\(x_{2}\\)</span>\n</p>\n<p></p>\n<p></p>\n<p>normalization</p>\n<p>normalization<br>\n<span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\min(\\mathrm{x})}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}\\]</span></p>\n<p><span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\mu}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}\\]</span></p>\n<p><span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\mu}\\sigma\\]</span></p>\n<p> <span class=\"math inline\">\\(\\mu\\)</span> <span class=\"math inline\">\\(\\sigma\\)</span> </p>\n<p>Z-score\nnormalization</p>\n<p></p>\n<p>PCA</p>\n<h2 id=\"ics...\">ICS...</h2>\n<p>i.i.d.independent and identical\ndistribution</p>\n<p>i.i.d.</p>\n<p></p>\n<p></p>\n<p></p>\n<p>i.i.d.i.i.d.</p>\n<p>PCAi.i.d.</p>\n<p></p>\n<p>i.i.d.</p>\n<p>ICSinternal covariate shift</p>\n<p></p>\n<p>normalization</p>\n<p>normalizationbatchnormICSbatchnorm</p>\n<p>ICS</p>\n<h2 id=\"\"></h2>\n<p>sigmoid</p>\n<img src=\"/6a40bfa5/sigmoid.png\" class title=\"sigmoid\">\n<p> &gt; 6  &lt; -6\nsigmoid</p>\n<p></p>\n<p>ICSnormalization</p>\n<p>normalization</p>\n<h1 id=\"batchnorm\">batchnorm</h1>\n<p>normalizationbatchnormlayernorm</p>\n<h2 id=\"batchnorm\">batchnorm</h2>\n<p> <span class=\"math inline\">\\([B,C]\\)</span>\n <span class=\"math inline\">\\(B\\)</span> batch size<span class=\"math inline\">\\(C\\)</span> </p>\n<p> <span class=\"math inline\">\\(C\\)</span>\nnormalization\n<span class=\"math inline\">\\(C\\)</span> </p>\n<p> <span class=\"math inline\">\\(i\\)</span>\nbatch</p>\n<p><span class=\"math display\">\\[\n\\mu_{i}=\\frac{1}{B}\\sum_{j=1}^{B}x_{i,j}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\sigma_{i}^{2}=\\frac{1}{B}\\sum_{j=1}^{B}(x_{i,j}-\\mu_{i})^2\n\\]</span></p>\n<p>batchZ-score\nnormalization</p>\n<p><span class=\"math display\">\\[\nx_{i,j}&#39;=\\frac{x_{i,j}-\\mu_{i}}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\epsilon\\)</span>\n0</p>\n<p> <span class=\"math inline\">\\(C\\)</span>\n01</p>\n<p></p>\n<p>sigmoid</p>\n<p></p>\n<p>\n <span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p><span class=\"math display\">\\[\ny_{i,j} = \\gamma_{i} x_{i,j}&#39; + \\beta_{i}\n\\]</span></p>\n<p></p>\n<p>batchnorm\n<span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span>\n</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/340856414\">Batch\nNormalization: Accelerating Deep Network</a></p>\n<img src=\"/6a40bfa5/bn_algo.png\" class title=\"batch norm\">\n<h2 id=\"cnnbatchnorm\">CNNbatchnorm</h2>\n<p>batchnormCNN</p>\n<p>CNNfeature mapsize <span class=\"math inline\">\\([B,C,H,W]\\)</span>  <span class=\"math inline\">\\(B\\)</span> batch size<span class=\"math inline\">\\(C\\)</span> channel<span class=\"math inline\">\\(H\\)</span>  <span class=\"math inline\">\\(W\\)</span> </p>\n<p>batchnorm <span class=\"math inline\">\\(C\\times H\\times W\\)</span> \n<span class=\"math inline\">\\(B\\)</span>  <span class=\"math inline\">\\(B\\)</span> </p>\n<p>CNNchannel <span class=\"math inline\">\\(H\\times W\\)</span>\n <span class=\"math inline\">\\(H\\times W\\)</span>\nbatch</p>\n<p>batch <span class=\"math inline\">\\(B\\times H\\times W\\)</span>  <span class=\"math inline\">\\(C\\)</span>  <span class=\"math inline\">\\(C\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p>batchnormbatchnormrelufcbatchnormfcbias</p>\n<p>btwbatchnorm <span class=\"math inline\">\\(\\gamma\\)</span> 1 <span class=\"math inline\">\\(\\beta\\)</span>\n0batchnorm</p>\n<h2 id=\"\"></h2>\n<p>batchnormmini-batch</p>\n<p><span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span>\n</p>\n<p>samplesamplebatch</p>\n<p>i.i.d.</p>\n<p></p>\n<p>moving_mean = momentum  moving_mean + (1.0  momentum)  mean</p>\n<p>moving_var = momentum  moving_var + (1.0  momentum)  var</p>\n<p>batch</p>\n<p>momentum TF/Keras 0.99 Pytorch\n0.9</p>\n<p>momentum</p>\n<p>momentum</p>\n<p>batch sizemini\nbatchmomentum</p>\n<p>batch</p>\n<p>batch sizemini\nbatchbatchnormbatch\nsize</p>\n<img src=\"/6a40bfa5/bs_bn.png\" class title=\"batch size\">\n<p>batchnorm</p>\n<p>batch</p>\n<p>momentumbatch\nsizebatch size</p>\n<h2 id=\"batchnorm\">batchnorm</h2>\n<p>batchnormbatchnormICS2018How\nDoes Batch Normalization Help Optimization?</p>\n<p>batchnormICSICSbatchnormcovariate\nshiftbatchnorm</p>\n<img src=\"/6a40bfa5/bn_ics.png\" class title=\"ICS\">\n<p>ICSbatchnormICS</p>\n<p>ICSbatchnorm</p>\n<p>batchnormICS</p>\n<p>ICSICS</p>\n<p>ICS</p>\n<img src=\"/6a40bfa5/ics_define.png\" class title=\"ICS \">\n<p>iL2</p>\n<p><span class=\"math inline\">\\(G_{t,i}\\)</span>\nt</p>\n<p><span class=\"math inline\">\\(G_{t,i}&#39;\\)</span>\nt</p>\n<p></p>\n<p>ICS</p>\n<img src=\"/6a40bfa5/ics_measure.png\" class title=\"ICS measure\">\n<p>batchnorm</p>\n<p>batchnormICS</p>\n<p>batchnorm</p>\n<p>batchnorm</p>\n<h1 id=\"layernorm\">layernorm</h1>\n<p>batchnormlayernorm</p>\n<h2 id=\"layernorm\">layernorm</h2>\n<p>layernormlayerlayer</p>\n<p>batchnormbatchlayernorm</p>\n<img src=\"/6a40bfa5/bn_and_ln.png\" class title=\"bnln\">\n<p>NLPbatchnormlayernormbatchnormlayernorm</p>\n<p>layernormNLPRNNtransfomrer</p>\n<p>transformer <span class=\"math inline\">\\([B,S,H]\\)</span> <span class=\"math inline\">\\(S\\)</span>\npaddingzero-paddingbatch</p>\n<p></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">  </span><br><span class=\"line\">    </span><br></pre></td></tr></table></figure>\n<p>pad</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">    [P] [P]</span><br><span class=\"line\">      [P]</span><br></pre></td></tr></table></figure>\n<p>paddingbatchnorm</p>\n<p>[P] [P]\nbatch size2 [P]\nnormalization</p>\n<p>batch</p>\n<p> [P]\ntoken</p>\n<p>layernorm <span class=\"math inline\">\\(H\\)</span>\nnormalization <span class=\"math inline\">\\(H\\)</span>\n<span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p>token</p>\n<h2 id=\"transformerlayernorm\">transformerlayernorm</h2>\n<p>batchnormlayernormbatch</p>\n<p>layernormbatchbatch size</p>\n<p>nlplayernormbatchnorm</p>\n<p>PowerNorm: Rethinking Batch Normalization in\nTransformerstransformerBN</p>\n<p>batchbatchrunning\nstatisticsNLPIWSLT14batchrunning\nstatisticsCV</p>\n<p>magnitudeCV</p>\n<p>transformerBNCVNLPNLPbatch</p>\n<p>layernormNLPbatchnorm</p>\n<h2 id=\"rmsnorm\">RMSnorm</h2>\n<p>19Root Mean Square Layer\nNormalizationnormalizationRMSnormlayernorm</p>\n<p>RMSnormlayernorm</p>\n<img src=\"/6a40bfa5/rmsnorm.png\" class title=\"RMSnorm\">\n<p>layernorm</p>\n<img src=\"/6a40bfa5/rmsnorm_eff.png\" class title=\"RMSnorm\">\n<p>GRUlayernormlayernormlayernorm</p>\n<p>layernorm</p>\n<p>pRMSnormp%</p>\n<img src=\"/6a40bfa5/prmsnorm.png\" class title=\"prmsnorm\">\n<p>RMSnorm</p>\n<h1 id=\"post-norm-pre-norm\">post-norm &amp; pre-norm</h1>\n<h2 id=\"\"></h2>\n<p>layernorm</p>\n<p>transformerpost-normOn Layer Normalization in\nthe Transformer Architecturepre-norm</p>\n<p>post-normpre-norm</p>\n<img src=\"/6a40bfa5/postnorm_prenorm.png\" class title=\"postnorm and prenorm\">\n<p>post-normpre-norm</p>\n<p>post-normpre-normpre-normpost-normpre-norm</p>\n<p> <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(l+1\\)</span> post-norm</p>\n<p><span class=\"math display\">\\[\nx_{l+1}=\\mathrm{Norm}(x_l+\\mathrm{F}_t(x_l))\n\\]</span></p>\n<p>pre-norm</p>\n<p><span class=\"math display\">\\[\nx_{l+1}=x_l+\\mathrm{F}_l(\\mathrm{Norm}(x_l))\n\\]</span></p>\n<p>Pre NormPost Norm\n<span class=\"math inline\">\\(\\mathrm{F}_l(\\mathrm{Norm}(x_l))\\)</span>\nnorm</p>\n<p> <span class=\"math inline\">\\(l\\)</span> <span class=\"math inline\">\\(x_{l}x_{l+1}\\)</span>  <span class=\"math inline\">\\(\\mathrm{F}_l(\\mathrm{Norm}(x_l))\\)</span>  <span class=\"math inline\">\\(\\mathrm{F}_{l+1}(\\mathrm{Norm}(x_{l+1}))\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\mathrm{F}_l(\\operatorname{Norm}(x_l))+\\operatorname{F}_{l+1}(\\operatorname{Norm}(x_{l+1}))\n\\\\\n&amp;{\\approx}\\mathrm{F}_l(\\mathrm{Norm}(x_l))+\\mathrm{F}_{l+1}\\left(\\mathrm{Norm}(x_l)\\right)\n\\\\\n&amp;=(\\mathrm{F}_l\\oplus\\mathrm{F}_{l+1})(\\mathrm{Norm}(\\pmb{x_l}))\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(l+1\\)</span>  <span class=\"math inline\">\\(l\\)</span> </p>\n<p>pre-norm <span class=\"math inline\">\\(l\\)</span>\npost-norm</p>\n<p>normalizationpre-norm--</p>\n<p></p>\n<p>post-normlossnormpost-normpre-norm</p>\n<h2 id=\"warmup\">warmup</h2>\n<p>On Layer Normalization in the Transformer\nArchitecturepre-normpost-normtransformerwarmup</p>\n<p>warmup</p>\n<p>pre-normtransformerwarmuppost-norm+warmuppost-normwarmup</p>\n<img src=\"/6a40bfa5/warmup_effect.png\" class title=\"warmup\">\n<h2 id=\"deepnorm\">Deepnorm</h2>\n<p>2022DeepNet: Scaling Transformers to 1,000\nLayerstransformer</p>\n<p>Deepnorm</p>\n<img src=\"/6a40bfa5/deepnorm.png\" class title=\"deepnorm\">\n<p> <span class=\"math inline\">\\(\\alpha&gt;1\\)</span>\npost-norm</p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span> \n<span class=\"math inline\">\\(G_{l}\\)</span>\n</p>\n<p>deepnormpre-normpost-norm</p>\n<img src=\"/6a40bfa5/deepnorm_result.png\" class title=\"deepnorm result\">\n<p>post-norm</p>\n<h2 id=\"realformer--residual-attention\">Realformer--residual\nattention</h2>\n<p>post-normpre-normRealFormer:\nTransformer Likes Residual Attention</p>\n<img src=\"/6a40bfa5/realformer.png\" class title=\"realformer\">\n<p>RealFormerTransformerSoftmax</p>\n<p></p>\n<img src=\"/6a40bfa5/realformer_attention.png\" class title=\"realformer attention\">\n<p> <span class=\"math inline\">\\(Prev&#39;\\)</span>\nsoftmax\n<span class=\"math inline\">\\(\\frac{Q^{\\prime}K^{\\prime\nT}}{\\sqrt{d_k}}+Prev&#39;\\)</span> attention</p>\n<h1 id=\"\"></h1>\n<p>normalizationbatchnormlayernormtransformer</p>\n<p>rmsnorm + prenorm</p>\n<p>normalization</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1https://www.zhihu.com/question/487766088<br>\n2Towards Stabilizing Batch Statistics in Backward Propagation of\nBatch Normalization https://arxiv.org/abs/2001.06838<br>\n3Transformer()&amp;\nhttps://zhuanlan.zhihu.com/p/476102712<br>\n4Batch Normalization: Accelerating Deep Network Training by Reducing\nInternal Covariate Shift https://arxiv.org/abs/1502.03167<br>\n5How Does Batch Normalization Help Optimization?\nhttps://arxiv.org/abs/1805.11604<br>\n6Batch Normalization: Accelerating Deep Network\nhttps://zhuanlan.zhihu.com/p/340856414<br>\n7Layer Normalization https://arxiv.org/abs/1607.06450<br>\n8NormalizationBN/LN/WN\nhttps://zhuanlan.zhihu.com/p/33173246<br>\n9Transformer()BatchNormalization\nhttps://zhuanlan.zhihu.com/p/481277619<br>\n10Layer Normalization https://arxiv.org/abs/1607.06450<br>\n11PowerNorm: Rethinking Batch Normalization in Transformers\nhttps://arxiv.org/abs/2003.07845<br>\n12Root Mean Square Layer Normalization\nhttps://arxiv.org/abs/1910.07467<br>\n13On Layer Normalization in the Transformer Architecture\nhttps://arxiv.org/abs/2002.04745<br>\n14Pre NormPost Norm\nhttps://spaces.ac.cn/archives/9009<br>\n15Understanding the Difficulty of Training Transformers\nhttps://arxiv.org/abs/2004.08249<br>\n16RealFormer: Transformer Likes Residual Attention\nhttps://arxiv.org/abs/2012.11747<br>\n17DeepNet: Scaling Transformers to 1,000 Layers\nhttps://arxiv.org/abs/2203.00555</p>\n","length":11986,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>Normalizationattention</p>\n<p>normalizationtransformer</p>\n<p></p>\n<h1 id=\"why-normalization\">why normalization</h1>\n<p>normalization</p>\n<p></p>\n<h2 id=\"normalization\">normalization</h2>\n<p> <span class=\"math inline\">\\(Loss(x_1,x_2)=x_1^2+x_2^2+b\\)</span>\n</p>\n<img src=\"/6a40bfa5/lossfunc_surface.jpeg\" class title=\"loss function surface\">\n<p></p>\n<p></p>\n<p>minimum</p>\n<p></p>\n<img src=\"/6a40bfa5/ellipse_1.png\" class title=\"ellipse\">\n<p></p>\n<p></p>\n<img src=\"/6a40bfa5/ellipse_2.png\" class title=\"ellipse\">\n<p></p>\n<p></p>\n<p> <span class=\"math inline\">\\(x_{1}\\)</span> <span class=\"math inline\">\\(x_{2}\\)</span>-&gt;</p>\n<p>0.x~2.x0</p>\n<p></p>\n<p> <span class=\"math inline\">\\(x_{2}\\)</span>\n</p>\n<p></p>\n<p></p>\n<p>normalization</p>\n<p>normalization<br>\n<span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\min(\\mathrm{x})}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}\\]</span></p>\n<p><span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\mu}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}\\]</span></p>\n<p><span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\mu}\\sigma\\]</span></p>\n<p> <span class=\"math inline\">\\(\\mu\\)</span> <span class=\"math inline\">\\(\\sigma\\)</span> </p>\n<p>Z-score\nnormalization</p>\n<p></p>\n<p>PCA</p>\n<h2 id=\"ics...\">ICS...</h2>\n<p>i.i.d.independent and identical\ndistribution</p>\n<p>i.i.d.</p>\n<p></p>\n<p></p>\n<p></p>\n<p>i.i.d.i.i.d.</p>\n<p>PCAi.i.d.</p>\n<p></p>\n<p>i.i.d.</p>\n<p>ICSinternal covariate shift</p>\n<p></p>\n<p>normalization</p>\n<p>normalizationbatchnormICSbatchnorm</p>\n<p>ICS</p>\n<h2 id=\"\"></h2>\n<p>sigmoid</p>\n<img src=\"/6a40bfa5/sigmoid.png\" class title=\"sigmoid\">\n<p> &gt; 6  &lt; -6\nsigmoid</p>\n<p></p>\n<p>ICSnormalization</p>\n<p>normalization</p>\n<h1 id=\"batchnorm\">batchnorm</h1>\n<p>normalizationbatchnormlayernorm</p>\n<h2 id=\"batchnorm\">batchnorm</h2>\n<p> <span class=\"math inline\">\\([B,C]\\)</span>\n <span class=\"math inline\">\\(B\\)</span> batch size<span class=\"math inline\">\\(C\\)</span> </p>\n<p> <span class=\"math inline\">\\(C\\)</span>\nnormalization\n<span class=\"math inline\">\\(C\\)</span> </p>\n<p> <span class=\"math inline\">\\(i\\)</span>\nbatch</p>\n<p><span class=\"math display\">\\[\n\\mu_{i}=\\frac{1}{B}\\sum_{j=1}^{B}x_{i,j}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\sigma_{i}^{2}=\\frac{1}{B}\\sum_{j=1}^{B}(x_{i,j}-\\mu_{i})^2\n\\]</span></p>\n<p>batchZ-score\nnormalization</p>\n<p><span class=\"math display\">\\[\nx_{i,j}&#39;=\\frac{x_{i,j}-\\mu_{i}}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\epsilon\\)</span>\n0</p>\n<p> <span class=\"math inline\">\\(C\\)</span>\n01</p>\n<p></p>\n<p>sigmoid</p>\n<p></p>\n<p>\n <span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p><span class=\"math display\">\\[\ny_{i,j} = \\gamma_{i} x_{i,j}&#39; + \\beta_{i}\n\\]</span></p>\n<p></p>\n<p>batchnorm\n<span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span>\n</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/340856414\">Batch\nNormalization: Accelerating Deep Network</a></p>\n<img src=\"/6a40bfa5/bn_algo.png\" class title=\"batch norm\">\n<h2 id=\"cnnbatchnorm\">CNNbatchnorm</h2>\n<p>batchnormCNN</p>\n<p>CNNfeature mapsize <span class=\"math inline\">\\([B,C,H,W]\\)</span>  <span class=\"math inline\">\\(B\\)</span> batch size<span class=\"math inline\">\\(C\\)</span> channel<span class=\"math inline\">\\(H\\)</span>  <span class=\"math inline\">\\(W\\)</span> </p>\n<p>batchnorm <span class=\"math inline\">\\(C\\times H\\times W\\)</span> \n<span class=\"math inline\">\\(B\\)</span>  <span class=\"math inline\">\\(B\\)</span> </p>\n<p>CNNchannel <span class=\"math inline\">\\(H\\times W\\)</span>\n <span class=\"math inline\">\\(H\\times W\\)</span>\nbatch</p>\n<p>batch <span class=\"math inline\">\\(B\\times H\\times W\\)</span>  <span class=\"math inline\">\\(C\\)</span>  <span class=\"math inline\">\\(C\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p>batchnormbatchnormrelufcbatchnormfcbias</p>\n<p>btwbatchnorm <span class=\"math inline\">\\(\\gamma\\)</span> 1 <span class=\"math inline\">\\(\\beta\\)</span>\n0batchnorm</p>\n<h2 id=\"\"></h2>\n<p>batchnormmini-batch</p>\n<p><span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span>\n</p>\n<p>samplesamplebatch</p>\n<p>i.i.d.</p>\n<p></p>\n<p>moving_mean = momentum  moving_mean + (1.0  momentum)  mean</p>\n<p>moving_var = momentum  moving_var + (1.0  momentum)  var</p>\n<p>batch</p>\n<p>momentum TF/Keras 0.99 Pytorch\n0.9</p>\n<p>momentum</p>\n<p>momentum</p>\n<p>batch sizemini\nbatchmomentum</p>\n<p>batch</p>\n<p>batch sizemini\nbatchbatchnormbatch\nsize</p>\n<img src=\"/6a40bfa5/bs_bn.png\" class title=\"batch size\">\n<p>batchnorm</p>\n<p>batch</p>\n<p>momentumbatch\nsizebatch size</p>\n<h2 id=\"batchnorm\">batchnorm</h2>\n<p>batchnormbatchnormICS2018How\nDoes Batch Normalization Help Optimization?</p>\n<p>batchnormICSICSbatchnormcovariate\nshiftbatchnorm</p>\n<img src=\"/6a40bfa5/bn_ics.png\" class title=\"ICS\">\n<p>ICSbatchnormICS</p>\n<p>ICSbatchnorm</p>\n<p>batchnormICS</p>\n<p>ICSICS</p>\n<p>ICS</p>\n<img src=\"/6a40bfa5/ics_define.png\" class title=\"ICS \">\n<p>iL2</p>\n<p><span class=\"math inline\">\\(G_{t,i}\\)</span>\nt</p>\n<p><span class=\"math inline\">\\(G_{t,i}&#39;\\)</span>\nt</p>\n<p></p>\n<p>ICS</p>\n<img src=\"/6a40bfa5/ics_measure.png\" class title=\"ICS measure\">\n<p>batchnorm</p>\n<p>batchnormICS</p>\n<p>batchnorm</p>\n<p>batchnorm</p>\n<h1 id=\"layernorm\">layernorm</h1>\n<p>batchnormlayernorm</p>\n<h2 id=\"layernorm\">layernorm</h2>\n<p>layernormlayerlayer</p>\n<p>batchnormbatchlayernorm</p>\n<img src=\"/6a40bfa5/bn_and_ln.png\" class title=\"bnln\">\n<p>NLPbatchnormlayernormbatchnormlayernorm</p>\n<p>layernormNLPRNNtransfomrer</p>\n<p>transformer <span class=\"math inline\">\\([B,S,H]\\)</span> <span class=\"math inline\">\\(S\\)</span>\npaddingzero-paddingbatch</p>\n<p></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">  </span><br><span class=\"line\">    </span><br></pre></td></tr></table></figure>\n<p>pad</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">    [P] [P]</span><br><span class=\"line\">      [P]</span><br></pre></td></tr></table></figure>\n<p>paddingbatchnorm</p>\n<p>[P] [P]\nbatch size2 [P]\nnormalization</p>\n<p>batch</p>\n<p> [P]\ntoken</p>\n<p>layernorm <span class=\"math inline\">\\(H\\)</span>\nnormalization <span class=\"math inline\">\\(H\\)</span>\n<span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p>token</p>\n<h2 id=\"transformerlayernorm\">transformerlayernorm</h2>\n<p>batchnormlayernormbatch</p>\n<p>layernormbatchbatch size</p>\n<p>nlplayernormbatchnorm</p>\n<p>PowerNorm: Rethinking Batch Normalization in\nTransformerstransformerBN</p>\n<p>batchbatchrunning\nstatisticsNLPIWSLT14batchrunning\nstatisticsCV</p>\n<p>magnitudeCV</p>\n<p>transformerBNCVNLPNLPbatch</p>\n<p>layernormNLPbatchnorm</p>\n<h2 id=\"rmsnorm\">RMSnorm</h2>\n<p>19Root Mean Square Layer\nNormalizationnormalizationRMSnormlayernorm</p>\n<p>RMSnormlayernorm</p>\n<img src=\"/6a40bfa5/rmsnorm.png\" class title=\"RMSnorm\">\n<p>layernorm</p>\n<img src=\"/6a40bfa5/rmsnorm_eff.png\" class title=\"RMSnorm\">\n<p>GRUlayernormlayernormlayernorm</p>\n<p>layernorm</p>\n<p>pRMSnormp%</p>\n<img src=\"/6a40bfa5/prmsnorm.png\" class title=\"prmsnorm\">\n<p>RMSnorm</p>\n<h1 id=\"post-norm-pre-norm\">post-norm &amp; pre-norm</h1>\n<h2 id=\"\"></h2>\n<p>layernorm</p>\n<p>transformerpost-normOn Layer Normalization in\nthe Transformer Architecturepre-norm</p>\n<p>post-normpre-norm</p>\n<img src=\"/6a40bfa5/postnorm_prenorm.png\" class title=\"postnorm and prenorm\">\n<p>post-normpre-norm</p>\n<p>post-normpre-normpre-normpost-normpre-norm</p>\n<p> <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(l+1\\)</span> post-norm</p>\n<p><span class=\"math display\">\\[\nx_{l+1}=\\mathrm{Norm}(x_l+\\mathrm{F}_t(x_l))\n\\]</span></p>\n<p>pre-norm</p>\n<p><span class=\"math display\">\\[\nx_{l+1}=x_l+\\mathrm{F}_l(\\mathrm{Norm}(x_l))\n\\]</span></p>\n<p>Pre NormPost Norm\n<span class=\"math inline\">\\(\\mathrm{F}_l(\\mathrm{Norm}(x_l))\\)</span>\nnorm</p>\n<p> <span class=\"math inline\">\\(l\\)</span> <span class=\"math inline\">\\(x_{l}x_{l+1}\\)</span>  <span class=\"math inline\">\\(\\mathrm{F}_l(\\mathrm{Norm}(x_l))\\)</span>  <span class=\"math inline\">\\(\\mathrm{F}_{l+1}(\\mathrm{Norm}(x_{l+1}))\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\mathrm{F}_l(\\operatorname{Norm}(x_l))+\\operatorname{F}_{l+1}(\\operatorname{Norm}(x_{l+1}))\n\\\\\n&amp;{\\approx}\\mathrm{F}_l(\\mathrm{Norm}(x_l))+\\mathrm{F}_{l+1}\\left(\\mathrm{Norm}(x_l)\\right)\n\\\\\n&amp;=(\\mathrm{F}_l\\oplus\\mathrm{F}_{l+1})(\\mathrm{Norm}(\\pmb{x_l}))\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(l+1\\)</span>  <span class=\"math inline\">\\(l\\)</span> </p>\n<p>pre-norm <span class=\"math inline\">\\(l\\)</span>\npost-norm</p>\n<p>normalizationpre-norm--</p>\n<p></p>\n<p>post-normlossnormpost-normpre-norm</p>\n<h2 id=\"warmup\">warmup</h2>\n<p>On Layer Normalization in the Transformer\nArchitecturepre-normpost-normtransformerwarmup</p>\n<p>warmup</p>\n<p>pre-normtransformerwarmuppost-norm+warmuppost-normwarmup</p>\n<img src=\"/6a40bfa5/warmup_effect.png\" class title=\"warmup\">\n<h2 id=\"deepnorm\">Deepnorm</h2>\n<p>2022DeepNet: Scaling Transformers to 1,000\nLayerstransformer</p>\n<p>Deepnorm</p>\n<img src=\"/6a40bfa5/deepnorm.png\" class title=\"deepnorm\">\n<p> <span class=\"math inline\">\\(\\alpha&gt;1\\)</span>\npost-norm</p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span> \n<span class=\"math inline\">\\(G_{l}\\)</span>\n</p>\n<p>deepnormpre-normpost-norm</p>\n<img src=\"/6a40bfa5/deepnorm_result.png\" class title=\"deepnorm result\">\n<p>post-norm</p>\n<h2 id=\"realformer--residual-attention\">Realformer--residual\nattention</h2>\n<p>post-normpre-normRealFormer:\nTransformer Likes Residual Attention</p>\n<img src=\"/6a40bfa5/realformer.png\" class title=\"realformer\">\n<p>RealFormerTransformerSoftmax</p>\n<p></p>\n<img src=\"/6a40bfa5/realformer_attention.png\" class title=\"realformer attention\">\n<p> <span class=\"math inline\">\\(Prev&#39;\\)</span>\nsoftmax\n<span class=\"math inline\">\\(\\frac{Q^{\\prime}K^{\\prime\nT}}{\\sqrt{d_k}}+Prev&#39;\\)</span> attention</p>\n<h1 id=\"\"></h1>\n<p>normalizationbatchnormlayernormtransformer</p>\n<p>rmsnorm + prenorm</p>\n<p>normalization</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1https://www.zhihu.com/question/487766088<br>\n2Towards Stabilizing Batch Statistics in Backward Propagation of\nBatch Normalization https://arxiv.org/abs/2001.06838<br>\n3Transformer()&amp;\nhttps://zhuanlan.zhihu.com/p/476102712<br>\n4Batch Normalization: Accelerating Deep Network Training by Reducing\nInternal Covariate Shift https://arxiv.org/abs/1502.03167<br>\n5How Does Batch Normalization Help Optimization?\nhttps://arxiv.org/abs/1805.11604<br>\n6Batch Normalization: Accelerating Deep Network\nhttps://zhuanlan.zhihu.com/p/340856414<br>\n7Layer Normalization https://arxiv.org/abs/1607.06450<br>\n8NormalizationBN/LN/WN\nhttps://zhuanlan.zhihu.com/p/33173246<br>\n9Transformer()BatchNormalization\nhttps://zhuanlan.zhihu.com/p/481277619<br>\n10Layer Normalization https://arxiv.org/abs/1607.06450<br>\n11PowerNorm: Rethinking Batch Normalization in Transformers\nhttps://arxiv.org/abs/2003.07845<br>\n12Root Mean Square Layer Normalization\nhttps://arxiv.org/abs/1910.07467<br>\n13On Layer Normalization in the Transformer Architecture\nhttps://arxiv.org/abs/2002.04745<br>\n14Pre NormPost Norm\nhttps://spaces.ac.cn/archives/9009<br>\n15Understanding the Difficulty of Training Transformers\nhttps://arxiv.org/abs/2004.08249<br>\n16RealFormer: Transformer Likes Residual Attention\nhttps://arxiv.org/abs/2012.11747<br>\n17DeepNet: Scaling Transformers to 1,000 Layers\nhttps://arxiv.org/abs/2203.00555</p>\n"},{"title":"(2)","abbrlink":"ad0bba9d","date":"2024-03-24T03:24:47.000Z","_content":"\n![](/images/cover.png)  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~~\n\n***  \n\n# 1Berttoken embedding()position encoding  \n\n1concat+  \n\n2(768)250076810007681-1  \n\n3one-hotone-hot  \n\n4  \n\n# 2LoRALoRA  \n\n1LoRALoRA  \n\n2//LoRA  \n\n3LoRA1batch23optionalint8/int4  \n\n# 3normalizationbatchnorm/layernorm  \n\n118018normalization/  \n\n2batchnormICSinternal covariate shifti.i.d.normalization  \n\n3.How Does Batch Normalization Help Optimization?batchnormICSbatchnormICSbatchnorm  \n\n# 4Transformerpre-normpost-norm?  \n\n1.Transformerpost-normadd & normpost-norm  \n\n2.Pre-normpost-normL+1L  \n\n3.post-normpre-normpost-normpre-norm  \n\n# 5Multi-Head Attentionhidden size=DhdD=dhsbatch size=1self-attentionFloat Operations  \n\n1.QKV6  s  D^22D  \n\n2.QKh  2  d  s^2h  \n\n3.scalingh  s^2  \n\n4.softmaxh  3  s^2softmaxsexpsexps  \n\n5.reductionVh  2  d  s^2  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)  \n","source":"_posts/cs/nlp/2024/03/-2.md","raw":"---\ntitle: (2)\ntags:\n  - NLP\n  - LLM\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: ad0bba9d\ndate: 2024-03-24 11:24:47\n---\n\n![](/images/cover.png)  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~~\n\n***  \n\n# 1Berttoken embedding()position encoding  \n\n1concat+  \n\n2(768)250076810007681-1  \n\n3one-hotone-hot  \n\n4  \n\n# 2LoRALoRA  \n\n1LoRALoRA  \n\n2//LoRA  \n\n3LoRA1batch23optionalint8/int4  \n\n# 3normalizationbatchnorm/layernorm  \n\n118018normalization/  \n\n2batchnormICSinternal covariate shifti.i.d.normalization  \n\n3.How Does Batch Normalization Help Optimization?batchnormICSbatchnormICSbatchnorm  \n\n# 4Transformerpre-normpost-norm?  \n\n1.Transformerpost-normadd & normpost-norm  \n\n2.Pre-normpost-normL+1L  \n\n3.post-normpre-normpost-normpre-norm  \n\n# 5Multi-Head Attentionhidden size=DhdD=dhsbatch size=1self-attentionFloat Operations  \n\n1.QKV6  s  D^22D  \n\n2.QKh  2  d  s^2h  \n\n3.scalingh  s^2  \n\n4.softmaxh  3  s^2softmaxsexpsexps  \n\n5.reductionVh  2  d  s^2  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)  \n","slug":"cs/nlp/2024/03/-2","published":1,"updated":"2024-03-24T04:16:09.176Z","comments":1,"layout":"post","photos":[],"_id":"clv7wdj0r000l794kbztk6piz","content":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM<sub></sub></p>\n<hr>\n<h1 id=\"berttoken-embeddingposition-encoding\">1Berttoken\nembedding()position encoding</h1>\n<p>1concat+</p>\n<p>2(768)250076810007681-1</p>\n<p>3one-hotone-hot</p>\n<p>4</p>\n<h1 id=\"loralora\">2LoRALoRA</h1>\n<p>1LoRALoRA</p>\n<p>2//LoRA</p>\n<p>3LoRA1batch23optionalint8/int4</p>\n<h1 id=\"normalizationbatchnormlayernorm\">3normalizationbatchnorm/layernorm</h1>\n<p>118018normalization/</p>\n<p>2batchnormICSinternal\ncovariate\nshifti.i.d.normalization</p>\n<p>3.How Does Batch Normalization Help\nOptimization?batchnormICSbatchnormICSbatchnorm</p>\n<h1 id=\"transformerpre-normpost-norm\">4Transformerpre-normpost-norm?</h1>\n<p>1.Transformerpost-normadd &amp;\nnormpost-norm</p>\n<p>2.Pre-normpost-normL+1L</p>\n<p>3.post-normpre-normpost-normpre-norm</p>\n<h1 id=\"multi-head-attentionhidden-sizedhdddhsbatch-size1self-attentionfloat-operations\">5Multi-Head\nAttentionhidden\nsize=DhdD=dhsbatch\nsize=1self-attentionFloat\nOperations</h1>\n<p>1.QKV6  s \nD^22D</p>\n<p>2.QKh  2  d  s^2h</p>\n<p>3.scalingh  s^2</p>\n<p>4.softmaxh  3 \ns^2softmaxsexpsexps</p>\n<p>5.reductionVh  2  d  s^2</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n","length":2220,"excerpt":"","more":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM<sub></sub></p>\n<hr>\n<h1 id=\"berttoken-embeddingposition-encoding\">1Berttoken\nembedding()position encoding</h1>\n<p>1concat+</p>\n<p>2(768)250076810007681-1</p>\n<p>3one-hotone-hot</p>\n<p>4</p>\n<h1 id=\"loralora\">2LoRALoRA</h1>\n<p>1LoRALoRA</p>\n<p>2//LoRA</p>\n<p>3LoRA1batch23optionalint8/int4</p>\n<h1 id=\"normalizationbatchnormlayernorm\">3normalizationbatchnorm/layernorm</h1>\n<p>118018normalization/</p>\n<p>2batchnormICSinternal\ncovariate\nshifti.i.d.normalization</p>\n<p>3.How Does Batch Normalization Help\nOptimization?batchnormICSbatchnormICSbatchnorm</p>\n<h1 id=\"transformerpre-normpost-norm\">4Transformerpre-normpost-norm?</h1>\n<p>1.Transformerpost-normadd &amp;\nnormpost-norm</p>\n<p>2.Pre-normpost-normL+1L</p>\n<p>3.post-normpre-normpost-normpre-norm</p>\n<h1 id=\"multi-head-attentionhidden-sizedhdddhsbatch-size1self-attentionfloat-operations\">5Multi-Head\nAttentionhidden\nsize=DhdD=dhsbatch\nsize=1self-attentionFloat\nOperations</h1>\n<p>1.QKV6  s \nD^22D</p>\n<p>2.QKh  2  d  s^2h</p>\n<p>3.scalingh  s^2</p>\n<p>4.softmaxh  3 \ns^2softmaxsexpsexps</p>\n<p>5.reductionVh  2  d  s^2</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n"},{"title":"ChatGPT","abbrlink":"14e576c","date":"2023-03-17T02:36:53.000Z","_content":"\n{% asset_img 1.png page_1 %}  \n\n{% asset_img 2.png page_2 %}  \n\n{% asset_img 3.png page_3 %}  \n\n{% asset_img 4.png page_4 %}  \n\n{% asset_img 5.png page_5 %}  \n\n{% asset_img 6.png page_6 %}  \n\n{% asset_img 7.png page_7 %}  \n\n{% asset_img 8.png page_8 %}  \n\n{% asset_img 9.png page_9 %}  \n\n{% asset_img 10.png page_10 %}  \n\n{% asset_img 11.png page_11 %}  \n\n{% asset_img 12.png page_12 %}  \n\n{% asset_img 13.png page_13 %}  \n\n{% asset_img 14.png page_14 %}  \n\n{% asset_img 15.png page_15 %}  \n\n{% asset_img 16.png page_16 %}  \n\n{% asset_img 17.png page_17 %}  \n\n{% asset_img 18.png page_18 %}  \n\n{% asset_img 19.png page_19 %}  \n\n{% asset_img 20.png page_20 %}  \n\n{% asset_img 21.png page_21 %}  \n\n{% asset_img 22.png page_22 %}  \n\n{% asset_img 23.png page_23 %}  \n\n{% asset_img 24.png page_24 %}  \n\n{% asset_img 25.png page_25 %}  \n\n{% asset_img 26.png page_26 %}  \n\n{% asset_img 27.png page_27 %}  \n\n{% asset_img 28.png page_28 %}  ","source":"_posts/cs/nlp/2024/03/ChatGPT.md","raw":"---\ntitle: ChatGPT\ntags:\n  - NLP\n  - LLM\n  - ChatGPT\n  - Sparrow\n  - LaMDA\n  - GopherCite\n  - WebGPT\n  - InstructGPT\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 14e576c\ndate: 2023-03-17 10:36:53\n---\n\n{% asset_img 1.png page_1 %}  \n\n{% asset_img 2.png page_2 %}  \n\n{% asset_img 3.png page_3 %}  \n\n{% asset_img 4.png page_4 %}  \n\n{% asset_img 5.png page_5 %}  \n\n{% asset_img 6.png page_6 %}  \n\n{% asset_img 7.png page_7 %}  \n\n{% asset_img 8.png page_8 %}  \n\n{% asset_img 9.png page_9 %}  \n\n{% asset_img 10.png page_10 %}  \n\n{% asset_img 11.png page_11 %}  \n\n{% asset_img 12.png page_12 %}  \n\n{% asset_img 13.png page_13 %}  \n\n{% asset_img 14.png page_14 %}  \n\n{% asset_img 15.png page_15 %}  \n\n{% asset_img 16.png page_16 %}  \n\n{% asset_img 17.png page_17 %}  \n\n{% asset_img 18.png page_18 %}  \n\n{% asset_img 19.png page_19 %}  \n\n{% asset_img 20.png page_20 %}  \n\n{% asset_img 21.png page_21 %}  \n\n{% asset_img 22.png page_22 %}  \n\n{% asset_img 23.png page_23 %}  \n\n{% asset_img 24.png page_24 %}  \n\n{% asset_img 25.png page_25 %}  \n\n{% asset_img 26.png page_26 %}  \n\n{% asset_img 27.png page_27 %}  \n\n{% asset_img 28.png page_28 %}  ","slug":"cs/nlp/2024/03/ChatGPT","published":1,"updated":"2024-03-17T03:28:29.054Z","comments":1,"layout":"post","photos":[],"_id":"clv7wdj0s000o794k2b3kbhtn","content":"<img src=\"/14e576c/1.png\" class title=\"page_1\">\n<img src=\"/14e576c/2.png\" class title=\"page_2\">\n<img src=\"/14e576c/3.png\" class title=\"page_3\">\n<img src=\"/14e576c/4.png\" class title=\"page_4\">\n<img src=\"/14e576c/5.png\" class title=\"page_5\">\n<img src=\"/14e576c/6.png\" class title=\"page_6\">\n<img src=\"/14e576c/7.png\" class title=\"page_7\">\n<img src=\"/14e576c/8.png\" class title=\"page_8\">\n<img src=\"/14e576c/9.png\" class title=\"page_9\">\n<img src=\"/14e576c/10.png\" class title=\"page_10\">\n<img src=\"/14e576c/11.png\" class title=\"page_11\">\n<img src=\"/14e576c/12.png\" class title=\"page_12\">\n<img src=\"/14e576c/13.png\" class title=\"page_13\">\n<img src=\"/14e576c/14.png\" class title=\"page_14\">\n<img src=\"/14e576c/15.png\" class title=\"page_15\">\n<img src=\"/14e576c/16.png\" class title=\"page_16\">\n<img src=\"/14e576c/17.png\" class title=\"page_17\">\n<img src=\"/14e576c/18.png\" class title=\"page_18\">\n<img src=\"/14e576c/19.png\" class title=\"page_19\">\n<img src=\"/14e576c/20.png\" class title=\"page_20\">\n<img src=\"/14e576c/21.png\" class title=\"page_21\">\n<img src=\"/14e576c/22.png\" class title=\"page_22\">\n<img src=\"/14e576c/23.png\" class title=\"page_23\">\n<img src=\"/14e576c/24.png\" class title=\"page_24\">\n<img src=\"/14e576c/25.png\" class title=\"page_25\">\n<img src=\"/14e576c/26.png\" class title=\"page_26\">\n<img src=\"/14e576c/27.png\" class title=\"page_27\">\n<img src=\"/14e576c/28.png\" class title=\"page_28\">\n","length":0,"excerpt":"","more":"<img src=\"/14e576c/1.png\" class title=\"page_1\">\n<img src=\"/14e576c/2.png\" class title=\"page_2\">\n<img src=\"/14e576c/3.png\" class title=\"page_3\">\n<img src=\"/14e576c/4.png\" class title=\"page_4\">\n<img src=\"/14e576c/5.png\" class title=\"page_5\">\n<img src=\"/14e576c/6.png\" class title=\"page_6\">\n<img src=\"/14e576c/7.png\" class title=\"page_7\">\n<img src=\"/14e576c/8.png\" class title=\"page_8\">\n<img src=\"/14e576c/9.png\" class title=\"page_9\">\n<img src=\"/14e576c/10.png\" class title=\"page_10\">\n<img src=\"/14e576c/11.png\" class title=\"page_11\">\n<img src=\"/14e576c/12.png\" class title=\"page_12\">\n<img src=\"/14e576c/13.png\" class title=\"page_13\">\n<img src=\"/14e576c/14.png\" class title=\"page_14\">\n<img src=\"/14e576c/15.png\" class title=\"page_15\">\n<img src=\"/14e576c/16.png\" class title=\"page_16\">\n<img src=\"/14e576c/17.png\" class title=\"page_17\">\n<img src=\"/14e576c/18.png\" class title=\"page_18\">\n<img src=\"/14e576c/19.png\" class title=\"page_19\">\n<img src=\"/14e576c/20.png\" class title=\"page_20\">\n<img src=\"/14e576c/21.png\" class title=\"page_21\">\n<img src=\"/14e576c/22.png\" class title=\"page_22\">\n<img src=\"/14e576c/23.png\" class title=\"page_23\">\n<img src=\"/14e576c/24.png\" class title=\"page_24\">\n<img src=\"/14e576c/25.png\" class title=\"page_25\">\n<img src=\"/14e576c/26.png\" class title=\"page_26\">\n<img src=\"/14e576c/27.png\" class title=\"page_27\">\n<img src=\"/14e576c/28.png\" class title=\"page_28\">\n"},{"title":"MoE","abbrlink":"44e38c1b","date":"2024-03-30T01:56:05.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n202434MoEQwen1.5-MoEDBRXJamba  \n\nMoE  \n\n<center>\n\n|  |  |  |\n| :----: | :----: | :----: |\n| GPT4 | 20233 | 236George HotzGPT48220B |\n| Mistral-87B | 202312 | Mistral AI |\n| LLAMA-MoE | 202312 | github |\n| DeepSeek-MoE | 20241 | MoE |\n| abab6 |20241 | MiniMaxMoE |\n| 2.0 | 20242 |  |\n| Step-2 | 20243 |  |\n| MM1 | 20243 | MoE |\n| Grok-1 | 20243 | X |\n| Qwen1.5-MoE-A2.7B| 20243 |  |\n| DBRX | 20243 | Databricks |\n| Jamba | 20243 | AI21 |\n| Mistral-822B | 20244 | Mistral AI |\n| WizardLM-2-822B | 20244 |  |\n| 3.0 | 20244 | 4000MoE |\n\n</center>  \n\nMoE  \n\n{% asset_img xiaomi_moe.jpg MoE %}  \n\nMoE  \n\nMoEMoE  \n\n20244DeepSeek-MoEQwen1.5-MoE\n\n#   \n\nMoEMoEGoogle  \n\n##  \n\n17MoE1991[Adaptive Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)Geoffrey HintonMichael I. Jordan  \n\nMoE  \n\n>This idea was first presented by Jacobs and Hinton at the Connectionist Summer School in Pittsburg in 1988.  \n\nMoEMoE  \n\n## RNN\n\nGoogle20171[Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)MoELSTM137BLSTM\n\n## Transformer\n\n1. 20206Google[GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)MoEencoder-decodertransformer600B  \n\n2. 20211Google[Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) T5encoder-decoder1.6Tswitch transformer  \n\n3. 20224Google[ST-MoE: Designing Stable and Transferable Sparse Expert Models](https://arxiv.org/abs/2202.08906)encoder-decoderMoE269B32B  \n\n## GPT\n\n1. 2112GoogleGLaM[GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://arxiv.org/abs/2112.06905)1.2TDecoder-only  \n\n2. 20241[DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/abs/2401.06066)DeepSeek-MoE\n\n# \n\nGeoffrey HintonMichael I. Jordan[Adaptive Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)MoE  \n\n  \n\n  \n\nMoEexpert  \n\nMoEvowel discrimination task  \n\nMoEexpert networkgating networkexpertgating networkexpertexpertstochastic  \n\n{% asset_img vanilla_moe.png Vanilla MoE %}  \n\nMoEideaJacobsHinton1988lossensembleexpertexpertexpertresidual  \n\ncase $c$ $d^c$ ground truth $i$ expert $o_{i}^c$$p_{i}^c$ gating network $i$ expert $E^{c}$ \n\n$$E^{c}=\\left|\\left|d^{c}-\\sum_{i}p_{i}^{c}o_{i}^{c}\\right|\\right|^{2}$$\n\nexpert  \n\nexpertexpertexpert  \n\nexpertexpertexpertexpert  \n\n  \n\nHintonJordanlossexpert  \n\ngating networkexpert  \n\n$$E^{c}=\\langle\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2\\rangle=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}$$\n\nexpertexpertexpert  \n\nlosslocalizationcaseexpertgating networkexpert  \n\nlocalizationexpert  \n\nexpertexpertgating networkexpert error+-\n\nexpert  \n\nlosslossloss\n\n$$\\text{loss}E^{c}=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}$$  \n\n$$\\text{loss}E^c=-log\\sum_ip_i^ce^{-\\frac12\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}$$  \n\nlossloss  \n\n$$\\text{loss}\\frac{\\partial E^c}{\\partial\\mathbf{o}_i^c}=-2p_i^c(\\mathbf{d}^c-\\mathbf{o}_i^c)$$\n\n$$\\text{loss}\\frac{\\partial E^c}{\\partial\\mathbf{o}_i^c}=-\\left[\\frac{p_i^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}}{\\sum_jp_j^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_j^c\\|^2}}\\right](\\mathbf{d}^c-\\mathbf{o}_i^c)$$\n\nlossloss $i$ expertexpertexpert $i$ casegating networklosscaseexpertlosslossexpert  \n\nlocalizationexpert  \n\nBTWloss  \n\nMoE  \n\n# LSTM MoE\n\nGoogle20171\n[OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER](https://arxiv.org/abs/1701.06538)MoELSTM137BLSTM  \n\nTransformer\n\nconditional computationconditional computationMoE  \n\n  \n\n- MoEexpertbatch sizebatch size  \nbatch size3216expertexpert2batch sizebatch sizebatch size  \n-   \nNLP  \n-   \n  \n-   \nGPU  \n- GPU  \nGPUbranchingif/elseMoEgating network  \n\n  \n\n## \n\n  \n\nLSTMMoEembedding  \n\n{% asset_img rnn_moe.png LSTM MoE %}  \n\nexpertfeed-forward neural networknexpertgating networkn  \n\n$$\\begin{aligned}y=\\sum_{i=1}^nG(x)_iE_i(x)\\end{aligned}$$  \n\n$E_{i}(x)$  $i$ expert$G(x)_{i}$ gating network $i$ expert $G(x)_{i}$ 0expert  \n\nexperttwo-level hierarchical MoEgating networkgating networkexpertexpertgating networkexpert  \n\ngating network\n\nsoftmaxgating function  \n\n$$\\begin{aligned}G_\\sigma(x)=Softmax(x\\cdot W_g)\\end{aligned}$$  \n\ntopkksoftmax0expertsparsitygating function  \n\nGaussian noisenoise  \n\n$$\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}$$  \n\n$$KeepTopK(v,k)_i=\\begin{cases}v_i&\\text{if }v_i\\text{ is in the top }k\\text{ elements of }v.\\\\-\\infty&\\text{otherwise.}\\end{cases}$$  \n\n$$\\begin{aligned}H(x)_i=(x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\end{aligned}$$  \n\nnoiseReLU  \n\n{% asset_img softplus.png softplus %}  \n\n  \n\n## \n\nexpertbatch sizenexpertkbatch sizebexpertbatch sizekb/nexpertbatch size\n\n- batchbatchMoEexpertdexpertkbd/nbatch size\n- LSTMbatch size\n\n  \n\nexpertinputoutput[input_size, hidden_size][hidden_size, output_size]GPU1000hidden_sizeexpert1000  \n\n##   \n\ngatingexpertexpertexpert  \n\nhard constraintexpertsoft constraint\n\nexpertbatchbatchexpert  \n\n$$Importance(X)=\\sum_{x\\in X}G(x)$$  \n\n$G(x)$ gating networkexpert  \n\n $L_{importance}$$L_{importance}$   \n\n$$L_{importance}(X)=w_{importance}\\cdot CV(Importance(X))^2$$  \n\n$w_{importance}$ CVcoefficient of variation  \n\ncoefficient of variation $\\sigma$   $\\mu$ MoEexpertgating $L_{importance}$   \n\n $L_{importance}$  $L_{importance}$  $L_{importance}$ expertexpertbatch  \n\n $L_{load}$ expert  \n\nexpertexpertback propagation $L_{load}$ expert  \n\nMoE $H(x)$ KeepTopK  \n\n$$\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}$$  \n\n$$\\begin{aligned}H(x)_i=(x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\end{aligned}$$  \n\n $kth\\_excluding(H(x),k,i)$ $H(x)$  $i$  $k$  $P(x,i)$ noise $i$ noise $kth\\_excluding(H(x),k,i)$   \n\n$$\\begin{aligned}P(x,i)=Pr\\Big((x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\\\>kth\\_excluding(H(x),k,i)\\Big)\\end{aligned}$$  \n\n  \n\n$$\\begin{aligned}P(x,i)&=\\Phi\\Big(\\frac{(x\\cdot W_g)_i-kth\\_excluding(H(x),k,i)}{Softplus((x\\cdot W_{noise})_i)}\\Big)\\end{aligned}$$  \n\n $\\Phi$ CDF  \n\n $i$ expert  \n\n$$\\begin{aligned}Load(X)_i=\\sum_{x\\in X}P(x,i)\\end{aligned}$$  \n\nexpert  \n\n$$L_{load}(X)=w_{load}\\cdot CV(Load(X))^2$$  \n\nexpert $W_g$   $W_noise$ 0  \n\n\n\n{% asset_img rnn_moe_load_function.png  %}  \n\n## \n\n1.  & \n\nMoE4/32/256expertflat MoE256/1024/4096expert432256hierarchical MoEexpert1M4expert  \n\n\n\n{% asset_img rnn_moe_perf.png  %}  \n\n\n\nMoE\n\n2. \n\ndiminishing returns + 100B token32, 256, 10244096, 16384, 65536, 131072expertMoE137B\n\n\n\n{% asset_img rnn_moe_137b.png 137 %}  \n\n1. Expert Specialization\n\nexperttokenspecialization  \n\n{% asset_img rnn_moe_specilized.png RNN MoE  %}  \n\n# GShard\n\n2018Berttransformer20206GoogleGShard: Scaling Giant Models with Conditional Computation and Automatic ShardingMoEencoder-decodertransformerMoE  \n\nGShardMoE600B2024  \n\n{% asset_img gshard_moe_family.png GShard MoE family %}  \n\nexpertLSMT MoE -- expert  \n\nGShardMoE  \n\n  \n\nGoogleencoder-decoder transfomerGShardencoder-decoder transfomerencoderdecoderFFNMoENN/2MoE  \n\n{% asset_img gshard_model.png GShard %}  \n\ntop-2 expert  \n\nGShardOutrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layergating functionauxiliary loss function\n\nMoE\n\n$$\\begin{aligned}\n\\mathcal{G}_{s,E}& =\\mathrm{GATE}(x_s)  \\\\\n\\mathrm{FFN}_e(x_s)& =wo_e\\cdot\\text{ReLU}(wi_e\\cdot x_s)  \\\\\ny_{s}& =\\sum_{e=1}^E\\mathcal{G}_{s,e}\\cdot\\mathrm{FFN}_e(x_s) \n\\end{aligned}$$\n\n $x_s$ MoEtoken$w_i$  $w_o$ $\\mathcal{G}_{s}$ gating function\n\nGShardgating function\n\n-   \n- NtokenEexpertNEgating function  \n\ngating function  \n\n-  expert capacity  \nexperttokenexperttoken2N/EGATE()expert $c_e$ tokentokenexpert  \n-  Local group dispatching\ntokenG2N/EG  \n-  Auxiliary loss\ngating function17LSTM MoE $\\ell_{aux}=\\frac1E\\sum_{e=1}^E\\frac{c_e}S\\cdot m_e$$S$ token $\\frac{c_e}S$  $\\frac{c_e}S$  $m_e$  $m_e$  $e$ expert $S$ token\n\n{% asset_img gshard_algo_1.png GShard gating  %}  \n- Random routing\n2nd expertg2\n\n# Switch Transformer\n\n20224ChatGPTGoogleSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity2021GoogleSwitch Transformer  \n\nSwitch TransformerGShardencoder-decoderT51.6T2048expert\n\nSwitch TransformerSwitch TransformerSwitch TransformerFLOPS/token\n\nSwitch Transformer  \n\n- TransformerMoESwitch Transformer  \n- MoE to denseMoEdenseMoE99%dense  \n- 1bf16MoE2MoE3  \n- 1TMoE  \n- 101  \n- FLOPS/tokenSwitch Transformer  \n\nSwitch Transformer  \n\n##   \n\nSwitch TransformerGShardtransformerMoE  \n\n{% asset_img switch_transformer_structure.png Switch Transformer  %}  \n\nSwitch Transformergating functionrouting  \n\nkexpertSwitch Transformer1expertk=1MoESwitch layer3  \n\n- routing\n- router  \n- expertbatch sizeexpert capacity\n\n## \n\nGShardexpertexpert capacityexpertbatchtokentokenexpertoverflowtoken  \n\ncapacity factor  \n\n$$\\text{expert capacity}=\\left(\\frac{\\text{tokens per batch}}{\\text{number of experts}}\\right)\\times\\text{capacity factor}.$$  \n\ncapacity factorexperttokenoverflow\n\ncapacity factor  \n\n{% asset_img switch_transformer_diff_expert_capacity.png expert capacity %}  \n\nexpert capacity\n\ncapacity factor1overflow  \n\nexpertoverflow128expert\n\ntoken\n\n{% asset_img switch_transformer_capacity_effect.png expert capacity %}  \n\nloss\nmoeexpert capacity21.25\n31(1) Switch TransformersMoE TransformersSwitch Transformers(2) Switch TransformerMoEMoE TransformerMoEDense(3) Switch Transformers1.01.25\n\nloss $N$ expert $T$ tokenbatch $\\mathcal{B}$ \n\n$$\\begin{aligned}\\text{loss}&=\\alpha\\cdot N\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}$$  \n\n$f_{i}$  $i$ experttoken  \n\n$$\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax }p(x)=i\\}\\end{aligned}$$  \n\n$P_i$ batchtoken$i$ expert  \n\n$$\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}$$  \n\n$f$  $P$  $1/N$loss\n\n$\\alpha$ 1e-51e-11e-2\n\n## \n\n1. trick\ntrick  \n- bf16  \n-  $\\mu=0$$\\sigma=\\sqrt{s}/n$sne.g. fan-inTransformers=1.010  \n{% asset_img switch_transformer_init.png  %}  \n- switch transformerdropout  \n{% asset_img switch_transformer_dropout.png dropout %}  \ndropoutdense0.1expertdropout\n\n2. scaling\n\n\n1step basis\n\n{% asset_img switch_transformer_scaling_step.png step scaling %}  \n\n2time basis\n\nFor a fixed training duration and computational budget, should one train a dense or a sparse model?\n\n{% asset_img switch_transformer_scaling_time.png time scaling %}  \n\n3dense\n\n{% asset_img switch_transformer_scaling_dense.png dense %}  \n\n\n3. sft\n\ndenseeval\n\n{% asset_img switch_transformer_sft_result.png sft %}  \n\nmoedense\n\n1moemoedense2label25%teacher label75%ground truth\n\n{% asset_img switch_transformer_distill.png  %}  \n\n4. \n99%\n{% asset_img switch_transformer_distill_diff_model.png  %}  \n\n\n\n\n{% asset_img switch_transformer_distill_sft.png sft %}  \n\n\n\n\n\n\n\n\n# GLaM\n\nGLaM (Generalist Language Model)focus on pretrainzero-shot like gpt3without sft  \n\nThe largest version of GLaM has 1.2T parameters in total with 64 experts per MoE layertoken96.6B\n\n{% asset_img glam_compare_gpt3.png glamgpt3 %}  \n\n{% asset_img glam_compare_gpt3_2.png glamgpt3 %}  \n\nrelated workmoe\nBeyond distillation: Task-level mixture-of-experts for efficient inferenceDEEP LEARNING SCALING IS PREDICTABLE, EMPIRICALLY\nScaling to trillion parameter models with simple and efficient sparsity\n\nmoeexpertEOE^2\n\n{% asset_img glam_model.png glam %}  \n\nXLNET\n\nGLU\nIn the non-MoE Transformer feed-forward sub-layers, we replace the first linear projection and the activation function with the Gated Linear Unitwhich computes the component-wise product of two linear transformation of the input, followed by a Gaussian Error Linear Unit\n\n\nmoe\n\n{% asset_img glam_family.png glam %}  \n\ntrick\n\nWe skip weight updates for a batch if there are any NaNs or Inf s in the gradientsLingvo: a modular and scalable framework for sequence-to-sequence modeling\n\nbpnancheckpointnannan\n\n{% asset_img glam_perf.png glam %}  \n\n# ST-MoE\n\n\n\n# DeepseekMoE\n\n20241DeepseekMoEMoEDeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language ModelsDeepSeekMoE  \n\nDeepSeekMoEMoE1expert2expertexpertshared expert  \n\nexpert(specialization)  \n\nDeepSeekMoE2BMoE16B  \n\nDeepSeekMoE-2B2BDeepSeekMoE-16B7B40%  \n\nDeepSeekMoE-16Bdense  \n\n{% asset_img ds_moe_perf.png deepseek moe %}  \n\n2B16B  \n\nDeepSeekMoE-145BDeepSeek-67B  \n\n## \n\nmoeknowledge hybridity and knowledge redundancyexpert specializationnon-overlap & foucusd knowledge\n\nswitch 1expertgshard\n\nMoE816\n\n\n\n\nMoEMoE\n\ndeepseek moe2\n\n1Fine-Grained Expert Segmentation:  \n\n2Shared Expert Isolation:common knowledge\n\ndeepseekmoe 16b40gb\n\n{% asset_img ds_moe_structure.png deepseek moe  %}  \n\n\nexpert isolationideaDeepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation AI scale\n\n\n\ngating\n\n1routing collapseexpert\n\n2\n\nexpert-level balance loss\n\n$$\\begin{aligned}\n\\text{LExpBal}& =\\alpha_1\\sum_{i=1}^{N'}f_iP_i,  \\\\\nf_{i}& =\\frac{N^{\\prime}}{K^{\\prime}T}\\sum_{t=1}^T\\mathbb{1}(\\text{Token }t\\text{ selects Expert }i),  \\\\\nP_{i}& =\\frac1T\\sum_{t=1}^Ts_{i,t}, \n\\end{aligned}$$\n\ndevicelevel balance loss\n\nD{E1, E2, ..., ED}\n\n$$\\begin{aligned}\n\\mathcal{L}_{\\mathrm{DevBal}}& =\\alpha_2\\sum_{i=1}^Df_i'P_i',  \\\\\nf_i^{\\prime}& =\\frac1{|\\mathcal{E}_i|}\\sum_{j\\in\\mathcal{E}_i}f_j,  \\\\\nP_{i}^{\\prime}& =\\sum_{j\\in\\mathcal{E}_i}P_j, \n\\end{aligned}$$\n\n\n## \n\n100B\n\nHAI-LLM\n\n\ndeepseek moe 2b13bbenchmark5densehash layermoeHash layers for large sparse modelsswitch transformergshard\n\n{% asset_img ds_moe_comparison.png deepseek moe %}  \n\ndeepseek moedensegsharddeepseek moe\n\ndeepseek moe 2b, gshard  1.5\n\n{% asset_img ds_moe_upper_bound_2b.png deepseek moe upper bound %}  \n\ndeepseek moe 13b, gshard  1.2\n\n{% asset_img ds_moe_upper_bound_13b.png deepseek moe upper bound %}  \n\n\n\n\n{% asset_img ds_moe_ablation.png deepseek moe upper bound  %}  \n\n1\n\n2\n\n364expert1/2/4pileloss1.808,1.806,1.8111:32+6\n\n\n\n{% asset_img ds_moe_expert_specialization.png  %}  \n\ndeepseek moe2bgshard2b*1.5\n\ntopdeepseek moedeepseek  \n\ndeepseek moeloss\n\ngshardDeepSeekMoE\n\n{% asset_img ds_moe_less_activated_expert.png  %}  \n\n\n132bgsharddeepseek moe\n\n16b moe2T\n\nmoeloss\n\n2 + 6/64 \n\ndimension\n\n16.4b2.8b\n\nbatch size=4.5k4kbatch18Mtoken\n\n2T10.6w \n\npipeline parallelism\n\nexpert level balance loss0.001\n\ndense\n\n\n\n\n\nEfficient large scale language modeling with mixtures of expertsswitch transformersftmoe\n\nFlan-moe: Scaling instruction-finetuned language models with sparse mixture of experts.suggesting that MoE models can indeed benefit from instruction tuning\n\ndeepseek\n\n51DeepSeekMoE Chat 16B40%7BPIQAARCBBHRACEGSM8KMATHTriviaQANaturalQuestions 2DeepSeekMoE Chat 16BLLaMA2 SFT 7BHumanEvalMBPPDeepSeek Chat 7B 3MMLUCEvalCMMLUDeepSeekMoE Chat 16BDeepSeek Chat 7B5.2.1DeepSeekMoE 16BDeepSeek 7B 4DeepSeekMoE Chat 16BLLaMA2 SFT 7BDeepSeekMoE 16BDeepSeekMoE 16B40%\n\n\n{% asset_img ds_moe_sft.png sft %}  \n\n\ndeepseek moe 145b245btoken\n\n{% asset_img ds_moe_145b.png 145b %}  \n\nrelated work \n\nStablemoe: Stable routing strategy for mixture of experts\n\ntokenmoeMixture-of-experts with expert choice routing\n\nDesigning effective sparse expert models\n\n# Qwen1.5-MoE \n\n# DBRX\n\n#   \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n# Reference  \n1Adaptive Mixtures of Local Experts https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf  \n2Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer https://arxiv.org/abs/1701.06538  \n3GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding https://arxiv.org/abs/2006.16668  \n4Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity https://arxiv.org/abs/2101.03961  \n5GLaM: Efficient Scaling of Language Models with Mixture-of-Experts https://arxiv.org/abs/2112.06905  \n6ST-MoE: Designing Stable and Transferable Sparse Expert Models https://arxiv.org/abs/2202.08906  \n7DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models https://arxiv.org/abs/2401.06066  \n8Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters https://qwenlm.github.io/zh/blog/qwen-moe/  \n9Introducing DBRX: A New State-of-the-Art Open LLM https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm  \n\n\n7A Review of Sparse Expert Models in Deep Learning https://arxiv.org/abs/2209.01667  \n\n8LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training https://github.com/pjlab-sys4nlp/llama-moe  \n9MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training https://arxiv.org/abs/2403.09611  \n12Introducing Jamba https://www.ai21.com/jamba  \n13Go Wider Instead of Deeper https://arxiv.org/abs/2107.11817  \n14MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation https://arxiv.org/abs/2204.07675  \n15Learning Factored Representations in a Deep Mixture of Experts https://arxiv.org/abs/1312.4314  \n17MegaBlocks: Efficient Sparse Training with Mixture-of-Experts https://arxiv.org/abs/2211.15841  \n18Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models https://arxiv.org/abs/2305.14705  \n19","source":"_posts/cs/nlp/2024/03/MoE-.md","raw":"---\ntitle: MoE\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - MoE\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 44e38c1b\ndate: 2024-03-30 09:56:05\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n202434MoEQwen1.5-MoEDBRXJamba  \n\nMoE  \n\n<center>\n\n|  |  |  |\n| :----: | :----: | :----: |\n| GPT4 | 20233 | 236George HotzGPT48220B |\n| Mistral-87B | 202312 | Mistral AI |\n| LLAMA-MoE | 202312 | github |\n| DeepSeek-MoE | 20241 | MoE |\n| abab6 |20241 | MiniMaxMoE |\n| 2.0 | 20242 |  |\n| Step-2 | 20243 |  |\n| MM1 | 20243 | MoE |\n| Grok-1 | 20243 | X |\n| Qwen1.5-MoE-A2.7B| 20243 |  |\n| DBRX | 20243 | Databricks |\n| Jamba | 20243 | AI21 |\n| Mistral-822B | 20244 | Mistral AI |\n| WizardLM-2-822B | 20244 |  |\n| 3.0 | 20244 | 4000MoE |\n\n</center>  \n\nMoE  \n\n{% asset_img xiaomi_moe.jpg MoE %}  \n\nMoE  \n\nMoEMoE  \n\n20244DeepSeek-MoEQwen1.5-MoE\n\n#   \n\nMoEMoEGoogle  \n\n##  \n\n17MoE1991[Adaptive Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)Geoffrey HintonMichael I. Jordan  \n\nMoE  \n\n>This idea was first presented by Jacobs and Hinton at the Connectionist Summer School in Pittsburg in 1988.  \n\nMoEMoE  \n\n## RNN\n\nGoogle20171[Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)MoELSTM137BLSTM\n\n## Transformer\n\n1. 20206Google[GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)MoEencoder-decodertransformer600B  \n\n2. 20211Google[Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) T5encoder-decoder1.6Tswitch transformer  \n\n3. 20224Google[ST-MoE: Designing Stable and Transferable Sparse Expert Models](https://arxiv.org/abs/2202.08906)encoder-decoderMoE269B32B  \n\n## GPT\n\n1. 2112GoogleGLaM[GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://arxiv.org/abs/2112.06905)1.2TDecoder-only  \n\n2. 20241[DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/abs/2401.06066)DeepSeek-MoE\n\n# \n\nGeoffrey HintonMichael I. Jordan[Adaptive Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)MoE  \n\n  \n\n  \n\nMoEexpert  \n\nMoEvowel discrimination task  \n\nMoEexpert networkgating networkexpertgating networkexpertexpertstochastic  \n\n{% asset_img vanilla_moe.png Vanilla MoE %}  \n\nMoEideaJacobsHinton1988lossensembleexpertexpertexpertresidual  \n\ncase $c$ $d^c$ ground truth $i$ expert $o_{i}^c$$p_{i}^c$ gating network $i$ expert $E^{c}$ \n\n$$E^{c}=\\left|\\left|d^{c}-\\sum_{i}p_{i}^{c}o_{i}^{c}\\right|\\right|^{2}$$\n\nexpert  \n\nexpertexpertexpert  \n\nexpertexpertexpertexpert  \n\n  \n\nHintonJordanlossexpert  \n\ngating networkexpert  \n\n$$E^{c}=\\langle\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2\\rangle=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}$$\n\nexpertexpertexpert  \n\nlosslocalizationcaseexpertgating networkexpert  \n\nlocalizationexpert  \n\nexpertexpertgating networkexpert error+-\n\nexpert  \n\nlosslossloss\n\n$$\\text{loss}E^{c}=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}$$  \n\n$$\\text{loss}E^c=-log\\sum_ip_i^ce^{-\\frac12\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}$$  \n\nlossloss  \n\n$$\\text{loss}\\frac{\\partial E^c}{\\partial\\mathbf{o}_i^c}=-2p_i^c(\\mathbf{d}^c-\\mathbf{o}_i^c)$$\n\n$$\\text{loss}\\frac{\\partial E^c}{\\partial\\mathbf{o}_i^c}=-\\left[\\frac{p_i^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}}{\\sum_jp_j^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_j^c\\|^2}}\\right](\\mathbf{d}^c-\\mathbf{o}_i^c)$$\n\nlossloss $i$ expertexpertexpert $i$ casegating networklosscaseexpertlosslossexpert  \n\nlocalizationexpert  \n\nBTWloss  \n\nMoE  \n\n# LSTM MoE\n\nGoogle20171\n[OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER](https://arxiv.org/abs/1701.06538)MoELSTM137BLSTM  \n\nTransformer\n\nconditional computationconditional computationMoE  \n\n  \n\n- MoEexpertbatch sizebatch size  \nbatch size3216expertexpert2batch sizebatch sizebatch size  \n-   \nNLP  \n-   \n  \n-   \nGPU  \n- GPU  \nGPUbranchingif/elseMoEgating network  \n\n  \n\n## \n\n  \n\nLSTMMoEembedding  \n\n{% asset_img rnn_moe.png LSTM MoE %}  \n\nexpertfeed-forward neural networknexpertgating networkn  \n\n$$\\begin{aligned}y=\\sum_{i=1}^nG(x)_iE_i(x)\\end{aligned}$$  \n\n$E_{i}(x)$  $i$ expert$G(x)_{i}$ gating network $i$ expert $G(x)_{i}$ 0expert  \n\nexperttwo-level hierarchical MoEgating networkgating networkexpertexpertgating networkexpert  \n\ngating network\n\nsoftmaxgating function  \n\n$$\\begin{aligned}G_\\sigma(x)=Softmax(x\\cdot W_g)\\end{aligned}$$  \n\ntopkksoftmax0expertsparsitygating function  \n\nGaussian noisenoise  \n\n$$\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}$$  \n\n$$KeepTopK(v,k)_i=\\begin{cases}v_i&\\text{if }v_i\\text{ is in the top }k\\text{ elements of }v.\\\\-\\infty&\\text{otherwise.}\\end{cases}$$  \n\n$$\\begin{aligned}H(x)_i=(x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\end{aligned}$$  \n\nnoiseReLU  \n\n{% asset_img softplus.png softplus %}  \n\n  \n\n## \n\nexpertbatch sizenexpertkbatch sizebexpertbatch sizekb/nexpertbatch size\n\n- batchbatchMoEexpertdexpertkbd/nbatch size\n- LSTMbatch size\n\n  \n\nexpertinputoutput[input_size, hidden_size][hidden_size, output_size]GPU1000hidden_sizeexpert1000  \n\n##   \n\ngatingexpertexpertexpert  \n\nhard constraintexpertsoft constraint\n\nexpertbatchbatchexpert  \n\n$$Importance(X)=\\sum_{x\\in X}G(x)$$  \n\n$G(x)$ gating networkexpert  \n\n $L_{importance}$$L_{importance}$   \n\n$$L_{importance}(X)=w_{importance}\\cdot CV(Importance(X))^2$$  \n\n$w_{importance}$ CVcoefficient of variation  \n\ncoefficient of variation $\\sigma$   $\\mu$ MoEexpertgating $L_{importance}$   \n\n $L_{importance}$  $L_{importance}$  $L_{importance}$ expertexpertbatch  \n\n $L_{load}$ expert  \n\nexpertexpertback propagation $L_{load}$ expert  \n\nMoE $H(x)$ KeepTopK  \n\n$$\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}$$  \n\n$$\\begin{aligned}H(x)_i=(x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\end{aligned}$$  \n\n $kth\\_excluding(H(x),k,i)$ $H(x)$  $i$  $k$  $P(x,i)$ noise $i$ noise $kth\\_excluding(H(x),k,i)$   \n\n$$\\begin{aligned}P(x,i)=Pr\\Big((x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\\\>kth\\_excluding(H(x),k,i)\\Big)\\end{aligned}$$  \n\n  \n\n$$\\begin{aligned}P(x,i)&=\\Phi\\Big(\\frac{(x\\cdot W_g)_i-kth\\_excluding(H(x),k,i)}{Softplus((x\\cdot W_{noise})_i)}\\Big)\\end{aligned}$$  \n\n $\\Phi$ CDF  \n\n $i$ expert  \n\n$$\\begin{aligned}Load(X)_i=\\sum_{x\\in X}P(x,i)\\end{aligned}$$  \n\nexpert  \n\n$$L_{load}(X)=w_{load}\\cdot CV(Load(X))^2$$  \n\nexpert $W_g$   $W_noise$ 0  \n\n\n\n{% asset_img rnn_moe_load_function.png  %}  \n\n## \n\n1.  & \n\nMoE4/32/256expertflat MoE256/1024/4096expert432256hierarchical MoEexpert1M4expert  \n\n\n\n{% asset_img rnn_moe_perf.png  %}  \n\n\n\nMoE\n\n2. \n\ndiminishing returns + 100B token32, 256, 10244096, 16384, 65536, 131072expertMoE137B\n\n\n\n{% asset_img rnn_moe_137b.png 137 %}  \n\n1. Expert Specialization\n\nexperttokenspecialization  \n\n{% asset_img rnn_moe_specilized.png RNN MoE  %}  \n\n# GShard\n\n2018Berttransformer20206GoogleGShard: Scaling Giant Models with Conditional Computation and Automatic ShardingMoEencoder-decodertransformerMoE  \n\nGShardMoE600B2024  \n\n{% asset_img gshard_moe_family.png GShard MoE family %}  \n\nexpertLSMT MoE -- expert  \n\nGShardMoE  \n\n  \n\nGoogleencoder-decoder transfomerGShardencoder-decoder transfomerencoderdecoderFFNMoENN/2MoE  \n\n{% asset_img gshard_model.png GShard %}  \n\ntop-2 expert  \n\nGShardOutrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layergating functionauxiliary loss function\n\nMoE\n\n$$\\begin{aligned}\n\\mathcal{G}_{s,E}& =\\mathrm{GATE}(x_s)  \\\\\n\\mathrm{FFN}_e(x_s)& =wo_e\\cdot\\text{ReLU}(wi_e\\cdot x_s)  \\\\\ny_{s}& =\\sum_{e=1}^E\\mathcal{G}_{s,e}\\cdot\\mathrm{FFN}_e(x_s) \n\\end{aligned}$$\n\n $x_s$ MoEtoken$w_i$  $w_o$ $\\mathcal{G}_{s}$ gating function\n\nGShardgating function\n\n-   \n- NtokenEexpertNEgating function  \n\ngating function  \n\n-  expert capacity  \nexperttokenexperttoken2N/EGATE()expert $c_e$ tokentokenexpert  \n-  Local group dispatching\ntokenG2N/EG  \n-  Auxiliary loss\ngating function17LSTM MoE $\\ell_{aux}=\\frac1E\\sum_{e=1}^E\\frac{c_e}S\\cdot m_e$$S$ token $\\frac{c_e}S$  $\\frac{c_e}S$  $m_e$  $m_e$  $e$ expert $S$ token\n\n{% asset_img gshard_algo_1.png GShard gating  %}  \n- Random routing\n2nd expertg2\n\n# Switch Transformer\n\n20224ChatGPTGoogleSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity2021GoogleSwitch Transformer  \n\nSwitch TransformerGShardencoder-decoderT51.6T2048expert\n\nSwitch TransformerSwitch TransformerSwitch TransformerFLOPS/token\n\nSwitch Transformer  \n\n- TransformerMoESwitch Transformer  \n- MoE to denseMoEdenseMoE99%dense  \n- 1bf16MoE2MoE3  \n- 1TMoE  \n- 101  \n- FLOPS/tokenSwitch Transformer  \n\nSwitch Transformer  \n\n##   \n\nSwitch TransformerGShardtransformerMoE  \n\n{% asset_img switch_transformer_structure.png Switch Transformer  %}  \n\nSwitch Transformergating functionrouting  \n\nkexpertSwitch Transformer1expertk=1MoESwitch layer3  \n\n- routing\n- router  \n- expertbatch sizeexpert capacity\n\n## \n\nGShardexpertexpert capacityexpertbatchtokentokenexpertoverflowtoken  \n\ncapacity factor  \n\n$$\\text{expert capacity}=\\left(\\frac{\\text{tokens per batch}}{\\text{number of experts}}\\right)\\times\\text{capacity factor}.$$  \n\ncapacity factorexperttokenoverflow\n\ncapacity factor  \n\n{% asset_img switch_transformer_diff_expert_capacity.png expert capacity %}  \n\nexpert capacity\n\ncapacity factor1overflow  \n\nexpertoverflow128expert\n\ntoken\n\n{% asset_img switch_transformer_capacity_effect.png expert capacity %}  \n\nloss\nmoeexpert capacity21.25\n31(1) Switch TransformersMoE TransformersSwitch Transformers(2) Switch TransformerMoEMoE TransformerMoEDense(3) Switch Transformers1.01.25\n\nloss $N$ expert $T$ tokenbatch $\\mathcal{B}$ \n\n$$\\begin{aligned}\\text{loss}&=\\alpha\\cdot N\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}$$  \n\n$f_{i}$  $i$ experttoken  \n\n$$\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax }p(x)=i\\}\\end{aligned}$$  \n\n$P_i$ batchtoken$i$ expert  \n\n$$\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}$$  \n\n$f$  $P$  $1/N$loss\n\n$\\alpha$ 1e-51e-11e-2\n\n## \n\n1. trick\ntrick  \n- bf16  \n-  $\\mu=0$$\\sigma=\\sqrt{s}/n$sne.g. fan-inTransformers=1.010  \n{% asset_img switch_transformer_init.png  %}  \n- switch transformerdropout  \n{% asset_img switch_transformer_dropout.png dropout %}  \ndropoutdense0.1expertdropout\n\n2. scaling\n\n\n1step basis\n\n{% asset_img switch_transformer_scaling_step.png step scaling %}  \n\n2time basis\n\nFor a fixed training duration and computational budget, should one train a dense or a sparse model?\n\n{% asset_img switch_transformer_scaling_time.png time scaling %}  \n\n3dense\n\n{% asset_img switch_transformer_scaling_dense.png dense %}  \n\n\n3. sft\n\ndenseeval\n\n{% asset_img switch_transformer_sft_result.png sft %}  \n\nmoedense\n\n1moemoedense2label25%teacher label75%ground truth\n\n{% asset_img switch_transformer_distill.png  %}  \n\n4. \n99%\n{% asset_img switch_transformer_distill_diff_model.png  %}  \n\n\n\n\n{% asset_img switch_transformer_distill_sft.png sft %}  \n\n\n\n\n\n\n\n\n# GLaM\n\nGLaM (Generalist Language Model)focus on pretrainzero-shot like gpt3without sft  \n\nThe largest version of GLaM has 1.2T parameters in total with 64 experts per MoE layertoken96.6B\n\n{% asset_img glam_compare_gpt3.png glamgpt3 %}  \n\n{% asset_img glam_compare_gpt3_2.png glamgpt3 %}  \n\nrelated workmoe\nBeyond distillation: Task-level mixture-of-experts for efficient inferenceDEEP LEARNING SCALING IS PREDICTABLE, EMPIRICALLY\nScaling to trillion parameter models with simple and efficient sparsity\n\nmoeexpertEOE^2\n\n{% asset_img glam_model.png glam %}  \n\nXLNET\n\nGLU\nIn the non-MoE Transformer feed-forward sub-layers, we replace the first linear projection and the activation function with the Gated Linear Unitwhich computes the component-wise product of two linear transformation of the input, followed by a Gaussian Error Linear Unit\n\n\nmoe\n\n{% asset_img glam_family.png glam %}  \n\ntrick\n\nWe skip weight updates for a batch if there are any NaNs or Inf s in the gradientsLingvo: a modular and scalable framework for sequence-to-sequence modeling\n\nbpnancheckpointnannan\n\n{% asset_img glam_perf.png glam %}  \n\n# ST-MoE\n\n\n\n# DeepseekMoE\n\n20241DeepseekMoEMoEDeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language ModelsDeepSeekMoE  \n\nDeepSeekMoEMoE1expert2expertexpertshared expert  \n\nexpert(specialization)  \n\nDeepSeekMoE2BMoE16B  \n\nDeepSeekMoE-2B2BDeepSeekMoE-16B7B40%  \n\nDeepSeekMoE-16Bdense  \n\n{% asset_img ds_moe_perf.png deepseek moe %}  \n\n2B16B  \n\nDeepSeekMoE-145BDeepSeek-67B  \n\n## \n\nmoeknowledge hybridity and knowledge redundancyexpert specializationnon-overlap & foucusd knowledge\n\nswitch 1expertgshard\n\nMoE816\n\n\n\n\nMoEMoE\n\ndeepseek moe2\n\n1Fine-Grained Expert Segmentation:  \n\n2Shared Expert Isolation:common knowledge\n\ndeepseekmoe 16b40gb\n\n{% asset_img ds_moe_structure.png deepseek moe  %}  \n\n\nexpert isolationideaDeepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation AI scale\n\n\n\ngating\n\n1routing collapseexpert\n\n2\n\nexpert-level balance loss\n\n$$\\begin{aligned}\n\\text{LExpBal}& =\\alpha_1\\sum_{i=1}^{N'}f_iP_i,  \\\\\nf_{i}& =\\frac{N^{\\prime}}{K^{\\prime}T}\\sum_{t=1}^T\\mathbb{1}(\\text{Token }t\\text{ selects Expert }i),  \\\\\nP_{i}& =\\frac1T\\sum_{t=1}^Ts_{i,t}, \n\\end{aligned}$$\n\ndevicelevel balance loss\n\nD{E1, E2, ..., ED}\n\n$$\\begin{aligned}\n\\mathcal{L}_{\\mathrm{DevBal}}& =\\alpha_2\\sum_{i=1}^Df_i'P_i',  \\\\\nf_i^{\\prime}& =\\frac1{|\\mathcal{E}_i|}\\sum_{j\\in\\mathcal{E}_i}f_j,  \\\\\nP_{i}^{\\prime}& =\\sum_{j\\in\\mathcal{E}_i}P_j, \n\\end{aligned}$$\n\n\n## \n\n100B\n\nHAI-LLM\n\n\ndeepseek moe 2b13bbenchmark5densehash layermoeHash layers for large sparse modelsswitch transformergshard\n\n{% asset_img ds_moe_comparison.png deepseek moe %}  \n\ndeepseek moedensegsharddeepseek moe\n\ndeepseek moe 2b, gshard  1.5\n\n{% asset_img ds_moe_upper_bound_2b.png deepseek moe upper bound %}  \n\ndeepseek moe 13b, gshard  1.2\n\n{% asset_img ds_moe_upper_bound_13b.png deepseek moe upper bound %}  \n\n\n\n\n{% asset_img ds_moe_ablation.png deepseek moe upper bound  %}  \n\n1\n\n2\n\n364expert1/2/4pileloss1.808,1.806,1.8111:32+6\n\n\n\n{% asset_img ds_moe_expert_specialization.png  %}  \n\ndeepseek moe2bgshard2b*1.5\n\ntopdeepseek moedeepseek  \n\ndeepseek moeloss\n\ngshardDeepSeekMoE\n\n{% asset_img ds_moe_less_activated_expert.png  %}  \n\n\n132bgsharddeepseek moe\n\n16b moe2T\n\nmoeloss\n\n2 + 6/64 \n\ndimension\n\n16.4b2.8b\n\nbatch size=4.5k4kbatch18Mtoken\n\n2T10.6w \n\npipeline parallelism\n\nexpert level balance loss0.001\n\ndense\n\n\n\n\n\nEfficient large scale language modeling with mixtures of expertsswitch transformersftmoe\n\nFlan-moe: Scaling instruction-finetuned language models with sparse mixture of experts.suggesting that MoE models can indeed benefit from instruction tuning\n\ndeepseek\n\n51DeepSeekMoE Chat 16B40%7BPIQAARCBBHRACEGSM8KMATHTriviaQANaturalQuestions 2DeepSeekMoE Chat 16BLLaMA2 SFT 7BHumanEvalMBPPDeepSeek Chat 7B 3MMLUCEvalCMMLUDeepSeekMoE Chat 16BDeepSeek Chat 7B5.2.1DeepSeekMoE 16BDeepSeek 7B 4DeepSeekMoE Chat 16BLLaMA2 SFT 7BDeepSeekMoE 16BDeepSeekMoE 16B40%\n\n\n{% asset_img ds_moe_sft.png sft %}  \n\n\ndeepseek moe 145b245btoken\n\n{% asset_img ds_moe_145b.png 145b %}  \n\nrelated work \n\nStablemoe: Stable routing strategy for mixture of experts\n\ntokenmoeMixture-of-experts with expert choice routing\n\nDesigning effective sparse expert models\n\n# Qwen1.5-MoE \n\n# DBRX\n\n#   \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n# Reference  \n1Adaptive Mixtures of Local Experts https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf  \n2Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer https://arxiv.org/abs/1701.06538  \n3GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding https://arxiv.org/abs/2006.16668  \n4Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity https://arxiv.org/abs/2101.03961  \n5GLaM: Efficient Scaling of Language Models with Mixture-of-Experts https://arxiv.org/abs/2112.06905  \n6ST-MoE: Designing Stable and Transferable Sparse Expert Models https://arxiv.org/abs/2202.08906  \n7DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models https://arxiv.org/abs/2401.06066  \n8Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters https://qwenlm.github.io/zh/blog/qwen-moe/  \n9Introducing DBRX: A New State-of-the-Art Open LLM https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm  \n\n\n7A Review of Sparse Expert Models in Deep Learning https://arxiv.org/abs/2209.01667  \n\n8LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training https://github.com/pjlab-sys4nlp/llama-moe  \n9MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training https://arxiv.org/abs/2403.09611  \n12Introducing Jamba https://www.ai21.com/jamba  \n13Go Wider Instead of Deeper https://arxiv.org/abs/2107.11817  \n14MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation https://arxiv.org/abs/2204.07675  \n15Learning Factored Representations in a Deep Mixture of Experts https://arxiv.org/abs/1312.4314  \n17MegaBlocks: Efficient Sparse Training with Mixture-of-Experts https://arxiv.org/abs/2211.15841  \n18Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models https://arxiv.org/abs/2305.14705  \n19","slug":"cs/nlp/2024/03/MoE-","published":1,"updated":"2024-04-20T08:56:17.731Z","comments":1,"layout":"post","photos":[],"_id":"clv7wdj11004s794k528u0tt7","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>202434MoEQwen1.5-MoEDBRXJamba</p>\n<p>MoE</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">GPT4</td>\n<td style=\"text-align: center;\">20233</td>\n<td style=\"text-align: center;\">236George\nHotzGPT48220B</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Mistral-87B</td>\n<td style=\"text-align: center;\">202312</td>\n<td style=\"text-align: center;\">Mistral AI</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">LLAMA-MoE</td>\n<td style=\"text-align: center;\">202312</td>\n<td style=\"text-align: center;\">github</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">DeepSeek-MoE</td>\n<td style=\"text-align: center;\">20241</td>\n<td style=\"text-align: center;\">MoE</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">abab6</td>\n<td style=\"text-align: center;\">20241</td>\n<td style=\"text-align: center;\">MiniMaxMoE</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">2.0</td>\n<td style=\"text-align: center;\">20242</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Step-2</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">MM1</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">MoE</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Grok-1</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">X</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Qwen1.5-MoE-A2.7B</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">DBRX</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">Databricks</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Jamba</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">AI21</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Mistral-822B</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\">Mistral AI</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">WizardLM-2-822B</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">3.0</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\">4000MoE</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>MoE</p>\n<img src=\"/44e38c1b/xiaomi_moe.jpg\" class title=\"MoE\">\n<p>MoE</p>\n<p>MoEMoE</p>\n<p>20244DeepSeek-MoEQwen1.5-MoE</p>\n<h1 id=\"\"></h1>\n<p>MoEMoEGoogle</p>\n<h2 id=\"\"></h2>\n<p>17MoE1991<a href=\"https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf\">Adaptive\nMixtures of Local Experts</a>Geoffrey HintonMichael I.\nJordan</p>\n<p>MoE</p>\n<blockquote>\n<p>This idea was first presented by Jacobs and Hinton at the\nConnectionist Summer School in Pittsburg in 1988.</p>\n</blockquote>\n<p>MoEMoE</p>\n<h2 id=\"rnn\">RNN</h2>\n<p>Google20171<a href=\"https://arxiv.org/abs/1701.06538\">Outrageously Large Neural\nNetworks: The Sparsely-Gated Mixture-of-Experts\nLayer</a>MoELSTM137BLSTM</p>\n<h2 id=\"transformer\">Transformer</h2>\n<ol type=\"1\">\n<li><p>20206Google<a href=\"https://arxiv.org/abs/2006.16668\">GShard: Scaling Giant Models\nwith Conditional Computation and Automatic\nSharding</a>MoEencoder-decodertransformer600B</p></li>\n<li><p>20211Google<a href=\"https://arxiv.org/abs/2101.03961\">Switch Transformers: Scaling\nto Trillion Parameter Models with Simple and Efficient Sparsity</a>\nT5encoder-decoder1.6Tswitch\ntransformer</p></li>\n<li><p>20224Google<a href=\"https://arxiv.org/abs/2202.08906\">ST-MoE: Designing Stable and\nTransferable Sparse Expert\nModels</a>encoder-decoderMoE269B32B</p></li>\n</ol>\n<h2 id=\"gpt\">GPT</h2>\n<ol type=\"1\">\n<li><p>2112GoogleGLaM<a href=\"https://arxiv.org/abs/2112.06905\">GLaM: Efficient Scaling of\nLanguage Models with\nMixture-of-Experts</a>1.2TDecoder-only</p></li>\n<li><p>20241<a href=\"https://arxiv.org/abs/2401.06066\">DeepSeekMoE: Towards Ultimate\nExpert Specialization in Mixture-of-Experts Language\nModels</a>DeepSeek-MoE</p></li>\n</ol>\n<h1 id=\"\"></h1>\n<p>Geoffrey HintonMichael I. Jordan<a href=\"https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf\">Adaptive\nMixtures of Local Experts</a>MoE</p>\n<p></p>\n<p></p>\n<p>MoEexpert</p>\n<p>MoEvowel discrimination\ntask</p>\n<p>MoEexpert networkgating\nnetworkexpertgating\nnetworkexpertexpertstochastic</p>\n<img src=\"/44e38c1b/vanilla_moe.png\" class title=\"Vanilla MoE\">\n<p>MoEideaJacobsHinton1988lossensembleexpertexpertexpertresidual</p>\n<p>case <span class=\"math inline\">\\(c\\)</span>\n<span class=\"math inline\">\\(d^c\\)</span> ground truth <span class=\"math inline\">\\(i\\)</span> expert <span class=\"math inline\">\\(o_{i}^c\\)</span><span class=\"math inline\">\\(p_{i}^c\\)</span> gating network <span class=\"math inline\">\\(i\\)</span>\nexpert <span class=\"math inline\">\\(E^{c}\\)</span> </p>\n<p><span class=\"math display\">\\[E^{c}=\\left|\\left|d^{c}-\\sum_{i}p_{i}^{c}o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p>expert</p>\n<p>expertexpertexpert</p>\n<p>expertexpertexpertexpert</p>\n<p></p>\n<p>HintonJordanlossexpert</p>\n<p>gating networkexpert</p>\n<p><span class=\"math display\">\\[E^{c}=\\langle\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2\\rangle=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p>expertexpertexpert</p>\n<p>losslocalizationcaseexpertgating\nnetworkexpert</p>\n<p>localizationexpert</p>\n<p>expertexpertgating\nnetworkexpert\nerror+-</p>\n<p>expert</p>\n<p>losslossloss</p>\n<p><span class=\"math display\">\\[\\text{loss}E^{c}=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p><span class=\"math display\">\\[\\text{loss}E^c=-log\\sum_ip_i^ce^{-\\frac12\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}\\]</span></p>\n<p>lossloss</p>\n<p><span class=\"math display\">\\[\\text{loss}\\frac{\\partial\nE^c}{\\partial\\mathbf{o}_i^c}=-2p_i^c(\\mathbf{d}^c-\\mathbf{o}_i^c)\\]</span></p>\n<p><span class=\"math display\">\\[\\text{loss}\\frac{\\partial\nE^c}{\\partial\\mathbf{o}_i^c}=-\\left[\\frac{p_i^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}}{\\sum_jp_j^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_j^c\\|^2}}\\right](\\mathbf{d}^c-\\mathbf{o}_i^c)\\]</span></p>\n<p>lossloss <span class=\"math inline\">\\(i\\)</span>\nexpertexpertexpert <span class=\"math inline\">\\(i\\)</span>\ncasegating\nnetworklosscaseexpertlosslossexpert</p>\n<p>localizationexpert</p>\n<p>BTWloss</p>\n<p>MoE</p>\n<h1 id=\"lstm-moe\">LSTM MoE</h1>\n<p>Google20171 <a href=\"https://arxiv.org/abs/1701.06538\">OUTRAGEOUSLY LARGE NEURAL\nNETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS\nLAYER</a>MoELSTM137BLSTM</p>\n<p>Transformer</p>\n<p>conditional\ncomputationconditional\ncomputationMoE</p>\n<p></p>\n<ul>\n<li>MoEexpertbatch sizebatch\nsize<br>\nbatch\nsize3216expertexpert2batch\nsizebatch\nsizebatch\nsize<br>\n</li>\n<li><br>\nNLP<br>\n</li>\n<li><br>\n<br>\n</li>\n<li><br>\nGPU<br>\n</li>\n<li>GPU<br>\nGPUbranchingif/elseMoEgating\nnetwork</li>\n</ul>\n<p></p>\n<h2 id=\"\"></h2>\n<p></p>\n<p>LSTMMoEembedding</p>\n<img src=\"/44e38c1b/rnn_moe.png\" class title=\"LSTM MoE\">\n<p>expertfeed-forward neural\nnetworknexpertgating networkn</p>\n<p><span class=\"math display\">\\[\\begin{aligned}y=\\sum_{i=1}^nG(x)_iE_i(x)\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(E_{i}(x)\\)</span>  <span class=\"math inline\">\\(i\\)</span> expert<span class=\"math inline\">\\(G(x)_{i}\\)</span> gating network <span class=\"math inline\">\\(i\\)</span> expert <span class=\"math inline\">\\(G(x)_{i}\\)</span>\n0expert</p>\n<p>experttwo-level hierarchical\nMoEgating networkgating\nnetworkexpertexpertgating\nnetworkexpert</p>\n<p>gating network</p>\n<p>softmaxgating\nfunction</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G_\\sigma(x)=Softmax(x\\cdot\nW_g)\\end{aligned}\\]</span></p>\n<p>topkksoftmax0expertsparsitygating\nfunction</p>\n<p>Gaussian\nnoisenoise</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[KeepTopK(v,k)_i=\\begin{cases}v_i&amp;\\text{if\n}v_i\\text{ is in the top }k\\text{ elements of\n}v.\\\\-\\infty&amp;\\text{otherwise.}\\end{cases}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}H(x)_i=(x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\end{aligned}\\]</span></p>\n<p>noiseReLU</p>\n<img src=\"/44e38c1b/softplus.png\" class title=\"softplus\">\n<p></p>\n<h2 id=\"\"></h2>\n<p>expertbatch\nsizenexpertkbatch\nsizebexpertbatch\nsizekb/nexpertbatch size</p>\n<ul>\n<li>batchbatchMoEexpertdexpertkbd/nbatch\nsize</li>\n<li>LSTMbatch size</li>\n</ul>\n<p></p>\n<p>expertinputoutput[input_size,\nhidden_size][hidden_size,\noutput_size]GPU1000hidden_sizeexpert1000</p>\n<h2 id=\"\"></h2>\n<p>gatingexpertexpertexpert</p>\n<p>hard\nconstraintexpertsoft\nconstraint</p>\n<p>expertbatchbatchexpert</p>\n<p><span class=\"math display\">\\[Importance(X)=\\sum_{x\\in\nX}G(x)\\]</span></p>\n<p><span class=\"math inline\">\\(G(x)\\)</span> gating\nnetworkexpert</p>\n<p> <span class=\"math inline\">\\(L_{importance}\\)</span><span class=\"math inline\">\\(L_{importance}\\)</span> </p>\n<p><span class=\"math display\">\\[L_{importance}(X)=w_{importance}\\cdot\nCV(Importance(X))^2\\]</span></p>\n<p><span class=\"math inline\">\\(w_{importance}\\)</span>\nCVcoefficient of variation</p>\n<p>coefficient of\nvariation\n<span class=\"math inline\">\\(\\sigma\\)</span>   <span class=\"math inline\">\\(\\mu\\)</span>\nMoEexpertgating\n<span class=\"math inline\">\\(L_{importance}\\)</span> </p>\n<p> <span class=\"math inline\">\\(L_{importance}\\)</span>  <span class=\"math inline\">\\(L_{importance}\\)</span>\n <span class=\"math inline\">\\(L_{importance}\\)</span>\nexpertexpertbatch</p>\n<p> <span class=\"math inline\">\\(L_{load}\\)</span>\nexpert</p>\n<p>expertexpertback\npropagation <span class=\"math inline\">\\(L_{load}\\)</span>\nexpert</p>\n<p>MoE <span class=\"math inline\">\\(H(x)\\)</span>\nKeepTopK</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}H(x)_i=(x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(kth\\_excluding(H(x),k,i)\\)</span>\n<span class=\"math inline\">\\(H(x)\\)</span>  <span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(P(x,i)\\)</span>\nnoise <span class=\"math inline\">\\(i\\)</span> noise <span class=\"math inline\">\\(kth\\_excluding(H(x),k,i)\\)</span> </p>\n<p><span class=\"math display\">\\[\\begin{aligned}P(x,i)=Pr\\Big((x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\\\&gt;kth\\_excluding(H(x),k,i)\\Big)\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}P(x,i)&amp;=\\Phi\\Big(\\frac{(x\\cdot\nW_g)_i-kth\\_excluding(H(x),k,i)}{Softplus((x\\cdot\nW_{noise})_i)}\\Big)\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\Phi\\)</span>\nCDF</p>\n<p> <span class=\"math inline\">\\(i\\)</span>\nexpert</p>\n<p><span class=\"math display\">\\[\\begin{aligned}Load(X)_i=\\sum_{x\\in\nX}P(x,i)\\end{aligned}\\]</span></p>\n<p>expert</p>\n<p><span class=\"math display\">\\[L_{load}(X)=w_{load}\\cdot\nCV(Load(X))^2\\]</span></p>\n<p>expert\n<span class=\"math inline\">\\(W_g\\)</span>  <span class=\"math inline\">\\(W_noise\\)</span>\n0</p>\n<p></p>\n<img src=\"/44e38c1b/rnn_moe_load_function.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<ol type=\"1\">\n<li> &amp; </li>\n</ol>\n<p>MoE4/32/256expertflat\nMoE256/1024/4096expert432256hierarchical\nMoEexpert1M4expert</p>\n<p></p>\n<img src=\"/44e38c1b/rnn_moe_perf.png\" class title=\"\">\n<p></p>\n<p>MoE</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p>diminishing\nreturns + 100B\ntoken32, 256, 10244096, 16384, 65536,\n131072expertMoE137B</p>\n<p></p>\n<img src=\"/44e38c1b/rnn_moe_137b.png\" class title=\"137\">\n<ol type=\"1\">\n<li>Expert Specialization</li>\n</ol>\n<p>experttokenspecialization</p>\n<img src=\"/44e38c1b/rnn_moe_specilized.png\" class title=\"RNN MoE \">\n<h1 id=\"gshard\">GShard</h1>\n<p>2018Berttransformer20206GoogleGShard:\nScaling Giant Models with Conditional Computation and Automatic\nShardingMoEencoder-decodertransformerMoE</p>\n<p>GShardMoE600B2024</p>\n<img src=\"/44e38c1b/gshard_moe_family.png\" class title=\"GShard MoE family\">\n<p>expertLSMT MoE --\nexpert</p>\n<p>GShardMoE</p>\n<p></p>\n<p>Googleencoder-decoder\ntransfomerGShardencoder-decoder\ntransfomerencoderdecoderFFNMoENN/2MoE</p>\n<img src=\"/44e38c1b/gshard_model.png\" class title=\"GShard\">\n<p>top-2 expert</p>\n<p>GShardOutrageously Large Neural Networks: The\nSparsely-Gated Mixture-of-Experts Layergating\nfunctionauxiliary loss function</p>\n<p>MoE</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{G}_{s,E}&amp; =\\mathrm{GATE}(x_s)  \\\\\n\\mathrm{FFN}_e(x_s)&amp; =wo_e\\cdot\\text{ReLU}(wi_e\\cdot x_s)  \\\\\ny_{s}&amp; =\\sum_{e=1}^E\\mathcal{G}_{s,e}\\cdot\\mathrm{FFN}_e(x_s)\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(x_s\\)</span> MoEtoken<span class=\"math inline\">\\(w_i\\)</span>  <span class=\"math inline\">\\(w_o\\)</span>\n<span class=\"math inline\">\\(\\mathcal{G}_{s}\\)</span> gating\nfunction</p>\n<p>GShardgating function</p>\n<ul>\n<li><br>\n</li>\n<li>NtokenEexpertNEgating\nfunction</li>\n</ul>\n<p>gating function</p>\n<ul>\n<li> expert capacity<br>\nexperttokenexperttoken2N/EGATE()expert\n<span class=\"math inline\">\\(c_e\\)</span>\ntokentokenexpert<br>\n</li>\n<li> Local group dispatching\ntokenG2N/EG<br>\n</li>\n<li> Auxiliary loss\ngating\nfunction17LSTM\nMoE <span class=\"math inline\">\\(\\ell_{aux}=\\frac1E\\sum_{e=1}^E\\frac{c_e}S\\cdot\nm_e\\)</span><span class=\"math inline\">\\(S\\)</span>\ntoken <span class=\"math inline\">\\(\\frac{c_e}S\\)</span>\n <span class=\"math inline\">\\(\\frac{c_e}S\\)</span>  <span class=\"math inline\">\\(m_e\\)</span>  <span class=\"math inline\">\\(m_e\\)</span>  <span class=\"math inline\">\\(e\\)</span> expert <span class=\"math inline\">\\(S\\)</span> token</li>\n</ul>\n<img src=\"/44e38c1b/gshard_algo_1.png\" class title=\"GShard gating \">\n<ul>\n<li>Random routing 2nd\nexpertg2</li>\n</ul>\n<h1 id=\"switch-transformer\">Switch Transformer</h1>\n<p>20224ChatGPTGoogleSwitch\nTransformers: Scaling to Trillion Parameter Models with Simple and\nEfficient Sparsity2021GoogleSwitch\nTransformer</p>\n<p>Switch\nTransformerGShardencoder-decoderT51.6T2048expert</p>\n<p>Switch\nTransformerSwitch\nTransformerSwitch\nTransformerFLOPS/token</p>\n<p>Switch Transformer</p>\n<ul>\n<li>TransformerMoESwitch\nTransformer<br>\n</li>\n<li>MoE to\ndenseMoEdenseMoE99%dense<br>\n</li>\n<li>1bf16MoE2MoE3<br>\n</li>\n<li>1TMoE<br>\n</li>\n<li>101<br>\n</li>\n<li>FLOPS/tokenSwitch\nTransformer</li>\n</ul>\n<p>Switch Transformer</p>\n<h2 id=\"-1\"></h2>\n<p>Switch\nTransformerGShardtransformerMoE</p>\n<img src=\"/44e38c1b/switch_transformer_structure.png\" class title=\"Switch Transformer \">\n<p>Switch Transformergating\nfunctionrouting</p>\n<p>kexpertSwitch\nTransformer1expertk=1MoESwitch\nlayer3</p>\n<ul>\n<li>routing</li>\n<li>router<br>\n</li>\n<li>expertbatch sizeexpert capacity</li>\n</ul>\n<h2 id=\"-1\"></h2>\n<p>GShardexpertexpert\ncapacityexpertbatchtokentokenexpertoverflowtoken</p>\n<p>capacity factor</p>\n<p><span class=\"math display\">\\[\\text{expert\ncapacity}=\\left(\\frac{\\text{tokens per batch}}{\\text{number of\nexperts}}\\right)\\times\\text{capacity factor}.\\]</span></p>\n<p>capacity\nfactorexperttokenoverflow</p>\n<p>capacity factor</p>\n<img src=\"/44e38c1b/switch_transformer_diff_expert_capacity.png\" class title=\"expert capacity\">\n<p>expert capacity</p>\n<p>capacity\nfactor1overflow</p>\n<p>expertoverflow128expert</p>\n<p>token</p>\n<img src=\"/44e38c1b/switch_transformer_capacity_effect.png\" class title=\"expert capacity\">\n<p>loss moeexpert\ncapacity21.25\n31(1) Switch\nTransformersMoE\nTransformersSwitch\nTransformers(2) Switch\nTransformerMoEMoE\nTransformerMoEDense(3)\nSwitch\nTransformers1.01.25</p>\n<p>loss <span class=\"math inline\">\\(N\\)</span>\nexpert <span class=\"math inline\">\\(T\\)</span> tokenbatch\n<span class=\"math inline\">\\(\\mathcal{B}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\text{loss}&amp;=\\alpha\\cdot\nN\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(f_{i}\\)</span>  <span class=\"math inline\">\\(i\\)</span> experttoken</p>\n<p><span class=\"math display\">\\[\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax\n}p(x)=i\\}\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(P_i\\)</span>\nbatchtoken<span class=\"math inline\">\\(i\\)</span>\nexpert</p>\n<p><span class=\"math display\">\\[\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(f\\)</span> \n<span class=\"math inline\">\\(P\\)</span>  <span class=\"math inline\">\\(1/N\\)</span>loss</p>\n<p><span class=\"math inline\">\\(\\alpha\\)</span>\n1e-51e-11e-2</p>\n<h2 id=\"-1\"></h2>\n<ol type=\"1\">\n<li>trick trick<br>\n</li>\n</ol>\n<ul>\n<li>bf16<br>\n</li>\n<li>\n<span class=\"math inline\">\\(\\mu=0\\)</span><span class=\"math inline\">\\(\\sigma=\\sqrt{s}/n\\)</span>sne.g.\nfan-inTransformers=1.010<br>\n<img src=\"/44e38c1b/switch_transformer_init.png\" class title=\"\"><br>\n</li>\n<li>switch\ntransformerdropout<br>\n<img src=\"/44e38c1b/switch_transformer_dropout.png\" class title=\"dropout\"><br>\ndropoutdense0.1expertdropout</li>\n</ul>\n<ol start=\"2\" type=\"1\">\n<li>scaling</li>\n</ol>\n<p>1step basis</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_step.png\" class title=\"step scaling\">\n<p>2time basis</p>\n<p>For a fixed training duration and computational budget, should one\ntrain a dense or a sparse model?</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_time.png\" class title=\"time scaling\">\n<p>3dense</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_dense.png\" class title=\"dense\">\n<ol start=\"3\" type=\"1\">\n<li>sft</li>\n</ol>\n<p>denseeval</p>\n<img src=\"/44e38c1b/switch_transformer_sft_result.png\" class title=\"sft\">\n<p>moedense</p>\n<p>1moemoedense2label25%teacher\nlabel75%ground truth</p>\n<img src=\"/44e38c1b/switch_transformer_distill.png\" class title=\"\">\n<ol start=\"4\" type=\"1\">\n<li> 99%\n<img src=\"/44e38c1b/switch_transformer_distill_diff_model.png\" class title=\"\"></li>\n</ol>\n<p></p>\n<img src=\"/44e38c1b/switch_transformer_distill_sft.png\" class title=\"sft\">\n<p></p>\n<h1 id=\"glam\">GLaM</h1>\n<p>GLaM (Generalist Language Model)focus on pretrainzero-shot like\ngpt3without sft</p>\n<p>The largest version of GLaM has 1.2T parameters in total with 64\nexperts per MoE layertoken96.6B</p>\n<img src=\"/44e38c1b/glam_compare_gpt3.png\" class title=\"glamgpt3\">\n<img src=\"/44e38c1b/glam_compare_gpt3_2.png\" class title=\"glamgpt3\">\n<p>related workmoe Beyond distillation: Task-level\nmixture-of-experts for efficient inferenceDEEP LEARNING SCALING IS\nPREDICTABLE, EMPIRICALLY Scaling to trillion parameter models with\nsimple and efficient sparsity</p>\n<p>moeexpertEOE^2</p>\n<img src=\"/44e38c1b/glam_model.png\" class title=\"glam\">\n<p>XLNET</p>\n<p>GLU In the non-MoE Transformer feed-forward sub-layers, we\nreplace the first linear projection and the activation function with the\nGated Linear Unitwhich computes the component-wise product of two\nlinear transformation of the input, followed by a Gaussian Error Linear\nUnit</p>\n<p>moe</p>\n<img src=\"/44e38c1b/glam_family.png\" class title=\"glam\">\n<p>trick</p>\n<p>We skip weight updates for a batch if there are any NaNs or Inf s in\nthe gradientsLingvo: a modular and scalable framework for\nsequence-to-sequence modeling</p>\n<p>bpnancheckpointnannan</p>\n<img src=\"/44e38c1b/glam_perf.png\" class title=\"glam\">\n<h1 id=\"st-moe\">ST-MoE</h1>\n<h1 id=\"deepseekmoe\">DeepseekMoE</h1>\n<p>20241DeepseekMoEMoEDeepSeekMoE:\nTowards Ultimate Expert Specialization in Mixture-of-Experts Language\nModelsDeepSeekMoE</p>\n<p>DeepSeekMoEMoE1expert2expertexpertshared\nexpert</p>\n<p>expert(specialization)</p>\n<p>DeepSeekMoE2BMoE16B</p>\n<p>DeepSeekMoE-2B2BDeepSeekMoE-16B7B40%</p>\n<p>DeepSeekMoE-16Bdense</p>\n<img src=\"/44e38c1b/ds_moe_perf.png\" class title=\"deepseek moe\">\n<p>2B16B</p>\n<p>DeepSeekMoE-145BDeepSeek-67B</p>\n<h2 id=\"-2\"></h2>\n<p>moeknowledge hybridity and knowledge\nredundancyexpert specializationnon-overlap &amp;\nfoucusd knowledge</p>\n<p>switch 1expertgshard</p>\n<p>MoE816\n</p>\n<p></p>\n<p>MoEMoE</p>\n<p>deepseek moe2</p>\n<p>1Fine-Grained Expert\nSegmentation:</p>\n<p>2Shared Expert\nIsolation:common\nknowledge</p>\n<p>deepseekmoe 16b40gb</p>\n<img src=\"/44e38c1b/ds_moe_structure.png\" class title=\"deepseek moe \">\n<p>expert isolationideaDeepspeed-moe: Advancing\nmixture-of-experts inference and training to power next-generation AI\nscale</p>\n<p></p>\n<p>gating</p>\n<p>1routing\ncollapseexpert</p>\n<p>2</p>\n<p>expert-level balance loss</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\text{LExpBal}&amp; =\\alpha_1\\sum_{i=1}^{N&#39;}f_iP_i,  \\\\\nf_{i}&amp;\n=\\frac{N^{\\prime}}{K^{\\prime}T}\\sum_{t=1}^T\\mathbb{1}(\\text{Token\n}t\\text{ selects Expert }i),  \\\\\nP_{i}&amp; =\\frac1T\\sum_{t=1}^Ts_{i,t},\n\\end{aligned}\\]</span></p>\n<p>devicelevel balance loss</p>\n<p>D{E1,\nE2, ..., ED}</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}_{\\mathrm{DevBal}}&amp;\n=\\alpha_2\\sum_{i=1}^Df_i&#39;P_i&#39;,  \\\\\nf_i^{\\prime}&amp;\n=\\frac1{|\\mathcal{E}_i|}\\sum_{j\\in\\mathcal{E}_i}f_j,  \\\\\nP_{i}^{\\prime}&amp; =\\sum_{j\\in\\mathcal{E}_i}P_j,\n\\end{aligned}\\]</span></p>\n<h2 id=\"-2\"></h2>\n<p>100B</p>\n<p>HAI-LLM</p>\n<p>deepseek moe\n2b13bbenchmark5densehash\nlayermoeHash layers for large sparse modelsswitch\ntransformergshard</p>\n<img src=\"/44e38c1b/ds_moe_comparison.png\" class title=\"deepseek moe \">\n<p>deepseek\nmoedensegsharddeepseek\nmoe</p>\n<p>deepseek moe 2b, gshard  1.5</p>\n<img src=\"/44e38c1b/ds_moe_upper_bound_2b.png\" class title=\"deepseek moe upper bound\">\n<p>deepseek moe 13b, gshard  1.2</p>\n<img src=\"/44e38c1b/ds_moe_upper_bound_13b.png\" class title=\"deepseek moe upper bound\">\n<p></p>\n<img src=\"/44e38c1b/ds_moe_ablation.png\" class title=\"deepseek moe upper bound \">\n<p>1</p>\n<p>2</p>\n<p>364expert1/2/4pileloss1.808,1.806,1.8111:32+6</p>\n<p></p>\n<img src=\"/44e38c1b/ds_moe_expert_specialization.png\" class title=\"\">\n<p>deepseek moe2bgshard2b*1.5</p>\n<p>topdeepseek\nmoedeepseek</p>\n<p>deepseek\nmoeloss</p>\n<p>gshardDeepSeekMoE</p>\n<img src=\"/44e38c1b/ds_moe_less_activated_expert.png\" class title=\"\">\n<p>132bgsharddeepseek\nmoe</p>\n<p>16b moe2T</p>\n<p>moeloss</p>\n<p>2 + 6/64 </p>\n<p>dimension</p>\n<p>16.4b2.8b</p>\n<p>batch size=4.5k4kbatch18Mtoken</p>\n<p>2T10.6w</p>\n<p>pipeline parallelism</p>\n<p>expert level balance\nloss0.001</p>\n<p>dense</p>\n<p></p>\n<p>Efficient large scale language modeling with mixtures of\nexpertsswitch transformersftmoe</p>\n<p>Flan-moe: Scaling instruction-finetuned language models with sparse\nmixture of experts.suggesting that MoE models can indeed benefit from\ninstruction tuning</p>\n<p>deepseek</p>\n<p>51DeepSeekMoE Chat\n16B40%7BPIQAARCBBHRACEGSM8KMATHTriviaQANaturalQuestions\n2DeepSeekMoE Chat 16BLLaMA2 SFT\n7BHumanEvalMBPPDeepSeek Chat\n7B\n3MMLUCEvalCMMLUDeepSeekMoE\nChat 16BDeepSeek Chat\n7B5.2.1DeepSeekMoE\n16BDeepSeek 7B\n4DeepSeekMoE Chat\n16BLLaMA2 SFT 7BDeepSeekMoE\n16BDeepSeekMoE\n16B40%</p>\n<img src=\"/44e38c1b/ds_moe_sft.png\" class title=\"sft\">\n<p>deepseek moe 145b245btoken</p>\n<img src=\"/44e38c1b/ds_moe_145b.png\" class title=\"145b\">\n<p>related work </p>\n<p>Stablemoe: Stable routing strategy for mixture of experts</p>\n<p>tokenmoeMixture-of-experts with expert choice\nrouting</p>\n<p>Designing effective sparse expert models</p>\n<h1 id=\"qwen1.5-moe\">Qwen1.5-MoE</h1>\n<h1 id=\"dbrx\">DBRX</h1>\n<h1 id=\"\"></h1>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Adaptive Mixtures of Local Experts\nhttps://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf<br>\n2Outrageously Large Neural Networks: The Sparsely-Gated\nMixture-of-Experts Layer https://arxiv.org/abs/1701.06538<br>\n3GShard: Scaling Giant Models with Conditional Computation and\nAutomatic Sharding https://arxiv.org/abs/2006.16668<br>\n4Switch Transformers: Scaling to Trillion Parameter Models with\nSimple and Efficient Sparsity https://arxiv.org/abs/2101.03961<br>\n5GLaM: Efficient Scaling of Language Models with Mixture-of-Experts\nhttps://arxiv.org/abs/2112.06905<br>\n6ST-MoE: Designing Stable and Transferable Sparse Expert Models\nhttps://arxiv.org/abs/2202.08906<br>\n7DeepSeekMoE: Towards Ultimate Expert Specialization in\nMixture-of-Experts Language Models\nhttps://arxiv.org/abs/2401.06066<br>\n8Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated\nParameters https://qwenlm.github.io/zh/blog/qwen-moe/<br>\n9Introducing DBRX: A New State-of-the-Art Open LLM\nhttps://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm</p>\n<p>7A Review of Sparse Expert Models in Deep Learning\nhttps://arxiv.org/abs/2209.01667</p>\n<p>8LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual\nPre-training https://github.com/pjlab-sys4nlp/llama-moe<br>\n9MM1: Methods, Analysis &amp; Insights from Multimodal LLM\nPre-training https://arxiv.org/abs/2403.09611<br>\n12Introducing Jamba https://www.ai21.com/jamba<br>\n13Go Wider Instead of Deeper https://arxiv.org/abs/2107.11817<br>\n14MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided\nAdaptation https://arxiv.org/abs/2204.07675<br>\n15Learning Factored Representations in a Deep Mixture of Experts\nhttps://arxiv.org/abs/1312.4314<br>\n17MegaBlocks: Efficient Sparse Training with Mixture-of-Experts\nhttps://arxiv.org/abs/2211.15841<br>\n18Mixture-of-Experts Meets Instruction Tuning:A Winning Combination\nfor Large Language Models https://arxiv.org/abs/2305.14705<br>\n19</p>\n","length":22353,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>202434MoEQwen1.5-MoEDBRXJamba</p>\n<p>MoE</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">GPT4</td>\n<td style=\"text-align: center;\">20233</td>\n<td style=\"text-align: center;\">236George\nHotzGPT48220B</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Mistral-87B</td>\n<td style=\"text-align: center;\">202312</td>\n<td style=\"text-align: center;\">Mistral AI</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">LLAMA-MoE</td>\n<td style=\"text-align: center;\">202312</td>\n<td style=\"text-align: center;\">github</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">DeepSeek-MoE</td>\n<td style=\"text-align: center;\">20241</td>\n<td style=\"text-align: center;\">MoE</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">abab6</td>\n<td style=\"text-align: center;\">20241</td>\n<td style=\"text-align: center;\">MiniMaxMoE</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">2.0</td>\n<td style=\"text-align: center;\">20242</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Step-2</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">MM1</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">MoE</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Grok-1</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">X</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Qwen1.5-MoE-A2.7B</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">DBRX</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">Databricks</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Jamba</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">AI21</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Mistral-822B</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\">Mistral AI</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">WizardLM-2-822B</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">3.0</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\">4000MoE</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>MoE</p>\n<img src=\"/44e38c1b/xiaomi_moe.jpg\" class title=\"MoE\">\n<p>MoE</p>\n<p>MoEMoE</p>\n<p>20244DeepSeek-MoEQwen1.5-MoE</p>\n<h1 id=\"\"></h1>\n<p>MoEMoEGoogle</p>\n<h2 id=\"\"></h2>\n<p>17MoE1991<a href=\"https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf\">Adaptive\nMixtures of Local Experts</a>Geoffrey HintonMichael I.\nJordan</p>\n<p>MoE</p>\n<blockquote>\n<p>This idea was first presented by Jacobs and Hinton at the\nConnectionist Summer School in Pittsburg in 1988.</p>\n</blockquote>\n<p>MoEMoE</p>\n<h2 id=\"rnn\">RNN</h2>\n<p>Google20171<a href=\"https://arxiv.org/abs/1701.06538\">Outrageously Large Neural\nNetworks: The Sparsely-Gated Mixture-of-Experts\nLayer</a>MoELSTM137BLSTM</p>\n<h2 id=\"transformer\">Transformer</h2>\n<ol type=\"1\">\n<li><p>20206Google<a href=\"https://arxiv.org/abs/2006.16668\">GShard: Scaling Giant Models\nwith Conditional Computation and Automatic\nSharding</a>MoEencoder-decodertransformer600B</p></li>\n<li><p>20211Google<a href=\"https://arxiv.org/abs/2101.03961\">Switch Transformers: Scaling\nto Trillion Parameter Models with Simple and Efficient Sparsity</a>\nT5encoder-decoder1.6Tswitch\ntransformer</p></li>\n<li><p>20224Google<a href=\"https://arxiv.org/abs/2202.08906\">ST-MoE: Designing Stable and\nTransferable Sparse Expert\nModels</a>encoder-decoderMoE269B32B</p></li>\n</ol>\n<h2 id=\"gpt\">GPT</h2>\n<ol type=\"1\">\n<li><p>2112GoogleGLaM<a href=\"https://arxiv.org/abs/2112.06905\">GLaM: Efficient Scaling of\nLanguage Models with\nMixture-of-Experts</a>1.2TDecoder-only</p></li>\n<li><p>20241<a href=\"https://arxiv.org/abs/2401.06066\">DeepSeekMoE: Towards Ultimate\nExpert Specialization in Mixture-of-Experts Language\nModels</a>DeepSeek-MoE</p></li>\n</ol>\n<h1 id=\"\"></h1>\n<p>Geoffrey HintonMichael I. Jordan<a href=\"https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf\">Adaptive\nMixtures of Local Experts</a>MoE</p>\n<p></p>\n<p></p>\n<p>MoEexpert</p>\n<p>MoEvowel discrimination\ntask</p>\n<p>MoEexpert networkgating\nnetworkexpertgating\nnetworkexpertexpertstochastic</p>\n<img src=\"/44e38c1b/vanilla_moe.png\" class title=\"Vanilla MoE\">\n<p>MoEideaJacobsHinton1988lossensembleexpertexpertexpertresidual</p>\n<p>case <span class=\"math inline\">\\(c\\)</span>\n<span class=\"math inline\">\\(d^c\\)</span> ground truth <span class=\"math inline\">\\(i\\)</span> expert <span class=\"math inline\">\\(o_{i}^c\\)</span><span class=\"math inline\">\\(p_{i}^c\\)</span> gating network <span class=\"math inline\">\\(i\\)</span>\nexpert <span class=\"math inline\">\\(E^{c}\\)</span> </p>\n<p><span class=\"math display\">\\[E^{c}=\\left|\\left|d^{c}-\\sum_{i}p_{i}^{c}o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p>expert</p>\n<p>expertexpertexpert</p>\n<p>expertexpertexpertexpert</p>\n<p></p>\n<p>HintonJordanlossexpert</p>\n<p>gating networkexpert</p>\n<p><span class=\"math display\">\\[E^{c}=\\langle\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2\\rangle=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p>expertexpertexpert</p>\n<p>losslocalizationcaseexpertgating\nnetworkexpert</p>\n<p>localizationexpert</p>\n<p>expertexpertgating\nnetworkexpert\nerror+-</p>\n<p>expert</p>\n<p>losslossloss</p>\n<p><span class=\"math display\">\\[\\text{loss}E^{c}=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p><span class=\"math display\">\\[\\text{loss}E^c=-log\\sum_ip_i^ce^{-\\frac12\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}\\]</span></p>\n<p>lossloss</p>\n<p><span class=\"math display\">\\[\\text{loss}\\frac{\\partial\nE^c}{\\partial\\mathbf{o}_i^c}=-2p_i^c(\\mathbf{d}^c-\\mathbf{o}_i^c)\\]</span></p>\n<p><span class=\"math display\">\\[\\text{loss}\\frac{\\partial\nE^c}{\\partial\\mathbf{o}_i^c}=-\\left[\\frac{p_i^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}}{\\sum_jp_j^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_j^c\\|^2}}\\right](\\mathbf{d}^c-\\mathbf{o}_i^c)\\]</span></p>\n<p>lossloss <span class=\"math inline\">\\(i\\)</span>\nexpertexpertexpert <span class=\"math inline\">\\(i\\)</span>\ncasegating\nnetworklosscaseexpertlosslossexpert</p>\n<p>localizationexpert</p>\n<p>BTWloss</p>\n<p>MoE</p>\n<h1 id=\"lstm-moe\">LSTM MoE</h1>\n<p>Google20171 <a href=\"https://arxiv.org/abs/1701.06538\">OUTRAGEOUSLY LARGE NEURAL\nNETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS\nLAYER</a>MoELSTM137BLSTM</p>\n<p>Transformer</p>\n<p>conditional\ncomputationconditional\ncomputationMoE</p>\n<p></p>\n<ul>\n<li>MoEexpertbatch sizebatch\nsize<br>\nbatch\nsize3216expertexpert2batch\nsizebatch\nsizebatch\nsize<br>\n</li>\n<li><br>\nNLP<br>\n</li>\n<li><br>\n<br>\n</li>\n<li><br>\nGPU<br>\n</li>\n<li>GPU<br>\nGPUbranchingif/elseMoEgating\nnetwork</li>\n</ul>\n<p></p>\n<h2 id=\"\"></h2>\n<p></p>\n<p>LSTMMoEembedding</p>\n<img src=\"/44e38c1b/rnn_moe.png\" class title=\"LSTM MoE\">\n<p>expertfeed-forward neural\nnetworknexpertgating networkn</p>\n<p><span class=\"math display\">\\[\\begin{aligned}y=\\sum_{i=1}^nG(x)_iE_i(x)\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(E_{i}(x)\\)</span>  <span class=\"math inline\">\\(i\\)</span> expert<span class=\"math inline\">\\(G(x)_{i}\\)</span> gating network <span class=\"math inline\">\\(i\\)</span> expert <span class=\"math inline\">\\(G(x)_{i}\\)</span>\n0expert</p>\n<p>experttwo-level hierarchical\nMoEgating networkgating\nnetworkexpertexpertgating\nnetworkexpert</p>\n<p>gating network</p>\n<p>softmaxgating\nfunction</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G_\\sigma(x)=Softmax(x\\cdot\nW_g)\\end{aligned}\\]</span></p>\n<p>topkksoftmax0expertsparsitygating\nfunction</p>\n<p>Gaussian\nnoisenoise</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[KeepTopK(v,k)_i=\\begin{cases}v_i&amp;\\text{if\n}v_i\\text{ is in the top }k\\text{ elements of\n}v.\\\\-\\infty&amp;\\text{otherwise.}\\end{cases}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}H(x)_i=(x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\end{aligned}\\]</span></p>\n<p>noiseReLU</p>\n<img src=\"/44e38c1b/softplus.png\" class title=\"softplus\">\n<p></p>\n<h2 id=\"\"></h2>\n<p>expertbatch\nsizenexpertkbatch\nsizebexpertbatch\nsizekb/nexpertbatch size</p>\n<ul>\n<li>batchbatchMoEexpertdexpertkbd/nbatch\nsize</li>\n<li>LSTMbatch size</li>\n</ul>\n<p></p>\n<p>expertinputoutput[input_size,\nhidden_size][hidden_size,\noutput_size]GPU1000hidden_sizeexpert1000</p>\n<h2 id=\"\"></h2>\n<p>gatingexpertexpertexpert</p>\n<p>hard\nconstraintexpertsoft\nconstraint</p>\n<p>expertbatchbatchexpert</p>\n<p><span class=\"math display\">\\[Importance(X)=\\sum_{x\\in\nX}G(x)\\]</span></p>\n<p><span class=\"math inline\">\\(G(x)\\)</span> gating\nnetworkexpert</p>\n<p> <span class=\"math inline\">\\(L_{importance}\\)</span><span class=\"math inline\">\\(L_{importance}\\)</span> </p>\n<p><span class=\"math display\">\\[L_{importance}(X)=w_{importance}\\cdot\nCV(Importance(X))^2\\]</span></p>\n<p><span class=\"math inline\">\\(w_{importance}\\)</span>\nCVcoefficient of variation</p>\n<p>coefficient of\nvariation\n<span class=\"math inline\">\\(\\sigma\\)</span>   <span class=\"math inline\">\\(\\mu\\)</span>\nMoEexpertgating\n<span class=\"math inline\">\\(L_{importance}\\)</span> </p>\n<p> <span class=\"math inline\">\\(L_{importance}\\)</span>  <span class=\"math inline\">\\(L_{importance}\\)</span>\n <span class=\"math inline\">\\(L_{importance}\\)</span>\nexpertexpertbatch</p>\n<p> <span class=\"math inline\">\\(L_{load}\\)</span>\nexpert</p>\n<p>expertexpertback\npropagation <span class=\"math inline\">\\(L_{load}\\)</span>\nexpert</p>\n<p>MoE <span class=\"math inline\">\\(H(x)\\)</span>\nKeepTopK</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}H(x)_i=(x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(kth\\_excluding(H(x),k,i)\\)</span>\n<span class=\"math inline\">\\(H(x)\\)</span>  <span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(P(x,i)\\)</span>\nnoise <span class=\"math inline\">\\(i\\)</span> noise <span class=\"math inline\">\\(kth\\_excluding(H(x),k,i)\\)</span> </p>\n<p><span class=\"math display\">\\[\\begin{aligned}P(x,i)=Pr\\Big((x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\\\&gt;kth\\_excluding(H(x),k,i)\\Big)\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}P(x,i)&amp;=\\Phi\\Big(\\frac{(x\\cdot\nW_g)_i-kth\\_excluding(H(x),k,i)}{Softplus((x\\cdot\nW_{noise})_i)}\\Big)\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\Phi\\)</span>\nCDF</p>\n<p> <span class=\"math inline\">\\(i\\)</span>\nexpert</p>\n<p><span class=\"math display\">\\[\\begin{aligned}Load(X)_i=\\sum_{x\\in\nX}P(x,i)\\end{aligned}\\]</span></p>\n<p>expert</p>\n<p><span class=\"math display\">\\[L_{load}(X)=w_{load}\\cdot\nCV(Load(X))^2\\]</span></p>\n<p>expert\n<span class=\"math inline\">\\(W_g\\)</span>  <span class=\"math inline\">\\(W_noise\\)</span>\n0</p>\n<p></p>\n<img src=\"/44e38c1b/rnn_moe_load_function.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<ol type=\"1\">\n<li> &amp; </li>\n</ol>\n<p>MoE4/32/256expertflat\nMoE256/1024/4096expert432256hierarchical\nMoEexpert1M4expert</p>\n<p></p>\n<img src=\"/44e38c1b/rnn_moe_perf.png\" class title=\"\">\n<p></p>\n<p>MoE</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p>diminishing\nreturns + 100B\ntoken32, 256, 10244096, 16384, 65536,\n131072expertMoE137B</p>\n<p></p>\n<img src=\"/44e38c1b/rnn_moe_137b.png\" class title=\"137\">\n<ol type=\"1\">\n<li>Expert Specialization</li>\n</ol>\n<p>experttokenspecialization</p>\n<img src=\"/44e38c1b/rnn_moe_specilized.png\" class title=\"RNN MoE \">\n<h1 id=\"gshard\">GShard</h1>\n<p>2018Berttransformer20206GoogleGShard:\nScaling Giant Models with Conditional Computation and Automatic\nShardingMoEencoder-decodertransformerMoE</p>\n<p>GShardMoE600B2024</p>\n<img src=\"/44e38c1b/gshard_moe_family.png\" class title=\"GShard MoE family\">\n<p>expertLSMT MoE --\nexpert</p>\n<p>GShardMoE</p>\n<p></p>\n<p>Googleencoder-decoder\ntransfomerGShardencoder-decoder\ntransfomerencoderdecoderFFNMoENN/2MoE</p>\n<img src=\"/44e38c1b/gshard_model.png\" class title=\"GShard\">\n<p>top-2 expert</p>\n<p>GShardOutrageously Large Neural Networks: The\nSparsely-Gated Mixture-of-Experts Layergating\nfunctionauxiliary loss function</p>\n<p>MoE</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{G}_{s,E}&amp; =\\mathrm{GATE}(x_s)  \\\\\n\\mathrm{FFN}_e(x_s)&amp; =wo_e\\cdot\\text{ReLU}(wi_e\\cdot x_s)  \\\\\ny_{s}&amp; =\\sum_{e=1}^E\\mathcal{G}_{s,e}\\cdot\\mathrm{FFN}_e(x_s)\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(x_s\\)</span> MoEtoken<span class=\"math inline\">\\(w_i\\)</span>  <span class=\"math inline\">\\(w_o\\)</span>\n<span class=\"math inline\">\\(\\mathcal{G}_{s}\\)</span> gating\nfunction</p>\n<p>GShardgating function</p>\n<ul>\n<li><br>\n</li>\n<li>NtokenEexpertNEgating\nfunction</li>\n</ul>\n<p>gating function</p>\n<ul>\n<li> expert capacity<br>\nexperttokenexperttoken2N/EGATE()expert\n<span class=\"math inline\">\\(c_e\\)</span>\ntokentokenexpert<br>\n</li>\n<li> Local group dispatching\ntokenG2N/EG<br>\n</li>\n<li> Auxiliary loss\ngating\nfunction17LSTM\nMoE <span class=\"math inline\">\\(\\ell_{aux}=\\frac1E\\sum_{e=1}^E\\frac{c_e}S\\cdot\nm_e\\)</span><span class=\"math inline\">\\(S\\)</span>\ntoken <span class=\"math inline\">\\(\\frac{c_e}S\\)</span>\n <span class=\"math inline\">\\(\\frac{c_e}S\\)</span>  <span class=\"math inline\">\\(m_e\\)</span>  <span class=\"math inline\">\\(m_e\\)</span>  <span class=\"math inline\">\\(e\\)</span> expert <span class=\"math inline\">\\(S\\)</span> token</li>\n</ul>\n<img src=\"/44e38c1b/gshard_algo_1.png\" class title=\"GShard gating \">\n<ul>\n<li>Random routing 2nd\nexpertg2</li>\n</ul>\n<h1 id=\"switch-transformer\">Switch Transformer</h1>\n<p>20224ChatGPTGoogleSwitch\nTransformers: Scaling to Trillion Parameter Models with Simple and\nEfficient Sparsity2021GoogleSwitch\nTransformer</p>\n<p>Switch\nTransformerGShardencoder-decoderT51.6T2048expert</p>\n<p>Switch\nTransformerSwitch\nTransformerSwitch\nTransformerFLOPS/token</p>\n<p>Switch Transformer</p>\n<ul>\n<li>TransformerMoESwitch\nTransformer<br>\n</li>\n<li>MoE to\ndenseMoEdenseMoE99%dense<br>\n</li>\n<li>1bf16MoE2MoE3<br>\n</li>\n<li>1TMoE<br>\n</li>\n<li>101<br>\n</li>\n<li>FLOPS/tokenSwitch\nTransformer</li>\n</ul>\n<p>Switch Transformer</p>\n<h2 id=\"-1\"></h2>\n<p>Switch\nTransformerGShardtransformerMoE</p>\n<img src=\"/44e38c1b/switch_transformer_structure.png\" class title=\"Switch Transformer \">\n<p>Switch Transformergating\nfunctionrouting</p>\n<p>kexpertSwitch\nTransformer1expertk=1MoESwitch\nlayer3</p>\n<ul>\n<li>routing</li>\n<li>router<br>\n</li>\n<li>expertbatch sizeexpert capacity</li>\n</ul>\n<h2 id=\"-1\"></h2>\n<p>GShardexpertexpert\ncapacityexpertbatchtokentokenexpertoverflowtoken</p>\n<p>capacity factor</p>\n<p><span class=\"math display\">\\[\\text{expert\ncapacity}=\\left(\\frac{\\text{tokens per batch}}{\\text{number of\nexperts}}\\right)\\times\\text{capacity factor}.\\]</span></p>\n<p>capacity\nfactorexperttokenoverflow</p>\n<p>capacity factor</p>\n<img src=\"/44e38c1b/switch_transformer_diff_expert_capacity.png\" class title=\"expert capacity\">\n<p>expert capacity</p>\n<p>capacity\nfactor1overflow</p>\n<p>expertoverflow128expert</p>\n<p>token</p>\n<img src=\"/44e38c1b/switch_transformer_capacity_effect.png\" class title=\"expert capacity\">\n<p>loss moeexpert\ncapacity21.25\n31(1) Switch\nTransformersMoE\nTransformersSwitch\nTransformers(2) Switch\nTransformerMoEMoE\nTransformerMoEDense(3)\nSwitch\nTransformers1.01.25</p>\n<p>loss <span class=\"math inline\">\\(N\\)</span>\nexpert <span class=\"math inline\">\\(T\\)</span> tokenbatch\n<span class=\"math inline\">\\(\\mathcal{B}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\text{loss}&amp;=\\alpha\\cdot\nN\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(f_{i}\\)</span>  <span class=\"math inline\">\\(i\\)</span> experttoken</p>\n<p><span class=\"math display\">\\[\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax\n}p(x)=i\\}\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(P_i\\)</span>\nbatchtoken<span class=\"math inline\">\\(i\\)</span>\nexpert</p>\n<p><span class=\"math display\">\\[\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(f\\)</span> \n<span class=\"math inline\">\\(P\\)</span>  <span class=\"math inline\">\\(1/N\\)</span>loss</p>\n<p><span class=\"math inline\">\\(\\alpha\\)</span>\n1e-51e-11e-2</p>\n<h2 id=\"-1\"></h2>\n<ol type=\"1\">\n<li>trick trick<br>\n</li>\n</ol>\n<ul>\n<li>bf16<br>\n</li>\n<li>\n<span class=\"math inline\">\\(\\mu=0\\)</span><span class=\"math inline\">\\(\\sigma=\\sqrt{s}/n\\)</span>sne.g.\nfan-inTransformers=1.010<br>\n<img src=\"/44e38c1b/switch_transformer_init.png\" class title=\"\"><br>\n</li>\n<li>switch\ntransformerdropout<br>\n<img src=\"/44e38c1b/switch_transformer_dropout.png\" class title=\"dropout\"><br>\ndropoutdense0.1expertdropout</li>\n</ul>\n<ol start=\"2\" type=\"1\">\n<li>scaling</li>\n</ol>\n<p>1step basis</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_step.png\" class title=\"step scaling\">\n<p>2time basis</p>\n<p>For a fixed training duration and computational budget, should one\ntrain a dense or a sparse model?</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_time.png\" class title=\"time scaling\">\n<p>3dense</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_dense.png\" class title=\"dense\">\n<ol start=\"3\" type=\"1\">\n<li>sft</li>\n</ol>\n<p>denseeval</p>\n<img src=\"/44e38c1b/switch_transformer_sft_result.png\" class title=\"sft\">\n<p>moedense</p>\n<p>1moemoedense2label25%teacher\nlabel75%ground truth</p>\n<img src=\"/44e38c1b/switch_transformer_distill.png\" class title=\"\">\n<ol start=\"4\" type=\"1\">\n<li> 99%\n<img src=\"/44e38c1b/switch_transformer_distill_diff_model.png\" class title=\"\"></li>\n</ol>\n<p></p>\n<img src=\"/44e38c1b/switch_transformer_distill_sft.png\" class title=\"sft\">\n<p></p>\n<h1 id=\"glam\">GLaM</h1>\n<p>GLaM (Generalist Language Model)focus on pretrainzero-shot like\ngpt3without sft</p>\n<p>The largest version of GLaM has 1.2T parameters in total with 64\nexperts per MoE layertoken96.6B</p>\n<img src=\"/44e38c1b/glam_compare_gpt3.png\" class title=\"glamgpt3\">\n<img src=\"/44e38c1b/glam_compare_gpt3_2.png\" class title=\"glamgpt3\">\n<p>related workmoe Beyond distillation: Task-level\nmixture-of-experts for efficient inferenceDEEP LEARNING SCALING IS\nPREDICTABLE, EMPIRICALLY Scaling to trillion parameter models with\nsimple and efficient sparsity</p>\n<p>moeexpertEOE^2</p>\n<img src=\"/44e38c1b/glam_model.png\" class title=\"glam\">\n<p>XLNET</p>\n<p>GLU In the non-MoE Transformer feed-forward sub-layers, we\nreplace the first linear projection and the activation function with the\nGated Linear Unitwhich computes the component-wise product of two\nlinear transformation of the input, followed by a Gaussian Error Linear\nUnit</p>\n<p>moe</p>\n<img src=\"/44e38c1b/glam_family.png\" class title=\"glam\">\n<p>trick</p>\n<p>We skip weight updates for a batch if there are any NaNs or Inf s in\nthe gradientsLingvo: a modular and scalable framework for\nsequence-to-sequence modeling</p>\n<p>bpnancheckpointnannan</p>\n<img src=\"/44e38c1b/glam_perf.png\" class title=\"glam\">\n<h1 id=\"st-moe\">ST-MoE</h1>\n<h1 id=\"deepseekmoe\">DeepseekMoE</h1>\n<p>20241DeepseekMoEMoEDeepSeekMoE:\nTowards Ultimate Expert Specialization in Mixture-of-Experts Language\nModelsDeepSeekMoE</p>\n<p>DeepSeekMoEMoE1expert2expertexpertshared\nexpert</p>\n<p>expert(specialization)</p>\n<p>DeepSeekMoE2BMoE16B</p>\n<p>DeepSeekMoE-2B2BDeepSeekMoE-16B7B40%</p>\n<p>DeepSeekMoE-16Bdense</p>\n<img src=\"/44e38c1b/ds_moe_perf.png\" class title=\"deepseek moe\">\n<p>2B16B</p>\n<p>DeepSeekMoE-145BDeepSeek-67B</p>\n<h2 id=\"-2\"></h2>\n<p>moeknowledge hybridity and knowledge\nredundancyexpert specializationnon-overlap &amp;\nfoucusd knowledge</p>\n<p>switch 1expertgshard</p>\n<p>MoE816\n</p>\n<p></p>\n<p>MoEMoE</p>\n<p>deepseek moe2</p>\n<p>1Fine-Grained Expert\nSegmentation:</p>\n<p>2Shared Expert\nIsolation:common\nknowledge</p>\n<p>deepseekmoe 16b40gb</p>\n<img src=\"/44e38c1b/ds_moe_structure.png\" class title=\"deepseek moe \">\n<p>expert isolationideaDeepspeed-moe: Advancing\nmixture-of-experts inference and training to power next-generation AI\nscale</p>\n<p></p>\n<p>gating</p>\n<p>1routing\ncollapseexpert</p>\n<p>2</p>\n<p>expert-level balance loss</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\text{LExpBal}&amp; =\\alpha_1\\sum_{i=1}^{N&#39;}f_iP_i,  \\\\\nf_{i}&amp;\n=\\frac{N^{\\prime}}{K^{\\prime}T}\\sum_{t=1}^T\\mathbb{1}(\\text{Token\n}t\\text{ selects Expert }i),  \\\\\nP_{i}&amp; =\\frac1T\\sum_{t=1}^Ts_{i,t},\n\\end{aligned}\\]</span></p>\n<p>devicelevel balance loss</p>\n<p>D{E1,\nE2, ..., ED}</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}_{\\mathrm{DevBal}}&amp;\n=\\alpha_2\\sum_{i=1}^Df_i&#39;P_i&#39;,  \\\\\nf_i^{\\prime}&amp;\n=\\frac1{|\\mathcal{E}_i|}\\sum_{j\\in\\mathcal{E}_i}f_j,  \\\\\nP_{i}^{\\prime}&amp; =\\sum_{j\\in\\mathcal{E}_i}P_j,\n\\end{aligned}\\]</span></p>\n<h2 id=\"-2\"></h2>\n<p>100B</p>\n<p>HAI-LLM</p>\n<p>deepseek moe\n2b13bbenchmark5densehash\nlayermoeHash layers for large sparse modelsswitch\ntransformergshard</p>\n<img src=\"/44e38c1b/ds_moe_comparison.png\" class title=\"deepseek moe \">\n<p>deepseek\nmoedensegsharddeepseek\nmoe</p>\n<p>deepseek moe 2b, gshard  1.5</p>\n<img src=\"/44e38c1b/ds_moe_upper_bound_2b.png\" class title=\"deepseek moe upper bound\">\n<p>deepseek moe 13b, gshard  1.2</p>\n<img src=\"/44e38c1b/ds_moe_upper_bound_13b.png\" class title=\"deepseek moe upper bound\">\n<p></p>\n<img src=\"/44e38c1b/ds_moe_ablation.png\" class title=\"deepseek moe upper bound \">\n<p>1</p>\n<p>2</p>\n<p>364expert1/2/4pileloss1.808,1.806,1.8111:32+6</p>\n<p></p>\n<img src=\"/44e38c1b/ds_moe_expert_specialization.png\" class title=\"\">\n<p>deepseek moe2bgshard2b*1.5</p>\n<p>topdeepseek\nmoedeepseek</p>\n<p>deepseek\nmoeloss</p>\n<p>gshardDeepSeekMoE</p>\n<img src=\"/44e38c1b/ds_moe_less_activated_expert.png\" class title=\"\">\n<p>132bgsharddeepseek\nmoe</p>\n<p>16b moe2T</p>\n<p>moeloss</p>\n<p>2 + 6/64 </p>\n<p>dimension</p>\n<p>16.4b2.8b</p>\n<p>batch size=4.5k4kbatch18Mtoken</p>\n<p>2T10.6w</p>\n<p>pipeline parallelism</p>\n<p>expert level balance\nloss0.001</p>\n<p>dense</p>\n<p></p>\n<p>Efficient large scale language modeling with mixtures of\nexpertsswitch transformersftmoe</p>\n<p>Flan-moe: Scaling instruction-finetuned language models with sparse\nmixture of experts.suggesting that MoE models can indeed benefit from\ninstruction tuning</p>\n<p>deepseek</p>\n<p>51DeepSeekMoE Chat\n16B40%7BPIQAARCBBHRACEGSM8KMATHTriviaQANaturalQuestions\n2DeepSeekMoE Chat 16BLLaMA2 SFT\n7BHumanEvalMBPPDeepSeek Chat\n7B\n3MMLUCEvalCMMLUDeepSeekMoE\nChat 16BDeepSeek Chat\n7B5.2.1DeepSeekMoE\n16BDeepSeek 7B\n4DeepSeekMoE Chat\n16BLLaMA2 SFT 7BDeepSeekMoE\n16BDeepSeekMoE\n16B40%</p>\n<img src=\"/44e38c1b/ds_moe_sft.png\" class title=\"sft\">\n<p>deepseek moe 145b245btoken</p>\n<img src=\"/44e38c1b/ds_moe_145b.png\" class title=\"145b\">\n<p>related work </p>\n<p>Stablemoe: Stable routing strategy for mixture of experts</p>\n<p>tokenmoeMixture-of-experts with expert choice\nrouting</p>\n<p>Designing effective sparse expert models</p>\n<h1 id=\"qwen1.5-moe\">Qwen1.5-MoE</h1>\n<h1 id=\"dbrx\">DBRX</h1>\n<h1 id=\"\"></h1>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Adaptive Mixtures of Local Experts\nhttps://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf<br>\n2Outrageously Large Neural Networks: The Sparsely-Gated\nMixture-of-Experts Layer https://arxiv.org/abs/1701.06538<br>\n3GShard: Scaling Giant Models with Conditional Computation and\nAutomatic Sharding https://arxiv.org/abs/2006.16668<br>\n4Switch Transformers: Scaling to Trillion Parameter Models with\nSimple and Efficient Sparsity https://arxiv.org/abs/2101.03961<br>\n5GLaM: Efficient Scaling of Language Models with Mixture-of-Experts\nhttps://arxiv.org/abs/2112.06905<br>\n6ST-MoE: Designing Stable and Transferable Sparse Expert Models\nhttps://arxiv.org/abs/2202.08906<br>\n7DeepSeekMoE: Towards Ultimate Expert Specialization in\nMixture-of-Experts Language Models\nhttps://arxiv.org/abs/2401.06066<br>\n8Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated\nParameters https://qwenlm.github.io/zh/blog/qwen-moe/<br>\n9Introducing DBRX: A New State-of-the-Art Open LLM\nhttps://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm</p>\n<p>7A Review of Sparse Expert Models in Deep Learning\nhttps://arxiv.org/abs/2209.01667</p>\n<p>8LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual\nPre-training https://github.com/pjlab-sys4nlp/llama-moe<br>\n9MM1: Methods, Analysis &amp; Insights from Multimodal LLM\nPre-training https://arxiv.org/abs/2403.09611<br>\n12Introducing Jamba https://www.ai21.com/jamba<br>\n13Go Wider Instead of Deeper https://arxiv.org/abs/2107.11817<br>\n14MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided\nAdaptation https://arxiv.org/abs/2204.07675<br>\n15Learning Factored Representations in a Deep Mixture of Experts\nhttps://arxiv.org/abs/1312.4314<br>\n17MegaBlocks: Efficient Sparse Training with Mixture-of-Experts\nhttps://arxiv.org/abs/2211.15841<br>\n18Mixture-of-Experts Meets Instruction Tuning:A Winning Combination\nfor Large Language Models https://arxiv.org/abs/2305.14705<br>\n19</p>\n"},{"title":"Attention:MHA,MQAGQA","abbrlink":"3dc22f96","date":"2024-03-05T10:49:38.000Z","_content":"\n//  \n\nAttentionMHAMulti-Head AttentionMQAMulti-Query AttentionGQAGrouped-Query AttentionKV Cache  \n\nAttentionFlashAttentionSliding Window Attention  \n\nLLM\n\n# AttentionRNNAttention\n\nattention\n\nattention\n\n## RNN\n\n>Memory is attention through time. ~ Alex Graves 2020\n\nTransformerRNNSeq2Seq\n\n{% asset_img seq2seq.png seq2seq %}  \n\n{% asset_img encoder.png encoder %}  \n\n{% asset_img decoder.png decoder %}  \n\n[AI Summer](https://theaisummer.com/attention/)  \n\nRNN cellhidden stateRNN encodercontext $z$ RNN decoder $z$ decodertoken[start]  \n\n $z$   \n\n  \n\nLSMTGRU  \n\n $z$  $z$   \n\n $z$   \n\n  \n\nCNNheatmap  \n\n{% asset_img cnn_heatmap.png heatmap %}  \n\nCNNimplicitly\n\nSeq2Seqimplicitexplicit  \n\nRNN $i$ $h_i$  $h_i$  $h_i$ hidden state  \n\n --   \n\n $i$ decoder $y_{i-1}$ encoder $\\mathbf{h}$ score\n\n$$\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in R^n$$  \n\n\n\n$$e_{ij}=\\text{attentiom}_{\\text{net }(\\mathbf{y}_{i-1},h_j)}$$  \n\n $\\mathbf{y}_{i-1}$  $h_j$  $e_{ij}$fc  \n \n  $e_{ij}$ attention netencoder hidden statesoftmax  \n\n$$\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}$$  \n\ndecoder  \n\n$$z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j$$\n\n{% asset_img seq2seq_attention.png seq2seq attention %}  \n\nattention netdecoderhidden stateencoder hidden state\n\nattentionattention  \n\n{% asset_img attention_calculation.png attention calculation %}  \n\nattention $\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot h$  $s$ decoderhidden state $y$ $h$ encoderhidden state  \n\nscaled dot-product attention  \n\n## Transformerattention\n\nRNN attentiontransformer attentionAttention Is All You NeedRNNtime stepattentionhidden stateattention  \n\n{% asset_img transformer_structure.png transformer structure.png %}  \n\nencoderdecoderencoderattentionself-attentiondecoderself-attentioncross-attention  \n\ntransformerattention $\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V$ $Q=W_{Q}YK=W_{K}XV=W_{V}X$ cross-attention $X$ encoderhidden states$Y$ decoderhidden statesself-attention $X=Y$  \n\nscaled dot-product attentionsoftmax\n\n$$\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V$$  \n\nattention  \n\n\n\n{% asset_img Scaled-dot-product-self-attention.pbm self-attention %}  \n\nquerykeyvalueattention  \n\n+  \n\n-30keyvalue  \n\n30querykey5  \n\ntop5 $[8,4,4,2,2]$  $[5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]$  \n \n1 $[0.4,0.2,0.2,0.1,0.1]$ 30 $0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}$ \n\ntransformer attention $QK^T$ softmax/  \n\nself-attention $QKV$  $X$sequencetokencross-attentiondecodersequence  \n\nself-attention $QKV$  $X$  $QK^T$  $QK^T$ MHA  \n\nattentionMHAMQAGQA\n\n[pytorch forcasting](https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention)\n\n```python\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self, dropout: float = None, scale: bool = True):\n        super(ScaledDotProductAttention, self).__init__()\n        if dropout is not None:\n            self.dropout = nn.Dropout(p=dropout)\n        else:\n            self.dropout = dropout\n        self.softmax = nn.Softmax(dim=2)\n        self.scale = scale\n\ndef forward(self, q, k, v, mask=None):\n        attn = torch.bmm(q, k.permute(0, 2, 1))  # query-key overlap\n\n        if self.scale:\n            dimension = torch.as_tensor(k.size(-1), dtype=attn.dtype, device=attn.device).sqrt()\n            attn = attn / dimension\n\n        if mask is not None:\n            attn = attn.masked_fill(mask, -1e9)\n        attn = self.softmax(attn)\n\n        if self.dropout is not None:\n            attn = self.dropout(attn)\n        output = torch.bmm(attn, v)\n        return output, attn\n```\n\n## scaling\n\nBTW $QK^T$  $\\sqrt{d}$   \n\nsoftmaxsoftmax  \n\n{% asset_img softmax.png softmax %}  \n\n[](https://spaces.ac.cn/archives/8620)attentionscaling $\\sqrt{d}$ softmaxnormalizationattentionscaling  \n\n# MHA\n\nattentionMHAmulti-head attention\n\nMHA2017Attention Is All You Needattentionattention  \n\n$$\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)$$  \n\n$$head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)$$  \n\nhidden size $d$ MHA $QKV$ hidden state $head_{num}$  $d_{head}$  $head_{num}$  $QKV$ attention  $head_{num}$  $d_{head}$ concat  \n\namazing  \n\n{% asset_img multihead_attention.png MHA %}  \n\n  \n\nAttention Is All You Need\n\n>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.  \n\nattention headattention head  \n\nCNN $3\\times3\\times128$ 128 $3\\times3$  $3\\times3$   \n\n$$\\left.\\left[\\begin{matrix}1&0&-1\\\\1&0&-1\\\\1&0&-1\\end{matrix}\\right.\\right]$$  \n\n  \n\n$$\\left.\\left[\\begin{matrix}1&1&1\\\\0&0&0\\\\-1&-1&-1\\end{matrix}\\right.\\right]$$  \n\n  \n\n128 $3\\times3$ 128MHA  \n\nexpect  \n\n[](https://zhuanlan.zhihu.com/p/626820422)12 $QK^T$   \n\nMHAattentionattention\n\n  \n\n[Are Sixteen Heads Really Better than One?](https://arxiv.org/pdf/1905.10650.pdf)MHA  \n\nhidden sizeLLM1216244896  \n\n[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)MHA  \n\n```python\ndef attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n             / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim = -1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\n\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        '''\n        h: head number\n        '''\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d\n        self.d = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n        \n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n        \n        # 1) Do all the linear projections in batch from d_model => h x d \n        query, key, value = \\\n            [l(x).view(nbatches, -1, self.h, self.d).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key, value))]\n        \n        # 2) Apply attention on all the projected vectors in batch. \n        x, self.attn = attention(query, key, value, mask=mask, \n                                 dropout=self.dropout)\n        \n        # 3) \"Concat\" using a view and apply a final linear. \n        x = x.transpose(1, 2).contiguous() \\\n             .view(nbatches, -1, self.h * self.d)\n        return self.linears[-1](x)\n```\n\n[transformers](https://github.com/huggingface/transformers)\n\n# KV Cache\n\nMQAGQAKV Cache  \n\nencoder-decoderAGIdecoder-onlyLLMauto-regressive  \n\n $\\text{input}_{i-1}$  $\\text{token}_{i}$  $\\text{token}_{i}$  $\\text{input}_{i-1}$  $\\text{input}_{i}$  $\\text{input}_{i}$  $\\text{token}_{i+1}$   \n\ntokentoken  \n\n```\nstep0: =[BOS]=\nstep1: =[BOS]=\nstep2: =[BOS]=\nstep3: =[BOS]=\nstep4: =[BOS]=\nstep5: =[BOS]=[EOS]\n```\n\n[BOS][EOS]  \n\nhidden state \n\nstepsteptokenstepstep\n\n\n\nattention  \n\n$$\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n$$\n\ndecodermask attention\n\n34attention\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n$$\n\n45attention\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n$$\n\n5 $o_{0}$  $o_{2}$   \n\n  \n\n  \n\nstep0101step515instruction800stepstep0800step1799...\n\nstep  \n\nKV Cache  \n\n $k$  $v$   \n\n34 $k$  $v$ \n\n\n\n$$\n\\text{cache}_l=\\text{None}\\\\\n$$\n\n<center></center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$  \n\nkv_cache $l$ \n\n5 $l$ <u>****</u> $k$  $v$  $o_{3}$  $o_{0}o_{1}o_{2}$   \n\n $l$  $o_{0}o_{1}o_{2}$ FNN $l+1$  $l+1$  $k$  $v$  $l+1$  $k$  $v$   \n\n $k$  $v$ \n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$\n\n<center></center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n$$  \n\nattentionFFN  \n\ntransformersuse_cache=TrueKV Cache  \n\nGPT2  \n\n```python\nClass GPT2Attention(nn.Module):\n    ...\n    ...\n    def forward(\n        self,\n        hidden_states: Optional[Tuple[torch.FloatTensor]],\n        layer_past: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n        if encoder_hidden_states is not None:\n            if not hasattr(self, \"q_attn\"):\n                raise ValueError(\n                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n                )\n\n            query = self.q_attn(hidden_states)\n            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n            attention_mask = encoder_attention_mask\n        else:\n            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n\n        query = self._split_heads(query, self.num_heads, self.head_dim)\n        key = self._split_heads(key, self.num_heads, self.head_dim)\n        value = self._split_heads(value, self.num_heads, self.head_dim)\n\n        # \n        if layer_past is not None:\n            past_key, past_value = layer_past\n            key = torch.cat((past_key, key), dim=-2)  # key\n            value = torch.cat((past_value, value), dim=-2)  # value\n\n        if use_cache is True:\n            present = (key, value)  # \n        else:\n            present = None\n\n        if self.reorder_and_upcast_attn:\n            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n        else:\n            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n\n        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n        attn_output = self.c_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output)\n\n        outputs = (attn_output, present)\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs  # a, present, (attentions)\n```\n\nKV Cachedecodermask attentiontokentoken  \n\nKV Cache  \n\n $s$  $L$ hidden size $d$   \n\n$$\n2\\times L\\times s\\times d\n$$  \n\n\n\n$$\n2\\times 2\\times L\\times s\\times d\n$$  \n\nLlama2 7B $L=32$  $L=4096$ token524,288 bytes52K $s=1024$ 536,870,912 bytes500M  \n\nbatch size=1batch size1G  \n\nMHA $qkv$ \n\n\n\n{% asset_img gpu_cache.png gpu cache %}  \n\nH10050ML2 CacheL1 CacheLlama2 7B100token  \n\nLLM34B/70B\n\nL2 CacheHBML2 Cache  \n\n{% asset_img sram_dram.png  %}  \n\n  \n\nCache  \n\n  \n\n# MQA\n\nMQA\n\nGoogle2019Fast Transformer Decoding: One Write-Head is All You NeedMQABert  \n\nMQAMHA $W_{Q}W_{K}W_{V}$ nn= $d_{model}$  $d_{head}$ attentionMQA $Q$ MHA $KV$  $d_{head}$ nQuery $KV$ attention  \n\nMHA $KV$ MQA $KV$ MHA  \n\n{% asset_img MQA.webp MQA %}  \n\n $KV$   \n\nLlama2 7B32MQA1024token1/32536,870,912 bytes / 32 = 16,777,216 bytes16M\n\n $KV$   \n\nMQAMHAhidden sizehead num  \n\n{% asset_img mqa_result_1.png MQA results 1 %}  \n\n{% asset_img mqa_result_3.png MQA results 3 %}  \n\n# GQA  \n\nMQAMHAGQAGrouped-Query AttentionMQAMHA  \n\nGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints2023\n\nGQA $Q$ MHA/MQA $KV$  $Q$  $Q$ groupgroup $Q$  $KV$ group $Q$  $KV$   \n\nMHA $KV$ GQAMQA $KV$ GQA  \n\n\n\n{% asset_img GQA.png GQA %}  \n\n  \n\n{% asset_img GQA_result_1.png GQA result %}  \n\n2/3/4GQAMHAMQAMHAMQAGQAaverage poolingMHAMHAGQA  \n\nLlama2GQAtech reportMHAMQAGQA  \n\n{% asset_img llama2_qga.png llama2 GQA result %}  \n\n#   \n\nMHAMQAGQA\n\nGQALLM  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n1The Annotated Transformer \n https://nlp.seas.harvard.edu/2018/04/03/attention.html  \n2Attention Is All You Need https://arxiv.org/pdf/1706.03762.pdf  \n3Fast Transformer Decoding: One Write-Head is All You Need https://arxiv.org/pdf/1911.02150.pdf  \n4https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096  \n5GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf  \n6How Attention works in Deep Learning: understanding the attention mechanism in sequence models https://theaisummer.com/attention/  \n7A simple overview of RNN, LSTM and Attention Mechanism \nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b  \n8https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention  \n9Transformer https://spaces.ac.cn/archives/8620  \n10https://theaisummer.com/self-attention/  https://theaisummer.com/self-attention/  \n11https://zhuanlan.zhihu.com/p/626820422 https://zhuanlan.zhihu.com/p/626820422  \n12Are Sixteen Heads Really Better than One? \nhttps://arxiv.org/pdf/1905.10650.pdf  \n13This post is all you needTransformer \nhttps://zhuanlan.zhihu.com/p/420820453  \n14The Illustrated Transformer https://jalammar.github.io/illustrated-transformer/  \n15Multi-Query Attention is All You Need https://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055  \n\n","source":"_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA.md","raw":"---\ntitle: 'Attention:MHA,MQAGQA'\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - attention\n  - KV Cache\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 3dc22f96\ndate: 2024-03-05 18:49:38\n---\n\n//  \n\nAttentionMHAMulti-Head AttentionMQAMulti-Query AttentionGQAGrouped-Query AttentionKV Cache  \n\nAttentionFlashAttentionSliding Window Attention  \n\nLLM\n\n# AttentionRNNAttention\n\nattention\n\nattention\n\n## RNN\n\n>Memory is attention through time. ~ Alex Graves 2020\n\nTransformerRNNSeq2Seq\n\n{% asset_img seq2seq.png seq2seq %}  \n\n{% asset_img encoder.png encoder %}  \n\n{% asset_img decoder.png decoder %}  \n\n[AI Summer](https://theaisummer.com/attention/)  \n\nRNN cellhidden stateRNN encodercontext $z$ RNN decoder $z$ decodertoken[start]  \n\n $z$   \n\n  \n\nLSMTGRU  \n\n $z$  $z$   \n\n $z$   \n\n  \n\nCNNheatmap  \n\n{% asset_img cnn_heatmap.png heatmap %}  \n\nCNNimplicitly\n\nSeq2Seqimplicitexplicit  \n\nRNN $i$ $h_i$  $h_i$  $h_i$ hidden state  \n\n --   \n\n $i$ decoder $y_{i-1}$ encoder $\\mathbf{h}$ score\n\n$$\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in R^n$$  \n\n\n\n$$e_{ij}=\\text{attentiom}_{\\text{net }(\\mathbf{y}_{i-1},h_j)}$$  \n\n $\\mathbf{y}_{i-1}$  $h_j$  $e_{ij}$fc  \n \n  $e_{ij}$ attention netencoder hidden statesoftmax  \n\n$$\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}$$  \n\ndecoder  \n\n$$z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j$$\n\n{% asset_img seq2seq_attention.png seq2seq attention %}  \n\nattention netdecoderhidden stateencoder hidden state\n\nattentionattention  \n\n{% asset_img attention_calculation.png attention calculation %}  \n\nattention $\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot h$  $s$ decoderhidden state $y$ $h$ encoderhidden state  \n\nscaled dot-product attention  \n\n## Transformerattention\n\nRNN attentiontransformer attentionAttention Is All You NeedRNNtime stepattentionhidden stateattention  \n\n{% asset_img transformer_structure.png transformer structure.png %}  \n\nencoderdecoderencoderattentionself-attentiondecoderself-attentioncross-attention  \n\ntransformerattention $\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V$ $Q=W_{Q}YK=W_{K}XV=W_{V}X$ cross-attention $X$ encoderhidden states$Y$ decoderhidden statesself-attention $X=Y$  \n\nscaled dot-product attentionsoftmax\n\n$$\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V$$  \n\nattention  \n\n\n\n{% asset_img Scaled-dot-product-self-attention.pbm self-attention %}  \n\nquerykeyvalueattention  \n\n+  \n\n-30keyvalue  \n\n30querykey5  \n\ntop5 $[8,4,4,2,2]$  $[5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]$  \n \n1 $[0.4,0.2,0.2,0.1,0.1]$ 30 $0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}$ \n\ntransformer attention $QK^T$ softmax/  \n\nself-attention $QKV$  $X$sequencetokencross-attentiondecodersequence  \n\nself-attention $QKV$  $X$  $QK^T$  $QK^T$ MHA  \n\nattentionMHAMQAGQA\n\n[pytorch forcasting](https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention)\n\n```python\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self, dropout: float = None, scale: bool = True):\n        super(ScaledDotProductAttention, self).__init__()\n        if dropout is not None:\n            self.dropout = nn.Dropout(p=dropout)\n        else:\n            self.dropout = dropout\n        self.softmax = nn.Softmax(dim=2)\n        self.scale = scale\n\ndef forward(self, q, k, v, mask=None):\n        attn = torch.bmm(q, k.permute(0, 2, 1))  # query-key overlap\n\n        if self.scale:\n            dimension = torch.as_tensor(k.size(-1), dtype=attn.dtype, device=attn.device).sqrt()\n            attn = attn / dimension\n\n        if mask is not None:\n            attn = attn.masked_fill(mask, -1e9)\n        attn = self.softmax(attn)\n\n        if self.dropout is not None:\n            attn = self.dropout(attn)\n        output = torch.bmm(attn, v)\n        return output, attn\n```\n\n## scaling\n\nBTW $QK^T$  $\\sqrt{d}$   \n\nsoftmaxsoftmax  \n\n{% asset_img softmax.png softmax %}  \n\n[](https://spaces.ac.cn/archives/8620)attentionscaling $\\sqrt{d}$ softmaxnormalizationattentionscaling  \n\n# MHA\n\nattentionMHAmulti-head attention\n\nMHA2017Attention Is All You Needattentionattention  \n\n$$\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)$$  \n\n$$head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)$$  \n\nhidden size $d$ MHA $QKV$ hidden state $head_{num}$  $d_{head}$  $head_{num}$  $QKV$ attention  $head_{num}$  $d_{head}$ concat  \n\namazing  \n\n{% asset_img multihead_attention.png MHA %}  \n\n  \n\nAttention Is All You Need\n\n>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.  \n\nattention headattention head  \n\nCNN $3\\times3\\times128$ 128 $3\\times3$  $3\\times3$   \n\n$$\\left.\\left[\\begin{matrix}1&0&-1\\\\1&0&-1\\\\1&0&-1\\end{matrix}\\right.\\right]$$  \n\n  \n\n$$\\left.\\left[\\begin{matrix}1&1&1\\\\0&0&0\\\\-1&-1&-1\\end{matrix}\\right.\\right]$$  \n\n  \n\n128 $3\\times3$ 128MHA  \n\nexpect  \n\n[](https://zhuanlan.zhihu.com/p/626820422)12 $QK^T$   \n\nMHAattentionattention\n\n  \n\n[Are Sixteen Heads Really Better than One?](https://arxiv.org/pdf/1905.10650.pdf)MHA  \n\nhidden sizeLLM1216244896  \n\n[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)MHA  \n\n```python\ndef attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n             / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim = -1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\n\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        '''\n        h: head number\n        '''\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d\n        self.d = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n        \n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n        \n        # 1) Do all the linear projections in batch from d_model => h x d \n        query, key, value = \\\n            [l(x).view(nbatches, -1, self.h, self.d).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key, value))]\n        \n        # 2) Apply attention on all the projected vectors in batch. \n        x, self.attn = attention(query, key, value, mask=mask, \n                                 dropout=self.dropout)\n        \n        # 3) \"Concat\" using a view and apply a final linear. \n        x = x.transpose(1, 2).contiguous() \\\n             .view(nbatches, -1, self.h * self.d)\n        return self.linears[-1](x)\n```\n\n[transformers](https://github.com/huggingface/transformers)\n\n# KV Cache\n\nMQAGQAKV Cache  \n\nencoder-decoderAGIdecoder-onlyLLMauto-regressive  \n\n $\\text{input}_{i-1}$  $\\text{token}_{i}$  $\\text{token}_{i}$  $\\text{input}_{i-1}$  $\\text{input}_{i}$  $\\text{input}_{i}$  $\\text{token}_{i+1}$   \n\ntokentoken  \n\n```\nstep0: =[BOS]=\nstep1: =[BOS]=\nstep2: =[BOS]=\nstep3: =[BOS]=\nstep4: =[BOS]=\nstep5: =[BOS]=[EOS]\n```\n\n[BOS][EOS]  \n\nhidden state \n\nstepsteptokenstepstep\n\n\n\nattention  \n\n$$\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n$$\n\ndecodermask attention\n\n34attention\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n$$\n\n45attention\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n$$\n\n5 $o_{0}$  $o_{2}$   \n\n  \n\n  \n\nstep0101step515instruction800stepstep0800step1799...\n\nstep  \n\nKV Cache  \n\n $k$  $v$   \n\n34 $k$  $v$ \n\n\n\n$$\n\\text{cache}_l=\\text{None}\\\\\n$$\n\n<center></center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$  \n\nkv_cache $l$ \n\n5 $l$ <u>****</u> $k$  $v$  $o_{3}$  $o_{0}o_{1}o_{2}$   \n\n $l$  $o_{0}o_{1}o_{2}$ FNN $l+1$  $l+1$  $k$  $v$  $l+1$  $k$  $v$   \n\n $k$  $v$ \n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$\n\n<center></center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n$$  \n\nattentionFFN  \n\ntransformersuse_cache=TrueKV Cache  \n\nGPT2  \n\n```python\nClass GPT2Attention(nn.Module):\n    ...\n    ...\n    def forward(\n        self,\n        hidden_states: Optional[Tuple[torch.FloatTensor]],\n        layer_past: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n        if encoder_hidden_states is not None:\n            if not hasattr(self, \"q_attn\"):\n                raise ValueError(\n                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n                )\n\n            query = self.q_attn(hidden_states)\n            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n            attention_mask = encoder_attention_mask\n        else:\n            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n\n        query = self._split_heads(query, self.num_heads, self.head_dim)\n        key = self._split_heads(key, self.num_heads, self.head_dim)\n        value = self._split_heads(value, self.num_heads, self.head_dim)\n\n        # \n        if layer_past is not None:\n            past_key, past_value = layer_past\n            key = torch.cat((past_key, key), dim=-2)  # key\n            value = torch.cat((past_value, value), dim=-2)  # value\n\n        if use_cache is True:\n            present = (key, value)  # \n        else:\n            present = None\n\n        if self.reorder_and_upcast_attn:\n            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n        else:\n            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n\n        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n        attn_output = self.c_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output)\n\n        outputs = (attn_output, present)\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs  # a, present, (attentions)\n```\n\nKV Cachedecodermask attentiontokentoken  \n\nKV Cache  \n\n $s$  $L$ hidden size $d$   \n\n$$\n2\\times L\\times s\\times d\n$$  \n\n\n\n$$\n2\\times 2\\times L\\times s\\times d\n$$  \n\nLlama2 7B $L=32$  $L=4096$ token524,288 bytes52K $s=1024$ 536,870,912 bytes500M  \n\nbatch size=1batch size1G  \n\nMHA $qkv$ \n\n\n\n{% asset_img gpu_cache.png gpu cache %}  \n\nH10050ML2 CacheL1 CacheLlama2 7B100token  \n\nLLM34B/70B\n\nL2 CacheHBML2 Cache  \n\n{% asset_img sram_dram.png  %}  \n\n  \n\nCache  \n\n  \n\n# MQA\n\nMQA\n\nGoogle2019Fast Transformer Decoding: One Write-Head is All You NeedMQABert  \n\nMQAMHA $W_{Q}W_{K}W_{V}$ nn= $d_{model}$  $d_{head}$ attentionMQA $Q$ MHA $KV$  $d_{head}$ nQuery $KV$ attention  \n\nMHA $KV$ MQA $KV$ MHA  \n\n{% asset_img MQA.webp MQA %}  \n\n $KV$   \n\nLlama2 7B32MQA1024token1/32536,870,912 bytes / 32 = 16,777,216 bytes16M\n\n $KV$   \n\nMQAMHAhidden sizehead num  \n\n{% asset_img mqa_result_1.png MQA results 1 %}  \n\n{% asset_img mqa_result_3.png MQA results 3 %}  \n\n# GQA  \n\nMQAMHAGQAGrouped-Query AttentionMQAMHA  \n\nGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints2023\n\nGQA $Q$ MHA/MQA $KV$  $Q$  $Q$ groupgroup $Q$  $KV$ group $Q$  $KV$   \n\nMHA $KV$ GQAMQA $KV$ GQA  \n\n\n\n{% asset_img GQA.png GQA %}  \n\n  \n\n{% asset_img GQA_result_1.png GQA result %}  \n\n2/3/4GQAMHAMQAMHAMQAGQAaverage poolingMHAMHAGQA  \n\nLlama2GQAtech reportMHAMQAGQA  \n\n{% asset_img llama2_qga.png llama2 GQA result %}  \n\n#   \n\nMHAMQAGQA\n\nGQALLM  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n1The Annotated Transformer \n https://nlp.seas.harvard.edu/2018/04/03/attention.html  \n2Attention Is All You Need https://arxiv.org/pdf/1706.03762.pdf  \n3Fast Transformer Decoding: One Write-Head is All You Need https://arxiv.org/pdf/1911.02150.pdf  \n4https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096  \n5GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf  \n6How Attention works in Deep Learning: understanding the attention mechanism in sequence models https://theaisummer.com/attention/  \n7A simple overview of RNN, LSTM and Attention Mechanism \nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b  \n8https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention  \n9Transformer https://spaces.ac.cn/archives/8620  \n10https://theaisummer.com/self-attention/  https://theaisummer.com/self-attention/  \n11https://zhuanlan.zhihu.com/p/626820422 https://zhuanlan.zhihu.com/p/626820422  \n12Are Sixteen Heads Really Better than One? \nhttps://arxiv.org/pdf/1905.10650.pdf  \n13This post is all you needTransformer \nhttps://zhuanlan.zhihu.com/p/420820453  \n14The Illustrated Transformer https://jalammar.github.io/illustrated-transformer/  \n15Multi-Query Attention is All You Need https://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055  \n\n","slug":"cs/nlp/2024/03/Attention-MHA-MQAGQA","published":1,"updated":"2024-03-14T14:17:54.411Z","comments":1,"layout":"post","photos":[],"_id":"clv7wdj11004t794k0h03gngv","content":"<p>//</p>\n<p>AttentionMHAMulti-Head\nAttentionMQAMulti-Query AttentionGQAGrouped-Query\nAttentionKV\nCache</p>\n<p>AttentionFlashAttentionSliding\nWindow Attention</p>\n<p>LLM</p>\n<h1 id=\"attentionrnnattention\">AttentionRNNAttention</h1>\n<p>attention</p>\n<p>attention</p>\n<h2 id=\"rnn\">RNN</h2>\n<blockquote>\n<p>Memory is attention through time. ~ Alex Graves 2020</p>\n</blockquote>\n<p>TransformerRNNSeq2Seq</p>\n<img src=\"/3dc22f96/seq2seq.png\" class title=\"seq2seq\">\n<img src=\"/3dc22f96/encoder.png\" class title=\"encoder\">\n<img src=\"/3dc22f96/decoder.png\" class title=\"decoder\">\n<p><a href=\"https://theaisummer.com/attention/\">AI\nSummer</a></p>\n<p>RNN cellhidden stateRNN\nencodercontext <span class=\"math inline\">\\(z\\)</span> RNN decoder <span class=\"math inline\">\\(z\\)</span>\ndecodertoken[start]</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p></p>\n<p>LSMTGRU</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p></p>\n<p>CNNheatmap</p>\n<img src=\"/3dc22f96/cnn_heatmap.png\" class title=\"heatmap\">\n<p>CNNimplicitly</p>\n<p>Seq2Seqimplicitexplicit</p>\n<p>RNN <span class=\"math inline\">\\(i\\)</span> <span class=\"math inline\">\\(h_i\\)</span>  <span class=\"math inline\">\\(h_i\\)</span>\n\n<span class=\"math inline\">\\(h_i\\)</span>\nhidden\nstate</p>\n<p>\n--\n</p>\n<p> <span class=\"math inline\">\\(i\\)</span>\ndecoder <span class=\"math inline\">\\(y_{i-1}\\)</span>\nencoder <span class=\"math inline\">\\(\\mathbf{h}\\)</span>\nscore</p>\n<p><span class=\"math display\">\\[\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in\nR^n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[e_{ij}=\\text{attentiom}_{\\text{net\n}(\\mathbf{y}_{i-1},h_j)}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\mathbf{y}_{i-1}\\)</span>\n <span class=\"math inline\">\\(h_j\\)</span>  <span class=\"math inline\">\\(e_{ij}\\)</span>fc</p>\n<p> <span class=\"math inline\">\\(e_{ij}\\)</span>\nattention\nnetencoder hidden statesoftmax</p>\n<p><span class=\"math display\">\\[\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}\\]</span></p>\n<p>decoder</p>\n<p><span class=\"math display\">\\[z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j\\]</span></p>\n<img src=\"/3dc22f96/seq2seq_attention.png\" class title=\"seq2seq attention\">\n<p>attention netdecoderhidden\nstateencoder hidden\nstate</p>\n<p>attentionattention</p>\n<img src=\"/3dc22f96/attention_calculation.png\" class title=\"attention calculation\">\n<p>attention <span class=\"math inline\">\\(\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot\nh\\)</span>  <span class=\"math inline\">\\(s\\)</span>\ndecoderhidden state <span class=\"math inline\">\\(y\\)</span> <span class=\"math inline\">\\(h\\)</span> encoderhidden state</p>\n<p>scaled dot-product\nattention</p>\n<h2 id=\"transformerattention\">Transformerattention</h2>\n<p>RNN attentiontransformer\nattentionAttention Is All You\nNeedRNNtime\nstepattentionhidden\nstateattention</p>\n<img src=\"/3dc22f96/transformer_structure.png\" class title=\"transformer structure.png\">\n<p>encoderdecoderencoderattentionself-attentiondecoderself-attentioncross-attention</p>\n<p>transformerattention <span class=\"math inline\">\\(\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V\\)</span>\n<span class=\"math inline\">\\(Q=W_{Q}YK=W_{K}XV=W_{V}X\\)</span>\ncross-attention <span class=\"math inline\">\\(X\\)</span>\nencoderhidden states<span class=\"math inline\">\\(Y\\)</span>\ndecoderhidden statesself-attention <span class=\"math inline\">\\(X=Y\\)</span></p>\n<p>scaled dot-product attentionsoftmax</p>\n<p><span class=\"math display\">\\[\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V\\]</span></p>\n<p>attention</p>\n<p></p>\n<img src=\"/3dc22f96/Scaled-dot-product-self-attention.pbm\" class title=\"self-attention\">\n<p>querykeyvalueattention</p>\n<p>+</p>\n<p>-30keyvalue</p>\n<p>30querykey5</p>\n<p>top5 <span class=\"math inline\">\\([8,4,4,2,2]\\)</span>  <span class=\"math inline\">\\([5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]\\)</span>\n</p>\n<p>1 <span class=\"math inline\">\\([0.4,0.2,0.2,0.1,0.1]\\)</span>\n30 <span class=\"math inline\">\\(0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}\\)</span>\n</p>\n<p>transformer attention <span class=\"math inline\">\\(QK^T\\)</span>\nsoftmax/</p>\n<p>self-attention <span class=\"math inline\">\\(QKV\\)</span>\n <span class=\"math inline\">\\(X\\)</span>sequencetokencross-attentiondecodersequence</p>\n<p>self-attention <span class=\"math inline\">\\(QKV\\)</span>\n <span class=\"math inline\">\\(X\\)</span>  <span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(QK^T\\)</span>\nMHA</p>\n<p>attentionMHAMQAGQA</p>\n<p><a href=\"https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention\">pytorch\nforcasting</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ScaledDotProductAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, dropout: <span class=\"built_in\">float</span> = <span class=\"literal\">None</span>, scale: <span class=\"built_in\">bool</span> = <span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.dropout = dropout</span><br><span class=\"line\">        self.softmax = nn.Softmax(dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">        self.scale = scale</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, q, k, v, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        attn = torch.bmm(q, k.permute(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))  <span class=\"comment\"># query-key overlap</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.scale:</span><br><span class=\"line\">            dimension = torch.as_tensor(k.size(-<span class=\"number\">1</span>), dtype=attn.dtype, device=attn.device).sqrt()</span><br><span class=\"line\">            attn = attn / dimension</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = attn.masked_fill(mask, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">        attn = self.softmax(attn)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = self.dropout(attn)</span><br><span class=\"line\">        output = torch.bmm(attn, v)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, attn</span><br></pre></td></tr></table></figure>\n<h2 id=\"scaling\">scaling</h2>\n<p>BTW <span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(\\sqrt{d}\\)</span> </p>\n<p>softmaxsoftmax</p>\n<img src=\"/3dc22f96/softmax.png\" class title=\"softmax\">\n<p><a href=\"https://spaces.ac.cn/archives/8620\"></a>attentionscaling\n<span class=\"math inline\">\\(\\sqrt{d}\\)</span>\nsoftmaxnormalizationattentionscaling</p>\n<h1 id=\"mha\">MHA</h1>\n<p>attentionMHAmulti-head\nattention</p>\n<p>MHA2017Attention Is All You\nNeedattentionattention</p>\n<p><span class=\"math display\">\\[\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)\\]</span></p>\n<p><span class=\"math display\">\\[head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)\\]</span></p>\n<p>hidden size <span class=\"math inline\">\\(d\\)</span>\nMHA <span class=\"math inline\">\\(QKV\\)</span>\nhidden state <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span>  <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(QKV\\)</span>\nattention <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span> concat</p>\n<p>amazing</p>\n<img src=\"/3dc22f96/multihead_attention.png\" class title=\"MHA\">\n<p></p>\n<p>Attention Is All You Need</p>\n<blockquote>\n<p>Multi-head attention allows the model to jointly attend to\ninformation from different representation subspaces at different\npositions.</p>\n</blockquote>\n<p>attention\nheadattention\nhead</p>\n<p>CNN <span class=\"math inline\">\\(3\\times3\\times128\\)</span> 128 <span class=\"math inline\">\\(3\\times3\\)</span>\n <span class=\"math inline\">\\(3\\times3\\)</span> </p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;1&amp;1\\\\0&amp;0&amp;0\\\\-1&amp;-1&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p></p>\n<p>128 <span class=\"math inline\">\\(3\\times3\\)</span>\n128MHA</p>\n<p>expect</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/626820422\"></a>12\n<span class=\"math inline\">\\(QK^T\\)</span> </p>\n<p>MHAattentionattention</p>\n<p></p>\n<p><a href=\"https://arxiv.org/pdf/1905.10650.pdf\">Are Sixteen Heads Really\nBetter than One?</a>MHA</p>\n<p>hidden\nsizeLLM1216244896</p>\n<p><a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\">The\nAnnotated Transformer</a>MHA</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">attention</span>(<span class=\"params\">query, key, value, mask=<span class=\"literal\">None</span>, dropout=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class=\"line\">    d_k = query.size(-<span class=\"number\">1</span>)</span><br><span class=\"line\">    scores = torch.matmul(query, key.transpose(-<span class=\"number\">2</span>, -<span class=\"number\">1</span>)) \\</span><br><span class=\"line\">             / math.sqrt(d_k)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        scores = scores.masked_fill(mask == <span class=\"number\">0</span>, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">    p_attn = F.softmax(scores, dim = -<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        p_attn = dropout(p_attn)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultiHeadedAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, h, d_model, dropout=<span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        h: head number</span></span><br><span class=\"line\"><span class=\"string\">        &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> d_model % h == <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"comment\"># We assume d_v always equals d</span></span><br><span class=\"line\">        self.d = d_model // h</span><br><span class=\"line\">        self.h = h</span><br><span class=\"line\">        self.linears = clones(nn.Linear(d_model, d_model), <span class=\"number\">4</span>)</span><br><span class=\"line\">        self.attn = <span class=\"literal\">None</span></span><br><span class=\"line\">        self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, query, key, value, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"comment\"># Same mask applied to all h heads.</span></span><br><span class=\"line\">            mask = mask.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">        nbatches = query.size(<span class=\"number\">0</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 1) Do all the linear projections in batch from d_model =&gt; h x d </span></span><br><span class=\"line\">        query, key, value = \\</span><br><span class=\"line\">            [l(x).view(nbatches, -<span class=\"number\">1</span>, self.h, self.d).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">             <span class=\"keyword\">for</span> l, x <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(self.linears, (query, key, value))]</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class=\"line\">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class=\"line\">                                 dropout=self.dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class=\"line\">        x = x.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous() \\</span><br><span class=\"line\">             .view(nbatches, -<span class=\"number\">1</span>, self.h * self.d)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.linears[-<span class=\"number\">1</span>](x)</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://github.com/huggingface/transformers\">transformers</a></p>\n<h1 id=\"kv-cache\">KV Cache</h1>\n<p>MQAGQAKV\nCache</p>\n<p>encoder-decoderAGIdecoder-onlyLLMauto-regressive</p>\n<p> <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span> \n<span class=\"math inline\">\\(\\text{input}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{input}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i+1}\\)</span>\n</p>\n<p>tokentoken</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">step0: =[BOS]=</span><br><span class=\"line\">step1: =[BOS]=</span><br><span class=\"line\">step2: =[BOS]=</span><br><span class=\"line\">step3: =[BOS]=</span><br><span class=\"line\">step4: =[BOS]=</span><br><span class=\"line\">step5: =[BOS]=[EOS]</span><br></pre></td></tr></table></figure>\n<p>[BOS][EOS]</p>\n<p>hidden\nstate</p>\n<p>stepsteptokenstepstep</p>\n<p></p>\n<p>attention</p>\n<p><span class=\"math display\">\\[\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n\\]</span></p>\n<p>decodermask\nattention</p>\n<p>34attention</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>45attention</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&amp;=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>5 <span class=\"math inline\">\\(o_{0}\\)</span>  <span class=\"math inline\">\\(o_{2}\\)</span> </p>\n<p></p>\n<p></p>\n<p>step0101step515instruction800stepstep0800step1799...</p>\n<p>step</p>\n<p>KV\nCache</p>\n<p> <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p>34\n<span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=\\text{None}\\\\\n\\]</span></p>\n<center>\n\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<p>kv_cache <span class=\"math inline\">\\(l\\)</span>\n</p>\n<p>5 <span class=\"math inline\">\\(l\\)</span>\n<u><strong></strong></u> <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span>  <span class=\"math inline\">\\(o_{3}\\)</span>  <span class=\"math inline\">\\(o_{0}o_{1}o_{2}\\)</span> </p>\n<p> <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(o_{0}o_{1}o_{2}\\)</span>\nFNN <span class=\"math inline\">\\(l+1\\)</span>\n <span class=\"math inline\">\\(l+1\\)</span>\n <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span>  <span class=\"math inline\">\\(l+1\\)</span>  <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p> <span class=\"math inline\">\\(k\\)</span> \n<span class=\"math inline\">\\(v\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<center>\n\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n\\]</span></p>\n<p>attentionFFN</p>\n<p>transformersuse_cache=TrueKV Cache</p>\n<p>GPT2</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Class GPT2Attention(nn.Module):</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">        self,</span></span><br><span class=\"line\"><span class=\"params\">        hidden_states: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.FloatTensor]],</span></span><br><span class=\"line\"><span class=\"params\">        layer_past: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.Tensor]] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        head_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_hidden_states: <span class=\"type\">Optional</span>[torch.Tensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        use_cache: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">        output_attentions: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">    </span>) -&gt; <span class=\"type\">Tuple</span>[<span class=\"type\">Union</span>[torch.Tensor, <span class=\"type\">Tuple</span>[torch.Tensor]], ...]:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> encoder_hidden_states <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">hasattr</span>(self, <span class=\"string\">&quot;q_attn&quot;</span>):</span><br><span class=\"line\">                <span class=\"keyword\">raise</span> ValueError(</span><br><span class=\"line\">                    <span class=\"string\">&quot;If class is used as cross attention, the weights `q_attn` have to be defined. &quot;</span></span><br><span class=\"line\">                    <span class=\"string\">&quot;Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.&quot;</span></span><br><span class=\"line\">                )</span><br><span class=\"line\"></span><br><span class=\"line\">            query = self.q_attn(hidden_states)</span><br><span class=\"line\">            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">            attention_mask = encoder_attention_mask</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        query = self._split_heads(query, self.num_heads, self.head_dim)</span><br><span class=\"line\">        key = self._split_heads(key, self.num_heads, self.head_dim)</span><br><span class=\"line\">        value = self._split_heads(value, self.num_heads, self.head_dim)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># </span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> layer_past <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            past_key, past_value = layer_past</span><br><span class=\"line\">            key = torch.cat((past_key, key), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># key</span></span><br><span class=\"line\">            value = torch.cat((past_value, value), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># value</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> use_cache <span class=\"keyword\">is</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">            present = (key, value)  <span class=\"comment\"># </span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            present = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.reorder_and_upcast_attn:</span><br><span class=\"line\">            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)</span><br><span class=\"line\">        attn_output = self.c_proj(attn_output)</span><br><span class=\"line\">        attn_output = self.resid_dropout(attn_output)</span><br><span class=\"line\"></span><br><span class=\"line\">        outputs = (attn_output, present)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> output_attentions:</span><br><span class=\"line\">            outputs += (attn_weights,)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> outputs  <span class=\"comment\"># a, present, (attentions)</span></span><br></pre></td></tr></table></figure>\n<p>KV\nCachedecodermask\nattentiontokentoken</p>\n<p>KV Cache</p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n <span class=\"math inline\">\\(L\\)</span> hidden size <span class=\"math inline\">\\(d\\)</span> </p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d\n\\]</span></p>\n<p>Llama2 7B <span class=\"math inline\">\\(L=32\\)</span> \n<span class=\"math inline\">\\(L=4096\\)</span>\ntoken524,288 bytes52K <span class=\"math inline\">\\(s=1024\\)</span> 536,870,912\nbytes500M</p>\n<p>batch size=1batch\nsize1G</p>\n<p>MHA <span class=\"math inline\">\\(qkv\\)</span>\n</p>\n<p></p>\n<img src=\"/3dc22f96/gpu_cache.png\" class title=\"gpu cache\">\n<p>H10050ML2 CacheL1\nCacheLlama2\n7B100token</p>\n<p>LLM34B/70B</p>\n<p>L2 CacheHBML2\nCache</p>\n<img src=\"/3dc22f96/sram_dram.png\" class title=\"\">\n<p></p>\n<p>Cache</p>\n<p></p>\n<h1 id=\"mqa\">MQA</h1>\n<p>MQA</p>\n<p>Google2019Fast Transformer Decoding: One Write-Head is All\nYou\nNeedMQABert</p>\n<p>MQAMHA <span class=\"math inline\">\\(W_{Q}W_{K}W_{V}\\)</span>\nnn= <span class=\"math inline\">\\(d_{model}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span>\nattentionMQA <span class=\"math inline\">\\(Q\\)</span> MHA <span class=\"math inline\">\\(KV\\)</span> \n<span class=\"math inline\">\\(d_{head}\\)</span>\nnQuery <span class=\"math inline\">\\(KV\\)</span>\nattention</p>\n<p>MHA <span class=\"math inline\">\\(KV\\)</span>\nMQA <span class=\"math inline\">\\(KV\\)</span>\nMHA</p>\n<img src=\"/3dc22f96/MQA.webp\" class title=\"MQA\">\n<p> <span class=\"math inline\">\\(KV\\)</span>\n</p>\n<p>Llama2\n7B32MQA1024token1/32536,870,912\nbytes / 32 = 16,777,216 bytes16M</p>\n<p> <span class=\"math inline\">\\(KV\\)</span>\n</p>\n<p>MQAMHAhidden\nsizehead num</p>\n<img src=\"/3dc22f96/mqa_result_1.png\" class title=\"MQA results 1\">\n<img src=\"/3dc22f96/mqa_result_3.png\" class title=\"MQA results 3\">\n<h1 id=\"gqa\">GQA</h1>\n<p>MQAMHAGQAGrouped-Query\nAttentionMQAMHA</p>\n<p>GQA: Training Generalized Multi-Query Transformer Models\nfrom Multi-Head Checkpoints2023</p>\n<p>GQA <span class=\"math inline\">\\(Q\\)</span>\nMHA/MQA <span class=\"math inline\">\\(KV\\)</span>\n <span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(Q\\)</span> groupgroup\n<span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(KV\\)</span> group <span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(KV\\)</span> </p>\n<p>MHA <span class=\"math inline\">\\(KV\\)</span>\nGQAMQA <span class=\"math inline\">\\(KV\\)</span> GQA</p>\n<p></p>\n<img src=\"/3dc22f96/GQA.png\" class title=\"GQA\">\n<p></p>\n<img src=\"/3dc22f96/GQA_result_1.png\" class title=\"GQA result\">\n<p>2/3/4GQAMHAMQAMHAMQAGQAaverage\npoolingMHAMHAGQA</p>\n<p>Llama2GQAtech\nreportMHAMQAGQA</p>\n<img src=\"/3dc22f96/llama2_qga.png\" class title=\"llama2 GQA result\">\n<h1 id=\"\"></h1>\n<p>MHAMQAGQA</p>\n<p>GQALLM</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1The Annotated Transformer\nhttps://nlp.seas.harvard.edu/2018/04/03/attention.html<br>\n2Attention Is All You Need\nhttps://arxiv.org/pdf/1706.03762.pdf<br>\n3Fast Transformer Decoding: One Write-Head is All You Need\nhttps://arxiv.org/pdf/1911.02150.pdf<br>\n4https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096<br>\n5GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf<br>\n6How Attention works in Deep Learning: understanding the attention\nmechanism in sequence models https://theaisummer.com/attention/<br>\n7A simple overview of RNN, LSTM and Attention Mechanism\nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b<br>\n8https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention<br>\n9Transformer\nhttps://spaces.ac.cn/archives/8620<br>\n10https://theaisummer.com/self-attention/\nhttps://theaisummer.com/self-attention/<br>\n11https://zhuanlan.zhihu.com/p/626820422\nhttps://zhuanlan.zhihu.com/p/626820422<br>\n12Are Sixteen Heads Really Better than One?\nhttps://arxiv.org/pdf/1905.10650.pdf<br>\n13This post is all you needTransformer\nhttps://zhuanlan.zhihu.com/p/420820453<br>\n14The Illustrated Transformer\nhttps://jalammar.github.io/illustrated-transformer/<br>\n15Multi-Query Attention is All You Need\nhttps://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055</p>\n","length":16733,"excerpt":"","more":"<p>//</p>\n<p>AttentionMHAMulti-Head\nAttentionMQAMulti-Query AttentionGQAGrouped-Query\nAttentionKV\nCache</p>\n<p>AttentionFlashAttentionSliding\nWindow Attention</p>\n<p>LLM</p>\n<h1 id=\"attentionrnnattention\">AttentionRNNAttention</h1>\n<p>attention</p>\n<p>attention</p>\n<h2 id=\"rnn\">RNN</h2>\n<blockquote>\n<p>Memory is attention through time. ~ Alex Graves 2020</p>\n</blockquote>\n<p>TransformerRNNSeq2Seq</p>\n<img src=\"/3dc22f96/seq2seq.png\" class title=\"seq2seq\">\n<img src=\"/3dc22f96/encoder.png\" class title=\"encoder\">\n<img src=\"/3dc22f96/decoder.png\" class title=\"decoder\">\n<p><a href=\"https://theaisummer.com/attention/\">AI\nSummer</a></p>\n<p>RNN cellhidden stateRNN\nencodercontext <span class=\"math inline\">\\(z\\)</span> RNN decoder <span class=\"math inline\">\\(z\\)</span>\ndecodertoken[start]</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p></p>\n<p>LSMTGRU</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p></p>\n<p>CNNheatmap</p>\n<img src=\"/3dc22f96/cnn_heatmap.png\" class title=\"heatmap\">\n<p>CNNimplicitly</p>\n<p>Seq2Seqimplicitexplicit</p>\n<p>RNN <span class=\"math inline\">\\(i\\)</span> <span class=\"math inline\">\\(h_i\\)</span>  <span class=\"math inline\">\\(h_i\\)</span>\n\n<span class=\"math inline\">\\(h_i\\)</span>\nhidden\nstate</p>\n<p>\n--\n</p>\n<p> <span class=\"math inline\">\\(i\\)</span>\ndecoder <span class=\"math inline\">\\(y_{i-1}\\)</span>\nencoder <span class=\"math inline\">\\(\\mathbf{h}\\)</span>\nscore</p>\n<p><span class=\"math display\">\\[\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in\nR^n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[e_{ij}=\\text{attentiom}_{\\text{net\n}(\\mathbf{y}_{i-1},h_j)}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\mathbf{y}_{i-1}\\)</span>\n <span class=\"math inline\">\\(h_j\\)</span>  <span class=\"math inline\">\\(e_{ij}\\)</span>fc</p>\n<p> <span class=\"math inline\">\\(e_{ij}\\)</span>\nattention\nnetencoder hidden statesoftmax</p>\n<p><span class=\"math display\">\\[\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}\\]</span></p>\n<p>decoder</p>\n<p><span class=\"math display\">\\[z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j\\]</span></p>\n<img src=\"/3dc22f96/seq2seq_attention.png\" class title=\"seq2seq attention\">\n<p>attention netdecoderhidden\nstateencoder hidden\nstate</p>\n<p>attentionattention</p>\n<img src=\"/3dc22f96/attention_calculation.png\" class title=\"attention calculation\">\n<p>attention <span class=\"math inline\">\\(\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot\nh\\)</span>  <span class=\"math inline\">\\(s\\)</span>\ndecoderhidden state <span class=\"math inline\">\\(y\\)</span> <span class=\"math inline\">\\(h\\)</span> encoderhidden state</p>\n<p>scaled dot-product\nattention</p>\n<h2 id=\"transformerattention\">Transformerattention</h2>\n<p>RNN attentiontransformer\nattentionAttention Is All You\nNeedRNNtime\nstepattentionhidden\nstateattention</p>\n<img src=\"/3dc22f96/transformer_structure.png\" class title=\"transformer structure.png\">\n<p>encoderdecoderencoderattentionself-attentiondecoderself-attentioncross-attention</p>\n<p>transformerattention <span class=\"math inline\">\\(\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V\\)</span>\n<span class=\"math inline\">\\(Q=W_{Q}YK=W_{K}XV=W_{V}X\\)</span>\ncross-attention <span class=\"math inline\">\\(X\\)</span>\nencoderhidden states<span class=\"math inline\">\\(Y\\)</span>\ndecoderhidden statesself-attention <span class=\"math inline\">\\(X=Y\\)</span></p>\n<p>scaled dot-product attentionsoftmax</p>\n<p><span class=\"math display\">\\[\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V\\]</span></p>\n<p>attention</p>\n<p></p>\n<img src=\"/3dc22f96/Scaled-dot-product-self-attention.pbm\" class title=\"self-attention\">\n<p>querykeyvalueattention</p>\n<p>+</p>\n<p>-30keyvalue</p>\n<p>30querykey5</p>\n<p>top5 <span class=\"math inline\">\\([8,4,4,2,2]\\)</span>  <span class=\"math inline\">\\([5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]\\)</span>\n</p>\n<p>1 <span class=\"math inline\">\\([0.4,0.2,0.2,0.1,0.1]\\)</span>\n30 <span class=\"math inline\">\\(0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}\\)</span>\n</p>\n<p>transformer attention <span class=\"math inline\">\\(QK^T\\)</span>\nsoftmax/</p>\n<p>self-attention <span class=\"math inline\">\\(QKV\\)</span>\n <span class=\"math inline\">\\(X\\)</span>sequencetokencross-attentiondecodersequence</p>\n<p>self-attention <span class=\"math inline\">\\(QKV\\)</span>\n <span class=\"math inline\">\\(X\\)</span>  <span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(QK^T\\)</span>\nMHA</p>\n<p>attentionMHAMQAGQA</p>\n<p><a href=\"https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention\">pytorch\nforcasting</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ScaledDotProductAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, dropout: <span class=\"built_in\">float</span> = <span class=\"literal\">None</span>, scale: <span class=\"built_in\">bool</span> = <span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.dropout = dropout</span><br><span class=\"line\">        self.softmax = nn.Softmax(dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">        self.scale = scale</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, q, k, v, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        attn = torch.bmm(q, k.permute(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))  <span class=\"comment\"># query-key overlap</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.scale:</span><br><span class=\"line\">            dimension = torch.as_tensor(k.size(-<span class=\"number\">1</span>), dtype=attn.dtype, device=attn.device).sqrt()</span><br><span class=\"line\">            attn = attn / dimension</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = attn.masked_fill(mask, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">        attn = self.softmax(attn)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = self.dropout(attn)</span><br><span class=\"line\">        output = torch.bmm(attn, v)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, attn</span><br></pre></td></tr></table></figure>\n<h2 id=\"scaling\">scaling</h2>\n<p>BTW <span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(\\sqrt{d}\\)</span> </p>\n<p>softmaxsoftmax</p>\n<img src=\"/3dc22f96/softmax.png\" class title=\"softmax\">\n<p><a href=\"https://spaces.ac.cn/archives/8620\"></a>attentionscaling\n<span class=\"math inline\">\\(\\sqrt{d}\\)</span>\nsoftmaxnormalizationattentionscaling</p>\n<h1 id=\"mha\">MHA</h1>\n<p>attentionMHAmulti-head\nattention</p>\n<p>MHA2017Attention Is All You\nNeedattentionattention</p>\n<p><span class=\"math display\">\\[\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)\\]</span></p>\n<p><span class=\"math display\">\\[head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)\\]</span></p>\n<p>hidden size <span class=\"math inline\">\\(d\\)</span>\nMHA <span class=\"math inline\">\\(QKV\\)</span>\nhidden state <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span>  <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(QKV\\)</span>\nattention <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span> concat</p>\n<p>amazing</p>\n<img src=\"/3dc22f96/multihead_attention.png\" class title=\"MHA\">\n<p></p>\n<p>Attention Is All You Need</p>\n<blockquote>\n<p>Multi-head attention allows the model to jointly attend to\ninformation from different representation subspaces at different\npositions.</p>\n</blockquote>\n<p>attention\nheadattention\nhead</p>\n<p>CNN <span class=\"math inline\">\\(3\\times3\\times128\\)</span> 128 <span class=\"math inline\">\\(3\\times3\\)</span>\n <span class=\"math inline\">\\(3\\times3\\)</span> </p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;1&amp;1\\\\0&amp;0&amp;0\\\\-1&amp;-1&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p></p>\n<p>128 <span class=\"math inline\">\\(3\\times3\\)</span>\n128MHA</p>\n<p>expect</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/626820422\"></a>12\n<span class=\"math inline\">\\(QK^T\\)</span> </p>\n<p>MHAattentionattention</p>\n<p></p>\n<p><a href=\"https://arxiv.org/pdf/1905.10650.pdf\">Are Sixteen Heads Really\nBetter than One?</a>MHA</p>\n<p>hidden\nsizeLLM1216244896</p>\n<p><a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\">The\nAnnotated Transformer</a>MHA</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">attention</span>(<span class=\"params\">query, key, value, mask=<span class=\"literal\">None</span>, dropout=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class=\"line\">    d_k = query.size(-<span class=\"number\">1</span>)</span><br><span class=\"line\">    scores = torch.matmul(query, key.transpose(-<span class=\"number\">2</span>, -<span class=\"number\">1</span>)) \\</span><br><span class=\"line\">             / math.sqrt(d_k)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        scores = scores.masked_fill(mask == <span class=\"number\">0</span>, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">    p_attn = F.softmax(scores, dim = -<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        p_attn = dropout(p_attn)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultiHeadedAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, h, d_model, dropout=<span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        h: head number</span></span><br><span class=\"line\"><span class=\"string\">        &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> d_model % h == <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"comment\"># We assume d_v always equals d</span></span><br><span class=\"line\">        self.d = d_model // h</span><br><span class=\"line\">        self.h = h</span><br><span class=\"line\">        self.linears = clones(nn.Linear(d_model, d_model), <span class=\"number\">4</span>)</span><br><span class=\"line\">        self.attn = <span class=\"literal\">None</span></span><br><span class=\"line\">        self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, query, key, value, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"comment\"># Same mask applied to all h heads.</span></span><br><span class=\"line\">            mask = mask.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">        nbatches = query.size(<span class=\"number\">0</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 1) Do all the linear projections in batch from d_model =&gt; h x d </span></span><br><span class=\"line\">        query, key, value = \\</span><br><span class=\"line\">            [l(x).view(nbatches, -<span class=\"number\">1</span>, self.h, self.d).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">             <span class=\"keyword\">for</span> l, x <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(self.linears, (query, key, value))]</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class=\"line\">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class=\"line\">                                 dropout=self.dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class=\"line\">        x = x.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous() \\</span><br><span class=\"line\">             .view(nbatches, -<span class=\"number\">1</span>, self.h * self.d)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.linears[-<span class=\"number\">1</span>](x)</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://github.com/huggingface/transformers\">transformers</a></p>\n<h1 id=\"kv-cache\">KV Cache</h1>\n<p>MQAGQAKV\nCache</p>\n<p>encoder-decoderAGIdecoder-onlyLLMauto-regressive</p>\n<p> <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span> \n<span class=\"math inline\">\\(\\text{input}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{input}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i+1}\\)</span>\n</p>\n<p>tokentoken</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">step0: =[BOS]=</span><br><span class=\"line\">step1: =[BOS]=</span><br><span class=\"line\">step2: =[BOS]=</span><br><span class=\"line\">step3: =[BOS]=</span><br><span class=\"line\">step4: =[BOS]=</span><br><span class=\"line\">step5: =[BOS]=[EOS]</span><br></pre></td></tr></table></figure>\n<p>[BOS][EOS]</p>\n<p>hidden\nstate</p>\n<p>stepsteptokenstepstep</p>\n<p></p>\n<p>attention</p>\n<p><span class=\"math display\">\\[\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n\\]</span></p>\n<p>decodermask\nattention</p>\n<p>34attention</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>45attention</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&amp;=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>5 <span class=\"math inline\">\\(o_{0}\\)</span>  <span class=\"math inline\">\\(o_{2}\\)</span> </p>\n<p></p>\n<p></p>\n<p>step0101step515instruction800stepstep0800step1799...</p>\n<p>step</p>\n<p>KV\nCache</p>\n<p> <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p>34\n<span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=\\text{None}\\\\\n\\]</span></p>\n<center>\n\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<p>kv_cache <span class=\"math inline\">\\(l\\)</span>\n</p>\n<p>5 <span class=\"math inline\">\\(l\\)</span>\n<u><strong></strong></u> <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span>  <span class=\"math inline\">\\(o_{3}\\)</span>  <span class=\"math inline\">\\(o_{0}o_{1}o_{2}\\)</span> </p>\n<p> <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(o_{0}o_{1}o_{2}\\)</span>\nFNN <span class=\"math inline\">\\(l+1\\)</span>\n <span class=\"math inline\">\\(l+1\\)</span>\n <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span>  <span class=\"math inline\">\\(l+1\\)</span>  <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p> <span class=\"math inline\">\\(k\\)</span> \n<span class=\"math inline\">\\(v\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<center>\n\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n\\]</span></p>\n<p>attentionFFN</p>\n<p>transformersuse_cache=TrueKV Cache</p>\n<p>GPT2</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Class GPT2Attention(nn.Module):</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">        self,</span></span><br><span class=\"line\"><span class=\"params\">        hidden_states: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.FloatTensor]],</span></span><br><span class=\"line\"><span class=\"params\">        layer_past: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.Tensor]] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        head_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_hidden_states: <span class=\"type\">Optional</span>[torch.Tensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        use_cache: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">        output_attentions: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">    </span>) -&gt; <span class=\"type\">Tuple</span>[<span class=\"type\">Union</span>[torch.Tensor, <span class=\"type\">Tuple</span>[torch.Tensor]], ...]:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> encoder_hidden_states <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">hasattr</span>(self, <span class=\"string\">&quot;q_attn&quot;</span>):</span><br><span class=\"line\">                <span class=\"keyword\">raise</span> ValueError(</span><br><span class=\"line\">                    <span class=\"string\">&quot;If class is used as cross attention, the weights `q_attn` have to be defined. &quot;</span></span><br><span class=\"line\">                    <span class=\"string\">&quot;Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.&quot;</span></span><br><span class=\"line\">                )</span><br><span class=\"line\"></span><br><span class=\"line\">            query = self.q_attn(hidden_states)</span><br><span class=\"line\">            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">            attention_mask = encoder_attention_mask</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        query = self._split_heads(query, self.num_heads, self.head_dim)</span><br><span class=\"line\">        key = self._split_heads(key, self.num_heads, self.head_dim)</span><br><span class=\"line\">        value = self._split_heads(value, self.num_heads, self.head_dim)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># </span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> layer_past <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            past_key, past_value = layer_past</span><br><span class=\"line\">            key = torch.cat((past_key, key), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># key</span></span><br><span class=\"line\">            value = torch.cat((past_value, value), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># value</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> use_cache <span class=\"keyword\">is</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">            present = (key, value)  <span class=\"comment\"># </span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            present = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.reorder_and_upcast_attn:</span><br><span class=\"line\">            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)</span><br><span class=\"line\">        attn_output = self.c_proj(attn_output)</span><br><span class=\"line\">        attn_output = self.resid_dropout(attn_output)</span><br><span class=\"line\"></span><br><span class=\"line\">        outputs = (attn_output, present)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> output_attentions:</span><br><span class=\"line\">            outputs += (attn_weights,)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> outputs  <span class=\"comment\"># a, present, (attentions)</span></span><br></pre></td></tr></table></figure>\n<p>KV\nCachedecodermask\nattentiontokentoken</p>\n<p>KV Cache</p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n <span class=\"math inline\">\\(L\\)</span> hidden size <span class=\"math inline\">\\(d\\)</span> </p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d\n\\]</span></p>\n<p>Llama2 7B <span class=\"math inline\">\\(L=32\\)</span> \n<span class=\"math inline\">\\(L=4096\\)</span>\ntoken524,288 bytes52K <span class=\"math inline\">\\(s=1024\\)</span> 536,870,912\nbytes500M</p>\n<p>batch size=1batch\nsize1G</p>\n<p>MHA <span class=\"math inline\">\\(qkv\\)</span>\n</p>\n<p></p>\n<img src=\"/3dc22f96/gpu_cache.png\" class title=\"gpu cache\">\n<p>H10050ML2 CacheL1\nCacheLlama2\n7B100token</p>\n<p>LLM34B/70B</p>\n<p>L2 CacheHBML2\nCache</p>\n<img src=\"/3dc22f96/sram_dram.png\" class title=\"\">\n<p></p>\n<p>Cache</p>\n<p></p>\n<h1 id=\"mqa\">MQA</h1>\n<p>MQA</p>\n<p>Google2019Fast Transformer Decoding: One Write-Head is All\nYou\nNeedMQABert</p>\n<p>MQAMHA <span class=\"math inline\">\\(W_{Q}W_{K}W_{V}\\)</span>\nnn= <span class=\"math inline\">\\(d_{model}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span>\nattentionMQA <span class=\"math inline\">\\(Q\\)</span> MHA <span class=\"math inline\">\\(KV\\)</span> \n<span class=\"math inline\">\\(d_{head}\\)</span>\nnQuery <span class=\"math inline\">\\(KV\\)</span>\nattention</p>\n<p>MHA <span class=\"math inline\">\\(KV\\)</span>\nMQA <span class=\"math inline\">\\(KV\\)</span>\nMHA</p>\n<img src=\"/3dc22f96/MQA.webp\" class title=\"MQA\">\n<p> <span class=\"math inline\">\\(KV\\)</span>\n</p>\n<p>Llama2\n7B32MQA1024token1/32536,870,912\nbytes / 32 = 16,777,216 bytes16M</p>\n<p> <span class=\"math inline\">\\(KV\\)</span>\n</p>\n<p>MQAMHAhidden\nsizehead num</p>\n<img src=\"/3dc22f96/mqa_result_1.png\" class title=\"MQA results 1\">\n<img src=\"/3dc22f96/mqa_result_3.png\" class title=\"MQA results 3\">\n<h1 id=\"gqa\">GQA</h1>\n<p>MQAMHAGQAGrouped-Query\nAttentionMQAMHA</p>\n<p>GQA: Training Generalized Multi-Query Transformer Models\nfrom Multi-Head Checkpoints2023</p>\n<p>GQA <span class=\"math inline\">\\(Q\\)</span>\nMHA/MQA <span class=\"math inline\">\\(KV\\)</span>\n <span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(Q\\)</span> groupgroup\n<span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(KV\\)</span> group <span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(KV\\)</span> </p>\n<p>MHA <span class=\"math inline\">\\(KV\\)</span>\nGQAMQA <span class=\"math inline\">\\(KV\\)</span> GQA</p>\n<p></p>\n<img src=\"/3dc22f96/GQA.png\" class title=\"GQA\">\n<p></p>\n<img src=\"/3dc22f96/GQA_result_1.png\" class title=\"GQA result\">\n<p>2/3/4GQAMHAMQAMHAMQAGQAaverage\npoolingMHAMHAGQA</p>\n<p>Llama2GQAtech\nreportMHAMQAGQA</p>\n<img src=\"/3dc22f96/llama2_qga.png\" class title=\"llama2 GQA result\">\n<h1 id=\"\"></h1>\n<p>MHAMQAGQA</p>\n<p>GQALLM</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1The Annotated Transformer\nhttps://nlp.seas.harvard.edu/2018/04/03/attention.html<br>\n2Attention Is All You Need\nhttps://arxiv.org/pdf/1706.03762.pdf<br>\n3Fast Transformer Decoding: One Write-Head is All You Need\nhttps://arxiv.org/pdf/1911.02150.pdf<br>\n4https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096<br>\n5GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf<br>\n6How Attention works in Deep Learning: understanding the attention\nmechanism in sequence models https://theaisummer.com/attention/<br>\n7A simple overview of RNN, LSTM and Attention Mechanism\nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b<br>\n8https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention<br>\n9Transformer\nhttps://spaces.ac.cn/archives/8620<br>\n10https://theaisummer.com/self-attention/\nhttps://theaisummer.com/self-attention/<br>\n11https://zhuanlan.zhihu.com/p/626820422\nhttps://zhuanlan.zhihu.com/p/626820422<br>\n12Are Sixteen Heads Really Better than One?\nhttps://arxiv.org/pdf/1905.10650.pdf<br>\n13This post is all you needTransformer\nhttps://zhuanlan.zhihu.com/p/420820453<br>\n14The Illustrated Transformer\nhttps://jalammar.github.io/illustrated-transformer/<br>\n15Multi-Query Attention is All You Need\nhttps://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055</p>\n"}],"PostAsset":[{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi.png","slug":"meta_pi.png","post":"clv7wdj0j0003794kdn9qe4rt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi_explanation.png","slug":"meta_pi_explanation.png","post":"clv7wdj0j0003794kdn9qe4rt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi_nosft.png","slug":"meta_pi_nosft.png","post":"clv7wdj0j0003794kdn9qe4rt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_rope_ext.png","slug":"meta_rope_ext.png","post":"clv7wdj0j0003794kdn9qe4rt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/mix_precision_fp16.png","slug":"mix_precision_fp16.png","post":"clv7wdj0j0003794kdn9qe4rt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/rope_matrix.png","slug":"rope_matrix.png","post":"clv7wdj0j0003794kdn9qe4rt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/complex_number.png","slug":"complex_number.png","post":"clv7wdj0h0001794k723j4p9n","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/remote_attenuation.png","slug":"remote_attenuation.png","post":"clv7wdj0h0001794k723j4p9n","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/rope.png","slug":"rope.png","post":"clv7wdj0h0001794k723j4p9n","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/bn_and_ln.png","slug":"bn_and_ln.png","post":"clv7wdj0m0007794k8auaeubi","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/cv_batchnorm.png","slug":"cv_batchnorm.png","post":"clv7wdj0m0007794k8auaeubi","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/cv_layernorm.jpeg","slug":"cv_layernorm.jpeg","post":"clv7wdj0m0007794k8auaeubi","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/norm_in_nlp.png","slug":"norm_in_nlp.png","post":"clv7wdj0m0007794k8auaeubi","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/04/-4/transformer.png","slug":"transformer.png","post":"clv7wdj0p000c794k57zc8pi2","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/9B.png","slug":"9B.png","post":"clv7wdj0q000g794k9qwk2stf","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/base_model_eval.png","slug":"base_model_eval.png","post":"clv7wdj0q000g794k9qwk2stf","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/cover.png","slug":"cover.png","post":"clv7wdj0q000g794k9qwk2stf","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/eval.png","slug":"eval.png","post":"clv7wdj0q000g794k9qwk2stf","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/ict.png","slug":"ict.png","post":"clv7wdj0q000g794k9qwk2stf","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/long_context_result.png","slug":"long_context_result.png","post":"clv7wdj0q000g794k9qwk2stf","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/model.png","slug":"model.png","post":"clv7wdj0q000g794k9qwk2stf","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/multimodal.png","slug":"multimodal.png","post":"clv7wdj0q000g794k9qwk2stf","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/perf.png","slug":"perf.png","post":"clv7wdj0q000g794k9qwk2stf","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/pretrain_data_dist.png","slug":"pretrain_data_dist.png","post":"clv7wdj0q000g794k9qwk2stf","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/pretrain_data_pipeline.png","slug":"pretrain_data_pipeline.png","post":"clv7wdj0q000g794k9qwk2stf","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/sft.png","slug":"sft.png","post":"clv7wdj0q000g794k9qwk2stf","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/third_party.png","slug":"third_party.png","post":"clv7wdj0q000g794k9qwk2stf","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/big_bird_attention.png","slug":"big_bird_attention.png","post":"clv7wdj0p000d794khco2341a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/dilated_conv.png","slug":"dilated_conv.png","post":"clv7wdj0p000d794khco2341a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/longformer_attention.png","slug":"longformer_attention.png","post":"clv7wdj0p000d794khco2341a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_architechture.png","slug":"mistral_architechture.png","post":"clv7wdj0p000d794khco2341a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_large_performance.jpeg","slug":"mistral_large_performance.jpeg","post":"clv7wdj0p000d794khco2341a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_perf.png","slug":"mistral_perf.png","post":"clv7wdj0p000d794khco2341a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_swa.png","slug":"mistral_swa.png","post":"clv7wdj0p000d794khco2341a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/ms_invest_mistral.png","slug":"ms_invest_mistral.png","post":"clv7wdj0p000d794khco2341a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/prefill_and_chunking.png","slug":"prefill_and_chunking.png","post":"clv7wdj0p000d794khco2341a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/receptive_field_cnn.png","slug":"receptive_field_cnn.png","post":"clv7wdj0p000d794khco2341a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/rolling_buffer.png","slug":"rolling_buffer.png","post":"clv7wdj0p000d794khco2341a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/1.png","slug":"1.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/10.png","slug":"10.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/11.png","slug":"11.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/12.png","slug":"12.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/13.png","slug":"13.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/14.png","slug":"14.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/15.png","slug":"15.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/16.png","slug":"16.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/17.png","slug":"17.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/18.png","slug":"18.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/19.png","slug":"19.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/2.png","slug":"2.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/20.png","slug":"20.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/21.png","slug":"21.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/22.png","slug":"22.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/23.png","slug":"23.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/24.png","slug":"24.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/25.png","slug":"25.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/26.png","slug":"26.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/27.png","slug":"27.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/28.png","slug":"28.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/3.png","slug":"3.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/4.png","slug":"4.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/5.png","slug":"5.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/6.png","slug":"6.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/7.png","slug":"7.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/8.png","slug":"8.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/9.png","slug":"9.png","post":"clv7wdj0s000o794k2b3kbhtn","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_algo.png","slug":"bn_algo.png","post":"clv7wdj0r000k794k5owmantc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_and_ln.png","slug":"bn_and_ln.png","post":"clv7wdj0r000k794k5owmantc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_ics.png","slug":"bn_ics.png","post":"clv7wdj0r000k794k5owmantc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_ln_gn_in.png","slug":"bn_ln_gn_in.png","post":"clv7wdj0r000k794k5owmantc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bs_bn.png","slug":"bs_bn.png","post":"clv7wdj0r000k794k5owmantc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/deepnorm.png","slug":"deepnorm.png","post":"clv7wdj0r000k794k5owmantc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/deepnorm_result.png","slug":"deepnorm_result.png","post":"clv7wdj0r000k794k5owmantc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ellipse_1.png","slug":"ellipse_1.png","post":"clv7wdj0r000k794k5owmantc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ellipse_2.png","slug":"ellipse_2.png","post":"clv7wdj0r000k794k5owmantc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ics_define.png","slug":"ics_define.png","post":"clv7wdj0r000k794k5owmantc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ics_measure.png","slug":"ics_measure.png","post":"clv7wdj0r000k794k5owmantc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/lossfunc_surface.jpeg","slug":"lossfunc_surface.jpeg","post":"clv7wdj0r000k794k5owmantc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/postnorm_prenorm.png","slug":"postnorm_prenorm.png","post":"clv7wdj0r000k794k5owmantc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/prmsnorm.png","slug":"prmsnorm.png","post":"clv7wdj0r000k794k5owmantc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/realformer.png","slug":"realformer.png","post":"clv7wdj0r000k794k5owmantc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/realformer_attention.png","slug":"realformer_attention.png","post":"clv7wdj0r000k794k5owmantc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/rmsnorm.png","slug":"rmsnorm.png","post":"clv7wdj0r000k794k5owmantc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/rmsnorm_eff.png","slug":"rmsnorm_eff.png","post":"clv7wdj0r000k794k5owmantc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/sigmoid.png","slug":"sigmoid.png","post":"clv7wdj0r000k794k5owmantc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/warmup_effect.png","slug":"warmup_effect.png","post":"clv7wdj0r000k794k5owmantc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/GQA.png","slug":"GQA.png","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/GQA_result_1.png","slug":"GQA_result_1.png","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/MQA.png","slug":"MQA.png","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/MQA.webp","slug":"MQA.webp","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Markdown _  Nice.html","slug":"Markdown _  Nice.html","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Scaled-dot-product-self-attention.pbm","slug":"Scaled-dot-product-self-attention.pbm","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Scaled-dot-product-self-attention.png","slug":"Scaled-dot-product-self-attention.png","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/attention_calculation.png","slug":"attention_calculation.png","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/cnn_heatmap.png","slug":"cnn_heatmap.png","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/decoder.png","slug":"decoder.png","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/encoder.png","slug":"encoder.png","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/gpu_cache.png","slug":"gpu_cache.png","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/lihongyi_self_attention.png","slug":"lihongyi_self_attention.png","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/llama2_qga.png","slug":"llama2_qga.png","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/mqa_result_1.png","slug":"mqa_result_1.png","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/mqa_result_3.png","slug":"mqa_result_3.png","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/multihead_attention.png","slug":"multihead_attention.png","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/seq2seq.png","slug":"seq2seq.png","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/seq2seq_attention.png","slug":"seq2seq_attention.png","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/softmax.png","slug":"softmax.png","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/sram_dram.png","slug":"sram_dram.png","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/transformer_structure.png","slug":"transformer_structure.png","post":"clv7wdj11004t794k0h03gngv","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_145b.png","slug":"ds_moe_145b.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_ablation.png","slug":"ds_moe_ablation.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_comparison.png","slug":"ds_moe_comparison.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_expert_specialization.png","slug":"ds_moe_expert_specialization.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_less_activated_expert.png","slug":"ds_moe_less_activated_expert.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_perf.png","slug":"ds_moe_perf.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_sft.png","slug":"ds_moe_sft.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_structure.png","slug":"ds_moe_structure.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_upper_bound_13b.png","slug":"ds_moe_upper_bound_13b.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_upper_bound_2b.png","slug":"ds_moe_upper_bound_2b.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_compare_gpt3.png","slug":"glam_compare_gpt3.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_compare_gpt3_2.png","slug":"glam_compare_gpt3_2.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_family.png","slug":"glam_family.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_model.png","slug":"glam_model.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_perf.png","slug":"glam_perf.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_algo_1.png","slug":"gshard_algo_1.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_model.png","slug":"gshard_model.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_moe_family.png","slug":"gshard_moe_family.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_result.png","slug":"gshard_result.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/modular_connectionist.png","slug":"modular_connectionist.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe.png","slug":"rnn_moe.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_137b.png","slug":"rnn_moe_137b.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_hierarchical_gating.png","slug":"rnn_moe_hierarchical_gating.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_load_function.png","slug":"rnn_moe_load_function.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_perf.png","slug":"rnn_moe_perf.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_specilized.png","slug":"rnn_moe_specilized.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/softplus.png","slug":"softplus.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_capacity_effect.png","slug":"switch_transformer_capacity_effect.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_diff_expert_capacity.png","slug":"switch_transformer_diff_expert_capacity.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_distill.png","slug":"switch_transformer_distill.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_distill_diff_model.png","slug":"switch_transformer_distill_diff_model.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_distill_sft.png","slug":"switch_transformer_distill_sft.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_dropout.png","slug":"switch_transformer_dropout.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_init.png","slug":"switch_transformer_init.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_pretrain_result.png","slug":"switch_transformer_pretrain_result.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_scaling_dense.png","slug":"switch_transformer_scaling_dense.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_scaling_step.png","slug":"switch_transformer_scaling_step.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_scaling_time.png","slug":"switch_transformer_scaling_time.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_sft_result.png","slug":"switch_transformer_sft_result.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_structure.png","slug":"switch_transformer_structure.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/vanilla_moe.png","slug":"vanilla_moe.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/vanilla_moe_result.png","slug":"vanilla_moe_result.png","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/xiaomi_moe.jpg","slug":"xiaomi_moe.jpg","post":"clv7wdj11004s794k528u0tt7","modified":0,"renderable":0}],"PostCategory":[{"post_id":"clv7wdj0r000k794k5owmantc","category_id":"clv7wdj0k0004794kh75tf46r","_id":"clv7wdj0v001g794k13okflvn"},{"post_id":"clv7wdj0r000k794k5owmantc","category_id":"clv7wdj0q000i794k1vl9636s","_id":"clv7wdj0v001h794k3vi094ni"},{"post_id":"clv7wdj0r000k794k5owmantc","category_id":"clv7wdj0u0016794kbxxmbbg6","_id":"clv7wdj0v001k794k5ctv2ujq"},{"post_id":"clv7wdj0m0008794kcrdcgj7d","category_id":"clv7wdj0k0004794kh75tf46r","_id":"clv7wdj0v001l794kf6p39ckw"},{"post_id":"clv7wdj0m0008794kcrdcgj7d","category_id":"clv7wdj0q000i794k1vl9636s","_id":"clv7wdj0v001o794kfn230gld"},{"post_id":"clv7wdj0m0008794kcrdcgj7d","category_id":"clv7wdj0u0016794kbxxmbbg6","_id":"clv7wdj0v001p794kesng1xwy"},{"post_id":"clv7wdj0r000l794kbztk6piz","category_id":"clv7wdj0k0004794kh75tf46r","_id":"clv7wdj0v001s794k7gcd9x98"},{"post_id":"clv7wdj0r000l794kbztk6piz","category_id":"clv7wdj0q000i794k1vl9636s","_id":"clv7wdj0v001t794k20nj79kn"},{"post_id":"clv7wdj0r000l794kbztk6piz","category_id":"clv7wdj0u0016794kbxxmbbg6","_id":"clv7wdj0w001w794k06oe9zp5"},{"post_id":"clv7wdj0s000o794k2b3kbhtn","category_id":"clv7wdj0k0004794kh75tf46r","_id":"clv7wdj0w001y794k2qj3cty8"},{"post_id":"clv7wdj0s000o794k2b3kbhtn","category_id":"clv7wdj0q000i794k1vl9636s","_id":"clv7wdj0w0022794k1il53287"},{"post_id":"clv7wdj0s000o794k2b3kbhtn","category_id":"clv7wdj0u0016794kbxxmbbg6","_id":"clv7wdj0w0024794k5kwg1wbg"},{"post_id":"clv7wdj0h0001794k723j4p9n","category_id":"clv7wdj0k0004794kh75tf46r","_id":"clv7wdj0w0028794khb4e88h0"},{"post_id":"clv7wdj0h0001794k723j4p9n","category_id":"clv7wdj0q000i794k1vl9636s","_id":"clv7wdj0w002a794kca596i1h"},{"post_id":"clv7wdj0h0001794k723j4p9n","category_id":"clv7wdj0u0016794kbxxmbbg6","_id":"clv7wdj0w002e794kcb8adje0"},{"post_id":"clv7wdj0n0009794kh92k0fxj","category_id":"clv7wdj0k0004794kh75tf46r","_id":"clv7wdj0w002f794k937f3z2o"},{"post_id":"clv7wdj0n0009794kh92k0fxj","category_id":"clv7wdj0q000i794k1vl9636s","_id":"clv7wdj0w002h794k00jfb1t5"},{"post_id":"clv7wdj0n0009794kh92k0fxj","category_id":"clv7wdj0u0016794kbxxmbbg6","_id":"clv7wdj0x002j794k7qrl7ep8"},{"post_id":"clv7wdj0j0003794kdn9qe4rt","category_id":"clv7wdj0k0004794kh75tf46r","_id":"clv7wdj0x002k794k12t14n66"},{"post_id":"clv7wdj0j0003794kdn9qe4rt","category_id":"clv7wdj0q000i794k1vl9636s","_id":"clv7wdj0x002m794k3ybj1l21"},{"post_id":"clv7wdj0j0003794kdn9qe4rt","category_id":"clv7wdj0u0016794kbxxmbbg6","_id":"clv7wdj0x002o794k2hk70rta"},{"post_id":"clv7wdj0p000d794khco2341a","category_id":"clv7wdj0k0004794kh75tf46r","_id":"clv7wdj0x002r794k3n7f2qp7"},{"post_id":"clv7wdj0p000d794khco2341a","category_id":"clv7wdj0q000i794k1vl9636s","_id":"clv7wdj0x002t794k6zqg2pii"},{"post_id":"clv7wdj0p000d794khco2341a","category_id":"clv7wdj0u0016794kbxxmbbg6","_id":"clv7wdj0x002w794k5ffgdkcn"},{"post_id":"clv7wdj0q000g794k9qwk2stf","category_id":"clv7wdj0k0004794kh75tf46r","_id":"clv7wdj0x002y794k28zkc989"},{"post_id":"clv7wdj0q000g794k9qwk2stf","category_id":"clv7wdj0q000i794k1vl9636s","_id":"clv7wdj0x0031794k3bk13za6"},{"post_id":"clv7wdj0q000g794k9qwk2stf","category_id":"clv7wdj0u0016794kbxxmbbg6","_id":"clv7wdj0x0033794k9iw54ku5"},{"post_id":"clv7wdj0m0007794k8auaeubi","category_id":"clv7wdj0k0004794kh75tf46r","_id":"clv7wdj0x0036794kgzp6gfj8"},{"post_id":"clv7wdj0m0007794k8auaeubi","category_id":"clv7wdj0q000i794k1vl9636s","_id":"clv7wdj0x0038794k3dcf9abf"},{"post_id":"clv7wdj0m0007794k8auaeubi","category_id":"clv7wdj0u0016794kbxxmbbg6","_id":"clv7wdj0x003a794kcphpea44"},{"post_id":"clv7wdj0q000h794k2fip10h4","category_id":"clv7wdj0k0004794kh75tf46r","_id":"clv7wdj0x003b794k41650r3d"},{"post_id":"clv7wdj0q000h794k2fip10h4","category_id":"clv7wdj0q000i794k1vl9636s","_id":"clv7wdj0y003c794kfgy29ug1"},{"post_id":"clv7wdj0q000h794k2fip10h4","category_id":"clv7wdj0u0016794kbxxmbbg6","_id":"clv7wdj0y003f794kd0o156st"},{"post_id":"clv7wdj11004s794k528u0tt7","category_id":"clv7wdj0k0004794kh75tf46r","_id":"clv7wdj12004v794k3snacg08"},{"post_id":"clv7wdj11004s794k528u0tt7","category_id":"clv7wdj0q000i794k1vl9636s","_id":"clv7wdj12004x794kgriu6e32"},{"post_id":"clv7wdj11004s794k528u0tt7","category_id":"clv7wdj0u0016794kbxxmbbg6","_id":"clv7wdj12004z794k11wigd4e"},{"post_id":"clv7wdj11004t794k0h03gngv","category_id":"clv7wdj0k0004794kh75tf46r","_id":"clv7wdj120051794k6l7f1bhc"},{"post_id":"clv7wdj11004t794k0h03gngv","category_id":"clv7wdj0q000i794k1vl9636s","_id":"clv7wdj120053794ke86i97kb"},{"post_id":"clv7wdj11004t794k0h03gngv","category_id":"clv7wdj0u0016794kbxxmbbg6","_id":"clv7wdj120055794k40o9gw8q"}],"PostTag":[{"post_id":"clv7wdj0h0001794k723j4p9n","tag_id":"clv7wdj0l0005794k93mk1l95","_id":"clv7wdj0s000r794k63b4ccbc"},{"post_id":"clv7wdj0h0001794k723j4p9n","tag_id":"clv7wdj0n000b794k4zts9vvq","_id":"clv7wdj0t000t794k574rds0h"},{"post_id":"clv7wdj0h0001794k723j4p9n","tag_id":"clv7wdj0q000f794kaxpm8nsj","_id":"clv7wdj0t000v794kg5lv0p44"},{"post_id":"clv7wdj0h0001794k723j4p9n","tag_id":"clv7wdj0r000j794kcjcs8ipr","_id":"clv7wdj0t000x794k3uha82zj"},{"post_id":"clv7wdj0h0001794k723j4p9n","tag_id":"clv7wdj0s000n794k7tv6bqb5","_id":"clv7wdj0t000z794k82bl2yh2"},{"post_id":"clv7wdj0j0003794kdn9qe4rt","tag_id":"clv7wdj0l0005794k93mk1l95","_id":"clv7wdj0u0017794k1vwa4c8a"},{"post_id":"clv7wdj0j0003794kdn9qe4rt","tag_id":"clv7wdj0n000b794k4zts9vvq","_id":"clv7wdj0u0018794kerkm6tvw"},{"post_id":"clv7wdj0j0003794kdn9qe4rt","tag_id":"clv7wdj0q000f794kaxpm8nsj","_id":"clv7wdj0u001a794kb0oodyxp"},{"post_id":"clv7wdj0j0003794kdn9qe4rt","tag_id":"clv7wdj0t0011794keozf5kr0","_id":"clv7wdj0u001c794k2c739jec"},{"post_id":"clv7wdj0j0003794kdn9qe4rt","tag_id":"clv7wdj0u0013794k5cktbxi8","_id":"clv7wdj0v001e794keqjqfocc"},{"post_id":"clv7wdj0m0007794k8auaeubi","tag_id":"clv7wdj0l0005794k93mk1l95","_id":"clv7wdj0w001x794k8z1y0lsc"},{"post_id":"clv7wdj0m0007794k8auaeubi","tag_id":"clv7wdj0n000b794k4zts9vvq","_id":"clv7wdj0w001z794kfkl9e1mk"},{"post_id":"clv7wdj0m0007794k8auaeubi","tag_id":"clv7wdj0q000f794kaxpm8nsj","_id":"clv7wdj0w0023794k3p2c4828"},{"post_id":"clv7wdj0m0007794k8auaeubi","tag_id":"clv7wdj0v001i794k11yr603z","_id":"clv7wdj0w0025794kfqdae6k4"},{"post_id":"clv7wdj0m0007794k8auaeubi","tag_id":"clv7wdj0v001m794k5himbnmi","_id":"clv7wdj0w0029794kf3ts9hdv"},{"post_id":"clv7wdj0m0007794k8auaeubi","tag_id":"clv7wdj0v001q794kd0q0apoi","_id":"clv7wdj0w002b794kaaad9u09"},{"post_id":"clv7wdj0m0008794kcrdcgj7d","tag_id":"clv7wdj0l0005794k93mk1l95","_id":"clv7wdj0x002n794k2z3s9evg"},{"post_id":"clv7wdj0m0008794kcrdcgj7d","tag_id":"clv7wdj0n000b794k4zts9vvq","_id":"clv7wdj0x002p794kgx8q5qp3"},{"post_id":"clv7wdj0m0008794kcrdcgj7d","tag_id":"clv7wdj0q000f794kaxpm8nsj","_id":"clv7wdj0x002s794k6les93td"},{"post_id":"clv7wdj0m0008794kcrdcgj7d","tag_id":"clv7wdj0t0011794keozf5kr0","_id":"clv7wdj0x002u794kd8ad534r"},{"post_id":"clv7wdj0m0008794kcrdcgj7d","tag_id":"clv7wdj0w002c794k6csh732d","_id":"clv7wdj0x002x794k1fi574g4"},{"post_id":"clv7wdj0m0008794kcrdcgj7d","tag_id":"clv7wdj0w002i794k1h70445q","_id":"clv7wdj0x002z794k7ksu1kle"},{"post_id":"clv7wdj0n0009794kh92k0fxj","tag_id":"clv7wdj0l0005794k93mk1l95","_id":"clv7wdj0x0032794k5a8v33oj"},{"post_id":"clv7wdj0n0009794kh92k0fxj","tag_id":"clv7wdj0n000b794k4zts9vvq","_id":"clv7wdj0x0034794k0gew4bw6"},{"post_id":"clv7wdj0n0009794kh92k0fxj","tag_id":"clv7wdj0x002q794kc02t6es7","_id":"clv7wdj0x0037794k1qf5d3r4"},{"post_id":"clv7wdj0p000d794khco2341a","tag_id":"clv7wdj0l0005794k93mk1l95","_id":"clv7wdj0y003e794kez91bes0"},{"post_id":"clv7wdj0p000d794khco2341a","tag_id":"clv7wdj0n000b794k4zts9vvq","_id":"clv7wdj0y003g794khfl8dvqy"},{"post_id":"clv7wdj0p000d794khco2341a","tag_id":"clv7wdj0q000f794kaxpm8nsj","_id":"clv7wdj0y003i794k7pjl27ly"},{"post_id":"clv7wdj0p000d794khco2341a","tag_id":"clv7wdj0w002i794k1h70445q","_id":"clv7wdj0y003j794k3l0ieksf"},{"post_id":"clv7wdj0p000d794khco2341a","tag_id":"clv7wdj0x0035794k8tv24fw6","_id":"clv7wdj0y003l794kapzgezds"},{"post_id":"clv7wdj0p000d794khco2341a","tag_id":"clv7wdj0x0039794kayb0bkde","_id":"clv7wdj0y003m794kdf7xfeln"},{"post_id":"clv7wdj0q000g794k9qwk2stf","tag_id":"clv7wdj0l0005794k93mk1l95","_id":"clv7wdj0y003o794k468o1vo4"},{"post_id":"clv7wdj0q000g794k9qwk2stf","tag_id":"clv7wdj0n000b794k4zts9vvq","_id":"clv7wdj0y003p794k6k2uc5rs"},{"post_id":"clv7wdj0q000g794k9qwk2stf","tag_id":"clv7wdj0q000f794kaxpm8nsj","_id":"clv7wdj0y003r794k3hju7r56"},{"post_id":"clv7wdj0q000g794k9qwk2stf","tag_id":"clv7wdj0y003d794keaby2vel","_id":"clv7wdj0y003s794k0n0qcqnw"},{"post_id":"clv7wdj0q000g794k9qwk2stf","tag_id":"clv7wdj0y003h794kdwbt1eqb","_id":"clv7wdj0y003u794kbuby3d2h"},{"post_id":"clv7wdj0q000g794k9qwk2stf","tag_id":"clv7wdj0t0011794keozf5kr0","_id":"clv7wdj0y003v794kdi0vgkyf"},{"post_id":"clv7wdj0q000h794k2fip10h4","tag_id":"clv7wdj0l0005794k93mk1l95","_id":"clv7wdj0y003x794k3j221v23"},{"post_id":"clv7wdj0q000h794k2fip10h4","tag_id":"clv7wdj0n000b794k4zts9vvq","_id":"clv7wdj0y003y794k4oi9bzb4"},{"post_id":"clv7wdj0q000h794k2fip10h4","tag_id":"clv7wdj0x002q794kc02t6es7","_id":"clv7wdj0z0040794k74lu9ktg"},{"post_id":"clv7wdj0r000k794k5owmantc","tag_id":"clv7wdj0l0005794k93mk1l95","_id":"clv7wdj0z0043794k6c700zsc"},{"post_id":"clv7wdj0r000k794k5owmantc","tag_id":"clv7wdj0n000b794k4zts9vvq","_id":"clv7wdj0z0044794k2la71z9j"},{"post_id":"clv7wdj0r000k794k5owmantc","tag_id":"clv7wdj0q000f794kaxpm8nsj","_id":"clv7wdj0z0046794kepzkg9dm"},{"post_id":"clv7wdj0r000k794k5owmantc","tag_id":"clv7wdj0v001i794k11yr603z","_id":"clv7wdj0z0047794k31hlbbt7"},{"post_id":"clv7wdj0r000k794k5owmantc","tag_id":"clv7wdj0y003t794k8gwr9an7","_id":"clv7wdj0z0049794k425ablb2"},{"post_id":"clv7wdj0r000k794k5owmantc","tag_id":"clv7wdj0y003w794k33rw7ylk","_id":"clv7wdj0z004a794k3fgydwdi"},{"post_id":"clv7wdj0r000k794k5owmantc","tag_id":"clv7wdj0v001m794k5himbnmi","_id":"clv7wdj0z004c794k03ql2rvx"},{"post_id":"clv7wdj0r000k794k5owmantc","tag_id":"clv7wdj0v001q794kd0q0apoi","_id":"clv7wdj0z004d794k4odihyqf"},{"post_id":"clv7wdj0r000l794kbztk6piz","tag_id":"clv7wdj0l0005794k93mk1l95","_id":"clv7wdj10004f794khstv3ac1"},{"post_id":"clv7wdj0r000l794kbztk6piz","tag_id":"clv7wdj0n000b794k4zts9vvq","_id":"clv7wdj10004g794k3213d9u2"},{"post_id":"clv7wdj0r000l794kbztk6piz","tag_id":"clv7wdj0x002q794kc02t6es7","_id":"clv7wdj10004h794k6fco4vg3"},{"post_id":"clv7wdj0s000o794k2b3kbhtn","tag_id":"clv7wdj0l0005794k93mk1l95","_id":"clv7wdj10004k794kaglhfiff"},{"post_id":"clv7wdj0s000o794k2b3kbhtn","tag_id":"clv7wdj0n000b794k4zts9vvq","_id":"clv7wdj10004l794kg4nzeuwn"},{"post_id":"clv7wdj0s000o794k2b3kbhtn","tag_id":"clv7wdj0z0045794k45hr1bzv","_id":"clv7wdj10004m794kc1uuh0l8"},{"post_id":"clv7wdj0s000o794k2b3kbhtn","tag_id":"clv7wdj0z0048794k4d3wfn7g","_id":"clv7wdj10004n794k7brgcejk"},{"post_id":"clv7wdj0s000o794k2b3kbhtn","tag_id":"clv7wdj0z004b794kdfzfer5q","_id":"clv7wdj10004o794kfyoe64zw"},{"post_id":"clv7wdj0s000o794k2b3kbhtn","tag_id":"clv7wdj0z004e794kahjgbpu4","_id":"clv7wdj10004p794kfhy3ejj6"},{"post_id":"clv7wdj0s000o794k2b3kbhtn","tag_id":"clv7wdj10004i794k2ld8dpoi","_id":"clv7wdj10004q794k1zgo27ja"},{"post_id":"clv7wdj0s000o794k2b3kbhtn","tag_id":"clv7wdj10004j794ka99z43mj","_id":"clv7wdj10004r794k1kwo8w4o"},{"post_id":"clv7wdj11004s794k528u0tt7","tag_id":"clv7wdj0l0005794k93mk1l95","_id":"clv7wdj12004y794k7zsp1q3b"},{"post_id":"clv7wdj11004s794k528u0tt7","tag_id":"clv7wdj0n000b794k4zts9vvq","_id":"clv7wdj120050794kfzpyam53"},{"post_id":"clv7wdj11004s794k528u0tt7","tag_id":"clv7wdj0q000f794kaxpm8nsj","_id":"clv7wdj120052794ke45f4uqu"},{"post_id":"clv7wdj11004s794k528u0tt7","tag_id":"clv7wdj12004u794kh5iz67hs","_id":"clv7wdj120054794kahit1jo6"},{"post_id":"clv7wdj11004t794k0h03gngv","tag_id":"clv7wdj0l0005794k93mk1l95","_id":"clv7wdj120056794k6mij632w"},{"post_id":"clv7wdj11004t794k0h03gngv","tag_id":"clv7wdj0n000b794k4zts9vvq","_id":"clv7wdj120057794k6cl274ct"},{"post_id":"clv7wdj11004t794k0h03gngv","tag_id":"clv7wdj0q000f794kaxpm8nsj","_id":"clv7wdj120058794k3th530w2"},{"post_id":"clv7wdj11004t794k0h03gngv","tag_id":"clv7wdj0w002i794k1h70445q","_id":"clv7wdj120059794k9st00z4b"},{"post_id":"clv7wdj11004t794k0h03gngv","tag_id":"clv7wdj12004w794kb05wdb6e","_id":"clv7wdj12005a794khv1a9hbf"}],"Tag":[{"name":"NLP","_id":"clv7wdj0l0005794k93mk1l95"},{"name":"LLM","_id":"clv7wdj0n000b794k4zts9vvq"},{"name":"transformer","_id":"clv7wdj0q000f794kaxpm8nsj"},{"name":"positional encoding","_id":"clv7wdj0r000j794kcjcs8ipr"},{"name":"RoPE","_id":"clv7wdj0s000n794k7tv6bqb5"},{"name":"","_id":"clv7wdj0t0011794keozf5kr0"},{"name":"","_id":"clv7wdj0u0013794k5cktbxi8"},{"name":"layernorm","_id":"clv7wdj0v001i794k11yr603z"},{"name":"normalization","_id":"clv7wdj0v001m794k5himbnmi"},{"name":"batchnorm","_id":"clv7wdj0v001q794kd0q0apoi"},{"name":"","_id":"clv7wdj0w002c794k6csh732d"},{"name":"attention","_id":"clv7wdj0w002i794k1h70445q"},{"name":"","_id":"clv7wdj0x002q794kc02t6es7"},{"name":"sliding window attention","_id":"clv7wdj0x0035794k8tv24fw6"},{"name":"sparse attention","_id":"clv7wdj0x0039794kayb0bkde"},{"name":"","_id":"clv7wdj0y003d794keaby2vel"},{"name":"","_id":"clv7wdj0y003h794kdwbt1eqb"},{"name":"post-norm","_id":"clv7wdj0y003t794k8gwr9an7"},{"name":"pre-norm","_id":"clv7wdj0y003w794k33rw7ylk"},{"name":"ChatGPT","_id":"clv7wdj0z0045794k45hr1bzv"},{"name":"Sparrow","_id":"clv7wdj0z0048794k4d3wfn7g"},{"name":"LaMDA","_id":"clv7wdj0z004b794kdfzfer5q"},{"name":"GopherCite","_id":"clv7wdj0z004e794kahjgbpu4"},{"name":"WebGPT","_id":"clv7wdj10004i794k2ld8dpoi"},{"name":"InstructGPT","_id":"clv7wdj10004j794ka99z43mj"},{"name":"MoE","_id":"clv7wdj12004u794kh5iz67hs"},{"name":"KV Cache","_id":"clv7wdj12004w794kb05wdb6e"}]}}