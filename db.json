{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","path":"css/noscript.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","path":"images/logo-algolia-nebula-blue-full.svg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","path":"js/bookmark.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","path":"js/comments-buttons.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","path":"js/comments.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/config.js","path":"js/config.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","path":"js/motion.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","path":"js/next-boot.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","path":"js/pjax.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","path":"js/schedule.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/addtoany.js","path":"js/third-party/addtoany.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/fancybox.js","path":"js/third-party/fancybox.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/pace.js","path":"js/third-party/pace.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","path":"js/third-party/quicklink.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","path":"js/third-party/analytics/baidu-analytics.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","path":"js/third-party/analytics/google-analytics.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","path":"js/third-party/analytics/growingio.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/matomo.js","path":"js/third-party/analytics/matomo.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","path":"js/third-party/chat/chatra.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","path":"js/third-party/chat/tidio.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","path":"js/third-party/comments/changyan.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","path":"js/third-party/comments/disqus.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","path":"js/third-party/comments/disqusjs.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","path":"js/third-party/comments/gitalk.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","path":"js/third-party/comments/isso.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","path":"js/third-party/comments/livere.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","path":"js/third-party/comments/utterances.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","path":"js/third-party/math/katex.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","path":"js/third-party/math/mathjax.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","path":"js/third-party/search/algolia-search.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","path":"js/third-party/search/local-search.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","path":"js/third-party/statistics/firestore.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","path":"js/third-party/statistics/lean-analytics.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","path":"js/third-party/tags/mermaid.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","path":"js/third-party/tags/pdf.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/wavedrom.js","path":"js/third-party/tags/wavedrom.js","modified":1,"renderable":1},{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"source/images/qrcode.jpg","path":"images/qrcode.jpg","modified":1,"renderable":0},{"_id":"source/images/background/wallhaven-gpxpg3.png","path":"images/background/wallhaven-gpxpg3.png","modified":1,"renderable":0},{"_id":"source/images/background/wallhaven-p97q73.png","path":"images/background/wallhaven-p97q73.png","modified":1,"renderable":0},{"_id":"source/images/background/wallhaven-2ywymm.png","path":"images/background/wallhaven-2ywymm.png","modified":1,"renderable":0},{"_id":"source/images/background/wallhaven-x636oz.png","path":"images/background/wallhaven-x636oz.png","modified":1,"renderable":0},{"_id":"source/images/avatar/20180303210737_XsJVr.jpeg","path":"images/avatar/20180303210737_XsJVr.jpeg","modified":1,"renderable":0},{"_id":"source/images/avatar/Picasso_Elephant.png","path":"images/avatar/Picasso_Elephant.png","modified":1,"renderable":0},{"_id":"source/images/avatar/shadow.png","path":"images/avatar/shadow.png","modified":1,"renderable":0},{"_id":"source/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","path":"images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","modified":1,"renderable":0},{"_id":"source/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","path":"images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","modified":1,"renderable":0},{"_id":"source/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","path":"images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/about.txt","path":"images/favicon/favicon_io/about.txt","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/android-chrome-192x192.png","path":"images/favicon/favicon_io/android-chrome-192x192.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/apple-touch-icon.png","path":"images/favicon/favicon_io/apple-touch-icon.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon-16x16.png","path":"images/favicon/favicon_io/favicon-16x16.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon-32x32.png","path":"images/favicon/favicon_io/favicon-32x32.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon.ico","path":"images/favicon/favicon_io/favicon.ico","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/android-chrome-512x512.png","path":"images/favicon/favicon_io/android-chrome-512x512.png","modified":1,"renderable":0},{"_id":"source/images/favicon/favicon_io/site.webmanifest","path":"images/favicon/favicon_io/site.webmanifest","modified":1,"renderable":0}],"Cache":[{"_id":"node_modules/hexo-theme-next/.DS_Store","hash":"bef639ede199b5b3aea85285a9d03c82257c52aa","modified":1709026831096},{"_id":"node_modules/hexo-theme-next/_vendors.yml","hash":"4f6046ceb1470be9ff334ede20b73871c951d845","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/package.json","hash":"4b48877b223ec717e708540a2df03d64983c02ab","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/LICENSE.md","hash":"68fc9a03d50fd4b5ea97092b05967d1819dea2c4","modified":1706697684353},{"_id":"node_modules/hexo-theme-next/README.md","hash":"d6820f46d03a93bd6dc8b10f49f58aec82ad2b06","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/_config.yml","hash":"255c963c680da5da34c259c560dd8211b75188ca","modified":1708604632809},{"_id":"node_modules/hexo-theme-next/source/.DS_Store","hash":"20f7a9b9a682cc55305492b2e240489f6bf832e6","modified":1709026838425},{"_id":"node_modules/hexo-theme-next/languages/README.md","hash":"b2567e32805dda79601157351a07e5ca9fe01315","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/languages/ar.yml","hash":"7d0f39e8684284a04bb9808521c87fecda8bd131","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/languages/de.yml","hash":"79b37df731c29665dee6cd7c90d278e1edfb6e24","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/en.yml","hash":"ba0fd79a2b1d8db01a034180556061745965ff05","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/eo.yml","hash":"e34bb33ae827bf2f0727088599a73bc64bdad1b0","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/es.yml","hash":"dffc63ef42e1266b88e0acf08994fd17a9908d53","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/bn.yml","hash":"564bed75da6e05b11dce6164508f97a15e2fb6c2","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/fr.yml","hash":"8ac44e58f71a38b7697a2f7f98a6971ed818cb5b","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/it.yml","hash":"16d716ecfd748def2f6486ef5a82d0ab7ceb4890","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/fa.yml","hash":"f3ffc444599f4ac92d62e9ed00a1490ebc277d70","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/ja.yml","hash":"543222bfc516aab6c33e8534f807972ecb8943a9","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/ko.yml","hash":"d345a303310c8a5f4836c3683f3580f861ebd1b4","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/nl.yml","hash":"3cb3687696635ec71b4ca40c5fc43b56acc8843e","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/pt-BR.yml","hash":"76b8576ce228d540a16b1f0af5af2cce20923194","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/pt.yml","hash":"70de366e10ea584ba039d40d6b35ac97f93454ad","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/ru.yml","hash":"c6d8de0ff7d8148d09993257cfd3b7aca755696c","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/si.yml","hash":"2d712eedf3f60d04d36c3108cf5a12e2a52e875c","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/th.yml","hash":"6829e998b39f8f143e20b276bb1f62d95a29de58","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/tk.yml","hash":"511726054873f6f8d7ce0d2e803f6731de0ddbe7","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/id.yml","hash":"929df147f4f17d638b07de5fe52ca13e2549ab1c","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/tr.yml","hash":"a57e4ed089b893a95f5e1ecff17ce625165f4d46","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/uk.yml","hash":"ff537047b4b4c3ca9a7b64fa7f428a9942751eeb","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/zh-HK.yml","hash":"88ea50eeb9097ab4a87a44981a102d8594feb064","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/vi.yml","hash":"7ebcba5e1128784195e4681dffc9d34c4e873fec","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/zh-CN.yml","hash":"741d7efe0262c9cdc2c648014b55599665d90f6b","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/docs/LICENSE.txt","hash":"f5b14f791b7cfa1d16da981d929152e088a5d1b8","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/layout/_layout.njk","hash":"fc0a45112f2dcfc2642404e8934ea32a793c3bd7","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/archive.njk","hash":"d759f4d2cf5ddc6875ea250113a00662c1caf6d1","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/languages/zh-TW.yml","hash":"4695c87d6b81b3a23d16ad6513d9eaa925f8d8ad","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1706697684352},{"_id":"node_modules/hexo-theme-next/layout/category.njk","hash":"c68b7343d0f8145010f93351908cc36ef6212ec1","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/docs/AUTHORS.md","hash":"a648823121563c34a177ae91f5a774b5e29f01a0","modified":1706697684352},{"_id":"node_modules/hexo-theme-next/layout/index.njk","hash":"dd63e488ae8cc144335a5958acedf6a16edd7a92","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/post.njk","hash":"0bfce9f133f501a9a4837257e3b862b3bbca15be","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/page.njk","hash":"b0660b2af0ac7d3fda14ca4d9f2c9e79ef06c6f9","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/source/css/_mixins.styl","hash":"83647a6207333b9609ba90b0946b3fa9548e6381","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/layout/.DS_Store","hash":"91040d8017183ac4f7319d2d695edb07ec1b09c8","modified":1707031368282},{"_id":"node_modules/hexo-theme-next/source/css/_colors.styl","hash":"3c6798c10cc220d83481cb3f3782e78558cee789","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/layout/tag.njk","hash":"9e16ba20c28a7f2c6bc75aa427f48122301a30aa","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","hash":"dadc81256afb127b77eac6763d5ee0ec9c77f0a3","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/main.styl","hash":"921a58577f411cf4eb5cfd66db0a241f8f88578c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/scripts/filters/minify.js","hash":"447db39d17775b2bd18d8af9c9d65b7b8449f751","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/scripts/filters/locals.js","hash":"9eb5310664759931287dd28ea39165dfb67f12ed","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/scripts/events/index.js","hash":"bd9ea82376cd87df611ea3ae077875c7c595a3df","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/filters/default-injects.js","hash":"872f01cb10e422a648ea505436532e776e92926b","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/scripts/filters/post.js","hash":"fdc8a0af90035e89c3fcb754a0eb189b8951a2bc","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/scripts/helpers/engine.js","hash":"d292b78485e8e8055712b0ed6de7cf559c5fbdcd","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-paginator.js","hash":"e86c764b546e4fbb87970cabc4135a56f9ef9fe1","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-config.js","hash":"ead37e9167b682f1fa34b5401c3050e18c7ee4a3","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/helpers/font.js","hash":"3394185a7f0393c16ce52c8028f90da3e9239c55","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/scripts/helpers/navigation.js","hash":"78107021101553c3d23e89290f7530b60cf4aa86","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-url.js","hash":"6281d47c1de98eb38f3aa0f6df29bbb19d412173","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-vendors.js","hash":"957241c28796ff352de7f4cffba7bb289b043586","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1706697684330},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/source/css/.DS_Store","hash":"6b12ac4edf32b2194ccd6b95c3f5930b07c7d56b","modified":1709026838423},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","hash":"745bd828205da7e5fbd3f860cc3697097630f5f3","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","hash":"770d63c26f22705311028a36b52e999cc8a2da82","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/scripts/tags/button.js","hash":"c6ad2ed544fbb25ecb5d820c36e76302504271b7","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/scripts/tags/center-quote.js","hash":"92c19d796bdb3320df9caea59bf52df7a95d9da9","modified":1706697684337},{"_id":"node_modules/hexo-theme-next/scripts/tags/caniuse.js","hash":"935a311142a409c1896b3ae3f01fe7a9e2db1134","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/scripts/tags/group-pictures.js","hash":"9ed799c329abf830f623689d7e136991256a24ca","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/scripts/tags/index.js","hash":"1f6aba7820f1fb58b61969485148db21846e1aa9","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/tags/mermaid.js","hash":"4fb01ca650fa8b256b8d48f50dc1b18350bd3d6d","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/scripts/tags/note.js","hash":"7b94ddb46b7d4b0fe815f2fbe4bd375f07f55363","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/pdf.js","hash":"344636b6fd7e27e8831c1e194039afc0d61931cd","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/video.js","hash":"2ee926448583be8f95af1f2884ae2c9c4830151d","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/scripts/tags/tabs.js","hash":"0eabe51da40b4b13e16419c8fe02452d9a4fef73","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/scripts/tags/link-grid.js","hash":"18a483c2d5afd701f6080ffdddf2d1321370336c","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/scripts/tags/wavedrom.js","hash":"b44dfeeb58b41945d469141787f3dbce4b117d08","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","hash":"b03ba78c6916ad2f390d55bc1bc18fafb64b0ebf","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","hash":"d775148c2bf20c028622af5609a788167352bf1e","modified":1706697684350},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/README.md","hash":"12a3e96581964a22b474cc739675d52ef93ff932","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/docs/ru/README.md","hash":"29c89a41b371f893e56c87ea61adabc444ec58cc","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/layout/_macro/post-collapse.njk","hash":"abda600685ee972e1f6b7a2dcc56f13e2daa6263","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CONTRIBUTING.md","hash":"a089f7a8368ab0b7d7b9b7ec0ac3767a453435df","modified":1706697684353},{"_id":"node_modules/hexo-theme-next/scripts/tags/label.js","hash":"8a73348186113bae0a51ea2f891c1bb882fab05a","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/layout/_macro/sidebar.njk","hash":"547c62ab14d9e05d2d9116db9048a677fbe1fb6d","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_macro/post.njk","hash":"cbe208445e4d1df82ebd1761e1eaced3eab77fb3","modified":1706698899947},{"_id":"node_modules/hexo-theme-next/layout/_partials/pagination.njk","hash":"bc719473ed5948ab6859449d60b8d36cfc1542b4","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"12a6631617695504d5cf2a94b57d87bd331bef6f","modified":1706697684353},{"_id":"node_modules/hexo-theme-next/layout/_partials/footer.njk","hash":"d77ec95cfee58b17807763dc2adb7946829cb316","modified":1706757600094},{"_id":"node_modules/hexo-theme-next/layout/_third-party/addtoany.njk","hash":"ef64c6bfb8540cd874701236b9be47db2496e98e","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_third-party/fancybox.njk","hash":"844559f46e2ff1c8be234d5763703106e2072a7b","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/index.njk","hash":"dfd7cdd6ba89f8c3deabc27726c7a350cadafd11","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/pace.njk","hash":"d7ad5714079f7f65446f880baf14722435ca9061","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/quicklink.njk","hash":"0efed71ed530447718c4ea5bbd5fc8695b0b0d5f","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/.DS_Store","hash":"88e4de27e19e826f7296d295124581534c0c2c8b","modified":1707031373963},{"_id":"node_modules/hexo-theme-next/layout/_scripts/index.njk","hash":"6668878a0f9a1166c6a879755f54a08d942da870","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_scripts/vendors.njk","hash":"be80b9fe415a9a09d74c28e230995fd292dfc123","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_partials/widgets.njk","hash":"e7f988ecddb2159313699a00827a45eca5622bd4","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/css/_common/.DS_Store","hash":"71c6bca6ae43dd79b3d75183550713e9ad0f9f8e","modified":1709197037676},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/config.js","hash":"9ec51eb61f7fee612ffc5252f489003a0fa301fc","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Gemini.styl","hash":"96e0a7c2a65ce68215e17e369085b2ea2f1334f2","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Muse.styl","hash":"e3be898f5ebcf435a26542653a9297ff2c71aeb0","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Mist.styl","hash":"a1418c9dc8c0f1a0ad4ded0f4627c45bf0db1a10","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/highlight.js","hash":"6aec7b2c38c50989a23bfaa0d560e75c7f553e12","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/injects.js","hash":"d987709267a1bc6e5014411e9983d7c49c102c16","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/navigation.js","hash":"dd3562686d95a50375e6fd32e717ccb0d99c1e3d","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/vendors.js","hash":"464db1e7182e5b9cdbd32e8b5368d5e683b1d9c7","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/utils.js","hash":"6853e5433e3eaa19ea43fa20b08d956ba4cec4ac","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/changyan.js","hash":"5798cfc8f63665031dd3e01debed051628cec319","modified":1706697684338},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/common.js","hash":"19a402a225c31edffc50f202a14e0d582d3db23e","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/default-config.js","hash":"93ee5f9109dad885dc38c49bcee630c10f9dce6e","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqus.js","hash":"7f71d6b271ba65ff333d5682e7575711d368c0d2","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqusjs.js","hash":"a600a98e7436edeb31e291abca359885567df3c9","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/isso.js","hash":"ff8b5b5145220a17d0ecd9508ba9bd2d3b2da47d","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/gitalk.js","hash":"7bb7dafdd7f6bca8464b54e17e552ce7f1714195","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/utterances.js","hash":"d3bded697bc32dace689d2a6dfb6eb7514169d15","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/livere.js","hash":"5a07d8bb52bc1d51a624ca8db54be144566c306b","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/addtoany.js","hash":"5276c8f78ee562a8965216dc67d762e59cb4a9f2","modified":1706697684332},{"_id":"node_modules/hexo-theme-next/source/js/third-party/fancybox.js","hash":"819f382c561fe5ec23c67cc5fabd63dd1cc22dc1","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/source/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Pisces.styl","hash":"48f4f277946a168d0db1ea02804e85c22ca2c7db","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_variables/base.styl","hash":"c4fc4e862d09221265ab1466085f057be2ad2e4d","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","hash":"ba7ba2c129d1f240c6a22cec3e53f3f22af64b6b","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head-unique.njk","hash":"8da52a144060db1a0a088ccb2e6cc8376d1fce70","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/brand.njk","hash":"dd9c4c03e99dfde0dfb8edefcb2c933f2f560efc","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/index.njk","hash":"650de421a8ce4cf685428ffbe0087ff84cbd1356","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu-item.njk","hash":"41a8b0cc16f60fa085cb719d07216d86b6bc4bf8","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu.njk","hash":"ee6fc2f111572d3eeab0a2fecbb2d6b3e37ab26b","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head.njk","hash":"5388b157bba4a40b9312f4a45c6678974ccf0837","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/languages.njk","hash":"e43f22198cccb5f6e306b1ce0d28d12a4fb891f8","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/sub-menu.njk","hash":"06480d8ec5f0b87eafd47f082f07968d7282dd5c","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/breadcrumb.njk","hash":"89825e75cc45e9709fa6ba89883669eedaff6f46","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/page-header.njk","hash":"7ed4f102a1825195cff8d7995bf9219f323a9034","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/schedule.njk","hash":"0f4bc8e257da60f77c0c1738607b2bde55810684","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/tags.njk","hash":"a18d1598e36cc72f2b0b24c3cc3c5990dfaa3254","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-copyright.njk","hash":"bfff923526d6800218f08dba6ce0bbf5c17755fd","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-followme.njk","hash":"c1e33b4889f75acc490af3c8bde0ec56c518ff41","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-meta.njk","hash":"9fa47e4fb342811da590ee4adc91cf81118c0a39","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-related.njk","hash":"e0986db00a0201dd3c60570f964829c84ba5bc68","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/categories.njk","hash":"17156d99941f28a225951ffdcfa9a115e20dc2d2","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-reward.njk","hash":"e8b8a7c41e9ec612d0c0c73419529d55d1c16256","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/comments.njk","hash":"d0c470b0f6690aa217e9ada848c5e2e73fb27c6f","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-share.njk","hash":"16696990e4ce65fc8db18c4635082a5d5d06ff07","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/algolia-search.njk","hash":"efb2b6f19df02ba5ae623a1f274fff52aed21e6f","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/index.njk","hash":"8f6f256ab3b351ffc80f1f3f1d9834e9a7cfac31","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/baidu-analytics.njk","hash":"6215309aee028dcb734452beec448c5afb6c63fc","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/cloudflare.njk","hash":"a5b8297c2c383124dd6a56e256ecc0c0dcf489be","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/localsearch.njk","hash":"661f7acae43f0be694266323320f977d84119abe","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/google-analytics.njk","hash":"d89066ff53879693f023e540d59c86137172c529","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/index.njk","hash":"f900306497b133e8b098bd9f4b96b93d1d96c185","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/growingio.njk","hash":"8afaa772c390bd9d53a5cff9645ac3168334eb98","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/plausible.njk","hash":"ef9f2bb7110507f1c4336800af9157d5fa9765bd","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/matomo.njk","hash":"4e89648a8ec8194c5823064cbca39c938a799006","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/microsoft-clarity.njk","hash":"9dc00fcb0a05899f048eace9f9160b78956655d5","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/tidio.njk","hash":"02aab857c27fc103216029be991688b12a73a525","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/chatra.njk","hash":"d7263fca16d0278ccf1f6aa1c6df6902a6344a09","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqus.njk","hash":"9375b19a89b7fa9474e558d085af5448d4c5c50c","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/changyan.njk","hash":"d1c950f8fbdf85e7a3eae5463767a89e858e8220","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/gitalk.njk","hash":"b63b7e2ede0d3e66e732fa1a06bda9b19e1e85d4","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/umami.njk","hash":"3343750682fbd8535e50f8129be3003ad26015b4","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_partials/sidebar/site-overview.njk","hash":"78a1a8cac44de7e963ab4cd51c988442eb3e789a","modified":1707031409664},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/livere.njk","hash":"3b13b09fba84ec6000886890a6710736a2b8fafe","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/isso.njk","hash":"64cc3bdaf644fd32c0d0a247f29f5b6904da9af3","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/algolia-search.njk","hash":"24ed76e0c72a25ac152820c750a05826a706b6f4","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/utterances.njk","hash":"5a94032bc3512a10ad4328fc19ec07b819a1d687","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/index.njk","hash":"abf37fc55aa86702118e8fdf5bf2d389dd589aa0","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/localsearch.njk","hash":"e45ea3542cdc9ed7ec8447b5e6f35df4c5e82758","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/mathjax.njk","hash":"3677017fd4572b158311f5f5d870590ab25184e0","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/katex.njk","hash":"1ebf658690468ea197bdd0416eb7cfa4bd0b083a","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/mermaid.njk","hash":"099e031f52fb8e47b3af5b2684737efc9e643ee7","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/pdf.njk","hash":"2c81984cc4f5123103460442f6e046f5b6c97127","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/firestore.njk","hash":"d32ebe94560fa95824478ebbff531bffc47b194d","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/wavedrom.njk","hash":"02202bf563fb5eedde2ccad4d6c5b9109d30a703","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/index.njk","hash":"568ddf7955d11d93fb5e842b403a7ac8b1b7fdb1","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/busuanzi-counter.njk","hash":"a4bc501da0f22f7e420f0ca47e83988ce90b1368","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/lean-analytics.njk","hash":"2446e748cdc102c78492216319ac02148db7daf6","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_menu.styl","hash":"fb550935d374e0bdf1097fce187337dc05cad3e1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/index.styl","hash":"ab16a3dcdc0393b9b582ef59dcc13db9320e917c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_posts-expand.styl","hash":"485d23ccb42c0d0c8ead7ea8930dd3e06d79a285","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_header.styl","hash":"dafc6d23c80d6fe3e55a7711e94210d2479b629a","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_header.styl","hash":"ac2dc0ce9c775a83ef7132ae957b54539366ac9c","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_menu.styl","hash":"72dc825c50357402c342d62ab60fc0c478ab6bc1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sidebar.styl","hash":"91dbf3ca5c3a613d4e30618c120da535bf2d0336","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"778ed2ad5643b93970c95626b325defeb586733f","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/base.styl","hash":"d0a7c99095f490b0d2ed6b1be43d435960798cec","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/buttons.styl","hash":"a042571d85ff7265f799004239a45f36b716b8a6","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_layout.styl","hash":"26a0cba1eee5de45a45a5e14e17707f905390512","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/index.styl","hash":"8000075b227749a7495eaf417cac6ccfbe441580","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/comments.styl","hash":"e4fecc889ba3317a64e9abba5842c79dff9b7827","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/normalize.styl","hash":"b56367ea676ea8e8783ea89cd4ab150c7da7a060","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/index.styl","hash":"523fb7b653b87ae37fc91fc8813e4ffad87b0d7e","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/back-to-top.styl","hash":"7664491542046df9a3887cf40a06e00c0b4086a9","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/pagination.styl","hash":"f4228c759db4a650c8d38745c2edd1dc83c45687","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/index.styl","hash":"2298e521253b3bf376a2412271bc2a7d305051f3","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqusjs.njk","hash":"0749cb6902baecdfd01f779a2a2513f6d2f6a823","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_header.styl","hash":"3fbfab591f280e2e7f3b0265901c93bc4bd137ed","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_layout.styl","hash":"fa4fd8f76464e214fb7318f325b13c2b62f4b478","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_layout.styl","hash":"6569a6640f79d247a8235b3914772c0e2f99ead2","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_menu.styl","hash":"82cda756f5b7092df2eee6641b9786df71623bdb","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/reading-progress.styl","hash":"90a86045a33c1bae49fc2f6fa1e1b53170c7f77b","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sub-menu.styl","hash":"c48ccd8d6651fe1a01faff8f01179456d39ba9b1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tables.styl","hash":"e840b23d33023e6d45e018f6e84b683dd56efd8d","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/toggles.styl","hash":"782ee1fc5e669d3ddbfeb82b73ad7fe561f1a4fb","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sidebar.styl","hash":"547c0b5cd5e7ea10d21863d13a6b16579a49396c","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/index.styl","hash":"8e34df131830d4fa3725e4590a672ba1cf1903e5","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Gemini/index.styl","hash":"9dfe853c901bdc52fc950bacdf15484dbb9bf140","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/mobile.styl","hash":"1dbf2c339adcd27026c3a2ded32ee91ce08cea26","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1706697684335},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","hash":"def07bcc7c17d8a0caad177fb1dd2f3a5e5b3536","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","hash":"da361917d65e5dca8362f8cdeb6c8cc0e8316cec","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","hash":"1e826dea3f684c0515f362dc1352447a1f0eae71","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","hash":"4536cb6d0a9bbaaa86fab3fa0101f6a3a3ec5a76","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","hash":"6e0682bb42170d61b13b786295f45f9c785f8b73","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","hash":"fdb7b7cef1a147d897e7f7cd8903b58368ec2062","modified":1706697684333},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","hash":"835cbf54c49ef1327f47df70ff2636ad36b6f57d","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","hash":"6bf821310342c5b87a631873e7650a475a0765f1","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/wavedrom.js","hash":"40dcd10df6edf124088c329346e0cc0bdac74ef1","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"f634f94828620e88c3f5a8db56f7944f6ba232b0","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/fold.styl","hash":"42a0b65491ad85438596b3fe0b7f23973e4cef34","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/index.styl","hash":"138f78147bc6bd6005f329ada34dc79b7625542d","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"d6418fd2bbfba7b73ddf11ec62db9637fdf5d8af","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"393ff96234e4196b569d4b11496774eb78e147de","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/index.styl","hash":"22cd37bd5df9972d5074710896aba4424ad5161c","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/label.styl","hash":"debee14539272fbe3835a7d3853af2230baa3501","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/link-grid.styl","hash":"7f8a7345e6537a62cd9e9a94c8f7065b541d9b04","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/mermaid.styl","hash":"48d35dba575a7c9e8845b16652e76b7d4a4646de","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/pdf.styl","hash":"b6654a1d7cf82577d8263faffee8af3ad4a5c0e8","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/tabs.styl","hash":"33dd6ad015dde65fd46f34961655442e8e82b52e","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/breadcrumb.styl","hash":"8afdc311c6b8db121758371f95cf1c5e77354f42","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/wavedrom.styl","hash":"af113411ad9cca7674177be36af8dd399680834d","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/index.styl","hash":"7504dbc5c70262b048143b2c37d2b5aa2809afa2","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/categories.styl","hash":"b6e2eb1550a7845cb2adf86081a4ab6c7bde1e68","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/schedule.styl","hash":"6b816c2511242ee503fb5f34cd3e4dcdafc06b85","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/tag-cloud.styl","hash":"1a81d1a71fcf0699629ce6e72dfd0a15f3a2dd0a","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/disqusjs.styl","hash":"877a537d5b95beb048142e4fdee6f17e6ef9c7bb","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/index.styl","hash":"54d12e2c5d9982f7b9e5b23be5133954a8514e9d","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/math.styl","hash":"9d995eb4871a6c273d9d51558676a1fdabf69e72","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/search.styl","hash":"e72799ce3f9b79753e365b2f8c8ef6c310668d4a","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/note.styl","hash":"98d4c20aff0f0fcfe1824017fb06ab21ef0d218e","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/utterances.styl","hash":"56d90ae0559caa55b75f3c300ff2711f9ed65fc4","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/index.styl","hash":"098d4bd034e986fcf7e443eac4fc2193935461b7","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-collapse.styl","hash":"7369928305330c73ae0b3f063a681a8384d8fde4","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-followme.styl","hash":"1ecfd64507954810b07a9d21fb5305b5378feda0","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-body.styl","hash":"56d5b7ff73f466c9ae54f7204ae899281295d749","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-footer.styl","hash":"11497388f124bfbb4001495a67d3629a9f618405","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-gallery.styl","hash":"aa366d37389760c8595529b850f461569577a1c5","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/gitalk.styl","hash":"8f094c4ac17e2ab45569b12d157747f9c7333c12","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-header.styl","hash":"1191f1bfa5c43e54be8e5b3cc0d802984e161747","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-nav.styl","hash":"9ac6f477177264c26a46e8333b8456720a0444dc","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-widgets.styl","hash":"ebfba158a0a4af3d1dabcacbc58986664de52140","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-reward.styl","hash":"04cf4a69537fc14d3b8904f965d283356853847f","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/footer/index.styl","hash":"4e967702cf4c637132346bc74ec8854426f1a68c","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/bookmark.styl","hash":"e74f4bb47a101b014ee2a1783c87f3b87323f9a0","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/github-banner.styl","hash":"38c64c2d04e46848382bfa246a0e9c508294767b","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/index.styl","hash":"6e0d0796ef7fbbb62ffdfb448753a850de82c74f","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/menu.styl","hash":"bbbc40b03cb299d2a6a568f329b2ce98e1cdc430","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/index.styl","hash":"da5e88f8debd5ac8d7af5c6ba6240df66104955f","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/related-posts.styl","hash":"b05908f04ef95f2d91e6eba89b12411c378d050f","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-nav.styl","hash":"bf3ad8b4268f763a1e26377681644887694bc009","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"0847400d8579b0a2dd1bf662c78954c10adf2680","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"c6a27beb3f741211a14576026f3b4cfc44cc6407","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-meta.styl","hash":"a851e9d5aefcd027c95eeb323860b6da70f202d1","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"24752d145c6fb8f5344dca9c7b9640839c02e009","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"5b38ac4a0f1ade0e681aff0e3366c481d9cf3dcd","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"46eece42510c2c89bb9209afb0262ad76a4b0b36","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"9a7c71560fbdc936ad4e736fe15063ea3e8a644b","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"c2e354a565c8c1b32bd0ceacc972b17982758b67","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/site-state.styl","hash":"26dd0adfcb1db6df29c6090c8d7e9b5a43583fb0","modified":1706697684374},{"_id":"source/categories/index.md","hash":"f5c920fbc09ea3d8edf250de7e31bcc6b3e765ae","modified":1706698717077},{"_id":"source/.DS_Store","hash":"ea4769cc264d17ee983394dc31d8d5cac0de7a50","modified":1710516092318},{"_id":"source/_posts/.DS_Store","hash":"f6c441d5db8a680fc3cf3bc13f8aea7f90886f6b","modified":1710516092315},{"_id":"source/CNAME","hash":"07ad8fed7f1b810bf3715a0e68324e5cbb1d47ac","modified":1706872557451},{"_id":"source/about/index.md","hash":"9294d008cc673abc2eaf740f101ebac560029267","modified":1706698701349},{"_id":"source/tags/index.md","hash":"e995ed2b8452b1906600b3853b920f13423098b7","modified":1706698644396},{"_id":"source/_data/styles.styl","hash":"f4bb55ef0972c829e3382d1bae1786b3ab5d54ef","modified":1707045638288},{"_id":"source/_posts/cs/.DS_Store","hash":"db1ba195a60412ec7a7d63e3f3ffaa7a874fe079","modified":1710516092319},{"_id":"source/images/.DS_Store","hash":"60fbc9bc8c2d88510803a394d229a0442cae1cb2","modified":1709016856181},{"_id":"source/_posts/cs/nlp/.DS_Store","hash":"852b2571c397715dd345e664be2511fd2cedd28f","modified":1710516092315},{"_id":"source/images/qrcode.jpg","hash":"eab6c8893d0c79cf705cf64d738f560563365b0c","modified":1707548301740},{"_id":"source/images/background/.DS_Store","hash":"88f5d31d0db89adcf679f2a7fefc8947139a1c1f","modified":1709026807046},{"_id":"source/images/avatar/.DS_Store","hash":"c3fa37607ceb3f7ba411cf4203d2a333f773d921","modified":1707118756919},{"_id":"source/images/avatar/20180303210737_XsJVr.jpeg","hash":"2ebbbcead4446a69b0ac0698ab227a1af49735ce","modified":1707030615190},{"_id":"source/images/favicon/.DS_Store","hash":"83ddccadffca5384db3dfc167728b7c7cacd9a87","modified":1707796842439},{"_id":"source/_posts/cs/nlp/2024/.DS_Store","hash":"844b88fcf49afe5de8acf941e83bcf545f0f45dc","modified":1710516100526},{"_id":"source/images/favicon/favicon_io/about.txt","hash":"d1f084751c81e98b1675b8c0745f6f306a1eaf66","modified":1706844814000},{"_id":"source/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","hash":"3e7db51e59a8261de04f55814475556494d153f6","modified":1707048498957},{"_id":"source/images/favicon/favicon_io/android-chrome-192x192.png","hash":"2164889376f2bd6f9dd2066ebdfe4abd547f49d9","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/favicon-16x16.png","hash":"fb63ee2fc88b9ce42dec5bf68088da14be9350f5","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/favicon-32x32.png","hash":"f42c950146a171afca4ad70ff425b5b4ff02f926","modified":1706844814000},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention.md","hash":"8d504d5a027d6681b6dd1e6f60f8aa680c09c7ae","modified":1710563108847},{"_id":"source/images/favicon/favicon_io/apple-touch-icon.png","hash":"5e36b0bfe67bcf96f4f0e92366a5de509fe9a78a","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/site.webmanifest","hash":"bf31baf91bdd2fcde24a45e3f2a1be33733c6f69","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/favicon.ico","hash":"fceaa7158ae89a98fe5b93152dd7dec2d36c1ff4","modified":1706844814000},{"_id":"source/_posts/cs/nlp/2024/02/.DS_Store","hash":"055b036ca6d94a43ddf5cc44ff50734997452711","modified":1709897296542},{"_id":"source/images/favicon/favicon_io/android-chrome-512x512.png","hash":"419f0b0317ad2a31acfa7dc19e1502731e92e05f","modified":1706844814000},{"_id":"source/_posts/cs/nlp/2024/03/.DS_Store","hash":"3304d20f512bb2607b9b026ee250e81526e44ebf","modified":1710516100525},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/.DS_Store","hash":"93c7345df383234160af43ee62b335152dbfefc4","modified":1710560423245},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA.md","hash":"c5802e9f2c1eb743e74e4b89a0f4005a5c427b07","modified":1710425874411},{"_id":"source/_posts/cs/nlp/2024/02/LLM.md","hash":"dd73e47b1cdb864f26d5780ac4fe08603bcc9b3c","modified":1710314618942},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE.md","hash":"1dae1211b15ab91c1b9f991209aa0a16fc6dbceb","modified":1709015970683},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/dilated_conv.png","hash":"bbc2ff2e9f891da4bfaf6d535ab8545acc18e8a6","modified":1710560488146},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_large_performance.jpeg","hash":"54e3ed874802ac9465580d6b5fcc5d6c1de96244","modified":1710250364698},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/receptive_field_cnn.png","hash":"46515aa3bce1eb0fc244f62ceee7b899c28183e8","modified":1710321816411},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/.DS_Store","hash":"9d409e9dac238eb07cc6841f8e8d05eed83df842","modified":1710056957220},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/MQA.webp","hash":"456a8ab19cc1564912034c375e8c3c5a42be6837","modified":1709973970557},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/decoder.png","hash":"28ee3d1ab68bd325ecb9d2066bc264a63d7de081","modified":1709716894560},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/encoder.png","hash":"d6a3a39c420d90e50f02f8b34f127bfe34177331","modified":1709716888116},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/cnn_heatmap.png","hash":"cb0bde73c9c4d0646133947ebaab16c44c753667","modified":1709723125449},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/attention_calculation.png","hash":"1f020c39c78221e41c5c6953ef97239e9f42aa3c","modified":1709780575011},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/seq2seq.png","hash":"9baa57cc8000a918d0adca6dceaac3ea54791ea8","modified":1709716876496},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/softmax.png","hash":"de80ba20e55abf7457cac958aa87627d0a7e5d77","modified":1709821278308},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/seq2seq_attention.png","hash":"b95046eee028b45dd9734639ecde8189e93b2374","modified":1709781776387},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/.DS_Store","hash":"2e33b8d145af72ec31cbad8c19fc2528fc2a909f","modified":1709014663934},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/complex_number.png","hash":"255aa16a44913d53784b09d0a08762df6d21dc16","modified":1708758408339},{"_id":"source/_posts/cs/nlp/2024/02/LLM/mix_precision_fp16.png","hash":"4da22fa80039f8271ffd839aba70f6b1aca7afe4","modified":1709188879237},{"_id":"source/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","hash":"0629f15db217c8b7037a5b46ef187d79b9a66a09","modified":1707048511782},{"_id":"source/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","hash":"6a3863489ed0982c7fe19d575eae28ba8c286ccf","modified":1707048415396},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/big_bird_attention.png","hash":"ed6c76b9bb77b98d34c429333498d04dac8e3ed9","modified":1710561804292},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_architechture.png","hash":"5e4c347dc41d7f070f54b386fdccf675cfeb8f10","modified":1710255091449},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/llama2_qga.png","hash":"5e0dea7d03de9144eb524a0a9adb102e91b52aaa","modified":1709983486925},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi_nosft.png","hash":"d3b46ed19e458973c3586f3bf2ef906deb4080a8","modified":1709199016415},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/remote_attenuation.png","hash":"90aa3d8f76c9b5c18de8b7740649b499241cab0c","modified":1708957811757},{"_id":"source/_posts/cs/nlp/2024/02/LLM/rope_matrix.png","hash":"84372b91204098bab2ff09d2d865ad869f93bdbd","modified":1709262766062},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/multihead_attention.png","hash":"6f8ee285f2646dc163b6b3164a0639aa9ddd7f27","modified":1709637863252},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/sram_dram.png","hash":"ae7a9296b67c02608460a334fbbad3781b890302","modified":1709971938995},{"_id":"source/images/avatar/Picasso_Elephant.png","hash":"6d38d4b24c13a3fd506ee37a562126b76a24de83","modified":1707118741657},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/longformer_attention.png","hash":"64860379955872ecac5835b3f9d8c6d130c7e485","modified":1710560038203},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/rolling_buffer.png","hash":"34d4db9f4855926db561faa80e934dd971c0974e","modified":1710516051198},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/mqa_result_1.png","hash":"a052b57f71eb78e3158ed2ee06ff0e5597607a2f","modified":1709975039443},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/rope.png","hash":"5535c87dbcfee3ed646db93662a0b6a1ccd3b31b","modified":1708958930811},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/prefill_and_chunking.png","hash":"0c706e0728ea462b2b00c59a97c79ccf5f05b598","modified":1710516401027},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/GQA.png","hash":"d6b0cd20ae5f68a96f0865b3d0b6b129969caa4b","modified":1709982190361},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Scaled-dot-product-self-attention.pbm","hash":"03da711b1547c944deea60d9bf345eb30e7c566f","modified":1709638849226},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/GQA_result_1.png","hash":"87f2c3632fdf83828e8bd07a95cd8e7bf277fc88","modified":1709982952107},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/gpu_cache.png","hash":"edb6b1abdecd3099f2d68c2a729c0ca9b1fb0db7","modified":1709970717456},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/transformer_structure.png","hash":"87f0258e43922eface0277e13167a4ba8c1402bd","modified":1709802508932},{"_id":"source/images/background/wallhaven-2ywymm.png","hash":"aeaf089b64472a1c72cdd9778c5deac4086eb38f","modified":1707045207190},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi_explanation.png","hash":"df22d646e5a47e6223b7722007cfb7dde7c27280","modified":1709198077742},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_swa.png","hash":"59037b91ba8f256fd89b3d60b8ce477e4c8f4b3a","modified":1710252446580},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_perf.png","hash":"c9d7ce0a301920c4e722e341200f311995923735","modified":1710558943189},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/mqa_result_3.png","hash":"12e310102ace1f9e89c0e9a352cf4a3462335a60","modified":1709986493059},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi.png","hash":"193df6c392d71dd4b90202d9be97f000f37b68ec","modified":1709197025669},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_rope_ext.png","hash":"43e1131d146b8104e6936d73aca51d8ace4aff61","modified":1709206562025},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/lihongyi_self_attention.png","hash":"39db6256143fd9a494e848240a8daa434aaddea5","modified":1709965340148},{"_id":"source/images/avatar/shadow.png","hash":"da67a14e556fb3223c1c4ec8b9f33eb52c612f50","modified":1706779539112},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Scaled-dot-product-self-attention.png","hash":"983eae2b767df413ef3211ddaf31f1b833d7c86f","modified":1709986303475},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/MQA.png","hash":"7e3f3037311be60e79a7b5388338febc9f3b6d7c","modified":1709986434286},{"_id":"source/images/background/wallhaven-p97q73.png","hash":"e83875b9cd1dcfa87f0e76dc1a327cbe2921c5da","modified":1707045618160},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Markdown _  Nice.html","hash":"c905c942579a520c7b3c788a00cdb9ae359d4a32","modified":1709897264700},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/ms_invest_mistral.png","hash":"faf324c0b57843516a0b256750e6475ec0c2ce93","modified":1710316114714},{"_id":"source/images/background/wallhaven-gpxpg3.png","hash":"d798eaa0d679ed8a14022b0741098711c3402ca8","modified":1707045245660},{"_id":"source/images/background/wallhaven-x636oz.png","hash":"d419359164da44691e2bc8641256ca7c8111e227","modified":1706875075740},{"_id":"public/baidusitemap.xml","hash":"8d557095262039b16f7f207d0fd3787a85f0c004","modified":1710563124410},{"_id":"public/search.xml","hash":"e83d9ee1f40c259491f5d18045ea3d5618716867","modified":1710563124410},{"_id":"public/sitemap.xml","hash":"e86ad59922e90f5893bacde1e4a0a11b0dbbc5ef","modified":1710563124410},{"_id":"public/sitemap.txt","hash":"a5003abfdb8b49c59e4d1b2c564b6b76e81d923a","modified":1710563124410},{"_id":"public/tags/index.html","hash":"72e3e07c8042cf3a87a7d1a82ae67680947e5605","modified":1710563124410},{"_id":"public/categories/index.html","hash":"500460269d11f9e848b5f95828031beeb03c9351","modified":1710563124410},{"_id":"public/about/index.html","hash":"30248384c1ace60f26449759ed65effff0fb74ff","modified":1710563124410},{"_id":"public/c61d17e3.html","hash":"d92aa32d9afdc0b6fada51ea1be8c94080f560cf","modified":1710563124410},{"_id":"public/3dc22f96.html","hash":"9ebb738d51426ffec5dbb5faee7e5e18401e499e","modified":1710563124410},{"_id":"public/c4da56c0.html","hash":"35180d99766c1bd8c050011351f799da664ce100","modified":1710563124410},{"_id":"public/a051710f.html","hash":"3c886119178b1eb49607544dab43baa3f4879449","modified":1710563124410},{"_id":"public/archives/index.html","hash":"96a6c9dd0713ee5d6cd7b76de8121e0657b9cd69","modified":1710563124410},{"_id":"public/archives/2024/index.html","hash":"502748965f346982aa4d3c5f71975c308461f36d","modified":1710563124410},{"_id":"public/archives/2024/02/index.html","hash":"bef885c10a95c06d6ae62e3dbbb0077af5115610","modified":1710563124410},{"_id":"public/archives/2024/03/index.html","hash":"546aee1996e0170a9b538dff0e3e8fa367ce2d5f","modified":1710563124410},{"_id":"public/categories/CS/index.html","hash":"ba7b9f3f518f15559bfe25e4ac9e872d081f5474","modified":1710563124410},{"_id":"public/categories/CS/NLP/index.html","hash":"7a9db55e2403b4ae44c7fc056e3dbfc39aaaa8cf","modified":1710563124410},{"_id":"public/categories/CS/NLP/LLM/index.html","hash":"903b0d67b8fccd1381b85ca498d88ff4645f3137","modified":1710563124410},{"_id":"public/index.html","hash":"e5c436aeff39e60864552c73c691c47bae6512c1","modified":1710563124410},{"_id":"public/tags/NLP/index.html","hash":"9d9221ccc857123566640c405be50abb820c4337","modified":1710563124410},{"_id":"public/tags/LLM/index.html","hash":"5974eceea2842b5f210e529e0baa951808c234fe","modified":1710563124410},{"_id":"public/tags/transformer/index.html","hash":"270975f20a1b115d664e39f4205a14a7289a8eb6","modified":1710563124410},{"_id":"public/tags//index.html","hash":"9554237804a77c34b45129317742adacd32da239","modified":1710563124410},{"_id":"public/tags//index.html","hash":"111cfd619a5f38e3fa4a44258904f9b90e1968c3","modified":1710563124410},{"_id":"public/tags/attention/index.html","hash":"84a2dc3f6e61232ec03040b0c75fb21860c4584c","modified":1710563124410},{"_id":"public/tags/sliding-window-attention/index.html","hash":"765360707b126689485e8a101b18a2462c1ad198","modified":1710563124410},{"_id":"public/tags/sparse-attention/index.html","hash":"48ac92d5ff6c7437f57694a0532418afc8d5bc44","modified":1710563124410},{"_id":"public/tags/positional-encoding/index.html","hash":"5649e84fa6852862598d15afda645ee99225a7cf","modified":1710563124410},{"_id":"public/tags/RoPE/index.html","hash":"bcb68167b2ddaa28eee689524e7de423bbd48280","modified":1710563124410},{"_id":"public/tags/KV-Cache/index.html","hash":"c79a6e692f28bad05ba5f1da37c25204349cb8da","modified":1710563124410},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1710563124410},{"_id":"public/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1710563124410},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1710563124410},{"_id":"public/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1710563124410},{"_id":"public/CNAME","hash":"07ad8fed7f1b810bf3715a0e68324e5cbb1d47ac","modified":1710563124410},{"_id":"public/images/qrcode.jpg","hash":"eab6c8893d0c79cf705cf64d738f560563365b0c","modified":1710563124410},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1710563124410},{"_id":"public/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1710563124410},{"_id":"public/images/avatar/20180303210737_XsJVr.jpeg","hash":"2ebbbcead4446a69b0ac0698ab227a1af49735ce","modified":1710563124410},{"_id":"public/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","hash":"3e7db51e59a8261de04f55814475556494d153f6","modified":1710563124410},{"_id":"public/images/favicon/favicon_io/apple-touch-icon.png","hash":"5e36b0bfe67bcf96f4f0e92366a5de509fe9a78a","modified":1710563124410},{"_id":"public/images/favicon/favicon_io/favicon-16x16.png","hash":"fb63ee2fc88b9ce42dec5bf68088da14be9350f5","modified":1710563124410},{"_id":"public/images/favicon/favicon_io/about.txt","hash":"d1f084751c81e98b1675b8c0745f6f306a1eaf66","modified":1710563124410},{"_id":"public/images/favicon/favicon_io/android-chrome-192x192.png","hash":"2164889376f2bd6f9dd2066ebdfe4abd547f49d9","modified":1710563124410},{"_id":"public/images/favicon/favicon_io/android-chrome-512x512.png","hash":"419f0b0317ad2a31acfa7dc19e1502731e92e05f","modified":1710563124410},{"_id":"public/images/favicon/favicon_io/favicon-32x32.png","hash":"f42c950146a171afca4ad70ff425b5b4ff02f926","modified":1710563124410},{"_id":"public/images/favicon/favicon_io/favicon.ico","hash":"fceaa7158ae89a98fe5b93152dd7dec2d36c1ff4","modified":1710563124410},{"_id":"public/images/favicon/favicon_io/site.webmanifest","hash":"bf31baf91bdd2fcde24a45e3f2a1be33733c6f69","modified":1710563124410},{"_id":"public/c4da56c0/mix_precision_fp16.png","hash":"4da22fa80039f8271ffd839aba70f6b1aca7afe4","modified":1710563124410},{"_id":"public/c61d17e3/dilated_conv.png","hash":"bbc2ff2e9f891da4bfaf6d535ab8545acc18e8a6","modified":1710563124410},{"_id":"public/c61d17e3/mistral_large_performance.jpeg","hash":"54e3ed874802ac9465580d6b5fcc5d6c1de96244","modified":1710563124410},{"_id":"public/c61d17e3/receptive_field_cnn.png","hash":"46515aa3bce1eb0fc244f62ceee7b899c28183e8","modified":1710563124410},{"_id":"public/a051710f/complex_number.png","hash":"255aa16a44913d53784b09d0a08762df6d21dc16","modified":1710563124410},{"_id":"public/3dc22f96/MQA.webp","hash":"456a8ab19cc1564912034c375e8c3c5a42be6837","modified":1710563124410},{"_id":"public/3dc22f96/attention_calculation.png","hash":"1f020c39c78221e41c5c6953ef97239e9f42aa3c","modified":1710563124410},{"_id":"public/3dc22f96/cnn_heatmap.png","hash":"cb0bde73c9c4d0646133947ebaab16c44c753667","modified":1710563124410},{"_id":"public/3dc22f96/decoder.png","hash":"28ee3d1ab68bd325ecb9d2066bc264a63d7de081","modified":1710563124410},{"_id":"public/3dc22f96/encoder.png","hash":"d6a3a39c420d90e50f02f8b34f127bfe34177331","modified":1710563124410},{"_id":"public/3dc22f96/seq2seq_attention.png","hash":"b95046eee028b45dd9734639ecde8189e93b2374","modified":1710563124410},{"_id":"public/3dc22f96/softmax.png","hash":"de80ba20e55abf7457cac958aa87627d0a7e5d77","modified":1710563124410},{"_id":"public/3dc22f96/seq2seq.png","hash":"9baa57cc8000a918d0adca6dceaac3ea54791ea8","modified":1710563124410},{"_id":"public/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","hash":"0629f15db217c8b7037a5b46ef187d79b9a66a09","modified":1710563124410},{"_id":"public/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","hash":"6a3863489ed0982c7fe19d575eae28ba8c286ccf","modified":1710563124410},{"_id":"public/c4da56c0/rope_matrix.png","hash":"84372b91204098bab2ff09d2d865ad869f93bdbd","modified":1710563124410},{"_id":"public/c4da56c0/meta_pi_nosft.png","hash":"d3b46ed19e458973c3586f3bf2ef906deb4080a8","modified":1710563124410},{"_id":"public/c61d17e3/big_bird_attention.png","hash":"ed6c76b9bb77b98d34c429333498d04dac8e3ed9","modified":1710563124410},{"_id":"public/c61d17e3/mistral_architechture.png","hash":"5e4c347dc41d7f070f54b386fdccf675cfeb8f10","modified":1710563124410},{"_id":"public/a051710f/remote_attenuation.png","hash":"90aa3d8f76c9b5c18de8b7740649b499241cab0c","modified":1710563124410},{"_id":"public/3dc22f96/llama2_qga.png","hash":"5e0dea7d03de9144eb524a0a9adb102e91b52aaa","modified":1710563124410},{"_id":"public/3dc22f96/multihead_attention.png","hash":"6f8ee285f2646dc163b6b3164a0639aa9ddd7f27","modified":1710563124410},{"_id":"public/3dc22f96/sram_dram.png","hash":"ae7a9296b67c02608460a334fbbad3781b890302","modified":1710563124410},{"_id":"public/css/main.css","hash":"6aea217c0462e6970606601a9fe39183cf15614c","modified":1710563124410},{"_id":"public/css/noscript.css","hash":"4cd5301e478e0e0d4b176740ec314087ec5cb707","modified":1710563124410},{"_id":"public/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1710563124410},{"_id":"public/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1710563124410},{"_id":"public/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1710563124410},{"_id":"public/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1710563124410},{"_id":"public/js/motion.js","hash":"770d63c26f22705311028a36b52e999cc8a2da82","modified":1710563124410},{"_id":"public/js/next-boot.js","hash":"745bd828205da7e5fbd3f860cc3697097630f5f3","modified":1710563124410},{"_id":"public/js/pjax.js","hash":"b03ba78c6916ad2f390d55bc1bc18fafb64b0ebf","modified":1710563124410},{"_id":"public/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1710563124410},{"_id":"public/js/utils.js","hash":"d775148c2bf20c028622af5609a788167352bf1e","modified":1710563124410},{"_id":"public/js/schemes/muse.js","hash":"ba7ba2c129d1f240c6a22cec3e53f3f22af64b6b","modified":1710563124410},{"_id":"public/js/third-party/addtoany.js","hash":"5276c8f78ee562a8965216dc67d762e59cb4a9f2","modified":1710563124410},{"_id":"public/js/third-party/fancybox.js","hash":"819f382c561fe5ec23c67cc5fabd63dd1cc22dc1","modified":1710563124410},{"_id":"public/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1710563124410},{"_id":"public/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1710563124410},{"_id":"public/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1710563124410},{"_id":"public/js/third-party/analytics/google-analytics.js","hash":"def07bcc7c17d8a0caad177fb1dd2f3a5e5b3536","modified":1710563124410},{"_id":"public/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1710563124410},{"_id":"public/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1710563124410},{"_id":"public/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1710563124410},{"_id":"public/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1710563124410},{"_id":"public/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1710563124410},{"_id":"public/js/third-party/comments/disqus.js","hash":"da361917d65e5dca8362f8cdeb6c8cc0e8316cec","modified":1710563124410},{"_id":"public/js/third-party/comments/disqusjs.js","hash":"1e826dea3f684c0515f362dc1352447a1f0eae71","modified":1710563124410},{"_id":"public/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1710563124410},{"_id":"public/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1710563124410},{"_id":"public/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1710563124410},{"_id":"public/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1710563124410},{"_id":"public/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1710563124410},{"_id":"public/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1710563124410},{"_id":"public/js/third-party/search/algolia-search.js","hash":"fdb7b7cef1a147d897e7f7cd8903b58368ec2062","modified":1710563124410},{"_id":"public/js/third-party/search/local-search.js","hash":"4536cb6d0a9bbaaa86fab3fa0101f6a3a3ec5a76","modified":1710563124410},{"_id":"public/js/third-party/statistics/firestore.js","hash":"6e0682bb42170d61b13b786295f45f9c785f8b73","modified":1710563124410},{"_id":"public/js/third-party/statistics/lean-analytics.js","hash":"835cbf54c49ef1327f47df70ff2636ad36b6f57d","modified":1710563124410},{"_id":"public/js/third-party/tags/mermaid.js","hash":"6bf821310342c5b87a631873e7650a475a0765f1","modified":1710563124410},{"_id":"public/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1710563124410},{"_id":"public/js/third-party/tags/wavedrom.js","hash":"40dcd10df6edf124088c329346e0cc0bdac74ef1","modified":1710563124410},{"_id":"public/images/avatar/Picasso_Elephant.png","hash":"6d38d4b24c13a3fd506ee37a562126b76a24de83","modified":1710563124410},{"_id":"public/c61d17e3/longformer_attention.png","hash":"64860379955872ecac5835b3f9d8c6d130c7e485","modified":1710563124410},{"_id":"public/c61d17e3/rolling_buffer.png","hash":"34d4db9f4855926db561faa80e934dd971c0974e","modified":1710563124410},{"_id":"public/a051710f/rope.png","hash":"5535c87dbcfee3ed646db93662a0b6a1ccd3b31b","modified":1710563124410},{"_id":"public/3dc22f96/mqa_result_1.png","hash":"a052b57f71eb78e3158ed2ee06ff0e5597607a2f","modified":1710563124410},{"_id":"public/c61d17e3/prefill_and_chunking.png","hash":"0c706e0728ea462b2b00c59a97c79ccf5f05b598","modified":1710563124410},{"_id":"public/3dc22f96/GQA.png","hash":"d6b0cd20ae5f68a96f0865b3d0b6b129969caa4b","modified":1710563124410},{"_id":"public/3dc22f96/GQA_result_1.png","hash":"87f2c3632fdf83828e8bd07a95cd8e7bf277fc88","modified":1710563124410},{"_id":"public/3dc22f96/Scaled-dot-product-self-attention.pbm","hash":"03da711b1547c944deea60d9bf345eb30e7c566f","modified":1710563124410},{"_id":"public/3dc22f96/gpu_cache.png","hash":"edb6b1abdecd3099f2d68c2a729c0ca9b1fb0db7","modified":1710563124410},{"_id":"public/3dc22f96/transformer_structure.png","hash":"87f0258e43922eface0277e13167a4ba8c1402bd","modified":1710563124410},{"_id":"public/3dc22f96/Markdown _  Nice.html","hash":"c905c942579a520c7b3c788a00cdb9ae359d4a32","modified":1710563124410},{"_id":"public/images/background/wallhaven-2ywymm.png","hash":"aeaf089b64472a1c72cdd9778c5deac4086eb38f","modified":1710563124410},{"_id":"public/c4da56c0/meta_pi_explanation.png","hash":"df22d646e5a47e6223b7722007cfb7dde7c27280","modified":1710563124410},{"_id":"public/c4da56c0/meta_pi.png","hash":"193df6c392d71dd4b90202d9be97f000f37b68ec","modified":1710563124410},{"_id":"public/c61d17e3/mistral_perf.png","hash":"c9d7ce0a301920c4e722e341200f311995923735","modified":1710563124410},{"_id":"public/c61d17e3/mistral_swa.png","hash":"59037b91ba8f256fd89b3d60b8ce477e4c8f4b3a","modified":1710563124410},{"_id":"public/3dc22f96/mqa_result_3.png","hash":"12e310102ace1f9e89c0e9a352cf4a3462335a60","modified":1710563124410},{"_id":"public/c4da56c0/meta_rope_ext.png","hash":"43e1131d146b8104e6936d73aca51d8ace4aff61","modified":1710563124410},{"_id":"public/3dc22f96/lihongyi_self_attention.png","hash":"39db6256143fd9a494e848240a8daa434aaddea5","modified":1710563124410},{"_id":"public/images/avatar/shadow.png","hash":"da67a14e556fb3223c1c4ec8b9f33eb52c612f50","modified":1710563124410},{"_id":"public/3dc22f96/Scaled-dot-product-self-attention.png","hash":"983eae2b767df413ef3211ddaf31f1b833d7c86f","modified":1710563124410},{"_id":"public/3dc22f96/MQA.png","hash":"7e3f3037311be60e79a7b5388338febc9f3b6d7c","modified":1710563124410},{"_id":"public/c61d17e3/ms_invest_mistral.png","hash":"faf324c0b57843516a0b256750e6475ec0c2ce93","modified":1710563124410},{"_id":"public/images/background/wallhaven-p97q73.png","hash":"e83875b9cd1dcfa87f0e76dc1a327cbe2921c5da","modified":1710563124410},{"_id":"public/images/background/wallhaven-gpxpg3.png","hash":"d798eaa0d679ed8a14022b0741098711c3402ca8","modified":1710563124410},{"_id":"public/images/background/wallhaven-x636oz.png","hash":"d419359164da44691e2bc8641256ca7c8111e227","modified":1710563124410}],"Category":[{"name":"CS","_id":"clttl4al00005rb4keyff2zjp"},{"name":"NLP","parent":"clttl4al00005rb4keyff2zjp","_id":"clttl4al3000drb4k4igca5dc"},{"name":"LLM","parent":"clttl4al3000drb4k4igca5dc","_id":"clttl4al3000orb4k08mp80z4"}],"Data":[{"_id":"styles","data":".post-toc .nav .nav-child {\n  display: block;\n}\n.post-toc ol {\n  font-size: 13px;\n}\nbody {\n  background: url(\"/images/background/wallhaven-p97q73.png\");\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-size: cover;\n  background-position: 50% 50%;\n}\n:root {\n  --content-bg-color: rgba(32,32,32,0.816);\n}\n"}],"Page":[{"title":"tags","date":"2024-01-31T10:50:02.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2024-01-31 18:50:02\ntype: \"tags\"\ncomments: false\n---\n","updated":"2024-01-31T10:57:24.396Z","path":"tags/index.html","layout":"page","_id":"clttl4akw0000rb4k57ly95hh","content":"\n","length":0,"excerpt":"","more":"\n"},{"title":"categories","date":"2024-01-31T10:57:57.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2024-01-31 18:57:57\ntype: \"categories\"\ncomments: false\n---\n","updated":"2024-01-31T10:58:37.077Z","path":"categories/index.html","layout":"page","_id":"clttl4akx0001rb4k2ukpa4n6","content":"\n","length":0,"excerpt":"","more":"\n"},{"title":"about","date":"2024-01-31T10:57:44.000Z","type":"about","comments":0,"_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2024-01-31 18:57:44\ntype: \"about\"\ncomments: false\n---\n","updated":"2024-01-31T10:58:21.349Z","path":"about/index.html","layout":"page","_id":"clttl4akz0003rb4kgomdbvw3","content":"\n","length":0,"excerpt":"","more":"\n"}],"Post":[{"title":"LLM","abbrlink":"c4da56c0","date":"2024-02-28T07:19:28.000Z","_content":"\n  \n\nRoPERoPE[](http://www.linsight.cn/a051710f.html) [](https://zhuanlan.zhihu.com/p/684072868) [](https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&mid=2247483759&idx=1&sn=f7b59b879476b8687a340606b5568eae&chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&token=88551061&lang=zh_CN#rd)\n\n#   \n\n2023LLM20235Claude100k tokens67ChatGPT3.516kChatGLM2-B32k  \n\nChatGLMAgentChatGLM3ChatGLM4  \n\nLM-SYSLongChatMosaicLMMPT16k\n\nQwen-1.532k  \n\n<center>\n\n|  |  |\n| :----: | :----: |\n| Baichuan2 | 192k |\n| GPT4-turbo  | 128k |\n| Yi | 200k |\n| Kimi Chat | 128k(20) |\n| Claude2 | 200k |  \n\n</center>\n\n  \n\n  \n\n#   \n\ntokenizertokentoken>1.5tokenizer2200ktoken30w  \n\n27  \n\n<big><u>****</u></big>  \n\nRAGRetrieval-augmented generationRAG  \n\n<big><u>****</u></big>prompt  \n\nprompt  \n\n1ppl2attention\n\n# \n\n  \n\n2k/4k8k16kPPLRoPE<u>****</u>  \n\n## \n\n2k/4k8k/16k/32k+  \n\n  \n\n1.  \n\n32k  \n\n4k8attention maskattention mask  \n\n>  \n\n2.  \n\ntransformer  \n\n $l$  $V$ hidden size $h$ batch size $b$  $s$ Adam1  \n\n(1) \n\n $\\Phi$  =  + $l$ * decoder = $Vh + l(12h^2 + 13h)$  \n\n $s$   \n\n(2)   \n\n = logits + $l$ *  $\\approx2bshV + l*(24bsh^2+4bs^2h)$\n\nsoftmaxsoftmax $s$ \n\n\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\frac{}{}\n&=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n$$\n\n $s << h$  $h$ 1k1w $s$  $sh$   \n\n(3)   \n\noptimizer\n\n$\\Phi$$\\Phi$$2\\Phi$ $(\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi + 2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]$ \n\n{% asset_img mix_precision_fp16.png  %}  \n\n\n\n  \n\nsoftmaxdropout  \n\nattention $x$  $QKV$  $x$  $QK$  $QK$ softmax $QK^T$  $11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2$  $l$  $l$   \n\n $s$ 4k32k64GPUbatch sizegradient accumulation<big><u>****</u></big>  \n\n2B7B16k32k200k34B70B+  \n\n2k4k  \n\n  \n\n##  Position Interpolation\n\n236Meta[EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION](https://arxiv.org/pdf/2306.15595.pdf)RoPEPIPosition Interpolation2k32k1kstep\n\n{% asset_img meta_pi.png PI %}  \n{% asset_img LLM/meta_pi.png PI %}  \n\n>In contrast, LLaMA models that are extended via direct fine-tuning only saw a minimal increase of the effective context window size kmax from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear indication of an acceleration in the increase of window size.  \n\n1w20482560\n\nRoPE  \n\nRoPERoPE $\\left|m-n \\right|$ <2048attention $\\left|m-n \\right|$ \n\n{% asset_img meta_rope_ext.png RoPE %}  \n\n3000attention score\n\n\n\nPI  \n\n{% asset_img meta_pi_nosft.png PI %}  \n\n2k2k2k4k\n\n{% asset_img meta_pi_explanation.png PI %}  \n\n123...11.522.5...0.5  \n\nattention score $\\tilde{a}(s)=a(Ls/L^{\\prime})$ $L$ 2048$L^{\\prime}$ 8k/16k/32k\n\nRoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}'(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L'}\\right)\n\\end{aligned}\n\\end{equation}\n$$\n\n $m$ 1 ${L}/{L'}$  \n\n  \n\n\n\n\n\n## NTK-Aware Interpolation \n\ncosNTK-Aware InterpolationRoPE<u>****</u>NTK-Aware Scaled RoPECodeLlama1M  \n\nNTKNeural Tangent KernelGLM4  \n\n>Neural Tangent Kernel (NTK) NTK   \n Neural Tangent Kernel  \nNTK   \nNTK \n\nNTK  \n\n  \n\nRoPE $m$   \n\n{% asset_img rope_matrix.png RoPE %}  \n\n22 $d/2$  $\\theta_j=10000^{-2j/d}$  $j$ $j$  $base=10000$  $base$   \n\n  \n\n  \n\n[RoPE](https://www.zhihu.com/people/us4ever)2  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle= \\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n$$  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n $h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*$ $s=m-n$   \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n $\\alpha=L'/L>1$  $s$   \n\nNTK-Aware Scaled RoPE $\\theta_j$ baseRoPE10000  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n $\\theta$  $\\alpha^{\\frac{-2j}{d-2}}$  $j$  $\\alpha^{\\frac{-2j}{d-2}}$ 1 $j$  $j$ 0 $d/2 - 1$$\\alpha^{\\frac{-2j}{d-2}}$  $\\alpha^{-1}$ \n\n[](https://zhuanlan.zhihu.com/p/645770522)NTK-Aware Interpolation  \n\n>RoPE 12 3 60  RoPE 1/60  1/60 4 RoPE NTK-Aware RoPE  1.5  2  90  24  129.6k  43.2k   \n\nRoPE[](https://kexue.fm/archives/9675)  \n\nYaRN[](https://arxiv.org/pdf/2309.00071.pdf)NTK  \n\n>Given the results from [6], this method performs much better at extending the context size of non-finetuned models compared to PI [9]. However, one major disadvantage of this method is that given it is not just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\" interpolation [6] yields inferior results to PI [9]. Furthermore, due to the \"out-of-bound\" values, the theoretical scale factor s does not accurately describe the true context extension scale. In practice, the scale value s has to be set higher than the expected scale for a given context length extension.  \n\nNTK4k32k $\\alpha=L'/L$ 816\n\n## NTK-by-parts\n\nNTK-by-partsNTKNTK-awareRoPENTK-by-parts  \n\n $j$ RoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n$\\lambda_j$  $j$   \n\n $j$  $L$ RoPE $sin$ 1/40~1-1~0 $j$   \n\nNTK-by-parts  \n\n-  $j$  $\\lambda_j$    \n-  $\\lambda_j\\geq$   \n- NTK-aware interpolation  \n\n $r(j)=\\frac{L}{\\lambda_j}$  $\\beta_1\\beta_2$  $r(j)<\\beta_1$  $r(j)\\geq \\beta_2$   \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&if&r(j)<\\beta_1\\\\1&if&r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n$$  \n\nNTK-by-parts $\\theta_j$   \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n$$\n\n $\\beta_1\\beta_2$  $\\beta_1=1\\beta_2=32$ 1/32   \n\n## Dynamically NTK Scaled RoPE  \n\nNTK-Aware InterpolationRoPEattention score $l$  $L$  $\\alpha$ baseDynamically NTK Scaled RoPENTK  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n$$  \n\n $l$  $l>L$  $\\alpha$ 1 $l\\leq L$   \n\nkv-cacheRoPE  \n\n## YaRN  \n\ntokensoftmaxRoPEtoken  \n\nRoPEsoftmaxlogitsoftmax $t>1$ RoPE $\\sqrt{t}$ RoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n$$  \n\nLlama 1Llama 2$\\begin{aligned}\\sqrt{\\frac1t}&=0.1\\ln(\\alpha)+1.\\end{aligned}$Llama  \n\nYaRNNTK-by-partsattention score  \n\nYaRN\n\n## logn  \n\nlognattention $\\sqrt{d}$ logn[](https://zhuanlan.zhihu.com/p/678755776)YaRN  \n\ntokentokentokenattention score  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L'}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n$$  \n\n $L'>L$ YaRN\n\n## \n\nwindow attentionstreaming LLMLongLoRAFocus Transformer\n\n#   \n\n2k4k  \n\n-   \n- token  \n\nattention score  \n\nPINTKNTKlognYaRN  \n\n# Reference  \n1transformerKV cache https://zhuanlan.zhihu.com/p/624740065  \n2EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION https://arxiv.org/pdf/2306.15595.pdf  \n3Transformer10RoPE https://kexue.fm/archives/9675  \n4YaRN: Efficient Context Window Extension of Large\nLanguage Models https://arxiv.org/pdf/2309.00071.pdf  \n5RoPE https://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ  \n6LLM https://zhuanlan.zhihu.com/p/645770522\n7prompt https://cloud.tencent.com/developer/article/2330611  \n8Transformer8 https://spaces.ac.cn/archives/9444  \n9RoPE192K https://zhuanlan.zhihu.com/p/678755776\n***\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)\n","source":"_posts/cs/nlp/2024/02/LLM.md","raw":"---\ntitle: LLM\nabbrlink: c4da56c0\ndate: 2024-02-28 15:19:28\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  \n\nRoPERoPE[](http://www.linsight.cn/a051710f.html) [](https://zhuanlan.zhihu.com/p/684072868) [](https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&mid=2247483759&idx=1&sn=f7b59b879476b8687a340606b5568eae&chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&token=88551061&lang=zh_CN#rd)\n\n#   \n\n2023LLM20235Claude100k tokens67ChatGPT3.516kChatGLM2-B32k  \n\nChatGLMAgentChatGLM3ChatGLM4  \n\nLM-SYSLongChatMosaicLMMPT16k\n\nQwen-1.532k  \n\n<center>\n\n|  |  |\n| :----: | :----: |\n| Baichuan2 | 192k |\n| GPT4-turbo  | 128k |\n| Yi | 200k |\n| Kimi Chat | 128k(20) |\n| Claude2 | 200k |  \n\n</center>\n\n  \n\n  \n\n#   \n\ntokenizertokentoken>1.5tokenizer2200ktoken30w  \n\n27  \n\n<big><u>****</u></big>  \n\nRAGRetrieval-augmented generationRAG  \n\n<big><u>****</u></big>prompt  \n\nprompt  \n\n1ppl2attention\n\n# \n\n  \n\n2k/4k8k16kPPLRoPE<u>****</u>  \n\n## \n\n2k/4k8k/16k/32k+  \n\n  \n\n1.  \n\n32k  \n\n4k8attention maskattention mask  \n\n>  \n\n2.  \n\ntransformer  \n\n $l$  $V$ hidden size $h$ batch size $b$  $s$ Adam1  \n\n(1) \n\n $\\Phi$  =  + $l$ * decoder = $Vh + l(12h^2 + 13h)$  \n\n $s$   \n\n(2)   \n\n = logits + $l$ *  $\\approx2bshV + l*(24bsh^2+4bs^2h)$\n\nsoftmaxsoftmax $s$ \n\n\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\frac{}{}\n&=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n$$\n\n $s << h$  $h$ 1k1w $s$  $sh$   \n\n(3)   \n\noptimizer\n\n$\\Phi$$\\Phi$$2\\Phi$ $(\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi + 2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]$ \n\n{% asset_img mix_precision_fp16.png  %}  \n\n\n\n  \n\nsoftmaxdropout  \n\nattention $x$  $QKV$  $x$  $QK$  $QK$ softmax $QK^T$  $11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2$  $l$  $l$   \n\n $s$ 4k32k64GPUbatch sizegradient accumulation<big><u>****</u></big>  \n\n2B7B16k32k200k34B70B+  \n\n2k4k  \n\n  \n\n##  Position Interpolation\n\n236Meta[EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION](https://arxiv.org/pdf/2306.15595.pdf)RoPEPIPosition Interpolation2k32k1kstep\n\n{% asset_img meta_pi.png PI %}  \n{% asset_img LLM/meta_pi.png PI %}  \n\n>In contrast, LLaMA models that are extended via direct fine-tuning only saw a minimal increase of the effective context window size kmax from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear indication of an acceleration in the increase of window size.  \n\n1w20482560\n\nRoPE  \n\nRoPERoPE $\\left|m-n \\right|$ <2048attention $\\left|m-n \\right|$ \n\n{% asset_img meta_rope_ext.png RoPE %}  \n\n3000attention score\n\n\n\nPI  \n\n{% asset_img meta_pi_nosft.png PI %}  \n\n2k2k2k4k\n\n{% asset_img meta_pi_explanation.png PI %}  \n\n123...11.522.5...0.5  \n\nattention score $\\tilde{a}(s)=a(Ls/L^{\\prime})$ $L$ 2048$L^{\\prime}$ 8k/16k/32k\n\nRoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}'(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L'}\\right)\n\\end{aligned}\n\\end{equation}\n$$\n\n $m$ 1 ${L}/{L'}$  \n\n  \n\n\n\n\n\n## NTK-Aware Interpolation \n\ncosNTK-Aware InterpolationRoPE<u>****</u>NTK-Aware Scaled RoPECodeLlama1M  \n\nNTKNeural Tangent KernelGLM4  \n\n>Neural Tangent Kernel (NTK) NTK   \n Neural Tangent Kernel  \nNTK   \nNTK \n\nNTK  \n\n  \n\nRoPE $m$   \n\n{% asset_img rope_matrix.png RoPE %}  \n\n22 $d/2$  $\\theta_j=10000^{-2j/d}$  $j$ $j$  $base=10000$  $base$   \n\n  \n\n  \n\n[RoPE](https://www.zhihu.com/people/us4ever)2  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle= \\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n$$  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n $h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*$ $s=m-n$   \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n $\\alpha=L'/L>1$  $s$   \n\nNTK-Aware Scaled RoPE $\\theta_j$ baseRoPE10000  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n $\\theta$  $\\alpha^{\\frac{-2j}{d-2}}$  $j$  $\\alpha^{\\frac{-2j}{d-2}}$ 1 $j$  $j$ 0 $d/2 - 1$$\\alpha^{\\frac{-2j}{d-2}}$  $\\alpha^{-1}$ \n\n[](https://zhuanlan.zhihu.com/p/645770522)NTK-Aware Interpolation  \n\n>RoPE 12 3 60  RoPE 1/60  1/60 4 RoPE NTK-Aware RoPE  1.5  2  90  24  129.6k  43.2k   \n\nRoPE[](https://kexue.fm/archives/9675)  \n\nYaRN[](https://arxiv.org/pdf/2309.00071.pdf)NTK  \n\n>Given the results from [6], this method performs much better at extending the context size of non-finetuned models compared to PI [9]. However, one major disadvantage of this method is that given it is not just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\" interpolation [6] yields inferior results to PI [9]. Furthermore, due to the \"out-of-bound\" values, the theoretical scale factor s does not accurately describe the true context extension scale. In practice, the scale value s has to be set higher than the expected scale for a given context length extension.  \n\nNTK4k32k $\\alpha=L'/L$ 816\n\n## NTK-by-parts\n\nNTK-by-partsNTKNTK-awareRoPENTK-by-parts  \n\n $j$ RoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n$\\lambda_j$  $j$   \n\n $j$  $L$ RoPE $sin$ 1/40~1-1~0 $j$   \n\nNTK-by-parts  \n\n-  $j$  $\\lambda_j$    \n-  $\\lambda_j\\geq$   \n- NTK-aware interpolation  \n\n $r(j)=\\frac{L}{\\lambda_j}$  $\\beta_1\\beta_2$  $r(j)<\\beta_1$  $r(j)\\geq \\beta_2$   \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&if&r(j)<\\beta_1\\\\1&if&r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n$$  \n\nNTK-by-parts $\\theta_j$   \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n$$\n\n $\\beta_1\\beta_2$  $\\beta_1=1\\beta_2=32$ 1/32   \n\n## Dynamically NTK Scaled RoPE  \n\nNTK-Aware InterpolationRoPEattention score $l$  $L$  $\\alpha$ baseDynamically NTK Scaled RoPENTK  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n$$  \n\n $l$  $l>L$  $\\alpha$ 1 $l\\leq L$   \n\nkv-cacheRoPE  \n\n## YaRN  \n\ntokensoftmaxRoPEtoken  \n\nRoPEsoftmaxlogitsoftmax $t>1$ RoPE $\\sqrt{t}$ RoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n$$  \n\nLlama 1Llama 2$\\begin{aligned}\\sqrt{\\frac1t}&=0.1\\ln(\\alpha)+1.\\end{aligned}$Llama  \n\nYaRNNTK-by-partsattention score  \n\nYaRN\n\n## logn  \n\nlognattention $\\sqrt{d}$ logn[](https://zhuanlan.zhihu.com/p/678755776)YaRN  \n\ntokentokentokenattention score  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L'}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n$$  \n\n $L'>L$ YaRN\n\n## \n\nwindow attentionstreaming LLMLongLoRAFocus Transformer\n\n#   \n\n2k4k  \n\n-   \n- token  \n\nattention score  \n\nPINTKNTKlognYaRN  \n\n# Reference  \n1transformerKV cache https://zhuanlan.zhihu.com/p/624740065  \n2EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION https://arxiv.org/pdf/2306.15595.pdf  \n3Transformer10RoPE https://kexue.fm/archives/9675  \n4YaRN: Efficient Context Window Extension of Large\nLanguage Models https://arxiv.org/pdf/2309.00071.pdf  \n5RoPE https://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ  \n6LLM https://zhuanlan.zhihu.com/p/645770522\n7prompt https://cloud.tencent.com/developer/article/2330611  \n8Transformer8 https://spaces.ac.cn/archives/9444  \n9RoPE192K https://zhuanlan.zhihu.com/p/678755776\n***\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)\n","slug":"cs/nlp/2024/02/LLM","published":1,"updated":"2024-03-13T07:23:38.942Z","comments":1,"layout":"post","photos":[],"_id":"clttl4aky0002rb4ke9vgh881","content":"<p></p>\n<p>RoPERoPE<a href=\"http://www.linsight.cn/a051710f.html\"></a> <a href=\"https://zhuanlan.zhihu.com/p/684072868\"></a> <a href=\"https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&amp;mid=2247483759&amp;idx=1&amp;sn=f7b59b879476b8687a340606b5568eae&amp;chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&amp;token=88551061&amp;lang=zh_CN#rd\"></a></p>\n<h1 id=\"\"></h1>\n<p>2023LLM20235Claude100k\ntokens67ChatGPT3.516kChatGLM2-B32k</p>\n<p>ChatGLMAgentChatGLM3ChatGLM4</p>\n<p>LM-SYSLongChatMosaicLMMPT16k</p>\n<p>Qwen-1.532k</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Baichuan2</td>\n<td style=\"text-align: center;\">192k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">GPT4-turbo</td>\n<td style=\"text-align: center;\">128k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Yi</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Kimi Chat</td>\n<td style=\"text-align: center;\">128k(20)</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Claude2</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p></p>\n<p></p>\n<h1 id=\"\"></h1>\n<p>tokenizertokentoken&gt;1.5tokenizer2200ktoken30w</p>\n<p>27</p>\n<p><big><u><strong></strong></u></big></p>\n<p>RAGRetrieval-augmented\ngenerationRAG</p>\n<p><big><u><strong></strong></u></big>prompt</p>\n<p>prompt</p>\n<p>1ppl2attention</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>2k/4k8k16kPPLRoPE<u><strong></strong></u></p>\n<h2 id=\"\"></h2>\n<p>2k/4k8k/16k/32k+</p>\n<p></p>\n<p>1.</p>\n<p>32k</p>\n<p>4k8attention\nmaskattention\nmask</p>\n<p>&gt;</p>\n<p>2.</p>\n<p>transformer</p>\n<p> <span class=\"math inline\">\\(l\\)</span> \n<span class=\"math inline\">\\(V\\)</span> hidden size <span class=\"math inline\">\\(h\\)</span> batch size <span class=\"math inline\">\\(b\\)</span>  <span class=\"math inline\">\\(s\\)</span>\nAdam1</p>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p> <span class=\"math inline\">\\(\\Phi\\)</span> = \n+ <span class=\"math inline\">\\(l\\)</span> * decoder = <span class=\"math inline\">\\(Vh + l(12h^2 + 13h)\\)</span></p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p> = logits + <span class=\"math inline\">\\(l\\)</span> *  <span class=\"math inline\">\\(\\approx2bshV + l*(24bsh^2+4bs^2h)\\)</span></p>\n<p>softmaxsoftmax <span class=\"math inline\">\\(s\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{}{}\n&amp;=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&amp;\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(s &lt;&lt; h\\)</span>\n <span class=\"math inline\">\\(h\\)</span> 1k1w\n<span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(sh\\)</span>\n</p>\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>optimizer</p>\n<p><span class=\"math inline\">\\(\\Phi\\)</span><span class=\"math inline\">\\(\\Phi\\)</span><span class=\"math inline\">\\(2\\Phi\\)</span>\n<span class=\"math inline\">\\((\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi +\n2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]\\)</span>\n</p>\n<img src=\"/c4da56c0/mix_precision_fp16.png\" class title=\"\">\n<p></p>\n<p></p>\n<p>softmaxdropout</p>\n<p>attention <span class=\"math inline\">\\(x\\)</span>\n <span class=\"math inline\">\\(QKV\\)</span>  <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(QK\\)</span>  <span class=\"math inline\">\\(QK\\)</span> softmax\n<span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2\\)</span> \n<span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(l\\)</span> </p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n4k32k64GPUbatch\nsizegradient\naccumulation<big><u><strong></strong></u></big></p>\n<p>2B7B16k32k200k34B70B+</p>\n<p>2k4k</p>\n<p></p>\n<h2 id=\"-position-interpolation\"> Position\nInterpolation</h2>\n<p>236Meta<a href=\"https://arxiv.org/pdf/2306.15595.pdf\">EXTENDING CONTEXT WINDOW\nOF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION</a>RoPEPIPosition\nInterpolation2k32k1kstep</p>\n<img src=\"/c4da56c0/meta_pi.png\" class title=\"PI\">\n\n<blockquote>\n<p>In contrast, LLaMA models that are extended via direct fine-tuning\nonly saw a minimal increase of the effective context window size kmax\nfrom 2048 to 2560, even after fine-tuning for more than 10000 steps,\nwith no clear indication of an acceleration in the increase of window\nsize.</p>\n</blockquote>\n<p>1w20482560</p>\n<p>RoPE</p>\n<p>RoPERoPE\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n&lt;2048attention\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n</p>\n<img src=\"/c4da56c0/meta_rope_ext.png\" class title=\"RoPE\">\n<p>3000attention\nscore</p>\n<p></p>\n<p>PI</p>\n<img src=\"/c4da56c0/meta_pi_nosft.png\" class title=\"PI\">\n<p>2k2k2k4k</p>\n<img src=\"/c4da56c0/meta_pi_explanation.png\" class title=\"PI\">\n<p>123...11.522.5...0.5</p>\n<p>attention\nscore <span class=\"math inline\">\\(\\tilde{a}(s)=a(Ls/L^{\\prime})\\)</span> <span class=\"math inline\">\\(L\\)</span> 2048<span class=\"math inline\">\\(L^{\\prime}\\)</span>\n8k/16k/32k</p>\n<p>RoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}&#39;(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L&#39;}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(m\\)</span> 1\n<span class=\"math inline\">\\({L}/{L&#39;}\\)</span></p>\n<p></p>\n<p></p>\n<p></p>\n<h2 id=\"ntk-aware-interpolation\">NTK-Aware Interpolation</h2>\n<p>cosNTK-Aware\nInterpolationRoPE<u><strong></strong></u>NTK-Aware\nScaled RoPECodeLlama1M</p>\n<p>NTKNeural Tangent\nKernelGLM4</p>\n<blockquote>\n<p>Neural Tangent Kernel (NTK)\nNTK\n<br>\n\nNeural Tangent\nKernel<br>\nNTK\n<br>\nNTK\n</p>\n</blockquote>\n<p>NTK</p>\n<p></p>\n<p>RoPE <span class=\"math inline\">\\(m\\)</span>\n</p>\n<img src=\"/c4da56c0/rope_matrix.png\" class title=\"RoPE\">\n<p>22 <span class=\"math inline\">\\(d/2\\)</span>\n\n<span class=\"math inline\">\\(\\theta_j=10000^{-2j/d}\\)</span> \n<span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(base=10000\\)</span>  <span class=\"math inline\">\\(base\\)</span>\n</p>\n<p></p>\n<p></p>\n<p><a href=\"https://www.zhihu.com/people/us4ever\">RoPE</a>2</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=\n\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&amp;\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*\\)</span>\n<span class=\"math inline\">\\(s=m-n\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha=L&#39;/L&gt;1\\)</span>\n <span class=\"math inline\">\\(s\\)</span> </p>\n<p>NTK-Aware Scaled RoPE <span class=\"math inline\">\\(\\theta_j\\)</span>\nbaseRoPE10000</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\theta\\)</span>  <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>  <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>\n1 <span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(j\\)</span> 0\n<span class=\"math inline\">\\(d/2 - 1\\)</span><span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>  <span class=\"math inline\">\\(\\alpha^{-1}\\)</span> </p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/645770522\"></a>NTK-Aware\nInterpolation</p>\n<blockquote>\n<p>RoPE\n12 3 60 \nRoPE 1/60 \n1/60 4 RoPE\nNTK-Aware\nRoPE  1.5\n 2  90  24\n 129.6k  43.2k\n</p>\n</blockquote>\n<p>RoPE<a href=\"https://kexue.fm/archives/9675\"></a></p>\n<p>YaRN<a href=\"https://arxiv.org/pdf/2309.00071.pdf\"></a>NTK</p>\n<blockquote>\n<p>Given the results from [6], this method performs much better at\nextending the context size of non-finetuned models compared to PI [9].\nHowever, one major disadvantage of this method is that given it is not\njust an interpolation scheme, some dimensions are slightly extrapolated\nto \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\"\ninterpolation [6] yields inferior results to PI [9]. Furthermore, due to\nthe \"out-of-bound\" values, the theoretical scale factor s does not\naccurately describe the true context extension scale. In practice, the\nscale value s has to be set higher than the expected scale for a given\ncontext length extension.</p>\n</blockquote>\n<p>NTK4k32k\n<span class=\"math inline\">\\(\\alpha=L&#39;/L\\)</span>\n816</p>\n<h2 id=\"ntk-by-parts\">NTK-by-parts</h2>\n<p>NTK-by-partsNTKNTK-awareRoPENTK-by-parts</p>\n<p> <span class=\"math inline\">\\(j\\)</span> RoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p><span class=\"math inline\">\\(\\lambda_j\\)</span>\n <span class=\"math inline\">\\(j\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(L\\)</span>\nRoPE\n<span class=\"math inline\">\\(sin\\)</span>\n1/40<sub>1-1</sub>0\n<span class=\"math inline\">\\(j\\)</span>\n</p>\n<p>NTK-by-parts</p>\n<ul>\n<li> <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(\\lambda_j\\)</span> \n<br>\n</li>\n<li> <span class=\"math inline\">\\(\\lambda_j\\geq\\)</span>\n<br>\n</li>\n<li>NTK-aware interpolation</li>\n</ul>\n<p> <span class=\"math inline\">\\(r(j)=\\frac{L}{\\lambda_j}\\)</span>\n <span class=\"math inline\">\\(\\beta_1\\beta_2\\)</span> \n<span class=\"math inline\">\\(r(j)&lt;\\beta_1\\)</span>\n <span class=\"math inline\">\\(r(j)\\geq\n\\beta_2\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&amp;if&amp;r(j)&lt;\\beta_1\\\\1&amp;if&amp;r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&amp;otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>NTK-by-parts <span class=\"math inline\">\\(\\theta_j\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\beta_1\\beta_2\\)</span>\n <span class=\"math inline\">\\(\\beta_1=1\\beta_2=32\\)</span>\n1/32</p>\n<h2 id=\"dynamically-ntk-scaled-rope\">Dynamically NTK Scaled RoPE</h2>\n<p>NTK-Aware\nInterpolationRoPEattention\nscore <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(L\\)</span>  <span class=\"math inline\">\\(\\alpha\\)</span>\nbaseDynamically NTK Scaled\nRoPENTK</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(l\\)</span> \n<span class=\"math inline\">\\(l&gt;L\\)</span>  <span class=\"math inline\">\\(\\alpha\\)</span> 1 <span class=\"math inline\">\\(l\\leq L\\)</span> </p>\n<p>kv-cacheRoPE</p>\n<h2 id=\"yarn\">YaRN</h2>\n<p>tokensoftmaxRoPEtoken</p>\n<p>RoPEsoftmaxlogitsoftmax\n<span class=\"math inline\">\\(t&gt;1\\)</span>\nRoPE <span class=\"math inline\">\\(\\sqrt{t}\\)</span>\nRoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>Llama 1Llama 2<span class=\"math inline\">\\(\\begin{aligned}\\sqrt{\\frac1t}&amp;=0.1\\ln(\\alpha)+1.\\end{aligned}\\)</span>Llama</p>\n<p>YaRNNTK-by-partsattention\nscore</p>\n<p>YaRN</p>\n<h2 id=\"logn\">logn</h2>\n<p>lognattention <span class=\"math inline\">\\(\\sqrt{d}\\)</span>\nlogn<a href=\"https://zhuanlan.zhihu.com/p/678755776\"></a>YaRN</p>\n<p>tokentokentokenattention\nscore</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L&#39;}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(L&#39;&gt;L\\)</span>\nYaRN</p>\n<h2 id=\"\"></h2>\n<p>window\nattentionstreaming LLMLongLoRAFocus\nTransformer</p>\n<h1 id=\"\"></h1>\n<p>2k4k</p>\n<ul>\n<li><br>\n</li>\n<li>token</li>\n</ul>\n<p>attention score</p>\n<p>PINTKNTKlognYaRN</p>\n<h1 id=\"reference\">Reference</h1>\n<p>1transformerKV cache\nhttps://zhuanlan.zhihu.com/p/624740065<br>\n2EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION https://arxiv.org/pdf/2306.15595.pdf<br>\n3Transformer10RoPE\nhttps://kexue.fm/archives/9675<br>\n4YaRN: Efficient Context Window Extension of Large Language Models\nhttps://arxiv.org/pdf/2309.00071.pdf<br>\n5RoPE\nhttps://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ<br>\n6LLM https://zhuanlan.zhihu.com/p/645770522\n7prompt\nhttps://cloud.tencent.com/developer/article/2330611<br>\n8Transformer8\nhttps://spaces.ac.cn/archives/9444<br>\n9RoPE192K\nhttps://zhuanlan.zhihu.com/p/678755776 ***</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n","length":12689,"excerpt":"","more":"<p></p>\n<p>RoPERoPE<a href=\"http://www.linsight.cn/a051710f.html\"></a> <a href=\"https://zhuanlan.zhihu.com/p/684072868\"></a> <a href=\"https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&amp;mid=2247483759&amp;idx=1&amp;sn=f7b59b879476b8687a340606b5568eae&amp;chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&amp;token=88551061&amp;lang=zh_CN#rd\"></a></p>\n<h1 id=\"\"></h1>\n<p>2023LLM20235Claude100k\ntokens67ChatGPT3.516kChatGLM2-B32k</p>\n<p>ChatGLMAgentChatGLM3ChatGLM4</p>\n<p>LM-SYSLongChatMosaicLMMPT16k</p>\n<p>Qwen-1.532k</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Baichuan2</td>\n<td style=\"text-align: center;\">192k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">GPT4-turbo</td>\n<td style=\"text-align: center;\">128k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Yi</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Kimi Chat</td>\n<td style=\"text-align: center;\">128k(20)</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Claude2</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p></p>\n<p></p>\n<h1 id=\"\"></h1>\n<p>tokenizertokentoken&gt;1.5tokenizer2200ktoken30w</p>\n<p>27</p>\n<p><big><u><strong></strong></u></big></p>\n<p>RAGRetrieval-augmented\ngenerationRAG</p>\n<p><big><u><strong></strong></u></big>prompt</p>\n<p>prompt</p>\n<p>1ppl2attention</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>2k/4k8k16kPPLRoPE<u><strong></strong></u></p>\n<h2 id=\"\"></h2>\n<p>2k/4k8k/16k/32k+</p>\n<p></p>\n<p>1.</p>\n<p>32k</p>\n<p>4k8attention\nmaskattention\nmask</p>\n<p>&gt;</p>\n<p>2.</p>\n<p>transformer</p>\n<p> <span class=\"math inline\">\\(l\\)</span> \n<span class=\"math inline\">\\(V\\)</span> hidden size <span class=\"math inline\">\\(h\\)</span> batch size <span class=\"math inline\">\\(b\\)</span>  <span class=\"math inline\">\\(s\\)</span>\nAdam1</p>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p> <span class=\"math inline\">\\(\\Phi\\)</span> = \n+ <span class=\"math inline\">\\(l\\)</span> * decoder = <span class=\"math inline\">\\(Vh + l(12h^2 + 13h)\\)</span></p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p> = logits + <span class=\"math inline\">\\(l\\)</span> *  <span class=\"math inline\">\\(\\approx2bshV + l*(24bsh^2+4bs^2h)\\)</span></p>\n<p>softmaxsoftmax <span class=\"math inline\">\\(s\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{}{}\n&amp;=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&amp;\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(s &lt;&lt; h\\)</span>\n <span class=\"math inline\">\\(h\\)</span> 1k1w\n<span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(sh\\)</span>\n</p>\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>optimizer</p>\n<p><span class=\"math inline\">\\(\\Phi\\)</span><span class=\"math inline\">\\(\\Phi\\)</span><span class=\"math inline\">\\(2\\Phi\\)</span>\n<span class=\"math inline\">\\((\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi +\n2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]\\)</span>\n</p>\n<img src=\"/c4da56c0/mix_precision_fp16.png\" class title=\"\">\n<p></p>\n<p></p>\n<p>softmaxdropout</p>\n<p>attention <span class=\"math inline\">\\(x\\)</span>\n <span class=\"math inline\">\\(QKV\\)</span>  <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(QK\\)</span>  <span class=\"math inline\">\\(QK\\)</span> softmax\n<span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2\\)</span> \n<span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(l\\)</span> </p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n4k32k64GPUbatch\nsizegradient\naccumulation<big><u><strong></strong></u></big></p>\n<p>2B7B16k32k200k34B70B+</p>\n<p>2k4k</p>\n<p></p>\n<h2 id=\"-position-interpolation\"> Position\nInterpolation</h2>\n<p>236Meta<a href=\"https://arxiv.org/pdf/2306.15595.pdf\">EXTENDING CONTEXT WINDOW\nOF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION</a>RoPEPIPosition\nInterpolation2k32k1kstep</p>\n<img src=\"/c4da56c0/meta_pi.png\" class title=\"PI\">\n\n<blockquote>\n<p>In contrast, LLaMA models that are extended via direct fine-tuning\nonly saw a minimal increase of the effective context window size kmax\nfrom 2048 to 2560, even after fine-tuning for more than 10000 steps,\nwith no clear indication of an acceleration in the increase of window\nsize.</p>\n</blockquote>\n<p>1w20482560</p>\n<p>RoPE</p>\n<p>RoPERoPE\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n&lt;2048attention\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n</p>\n<img src=\"/c4da56c0/meta_rope_ext.png\" class title=\"RoPE\">\n<p>3000attention\nscore</p>\n<p></p>\n<p>PI</p>\n<img src=\"/c4da56c0/meta_pi_nosft.png\" class title=\"PI\">\n<p>2k2k2k4k</p>\n<img src=\"/c4da56c0/meta_pi_explanation.png\" class title=\"PI\">\n<p>123...11.522.5...0.5</p>\n<p>attention\nscore <span class=\"math inline\">\\(\\tilde{a}(s)=a(Ls/L^{\\prime})\\)</span> <span class=\"math inline\">\\(L\\)</span> 2048<span class=\"math inline\">\\(L^{\\prime}\\)</span>\n8k/16k/32k</p>\n<p>RoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}&#39;(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L&#39;}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(m\\)</span> 1\n<span class=\"math inline\">\\({L}/{L&#39;}\\)</span></p>\n<p></p>\n<p></p>\n<p></p>\n<h2 id=\"ntk-aware-interpolation\">NTK-Aware Interpolation</h2>\n<p>cosNTK-Aware\nInterpolationRoPE<u><strong></strong></u>NTK-Aware\nScaled RoPECodeLlama1M</p>\n<p>NTKNeural Tangent\nKernelGLM4</p>\n<blockquote>\n<p>Neural Tangent Kernel (NTK)\nNTK\n<br>\n\nNeural Tangent\nKernel<br>\nNTK\n<br>\nNTK\n</p>\n</blockquote>\n<p>NTK</p>\n<p></p>\n<p>RoPE <span class=\"math inline\">\\(m\\)</span>\n</p>\n<img src=\"/c4da56c0/rope_matrix.png\" class title=\"RoPE\">\n<p>22 <span class=\"math inline\">\\(d/2\\)</span>\n\n<span class=\"math inline\">\\(\\theta_j=10000^{-2j/d}\\)</span> \n<span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(base=10000\\)</span>  <span class=\"math inline\">\\(base\\)</span>\n</p>\n<p></p>\n<p></p>\n<p><a href=\"https://www.zhihu.com/people/us4ever\">RoPE</a>2</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=\n\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&amp;\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*\\)</span>\n<span class=\"math inline\">\\(s=m-n\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha=L&#39;/L&gt;1\\)</span>\n <span class=\"math inline\">\\(s\\)</span> </p>\n<p>NTK-Aware Scaled RoPE <span class=\"math inline\">\\(\\theta_j\\)</span>\nbaseRoPE10000</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\theta\\)</span>  <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>  <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>\n1 <span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(j\\)</span> 0\n<span class=\"math inline\">\\(d/2 - 1\\)</span><span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>  <span class=\"math inline\">\\(\\alpha^{-1}\\)</span> </p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/645770522\"></a>NTK-Aware\nInterpolation</p>\n<blockquote>\n<p>RoPE\n12 3 60 \nRoPE 1/60 \n1/60 4 RoPE\nNTK-Aware\nRoPE  1.5\n 2  90  24\n 129.6k  43.2k\n</p>\n</blockquote>\n<p>RoPE<a href=\"https://kexue.fm/archives/9675\"></a></p>\n<p>YaRN<a href=\"https://arxiv.org/pdf/2309.00071.pdf\"></a>NTK</p>\n<blockquote>\n<p>Given the results from [6], this method performs much better at\nextending the context size of non-finetuned models compared to PI [9].\nHowever, one major disadvantage of this method is that given it is not\njust an interpolation scheme, some dimensions are slightly extrapolated\nto \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\"\ninterpolation [6] yields inferior results to PI [9]. Furthermore, due to\nthe \"out-of-bound\" values, the theoretical scale factor s does not\naccurately describe the true context extension scale. In practice, the\nscale value s has to be set higher than the expected scale for a given\ncontext length extension.</p>\n</blockquote>\n<p>NTK4k32k\n<span class=\"math inline\">\\(\\alpha=L&#39;/L\\)</span>\n816</p>\n<h2 id=\"ntk-by-parts\">NTK-by-parts</h2>\n<p>NTK-by-partsNTKNTK-awareRoPENTK-by-parts</p>\n<p> <span class=\"math inline\">\\(j\\)</span> RoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p><span class=\"math inline\">\\(\\lambda_j\\)</span>\n <span class=\"math inline\">\\(j\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(L\\)</span>\nRoPE\n<span class=\"math inline\">\\(sin\\)</span>\n1/40<sub>1-1</sub>0\n<span class=\"math inline\">\\(j\\)</span>\n</p>\n<p>NTK-by-parts</p>\n<ul>\n<li> <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(\\lambda_j\\)</span> \n<br>\n</li>\n<li> <span class=\"math inline\">\\(\\lambda_j\\geq\\)</span>\n<br>\n</li>\n<li>NTK-aware interpolation</li>\n</ul>\n<p> <span class=\"math inline\">\\(r(j)=\\frac{L}{\\lambda_j}\\)</span>\n <span class=\"math inline\">\\(\\beta_1\\beta_2\\)</span> \n<span class=\"math inline\">\\(r(j)&lt;\\beta_1\\)</span>\n <span class=\"math inline\">\\(r(j)\\geq\n\\beta_2\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&amp;if&amp;r(j)&lt;\\beta_1\\\\1&amp;if&amp;r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&amp;otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>NTK-by-parts <span class=\"math inline\">\\(\\theta_j\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\beta_1\\beta_2\\)</span>\n <span class=\"math inline\">\\(\\beta_1=1\\beta_2=32\\)</span>\n1/32</p>\n<h2 id=\"dynamically-ntk-scaled-rope\">Dynamically NTK Scaled RoPE</h2>\n<p>NTK-Aware\nInterpolationRoPEattention\nscore <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(L\\)</span>  <span class=\"math inline\">\\(\\alpha\\)</span>\nbaseDynamically NTK Scaled\nRoPENTK</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(l\\)</span> \n<span class=\"math inline\">\\(l&gt;L\\)</span>  <span class=\"math inline\">\\(\\alpha\\)</span> 1 <span class=\"math inline\">\\(l\\leq L\\)</span> </p>\n<p>kv-cacheRoPE</p>\n<h2 id=\"yarn\">YaRN</h2>\n<p>tokensoftmaxRoPEtoken</p>\n<p>RoPEsoftmaxlogitsoftmax\n<span class=\"math inline\">\\(t&gt;1\\)</span>\nRoPE <span class=\"math inline\">\\(\\sqrt{t}\\)</span>\nRoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>Llama 1Llama 2<span class=\"math inline\">\\(\\begin{aligned}\\sqrt{\\frac1t}&amp;=0.1\\ln(\\alpha)+1.\\end{aligned}\\)</span>Llama</p>\n<p>YaRNNTK-by-partsattention\nscore</p>\n<p>YaRN</p>\n<h2 id=\"logn\">logn</h2>\n<p>lognattention <span class=\"math inline\">\\(\\sqrt{d}\\)</span>\nlogn<a href=\"https://zhuanlan.zhihu.com/p/678755776\"></a>YaRN</p>\n<p>tokentokentokenattention\nscore</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L&#39;}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(L&#39;&gt;L\\)</span>\nYaRN</p>\n<h2 id=\"\"></h2>\n<p>window\nattentionstreaming LLMLongLoRAFocus\nTransformer</p>\n<h1 id=\"\"></h1>\n<p>2k4k</p>\n<ul>\n<li><br>\n</li>\n<li>token</li>\n</ul>\n<p>attention score</p>\n<p>PINTKNTKlognYaRN</p>\n<h1 id=\"reference\">Reference</h1>\n<p>1transformerKV cache\nhttps://zhuanlan.zhihu.com/p/624740065<br>\n2EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION https://arxiv.org/pdf/2306.15595.pdf<br>\n3Transformer10RoPE\nhttps://kexue.fm/archives/9675<br>\n4YaRN: Efficient Context Window Extension of Large Language Models\nhttps://arxiv.org/pdf/2309.00071.pdf<br>\n5RoPE\nhttps://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ<br>\n6LLM https://zhuanlan.zhihu.com/p/645770522\n7prompt\nhttps://cloud.tencent.com/developer/article/2330611<br>\n8Transformer8\nhttps://spaces.ac.cn/archives/9444<br>\n9RoPE192K\nhttps://zhuanlan.zhihu.com/p/678755776 ***</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n"},{"title":":sliding window attention","abbrlink":"c61d17e3","date":"2024-03-12T09:26:00.000Z","_content":"\n//  \n\nLLM  \n\n[LLM](http://www.linsight.cn/c4da56c0.html)32k+/128k+[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)MQAGQAKV  \n\nSWAsliding window attention  \n\nQwenMistralSWA  \n\nMistral  \n\nMistral AIAI202352023912Mistral 7BMoEMistral 8x7B  \n\n20242  \n\n{% asset_img ms_invest_mistral.png MS %}  \n\n20242Mistral Large & 32kMMLUGPT4  \n\n{% asset_img mistral_large_performance.jpeg Mistral Large MMLU Performance %}  \n\nMistralOPENAIMETA  \n\n# SWA\n\nSWAMistralMistral 7BSWA  \n\n## Mistral 7B\n\n202310MistralMistral 7B[](https://arxiv.org/pdf/2310.06825.pdf)LlamaMistralGQASWA  \n\nMistral 7B  \n\n{% asset_img mistral_architechture.png Mistral Architechture %}  \n\nMistralkv=8GQAintermediate sizeLlama211008  \n\n## \n\ncausal attentiontokentoken  \n\n $s$ 1 $s^2$ KV Cache  \n\n/ $s$ \n\n1\n\n $[m,n]\\times[n,p]$  $[m,p]$ $m\\times p$  $n$  $n$  $2mpn$ floating point operationsFLOPs  \n\n[Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)  \n\nMHA $s$  $L$ hidden size $d_{model}$  $d_{q}$   $n_{q}$ $d_{model} = n_{q}\\times d_{q}$ operationFLOPs  \n\n<center>\n\n| Operation | FLOPsMHA |\n| :---- | :----: |\n| Attention: QKV | $6\\times s\\times h_{model}^{2}$  |\n| Attention: QK logits ( $QK^T$ ) | $2\\times s^2\\times h_{model}$ |\n| Attention: Softmax | $3\\times n_{q}\\times s^2$ |\n| Attention: Reduction (apply to $V$) | $2\\times s^2\\times h_{model}$ |\n| Attention: Outupt Linear Project | $2\\times s\\times h_{model}^{2}$ |\n\n</center>\n\nSoftmax $[1,s]$ softmax $3s$  $s$ exp $s$  $s$  $[s,s]$ softmax  $3s^2$  $n_{q}$ \n\noperationscalingdropout\n\nMistral 7BGQA  \n\nKVkv $n_{kv}$\n\n<center>\n\n| Operation | FLOPsGQA |\n| :---- | :----: |\n| Attention: QKV | $2\\times s\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times n_{kv})$  |\n\n</center>\n\nQK logitsSoftmaxReduction $s$   \n\n2\n\nKV Cache  \n\n$$\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\n\n\n$$\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\nMistral 7B16kKV_Cache2G  \n\nGQA16k+\n\n## SWA\n\nattention $s$  $s$ \n\nCNN  \n\n{% asset_img receptive_field_cnn.png CNN Receptive Field %}  \n\n3 $3\\times 3$ CNNsliding window  \n\nlayer 3layer 2 $3\\times 3$ layer 2  \n\nlayer 2layer 1 $3\\times 3$ layer 2 $3\\times 3$ layer 1 $5\\times 5$ layer 3<u>****</u> $5\\times 5$   \n\nlayer 4layer 4layer 1  $7\\times 7$   \n\n  \n\n  \n\nCNN  \n\n  \n\n  \n\n\n\nMistralSWA  \n\n{% asset_img mistral_swa.png Mistral SWA %}  \n\ncausal attentionattention mask  \n\nSWAattention mask33  \n\nCNNLLM  \n\nMistral 7B409632 $4096\\times 32=131,072$ 131k  \n\nattentionQK logitsSoftmaxReduction $s$ SWAoperation4k131k131k $32\\times 32=1024$   \n\n $s$ 131k $31/32$   \n\nSWA4kcausal attention4k\n\n>In practice, for a sequence length of 16K and W = 4096, changes made to FlashAttention [11] and xFormers [18] yield a 2x speed improvement over a vanilla attention baseline.\n\nMistralSWAFlashAttentionxFormers16k2  \n\n## KV Cache\n\nsliding windowKV Cache  \n\nSWAkv  \n\n $W=4$ 5token1tokenkv  \n\n{% asset_img rolling_buffer.png swa rolling buffer %}  \n\nthroughputcase  \n\n## Prompt\n\nRAGfunciton callprompt  \n\nGPT4system promptOPENAI  \n\n>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\" and the user's locale is \"en-US\"\nYour knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07.\nImage input capabilities: Enabled\n>\n>Tools\n>\n>python\n>\n>When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 60.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is disabled. Do not make external web requests or API calls as they will fail.\n>\n>dalle\n>\n>Whenever a description of an image is given, create a prompt that dalle can use to generate the image and abide to the following policy:\n>1. The prompt must be in English. Translate to English if needed.\n>2. DO NOT ask for permission to generate the image, just do it!\n>3. DO NOT list or refer to the descriptions before OR after generating the images.\n>4. Do not create more than 1 image, even if the user requests more.\n>5. Do not create images in the style of artists, creative professionals or studios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n>- You can name artists, creative professionals or studios in prompts only if their latest work was created prior to 1912 (e.g. Van Gogh, Goya)\n>- If asked to generate an image that would violate this policy, instead apply the following procedure: (a) substitute the artist's name with three adjectives that capture key aspects of the style; (b) include an associated artistic movement or era to provide context; and (c) mention the primary medium used by the artist\n>6. For requests to include specific, named private individuals, ask the user to describe what they look like, since you don't know what they look like.\n>7. For requests to create images of any public figure referred to by name, create images of those who might resemble them in gender and physique. But they shouldn't look like them. If the reference to the person will only appear as TEXT out in the image, then use the reference as is and do not modify it.\n>8. Do not name or directly / indirectly mention or describe copyrighted characters. Rewrite prompts to describe in detail a specific different character with a different specific color, hair style, or other defining visual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around 100 words long.\nExample dalle invocation:\n>{\n>\"prompt\": \"<insert prompt here>\"\n>}\n>namespace dalle {\n>\n>Create images from a text-only prompt.\ntype text2im = (_: {\nThe size of the requested image. Use 1024x1024 (square) as the default, 1792x1024 if the user requests a wide image, and 1024x1792 for full-body portraits. Always include this parameter in the request.\nn?: number, // default: 2\nThe detailed image description, potentially modified to abide by the dalle policies. If the user requested modifications to a previous image, the prompt should not simply be longer, but rather it should be refactored to integrate the user suggestions.\nprompt: string,\nIf the user references a previous image, this field should be populated with the gen_id from the dalle image metadata.\nreferenced_image_ids?: string[],\n}) => any;\n} // namespace dalle\n>\n>voice_mode\n>Voice mode functions are not available in text conversations.\n>namespace voice_mode {   } // namespace voice_mode\n>\n>browser\n>\n>You have the tool `browser`. Use `browser` in the following circumstances:\n>    - User is asking about current events or something that requires real-time information (weather, sports scores, etc.)\n>    - User is asking about some term you are totally unfamiliar with (it might be new)\n>    - User explicitly asks you to browse or provide links to references\n>\n>Given a query that requires retrieval, your turn will consist of three steps:\n>1. Call the search function to get a list of results.\n>2. Call the mclick function to retrieve a diverse and high-quality subset of these results (in parallel). Remember to SELECT AT LEAST 3 sources when using `mclick`.\n>3. Write a response to the user based on these results. In your response, cite sources using the citation format below.\n>\n>In some cases, you should repeat step 1 twice, if the initial results are unsatisfactory, and you believe that you can refine the query to get better results.\n>\n>You can also open a url directly if one is provided by the user. Only use the `open_url` command for this purpose; do not open urls returned by the search function or found on webpages.\n>\n>The `browser` tool has the following commands:\n\t`search(query: str, recency_days: int)` Issues a query to a search engine and displays the results.\n         `mclick(ids: list[str])`. Retrieves the contents of the webpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST 3 and at most 10 pages. Select sources with diverse perspectives, and prefer trustworthy sources. Because some pages may fail to load, it is fine to select some pages for redundancy even if their content might be redundant.\n\t`open_url(url: str)` Opens the given URL and displays it.\n>\n>For citing quotes from the 'browser' tool: please render in this format: {message idx}{link text}.\nFor long citations: please render in this format: [link text](message idx).\nOtherwise do not render links.\n\nsystem promptkvsystem promptsliding windowsystem promptkv\n\n $W=4$system prompt9system promptkv [4,4,1]   \n\nwindowattention mask0  \n\nattention mask  \n\n  \n\n  \n\nprompt  \n\n{% asset_img prefill_and_chunking.png prefill and chunking %}  \n\nFlashAttention/PagedAttention\n\nMistral 7BLlamaLlama 34B  \n\n{% asset_img mistral_perf.png mistral performance %}  \n\nMistral7B  \n\n# Sparse Attention\n\nSWAsparse attentionsparse attention  \n\nsparse attention  \n\n## Longformer\n\nMistralSWA  \n\n2020[Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150.pdf)SWAsparse attention  \n\nLongformer  \n\n{% asset_img longformer_attention.png longformer %}  \n\nbSWABert  \n\nSWAdilated sliding window    \n\n{% asset_img dilated_conv.png dilated convolution %}  \n\nattentionSWAdilated sliding window  \n\n  \n\nBert[CLS] tokentoken  \n\nGPTinstructionprompt  \n\ntokenglobal attentionsliding windowd  \n\n## Big Bird\n\n2020Longformersparse attention[Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)  \n\nsliding windowglobal attentionLongformerBig Birdrandom attention  \n\n{% asset_img big_bird_attention.png big bird attention %}  \n\n $r=2$ 2\n\n#   \n\nSWA  \n\nSWAsparse attention<big>****</big>global + local attentionflash attentionrandom attention\n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n1Mistral 7B https://arxiv.org/pdf/2310.06825.pdf  \n2Longformer: The Long-Document Transformer \nhttps://arxiv.org/pdf/2004.05150.pdf  \n3Training Compute-Optimal Large Language Models https://arxiv.org/pdf/2203.15556.pdf  \n4GPT-4 System Prompt Revealed https://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed  \n5Big Bird: Transformers for Longer Sequences https://arxiv.org/abs/2007.14062  ","source":"_posts/cs/nlp/2024/03/LLM-sliding-window-attention.md","raw":"---\ntitle: ':sliding window attention'\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - attention\n  - sliding window attention\n  - sparse attention\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: c61d17e3\ndate: 2024-03-12 17:26:00\n---\n\n//  \n\nLLM  \n\n[LLM](http://www.linsight.cn/c4da56c0.html)32k+/128k+[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)MQAGQAKV  \n\nSWAsliding window attention  \n\nQwenMistralSWA  \n\nMistral  \n\nMistral AIAI202352023912Mistral 7BMoEMistral 8x7B  \n\n20242  \n\n{% asset_img ms_invest_mistral.png MS %}  \n\n20242Mistral Large & 32kMMLUGPT4  \n\n{% asset_img mistral_large_performance.jpeg Mistral Large MMLU Performance %}  \n\nMistralOPENAIMETA  \n\n# SWA\n\nSWAMistralMistral 7BSWA  \n\n## Mistral 7B\n\n202310MistralMistral 7B[](https://arxiv.org/pdf/2310.06825.pdf)LlamaMistralGQASWA  \n\nMistral 7B  \n\n{% asset_img mistral_architechture.png Mistral Architechture %}  \n\nMistralkv=8GQAintermediate sizeLlama211008  \n\n## \n\ncausal attentiontokentoken  \n\n $s$ 1 $s^2$ KV Cache  \n\n/ $s$ \n\n1\n\n $[m,n]\\times[n,p]$  $[m,p]$ $m\\times p$  $n$  $n$  $2mpn$ floating point operationsFLOPs  \n\n[Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)  \n\nMHA $s$  $L$ hidden size $d_{model}$  $d_{q}$   $n_{q}$ $d_{model} = n_{q}\\times d_{q}$ operationFLOPs  \n\n<center>\n\n| Operation | FLOPsMHA |\n| :---- | :----: |\n| Attention: QKV | $6\\times s\\times h_{model}^{2}$  |\n| Attention: QK logits ( $QK^T$ ) | $2\\times s^2\\times h_{model}$ |\n| Attention: Softmax | $3\\times n_{q}\\times s^2$ |\n| Attention: Reduction (apply to $V$) | $2\\times s^2\\times h_{model}$ |\n| Attention: Outupt Linear Project | $2\\times s\\times h_{model}^{2}$ |\n\n</center>\n\nSoftmax $[1,s]$ softmax $3s$  $s$ exp $s$  $s$  $[s,s]$ softmax  $3s^2$  $n_{q}$ \n\noperationscalingdropout\n\nMistral 7BGQA  \n\nKVkv $n_{kv}$\n\n<center>\n\n| Operation | FLOPsGQA |\n| :---- | :----: |\n| Attention: QKV | $2\\times s\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times n_{kv})$  |\n\n</center>\n\nQK logitsSoftmaxReduction $s$   \n\n2\n\nKV Cache  \n\n$$\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\n\n\n$$\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\nMistral 7B16kKV_Cache2G  \n\nGQA16k+\n\n## SWA\n\nattention $s$  $s$ \n\nCNN  \n\n{% asset_img receptive_field_cnn.png CNN Receptive Field %}  \n\n3 $3\\times 3$ CNNsliding window  \n\nlayer 3layer 2 $3\\times 3$ layer 2  \n\nlayer 2layer 1 $3\\times 3$ layer 2 $3\\times 3$ layer 1 $5\\times 5$ layer 3<u>****</u> $5\\times 5$   \n\nlayer 4layer 4layer 1  $7\\times 7$   \n\n  \n\n  \n\nCNN  \n\n  \n\n  \n\n\n\nMistralSWA  \n\n{% asset_img mistral_swa.png Mistral SWA %}  \n\ncausal attentionattention mask  \n\nSWAattention mask33  \n\nCNNLLM  \n\nMistral 7B409632 $4096\\times 32=131,072$ 131k  \n\nattentionQK logitsSoftmaxReduction $s$ SWAoperation4k131k131k $32\\times 32=1024$   \n\n $s$ 131k $31/32$   \n\nSWA4kcausal attention4k\n\n>In practice, for a sequence length of 16K and W = 4096, changes made to FlashAttention [11] and xFormers [18] yield a 2x speed improvement over a vanilla attention baseline.\n\nMistralSWAFlashAttentionxFormers16k2  \n\n## KV Cache\n\nsliding windowKV Cache  \n\nSWAkv  \n\n $W=4$ 5token1tokenkv  \n\n{% asset_img rolling_buffer.png swa rolling buffer %}  \n\nthroughputcase  \n\n## Prompt\n\nRAGfunciton callprompt  \n\nGPT4system promptOPENAI  \n\n>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\" and the user's locale is \"en-US\"\nYour knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07.\nImage input capabilities: Enabled\n>\n>Tools\n>\n>python\n>\n>When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 60.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is disabled. Do not make external web requests or API calls as they will fail.\n>\n>dalle\n>\n>Whenever a description of an image is given, create a prompt that dalle can use to generate the image and abide to the following policy:\n>1. The prompt must be in English. Translate to English if needed.\n>2. DO NOT ask for permission to generate the image, just do it!\n>3. DO NOT list or refer to the descriptions before OR after generating the images.\n>4. Do not create more than 1 image, even if the user requests more.\n>5. Do not create images in the style of artists, creative professionals or studios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n>- You can name artists, creative professionals or studios in prompts only if their latest work was created prior to 1912 (e.g. Van Gogh, Goya)\n>- If asked to generate an image that would violate this policy, instead apply the following procedure: (a) substitute the artist's name with three adjectives that capture key aspects of the style; (b) include an associated artistic movement or era to provide context; and (c) mention the primary medium used by the artist\n>6. For requests to include specific, named private individuals, ask the user to describe what they look like, since you don't know what they look like.\n>7. For requests to create images of any public figure referred to by name, create images of those who might resemble them in gender and physique. But they shouldn't look like them. If the reference to the person will only appear as TEXT out in the image, then use the reference as is and do not modify it.\n>8. Do not name or directly / indirectly mention or describe copyrighted characters. Rewrite prompts to describe in detail a specific different character with a different specific color, hair style, or other defining visual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around 100 words long.\nExample dalle invocation:\n>{\n>\"prompt\": \"<insert prompt here>\"\n>}\n>namespace dalle {\n>\n>Create images from a text-only prompt.\ntype text2im = (_: {\nThe size of the requested image. Use 1024x1024 (square) as the default, 1792x1024 if the user requests a wide image, and 1024x1792 for full-body portraits. Always include this parameter in the request.\nn?: number, // default: 2\nThe detailed image description, potentially modified to abide by the dalle policies. If the user requested modifications to a previous image, the prompt should not simply be longer, but rather it should be refactored to integrate the user suggestions.\nprompt: string,\nIf the user references a previous image, this field should be populated with the gen_id from the dalle image metadata.\nreferenced_image_ids?: string[],\n}) => any;\n} // namespace dalle\n>\n>voice_mode\n>Voice mode functions are not available in text conversations.\n>namespace voice_mode {   } // namespace voice_mode\n>\n>browser\n>\n>You have the tool `browser`. Use `browser` in the following circumstances:\n>    - User is asking about current events or something that requires real-time information (weather, sports scores, etc.)\n>    - User is asking about some term you are totally unfamiliar with (it might be new)\n>    - User explicitly asks you to browse or provide links to references\n>\n>Given a query that requires retrieval, your turn will consist of three steps:\n>1. Call the search function to get a list of results.\n>2. Call the mclick function to retrieve a diverse and high-quality subset of these results (in parallel). Remember to SELECT AT LEAST 3 sources when using `mclick`.\n>3. Write a response to the user based on these results. In your response, cite sources using the citation format below.\n>\n>In some cases, you should repeat step 1 twice, if the initial results are unsatisfactory, and you believe that you can refine the query to get better results.\n>\n>You can also open a url directly if one is provided by the user. Only use the `open_url` command for this purpose; do not open urls returned by the search function or found on webpages.\n>\n>The `browser` tool has the following commands:\n\t`search(query: str, recency_days: int)` Issues a query to a search engine and displays the results.\n         `mclick(ids: list[str])`. Retrieves the contents of the webpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST 3 and at most 10 pages. Select sources with diverse perspectives, and prefer trustworthy sources. Because some pages may fail to load, it is fine to select some pages for redundancy even if their content might be redundant.\n\t`open_url(url: str)` Opens the given URL and displays it.\n>\n>For citing quotes from the 'browser' tool: please render in this format: {message idx}{link text}.\nFor long citations: please render in this format: [link text](message idx).\nOtherwise do not render links.\n\nsystem promptkvsystem promptsliding windowsystem promptkv\n\n $W=4$system prompt9system promptkv [4,4,1]   \n\nwindowattention mask0  \n\nattention mask  \n\n  \n\n  \n\nprompt  \n\n{% asset_img prefill_and_chunking.png prefill and chunking %}  \n\nFlashAttention/PagedAttention\n\nMistral 7BLlamaLlama 34B  \n\n{% asset_img mistral_perf.png mistral performance %}  \n\nMistral7B  \n\n# Sparse Attention\n\nSWAsparse attentionsparse attention  \n\nsparse attention  \n\n## Longformer\n\nMistralSWA  \n\n2020[Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150.pdf)SWAsparse attention  \n\nLongformer  \n\n{% asset_img longformer_attention.png longformer %}  \n\nbSWABert  \n\nSWAdilated sliding window    \n\n{% asset_img dilated_conv.png dilated convolution %}  \n\nattentionSWAdilated sliding window  \n\n  \n\nBert[CLS] tokentoken  \n\nGPTinstructionprompt  \n\ntokenglobal attentionsliding windowd  \n\n## Big Bird\n\n2020Longformersparse attention[Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)  \n\nsliding windowglobal attentionLongformerBig Birdrandom attention  \n\n{% asset_img big_bird_attention.png big bird attention %}  \n\n $r=2$ 2\n\n#   \n\nSWA  \n\nSWAsparse attention<big>****</big>global + local attentionflash attentionrandom attention\n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n1Mistral 7B https://arxiv.org/pdf/2310.06825.pdf  \n2Longformer: The Long-Document Transformer \nhttps://arxiv.org/pdf/2004.05150.pdf  \n3Training Compute-Optimal Large Language Models https://arxiv.org/pdf/2203.15556.pdf  \n4GPT-4 System Prompt Revealed https://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed  \n5Big Bird: Transformers for Longer Sequences https://arxiv.org/abs/2007.14062  ","slug":"cs/nlp/2024/03/LLM-sliding-window-attention","published":1,"updated":"2024-03-16T04:25:08.847Z","comments":1,"layout":"post","photos":[],"_id":"clttl4al00004rb4k186cc9s1","content":"<p>//</p>\n<p>LLM</p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a>32k+/128k+<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a>MQAGQAKV</p>\n<p>SWAsliding\nwindow attention</p>\n<p>QwenMistralSWA</p>\n<p>Mistral</p>\n<p>Mistral\nAIAI202352023912Mistral\n7BMoEMistral 8x7B</p>\n<p>20242</p>\n<img src=\"/c61d17e3/ms_invest_mistral.png\" class title=\"MS\">\n<p>20242Mistral Large &amp;\n32kMMLUGPT4</p>\n<img src=\"/c61d17e3/mistral_large_performance.jpeg\" class title=\"Mistral Large MMLU Performance\">\n<p>MistralOPENAIMETA</p>\n<h1 id=\"swa\">SWA</h1>\n<p>SWAMistralMistral\n7BSWA</p>\n<h2 id=\"mistral-7b\">Mistral 7B</h2>\n<p>202310MistralMistral 7B<a href=\"https://arxiv.org/pdf/2310.06825.pdf\"></a>LlamaMistralGQASWA</p>\n<p>Mistral 7B</p>\n<img src=\"/c61d17e3/mistral_architechture.png\" class title=\"Mistral Architechture\">\n<p>Mistralkv=8GQAintermediate\nsizeLlama211008</p>\n<h2 id=\"\"></h2>\n<p>causal\nattentiontokentoken</p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n1 <span class=\"math inline\">\\(s^2\\)</span> KV\nCache</p>\n<p>/ <span class=\"math inline\">\\(s\\)</span>\n</p>\n<p>1</p>\n<p> <span class=\"math inline\">\\([m,n]\\times[n,p]\\)</span>  <span class=\"math inline\">\\([m,p]\\)</span> <span class=\"math inline\">\\(m\\times p\\)</span>  <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(2mpn\\)</span> floating point\noperationsFLOPs</p>\n<p><a href=\"https://arxiv.org/pdf/2203.15556.pdf\">Training\nCompute-Optimal Large Language Models</a></p>\n<p>MHA <span class=\"math inline\">\\(s\\)</span>\n <span class=\"math inline\">\\(L\\)</span> hidden size\n<span class=\"math inline\">\\(d_{model}\\)</span>  <span class=\"math inline\">\\(d_{q}\\)</span>   <span class=\"math inline\">\\(n_{q}\\)</span> <span class=\"math inline\">\\(d_{model} = n_{q}\\times d_{q}\\)</span>\noperationFLOPs</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPsMHA</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(6\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: QK logits ( <span class=\"math inline\">\\(QK^T\\)</span> )</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns^2\\times h_{model}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Softmax</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(3\\times\nn_{q}\\times s^2\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: Reduction (apply to <span class=\"math inline\">\\(V\\)</span>)</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns^2\\times h_{model}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Outupt Linear Project</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>Softmax <span class=\"math inline\">\\([1,s]\\)</span>\nsoftmax <span class=\"math inline\">\\(3s\\)</span> \n<span class=\"math inline\">\\(s\\)</span> exp <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\([s,s]\\)</span> softmax <span class=\"math inline\">\\(3s^2\\)</span> \n<span class=\"math inline\">\\(n_{q}\\)</span> </p>\n<p>operationscalingdropout</p>\n<p>Mistral 7BGQA</p>\n<p>KVkv <span class=\"math inline\">\\(n_{kv}\\)</span></p>\n<center>\n<table>\n<colgroup>\n<col style=\"width: 45%\">\n<col style=\"width: 54%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPsGQA</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times\nn_{kv})\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>QK logitsSoftmaxReduction <span class=\"math inline\">\\(s\\)</span> </p>\n<p>2</p>\n<p>KV Cache</p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p>Mistral 7B16kKV_Cache2G</p>\n<p>GQA16k+</p>\n<h2 id=\"swa\">SWA</h2>\n<p>attention <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(s\\)</span>\n</p>\n<p>CNN</p>\n<img src=\"/c61d17e3/receptive_field_cnn.png\" class title=\"CNN Receptive Field\">\n<p>3 <span class=\"math inline\">\\(3\\times 3\\)</span>\nCNNsliding window</p>\n<p>layer 3layer 2 <span class=\"math inline\">\\(3\\times 3\\)</span> layer\n2</p>\n<p>layer 2layer 1 <span class=\"math inline\">\\(3\\times 3\\)</span> layer 2 <span class=\"math inline\">\\(3\\times 3\\)</span> layer 1\n<span class=\"math inline\">\\(5\\times 5\\)</span> layer\n3<u><strong></strong></u> <span class=\"math inline\">\\(5\\times 5\\)</span> </p>\n<p>layer 4layer\n4layer 1  <span class=\"math inline\">\\(7\\times 7\\)</span> </p>\n<p></p>\n<p></p>\n<p>CNN</p>\n<p></p>\n<p></p>\n<p></p>\n<p>MistralSWA</p>\n<img src=\"/c61d17e3/mistral_swa.png\" class title=\"Mistral SWA\">\n<p>causal\nattentionattention\nmask</p>\n<p>SWAattention\nmask33</p>\n<p>CNNLLM</p>\n<p>Mistral 7B409632\n<span class=\"math inline\">\\(4096\\times 32=131,072\\)</span>\n131k</p>\n<p>attentionQK\nlogitsSoftmaxReduction <span class=\"math inline\">\\(s\\)</span>\nSWAoperation4k131k131k\n<span class=\"math inline\">\\(32\\times 32=1024\\)</span> </p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n131k <span class=\"math inline\">\\(31/32\\)</span> </p>\n<p>SWA4kcausal\nattention4k</p>\n<blockquote>\n<p>In practice, for a sequence length of 16K and W = 4096, changes made\nto FlashAttention [11] and xFormers [18] yield a 2x speed improvement\nover a vanilla attention baseline.</p>\n</blockquote>\n<p>MistralSWAFlashAttentionxFormers16k2</p>\n<h2 id=\"kv-cache\">KV Cache</h2>\n<p>sliding windowKV\nCache</p>\n<p>SWAkv</p>\n<p> <span class=\"math inline\">\\(W=4\\)</span>\n5token1tokenkv</p>\n<img src=\"/c61d17e3/rolling_buffer.png\" class title=\"swa rolling buffer\">\n<p>throughputcase</p>\n<h2 id=\"prompt\">Prompt</h2>\n<p>RAGfunciton\ncallprompt</p>\n<p>GPT4system\npromptOPENAI</p>\n<blockquote>\n<p>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\nAppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\nand the user's locale is \"en-US\" Your knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07. Image input capabilities: Enabled</p>\n<p>Tools</p>\n<p>python</p>\n<p>When you send a message containing Python code to python, it will be\nexecuted in a stateful Jupyter notebook environment. python will respond\nwith the output of the execution or time out after 60.0 seconds. The\ndrive at '/mnt/data' can be used to save and persist user files.\nInternet access for this session is disabled. Do not make external web\nrequests or API calls as they will fail.</p>\n<p>dalle</p>\n<p>Whenever a description of an image is given, create a prompt that\ndalle can use to generate the image and abide to the following policy:\n1. The prompt must be in English. Translate to English if needed. 2. DO\nNOT ask for permission to generate the image, just do it! 3. DO NOT list\nor refer to the descriptions before OR after generating the images. 4.\nDo not create more than 1 image, even if the user requests more. 5. Do\nnot create images in the style of artists, creative professionals or\nstudios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n- You can name artists, creative professionals or studios in prompts\nonly if their latest work was created prior to 1912 (e.g. Van Gogh,\nGoya) - If asked to generate an image that would violate this policy,\ninstead apply the following procedure: (a) substitute the artist's name\nwith three adjectives that capture key aspects of the style; (b) include\nan associated artistic movement or era to provide context; and (c)\nmention the primary medium used by the artist 6. For requests to include\nspecific, named private individuals, ask the user to describe what they\nlook like, since you don't know what they look like. 7. For requests to\ncreate images of any public figure referred to by name, create images of\nthose who might resemble them in gender and physique. But they shouldn't\nlook like them. If the reference to the person will only appear as TEXT\nout in the image, then use the reference as is and do not modify it. 8.\nDo not name or directly / indirectly mention or describe copyrighted\ncharacters. Rewrite prompts to describe in detail a specific different\ncharacter with a different specific color, hair style, or other defining\nvisual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around\n100 words long. Example dalle invocation: { \"prompt\":\n\"<insert prompt here>\" } namespace dalle {</insert></p>\n<p>Create images from a text-only prompt. type text2im = (_: { The size\nof the requested image. Use 1024x1024 (square) as the default, 1792x1024\nif the user requests a wide image, and 1024x1792 for full-body\nportraits. Always include this parameter in the request. n?: number, //\ndefault: 2 The detailed image description, potentially modified to abide\nby the dalle policies. If the user requested modifications to a previous\nimage, the prompt should not simply be longer, but rather it should be\nrefactored to integrate the user suggestions. prompt: string, If the\nuser references a previous image, this field should be populated with\nthe gen_id from the dalle image metadata. referenced_image_ids?:\nstring[], }) =&gt; any; } // namespace dalle</p>\n<p>voice_mode Voice mode functions are not available in text\nconversations. namespace voice_mode { } // namespace voice_mode</p>\n<p>browser</p>\n<p>You have the tool <code>browser</code>. Use <code>browser</code> in\nthe following circumstances: - User is asking about current events or\nsomething that requires real-time information (weather, sports scores,\netc.) - User is asking about some term you are totally unfamiliar with\n(it might be new) - User explicitly asks you to browse or provide links\nto references</p>\n<p>Given a query that requires retrieval, your turn will consist of\nthree steps: 1. Call the search function to get a list of results. 2.\nCall the mclick function to retrieve a diverse and high-quality subset\nof these results (in parallel). Remember to SELECT AT LEAST 3 sources\nwhen using <code>mclick</code>. 3. Write a response to the user based on\nthese results. In your response, cite sources using the citation format\nbelow.</p>\n<p>In some cases, you should repeat step 1 twice, if the initial results\nare unsatisfactory, and you believe that you can refine the query to get\nbetter results.</p>\n<p>You can also open a url directly if one is provided by the user. Only\nuse the <code>open_url</code> command for this purpose; do not open urls\nreturned by the search function or found on webpages.</p>\n<p>The <code>browser</code> tool has the following commands:\n<code>search(query: str, recency_days: int)</code> Issues a query to a\nsearch engine and displays the results.\n<code>mclick(ids: list[str])</code>. Retrieves the contents of the\nwebpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST\n3 and at most 10 pages. Select sources with diverse perspectives, and\nprefer trustworthy sources. Because some pages may fail to load, it is\nfine to select some pages for redundancy even if their content might be\nredundant. <code>open_url(url: str)</code> Opens the given URL and\ndisplays it.</p>\n<p>For citing quotes from the 'browser' tool: please render in this\nformat: {message idx}{link text}. For long citations: please render\nin this format: <a href=\"message%20idx\">link text</a>. Otherwise do not\nrender links.</p>\n</blockquote>\n<p>system\npromptkvsystem\npromptsliding windowsystem\npromptkv</p>\n<p> <span class=\"math inline\">\\(W=4\\)</span>system\nprompt9system promptkv [4,4,1] </p>\n<p>windowattention\nmask0</p>\n<p>attention\nmask</p>\n<p></p>\n<p></p>\n<p>prompt</p>\n<img src=\"/c61d17e3/prefill_and_chunking.png\" class title=\"prefill and chunking\">\n<p>FlashAttention/PagedAttention</p>\n<p>Mistral\n7BLlamaLlama\n34B</p>\n<img src=\"/c61d17e3/mistral_perf.png\" class title=\"mistral performance\">\n<p>Mistral7B</p>\n<h1 id=\"sparse-attention\">Sparse Attention</h1>\n<p>SWAsparse attentionsparse\nattention</p>\n<p>sparse\nattention</p>\n<h2 id=\"longformer\">Longformer</h2>\n<p>MistralSWA</p>\n<p>2020<a href=\"https://arxiv.org/pdf/2004.05150.pdf\">Longformer:\nThe Long-Document Transformer</a>SWAsparse\nattention</p>\n<p>Longformer</p>\n<img src=\"/c61d17e3/longformer_attention.png\" class title=\"longformer\">\n<p>bSWABert</p>\n<p>SWAdilated sliding\nwindow</p>\n<img src=\"/c61d17e3/dilated_conv.png\" class title=\"dilated convolution\">\n<p>attentionSWAdilated sliding\nwindow</p>\n<p></p>\n<p>Bert[CLS]\ntokentoken</p>\n<p>GPTinstructionprompt</p>\n<p>tokenglobal\nattentionsliding windowd</p>\n<h2 id=\"big-bird\">Big Bird</h2>\n<p>2020Longformersparse\nattention<a href=\"https://arxiv.org/abs/2007.14062\">Big Bird: Transformers for\nLonger Sequences</a></p>\n<p>sliding windowglobal attentionLongformerBig\nBirdrandom attention</p>\n<img src=\"/c61d17e3/big_bird_attention.png\" class title=\"big bird attention\">\n<p> <span class=\"math inline\">\\(r=2\\)</span>\n2</p>\n<h1 id=\"\"></h1>\n<p>SWA</p>\n<p>SWAsparse\nattention<big><strong></strong></big>global\n+ local attentionflash attentionrandom\nattention</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Mistral 7B https://arxiv.org/pdf/2310.06825.pdf<br>\n2Longformer: The Long-Document Transformer\nhttps://arxiv.org/pdf/2004.05150.pdf<br>\n3Training Compute-Optimal Large Language Models\nhttps://arxiv.org/pdf/2203.15556.pdf<br>\n4GPT-4 System Prompt Revealed\nhttps://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed<br>\n5Big Bird: Transformers for Longer Sequences\nhttps://arxiv.org/abs/2007.14062</p>\n","length":10769,"excerpt":"","more":"<p>//</p>\n<p>LLM</p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a>32k+/128k+<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a>MQAGQAKV</p>\n<p>SWAsliding\nwindow attention</p>\n<p>QwenMistralSWA</p>\n<p>Mistral</p>\n<p>Mistral\nAIAI202352023912Mistral\n7BMoEMistral 8x7B</p>\n<p>20242</p>\n<img src=\"/c61d17e3/ms_invest_mistral.png\" class title=\"MS\">\n<p>20242Mistral Large &amp;\n32kMMLUGPT4</p>\n<img src=\"/c61d17e3/mistral_large_performance.jpeg\" class title=\"Mistral Large MMLU Performance\">\n<p>MistralOPENAIMETA</p>\n<h1 id=\"swa\">SWA</h1>\n<p>SWAMistralMistral\n7BSWA</p>\n<h2 id=\"mistral-7b\">Mistral 7B</h2>\n<p>202310MistralMistral 7B<a href=\"https://arxiv.org/pdf/2310.06825.pdf\"></a>LlamaMistralGQASWA</p>\n<p>Mistral 7B</p>\n<img src=\"/c61d17e3/mistral_architechture.png\" class title=\"Mistral Architechture\">\n<p>Mistralkv=8GQAintermediate\nsizeLlama211008</p>\n<h2 id=\"\"></h2>\n<p>causal\nattentiontokentoken</p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n1 <span class=\"math inline\">\\(s^2\\)</span> KV\nCache</p>\n<p>/ <span class=\"math inline\">\\(s\\)</span>\n</p>\n<p>1</p>\n<p> <span class=\"math inline\">\\([m,n]\\times[n,p]\\)</span>  <span class=\"math inline\">\\([m,p]\\)</span> <span class=\"math inline\">\\(m\\times p\\)</span>  <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(2mpn\\)</span> floating point\noperationsFLOPs</p>\n<p><a href=\"https://arxiv.org/pdf/2203.15556.pdf\">Training\nCompute-Optimal Large Language Models</a></p>\n<p>MHA <span class=\"math inline\">\\(s\\)</span>\n <span class=\"math inline\">\\(L\\)</span> hidden size\n<span class=\"math inline\">\\(d_{model}\\)</span>  <span class=\"math inline\">\\(d_{q}\\)</span>   <span class=\"math inline\">\\(n_{q}\\)</span> <span class=\"math inline\">\\(d_{model} = n_{q}\\times d_{q}\\)</span>\noperationFLOPs</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPsMHA</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(6\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: QK logits ( <span class=\"math inline\">\\(QK^T\\)</span> )</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns^2\\times h_{model}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Softmax</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(3\\times\nn_{q}\\times s^2\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: Reduction (apply to <span class=\"math inline\">\\(V\\)</span>)</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns^2\\times h_{model}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Outupt Linear Project</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>Softmax <span class=\"math inline\">\\([1,s]\\)</span>\nsoftmax <span class=\"math inline\">\\(3s\\)</span> \n<span class=\"math inline\">\\(s\\)</span> exp <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\([s,s]\\)</span> softmax <span class=\"math inline\">\\(3s^2\\)</span> \n<span class=\"math inline\">\\(n_{q}\\)</span> </p>\n<p>operationscalingdropout</p>\n<p>Mistral 7BGQA</p>\n<p>KVkv <span class=\"math inline\">\\(n_{kv}\\)</span></p>\n<center>\n<table>\n<colgroup>\n<col style=\"width: 45%\">\n<col style=\"width: 54%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPsGQA</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times\nn_{kv})\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>QK logitsSoftmaxReduction <span class=\"math inline\">\\(s\\)</span> </p>\n<p>2</p>\n<p>KV Cache</p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p>Mistral 7B16kKV_Cache2G</p>\n<p>GQA16k+</p>\n<h2 id=\"swa\">SWA</h2>\n<p>attention <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(s\\)</span>\n</p>\n<p>CNN</p>\n<img src=\"/c61d17e3/receptive_field_cnn.png\" class title=\"CNN Receptive Field\">\n<p>3 <span class=\"math inline\">\\(3\\times 3\\)</span>\nCNNsliding window</p>\n<p>layer 3layer 2 <span class=\"math inline\">\\(3\\times 3\\)</span> layer\n2</p>\n<p>layer 2layer 1 <span class=\"math inline\">\\(3\\times 3\\)</span> layer 2 <span class=\"math inline\">\\(3\\times 3\\)</span> layer 1\n<span class=\"math inline\">\\(5\\times 5\\)</span> layer\n3<u><strong></strong></u> <span class=\"math inline\">\\(5\\times 5\\)</span> </p>\n<p>layer 4layer\n4layer 1  <span class=\"math inline\">\\(7\\times 7\\)</span> </p>\n<p></p>\n<p></p>\n<p>CNN</p>\n<p></p>\n<p></p>\n<p></p>\n<p>MistralSWA</p>\n<img src=\"/c61d17e3/mistral_swa.png\" class title=\"Mistral SWA\">\n<p>causal\nattentionattention\nmask</p>\n<p>SWAattention\nmask33</p>\n<p>CNNLLM</p>\n<p>Mistral 7B409632\n<span class=\"math inline\">\\(4096\\times 32=131,072\\)</span>\n131k</p>\n<p>attentionQK\nlogitsSoftmaxReduction <span class=\"math inline\">\\(s\\)</span>\nSWAoperation4k131k131k\n<span class=\"math inline\">\\(32\\times 32=1024\\)</span> </p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n131k <span class=\"math inline\">\\(31/32\\)</span> </p>\n<p>SWA4kcausal\nattention4k</p>\n<blockquote>\n<p>In practice, for a sequence length of 16K and W = 4096, changes made\nto FlashAttention [11] and xFormers [18] yield a 2x speed improvement\nover a vanilla attention baseline.</p>\n</blockquote>\n<p>MistralSWAFlashAttentionxFormers16k2</p>\n<h2 id=\"kv-cache\">KV Cache</h2>\n<p>sliding windowKV\nCache</p>\n<p>SWAkv</p>\n<p> <span class=\"math inline\">\\(W=4\\)</span>\n5token1tokenkv</p>\n<img src=\"/c61d17e3/rolling_buffer.png\" class title=\"swa rolling buffer\">\n<p>throughputcase</p>\n<h2 id=\"prompt\">Prompt</h2>\n<p>RAGfunciton\ncallprompt</p>\n<p>GPT4system\npromptOPENAI</p>\n<blockquote>\n<p>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\nAppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\nand the user's locale is \"en-US\" Your knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07. Image input capabilities: Enabled</p>\n<p>Tools</p>\n<p>python</p>\n<p>When you send a message containing Python code to python, it will be\nexecuted in a stateful Jupyter notebook environment. python will respond\nwith the output of the execution or time out after 60.0 seconds. The\ndrive at '/mnt/data' can be used to save and persist user files.\nInternet access for this session is disabled. Do not make external web\nrequests or API calls as they will fail.</p>\n<p>dalle</p>\n<p>Whenever a description of an image is given, create a prompt that\ndalle can use to generate the image and abide to the following policy:\n1. The prompt must be in English. Translate to English if needed. 2. DO\nNOT ask for permission to generate the image, just do it! 3. DO NOT list\nor refer to the descriptions before OR after generating the images. 4.\nDo not create more than 1 image, even if the user requests more. 5. Do\nnot create images in the style of artists, creative professionals or\nstudios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n- You can name artists, creative professionals or studios in prompts\nonly if their latest work was created prior to 1912 (e.g. Van Gogh,\nGoya) - If asked to generate an image that would violate this policy,\ninstead apply the following procedure: (a) substitute the artist's name\nwith three adjectives that capture key aspects of the style; (b) include\nan associated artistic movement or era to provide context; and (c)\nmention the primary medium used by the artist 6. For requests to include\nspecific, named private individuals, ask the user to describe what they\nlook like, since you don't know what they look like. 7. For requests to\ncreate images of any public figure referred to by name, create images of\nthose who might resemble them in gender and physique. But they shouldn't\nlook like them. If the reference to the person will only appear as TEXT\nout in the image, then use the reference as is and do not modify it. 8.\nDo not name or directly / indirectly mention or describe copyrighted\ncharacters. Rewrite prompts to describe in detail a specific different\ncharacter with a different specific color, hair style, or other defining\nvisual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around\n100 words long. Example dalle invocation: { \"prompt\":\n\"<insert prompt here>\" } namespace dalle {</insert></p>\n<p>Create images from a text-only prompt. type text2im = (_: { The size\nof the requested image. Use 1024x1024 (square) as the default, 1792x1024\nif the user requests a wide image, and 1024x1792 for full-body\nportraits. Always include this parameter in the request. n?: number, //\ndefault: 2 The detailed image description, potentially modified to abide\nby the dalle policies. If the user requested modifications to a previous\nimage, the prompt should not simply be longer, but rather it should be\nrefactored to integrate the user suggestions. prompt: string, If the\nuser references a previous image, this field should be populated with\nthe gen_id from the dalle image metadata. referenced_image_ids?:\nstring[], }) =&gt; any; } // namespace dalle</p>\n<p>voice_mode Voice mode functions are not available in text\nconversations. namespace voice_mode { } // namespace voice_mode</p>\n<p>browser</p>\n<p>You have the tool <code>browser</code>. Use <code>browser</code> in\nthe following circumstances: - User is asking about current events or\nsomething that requires real-time information (weather, sports scores,\netc.) - User is asking about some term you are totally unfamiliar with\n(it might be new) - User explicitly asks you to browse or provide links\nto references</p>\n<p>Given a query that requires retrieval, your turn will consist of\nthree steps: 1. Call the search function to get a list of results. 2.\nCall the mclick function to retrieve a diverse and high-quality subset\nof these results (in parallel). Remember to SELECT AT LEAST 3 sources\nwhen using <code>mclick</code>. 3. Write a response to the user based on\nthese results. In your response, cite sources using the citation format\nbelow.</p>\n<p>In some cases, you should repeat step 1 twice, if the initial results\nare unsatisfactory, and you believe that you can refine the query to get\nbetter results.</p>\n<p>You can also open a url directly if one is provided by the user. Only\nuse the <code>open_url</code> command for this purpose; do not open urls\nreturned by the search function or found on webpages.</p>\n<p>The <code>browser</code> tool has the following commands:\n<code>search(query: str, recency_days: int)</code> Issues a query to a\nsearch engine and displays the results.\n<code>mclick(ids: list[str])</code>. Retrieves the contents of the\nwebpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST\n3 and at most 10 pages. Select sources with diverse perspectives, and\nprefer trustworthy sources. Because some pages may fail to load, it is\nfine to select some pages for redundancy even if their content might be\nredundant. <code>open_url(url: str)</code> Opens the given URL and\ndisplays it.</p>\n<p>For citing quotes from the 'browser' tool: please render in this\nformat: {message idx}{link text}. For long citations: please render\nin this format: <a href=\"message%20idx\">link text</a>. Otherwise do not\nrender links.</p>\n</blockquote>\n<p>system\npromptkvsystem\npromptsliding windowsystem\npromptkv</p>\n<p> <span class=\"math inline\">\\(W=4\\)</span>system\nprompt9system promptkv [4,4,1] </p>\n<p>windowattention\nmask0</p>\n<p>attention\nmask</p>\n<p></p>\n<p></p>\n<p>prompt</p>\n<img src=\"/c61d17e3/prefill_and_chunking.png\" class title=\"prefill and chunking\">\n<p>FlashAttention/PagedAttention</p>\n<p>Mistral\n7BLlamaLlama\n34B</p>\n<img src=\"/c61d17e3/mistral_perf.png\" class title=\"mistral performance\">\n<p>Mistral7B</p>\n<h1 id=\"sparse-attention\">Sparse Attention</h1>\n<p>SWAsparse attentionsparse\nattention</p>\n<p>sparse\nattention</p>\n<h2 id=\"longformer\">Longformer</h2>\n<p>MistralSWA</p>\n<p>2020<a href=\"https://arxiv.org/pdf/2004.05150.pdf\">Longformer:\nThe Long-Document Transformer</a>SWAsparse\nattention</p>\n<p>Longformer</p>\n<img src=\"/c61d17e3/longformer_attention.png\" class title=\"longformer\">\n<p>bSWABert</p>\n<p>SWAdilated sliding\nwindow</p>\n<img src=\"/c61d17e3/dilated_conv.png\" class title=\"dilated convolution\">\n<p>attentionSWAdilated sliding\nwindow</p>\n<p></p>\n<p>Bert[CLS]\ntokentoken</p>\n<p>GPTinstructionprompt</p>\n<p>tokenglobal\nattentionsliding windowd</p>\n<h2 id=\"big-bird\">Big Bird</h2>\n<p>2020Longformersparse\nattention<a href=\"https://arxiv.org/abs/2007.14062\">Big Bird: Transformers for\nLonger Sequences</a></p>\n<p>sliding windowglobal attentionLongformerBig\nBirdrandom attention</p>\n<img src=\"/c61d17e3/big_bird_attention.png\" class title=\"big bird attention\">\n<p> <span class=\"math inline\">\\(r=2\\)</span>\n2</p>\n<h1 id=\"\"></h1>\n<p>SWA</p>\n<p>SWAsparse\nattention<big><strong></strong></big>global\n+ local attentionflash attentionrandom\nattention</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Mistral 7B https://arxiv.org/pdf/2310.06825.pdf<br>\n2Longformer: The Long-Document Transformer\nhttps://arxiv.org/pdf/2004.05150.pdf<br>\n3Training Compute-Optimal Large Language Models\nhttps://arxiv.org/pdf/2203.15556.pdf<br>\n4GPT-4 System Prompt Revealed\nhttps://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed<br>\n5Big Bird: Transformers for Longer Sequences\nhttps://arxiv.org/abs/2007.14062</p>\n"},{"title":"LLM:RoPE","abbrlink":"a051710f","date":"2024-02-21T13:18:13.000Z","mathjax":true,"_content":"\nLLMRoPE\n\n# RoPE\n\nRoPERotary Position Embedding2021TransformerRoPE<big><u>****</u></big>\n\n2023AlibiRoPE20232024RoPEAlibiAlibi\n\nRoPERoPE  \n\n# \n\nRoPE\n\n  \n\nBert256/512token  \n  \n<u>****</u>token-2token-1token-10002token-10001  \n<u>****</u><u>****</u>self-attention<u>****</u>  \n<u>****</u><u>****</u><u>****</u><u>****</u>  \n\n  \n\n3  \n\n## \n\nself-attention  \n\n$$\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}$$  \n\n\n$x_i$  $x_j$  $i$  $j$ $p$   \n\n$p$ $x$  $p$ attentionsoftmaxelement-wise addition\n\n $x + p$   $x * p$ \n\n## \n\n $x$  $p$   \n\n1 $e_1 = x_ + p_1$ 18 $e_8 = x_ + p_8$  $e_1$  $e_8$ <u>****</u>  \n\n15121512=512handle\n\n1 $q_{i}k_{j}^{T}$  \n\n$$\\begin{align*}q_1k_j^\\top&=\\left(x_i+p_j\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{\\color{red}W_K^\\top p_j^\\top}\\right)\\end{align*}\\tag{2}$$  \n\n $p_iW_\\mathbb{Q}$  $W_K^\\top p_j^\\top$   \n\n### Google\n\nGoogleSelf-Attention with Relative Position Representations $p_iW_\\mathbb{Q}$  $j$ $W_K^\\top p_j^\\top$  $i$$j$  $R_{ij}^K$attention<u>**input projection**</u>  \n\n$$\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n$$  \n\n$R_{ij}^K$ clip  \n\n$$\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n$$\n\n $p_\\mathrm{K}$  \n\nclip****tokentoken256>256\n\nGoogleinput\n\n$$\\begin{align*}\no_\\mathrm{i}&=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} + {\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}$$\n\nGoogle $p_{j}W_{\\mathrm{V}}$ \n\n$$\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}$$\n\n$R_{\\mathrm{i,j}}^{\\mathrm{V}}$  $R_{ij}^K$  + clip\n\n### XLNET\n\nXLNETGoogle  \n\n2  \n\n$$\\begin{align*}\nq_ik_j^T\n&= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&= \nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n$$  \n\n\n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n$$    \n\n $p_i$  $u$  $\\nu$  $p_j$  $R_{i-j}^\\top$   \n\n $u$  $\\nu$  $u$  $\\nu$ \n\n$$\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n$$   \n\nXLNET  \n\n$$\\begin{align*}\no_\\mathrm{i}\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}$$\n\nGoogleXLNET $\\mathrm{a_{i,j}}$ 2 $i$  $j$ clip\n\nT5  \n\n### T5\n\n6 $i$  $j$   \n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n$$    \n\nXLNETDeBertaT5\n\n## \n\nattention  \n\n1softmax33\n\n8433  \n\n\n\n\n\nself-attentionlinear attention  \n\n# RoPE\n\n## attention\n\nRoPE\n\n  \n\nself-attention1 =  + softmaxsoftmax  \n\n  \n\n$$\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n$$  \n\n $q_m$  $m$ query$k_n$  $n$ key$f_q$  $f_k$ querykey  \n\n $f_q$  $f_k$  $g$ 11  \n\nRoPE  \n\n## \n\n11 $g$ \n\n2  \n\n{% asset_img complex_number.png 282 401  %}\n\nquerykey2  \n hidden size = 2   \n\n211Roformer  \n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta} \\\\\nf_k(\\boldsymbol{k}_n,n)&=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta} \\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n$$  \n\n $\\boldsymbol{k}_n^*$  $\\boldsymbol{k}_n$   \n\n\n\n  \n\n$$\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib \n\\end{gathered}\n\\tag{13}\n$$\n\n11  \n\n  \n\n  \n\n$$\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n$$\n\n22 $q_m$   \n\n$$\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n$$\n\n\n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m) \n&= \\boldsymbol{q}_me^{im\\theta} \\\\\n&= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&= (q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n$$  \n\n16  \n\n$$\nf_q(\\boldsymbol{q}_m,m) = \\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n$$  \n\n1transpose  \n\n\n\n$$\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n) \n&= (k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n$$  \n\n\n\n$$\nf_k(\\boldsymbol{k}_n,n) = \\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n$$  \n\n  \n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)) \\\\&+ (q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\\\\\n=&q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right) \\\\\n&+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right) \\\\\n&+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta)) \\\\\n&+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta)) \\\\\n=&q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta) \\\\\n&-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta) +q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n$$\n\n\n$$\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n$$\n\n $g$  \n\n$$\n\\begin{aligned}\n&g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n= &\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right] \\\\\n= &\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) - i(q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})](\\cos((m - n)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n$$\n\n\n\n## \n\n17\n\n$$\nf_q(\\boldsymbol{q}_m,m)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n$$\n\n  \n\n$$\nf_k(\\boldsymbol{k}_n,n)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n$$\n\n  \n\n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta) + \\sin(m\\theta)\\sin(n\\theta)}&\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n$$  \n\n2223  \n\n$$\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n$$  \n\n $f_q$  $f_k$   \n\n\n\n## 2\n\n2 $f_q$  $f_k$  $g$ 11  \n\n  \n\n$$\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\\\0&0&0&0&\\cdots&\\sin m\\theta_{d/2-1}&\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n$$  \n\n$d$  $d/2$  $d/2$  $\\theta$\n\n$$\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n$$  \n\n $m$  $n$  $R_m$  $R_n$self-attention  \n\n $\\theta$ GoogleAttention is All You Need\n\n## \n\n25  \n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n$$\n\nelement-wise  \n\nLLAMAdecoder\n\n## \n\n  \n\n\n\n $\\theta$   \n\n[Roformer](https://arxiv.org/abs/2104.09864)[](https://spaces.ac.cn/archives/8265)  \n\n $d = 128$ \n\n{% asset_img remote_attenuation.png 775 457  %}  \n\n#   \n\nRoPEtransformer\n\n# Reference\n1Transformerhttps://spaces.ac.cn/archives/8130  \n2Transformer2https://spaces.ac.cn/archives/8265  \n3RoFormer: Enhanced Transformer with Rotary Position Embedding https://arxiv.org/abs/2104.09864  \n4RoPE https://zhuanlan.zhihu.com/p/647109286  \n\n***\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)\n","source":"_posts/cs/nlp/2024/02/LLMRoPE.md","raw":"---\ntitle: LLM:RoPE\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - positional encoding\n  - RoPE\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: a051710f\ndate: 2024-02-21 21:18:13\nmathjax: true\n---\n\nLLMRoPE\n\n# RoPE\n\nRoPERotary Position Embedding2021TransformerRoPE<big><u>****</u></big>\n\n2023AlibiRoPE20232024RoPEAlibiAlibi\n\nRoPERoPE  \n\n# \n\nRoPE\n\n  \n\nBert256/512token  \n  \n<u>****</u>token-2token-1token-10002token-10001  \n<u>****</u><u>****</u>self-attention<u>****</u>  \n<u>****</u><u>****</u><u>****</u><u>****</u>  \n\n  \n\n3  \n\n## \n\nself-attention  \n\n$$\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}$$  \n\n\n$x_i$  $x_j$  $i$  $j$ $p$   \n\n$p$ $x$  $p$ attentionsoftmaxelement-wise addition\n\n $x + p$   $x * p$ \n\n## \n\n $x$  $p$   \n\n1 $e_1 = x_ + p_1$ 18 $e_8 = x_ + p_8$  $e_1$  $e_8$ <u>****</u>  \n\n15121512=512handle\n\n1 $q_{i}k_{j}^{T}$  \n\n$$\\begin{align*}q_1k_j^\\top&=\\left(x_i+p_j\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{\\color{red}W_K^\\top p_j^\\top}\\right)\\end{align*}\\tag{2}$$  \n\n $p_iW_\\mathbb{Q}$  $W_K^\\top p_j^\\top$   \n\n### Google\n\nGoogleSelf-Attention with Relative Position Representations $p_iW_\\mathbb{Q}$  $j$ $W_K^\\top p_j^\\top$  $i$$j$  $R_{ij}^K$attention<u>**input projection**</u>  \n\n$$\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n$$  \n\n$R_{ij}^K$ clip  \n\n$$\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n$$\n\n $p_\\mathrm{K}$  \n\nclip****tokentoken256>256\n\nGoogleinput\n\n$$\\begin{align*}\no_\\mathrm{i}&=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} + {\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}$$\n\nGoogle $p_{j}W_{\\mathrm{V}}$ \n\n$$\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}$$\n\n$R_{\\mathrm{i,j}}^{\\mathrm{V}}$  $R_{ij}^K$  + clip\n\n### XLNET\n\nXLNETGoogle  \n\n2  \n\n$$\\begin{align*}\nq_ik_j^T\n&= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&= \nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n$$  \n\n\n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n$$    \n\n $p_i$  $u$  $\\nu$  $p_j$  $R_{i-j}^\\top$   \n\n $u$  $\\nu$  $u$  $\\nu$ \n\n$$\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n$$   \n\nXLNET  \n\n$$\\begin{align*}\no_\\mathrm{i}\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}$$\n\nGoogleXLNET $\\mathrm{a_{i,j}}$ 2 $i$  $j$ clip\n\nT5  \n\n### T5\n\n6 $i$  $j$   \n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n$$    \n\nXLNETDeBertaT5\n\n## \n\nattention  \n\n1softmax33\n\n8433  \n\n\n\n\n\nself-attentionlinear attention  \n\n# RoPE\n\n## attention\n\nRoPE\n\n  \n\nself-attention1 =  + softmaxsoftmax  \n\n  \n\n$$\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n$$  \n\n $q_m$  $m$ query$k_n$  $n$ key$f_q$  $f_k$ querykey  \n\n $f_q$  $f_k$  $g$ 11  \n\nRoPE  \n\n## \n\n11 $g$ \n\n2  \n\n{% asset_img complex_number.png 282 401  %}\n\nquerykey2  \n hidden size = 2   \n\n211Roformer  \n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta} \\\\\nf_k(\\boldsymbol{k}_n,n)&=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta} \\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n$$  \n\n $\\boldsymbol{k}_n^*$  $\\boldsymbol{k}_n$   \n\n\n\n  \n\n$$\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib \n\\end{gathered}\n\\tag{13}\n$$\n\n11  \n\n  \n\n  \n\n$$\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n$$\n\n22 $q_m$   \n\n$$\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n$$\n\n\n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m) \n&= \\boldsymbol{q}_me^{im\\theta} \\\\\n&= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&= (q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n$$  \n\n16  \n\n$$\nf_q(\\boldsymbol{q}_m,m) = \\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n$$  \n\n1transpose  \n\n\n\n$$\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n) \n&= (k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n$$  \n\n\n\n$$\nf_k(\\boldsymbol{k}_n,n) = \\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n$$  \n\n  \n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)) \\\\&+ (q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\\\\\n=&q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right) \\\\\n&+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right) \\\\\n&+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta)) \\\\\n&+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta)) \\\\\n=&q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta) \\\\\n&-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta) +q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n$$\n\n\n$$\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n$$\n\n $g$  \n\n$$\n\\begin{aligned}\n&g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n= &\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right] \\\\\n= &\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) - i(q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})](\\cos((m - n)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n$$\n\n\n\n## \n\n17\n\n$$\nf_q(\\boldsymbol{q}_m,m)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n$$\n\n  \n\n$$\nf_k(\\boldsymbol{k}_n,n)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n$$\n\n  \n\n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta) + \\sin(m\\theta)\\sin(n\\theta)}&\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n$$  \n\n2223  \n\n$$\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n$$  \n\n $f_q$  $f_k$   \n\n\n\n## 2\n\n2 $f_q$  $f_k$  $g$ 11  \n\n  \n\n$$\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\\\0&0&0&0&\\cdots&\\sin m\\theta_{d/2-1}&\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n$$  \n\n$d$  $d/2$  $d/2$  $\\theta$\n\n$$\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n$$  \n\n $m$  $n$  $R_m$  $R_n$self-attention  \n\n $\\theta$ GoogleAttention is All You Need\n\n## \n\n25  \n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n$$\n\nelement-wise  \n\nLLAMAdecoder\n\n## \n\n  \n\n\n\n $\\theta$   \n\n[Roformer](https://arxiv.org/abs/2104.09864)[](https://spaces.ac.cn/archives/8265)  \n\n $d = 128$ \n\n{% asset_img remote_attenuation.png 775 457  %}  \n\n#   \n\nRoPEtransformer\n\n# Reference\n1Transformerhttps://spaces.ac.cn/archives/8130  \n2Transformer2https://spaces.ac.cn/archives/8265  \n3RoFormer: Enhanced Transformer with Rotary Position Embedding https://arxiv.org/abs/2104.09864  \n4RoPE https://zhuanlan.zhihu.com/p/647109286  \n\n***\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)\n","slug":"cs/nlp/2024/02/LLMRoPE","published":1,"updated":"2024-02-27T06:39:30.683Z","comments":1,"layout":"post","photos":[],"_id":"clttl4al10007rb4k4nyqhg3o","content":"<p>LLMRoPE</p>\n<h1 id=\"rope\">RoPE</h1>\n<p>RoPERotary Position\nEmbedding2021TransformerRoPE<big><u><strong></strong></u></big></p>\n<p>2023AlibiRoPE20232024RoPEAlibiAlibi</p>\n<p>RoPERoPE</p>\n<h1 id=\"\"></h1>\n<p>RoPE</p>\n<p></p>\n<p>Bert256/512token<br>\n<br>\n<u><strong></strong></u>token-2token-1token-10002token-10001<br>\n<u><strong></strong></u><u><strong></strong></u>self-attention<u><strong></strong></u><br>\n<u><strong></strong></u><u><strong></strong></u><u><strong></strong></u><u><strong></strong></u></p>\n<p></p>\n<p>3</p>\n<h2 id=\"\"></h2>\n<p>self-attention</p>\n<p><span class=\"math display\">\\[\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}\\]</span></p>\n<p><span class=\"math inline\">\\(x_i\\)</span>  <span class=\"math inline\">\\(x_j\\)</span>  <span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(p\\)</span> </p>\n<p><span class=\"math inline\">\\(p\\)</span>\n<span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(p\\)</span>\nattentionsoftmaxelement-wise\naddition</p>\n<p>\n<span class=\"math inline\">\\(x + p\\)</span>  <span class=\"math inline\">\\(x * p\\)</span> </p>\n<h2 id=\"\"></h2>\n<p> <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(p\\)</span>\n</p>\n<p>1 <span class=\"math inline\">\\(e_1 =\nx_ + p_1\\)</span>\n18\n<span class=\"math inline\">\\(e_8 = x_ + p_8\\)</span>  <span class=\"math inline\">\\(e_1\\)</span>  <span class=\"math inline\">\\(e_8\\)</span>\n<u><strong></strong></u></p>\n<p>15121512=512handle</p>\n<p>1 <span class=\"math inline\">\\(q_{i}k_{j}^{T}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{align*}q_1k_j^\\top&amp;=\\left(x_i+p_j\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&amp;=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{\\color{red}W_K^\\top\np_j^\\top}\\right)\\end{align*}\\tag{2}\\]</span></p>\n<p> <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span> </p>\n<h3 id=\"google\">Google</h3>\n<p>GoogleSelf-Attention with Relative\nPosition Representations <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n\n<span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span>  <span class=\"math inline\">\\(i\\)</span><span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(R_{ij}^K\\)</span>attention<u><strong>input\nprojection</strong></u></p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n\\]</span></p>\n<p><span class=\"math inline\">\\(R_{ij}^K\\)</span>\nclip</p>\n<p><span class=\"math display\">\\[\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n\\]</span></p>\n<p> <span class=\"math inline\">\\(p_\\mathrm{K}\\)</span>\n</p>\n<p>clip<strong></strong>tokentoken256&gt;256</p>\n<p>Googleinput</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}&amp;=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} +\n{\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}\\]</span></p>\n<p>Google <span class=\"math inline\">\\(p_{j}W_{\\mathrm{V}}\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}\\]</span></p>\n<p><span class=\"math inline\">\\(R_{\\mathrm{i,j}}^{\\mathrm{V}}\\)</span> \n<span class=\"math inline\">\\(R_{ij}^K\\)</span> \n+ clip</p>\n<h3 id=\"xlnet\">XLNET</h3>\n<p>XLNETGoogle</p>\n<p>2</p>\n<p><span class=\"math display\">\\[\\begin{align*}\nq_ik_j^T\n&amp;= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&amp;=\nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu}\nW_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(p_i\\)</span> \n<span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span>  <span class=\"math inline\">\\(p_j\\)</span>  <span class=\"math inline\">\\(R_{i-j}^\\top\\)</span> </p>\n<p> <span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span>\n <span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span> </p>\n<p><span class=\"math display\">\\[\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n\\]</span></p>\n<p>XLNET</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}\\]</span></p>\n<p>GoogleXLNET <span class=\"math inline\">\\(\\mathrm{a_{i,j}}\\)</span>\n2\n<span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span>\nclip</p>\n<p>T5</p>\n<h3 id=\"t5\">T5</h3>\n<p>6\n<span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span>\n</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n\\]</span></p>\n<p>XLNETDeBertaT5</p>\n<h2 id=\"\"></h2>\n<p>attention</p>\n<p>1softmax33</p>\n<p>8433</p>\n<p></p>\n<p></p>\n<p>self-attentionlinear\nattention</p>\n<h1 id=\"rope\">RoPE</h1>\n<h2 id=\"attention\">attention</h2>\n<p>RoPE</p>\n<p></p>\n<p>self-attention1\n=  +\nsoftmaxsoftmax</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(q_m\\)</span>  <span class=\"math inline\">\\(m\\)</span> query<span class=\"math inline\">\\(k_n\\)</span>  <span class=\"math inline\">\\(n\\)</span> key<span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>\nquerykey</p>\n<p> <span class=\"math inline\">\\(f_q\\)</span> \n<span class=\"math inline\">\\(f_k\\)</span>  <span class=\"math inline\">\\(g\\)</span> 11</p>\n<p>RoPE</p>\n<h2 id=\"\"></h2>\n<p>11 <span class=\"math inline\">\\(g\\)</span>\n</p>\n<p>2</p>\n<img src=\"/a051710f/complex_number.png\" class width=\"282\" height=\"401\" title=\"\">\n<p>querykey2<br>\n hidden size = 2 </p>\n<p>211Roformer</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&amp;=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta}\n\\\\\nf_k(\\boldsymbol{k}_n,n)&amp;=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta}\n\\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&amp;=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\boldsymbol{k}_n^*\\)</span>  <span class=\"math inline\">\\(\\boldsymbol{k}_n\\)</span> </p>\n<p></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib\n\\end{gathered}\n\\tag{13}\n\\]</span></p>\n<p>11</p>\n<p></p>\n<p></p>\n<p><span class=\"math display\">\\[\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n\\]</span></p>\n<p>22 <span class=\"math inline\">\\(q_m\\)</span> </p>\n<p><span class=\"math display\">\\[\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)\n&amp;= \\boldsymbol{q}_me^{im\\theta} \\\\\n&amp;= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&amp;=\n(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n\\]</span></p>\n<p>16</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m) =\n\\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n\\]</span></p>\n<p>1transpose</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n)\n&amp;=\n(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n) =\n\\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n\\]</span></p>\n<p><br>\n<span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&amp;(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))\n\\\\&amp;+ (q_m^{(1)}\\sin(m\\theta) +\nq_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) +\nk_n^{(2)}\\cos(n\\theta))\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta))\n\\\\\n&amp;+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta))\n\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta)\n\\\\\n&amp;-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta)\n+q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n\\]</span></p>\n<p> <span class=\"math display\">\\[\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(g\\)</span></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n=\n&amp;\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\\\\n= &amp;\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) -\ni(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})](\\cos((m -\nn)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 -\nq_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n\\]</span></p>\n<p></p>\n<h2 id=\"\"></h2>\n<p>17</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta)\n+ \\sin(m\\theta)\\sin(n\\theta)}&amp;\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&amp;\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&amp;{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&amp;{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n\\]</span></p>\n<p>2223</p>\n<p><span class=\"math display\">\\[\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n\\]</span></p>\n<p> <span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>\n</p>\n<p></p>\n<h2 id=\"2\">2</h2>\n<p>2 <span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>  <span class=\"math inline\">\\(g\\)</span>\n11</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&amp;-\\sin\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\\\sin m\\theta_0&amp;\\cos\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\cos\nm\\theta_1&amp;-\\sin m\\theta_1&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\sin\nm\\theta_1&amp;\\cos\nm\\theta_1&amp;\\cdots&amp;0&amp;0\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\cos\nm\\theta_{d/2-1}&amp;-\\sin\nm\\theta_{d/2-1}\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\sin\nm\\theta_{d/2-1}&amp;\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n\\]</span></p>\n<p><span class=\"math inline\">\\(d\\)</span>\n <span class=\"math inline\">\\(d/2\\)</span>  <span class=\"math inline\">\\(d/2\\)</span> \n<span class=\"math inline\">\\(\\theta\\)</span></p>\n<p><span class=\"math display\">\\[\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(m\\)</span>\n <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(R_m\\)</span>  <span class=\"math inline\">\\(R_n\\)</span>self-attention</p>\n<p> <span class=\"math inline\">\\(\\theta\\)</span>\nGoogleAttention is All You\nNeed</p>\n<h2 id=\"\"></h2>\n<p>25</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n\\]</span></p>\n<p>element-wise</p>\n<p>LLAMAdecoder</p>\n<h2 id=\"\"></h2>\n<p></p>\n<p></p>\n<p>\n<span class=\"math inline\">\\(\\theta\\)</span>\n</p>\n<p><a href=\"https://arxiv.org/abs/2104.09864\">Roformer</a><a href=\"https://spaces.ac.cn/archives/8265\"></a></p>\n<p> <span class=\"math inline\">\\(d = 128\\)</span>\n</p>\n<img src=\"/a051710f/remote_attenuation.png\" class width=\"775\" height=\"457\" title=\"\">\n<h1 id=\"\"></h1>\n<p>RoPEtransformer</p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Transformerhttps://spaces.ac.cn/archives/8130<br>\n2Transformer2https://spaces.ac.cn/archives/8265<br>\n3RoFormer: Enhanced Transformer with Rotary Position Embedding\nhttps://arxiv.org/abs/2104.09864<br>\n4RoPE\nhttps://zhuanlan.zhihu.com/p/647109286</p>\n<hr>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n","length":14264,"excerpt":"","more":"<p>LLMRoPE</p>\n<h1 id=\"rope\">RoPE</h1>\n<p>RoPERotary Position\nEmbedding2021TransformerRoPE<big><u><strong></strong></u></big></p>\n<p>2023AlibiRoPE20232024RoPEAlibiAlibi</p>\n<p>RoPERoPE</p>\n<h1 id=\"\"></h1>\n<p>RoPE</p>\n<p></p>\n<p>Bert256/512token<br>\n<br>\n<u><strong></strong></u>token-2token-1token-10002token-10001<br>\n<u><strong></strong></u><u><strong></strong></u>self-attention<u><strong></strong></u><br>\n<u><strong></strong></u><u><strong></strong></u><u><strong></strong></u><u><strong></strong></u></p>\n<p></p>\n<p>3</p>\n<h2 id=\"\"></h2>\n<p>self-attention</p>\n<p><span class=\"math display\">\\[\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}\\]</span></p>\n<p><span class=\"math inline\">\\(x_i\\)</span>  <span class=\"math inline\">\\(x_j\\)</span>  <span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(p\\)</span> </p>\n<p><span class=\"math inline\">\\(p\\)</span>\n<span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(p\\)</span>\nattentionsoftmaxelement-wise\naddition</p>\n<p>\n<span class=\"math inline\">\\(x + p\\)</span>  <span class=\"math inline\">\\(x * p\\)</span> </p>\n<h2 id=\"\"></h2>\n<p> <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(p\\)</span>\n</p>\n<p>1 <span class=\"math inline\">\\(e_1 =\nx_ + p_1\\)</span>\n18\n<span class=\"math inline\">\\(e_8 = x_ + p_8\\)</span>  <span class=\"math inline\">\\(e_1\\)</span>  <span class=\"math inline\">\\(e_8\\)</span>\n<u><strong></strong></u></p>\n<p>15121512=512handle</p>\n<p>1 <span class=\"math inline\">\\(q_{i}k_{j}^{T}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{align*}q_1k_j^\\top&amp;=\\left(x_i+p_j\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&amp;=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{\\color{red}W_K^\\top\np_j^\\top}\\right)\\end{align*}\\tag{2}\\]</span></p>\n<p> <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span> </p>\n<h3 id=\"google\">Google</h3>\n<p>GoogleSelf-Attention with Relative\nPosition Representations <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n\n<span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span>  <span class=\"math inline\">\\(i\\)</span><span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(R_{ij}^K\\)</span>attention<u><strong>input\nprojection</strong></u></p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n\\]</span></p>\n<p><span class=\"math inline\">\\(R_{ij}^K\\)</span>\nclip</p>\n<p><span class=\"math display\">\\[\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n\\]</span></p>\n<p> <span class=\"math inline\">\\(p_\\mathrm{K}\\)</span>\n</p>\n<p>clip<strong></strong>tokentoken256&gt;256</p>\n<p>Googleinput</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}&amp;=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} +\n{\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}\\]</span></p>\n<p>Google <span class=\"math inline\">\\(p_{j}W_{\\mathrm{V}}\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}\\]</span></p>\n<p><span class=\"math inline\">\\(R_{\\mathrm{i,j}}^{\\mathrm{V}}\\)</span> \n<span class=\"math inline\">\\(R_{ij}^K\\)</span> \n+ clip</p>\n<h3 id=\"xlnet\">XLNET</h3>\n<p>XLNETGoogle</p>\n<p>2</p>\n<p><span class=\"math display\">\\[\\begin{align*}\nq_ik_j^T\n&amp;= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&amp;=\nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu}\nW_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(p_i\\)</span> \n<span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span>  <span class=\"math inline\">\\(p_j\\)</span>  <span class=\"math inline\">\\(R_{i-j}^\\top\\)</span> </p>\n<p> <span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span>\n <span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span> </p>\n<p><span class=\"math display\">\\[\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n\\]</span></p>\n<p>XLNET</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}\\]</span></p>\n<p>GoogleXLNET <span class=\"math inline\">\\(\\mathrm{a_{i,j}}\\)</span>\n2\n<span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span>\nclip</p>\n<p>T5</p>\n<h3 id=\"t5\">T5</h3>\n<p>6\n<span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span>\n</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n\\]</span></p>\n<p>XLNETDeBertaT5</p>\n<h2 id=\"\"></h2>\n<p>attention</p>\n<p>1softmax33</p>\n<p>8433</p>\n<p></p>\n<p></p>\n<p>self-attentionlinear\nattention</p>\n<h1 id=\"rope\">RoPE</h1>\n<h2 id=\"attention\">attention</h2>\n<p>RoPE</p>\n<p></p>\n<p>self-attention1\n=  +\nsoftmaxsoftmax</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(q_m\\)</span>  <span class=\"math inline\">\\(m\\)</span> query<span class=\"math inline\">\\(k_n\\)</span>  <span class=\"math inline\">\\(n\\)</span> key<span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>\nquerykey</p>\n<p> <span class=\"math inline\">\\(f_q\\)</span> \n<span class=\"math inline\">\\(f_k\\)</span>  <span class=\"math inline\">\\(g\\)</span> 11</p>\n<p>RoPE</p>\n<h2 id=\"\"></h2>\n<p>11 <span class=\"math inline\">\\(g\\)</span>\n</p>\n<p>2</p>\n<img src=\"/a051710f/complex_number.png\" class width=\"282\" height=\"401\" title=\"\">\n<p>querykey2<br>\n hidden size = 2 </p>\n<p>211Roformer</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&amp;=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta}\n\\\\\nf_k(\\boldsymbol{k}_n,n)&amp;=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta}\n\\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&amp;=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\boldsymbol{k}_n^*\\)</span>  <span class=\"math inline\">\\(\\boldsymbol{k}_n\\)</span> </p>\n<p></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib\n\\end{gathered}\n\\tag{13}\n\\]</span></p>\n<p>11</p>\n<p></p>\n<p></p>\n<p><span class=\"math display\">\\[\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n\\]</span></p>\n<p>22 <span class=\"math inline\">\\(q_m\\)</span> </p>\n<p><span class=\"math display\">\\[\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)\n&amp;= \\boldsymbol{q}_me^{im\\theta} \\\\\n&amp;= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&amp;=\n(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n\\]</span></p>\n<p>16</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m) =\n\\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n\\]</span></p>\n<p>1transpose</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n)\n&amp;=\n(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n) =\n\\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n\\]</span></p>\n<p><br>\n<span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&amp;(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))\n\\\\&amp;+ (q_m^{(1)}\\sin(m\\theta) +\nq_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) +\nk_n^{(2)}\\cos(n\\theta))\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta))\n\\\\\n&amp;+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta))\n\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta)\n\\\\\n&amp;-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta)\n+q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n\\]</span></p>\n<p> <span class=\"math display\">\\[\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(g\\)</span></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n=\n&amp;\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\\\\n= &amp;\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) -\ni(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})](\\cos((m -\nn)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 -\nq_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n\\]</span></p>\n<p></p>\n<h2 id=\"\"></h2>\n<p>17</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta)\n+ \\sin(m\\theta)\\sin(n\\theta)}&amp;\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&amp;\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&amp;{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&amp;{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n\\]</span></p>\n<p>2223</p>\n<p><span class=\"math display\">\\[\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n\\]</span></p>\n<p> <span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>\n</p>\n<p></p>\n<h2 id=\"2\">2</h2>\n<p>2 <span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>  <span class=\"math inline\">\\(g\\)</span>\n11</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&amp;-\\sin\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\\\sin m\\theta_0&amp;\\cos\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\cos\nm\\theta_1&amp;-\\sin m\\theta_1&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\sin\nm\\theta_1&amp;\\cos\nm\\theta_1&amp;\\cdots&amp;0&amp;0\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\cos\nm\\theta_{d/2-1}&amp;-\\sin\nm\\theta_{d/2-1}\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\sin\nm\\theta_{d/2-1}&amp;\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n\\]</span></p>\n<p><span class=\"math inline\">\\(d\\)</span>\n <span class=\"math inline\">\\(d/2\\)</span>  <span class=\"math inline\">\\(d/2\\)</span> \n<span class=\"math inline\">\\(\\theta\\)</span></p>\n<p><span class=\"math display\">\\[\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(m\\)</span>\n <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(R_m\\)</span>  <span class=\"math inline\">\\(R_n\\)</span>self-attention</p>\n<p> <span class=\"math inline\">\\(\\theta\\)</span>\nGoogleAttention is All You\nNeed</p>\n<h2 id=\"\"></h2>\n<p>25</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n\\]</span></p>\n<p>element-wise</p>\n<p>LLAMAdecoder</p>\n<h2 id=\"\"></h2>\n<p></p>\n<p></p>\n<p>\n<span class=\"math inline\">\\(\\theta\\)</span>\n</p>\n<p><a href=\"https://arxiv.org/abs/2104.09864\">Roformer</a><a href=\"https://spaces.ac.cn/archives/8265\"></a></p>\n<p> <span class=\"math inline\">\\(d = 128\\)</span>\n</p>\n<img src=\"/a051710f/remote_attenuation.png\" class width=\"775\" height=\"457\" title=\"\">\n<h1 id=\"\"></h1>\n<p>RoPEtransformer</p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Transformerhttps://spaces.ac.cn/archives/8130<br>\n2Transformer2https://spaces.ac.cn/archives/8265<br>\n3RoFormer: Enhanced Transformer with Rotary Position Embedding\nhttps://arxiv.org/abs/2104.09864<br>\n4RoPE\nhttps://zhuanlan.zhihu.com/p/647109286</p>\n<hr>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n"},{"title":"Attention:MHA,MQAGQA","abbrlink":"3dc22f96","date":"2024-03-05T10:49:38.000Z","_content":"\n//  \n\nAttentionMHAMulti-Head AttentionMQAMulti-Query AttentionGQAGrouped-Query AttentionKV Cache  \n\nAttentionFlashAttentionSliding Window Attention  \n\nLLM\n\n# AttentionRNNAttention\n\nattention\n\nattention\n\n## RNN\n\n>Memory is attention through time. ~ Alex Graves 2020\n\nTransformerRNNSeq2Seq\n\n{% asset_img seq2seq.png seq2seq %}  \n\n{% asset_img encoder.png encoder %}  \n\n{% asset_img decoder.png decoder %}  \n\n[AI Summer](https://theaisummer.com/attention/)  \n\nRNN cellhidden stateRNN encodercontext $z$ RNN decoder $z$ decodertoken[start]  \n\n $z$   \n\n  \n\nLSMTGRU  \n\n $z$  $z$   \n\n $z$   \n\n  \n\nCNNheatmap  \n\n{% asset_img cnn_heatmap.png heatmap %}  \n\nCNNimplicitly\n\nSeq2Seqimplicitexplicit  \n\nRNN $i$ $h_i$  $h_i$  $h_i$ hidden state  \n\n --   \n\n $i$ decoder $y_{i-1}$ encoder $\\mathbf{h}$ score\n\n$$\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in R^n$$  \n\n\n\n$$e_{ij}=\\text{attentiom}_{\\text{net }(\\mathbf{y}_{i-1},h_j)}$$  \n\n $\\mathbf{y}_{i-1}$  $h_j$  $e_{ij}$fc  \n \n  $e_{ij}$ attention netencoder hidden statesoftmax  \n\n$$\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}$$  \n\ndecoder  \n\n$$z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j$$\n\n{% asset_img seq2seq_attention.png seq2seq attention %}  \n\nattention netdecoderhidden stateencoder hidden state\n\nattentionattention  \n\n{% asset_img attention_calculation.png attention calculation %}  \n\nattention $\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot h$  $s$ decoderhidden state $y$ $h$ encoderhidden state  \n\nscaled dot-product attention  \n\n## Transformerattention\n\nRNN attentiontransformer attentionAttention Is All You NeedRNNtime stepattentionhidden stateattention  \n\n{% asset_img transformer_structure.png transformer structure.png %}  \n\nencoderdecoderencoderattentionself-attentiondecoderself-attentioncross-attention  \n\ntransformerattention $\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V$ $Q=W_{Q}YK=W_{K}XV=W_{V}X$ cross-attention $X$ encoderhidden states$Y$ decoderhidden statesself-attention $X=Y$  \n\nscaled dot-product attentionsoftmax\n\n$$\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V$$  \n\nattention  \n\n\n\n{% asset_img Scaled-dot-product-self-attention.pbm self-attention %}  \n\nquerykeyvalueattention  \n\n+  \n\n-30keyvalue  \n\n30querykey5  \n\ntop5 $[8,4,4,2,2]$  $[5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]$  \n \n1 $[0.4,0.2,0.2,0.1,0.1]$ 30 $0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}$ \n\ntransformer attention $QK^T$ softmax/  \n\nself-attention $QKV$  $X$sequencetokencross-attentiondecodersequence  \n\nself-attention $QKV$  $X$  $QK^T$  $QK^T$ MHA  \n\nattentionMHAMQAGQA\n\n[pytorch forcasting](https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention)\n\n```python\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self, dropout: float = None, scale: bool = True):\n        super(ScaledDotProductAttention, self).__init__()\n        if dropout is not None:\n            self.dropout = nn.Dropout(p=dropout)\n        else:\n            self.dropout = dropout\n        self.softmax = nn.Softmax(dim=2)\n        self.scale = scale\n\ndef forward(self, q, k, v, mask=None):\n        attn = torch.bmm(q, k.permute(0, 2, 1))  # query-key overlap\n\n        if self.scale:\n            dimension = torch.as_tensor(k.size(-1), dtype=attn.dtype, device=attn.device).sqrt()\n            attn = attn / dimension\n\n        if mask is not None:\n            attn = attn.masked_fill(mask, -1e9)\n        attn = self.softmax(attn)\n\n        if self.dropout is not None:\n            attn = self.dropout(attn)\n        output = torch.bmm(attn, v)\n        return output, attn\n```\n\n## scaling\n\nBTW $QK^T$  $\\sqrt{d}$   \n\nsoftmaxsoftmax  \n\n{% asset_img softmax.png softmax %}  \n\n[](https://spaces.ac.cn/archives/8620)attentionscaling $\\sqrt{d}$ softmaxnormalizationattentionscaling  \n\n# MHA\n\nattentionMHAmulti-head attention\n\nMHA2017Attention Is All You Needattentionattention  \n\n$$\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)$$  \n\n$$head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)$$  \n\nhidden size $d$ MHA $QKV$ hidden state $head_{num}$  $d_{head}$  $head_{num}$  $QKV$ attention  $head_{num}$  $d_{head}$ concat  \n\namazing  \n\n{% asset_img multihead_attention.png MHA %}  \n\n  \n\nAttention Is All You Need\n\n>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.  \n\nattention headattention head  \n\nCNN $3\\times3\\times128$ 128 $3\\times3$  $3\\times3$   \n\n$$\\left.\\left[\\begin{matrix}1&0&-1\\\\1&0&-1\\\\1&0&-1\\end{matrix}\\right.\\right]$$  \n\n  \n\n$$\\left.\\left[\\begin{matrix}1&1&1\\\\0&0&0\\\\-1&-1&-1\\end{matrix}\\right.\\right]$$  \n\n  \n\n128 $3\\times3$ 128MHA  \n\nexpect  \n\n[](https://zhuanlan.zhihu.com/p/626820422)12 $QK^T$   \n\nMHAattentionattention\n\n  \n\n[Are Sixteen Heads Really Better than One?](https://arxiv.org/pdf/1905.10650.pdf)MHA  \n\nhidden sizeLLM1216244896  \n\n[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)MHA  \n\n```python\ndef attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n             / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim = -1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\n\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        '''\n        h: head number\n        '''\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d\n        self.d = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n        \n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n        \n        # 1) Do all the linear projections in batch from d_model => h x d \n        query, key, value = \\\n            [l(x).view(nbatches, -1, self.h, self.d).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key, value))]\n        \n        # 2) Apply attention on all the projected vectors in batch. \n        x, self.attn = attention(query, key, value, mask=mask, \n                                 dropout=self.dropout)\n        \n        # 3) \"Concat\" using a view and apply a final linear. \n        x = x.transpose(1, 2).contiguous() \\\n             .view(nbatches, -1, self.h * self.d)\n        return self.linears[-1](x)\n```\n\n[transformers](https://github.com/huggingface/transformers)\n\n# KV Cache\n\nMQAGQAKV Cache  \n\nencoder-decoderAGIdecoder-onlyLLMauto-regressive  \n\n $\\text{input}_{i-1}$  $\\text{token}_{i}$  $\\text{token}_{i}$  $\\text{input}_{i-1}$  $\\text{input}_{i}$  $\\text{input}_{i}$  $\\text{token}_{i+1}$   \n\ntokentoken  \n\n```\nstep0: =[BOS]=\nstep1: =[BOS]=\nstep2: =[BOS]=\nstep3: =[BOS]=\nstep4: =[BOS]=\nstep5: =[BOS]=[EOS]\n```\n\n[BOS][EOS]  \n\nhidden state \n\nstepsteptokenstepstep\n\n\n\nattention  \n\n$$\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n$$\n\ndecodermask attention\n\n34attention\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n$$\n\n45attention\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n$$\n\n5 $o_{0}$  $o_{2}$   \n\n  \n\n  \n\nstep0101step515instruction800stepstep0800step1799...\n\nstep  \n\nKV Cache  \n\n $k$  $v$   \n\n34 $k$  $v$ \n\n\n\n$$\n\\text{cache}_l=\\text{None}\\\\\n$$\n\n<center></center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$  \n\nkv_cache $l$ \n\n5 $l$ <u>****</u> $k$  $v$  $o_{3}$  $o_{0}o_{1}o_{2}$   \n\n $l$  $o_{0}o_{1}o_{2}$ FNN $l+1$  $l+1$  $k$  $v$  $l+1$  $k$  $v$   \n\n $k$  $v$ \n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$\n\n<center></center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n$$  \n\nattentionFFN  \n\ntransformersuse_cache=TrueKV Cache  \n\nGPT2  \n\n```python\nClass GPT2Attention(nn.Module):\n    ...\n    ...\n    def forward(\n        self,\n        hidden_states: Optional[Tuple[torch.FloatTensor]],\n        layer_past: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n        if encoder_hidden_states is not None:\n            if not hasattr(self, \"q_attn\"):\n                raise ValueError(\n                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n                )\n\n            query = self.q_attn(hidden_states)\n            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n            attention_mask = encoder_attention_mask\n        else:\n            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n\n        query = self._split_heads(query, self.num_heads, self.head_dim)\n        key = self._split_heads(key, self.num_heads, self.head_dim)\n        value = self._split_heads(value, self.num_heads, self.head_dim)\n\n        # \n        if layer_past is not None:\n            past_key, past_value = layer_past\n            key = torch.cat((past_key, key), dim=-2)  # key\n            value = torch.cat((past_value, value), dim=-2)  # value\n\n        if use_cache is True:\n            present = (key, value)  # \n        else:\n            present = None\n\n        if self.reorder_and_upcast_attn:\n            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n        else:\n            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n\n        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n        attn_output = self.c_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output)\n\n        outputs = (attn_output, present)\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs  # a, present, (attentions)\n```\n\nKV Cachedecodermask attentiontokentoken  \n\nKV Cache  \n\n $s$  $L$ hidden size $d$   \n\n$$\n2\\times L\\times s\\times d\n$$  \n\n\n\n$$\n2\\times 2\\times L\\times s\\times d\n$$  \n\nLlama2 7B $L=32$  $L=4096$ token524,288 bytes52K $s=1024$ 536,870,912 bytes500M  \n\nbatch size=1batch size1G  \n\nMHA $qkv$ \n\n\n\n{% asset_img gpu_cache.png gpu cache %}  \n\nH10050ML2 CacheL1 CacheLlama2 7B100token  \n\nLLM34B/70B\n\nL2 CacheHBML2 Cache  \n\n{% asset_img sram_dram.png  %}  \n\n  \n\nCache  \n\n  \n\n# MQA\n\nMQA\n\nGoogle2019Fast Transformer Decoding: One Write-Head is All You NeedMQABert  \n\nMQAMHA $W_{Q}W_{K}W_{V}$ nn= $d_{model}$  $d_{head}$ attentionMQA $Q$ MHA $KV$  $d_{head}$ nQuery $KV$ attention  \n\nMHA $KV$ MQA $KV$ MHA  \n\n{% asset_img MQA.webp MQA %}  \n\n $KV$   \n\nLlama2 7B32MQA1024token1/32536,870,912 bytes / 32 = 16,777,216 bytes16M\n\n $KV$   \n\nMQAMHAhidden sizehead num  \n\n{% asset_img mqa_result_1.png MQA results 1 %}  \n\n{% asset_img mqa_result_3.png MQA results 3 %}  \n\n# GQA  \n\nMQAMHAGQAGrouped-Query AttentionMQAMHA  \n\nGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints2023\n\nGQA $Q$ MHA/MQA $KV$  $Q$  $Q$ groupgroup $Q$  $KV$ group $Q$  $KV$   \n\nMHA $KV$ GQAMQA $KV$ GQA  \n\n\n\n{% asset_img GQA.png GQA %}  \n\n  \n\n{% asset_img GQA_result_1.png GQA result %}  \n\n2/3/4GQAMHAMQAMHAMQAGQAaverage poolingMHAMHAGQA  \n\nLlama2GQAtech reportMHAMQAGQA  \n\n{% asset_img llama2_qga.png llama2 GQA result %}  \n\n#   \n\nMHAMQAGQA\n\nGQALLM  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n1The Annotated Transformer \n https://nlp.seas.harvard.edu/2018/04/03/attention.html  \n2Attention Is All You Need https://arxiv.org/pdf/1706.03762.pdf  \n3Fast Transformer Decoding: One Write-Head is All You Need https://arxiv.org/pdf/1911.02150.pdf  \n4https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096  \n5GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf  \n6How Attention works in Deep Learning: understanding the attention mechanism in sequence models https://theaisummer.com/attention/  \n7A simple overview of RNN, LSTM and Attention Mechanism \nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b  \n8https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention  \n9Transformer https://spaces.ac.cn/archives/8620  \n10https://theaisummer.com/self-attention/  https://theaisummer.com/self-attention/  \n11https://zhuanlan.zhihu.com/p/626820422 https://zhuanlan.zhihu.com/p/626820422  \n12Are Sixteen Heads Really Better than One? \nhttps://arxiv.org/pdf/1905.10650.pdf  \n13This post is all you needTransformer \nhttps://zhuanlan.zhihu.com/p/420820453  \n14The Illustrated Transformer https://jalammar.github.io/illustrated-transformer/  \n15Multi-Query Attention is All You Need https://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055  \n\n","source":"_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA.md","raw":"---\ntitle: 'Attention:MHA,MQAGQA'\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - attention\n  - KV Cache\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 3dc22f96\ndate: 2024-03-05 18:49:38\n---\n\n//  \n\nAttentionMHAMulti-Head AttentionMQAMulti-Query AttentionGQAGrouped-Query AttentionKV Cache  \n\nAttentionFlashAttentionSliding Window Attention  \n\nLLM\n\n# AttentionRNNAttention\n\nattention\n\nattention\n\n## RNN\n\n>Memory is attention through time. ~ Alex Graves 2020\n\nTransformerRNNSeq2Seq\n\n{% asset_img seq2seq.png seq2seq %}  \n\n{% asset_img encoder.png encoder %}  \n\n{% asset_img decoder.png decoder %}  \n\n[AI Summer](https://theaisummer.com/attention/)  \n\nRNN cellhidden stateRNN encodercontext $z$ RNN decoder $z$ decodertoken[start]  \n\n $z$   \n\n  \n\nLSMTGRU  \n\n $z$  $z$   \n\n $z$   \n\n  \n\nCNNheatmap  \n\n{% asset_img cnn_heatmap.png heatmap %}  \n\nCNNimplicitly\n\nSeq2Seqimplicitexplicit  \n\nRNN $i$ $h_i$  $h_i$  $h_i$ hidden state  \n\n --   \n\n $i$ decoder $y_{i-1}$ encoder $\\mathbf{h}$ score\n\n$$\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in R^n$$  \n\n\n\n$$e_{ij}=\\text{attentiom}_{\\text{net }(\\mathbf{y}_{i-1},h_j)}$$  \n\n $\\mathbf{y}_{i-1}$  $h_j$  $e_{ij}$fc  \n \n  $e_{ij}$ attention netencoder hidden statesoftmax  \n\n$$\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}$$  \n\ndecoder  \n\n$$z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j$$\n\n{% asset_img seq2seq_attention.png seq2seq attention %}  \n\nattention netdecoderhidden stateencoder hidden state\n\nattentionattention  \n\n{% asset_img attention_calculation.png attention calculation %}  \n\nattention $\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot h$  $s$ decoderhidden state $y$ $h$ encoderhidden state  \n\nscaled dot-product attention  \n\n## Transformerattention\n\nRNN attentiontransformer attentionAttention Is All You NeedRNNtime stepattentionhidden stateattention  \n\n{% asset_img transformer_structure.png transformer structure.png %}  \n\nencoderdecoderencoderattentionself-attentiondecoderself-attentioncross-attention  \n\ntransformerattention $\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V$ $Q=W_{Q}YK=W_{K}XV=W_{V}X$ cross-attention $X$ encoderhidden states$Y$ decoderhidden statesself-attention $X=Y$  \n\nscaled dot-product attentionsoftmax\n\n$$\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V$$  \n\nattention  \n\n\n\n{% asset_img Scaled-dot-product-self-attention.pbm self-attention %}  \n\nquerykeyvalueattention  \n\n+  \n\n-30keyvalue  \n\n30querykey5  \n\ntop5 $[8,4,4,2,2]$  $[5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]$  \n \n1 $[0.4,0.2,0.2,0.1,0.1]$ 30 $0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}$ \n\ntransformer attention $QK^T$ softmax/  \n\nself-attention $QKV$  $X$sequencetokencross-attentiondecodersequence  \n\nself-attention $QKV$  $X$  $QK^T$  $QK^T$ MHA  \n\nattentionMHAMQAGQA\n\n[pytorch forcasting](https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention)\n\n```python\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self, dropout: float = None, scale: bool = True):\n        super(ScaledDotProductAttention, self).__init__()\n        if dropout is not None:\n            self.dropout = nn.Dropout(p=dropout)\n        else:\n            self.dropout = dropout\n        self.softmax = nn.Softmax(dim=2)\n        self.scale = scale\n\ndef forward(self, q, k, v, mask=None):\n        attn = torch.bmm(q, k.permute(0, 2, 1))  # query-key overlap\n\n        if self.scale:\n            dimension = torch.as_tensor(k.size(-1), dtype=attn.dtype, device=attn.device).sqrt()\n            attn = attn / dimension\n\n        if mask is not None:\n            attn = attn.masked_fill(mask, -1e9)\n        attn = self.softmax(attn)\n\n        if self.dropout is not None:\n            attn = self.dropout(attn)\n        output = torch.bmm(attn, v)\n        return output, attn\n```\n\n## scaling\n\nBTW $QK^T$  $\\sqrt{d}$   \n\nsoftmaxsoftmax  \n\n{% asset_img softmax.png softmax %}  \n\n[](https://spaces.ac.cn/archives/8620)attentionscaling $\\sqrt{d}$ softmaxnormalizationattentionscaling  \n\n# MHA\n\nattentionMHAmulti-head attention\n\nMHA2017Attention Is All You Needattentionattention  \n\n$$\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)$$  \n\n$$head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)$$  \n\nhidden size $d$ MHA $QKV$ hidden state $head_{num}$  $d_{head}$  $head_{num}$  $QKV$ attention  $head_{num}$  $d_{head}$ concat  \n\namazing  \n\n{% asset_img multihead_attention.png MHA %}  \n\n  \n\nAttention Is All You Need\n\n>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.  \n\nattention headattention head  \n\nCNN $3\\times3\\times128$ 128 $3\\times3$  $3\\times3$   \n\n$$\\left.\\left[\\begin{matrix}1&0&-1\\\\1&0&-1\\\\1&0&-1\\end{matrix}\\right.\\right]$$  \n\n  \n\n$$\\left.\\left[\\begin{matrix}1&1&1\\\\0&0&0\\\\-1&-1&-1\\end{matrix}\\right.\\right]$$  \n\n  \n\n128 $3\\times3$ 128MHA  \n\nexpect  \n\n[](https://zhuanlan.zhihu.com/p/626820422)12 $QK^T$   \n\nMHAattentionattention\n\n  \n\n[Are Sixteen Heads Really Better than One?](https://arxiv.org/pdf/1905.10650.pdf)MHA  \n\nhidden sizeLLM1216244896  \n\n[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)MHA  \n\n```python\ndef attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n             / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim = -1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\n\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        '''\n        h: head number\n        '''\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d\n        self.d = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n        \n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n        \n        # 1) Do all the linear projections in batch from d_model => h x d \n        query, key, value = \\\n            [l(x).view(nbatches, -1, self.h, self.d).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key, value))]\n        \n        # 2) Apply attention on all the projected vectors in batch. \n        x, self.attn = attention(query, key, value, mask=mask, \n                                 dropout=self.dropout)\n        \n        # 3) \"Concat\" using a view and apply a final linear. \n        x = x.transpose(1, 2).contiguous() \\\n             .view(nbatches, -1, self.h * self.d)\n        return self.linears[-1](x)\n```\n\n[transformers](https://github.com/huggingface/transformers)\n\n# KV Cache\n\nMQAGQAKV Cache  \n\nencoder-decoderAGIdecoder-onlyLLMauto-regressive  \n\n $\\text{input}_{i-1}$  $\\text{token}_{i}$  $\\text{token}_{i}$  $\\text{input}_{i-1}$  $\\text{input}_{i}$  $\\text{input}_{i}$  $\\text{token}_{i+1}$   \n\ntokentoken  \n\n```\nstep0: =[BOS]=\nstep1: =[BOS]=\nstep2: =[BOS]=\nstep3: =[BOS]=\nstep4: =[BOS]=\nstep5: =[BOS]=[EOS]\n```\n\n[BOS][EOS]  \n\nhidden state \n\nstepsteptokenstepstep\n\n\n\nattention  \n\n$$\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n$$\n\ndecodermask attention\n\n34attention\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n$$\n\n45attention\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n$$\n\n5 $o_{0}$  $o_{2}$   \n\n  \n\n  \n\nstep0101step515instruction800stepstep0800step1799...\n\nstep  \n\nKV Cache  \n\n $k$  $v$   \n\n34 $k$  $v$ \n\n\n\n$$\n\\text{cache}_l=\\text{None}\\\\\n$$\n\n<center></center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$  \n\nkv_cache $l$ \n\n5 $l$ <u>****</u> $k$  $v$  $o_{3}$  $o_{0}o_{1}o_{2}$   \n\n $l$  $o_{0}o_{1}o_{2}$ FNN $l+1$  $l+1$  $k$  $v$  $l+1$  $k$  $v$   \n\n $k$  $v$ \n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$\n\n<center></center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n$$  \n\nattentionFFN  \n\ntransformersuse_cache=TrueKV Cache  \n\nGPT2  \n\n```python\nClass GPT2Attention(nn.Module):\n    ...\n    ...\n    def forward(\n        self,\n        hidden_states: Optional[Tuple[torch.FloatTensor]],\n        layer_past: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n        if encoder_hidden_states is not None:\n            if not hasattr(self, \"q_attn\"):\n                raise ValueError(\n                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n                )\n\n            query = self.q_attn(hidden_states)\n            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n            attention_mask = encoder_attention_mask\n        else:\n            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n\n        query = self._split_heads(query, self.num_heads, self.head_dim)\n        key = self._split_heads(key, self.num_heads, self.head_dim)\n        value = self._split_heads(value, self.num_heads, self.head_dim)\n\n        # \n        if layer_past is not None:\n            past_key, past_value = layer_past\n            key = torch.cat((past_key, key), dim=-2)  # key\n            value = torch.cat((past_value, value), dim=-2)  # value\n\n        if use_cache is True:\n            present = (key, value)  # \n        else:\n            present = None\n\n        if self.reorder_and_upcast_attn:\n            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n        else:\n            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n\n        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n        attn_output = self.c_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output)\n\n        outputs = (attn_output, present)\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs  # a, present, (attentions)\n```\n\nKV Cachedecodermask attentiontokentoken  \n\nKV Cache  \n\n $s$  $L$ hidden size $d$   \n\n$$\n2\\times L\\times s\\times d\n$$  \n\n\n\n$$\n2\\times 2\\times L\\times s\\times d\n$$  \n\nLlama2 7B $L=32$  $L=4096$ token524,288 bytes52K $s=1024$ 536,870,912 bytes500M  \n\nbatch size=1batch size1G  \n\nMHA $qkv$ \n\n\n\n{% asset_img gpu_cache.png gpu cache %}  \n\nH10050ML2 CacheL1 CacheLlama2 7B100token  \n\nLLM34B/70B\n\nL2 CacheHBML2 Cache  \n\n{% asset_img sram_dram.png  %}  \n\n  \n\nCache  \n\n  \n\n# MQA\n\nMQA\n\nGoogle2019Fast Transformer Decoding: One Write-Head is All You NeedMQABert  \n\nMQAMHA $W_{Q}W_{K}W_{V}$ nn= $d_{model}$  $d_{head}$ attentionMQA $Q$ MHA $KV$  $d_{head}$ nQuery $KV$ attention  \n\nMHA $KV$ MQA $KV$ MHA  \n\n{% asset_img MQA.webp MQA %}  \n\n $KV$   \n\nLlama2 7B32MQA1024token1/32536,870,912 bytes / 32 = 16,777,216 bytes16M\n\n $KV$   \n\nMQAMHAhidden sizehead num  \n\n{% asset_img mqa_result_1.png MQA results 1 %}  \n\n{% asset_img mqa_result_3.png MQA results 3 %}  \n\n# GQA  \n\nMQAMHAGQAGrouped-Query AttentionMQAMHA  \n\nGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints2023\n\nGQA $Q$ MHA/MQA $KV$  $Q$  $Q$ groupgroup $Q$  $KV$ group $Q$  $KV$   \n\nMHA $KV$ GQAMQA $KV$ GQA  \n\n\n\n{% asset_img GQA.png GQA %}  \n\n  \n\n{% asset_img GQA_result_1.png GQA result %}  \n\n2/3/4GQAMHAMQAMHAMQAGQAaverage poolingMHAMHAGQA  \n\nLlama2GQAtech reportMHAMQAGQA  \n\n{% asset_img llama2_qga.png llama2 GQA result %}  \n\n#   \n\nMHAMQAGQA\n\nGQALLM  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n1The Annotated Transformer \n https://nlp.seas.harvard.edu/2018/04/03/attention.html  \n2Attention Is All You Need https://arxiv.org/pdf/1706.03762.pdf  \n3Fast Transformer Decoding: One Write-Head is All You Need https://arxiv.org/pdf/1911.02150.pdf  \n4https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096  \n5GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf  \n6How Attention works in Deep Learning: understanding the attention mechanism in sequence models https://theaisummer.com/attention/  \n7A simple overview of RNN, LSTM and Attention Mechanism \nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b  \n8https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention  \n9Transformer https://spaces.ac.cn/archives/8620  \n10https://theaisummer.com/self-attention/  https://theaisummer.com/self-attention/  \n11https://zhuanlan.zhihu.com/p/626820422 https://zhuanlan.zhihu.com/p/626820422  \n12Are Sixteen Heads Really Better than One? \nhttps://arxiv.org/pdf/1905.10650.pdf  \n13This post is all you needTransformer \nhttps://zhuanlan.zhihu.com/p/420820453  \n14The Illustrated Transformer https://jalammar.github.io/illustrated-transformer/  \n15Multi-Query Attention is All You Need https://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055  \n\n","slug":"cs/nlp/2024/03/Attention-MHA-MQAGQA","published":1,"updated":"2024-03-14T14:17:54.411Z","comments":1,"layout":"post","photos":[],"_id":"clttl4al10008rb4k23jp14zi","content":"<p>//</p>\n<p>AttentionMHAMulti-Head\nAttentionMQAMulti-Query AttentionGQAGrouped-Query\nAttentionKV\nCache</p>\n<p>AttentionFlashAttentionSliding\nWindow Attention</p>\n<p>LLM</p>\n<h1 id=\"attentionrnnattention\">AttentionRNNAttention</h1>\n<p>attention</p>\n<p>attention</p>\n<h2 id=\"rnn\">RNN</h2>\n<blockquote>\n<p>Memory is attention through time. ~ Alex Graves 2020</p>\n</blockquote>\n<p>TransformerRNNSeq2Seq</p>\n<img src=\"/3dc22f96/seq2seq.png\" class title=\"seq2seq\">\n<img src=\"/3dc22f96/encoder.png\" class title=\"encoder\">\n<img src=\"/3dc22f96/decoder.png\" class title=\"decoder\">\n<p><a href=\"https://theaisummer.com/attention/\">AI\nSummer</a></p>\n<p>RNN cellhidden stateRNN\nencodercontext <span class=\"math inline\">\\(z\\)</span> RNN decoder <span class=\"math inline\">\\(z\\)</span>\ndecodertoken[start]</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p></p>\n<p>LSMTGRU</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p></p>\n<p>CNNheatmap</p>\n<img src=\"/3dc22f96/cnn_heatmap.png\" class title=\"heatmap\">\n<p>CNNimplicitly</p>\n<p>Seq2Seqimplicitexplicit</p>\n<p>RNN <span class=\"math inline\">\\(i\\)</span> <span class=\"math inline\">\\(h_i\\)</span>  <span class=\"math inline\">\\(h_i\\)</span>\n\n<span class=\"math inline\">\\(h_i\\)</span>\nhidden\nstate</p>\n<p>\n--\n</p>\n<p> <span class=\"math inline\">\\(i\\)</span>\ndecoder <span class=\"math inline\">\\(y_{i-1}\\)</span>\nencoder <span class=\"math inline\">\\(\\mathbf{h}\\)</span>\nscore</p>\n<p><span class=\"math display\">\\[\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in\nR^n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[e_{ij}=\\text{attentiom}_{\\text{net\n}(\\mathbf{y}_{i-1},h_j)}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\mathbf{y}_{i-1}\\)</span>\n <span class=\"math inline\">\\(h_j\\)</span>  <span class=\"math inline\">\\(e_{ij}\\)</span>fc</p>\n<p> <span class=\"math inline\">\\(e_{ij}\\)</span>\nattention\nnetencoder hidden statesoftmax</p>\n<p><span class=\"math display\">\\[\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}\\]</span></p>\n<p>decoder</p>\n<p><span class=\"math display\">\\[z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j\\]</span></p>\n<img src=\"/3dc22f96/seq2seq_attention.png\" class title=\"seq2seq attention\">\n<p>attention netdecoderhidden\nstateencoder hidden\nstate</p>\n<p>attentionattention</p>\n<img src=\"/3dc22f96/attention_calculation.png\" class title=\"attention calculation\">\n<p>attention <span class=\"math inline\">\\(\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot\nh\\)</span>  <span class=\"math inline\">\\(s\\)</span>\ndecoderhidden state <span class=\"math inline\">\\(y\\)</span> <span class=\"math inline\">\\(h\\)</span> encoderhidden state</p>\n<p>scaled dot-product\nattention</p>\n<h2 id=\"transformerattention\">Transformerattention</h2>\n<p>RNN attentiontransformer\nattentionAttention Is All You\nNeedRNNtime\nstepattentionhidden\nstateattention</p>\n<img src=\"/3dc22f96/transformer_structure.png\" class title=\"transformer structure.png\">\n<p>encoderdecoderencoderattentionself-attentiondecoderself-attentioncross-attention</p>\n<p>transformerattention <span class=\"math inline\">\\(\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V\\)</span>\n<span class=\"math inline\">\\(Q=W_{Q}YK=W_{K}XV=W_{V}X\\)</span>\ncross-attention <span class=\"math inline\">\\(X\\)</span>\nencoderhidden states<span class=\"math inline\">\\(Y\\)</span>\ndecoderhidden statesself-attention <span class=\"math inline\">\\(X=Y\\)</span></p>\n<p>scaled dot-product attentionsoftmax</p>\n<p><span class=\"math display\">\\[\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V\\]</span></p>\n<p>attention</p>\n<p></p>\n<img src=\"/3dc22f96/Scaled-dot-product-self-attention.pbm\" class title=\"self-attention\">\n<p>querykeyvalueattention</p>\n<p>+</p>\n<p>-30keyvalue</p>\n<p>30querykey5</p>\n<p>top5 <span class=\"math inline\">\\([8,4,4,2,2]\\)</span>  <span class=\"math inline\">\\([5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]\\)</span>\n</p>\n<p>1 <span class=\"math inline\">\\([0.4,0.2,0.2,0.1,0.1]\\)</span>\n30 <span class=\"math inline\">\\(0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}\\)</span>\n</p>\n<p>transformer attention <span class=\"math inline\">\\(QK^T\\)</span>\nsoftmax/</p>\n<p>self-attention <span class=\"math inline\">\\(QKV\\)</span>\n <span class=\"math inline\">\\(X\\)</span>sequencetokencross-attentiondecodersequence</p>\n<p>self-attention <span class=\"math inline\">\\(QKV\\)</span>\n <span class=\"math inline\">\\(X\\)</span>  <span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(QK^T\\)</span>\nMHA</p>\n<p>attentionMHAMQAGQA</p>\n<p><a href=\"https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention\">pytorch\nforcasting</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ScaledDotProductAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, dropout: <span class=\"built_in\">float</span> = <span class=\"literal\">None</span>, scale: <span class=\"built_in\">bool</span> = <span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.dropout = dropout</span><br><span class=\"line\">        self.softmax = nn.Softmax(dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">        self.scale = scale</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, q, k, v, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        attn = torch.bmm(q, k.permute(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))  <span class=\"comment\"># query-key overlap</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.scale:</span><br><span class=\"line\">            dimension = torch.as_tensor(k.size(-<span class=\"number\">1</span>), dtype=attn.dtype, device=attn.device).sqrt()</span><br><span class=\"line\">            attn = attn / dimension</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = attn.masked_fill(mask, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">        attn = self.softmax(attn)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = self.dropout(attn)</span><br><span class=\"line\">        output = torch.bmm(attn, v)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, attn</span><br></pre></td></tr></table></figure>\n<h2 id=\"scaling\">scaling</h2>\n<p>BTW <span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(\\sqrt{d}\\)</span> </p>\n<p>softmaxsoftmax</p>\n<img src=\"/3dc22f96/softmax.png\" class title=\"softmax\">\n<p><a href=\"https://spaces.ac.cn/archives/8620\"></a>attentionscaling\n<span class=\"math inline\">\\(\\sqrt{d}\\)</span>\nsoftmaxnormalizationattentionscaling</p>\n<h1 id=\"mha\">MHA</h1>\n<p>attentionMHAmulti-head\nattention</p>\n<p>MHA2017Attention Is All You\nNeedattentionattention</p>\n<p><span class=\"math display\">\\[\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)\\]</span></p>\n<p><span class=\"math display\">\\[head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)\\]</span></p>\n<p>hidden size <span class=\"math inline\">\\(d\\)</span>\nMHA <span class=\"math inline\">\\(QKV\\)</span>\nhidden state <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span>  <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(QKV\\)</span>\nattention <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span> concat</p>\n<p>amazing</p>\n<img src=\"/3dc22f96/multihead_attention.png\" class title=\"MHA\">\n<p></p>\n<p>Attention Is All You Need</p>\n<blockquote>\n<p>Multi-head attention allows the model to jointly attend to\ninformation from different representation subspaces at different\npositions.</p>\n</blockquote>\n<p>attention\nheadattention\nhead</p>\n<p>CNN <span class=\"math inline\">\\(3\\times3\\times128\\)</span> 128 <span class=\"math inline\">\\(3\\times3\\)</span>\n <span class=\"math inline\">\\(3\\times3\\)</span> </p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;1&amp;1\\\\0&amp;0&amp;0\\\\-1&amp;-1&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p></p>\n<p>128 <span class=\"math inline\">\\(3\\times3\\)</span>\n128MHA</p>\n<p>expect</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/626820422\"></a>12\n<span class=\"math inline\">\\(QK^T\\)</span> </p>\n<p>MHAattentionattention</p>\n<p></p>\n<p><a href=\"https://arxiv.org/pdf/1905.10650.pdf\">Are Sixteen Heads Really\nBetter than One?</a>MHA</p>\n<p>hidden\nsizeLLM1216244896</p>\n<p><a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\">The\nAnnotated Transformer</a>MHA</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">attention</span>(<span class=\"params\">query, key, value, mask=<span class=\"literal\">None</span>, dropout=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class=\"line\">    d_k = query.size(-<span class=\"number\">1</span>)</span><br><span class=\"line\">    scores = torch.matmul(query, key.transpose(-<span class=\"number\">2</span>, -<span class=\"number\">1</span>)) \\</span><br><span class=\"line\">             / math.sqrt(d_k)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        scores = scores.masked_fill(mask == <span class=\"number\">0</span>, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">    p_attn = F.softmax(scores, dim = -<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        p_attn = dropout(p_attn)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultiHeadedAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, h, d_model, dropout=<span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        h: head number</span></span><br><span class=\"line\"><span class=\"string\">        &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> d_model % h == <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"comment\"># We assume d_v always equals d</span></span><br><span class=\"line\">        self.d = d_model // h</span><br><span class=\"line\">        self.h = h</span><br><span class=\"line\">        self.linears = clones(nn.Linear(d_model, d_model), <span class=\"number\">4</span>)</span><br><span class=\"line\">        self.attn = <span class=\"literal\">None</span></span><br><span class=\"line\">        self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, query, key, value, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"comment\"># Same mask applied to all h heads.</span></span><br><span class=\"line\">            mask = mask.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">        nbatches = query.size(<span class=\"number\">0</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 1) Do all the linear projections in batch from d_model =&gt; h x d </span></span><br><span class=\"line\">        query, key, value = \\</span><br><span class=\"line\">            [l(x).view(nbatches, -<span class=\"number\">1</span>, self.h, self.d).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">             <span class=\"keyword\">for</span> l, x <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(self.linears, (query, key, value))]</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class=\"line\">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class=\"line\">                                 dropout=self.dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class=\"line\">        x = x.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous() \\</span><br><span class=\"line\">             .view(nbatches, -<span class=\"number\">1</span>, self.h * self.d)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.linears[-<span class=\"number\">1</span>](x)</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://github.com/huggingface/transformers\">transformers</a></p>\n<h1 id=\"kv-cache\">KV Cache</h1>\n<p>MQAGQAKV\nCache</p>\n<p>encoder-decoderAGIdecoder-onlyLLMauto-regressive</p>\n<p> <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span> \n<span class=\"math inline\">\\(\\text{input}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{input}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i+1}\\)</span>\n</p>\n<p>tokentoken</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">step0: =[BOS]=</span><br><span class=\"line\">step1: =[BOS]=</span><br><span class=\"line\">step2: =[BOS]=</span><br><span class=\"line\">step3: =[BOS]=</span><br><span class=\"line\">step4: =[BOS]=</span><br><span class=\"line\">step5: =[BOS]=[EOS]</span><br></pre></td></tr></table></figure>\n<p>[BOS][EOS]</p>\n<p>hidden\nstate</p>\n<p>stepsteptokenstepstep</p>\n<p></p>\n<p>attention</p>\n<p><span class=\"math display\">\\[\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n\\]</span></p>\n<p>decodermask\nattention</p>\n<p>34attention</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>45attention</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&amp;=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>5 <span class=\"math inline\">\\(o_{0}\\)</span>  <span class=\"math inline\">\\(o_{2}\\)</span> </p>\n<p></p>\n<p></p>\n<p>step0101step515instruction800stepstep0800step1799...</p>\n<p>step</p>\n<p>KV\nCache</p>\n<p> <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p>34\n<span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=\\text{None}\\\\\n\\]</span></p>\n<center>\n\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<p>kv_cache <span class=\"math inline\">\\(l\\)</span>\n</p>\n<p>5 <span class=\"math inline\">\\(l\\)</span>\n<u><strong></strong></u> <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span>  <span class=\"math inline\">\\(o_{3}\\)</span>  <span class=\"math inline\">\\(o_{0}o_{1}o_{2}\\)</span> </p>\n<p> <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(o_{0}o_{1}o_{2}\\)</span>\nFNN <span class=\"math inline\">\\(l+1\\)</span>\n <span class=\"math inline\">\\(l+1\\)</span>\n <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span>  <span class=\"math inline\">\\(l+1\\)</span>  <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p> <span class=\"math inline\">\\(k\\)</span> \n<span class=\"math inline\">\\(v\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<center>\n\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n\\]</span></p>\n<p>attentionFFN</p>\n<p>transformersuse_cache=TrueKV Cache</p>\n<p>GPT2</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Class GPT2Attention(nn.Module):</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">        self,</span></span><br><span class=\"line\"><span class=\"params\">        hidden_states: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.FloatTensor]],</span></span><br><span class=\"line\"><span class=\"params\">        layer_past: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.Tensor]] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        head_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_hidden_states: <span class=\"type\">Optional</span>[torch.Tensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        use_cache: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">        output_attentions: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">    </span>) -&gt; <span class=\"type\">Tuple</span>[<span class=\"type\">Union</span>[torch.Tensor, <span class=\"type\">Tuple</span>[torch.Tensor]], ...]:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> encoder_hidden_states <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">hasattr</span>(self, <span class=\"string\">&quot;q_attn&quot;</span>):</span><br><span class=\"line\">                <span class=\"keyword\">raise</span> ValueError(</span><br><span class=\"line\">                    <span class=\"string\">&quot;If class is used as cross attention, the weights `q_attn` have to be defined. &quot;</span></span><br><span class=\"line\">                    <span class=\"string\">&quot;Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.&quot;</span></span><br><span class=\"line\">                )</span><br><span class=\"line\"></span><br><span class=\"line\">            query = self.q_attn(hidden_states)</span><br><span class=\"line\">            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">            attention_mask = encoder_attention_mask</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        query = self._split_heads(query, self.num_heads, self.head_dim)</span><br><span class=\"line\">        key = self._split_heads(key, self.num_heads, self.head_dim)</span><br><span class=\"line\">        value = self._split_heads(value, self.num_heads, self.head_dim)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># </span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> layer_past <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            past_key, past_value = layer_past</span><br><span class=\"line\">            key = torch.cat((past_key, key), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># key</span></span><br><span class=\"line\">            value = torch.cat((past_value, value), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># value</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> use_cache <span class=\"keyword\">is</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">            present = (key, value)  <span class=\"comment\"># </span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            present = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.reorder_and_upcast_attn:</span><br><span class=\"line\">            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)</span><br><span class=\"line\">        attn_output = self.c_proj(attn_output)</span><br><span class=\"line\">        attn_output = self.resid_dropout(attn_output)</span><br><span class=\"line\"></span><br><span class=\"line\">        outputs = (attn_output, present)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> output_attentions:</span><br><span class=\"line\">            outputs += (attn_weights,)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> outputs  <span class=\"comment\"># a, present, (attentions)</span></span><br></pre></td></tr></table></figure>\n<p>KV\nCachedecodermask\nattentiontokentoken</p>\n<p>KV Cache</p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n <span class=\"math inline\">\\(L\\)</span> hidden size <span class=\"math inline\">\\(d\\)</span> </p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d\n\\]</span></p>\n<p>Llama2 7B <span class=\"math inline\">\\(L=32\\)</span> \n<span class=\"math inline\">\\(L=4096\\)</span>\ntoken524,288 bytes52K <span class=\"math inline\">\\(s=1024\\)</span> 536,870,912\nbytes500M</p>\n<p>batch size=1batch\nsize1G</p>\n<p>MHA <span class=\"math inline\">\\(qkv\\)</span>\n</p>\n<p></p>\n<img src=\"/3dc22f96/gpu_cache.png\" class title=\"gpu cache\">\n<p>H10050ML2 CacheL1\nCacheLlama2\n7B100token</p>\n<p>LLM34B/70B</p>\n<p>L2 CacheHBML2\nCache</p>\n<img src=\"/3dc22f96/sram_dram.png\" class title=\"\">\n<p></p>\n<p>Cache</p>\n<p></p>\n<h1 id=\"mqa\">MQA</h1>\n<p>MQA</p>\n<p>Google2019Fast Transformer Decoding: One Write-Head is All\nYou\nNeedMQABert</p>\n<p>MQAMHA <span class=\"math inline\">\\(W_{Q}W_{K}W_{V}\\)</span>\nnn= <span class=\"math inline\">\\(d_{model}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span>\nattentionMQA <span class=\"math inline\">\\(Q\\)</span> MHA <span class=\"math inline\">\\(KV\\)</span> \n<span class=\"math inline\">\\(d_{head}\\)</span>\nnQuery <span class=\"math inline\">\\(KV\\)</span>\nattention</p>\n<p>MHA <span class=\"math inline\">\\(KV\\)</span>\nMQA <span class=\"math inline\">\\(KV\\)</span>\nMHA</p>\n<img src=\"/3dc22f96/MQA.webp\" class title=\"MQA\">\n<p> <span class=\"math inline\">\\(KV\\)</span>\n</p>\n<p>Llama2\n7B32MQA1024token1/32536,870,912\nbytes / 32 = 16,777,216 bytes16M</p>\n<p> <span class=\"math inline\">\\(KV\\)</span>\n</p>\n<p>MQAMHAhidden\nsizehead num</p>\n<img src=\"/3dc22f96/mqa_result_1.png\" class title=\"MQA results 1\">\n<img src=\"/3dc22f96/mqa_result_3.png\" class title=\"MQA results 3\">\n<h1 id=\"gqa\">GQA</h1>\n<p>MQAMHAGQAGrouped-Query\nAttentionMQAMHA</p>\n<p>GQA: Training Generalized Multi-Query Transformer Models\nfrom Multi-Head Checkpoints2023</p>\n<p>GQA <span class=\"math inline\">\\(Q\\)</span>\nMHA/MQA <span class=\"math inline\">\\(KV\\)</span>\n <span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(Q\\)</span> groupgroup\n<span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(KV\\)</span> group <span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(KV\\)</span> </p>\n<p>MHA <span class=\"math inline\">\\(KV\\)</span>\nGQAMQA <span class=\"math inline\">\\(KV\\)</span> GQA</p>\n<p></p>\n<img src=\"/3dc22f96/GQA.png\" class title=\"GQA\">\n<p></p>\n<img src=\"/3dc22f96/GQA_result_1.png\" class title=\"GQA result\">\n<p>2/3/4GQAMHAMQAMHAMQAGQAaverage\npoolingMHAMHAGQA</p>\n<p>Llama2GQAtech\nreportMHAMQAGQA</p>\n<img src=\"/3dc22f96/llama2_qga.png\" class title=\"llama2 GQA result\">\n<h1 id=\"\"></h1>\n<p>MHAMQAGQA</p>\n<p>GQALLM</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1The Annotated Transformer\nhttps://nlp.seas.harvard.edu/2018/04/03/attention.html<br>\n2Attention Is All You Need\nhttps://arxiv.org/pdf/1706.03762.pdf<br>\n3Fast Transformer Decoding: One Write-Head is All You Need\nhttps://arxiv.org/pdf/1911.02150.pdf<br>\n4https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096<br>\n5GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf<br>\n6How Attention works in Deep Learning: understanding the attention\nmechanism in sequence models https://theaisummer.com/attention/<br>\n7A simple overview of RNN, LSTM and Attention Mechanism\nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b<br>\n8https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention<br>\n9Transformer\nhttps://spaces.ac.cn/archives/8620<br>\n10https://theaisummer.com/self-attention/\nhttps://theaisummer.com/self-attention/<br>\n11https://zhuanlan.zhihu.com/p/626820422\nhttps://zhuanlan.zhihu.com/p/626820422<br>\n12Are Sixteen Heads Really Better than One?\nhttps://arxiv.org/pdf/1905.10650.pdf<br>\n13This post is all you needTransformer\nhttps://zhuanlan.zhihu.com/p/420820453<br>\n14The Illustrated Transformer\nhttps://jalammar.github.io/illustrated-transformer/<br>\n15Multi-Query Attention is All You Need\nhttps://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055</p>\n","length":16733,"excerpt":"","more":"<p>//</p>\n<p>AttentionMHAMulti-Head\nAttentionMQAMulti-Query AttentionGQAGrouped-Query\nAttentionKV\nCache</p>\n<p>AttentionFlashAttentionSliding\nWindow Attention</p>\n<p>LLM</p>\n<h1 id=\"attentionrnnattention\">AttentionRNNAttention</h1>\n<p>attention</p>\n<p>attention</p>\n<h2 id=\"rnn\">RNN</h2>\n<blockquote>\n<p>Memory is attention through time. ~ Alex Graves 2020</p>\n</blockquote>\n<p>TransformerRNNSeq2Seq</p>\n<img src=\"/3dc22f96/seq2seq.png\" class title=\"seq2seq\">\n<img src=\"/3dc22f96/encoder.png\" class title=\"encoder\">\n<img src=\"/3dc22f96/decoder.png\" class title=\"decoder\">\n<p><a href=\"https://theaisummer.com/attention/\">AI\nSummer</a></p>\n<p>RNN cellhidden stateRNN\nencodercontext <span class=\"math inline\">\\(z\\)</span> RNN decoder <span class=\"math inline\">\\(z\\)</span>\ndecodertoken[start]</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p></p>\n<p>LSMTGRU</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p></p>\n<p>CNNheatmap</p>\n<img src=\"/3dc22f96/cnn_heatmap.png\" class title=\"heatmap\">\n<p>CNNimplicitly</p>\n<p>Seq2Seqimplicitexplicit</p>\n<p>RNN <span class=\"math inline\">\\(i\\)</span> <span class=\"math inline\">\\(h_i\\)</span>  <span class=\"math inline\">\\(h_i\\)</span>\n\n<span class=\"math inline\">\\(h_i\\)</span>\nhidden\nstate</p>\n<p>\n--\n</p>\n<p> <span class=\"math inline\">\\(i\\)</span>\ndecoder <span class=\"math inline\">\\(y_{i-1}\\)</span>\nencoder <span class=\"math inline\">\\(\\mathbf{h}\\)</span>\nscore</p>\n<p><span class=\"math display\">\\[\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in\nR^n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[e_{ij}=\\text{attentiom}_{\\text{net\n}(\\mathbf{y}_{i-1},h_j)}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\mathbf{y}_{i-1}\\)</span>\n <span class=\"math inline\">\\(h_j\\)</span>  <span class=\"math inline\">\\(e_{ij}\\)</span>fc</p>\n<p> <span class=\"math inline\">\\(e_{ij}\\)</span>\nattention\nnetencoder hidden statesoftmax</p>\n<p><span class=\"math display\">\\[\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}\\]</span></p>\n<p>decoder</p>\n<p><span class=\"math display\">\\[z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j\\]</span></p>\n<img src=\"/3dc22f96/seq2seq_attention.png\" class title=\"seq2seq attention\">\n<p>attention netdecoderhidden\nstateencoder hidden\nstate</p>\n<p>attentionattention</p>\n<img src=\"/3dc22f96/attention_calculation.png\" class title=\"attention calculation\">\n<p>attention <span class=\"math inline\">\\(\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot\nh\\)</span>  <span class=\"math inline\">\\(s\\)</span>\ndecoderhidden state <span class=\"math inline\">\\(y\\)</span> <span class=\"math inline\">\\(h\\)</span> encoderhidden state</p>\n<p>scaled dot-product\nattention</p>\n<h2 id=\"transformerattention\">Transformerattention</h2>\n<p>RNN attentiontransformer\nattentionAttention Is All You\nNeedRNNtime\nstepattentionhidden\nstateattention</p>\n<img src=\"/3dc22f96/transformer_structure.png\" class title=\"transformer structure.png\">\n<p>encoderdecoderencoderattentionself-attentiondecoderself-attentioncross-attention</p>\n<p>transformerattention <span class=\"math inline\">\\(\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V\\)</span>\n<span class=\"math inline\">\\(Q=W_{Q}YK=W_{K}XV=W_{V}X\\)</span>\ncross-attention <span class=\"math inline\">\\(X\\)</span>\nencoderhidden states<span class=\"math inline\">\\(Y\\)</span>\ndecoderhidden statesself-attention <span class=\"math inline\">\\(X=Y\\)</span></p>\n<p>scaled dot-product attentionsoftmax</p>\n<p><span class=\"math display\">\\[\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V\\]</span></p>\n<p>attention</p>\n<p></p>\n<img src=\"/3dc22f96/Scaled-dot-product-self-attention.pbm\" class title=\"self-attention\">\n<p>querykeyvalueattention</p>\n<p>+</p>\n<p>-30keyvalue</p>\n<p>30querykey5</p>\n<p>top5 <span class=\"math inline\">\\([8,4,4,2,2]\\)</span>  <span class=\"math inline\">\\([5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]\\)</span>\n</p>\n<p>1 <span class=\"math inline\">\\([0.4,0.2,0.2,0.1,0.1]\\)</span>\n30 <span class=\"math inline\">\\(0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}\\)</span>\n</p>\n<p>transformer attention <span class=\"math inline\">\\(QK^T\\)</span>\nsoftmax/</p>\n<p>self-attention <span class=\"math inline\">\\(QKV\\)</span>\n <span class=\"math inline\">\\(X\\)</span>sequencetokencross-attentiondecodersequence</p>\n<p>self-attention <span class=\"math inline\">\\(QKV\\)</span>\n <span class=\"math inline\">\\(X\\)</span>  <span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(QK^T\\)</span>\nMHA</p>\n<p>attentionMHAMQAGQA</p>\n<p><a href=\"https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention\">pytorch\nforcasting</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ScaledDotProductAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, dropout: <span class=\"built_in\">float</span> = <span class=\"literal\">None</span>, scale: <span class=\"built_in\">bool</span> = <span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.dropout = dropout</span><br><span class=\"line\">        self.softmax = nn.Softmax(dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">        self.scale = scale</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, q, k, v, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        attn = torch.bmm(q, k.permute(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))  <span class=\"comment\"># query-key overlap</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.scale:</span><br><span class=\"line\">            dimension = torch.as_tensor(k.size(-<span class=\"number\">1</span>), dtype=attn.dtype, device=attn.device).sqrt()</span><br><span class=\"line\">            attn = attn / dimension</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = attn.masked_fill(mask, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">        attn = self.softmax(attn)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = self.dropout(attn)</span><br><span class=\"line\">        output = torch.bmm(attn, v)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, attn</span><br></pre></td></tr></table></figure>\n<h2 id=\"scaling\">scaling</h2>\n<p>BTW <span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(\\sqrt{d}\\)</span> </p>\n<p>softmaxsoftmax</p>\n<img src=\"/3dc22f96/softmax.png\" class title=\"softmax\">\n<p><a href=\"https://spaces.ac.cn/archives/8620\"></a>attentionscaling\n<span class=\"math inline\">\\(\\sqrt{d}\\)</span>\nsoftmaxnormalizationattentionscaling</p>\n<h1 id=\"mha\">MHA</h1>\n<p>attentionMHAmulti-head\nattention</p>\n<p>MHA2017Attention Is All You\nNeedattentionattention</p>\n<p><span class=\"math display\">\\[\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)\\]</span></p>\n<p><span class=\"math display\">\\[head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)\\]</span></p>\n<p>hidden size <span class=\"math inline\">\\(d\\)</span>\nMHA <span class=\"math inline\">\\(QKV\\)</span>\nhidden state <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span>  <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(QKV\\)</span>\nattention <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span> concat</p>\n<p>amazing</p>\n<img src=\"/3dc22f96/multihead_attention.png\" class title=\"MHA\">\n<p></p>\n<p>Attention Is All You Need</p>\n<blockquote>\n<p>Multi-head attention allows the model to jointly attend to\ninformation from different representation subspaces at different\npositions.</p>\n</blockquote>\n<p>attention\nheadattention\nhead</p>\n<p>CNN <span class=\"math inline\">\\(3\\times3\\times128\\)</span> 128 <span class=\"math inline\">\\(3\\times3\\)</span>\n <span class=\"math inline\">\\(3\\times3\\)</span> </p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;1&amp;1\\\\0&amp;0&amp;0\\\\-1&amp;-1&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p></p>\n<p>128 <span class=\"math inline\">\\(3\\times3\\)</span>\n128MHA</p>\n<p>expect</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/626820422\"></a>12\n<span class=\"math inline\">\\(QK^T\\)</span> </p>\n<p>MHAattentionattention</p>\n<p></p>\n<p><a href=\"https://arxiv.org/pdf/1905.10650.pdf\">Are Sixteen Heads Really\nBetter than One?</a>MHA</p>\n<p>hidden\nsizeLLM1216244896</p>\n<p><a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\">The\nAnnotated Transformer</a>MHA</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">attention</span>(<span class=\"params\">query, key, value, mask=<span class=\"literal\">None</span>, dropout=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class=\"line\">    d_k = query.size(-<span class=\"number\">1</span>)</span><br><span class=\"line\">    scores = torch.matmul(query, key.transpose(-<span class=\"number\">2</span>, -<span class=\"number\">1</span>)) \\</span><br><span class=\"line\">             / math.sqrt(d_k)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        scores = scores.masked_fill(mask == <span class=\"number\">0</span>, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">    p_attn = F.softmax(scores, dim = -<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        p_attn = dropout(p_attn)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultiHeadedAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, h, d_model, dropout=<span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        h: head number</span></span><br><span class=\"line\"><span class=\"string\">        &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> d_model % h == <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"comment\"># We assume d_v always equals d</span></span><br><span class=\"line\">        self.d = d_model // h</span><br><span class=\"line\">        self.h = h</span><br><span class=\"line\">        self.linears = clones(nn.Linear(d_model, d_model), <span class=\"number\">4</span>)</span><br><span class=\"line\">        self.attn = <span class=\"literal\">None</span></span><br><span class=\"line\">        self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, query, key, value, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"comment\"># Same mask applied to all h heads.</span></span><br><span class=\"line\">            mask = mask.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">        nbatches = query.size(<span class=\"number\">0</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 1) Do all the linear projections in batch from d_model =&gt; h x d </span></span><br><span class=\"line\">        query, key, value = \\</span><br><span class=\"line\">            [l(x).view(nbatches, -<span class=\"number\">1</span>, self.h, self.d).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">             <span class=\"keyword\">for</span> l, x <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(self.linears, (query, key, value))]</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class=\"line\">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class=\"line\">                                 dropout=self.dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class=\"line\">        x = x.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous() \\</span><br><span class=\"line\">             .view(nbatches, -<span class=\"number\">1</span>, self.h * self.d)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.linears[-<span class=\"number\">1</span>](x)</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://github.com/huggingface/transformers\">transformers</a></p>\n<h1 id=\"kv-cache\">KV Cache</h1>\n<p>MQAGQAKV\nCache</p>\n<p>encoder-decoderAGIdecoder-onlyLLMauto-regressive</p>\n<p> <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span> \n<span class=\"math inline\">\\(\\text{input}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{input}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i+1}\\)</span>\n</p>\n<p>tokentoken</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">step0: =[BOS]=</span><br><span class=\"line\">step1: =[BOS]=</span><br><span class=\"line\">step2: =[BOS]=</span><br><span class=\"line\">step3: =[BOS]=</span><br><span class=\"line\">step4: =[BOS]=</span><br><span class=\"line\">step5: =[BOS]=[EOS]</span><br></pre></td></tr></table></figure>\n<p>[BOS][EOS]</p>\n<p>hidden\nstate</p>\n<p>stepsteptokenstepstep</p>\n<p></p>\n<p>attention</p>\n<p><span class=\"math display\">\\[\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n\\]</span></p>\n<p>decodermask\nattention</p>\n<p>34attention</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>45attention</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&amp;=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>5 <span class=\"math inline\">\\(o_{0}\\)</span>  <span class=\"math inline\">\\(o_{2}\\)</span> </p>\n<p></p>\n<p></p>\n<p>step0101step515instruction800stepstep0800step1799...</p>\n<p>step</p>\n<p>KV\nCache</p>\n<p> <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p>34\n<span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=\\text{None}\\\\\n\\]</span></p>\n<center>\n\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<p>kv_cache <span class=\"math inline\">\\(l\\)</span>\n</p>\n<p>5 <span class=\"math inline\">\\(l\\)</span>\n<u><strong></strong></u> <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span>  <span class=\"math inline\">\\(o_{3}\\)</span>  <span class=\"math inline\">\\(o_{0}o_{1}o_{2}\\)</span> </p>\n<p> <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(o_{0}o_{1}o_{2}\\)</span>\nFNN <span class=\"math inline\">\\(l+1\\)</span>\n <span class=\"math inline\">\\(l+1\\)</span>\n <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span>  <span class=\"math inline\">\\(l+1\\)</span>  <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p> <span class=\"math inline\">\\(k\\)</span> \n<span class=\"math inline\">\\(v\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<center>\n\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n\\]</span></p>\n<p>attentionFFN</p>\n<p>transformersuse_cache=TrueKV Cache</p>\n<p>GPT2</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Class GPT2Attention(nn.Module):</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">        self,</span></span><br><span class=\"line\"><span class=\"params\">        hidden_states: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.FloatTensor]],</span></span><br><span class=\"line\"><span class=\"params\">        layer_past: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.Tensor]] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        head_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_hidden_states: <span class=\"type\">Optional</span>[torch.Tensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        use_cache: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">        output_attentions: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">    </span>) -&gt; <span class=\"type\">Tuple</span>[<span class=\"type\">Union</span>[torch.Tensor, <span class=\"type\">Tuple</span>[torch.Tensor]], ...]:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> encoder_hidden_states <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">hasattr</span>(self, <span class=\"string\">&quot;q_attn&quot;</span>):</span><br><span class=\"line\">                <span class=\"keyword\">raise</span> ValueError(</span><br><span class=\"line\">                    <span class=\"string\">&quot;If class is used as cross attention, the weights `q_attn` have to be defined. &quot;</span></span><br><span class=\"line\">                    <span class=\"string\">&quot;Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.&quot;</span></span><br><span class=\"line\">                )</span><br><span class=\"line\"></span><br><span class=\"line\">            query = self.q_attn(hidden_states)</span><br><span class=\"line\">            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">            attention_mask = encoder_attention_mask</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        query = self._split_heads(query, self.num_heads, self.head_dim)</span><br><span class=\"line\">        key = self._split_heads(key, self.num_heads, self.head_dim)</span><br><span class=\"line\">        value = self._split_heads(value, self.num_heads, self.head_dim)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># </span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> layer_past <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            past_key, past_value = layer_past</span><br><span class=\"line\">            key = torch.cat((past_key, key), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># key</span></span><br><span class=\"line\">            value = torch.cat((past_value, value), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># value</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> use_cache <span class=\"keyword\">is</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">            present = (key, value)  <span class=\"comment\"># </span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            present = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.reorder_and_upcast_attn:</span><br><span class=\"line\">            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)</span><br><span class=\"line\">        attn_output = self.c_proj(attn_output)</span><br><span class=\"line\">        attn_output = self.resid_dropout(attn_output)</span><br><span class=\"line\"></span><br><span class=\"line\">        outputs = (attn_output, present)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> output_attentions:</span><br><span class=\"line\">            outputs += (attn_weights,)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> outputs  <span class=\"comment\"># a, present, (attentions)</span></span><br></pre></td></tr></table></figure>\n<p>KV\nCachedecodermask\nattentiontokentoken</p>\n<p>KV Cache</p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n <span class=\"math inline\">\\(L\\)</span> hidden size <span class=\"math inline\">\\(d\\)</span> </p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d\n\\]</span></p>\n<p>Llama2 7B <span class=\"math inline\">\\(L=32\\)</span> \n<span class=\"math inline\">\\(L=4096\\)</span>\ntoken524,288 bytes52K <span class=\"math inline\">\\(s=1024\\)</span> 536,870,912\nbytes500M</p>\n<p>batch size=1batch\nsize1G</p>\n<p>MHA <span class=\"math inline\">\\(qkv\\)</span>\n</p>\n<p></p>\n<img src=\"/3dc22f96/gpu_cache.png\" class title=\"gpu cache\">\n<p>H10050ML2 CacheL1\nCacheLlama2\n7B100token</p>\n<p>LLM34B/70B</p>\n<p>L2 CacheHBML2\nCache</p>\n<img src=\"/3dc22f96/sram_dram.png\" class title=\"\">\n<p></p>\n<p>Cache</p>\n<p></p>\n<h1 id=\"mqa\">MQA</h1>\n<p>MQA</p>\n<p>Google2019Fast Transformer Decoding: One Write-Head is All\nYou\nNeedMQABert</p>\n<p>MQAMHA <span class=\"math inline\">\\(W_{Q}W_{K}W_{V}\\)</span>\nnn= <span class=\"math inline\">\\(d_{model}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span>\nattentionMQA <span class=\"math inline\">\\(Q\\)</span> MHA <span class=\"math inline\">\\(KV\\)</span> \n<span class=\"math inline\">\\(d_{head}\\)</span>\nnQuery <span class=\"math inline\">\\(KV\\)</span>\nattention</p>\n<p>MHA <span class=\"math inline\">\\(KV\\)</span>\nMQA <span class=\"math inline\">\\(KV\\)</span>\nMHA</p>\n<img src=\"/3dc22f96/MQA.webp\" class title=\"MQA\">\n<p> <span class=\"math inline\">\\(KV\\)</span>\n</p>\n<p>Llama2\n7B32MQA1024token1/32536,870,912\nbytes / 32 = 16,777,216 bytes16M</p>\n<p> <span class=\"math inline\">\\(KV\\)</span>\n</p>\n<p>MQAMHAhidden\nsizehead num</p>\n<img src=\"/3dc22f96/mqa_result_1.png\" class title=\"MQA results 1\">\n<img src=\"/3dc22f96/mqa_result_3.png\" class title=\"MQA results 3\">\n<h1 id=\"gqa\">GQA</h1>\n<p>MQAMHAGQAGrouped-Query\nAttentionMQAMHA</p>\n<p>GQA: Training Generalized Multi-Query Transformer Models\nfrom Multi-Head Checkpoints2023</p>\n<p>GQA <span class=\"math inline\">\\(Q\\)</span>\nMHA/MQA <span class=\"math inline\">\\(KV\\)</span>\n <span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(Q\\)</span> groupgroup\n<span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(KV\\)</span> group <span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(KV\\)</span> </p>\n<p>MHA <span class=\"math inline\">\\(KV\\)</span>\nGQAMQA <span class=\"math inline\">\\(KV\\)</span> GQA</p>\n<p></p>\n<img src=\"/3dc22f96/GQA.png\" class title=\"GQA\">\n<p></p>\n<img src=\"/3dc22f96/GQA_result_1.png\" class title=\"GQA result\">\n<p>2/3/4GQAMHAMQAMHAMQAGQAaverage\npoolingMHAMHAGQA</p>\n<p>Llama2GQAtech\nreportMHAMQAGQA</p>\n<img src=\"/3dc22f96/llama2_qga.png\" class title=\"llama2 GQA result\">\n<h1 id=\"\"></h1>\n<p>MHAMQAGQA</p>\n<p>GQALLM</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1The Annotated Transformer\nhttps://nlp.seas.harvard.edu/2018/04/03/attention.html<br>\n2Attention Is All You Need\nhttps://arxiv.org/pdf/1706.03762.pdf<br>\n3Fast Transformer Decoding: One Write-Head is All You Need\nhttps://arxiv.org/pdf/1911.02150.pdf<br>\n4https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096<br>\n5GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf<br>\n6How Attention works in Deep Learning: understanding the attention\nmechanism in sequence models https://theaisummer.com/attention/<br>\n7A simple overview of RNN, LSTM and Attention Mechanism\nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b<br>\n8https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention<br>\n9Transformer\nhttps://spaces.ac.cn/archives/8620<br>\n10https://theaisummer.com/self-attention/\nhttps://theaisummer.com/self-attention/<br>\n11https://zhuanlan.zhihu.com/p/626820422\nhttps://zhuanlan.zhihu.com/p/626820422<br>\n12Are Sixteen Heads Really Better than One?\nhttps://arxiv.org/pdf/1905.10650.pdf<br>\n13This post is all you needTransformer\nhttps://zhuanlan.zhihu.com/p/420820453<br>\n14The Illustrated Transformer\nhttps://jalammar.github.io/illustrated-transformer/<br>\n15Multi-Query Attention is All You Need\nhttps://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055</p>\n"}],"PostAsset":[{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi.png","post":"clttl4aky0002rb4ke9vgh881","slug":"meta_pi.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi_explanation.png","post":"clttl4aky0002rb4ke9vgh881","slug":"meta_pi_explanation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi_nosft.png","post":"clttl4aky0002rb4ke9vgh881","slug":"meta_pi_nosft.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_rope_ext.png","post":"clttl4aky0002rb4ke9vgh881","slug":"meta_rope_ext.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/mix_precision_fp16.png","post":"clttl4aky0002rb4ke9vgh881","slug":"mix_precision_fp16.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/rope_matrix.png","post":"clttl4aky0002rb4ke9vgh881","slug":"rope_matrix.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/big_bird_attention.png","post":"clttl4al00004rb4k186cc9s1","slug":"big_bird_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/dilated_conv.png","post":"clttl4al00004rb4k186cc9s1","slug":"dilated_conv.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/longformer_attention.png","post":"clttl4al00004rb4k186cc9s1","slug":"longformer_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_architechture.png","post":"clttl4al00004rb4k186cc9s1","slug":"mistral_architechture.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_large_performance.jpeg","post":"clttl4al00004rb4k186cc9s1","slug":"mistral_large_performance.jpeg","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_perf.png","post":"clttl4al00004rb4k186cc9s1","slug":"mistral_perf.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_swa.png","post":"clttl4al00004rb4k186cc9s1","slug":"mistral_swa.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/ms_invest_mistral.png","post":"clttl4al00004rb4k186cc9s1","slug":"ms_invest_mistral.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/prefill_and_chunking.png","post":"clttl4al00004rb4k186cc9s1","slug":"prefill_and_chunking.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/receptive_field_cnn.png","post":"clttl4al00004rb4k186cc9s1","slug":"receptive_field_cnn.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/rolling_buffer.png","post":"clttl4al00004rb4k186cc9s1","slug":"rolling_buffer.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/complex_number.png","post":"clttl4al10007rb4k4nyqhg3o","slug":"complex_number.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/remote_attenuation.png","post":"clttl4al10007rb4k4nyqhg3o","slug":"remote_attenuation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/rope.png","post":"clttl4al10007rb4k4nyqhg3o","slug":"rope.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/GQA.png","post":"clttl4al10008rb4k23jp14zi","slug":"GQA.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/GQA_result_1.png","post":"clttl4al10008rb4k23jp14zi","slug":"GQA_result_1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/MQA.png","post":"clttl4al10008rb4k23jp14zi","slug":"MQA.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/MQA.webp","post":"clttl4al10008rb4k23jp14zi","slug":"MQA.webp","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Markdown _  Nice.html","post":"clttl4al10008rb4k23jp14zi","slug":"Markdown _  Nice.html","modified":1,"renderable":1},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Scaled-dot-product-self-attention.pbm","post":"clttl4al10008rb4k23jp14zi","slug":"Scaled-dot-product-self-attention.pbm","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Scaled-dot-product-self-attention.png","post":"clttl4al10008rb4k23jp14zi","slug":"Scaled-dot-product-self-attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/attention_calculation.png","post":"clttl4al10008rb4k23jp14zi","slug":"attention_calculation.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/cnn_heatmap.png","post":"clttl4al10008rb4k23jp14zi","slug":"cnn_heatmap.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/decoder.png","post":"clttl4al10008rb4k23jp14zi","slug":"decoder.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/encoder.png","post":"clttl4al10008rb4k23jp14zi","slug":"encoder.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/gpu_cache.png","post":"clttl4al10008rb4k23jp14zi","slug":"gpu_cache.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/lihongyi_self_attention.png","post":"clttl4al10008rb4k23jp14zi","slug":"lihongyi_self_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/llama2_qga.png","post":"clttl4al10008rb4k23jp14zi","slug":"llama2_qga.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/mqa_result_1.png","post":"clttl4al10008rb4k23jp14zi","slug":"mqa_result_1.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/mqa_result_3.png","post":"clttl4al10008rb4k23jp14zi","slug":"mqa_result_3.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/multihead_attention.png","post":"clttl4al10008rb4k23jp14zi","slug":"multihead_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/seq2seq.png","post":"clttl4al10008rb4k23jp14zi","slug":"seq2seq.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/seq2seq_attention.png","post":"clttl4al10008rb4k23jp14zi","slug":"seq2seq_attention.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/softmax.png","post":"clttl4al10008rb4k23jp14zi","slug":"softmax.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/sram_dram.png","post":"clttl4al10008rb4k23jp14zi","slug":"sram_dram.png","modified":1,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/transformer_structure.png","post":"clttl4al10008rb4k23jp14zi","slug":"transformer_structure.png","modified":1,"renderable":0}],"PostCategory":[{"post_id":"clttl4al10008rb4k23jp14zi","category_id":"clttl4al00005rb4keyff2zjp","_id":"clttl4al4000vrb4k4ehec6zy"},{"post_id":"clttl4al10008rb4k23jp14zi","category_id":"clttl4al3000drb4k4igca5dc","_id":"clttl4al4000xrb4k1han027h"},{"post_id":"clttl4al10008rb4k23jp14zi","category_id":"clttl4al3000orb4k08mp80z4","_id":"clttl4al4000zrb4kazuj2buy"},{"post_id":"clttl4aky0002rb4ke9vgh881","category_id":"clttl4al00005rb4keyff2zjp","_id":"clttl4al40010rb4ka1cgdhwb"},{"post_id":"clttl4aky0002rb4ke9vgh881","category_id":"clttl4al3000drb4k4igca5dc","_id":"clttl4al40012rb4k5jqm12o4"},{"post_id":"clttl4aky0002rb4ke9vgh881","category_id":"clttl4al3000orb4k08mp80z4","_id":"clttl4al40013rb4k695c84al"},{"post_id":"clttl4al00004rb4k186cc9s1","category_id":"clttl4al00005rb4keyff2zjp","_id":"clttl4al40015rb4kb1b89u6n"},{"post_id":"clttl4al00004rb4k186cc9s1","category_id":"clttl4al3000drb4k4igca5dc","_id":"clttl4al40017rb4k9fsy46hj"},{"post_id":"clttl4al00004rb4k186cc9s1","category_id":"clttl4al3000orb4k08mp80z4","_id":"clttl4al5001arb4k75hx4mkh"},{"post_id":"clttl4al10007rb4k4nyqhg3o","category_id":"clttl4al00005rb4keyff2zjp","_id":"clttl4al5001crb4k8wh1c24q"},{"post_id":"clttl4al10007rb4k4nyqhg3o","category_id":"clttl4al3000drb4k4igca5dc","_id":"clttl4al5001frb4kfsjv5l6i"},{"post_id":"clttl4al10007rb4k4nyqhg3o","category_id":"clttl4al3000orb4k08mp80z4","_id":"clttl4al5001hrb4k7pukd43s"}],"PostTag":[{"post_id":"clttl4aky0002rb4ke9vgh881","tag_id":"clttl4al10006rb4khrlb7570","_id":"clttl4al3000jrb4kbwn1gc4m"},{"post_id":"clttl4aky0002rb4ke9vgh881","tag_id":"clttl4al2000arb4k5vgyg94u","_id":"clttl4al3000lrb4kcujqc4g9"},{"post_id":"clttl4aky0002rb4ke9vgh881","tag_id":"clttl4al3000crb4k6if1e5ap","_id":"clttl4al3000nrb4k6tca6o51"},{"post_id":"clttl4aky0002rb4ke9vgh881","tag_id":"clttl4al3000erb4k09en2ljt","_id":"clttl4al4000prb4kgd6hdamk"},{"post_id":"clttl4aky0002rb4ke9vgh881","tag_id":"clttl4al3000grb4k6zho56xt","_id":"clttl4al4000rrb4kd11j2ecx"},{"post_id":"clttl4al00004rb4k186cc9s1","tag_id":"clttl4al10006rb4khrlb7570","_id":"clttl4al40016rb4ke01h8klc"},{"post_id":"clttl4al00004rb4k186cc9s1","tag_id":"clttl4al2000arb4k5vgyg94u","_id":"clttl4al50018rb4k1mhj0m18"},{"post_id":"clttl4al00004rb4k186cc9s1","tag_id":"clttl4al3000crb4k6if1e5ap","_id":"clttl4al5001brb4kg9lzhtur"},{"post_id":"clttl4al00004rb4k186cc9s1","tag_id":"clttl4al4000trb4khbyc189k","_id":"clttl4al5001drb4k0lo533ar"},{"post_id":"clttl4al00004rb4k186cc9s1","tag_id":"clttl4al4000wrb4kgrzdf2f0","_id":"clttl4al5001grb4kcchlb24f"},{"post_id":"clttl4al00004rb4k186cc9s1","tag_id":"clttl4al40011rb4kfg5janah","_id":"clttl4al5001irb4kbxlxecye"},{"post_id":"clttl4al10007rb4k4nyqhg3o","tag_id":"clttl4al10006rb4khrlb7570","_id":"clttl4al5001mrb4k8f9j73nb"},{"post_id":"clttl4al10007rb4k4nyqhg3o","tag_id":"clttl4al2000arb4k5vgyg94u","_id":"clttl4al5001nrb4kd44rdm7o"},{"post_id":"clttl4al10007rb4k4nyqhg3o","tag_id":"clttl4al3000crb4k6if1e5ap","_id":"clttl4al5001prb4kee5t078o"},{"post_id":"clttl4al10007rb4k4nyqhg3o","tag_id":"clttl4al5001jrb4kemj5bf3d","_id":"clttl4al5001qrb4kgg2l8m8f"},{"post_id":"clttl4al10007rb4k4nyqhg3o","tag_id":"clttl4al5001krb4kfysjd6ai","_id":"clttl4al5001srb4kcark3gyf"},{"post_id":"clttl4al10008rb4k23jp14zi","tag_id":"clttl4al10006rb4khrlb7570","_id":"clttl4al5001urb4keo5836bf"},{"post_id":"clttl4al10008rb4k23jp14zi","tag_id":"clttl4al2000arb4k5vgyg94u","_id":"clttl4al5001vrb4kb5dv16t4"},{"post_id":"clttl4al10008rb4k23jp14zi","tag_id":"clttl4al3000crb4k6if1e5ap","_id":"clttl4al5001wrb4k45ofdfwj"},{"post_id":"clttl4al10008rb4k23jp14zi","tag_id":"clttl4al4000trb4khbyc189k","_id":"clttl4al5001xrb4k137z4uyw"},{"post_id":"clttl4al10008rb4k23jp14zi","tag_id":"clttl4al5001trb4k7zmf7np9","_id":"clttl4al5001yrb4k53xa45g7"}],"Tag":[{"name":"NLP","_id":"clttl4al10006rb4khrlb7570"},{"name":"LLM","_id":"clttl4al2000arb4k5vgyg94u"},{"name":"transformer","_id":"clttl4al3000crb4k6if1e5ap"},{"name":"","_id":"clttl4al3000erb4k09en2ljt"},{"name":"","_id":"clttl4al3000grb4k6zho56xt"},{"name":"attention","_id":"clttl4al4000trb4khbyc189k"},{"name":"sliding window attention","_id":"clttl4al4000wrb4kgrzdf2f0"},{"name":"sparse attention","_id":"clttl4al40011rb4kfg5janah"},{"name":"positional encoding","_id":"clttl4al5001jrb4kemj5bf3d"},{"name":"RoPE","_id":"clttl4al5001krb4kfysjd6ai"},{"name":"KV Cache","_id":"clttl4al5001trb4k7zmf7np9"}]}}