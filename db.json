{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","path":"css/noscript.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","path":"images/logo-algolia-nebula-blue-full.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","path":"js/bookmark.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","path":"js/comments-buttons.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","path":"js/comments.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/config.js","path":"js/config.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","path":"js/motion.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","path":"js/pjax.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","path":"js/schedule.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","path":"js/next-boot.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/addtoany.js","path":"js/third-party/addtoany.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/fancybox.js","path":"js/third-party/fancybox.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/pace.js","path":"js/third-party/pace.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","path":"js/third-party/quicklink.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","path":"js/third-party/analytics/baidu-analytics.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","path":"js/third-party/analytics/google-analytics.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","path":"js/third-party/analytics/growingio.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/matomo.js","path":"js/third-party/analytics/matomo.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","path":"js/third-party/chat/chatra.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","path":"js/third-party/comments/changyan.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","path":"js/third-party/chat/tidio.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","path":"js/third-party/comments/disqus.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","path":"js/third-party/comments/gitalk.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","path":"js/third-party/comments/isso.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","path":"js/third-party/comments/disqusjs.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","path":"js/third-party/comments/livere.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","path":"js/third-party/comments/utterances.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","path":"js/third-party/math/katex.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","path":"js/third-party/math/mathjax.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","path":"js/third-party/search/algolia-search.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","path":"js/third-party/search/local-search.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","path":"js/third-party/statistics/firestore.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","path":"js/third-party/statistics/lean-analytics.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","path":"js/third-party/tags/mermaid.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","path":"js/third-party/tags/pdf.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/wavedrom.js","path":"js/third-party/tags/wavedrom.js","modified":0,"renderable":1},{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"source/images/cover.png","path":"images/cover.png","modified":0,"renderable":0},{"_id":"source/images/qrcode.jpg","path":"images/qrcode.jpg","modified":0,"renderable":0},{"_id":"source/images/avatar/20180303210737_XsJVr.jpeg","path":"images/avatar/20180303210737_XsJVr.jpeg","modified":0,"renderable":0},{"_id":"source/images/avatar/Picasso_Elephant.png","path":"images/avatar/Picasso_Elephant.png","modified":0,"renderable":0},{"_id":"source/images/avatar/shadow.png","path":"images/avatar/shadow.png","modified":0,"renderable":0},{"_id":"source/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","path":"images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","modified":0,"renderable":0},{"_id":"source/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","path":"images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","modified":0,"renderable":0},{"_id":"source/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","path":"images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","modified":0,"renderable":0},{"_id":"source/images/background/wallhaven-2ywymm.png","path":"images/background/wallhaven-2ywymm.png","modified":0,"renderable":0},{"_id":"source/images/background/wallhaven-gpxpg3.png","path":"images/background/wallhaven-gpxpg3.png","modified":0,"renderable":0},{"_id":"source/images/background/wallhaven-p97q73.png","path":"images/background/wallhaven-p97q73.png","modified":0,"renderable":0},{"_id":"source/images/background/wallhaven-x636oz.png","path":"images/background/wallhaven-x636oz.png","modified":0,"renderable":0},{"_id":"source/images/favicon/favicon_io/about.txt","path":"images/favicon/favicon_io/about.txt","modified":0,"renderable":0},{"_id":"source/images/favicon/favicon_io/android-chrome-192x192.png","path":"images/favicon/favicon_io/android-chrome-192x192.png","modified":0,"renderable":0},{"_id":"source/images/favicon/favicon_io/android-chrome-512x512.png","path":"images/favicon/favicon_io/android-chrome-512x512.png","modified":0,"renderable":0},{"_id":"source/images/favicon/favicon_io/apple-touch-icon.png","path":"images/favicon/favicon_io/apple-touch-icon.png","modified":0,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon-16x16.png","path":"images/favicon/favicon_io/favicon-16x16.png","modified":0,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon-32x32.png","path":"images/favicon/favicon_io/favicon-32x32.png","modified":0,"renderable":0},{"_id":"source/images/favicon/favicon_io/favicon.ico","path":"images/favicon/favicon_io/favicon.ico","modified":0,"renderable":0},{"_id":"source/images/favicon/favicon_io/site.webmanifest","path":"images/favicon/favicon_io/site.webmanifest","modified":0,"renderable":0}],"Cache":[{"_id":"node_modules/hexo-theme-next/_vendors.yml","hash":"4f6046ceb1470be9ff334ede20b73871c951d845","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/.DS_Store","hash":"1d67a44d93a3429d76ad084bde035dc4f20e3100","modified":1715072325708},{"_id":"node_modules/hexo-theme-next/package.json","hash":"4b48877b223ec717e708540a2df03d64983c02ab","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/LICENSE.md","hash":"68fc9a03d50fd4b5ea97092b05967d1819dea2c4","modified":1706697684353},{"_id":"node_modules/hexo-theme-next/README.md","hash":"d6820f46d03a93bd6dc8b10f49f58aec82ad2b06","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/_config.yml","hash":"255c963c680da5da34c259c560dd8211b75188ca","modified":1708604632809},{"_id":"node_modules/hexo-theme-next/docs/AUTHORS.md","hash":"a648823121563c34a177ae91f5a774b5e29f01a0","modified":1706697684352},{"_id":"node_modules/hexo-theme-next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1706697684352},{"_id":"node_modules/hexo-theme-next/layout/_layout.njk","hash":"fc0a45112f2dcfc2642404e8934ea32a793c3bd7","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/docs/LICENSE.txt","hash":"f5b14f791b7cfa1d16da981d929152e088a5d1b8","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/layout/.DS_Store","hash":"a2cb6a68d26ca70f79d97ee70364cdebc56f5b79","modified":1715072325706},{"_id":"node_modules/hexo-theme-next/layout/category.njk","hash":"c68b7343d0f8145010f93351908cc36ef6212ec1","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/archive.njk","hash":"d759f4d2cf5ddc6875ea250113a00662c1caf6d1","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/index.njk","hash":"dd63e488ae8cc144335a5958acedf6a16edd7a92","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/page.njk","hash":"b0660b2af0ac7d3fda14ca4d9f2c9e79ef06c6f9","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/languages/README.md","hash":"b2567e32805dda79601157351a07e5ca9fe01315","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/layout/tag.njk","hash":"9e16ba20c28a7f2c6bc75aa427f48122301a30aa","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/languages/ar.yml","hash":"7d0f39e8684284a04bb9808521c87fecda8bd131","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/languages/de.yml","hash":"79b37df731c29665dee6cd7c90d278e1edfb6e24","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/eo.yml","hash":"e34bb33ae827bf2f0727088599a73bc64bdad1b0","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/es.yml","hash":"dffc63ef42e1266b88e0acf08994fd17a9908d53","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/bn.yml","hash":"564bed75da6e05b11dce6164508f97a15e2fb6c2","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/en.yml","hash":"ba0fd79a2b1d8db01a034180556061745965ff05","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/id.yml","hash":"929df147f4f17d638b07de5fe52ca13e2549ab1c","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/fr.yml","hash":"8ac44e58f71a38b7697a2f7f98a6971ed818cb5b","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/scripts/.DS_Store","hash":"f185aa14c1236b351528993f4337fe74ba8c2af2","modified":1715072325671},{"_id":"node_modules/hexo-theme-next/languages/fa.yml","hash":"f3ffc444599f4ac92d62e9ed00a1490ebc277d70","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/it.yml","hash":"16d716ecfd748def2f6486ef5a82d0ab7ceb4890","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/ko.yml","hash":"d345a303310c8a5f4836c3683f3580f861ebd1b4","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/ja.yml","hash":"543222bfc516aab6c33e8534f807972ecb8943a9","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/layout/post.njk","hash":"0bfce9f133f501a9a4837257e3b862b3bbca15be","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/languages/pt.yml","hash":"70de366e10ea584ba039d40d6b35ac97f93454ad","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/pt-BR.yml","hash":"76b8576ce228d540a16b1f0af5af2cce20923194","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/th.yml","hash":"6829e998b39f8f143e20b276bb1f62d95a29de58","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/uk.yml","hash":"ff537047b4b4c3ca9a7b64fa7f428a9942751eeb","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/nl.yml","hash":"3cb3687696635ec71b4ca40c5fc43b56acc8843e","modified":1706697684376},{"_id":"node_modules/hexo-theme-next/languages/tk.yml","hash":"511726054873f6f8d7ce0d2e803f6731de0ddbe7","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/vi.yml","hash":"7ebcba5e1128784195e4681dffc9d34c4e873fec","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/si.yml","hash":"2d712eedf3f60d04d36c3108cf5a12e2a52e875c","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/languages/tr.yml","hash":"a57e4ed089b893a95f5e1ecff17ce625165f4d46","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/zh-CN.yml","hash":"741d7efe0262c9cdc2c648014b55599665d90f6b","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/languages/zh-HK.yml","hash":"88ea50eeb9097ab4a87a44981a102d8594feb064","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/source/.DS_Store","hash":"8b400bb5f7b29cca6335dd6ab550d517d0597767","modified":1715072325717},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/README.md","hash":"12a3e96581964a22b474cc739675d52ef93ff932","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CONTRIBUTING.md","hash":"a089f7a8368ab0b7d7b9b7ec0ac3767a453435df","modified":1706697684353},{"_id":"node_modules/hexo-theme-next/languages/zh-TW.yml","hash":"4695c87d6b81b3a23d16ad6513d9eaa925f8d8ad","modified":1706697684378},{"_id":"node_modules/hexo-theme-next/layout/_macro/post-collapse.njk","hash":"abda600685ee972e1f6b7a2dcc56f13e2daa6263","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_macro/sidebar.njk","hash":"547c62ab14d9e05d2d9116db9048a677fbe1fb6d","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_macro/post.njk","hash":"cbe208445e4d1df82ebd1761e1eaced3eab77fb3","modified":1706698899947},{"_id":"node_modules/hexo-theme-next/docs/ru/README.md","hash":"29c89a41b371f893e56c87ea61adabc444ec58cc","modified":1706697684354},{"_id":"node_modules/hexo-theme-next/layout/_partials/comments.njk","hash":"d0c470b0f6690aa217e9ada848c5e2e73fb27c6f","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"12a6631617695504d5cf2a94b57d87bd331bef6f","modified":1706697684353},{"_id":"node_modules/hexo-theme-next/layout/_partials/languages.njk","hash":"e43f22198cccb5f6e306b1ce0d28d12a4fb891f8","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_partials/footer.njk","hash":"d77ec95cfee58b17807763dc2adb7946829cb316","modified":1706757600094},{"_id":"node_modules/hexo-theme-next/layout/_scripts/index.njk","hash":"6668878a0f9a1166c6a879755f54a08d942da870","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_partials/pagination.njk","hash":"bc719473ed5948ab6859449d60b8d36cfc1542b4","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/widgets.njk","hash":"e7f988ecddb2159313699a00827a45eca5622bd4","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/index.njk","hash":"dfd7cdd6ba89f8c3deabc27726c7a350cadafd11","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_scripts/vendors.njk","hash":"be80b9fe415a9a09d74c28e230995fd292dfc123","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/languages/ru.yml","hash":"c6d8de0ff7d8148d09993257cfd3b7aca755696c","modified":1706697684377},{"_id":"node_modules/hexo-theme-next/layout/_third-party/addtoany.njk","hash":"ef64c6bfb8540cd874701236b9be47db2496e98e","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_partials/.DS_Store","hash":"168617768f812b394ad2e34bbffa6ca6fb8b2f98","modified":1715072325705},{"_id":"node_modules/hexo-theme-next/layout/_third-party/pace.njk","hash":"d7ad5714079f7f65446f880baf14722435ca9061","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/scripts/filters/locals.js","hash":"9eb5310664759931287dd28ea39165dfb67f12ed","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/scripts/filters/default-injects.js","hash":"872f01cb10e422a648ea505436532e776e92926b","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/scripts/filters/minify.js","hash":"447db39d17775b2bd18d8af9c9d65b7b8449f751","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/scripts/events/index.js","hash":"bd9ea82376cd87df611ea3ae077875c7c595a3df","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/filters/post.js","hash":"fdc8a0af90035e89c3fcb754a0eb189b8951a2bc","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/scripts/helpers/engine.js","hash":"d292b78485e8e8055712b0ed6de7cf559c5fbdcd","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/scripts/helpers/navigation.js","hash":"78107021101553c3d23e89290f7530b60cf4aa86","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-config.js","hash":"ead37e9167b682f1fa34b5401c3050e18c7ee4a3","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/helpers/font.js","hash":"3394185a7f0393c16ce52c8028f90da3e9239c55","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-paginator.js","hash":"e86c764b546e4fbb87970cabc4135a56f9ef9fe1","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-url.js","hash":"6281d47c1de98eb38f3aa0f6df29bbb19d412173","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-vendors.js","hash":"957241c28796ff352de7f4cffba7bb289b043586","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/button.js","hash":"c6ad2ed544fbb25ecb5d820c36e76302504271b7","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/scripts/tags/caniuse.js","hash":"935a311142a409c1896b3ae3f01fe7a9e2db1134","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/scripts/tags/center-quote.js","hash":"92c19d796bdb3320df9caea59bf52df7a95d9da9","modified":1706697684337},{"_id":"node_modules/hexo-theme-next/scripts/tags/index.js","hash":"1f6aba7820f1fb58b61969485148db21846e1aa9","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/tags/group-pictures.js","hash":"9ed799c329abf830f623689d7e136991256a24ca","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/layout/_third-party/quicklink.njk","hash":"0efed71ed530447718c4ea5bbd5fc8695b0b0d5f","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/scripts/tags/mermaid.js","hash":"4fb01ca650fa8b256b8d48f50dc1b18350bd3d6d","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/scripts/tags/link-grid.js","hash":"18a483c2d5afd701f6080ffdddf2d1321370336c","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/scripts/tags/note.js","hash":"7b94ddb46b7d4b0fe815f2fbe4bd375f07f55363","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/pdf.js","hash":"344636b6fd7e27e8831c1e194039afc0d61931cd","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/scripts/tags/tabs.js","hash":"0eabe51da40b4b13e16419c8fe02452d9a4fef73","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/scripts/tags/label.js","hash":"8a73348186113bae0a51ea2f891c1bb882fab05a","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/scripts/tags/wavedrom.js","hash":"b44dfeeb58b41945d469141787f3dbce4b117d08","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/layout/_third-party/fancybox.njk","hash":"844559f46e2ff1c8be234d5763703106e2072a7b","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/scripts/tags/video.js","hash":"2ee926448583be8f95af1f2884ae2c9c4830151d","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/source/css/_colors.styl","hash":"3c6798c10cc220d83481cb3f3782e78558cee789","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","hash":"dadc81256afb127b77eac6763d5ee0ec9c77f0a3","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/main.styl","hash":"921a58577f411cf4eb5cfd66db0a241f8f88578c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/css/_mixins.styl","hash":"83647a6207333b9609ba90b0946b3fa9548e6381","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/source/css/.DS_Store","hash":"31813a1741cf49d75d4d2d99255403d0d3bb7935","modified":1715072325704},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/layout/_third-party/.DS_Store","hash":"10b75eb755ab25235244a5b6e64dafa853e092f5","modified":1715072325673},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1706697684330},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1706697684336},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","hash":"770d63c26f22705311028a36b52e999cc8a2da82","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","hash":"b03ba78c6916ad2f390d55bc1bc18fafb64b0ebf","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","hash":"745bd828205da7e5fbd3f860cc3697097630f5f3","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head.njk","hash":"5388b157bba4a40b9312f4a45c6678974ccf0837","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head-unique.njk","hash":"8da52a144060db1a0a088ccb2e6cc8376d1fce70","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/brand.njk","hash":"dd9c4c03e99dfde0dfb8edefcb2c933f2f560efc","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/index.njk","hash":"650de421a8ce4cf685428ffbe0087ff84cbd1356","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu-item.njk","hash":"41a8b0cc16f60fa085cb719d07216d86b6bc4bf8","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu.njk","hash":"ee6fc2f111572d3eeab0a2fecbb2d6b3e37ab26b","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","hash":"d775148c2bf20c028622af5609a788167352bf1e","modified":1706697684350},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/breadcrumb.njk","hash":"89825e75cc45e9709fa6ba89883669eedaff6f46","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/page-header.njk","hash":"7ed4f102a1825195cff8d7995bf9219f323a9034","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/sub-menu.njk","hash":"06480d8ec5f0b87eafd47f082f07968d7282dd5c","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/schedule.njk","hash":"0f4bc8e257da60f77c0c1738607b2bde55810684","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/categories.njk","hash":"17156d99941f28a225951ffdcfa9a115e20dc2d2","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/tags.njk","hash":"a18d1598e36cc72f2b0b24c3cc3c5990dfaa3254","modified":1706697684363},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-copyright.njk","hash":"bfff923526d6800218f08dba6ce0bbf5c17755fd","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-followme.njk","hash":"c1e33b4889f75acc490af3c8bde0ec56c518ff41","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-meta.njk","hash":"9fa47e4fb342811da590ee4adc91cf81118c0a39","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-related.njk","hash":"e0986db00a0201dd3c60570f964829c84ba5bc68","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-reward.njk","hash":"e8b8a7c41e9ec612d0c0c73419529d55d1c16256","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/index.njk","hash":"8f6f256ab3b351ffc80f1f3f1d9834e9a7cfac31","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-share.njk","hash":"16696990e4ce65fc8db18c4635082a5d5d06ff07","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/algolia-search.njk","hash":"efb2b6f19df02ba5ae623a1f274fff52aed21e6f","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/baidu-analytics.njk","hash":"6215309aee028dcb734452beec448c5afb6c63fc","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/localsearch.njk","hash":"661f7acae43f0be694266323320f977d84119abe","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/google-analytics.njk","hash":"d89066ff53879693f023e540d59c86137172c529","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/cloudflare.njk","hash":"a5b8297c2c383124dd6a56e256ecc0c0dcf489be","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/index.njk","hash":"f900306497b133e8b098bd9f4b96b93d1d96c185","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/microsoft-clarity.njk","hash":"9dc00fcb0a05899f048eace9f9160b78956655d5","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/matomo.njk","hash":"4e89648a8ec8194c5823064cbca39c938a799006","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/plausible.njk","hash":"ef9f2bb7110507f1c4336800af9157d5fa9765bd","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/chatra.njk","hash":"d7263fca16d0278ccf1f6aa1c6df6902a6344a09","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/growingio.njk","hash":"8afaa772c390bd9d53a5cff9645ac3168334eb98","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/umami.njk","hash":"3343750682fbd8535e50f8129be3003ad26015b4","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/tidio.njk","hash":"02aab857c27fc103216029be991688b12a73a525","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/changyan.njk","hash":"d1c950f8fbdf85e7a3eae5463767a89e858e8220","modified":1706697684357},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqusjs.njk","hash":"0749cb6902baecdfd01f779a2a2513f6d2f6a823","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/gitalk.njk","hash":"b63b7e2ede0d3e66e732fa1a06bda9b19e1e85d4","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/livere.njk","hash":"3b13b09fba84ec6000886890a6710736a2b8fafe","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/isso.njk","hash":"64cc3bdaf644fd32c0d0a247f29f5b6904da9af3","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqus.njk","hash":"9375b19a89b7fa9474e558d085af5448d4c5c50c","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/utterances.njk","hash":"5a94032bc3512a10ad4328fc19ec07b819a1d687","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/index.njk","hash":"abf37fc55aa86702118e8fdf5bf2d389dd589aa0","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/mathjax.njk","hash":"3677017fd4572b158311f5f5d870590ab25184e0","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/algolia-search.njk","hash":"24ed76e0c72a25ac152820c750a05826a706b6f4","modified":1706697684355},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/busuanzi-counter.njk","hash":"a4bc501da0f22f7e420f0ca47e83988ce90b1368","modified":1706697684356},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/localsearch.njk","hash":"e45ea3542cdc9ed7ec8447b5e6f35df4c5e82758","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/katex.njk","hash":"1ebf658690468ea197bdd0416eb7cfa4bd0b083a","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/firestore.njk","hash":"d32ebe94560fa95824478ebbff531bffc47b194d","modified":1706697684358},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/index.njk","hash":"568ddf7955d11d93fb5e842b403a7ac8b1b7fdb1","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/layout/_partials/sidebar/site-overview.njk","hash":"78a1a8cac44de7e963ab4cd51c988442eb3e789a","modified":1707031409664},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/lean-analytics.njk","hash":"2446e748cdc102c78492216319ac02148db7daf6","modified":1706697684359},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/config.js","hash":"9ec51eb61f7fee612ffc5252f489003a0fa301fc","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/highlight.js","hash":"6aec7b2c38c50989a23bfaa0d560e75c7f553e12","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/navigation.js","hash":"dd3562686d95a50375e6fd32e717ccb0d99c1e3d","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/utils.js","hash":"6853e5433e3eaa19ea43fa20b08d956ba4cec4ac","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/injects.js","hash":"d987709267a1bc6e5014411e9983d7c49c102c16","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/mermaid.njk","hash":"099e031f52fb8e47b3af5b2684737efc9e643ee7","modified":1706697684360},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/wavedrom.njk","hash":"02202bf563fb5eedde2ccad4d6c5b9109d30a703","modified":1706697684364},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/vendors.js","hash":"464db1e7182e5b9cdbd32e8b5368d5e683b1d9c7","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/common.js","hash":"19a402a225c31edffc50f202a14e0d582d3db23e","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/changyan.js","hash":"5798cfc8f63665031dd3e01debed051628cec319","modified":1706697684338},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqus.js","hash":"7f71d6b271ba65ff333d5682e7575711d368c0d2","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/default-config.js","hash":"93ee5f9109dad885dc38c49bcee630c10f9dce6e","modified":1706697684340},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/isso.js","hash":"ff8b5b5145220a17d0ecd9508ba9bd2d3b2da47d","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/pdf.njk","hash":"2c81984cc4f5123103460442f6e046f5b6c97127","modified":1706697684361},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/gitalk.js","hash":"7bb7dafdd7f6bca8464b54e17e552ce7f1714195","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqusjs.js","hash":"a600a98e7436edeb31e291abca359885567df3c9","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/utterances.js","hash":"d3bded697bc32dace689d2a6dfb6eb7514169d15","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/livere.js","hash":"5a07d8bb52bc1d51a624ca8db54be144566c306b","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Mist.styl","hash":"a1418c9dc8c0f1a0ad4ded0f4627c45bf0db1a10","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Gemini.styl","hash":"96e0a7c2a65ce68215e17e369085b2ea2f1334f2","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Muse.styl","hash":"e3be898f5ebcf435a26542653a9297ff2c71aeb0","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Pisces.styl","hash":"48f4f277946a168d0db1ea02804e85c22ca2c7db","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_variables/base.styl","hash":"c4fc4e862d09221265ab1466085f057be2ad2e4d","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/source/js/third-party/.DS_Store","hash":"b7a4a8e8ee3bbf1c3d47bc7f5afa02917522698e","modified":1715072325672},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","hash":"ba7ba2c129d1f240c6a22cec3e53f3f22af64b6b","modified":1706697684347},{"_id":"node_modules/hexo-theme-next/source/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/css/_common/.DS_Store","hash":"cba17e35154c352959c43a31374ed836c96990a4","modified":1715072325721},{"_id":"node_modules/hexo-theme-next/source/js/third-party/addtoany.js","hash":"5276c8f78ee562a8965216dc67d762e59cb4a9f2","modified":1706697684332},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/back-to-top.styl","hash":"7664491542046df9a3887cf40a06e00c0b4086a9","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/js/third-party/fancybox.js","hash":"819f382c561fe5ec23c67cc5fabd63dd1cc22dc1","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/index.styl","hash":"2298e521253b3bf376a2412271bc2a7d305051f3","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/reading-progress.styl","hash":"90a86045a33c1bae49fc2f6fa1e1b53170c7f77b","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/index.styl","hash":"8e34df131830d4fa3725e4590a672ba1cf1903e5","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/mobile.styl","hash":"1dbf2c339adcd27026c3a2ded32ee91ce08cea26","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/base.styl","hash":"d0a7c99095f490b0d2ed6b1be43d435960798cec","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/buttons.styl","hash":"a042571d85ff7265f799004239a45f36b716b8a6","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/comments.styl","hash":"e4fecc889ba3317a64e9abba5842c79dff9b7827","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/index.styl","hash":"523fb7b653b87ae37fc91fc8813e4ffad87b0d7e","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tables.styl","hash":"e840b23d33023e6d45e018f6e84b683dd56efd8d","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/pagination.styl","hash":"f4228c759db4a650c8d38745c2edd1dc83c45687","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/normalize.styl","hash":"b56367ea676ea8e8783ea89cd4ab150c7da7a060","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Gemini/index.styl","hash":"9dfe853c901bdc52fc950bacdf15484dbb9bf140","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_header.styl","hash":"dafc6d23c80d6fe3e55a7711e94210d2479b629a","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/toggles.styl","hash":"782ee1fc5e669d3ddbfeb82b73ad7fe561f1a4fb","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_menu.styl","hash":"fb550935d374e0bdf1097fce187337dc05cad3e1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_posts-expand.styl","hash":"485d23ccb42c0d0c8ead7ea8930dd3e06d79a285","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_layout.styl","hash":"fa4fd8f76464e214fb7318f325b13c2b62f4b478","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/index.styl","hash":"ab16a3dcdc0393b9b582ef59dcc13db9320e917c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_header.styl","hash":"3fbfab591f280e2e7f3b0265901c93bc4bd137ed","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_menu.styl","hash":"82cda756f5b7092df2eee6641b9786df71623bdb","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_layout.styl","hash":"6569a6640f79d247a8235b3914772c0e2f99ead2","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sidebar.styl","hash":"547c0b5cd5e7ea10d21863d13a6b16579a49396c","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sub-menu.styl","hash":"c48ccd8d6651fe1a01faff8f01179456d39ba9b1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_header.styl","hash":"ac2dc0ce9c775a83ef7132ae957b54539366ac9c","modified":1706697684365},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_menu.styl","hash":"72dc825c50357402c342d62ab60fc0c478ab6bc1","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sidebar.styl","hash":"91dbf3ca5c3a613d4e30618c120da535bf2d0336","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_layout.styl","hash":"26a0cba1eee5de45a45a5e14e17707f905390512","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"778ed2ad5643b93970c95626b325defeb586733f","modified":1706697684366},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1706697684335},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/index.styl","hash":"8000075b227749a7495eaf417cac6ccfbe441580","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","hash":"def07bcc7c17d8a0caad177fb1dd2f3a5e5b3536","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1706697684339},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1706697684349},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1706697684343},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","hash":"da361917d65e5dca8362f8cdeb6c8cc0e8316cec","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","hash":"1e826dea3f684c0515f362dc1352447a1f0eae71","modified":1706697684341},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1706697684344},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/.DS_Store","hash":"7dc4c0e066c7f2249dc291afac6065e647136cc8","modified":1715072325686},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","hash":"4536cb6d0a9bbaaa86fab3fa0101f6a3a3ec5a76","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","hash":"fdb7b7cef1a147d897e7f7cd8903b58368ec2062","modified":1706697684333},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","hash":"6e0682bb42170d61b13b786295f45f9c785f8b73","modified":1706697684342},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","hash":"835cbf54c49ef1327f47df70ff2636ad36b6f57d","modified":1706697684345},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","hash":"6bf821310342c5b87a631873e7650a475a0765f1","modified":1706697684346},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/wavedrom.js","hash":"40dcd10df6edf124088c329346e0cc0bdac74ef1","modified":1706697684351},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1706697684348},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/index.styl","hash":"54d12e2c5d9982f7b9e5b23be5133954a8514e9d","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/gitalk.styl","hash":"8f094c4ac17e2ab45569b12d157747f9c7333c12","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/math.styl","hash":"9d995eb4871a6c273d9d51558676a1fdabf69e72","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/disqusjs.styl","hash":"877a537d5b95beb048142e4fdee6f17e6ef9c7bb","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/search.styl","hash":"e72799ce3f9b79753e365b2f8c8ef6c310668d4a","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/utterances.styl","hash":"56d90ae0559caa55b75f3c300ff2711f9ed65fc4","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-body.styl","hash":"56d5b7ff73f466c9ae54f7204ae899281295d749","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/footer/index.styl","hash":"4e967702cf4c637132346bc74ec8854426f1a68c","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-followme.styl","hash":"1ecfd64507954810b07a9d21fb5305b5378feda0","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/index.styl","hash":"098d4bd034e986fcf7e443eac4fc2193935461b7","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-collapse.styl","hash":"7369928305330c73ae0b3f063a681a8384d8fde4","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-nav.styl","hash":"9ac6f477177264c26a46e8333b8456720a0444dc","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-header.styl","hash":"1191f1bfa5c43e54be8e5b3cc0d802984e161747","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-gallery.styl","hash":"aa366d37389760c8595529b850f461569577a1c5","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-reward.styl","hash":"04cf4a69537fc14d3b8904f965d283356853847f","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-widgets.styl","hash":"ebfba158a0a4af3d1dabcacbc58986664de52140","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-footer.styl","hash":"11497388f124bfbb4001495a67d3629a9f618405","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/bookmark.styl","hash":"e74f4bb47a101b014ee2a1783c87f3b87323f9a0","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/index.styl","hash":"6e0d0796ef7fbbb62ffdfb448753a850de82c74f","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/index.styl","hash":"da5e88f8debd5ac8d7af5c6ba6240df66104955f","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/menu.styl","hash":"bbbc40b03cb299d2a6a568f329b2ce98e1cdc430","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-meta.styl","hash":"a851e9d5aefcd027c95eeb323860b6da70f202d1","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-nav.styl","hash":"bf3ad8b4268f763a1e26377681644887694bc009","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/related-posts.styl","hash":"b05908f04ef95f2d91e6eba89b12411c378d050f","modified":1706697684373},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"0847400d8579b0a2dd1bf662c78954c10adf2680","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"c6a27beb3f741211a14576026f3b4cfc44cc6407","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"5b38ac4a0f1ade0e681aff0e3366c481d9cf3dcd","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/github-banner.styl","hash":"38c64c2d04e46848382bfa246a0e9c508294767b","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"46eece42510c2c89bb9209afb0262ad76a4b0b36","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"c2e354a565c8c1b32bd0ceacc972b17982758b67","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"24752d145c6fb8f5344dca9c7b9640839c02e009","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/index.styl","hash":"138f78147bc6bd6005f329ada34dc79b7625542d","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"f634f94828620e88c3f5a8db56f7944f6ba232b0","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"9a7c71560fbdc936ad4e736fe15063ea3e8a644b","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/site-state.styl","hash":"26dd0adfcb1db6df29c6090c8d7e9b5a43583fb0","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"d6418fd2bbfba7b73ddf11ec62db9637fdf5d8af","modified":1706697684367},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/index.styl","hash":"22cd37bd5df9972d5074710896aba4424ad5161c","modified":1706697684370},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/note.styl","hash":"98d4c20aff0f0fcfe1824017fb06ab21ef0d218e","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/mermaid.styl","hash":"48d35dba575a7c9e8845b16652e76b7d4a4646de","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/label.styl","hash":"debee14539272fbe3835a7d3853af2230baa3501","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/link-grid.styl","hash":"7f8a7345e6537a62cd9e9a94c8f7065b541d9b04","modified":1706697684371},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/tabs.styl","hash":"33dd6ad015dde65fd46f34961655442e8e82b52e","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/wavedrom.styl","hash":"af113411ad9cca7674177be36af8dd399680834d","modified":1706697684375},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/categories.styl","hash":"b6e2eb1550a7845cb2adf86081a4ab6c7bde1e68","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/pdf.styl","hash":"b6654a1d7cf82577d8263faffee8af3ad4a5c0e8","modified":1706697684372},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/index.styl","hash":"7504dbc5c70262b048143b2c37d2b5aa2809afa2","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/breadcrumb.styl","hash":"8afdc311c6b8db121758371f95cf1c5e77354f42","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"393ff96234e4196b569d4b11496774eb78e147de","modified":1706697684369},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/fold.styl","hash":"42a0b65491ad85438596b3fe0b7f23973e4cef34","modified":1706697684368},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/tag-cloud.styl","hash":"1a81d1a71fcf0699629ce6e72dfd0a15f3a2dd0a","modified":1706697684374},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/schedule.styl","hash":"6b816c2511242ee503fb5f34cd3e4dcdafc06b85","modified":1706697684374},{"_id":"source/categories/index.md","hash":"f5c920fbc09ea3d8edf250de7e31bcc6b3e765ae","modified":1706698717077},{"_id":"source/_posts/.DS_Store","hash":"64b77a6624f7998659cbfb305adbeb5f65f1c8e0","modified":1722515387830},{"_id":"source/.DS_Store","hash":"d70e8824da9b4c9a698ecce264010b16c4f2525a","modified":1722515387827},{"_id":"source/_data/styles.styl","hash":"f4bb55ef0972c829e3382d1bae1786b3ab5d54ef","modified":1707045638288},{"_id":"source/about/index.md","hash":"9294d008cc673abc2eaf740f101ebac560029267","modified":1706698701349},{"_id":"source/images/qrcode.jpg","hash":"eab6c8893d0c79cf705cf64d738f560563365b0c","modified":1707548301740},{"_id":"source/images/.DS_Store","hash":"e08e12a8507734ef2fe42991af2aa57b0860cc60","modified":1715072325720},{"_id":"source/CNAME","hash":"07ad8fed7f1b810bf3715a0e68324e5cbb1d47ac","modified":1706872557451},{"_id":"source/tags/index.md","hash":"e995ed2b8452b1906600b3853b920f13423098b7","modified":1706698644396},{"_id":"source/_posts/cs/.DS_Store","hash":"8baeef22be545a5c762292db95c304f1314cbdd0","modified":1722515387831},{"_id":"source/_posts/cs/nlp/.DS_Store","hash":"11c60a72a12adca064ea0cc364bb7bdc96fe6888","modified":1722515387831},{"_id":"source/images/cover.png","hash":"2f2aa6173619dd38425673ba110b50b9156d4d10","modified":1710684380714},{"_id":"source/images/avatar/.DS_Store","hash":"c3fa37607ceb3f7ba411cf4203d2a333f773d921","modified":1707118756919},{"_id":"source/images/avatar/20180303210737_XsJVr.jpeg","hash":"2ebbbcead4446a69b0ac0698ab227a1af49735ce","modified":1707030615190},{"_id":"source/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","hash":"3e7db51e59a8261de04f55814475556494d153f6","modified":1707048498957},{"_id":"source/images/background/.DS_Store","hash":"88f5d31d0db89adcf679f2a7fefc8947139a1c1f","modified":1709026807046},{"_id":"source/images/favicon/.DS_Store","hash":"83ddccadffca5384db3dfc167728b7c7cacd9a87","modified":1707796842439},{"_id":"source/images/favicon/favicon_io/apple-touch-icon.png","hash":"5e36b0bfe67bcf96f4f0e92366a5de509fe9a78a","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/android-chrome-192x192.png","hash":"2164889376f2bd6f9dd2066ebdfe4abd547f49d9","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/about.txt","hash":"d1f084751c81e98b1675b8c0745f6f306a1eaf66","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/android-chrome-512x512.png","hash":"419f0b0317ad2a31acfa7dc19e1502731e92e05f","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/favicon-16x16.png","hash":"fb63ee2fc88b9ce42dec5bf68088da14be9350f5","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/favicon-32x32.png","hash":"f42c950146a171afca4ad70ff425b5b4ff02f926","modified":1706844814000},{"_id":"source/_posts/cs/nlp/2024/.DS_Store","hash":"194744d905e9bd7812ba9934571ba189542d56d2","modified":1723555308918},{"_id":"source/images/favicon/favicon_io/site.webmanifest","hash":"bf31baf91bdd2fcde24a45e3f2a1be33733c6f69","modified":1706844814000},{"_id":"source/images/favicon/favicon_io/favicon.ico","hash":"fceaa7158ae89a98fe5b93152dd7dec2d36c1ff4","modified":1706844814000},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE.md","hash":"dbe2c4a5a96fe27437a53c4fa9054bb19e05f28a","modified":1712299467271},{"_id":"source/_posts/cs/nlp/2024/02/LLM.md","hash":"dd73e47b1cdb864f26d5780ac4fe08603bcc9b3c","modified":1710314618942},{"_id":"source/_posts/cs/nlp/2024/02/.DS_Store","hash":"ab55fc2d73f2238323a1a5d7b8362d5620868275","modified":1715072325702},{"_id":"source/_posts/cs/nlp/2024/06/RoPE.md","hash":"e95970adeaae06307b574bc40ebb9f41df24e05e","modified":1719370756930},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM.md","hash":"30dfb09aaa495ff30731cdc6d9d1b9a9afcfa7d6","modified":1719202196529},{"_id":"source/_posts/cs/nlp/2024/06/GLM4.md","hash":"bba92579b8bc235c1d5a43514d8841cfdec9bdb7","modified":1719496075409},{"_id":"source/_posts/cs/nlp/2024/06/loss.md","hash":"3250729eaff9712b7a659248c72e7735f60a4060","modified":1718529415156},{"_id":"source/_posts/cs/nlp/2024/06/LLM.md","hash":"fdf118d5039194c8e5ca193612ccd6b3fa0df0b3","modified":1718717773750},{"_id":"source/_posts/cs/nlp/2024/06/-7.md","hash":"78da9544c79d8aa1e4e8f1f9d0e315c405650917","modified":1718202061594},{"_id":"source/_posts/cs/nlp/2024/06/-IPO.md","hash":"83e10e034aefd8cd0cff53397ee2e57b005d252d","modified":1717685520000},{"_id":"source/_posts/cs/nlp/2024/06/10wJetMoE.md","hash":"1d269c5349c126573508a86948dac1b422dfa95a","modified":1719406155240},{"_id":"source/_posts/cs/nlp/2024/06/-MEDUSA.md","hash":"da0dfbe26455cbd782985690ed8503cb54ad40d9","modified":1718439564061},{"_id":"source/_posts/cs/nlp/2024/06/.DS_Store","hash":"3b775d27f5eb483cfd4fa8fd6a74575a3be42684","modified":1721047354610},{"_id":"source/_posts/cs/nlp/2024/06/SkyworkMoE.md","hash":"63e30d0920f45051f9532e8bb37c608f029117f6","modified":1717591310292},{"_id":"source/_posts/cs/nlp/2024/03/Yi-.md","hash":"1871881f3afdaf9b2930d31a03d62079ca4ff9db","modified":1711713217115},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization.md","hash":"39dbb9473a0afe9654512f34be923e0210d5e1a0","modified":1712471774847},{"_id":"source/_posts/cs/nlp/2024/03/-1.md","hash":"f25140aae947e215b4bf597973bf28a52bce575a","modified":1710685009511},{"_id":"source/_posts/cs/nlp/2024/03/.DS_Store","hash":"5180229bc1298f43722a62eb24dde7596c686a72","modified":1721047346901},{"_id":"source/_posts/cs/nlp/2024/03/-2.md","hash":"d8b936ccac17c2d991f3894335231adb87aaabc9","modified":1711253769176},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention.md","hash":"31d3d5cbbde092ba1a855319647b24e32ffda8ef","modified":1710934710908},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT.md","hash":"9af08ff8d7274b005502eba5fb0461ea1a0729d9","modified":1710646109054},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA.md","hash":"bdd2ea9b7abe82b13ace07dff7be5b2a757db7f2","modified":1714231402122},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO.md","hash":"77ba73314fb67af0807703ad0dc91db8724b84f3","modified":1717158416345},{"_id":"source/_posts/cs/nlp/2024/05/.DS_Store","hash":"d1fce27ff8039364783ceda57a44fffc51861cac","modified":1721047354605},{"_id":"source/_posts/cs/nlp/2024/05/-.md","hash":"5a20cdecb741dac615b82bf8f603fa13b2136d70","modified":1716608290437},{"_id":"source/_posts/cs/nlp/2024/05/-DPO.md","hash":"7c876990c33a7d7d43fe3e58ed0972d85cd74611","modified":1716985993225},{"_id":"source/_posts/cs/nlp/2024/05/-simPO.md","hash":"39da16332aee228e964f83135764b8857bc81b04","modified":1717300939947},{"_id":"source/_posts/cs/nlp/2024/05/-5.md","hash":"ca99e9196eaf8743885528f6e6d6404bd8cf34ee","modified":1715323819009},{"_id":"source/_posts/cs/nlp/2024/05/-.md","hash":"9a46f37e067d004004a3e36f4f4d7cee608ad22b","modified":1715591178036},{"_id":"source/_posts/cs/nlp/2024/04/normalization-.md","hash":"0855c9b52c67a72d809c3f53240f4b62bb99de79","modified":1715323869591},{"_id":"source/_posts/cs/nlp/2024/05/.md","hash":"dc51b82a33b114c119bf88dab64733912bf050f2","modified":1715323820731},{"_id":"source/_posts/cs/nlp/2024/04/.DS_Store","hash":"327e321d722e88824dd22a5f789f1eaa20bc42e9","modified":1715351302735},{"_id":"source/_posts/cs/nlp/2024/05/-6.md","hash":"b76808355cc79f0f22debf8717a9002404c4e988","modified":1715686009973},{"_id":"source/_posts/cs/nlp/2024/04/-4.md","hash":"419c28f934cfe340a4ca4751668fe1512a26ab57","modified":1715323846826},{"_id":"source/_posts/cs/nlp/2024/04/-3.md","hash":"b56b2454ac63f79df92591824dda52efb8084e00","modified":1715323861150},{"_id":"source/_posts/cs/nlp/2024/07/.DS_Store","hash":"b3a019908130d02c3f1fde00fe04f1bdeb8aee59","modified":1722515392464},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2.md","hash":"f8bd8dcbaf07c30a2dbcad152eb0ecca098bc54b","modified":1719923108361},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA.md","hash":"76f6fd72f65579b0557fc59d4d2b1355e847a59f","modified":1720849467012},{"_id":"source/_posts/cs/nlp/2024/07/Llama-3-1-.md","hash":"bf28f4b78d7006cff47d40faa945f6e8c62e8078","modified":1721918543512},{"_id":"source/_posts/cs/nlp/2024/07/Llama3-1-post-training.md","hash":"91e84bdb36d8f5a7d4baf9c050d28d780bbbc41d","modified":1722083812204},{"_id":"source/_posts/cs/nlp/2024/07/Qwen2.md","hash":"7245961648ffbf3294edf6830dd2e991353411ec","modified":1721226468841},{"_id":"source/_posts/cs/nlp/2024/07/MoE-expert-choice-routing.md","hash":"c77f55009e633de6dae823a50e54dadf2eba18e1","modified":1721659565168},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32.md","hash":"0ebb6570e9b88b18a68a53ff6ca1803eedfaf76d","modified":1721305367447},{"_id":"source/_posts/cs/nlp/2024/07/bilibiliindex-1-9B.md","hash":"9296bc590805bb2f9d5c9ce3ebbd4a535e5f8519","modified":1720180297531},{"_id":"source/_posts/cs/nlp/2024/07/upcycling.md","hash":"81db23369463ba3b948f66a4e415e04533353fcb","modified":1721398910417},{"_id":"source/_posts/cs/nlp/2024/07/model-soup.md","hash":"725347833bfaf76115d9b7aa2ea0e059d5a5047a","modified":1722343896203},{"_id":"source/_posts/cs/nlp/2024/07/routing.md","hash":"0240dc6fbe304f31cc4fc52dd27229b6c710f1b6","modified":1721048591099},{"_id":"source/_posts/cs/nlp/2024/07/-8.md","hash":"944a6114158104da9ed47fa9144d7f53d5c840a9","modified":1721307558544},{"_id":"source/_posts/cs/nlp/2024/07/MoE.md","hash":"cabd7d8725ec7a6591acc3e70102c5faa1328e7e","modified":1721133197731},{"_id":"source/_posts/cs/nlp/2024/07/AFM.md","hash":"b7e5d0d9c01c4d883fdf35470a697bd5256d7435","modified":1722516594506},{"_id":"source/_posts/cs/nlp/2024/08/.md","hash":"5f922078474416499cc0a1ea0bb242687dce2ebc","modified":1723898753497},{"_id":"source/_posts/cs/nlp/2024/07/-.md","hash":"4593c86613769507e77b607950169c31014bfcaa","modified":1721830820308},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM.md","hash":"fcbece9127a6da095d1aaece9f754455455fd591","modified":1720006800701},{"_id":"source/_posts/cs/nlp/2024/08/phi.md","hash":"cde4ef809e28ccd6044e7a43ffc3a96c785284dd","modified":1723555985983},{"_id":"source/_posts/cs/nlp/2024/08/.DS_Store","hash":"7668cf86a77010e1c0ca9c9d21e5722b469c0720","modified":1723833723652},{"_id":"source/_posts/cs/nlp/2024/08/-9.md","hash":"799ccea86128153c93db68782a3b4bab3ab125b4","modified":1722953110936},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM.md","hash":"9e5d561b4009abfccbfe3130472f0c4adaa552bc","modified":1722611339571},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/.DS_Store","hash":"5353920a8962599ceb60f96de4bf901e325f55de","modified":1719106582974},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/complex_number.png","hash":"255aa16a44913d53784b09d0a08762df6d21dc16","modified":1708758408339},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/.DS_Store","hash":"2e33b8d145af72ec31cbad8c19fc2528fc2a909f","modified":1709014663934},{"_id":"source/_posts/cs/nlp/2024/02/LLM/mix_precision_fp16.png","hash":"4da22fa80039f8271ffd839aba70f6b1aca7afe4","modified":1709188879237},{"_id":"source/_posts/cs/nlp/2024/06/RoPE/1.png","hash":"cb86a4ce4589b590aa1b220dbdf32f9db559a1b8","modified":1719315883922},{"_id":"source/_posts/cs/nlp/2024/06/RoPE/3.png","hash":"8bff531083b17baa6de7d44ecdf464c29d175a31","modified":1719316336855},{"_id":"source/_posts/cs/nlp/2024/06/RoPE/5.png","hash":"982701399546708e62d974938e35f17a6aa5abb5","modified":1719317449536},{"_id":"source/_posts/cs/nlp/2024/06/RoPE/2.png","hash":"8c8c02a78f8e874162574caa267c32f469665110","modified":1719316049637},{"_id":"source/_posts/cs/nlp/2024/06/RoPE/4.png","hash":"491537b72fcab5676e6322c47d9ae7e1054f3387","modified":1719317189348},{"_id":"source/_posts/cs/nlp/2024/06/GLM4/.DS_Store","hash":"23e2dd4419a8ea0d2c9126a1006e1fb0b7c824a7","modified":1719728736989},{"_id":"source/_posts/cs/nlp/2024/06/-MEDUSA/.DS_Store","hash":"f19c5b5f3d05fd26a1348a12500b67efda4b3802","modified":1718367083334},{"_id":"source/_posts/cs/nlp/2024/06/LLM/.DS_Store","hash":"3c56377d253e8212100ccda316b258f134ef0f2c","modified":1718770249764},{"_id":"source/_posts/cs/nlp/2024/06/loss/.DS_Store","hash":"048e5b4d3acc0214cfb4641afefad1f2c57af2b1","modified":1718528197082},{"_id":"source/_posts/cs/nlp/2024/06/-IPO/.DS_Store","hash":"714de020e255973703c8f95f272f7aea6b76cdb1","modified":1717420106364},{"_id":"source/_posts/cs/nlp/2024/06/SkyworkMoE/.DS_Store","hash":"ae0d2ae0ce4e9a23c90d2154831259d19e4e1685","modified":1717593293091},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/.DS_Store","hash":"f036d46924a246fd06315b601bca6cc759d95300","modified":1710600789234},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/dilated_conv.png","hash":"bbc2ff2e9f891da4bfaf6d535ab8545acc18e8a6","modified":1710560488146},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_large_performance.jpeg","hash":"54e3ed874802ac9465580d6b5fcc5d6c1de96244","modified":1710250364698},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/receptive_field_cnn.png","hash":"46515aa3bce1eb0fc244f62ceee7b899c28183e8","modified":1710321816411},{"_id":"source/_posts/cs/nlp/2024/06/10wJetMoE/.DS_Store","hash":"cbd780c51ded1e8b39446982c4ef3e61a1f958b8","modified":1719431697095},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1711247018739},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_and_ln.png","hash":"ff10e75b9d2c95aa713f013b104ed307f2121e57","modified":1711118594120},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_ln_gn_in.png","hash":"9783c818f5e0eaea33d169718476bbe8874cf945","modified":1711120826525},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/lossfunc_surface.jpeg","hash":"c78a8df335da0d507963fb73a62fe2c3d145c91f","modified":1711005649925},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/rmsnorm.png","hash":"55bbcb42145011f7b5adf90cc613e22e2b94f060","modified":1711165884464},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/realformer_attention.png","hash":"e9a92e5c07c8ca6873ea70671ad54eb2f1a13332","modified":1711206279560},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/.DS_Store","hash":"5d08a407b858db4f8a5bad5168cbb23224622856","modified":1714982537175},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/.DS_Store","hash":"81b22bc167ab6b1514c4698d3c49fcf0836924dc","modified":1710670459475},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/.DS_Store","hash":"aa1bfdd048237db946dfb9288953b6fe1dcc6f18","modified":1720793313492},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/MQA.webp","hash":"456a8ab19cc1564912034c375e8c3c5a42be6837","modified":1709973970557},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/attention_calculation.png","hash":"1f020c39c78221e41c5c6953ef97239e9f42aa3c","modified":1709780575011},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/decoder.png","hash":"28ee3d1ab68bd325ecb9d2066bc264a63d7de081","modified":1709716894560},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/cnn_heatmap.png","hash":"cb0bde73c9c4d0646133947ebaab16c44c753667","modified":1709723125449},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/encoder.png","hash":"d6a3a39c420d90e50f02f8b34f127bfe34177331","modified":1709716888116},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/seq2seq.png","hash":"9baa57cc8000a918d0adca6dceaac3ea54791ea8","modified":1709716876496},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/seq2seq_attention.png","hash":"b95046eee028b45dd9734639ecde8189e93b2374","modified":1709781776387},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/softmax.png","hash":"de80ba20e55abf7457cac958aa87627d0a7e5d77","modified":1709821278308},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/.DS_Store","hash":"cee7f0dd4ed83bbcfac552e3b170580975b1bf69","modified":1717420098368},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/contingency_table.png","hash":"94b25e2d4803d9802d3c5455aed84911fe506089","modified":1717233039090},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/reward_accuracy_compare.png","hash":"0b5526d61a1cbb3188e8e53ea858b1d9d1953660","modified":1717259353701},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/.DS_Store","hash":"ef25e9af6860a6c162c9290b57f27b77606c64e9","modified":1717053157303},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/.DS_Store","hash":"f40652099a252049ed00339c05066bc05635ded0","modified":1714093902405},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/qwen1.5_moe_tps.png","hash":"5478a6583a6c6fb68f1bc9429c103e84fe39efaf","modified":1713691310250},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/softplus.png","hash":"bdc66c39227441390f2241b4f26c0b1fbab331d9","modified":1713279943448},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_round_error.png","hash":"0172ba008837b3490a3e456306aa72be65636d90","modified":1713862612528},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/xiaomi_moe.jpg","hash":"d898ba33f1ee70efa136dbff3cb38983b461524f","modified":1711814228139},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/.DS_Store","hash":"ff49d0cafe604c7a0b14ed0b6d60dfd2cb5edac7","modified":1717219419238},{"_id":"source/_posts/cs/nlp/2024/05/-5/.DS_Store","hash":"b4bd95f9044124fd6715a9d1baf9ee4858fdc14b","modified":1714914202734},{"_id":"source/_posts/cs/nlp/2024/05/-/.DS_Store","hash":"816910ea2f357470b3d5c3908437853f6d744a13","modified":1716541217062},{"_id":"source/_posts/cs/nlp/2024/05/-5/yarn.png","hash":"ee124a0823b429842082acebe78a7162915cc11c","modified":1714809224728},{"_id":"source/_posts/cs/nlp/2024/05/-/formula.png","hash":"65fe200098b51d1712b6c38d039aa8be22d38e82","modified":1716453725490},{"_id":"source/_posts/cs/nlp/2024/05//.DS_Store","hash":"a1143c0efa86d8f8e5cb1ddbbb0932d0adb854d9","modified":1715070961565},{"_id":"source/_posts/cs/nlp/2024/05/-/.DS_Store","hash":"24df341533adcab1c2da9cc93f748f972ed4e580","modified":1715591105150},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_attention_entropy.png","hash":"be91b6a49cfe30dd51ed4f8eb258eb4715a70e37","modified":1715176570143},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_attention_logits_explode.png","hash":"3cc54ee973126c1ca3bcd85d75039e490efc9acf","modified":1715175378314},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_middle_k.png","hash":"869d4b687714409a5a4b89c85dc5ee2c1f0c2c86","modified":1715240601309},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_starting_tokens_num.png","hash":"6a2c841a3d3fd354f57757aa7e663e83585545d6","modified":1715180059874},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/bn_and_ln.png","hash":"ff10e75b9d2c95aa713f013b104ed307f2121e57","modified":1711118594120},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/.DS_Store","hash":"96243fa0e625d8d7395157516c6299723b4ce769","modified":1712575638552},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/.DS_Store","hash":"b724fee0b4a69b9549b3440b22bad1b2cbcf3d3b","modified":1719911618408},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/ablation_4.png","hash":"f358308ba8c262ce443af4bcbd7643d32c729a6d","modified":1719838921927},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/ablation_5.png","hash":"edf1a94db2e64c8b0122a8a3d2e2041784084b64","modified":1719838962919},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/.DS_Store","hash":"146ec6b49905d08749c4c907cabb041296f70ea3","modified":1720848485298},{"_id":"source/_posts/cs/nlp/2024/07/Llama3-1-post-training/.DS_Store","hash":"b367e4c59ad35e4074ef5e081574e46d91eaa743","modified":1722343209238},{"_id":"source/_posts/cs/nlp/2024/07/Llama-3-1-/.DS_Store","hash":"7352ad06d08b8281c5d98a48c40cd58dc16af718","modified":1721945817126},{"_id":"source/_posts/cs/nlp/2024/07/MoE-expert-choice-routing/.DS_Store","hash":"cf4e23d5e5922bc783ce597662cff7dcade31af3","modified":1721744838714},{"_id":"source/_posts/cs/nlp/2024/07/MoE-expert-choice-routing/cf.png","hash":"950b15a49298c1add50dcb13af2ad862dc895733","modified":1721659241036},{"_id":"source/_posts/cs/nlp/2024/07/MoE-expert-choice-routing/dense.png","hash":"90a2d4c58443fbc6fe920ba25db4d4a5aba9e1aa","modified":1721659512294},{"_id":"source/_posts/cs/nlp/2024/07/MoE-expert-choice-routing/dist.png","hash":"26b518e55aede62e9af5b91847ee2a0aa5e3c7d1","modified":1721659021963},{"_id":"source/_posts/cs/nlp/2024/07/Qwen2/.DS_Store","hash":"f248928be4b1933fabaa1812d1be09585e835030","modified":1721305168216},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/.DS_Store","hash":"679205d32b361b0e4874162e1c40ac701589d0ae","modified":1720155480047},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/scalability.png","hash":"50158badd662c759906125f6ed46753ffe4b846e","modified":1720098814924},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/train_hp.png","hash":"c81fbe8950dfafca0d814ed77c4a7d55cee935fd","modified":1720098891600},{"_id":"source/_posts/cs/nlp/2024/07/model-soup/.DS_Store","hash":"63b7134a8fb4ca2243282107480c19de3ecac0a7","modified":1722343674619},{"_id":"source/_posts/cs/nlp/2024/07/routing/diff_layer.png","hash":"21c3838f4b9efb0dbd14d5cf853061ab5f65f27b","modified":1721047569669},{"_id":"source/_posts/cs/nlp/2024/07/routing/.DS_Store","hash":"20c4b1c3227fe4fe513192368ee85d68785220ed","modified":1721132480755},{"_id":"source/_posts/cs/nlp/2024/07/bilibiliindex-1-9B/.DS_Store","hash":"f1e47ee7ff5b3d2413d55b727296072ed2f7c130","modified":1720433076051},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/.DS_Store","hash":"93811700a37bb27ac2527cac9b1add7071bd5653","modified":1721499385998},{"_id":"source/_posts/cs/nlp/2024/07/MoE/.DS_Store","hash":"839a595f8fa402800499dddebfca661fd8158e0f","modified":1721132785211},{"_id":"source/_posts/cs/nlp/2024/07/MoE/t2.png","hash":"901faf24fd46677982d4b6191fc05a2a1b39740f","modified":1721132590713},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/.DS_Store","hash":"605b1ffed2f8cbcd69799ff36ecc6efd4bfc87bd","modified":1720006114390},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/formula.png","hash":"7a6299ae119624a92b0c2a198631673024924952","modified":1720006103636},{"_id":"source/_posts/cs/nlp/2024/07/AFM/.DS_Store","hash":"afb0a5cfe42b9831bb38b0f935f2e4d9e44e1003","modified":1722553472562},{"_id":"source/_posts/cs/nlp/2024/07/-/.DS_Store","hash":"ae3ecd86f3d018bf86be8e4b592e1cb32d104ec2","modified":1721830161838},{"_id":"source/_posts/cs/nlp/2024/07/-/base_freq.png","hash":"9b4dd13f3dd6152011b2db58824826fb5d70ea82","modified":1721824799902},{"_id":"source/_posts/cs/nlp/2024/08//.DS_Store","hash":"408dd758c650a3470c32078dab8a63b010af2da2","modified":1723835714712},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/.DS_Store","hash":"6623045968d812c6cf4d9b06be60eb381bca3b5e","modified":1722611326784},{"_id":"source/_posts/cs/nlp/2024/08/phi/.DS_Store","hash":"f399a5b4804ceb337dc65767d824a68164f4728b","modified":1723598272635},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_2_0.png","hash":"6e820cf29184fb7cbe780be7058df8a5f23774b0","modified":1723555951103},{"_id":"source/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","hash":"0629f15db217c8b7037a5b46ef187d79b9a66a09","modified":1707048511782},{"_id":"source/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","hash":"6a3863489ed0982c7fe19d575eae28ba8c286ccf","modified":1707048415396},{"_id":"source/_posts/cs/nlp/2024/03/MoE-.md","hash":"9769c51d515f2aee1cd86930faf7ddb9cc80f525","modified":1715323896425},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/2_stage.png","hash":"ca7a6d52aa5dbf3afa1e00de6548386ad0ea738b","modified":1718955041160},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/batch_size_2.png","hash":"d2f8c192c90acd0ccba2ec814bd36fc2e646e6af","modified":1718800118846},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/cos_lr.png","hash":"951b0fc36a8b0e7b9c58a39345950546f090fb3a","modified":1718801929626},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/remote_attenuation.png","hash":"90aa3d8f76c9b5c18de8b7740649b499241cab0c","modified":1708957811757},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi_nosft.png","hash":"d3b46ed19e458973c3586f3bf2ef906deb4080a8","modified":1709199016415},{"_id":"source/_posts/cs/nlp/2024/02/LLM/rope_matrix.png","hash":"84372b91204098bab2ff09d2d865ad869f93bdbd","modified":1709262766062},{"_id":"source/_posts/cs/nlp/2024/06/-MEDUSA/speed.png","hash":"402da175e2e365db3cc0e6c377b89df5b8615b6c","modified":1718362366610},{"_id":"source/_posts/cs/nlp/2024/06/LLM/ditto_2.png","hash":"abd8e2173d83ff20cc0adc636163a97f7fc46de2","modified":1718707274921},{"_id":"source/_posts/cs/nlp/2024/06/LLM/ditto_1.png","hash":"bfe3f56dee05d35b5b816a356ec7688d81c33b92","modified":1718706809378},{"_id":"source/_posts/cs/nlp/2024/06/loss/eng_data.png","hash":"3d16e16eb4a57436ff9d2c0b64e7042944dbfab8","modified":1718524164949},{"_id":"source/_posts/cs/nlp/2024/06/loss/exp1_param.png","hash":"5f940193d379f18bb36ac38f2bfa5d1aaef65b15","modified":1718508399845},{"_id":"source/_posts/cs/nlp/2024/06/LLM/5.png","hash":"8fa2b214c490a5262951301bbd4d0510d5199b3c","modified":1718713696291},{"_id":"source/_posts/cs/nlp/2024/06/-7/lora.png","hash":"c044aa39cff915b64ee576c558756cb1093e5075","modified":1718201952288},{"_id":"source/_posts/cs/nlp/2024/06/-IPO/curve.png","hash":"f81e68f050653f66ca087c2f13a222aca426384a","modified":1717420081841},{"_id":"source/_posts/cs/nlp/2024/06/SkyworkMoE/lr_exp.png","hash":"32b3dda778f6dd42388ec8ad6f8782738068f54f","modified":1717576006119},{"_id":"source/_posts/cs/nlp/2024/06/SkyworkMoE/gate_dist.png","hash":"912b7bf9c26345d2c1513050ac484efde65580b2","modified":1717555982403},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/big_bird_attention.png","hash":"ed6c76b9bb77b98d34c429333498d04dac8e3ed9","modified":1710561804292},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_architechture.png","hash":"5e4c347dc41d7f070f54b386fdccf675cfeb8f10","modified":1710255091449},{"_id":"source/_posts/cs/nlp/2024/06/10wJetMoE/model_param.png","hash":"a053d5204998fb025425706c16409d01f6589548","modified":1719386283837},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ellipse_2.png","hash":"ee20ecce8c3470d17b1a4bd43df811d16269ffd3","modified":1711006599335},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/sigmoid.png","hash":"f1ced5f06861a2e0296050aff17eebfe3d023a6f","modified":1711246985230},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/postnorm_prenorm.png","hash":"d8830735e89c73ca416baabf4a195d7891d9f0ed","modified":1711167201092},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/prmsnorm.png","hash":"d5826342f665f4cb04fdbb2e3d83e0b2607355c9","modified":1711166590963},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/model.png","hash":"631efc6d4e92da128d7a10a4dc6af307ee4ddcbf","modified":1711459921302},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/third_party.png","hash":"673fe2b2cad3b1f40c0fcfd190c0034d8dbe7f31","modified":1711615706510},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/multihead_attention.png","hash":"6f8ee285f2646dc163b6b3164a0639aa9ddd7f27","modified":1709637863252},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/llama2_qga.png","hash":"5e0dea7d03de9144eb524a0a9adb102e91b52aaa","modified":1709983486925},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/sram_dram.png","hash":"ae7a9296b67c02608460a334fbbad3781b890302","modified":1709971938995},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/dpo_correlation.png","hash":"2c1dafd42b7ffa3318395a4934df87692ba5fd62","modified":1717259082807},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/reward_accuracy.png","hash":"7fb3d4dd64e3013534bd77eb1f2def23ec57c8cd","modified":1717257132254},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/simpo_contingency.png","hash":"773700b1d041aba8b3912deee9c8bc7886fec099","modified":1717259285268},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/simpo_hyperparameters.png","hash":"f976162c0882c49b43b503beb1384b447e2d5d00","modified":1717246443198},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/gradient.png","hash":"0b72939cfd4770fc21727bb203efb9c5dd2491d1","modified":1716907479268},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/result_3.png","hash":"df5861c846176c90bd9a90bd5836919ef023b13e","modified":1716985497709},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_long_perf_2.png","hash":"2b6be0099f1eeb0ee2d2056eae7cf541e146b636","modified":1713699160099},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_train_efficiency.png","hash":"c117407ada2adab8d97250268e2eafa533bb9083","modified":1713699487306},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/qwen1.5_moe_params.png","hash":"93d14a2645969b08a4fb80a31aa75fd8e5201ff8","modified":1713691207069},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/mistral_8_7b_perf.png","hash":"f90d9ff5b14326b0eef2a0026b3f5940e0d42f0a","modified":1713700127874},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/modular_connectionist.png","hash":"b5865cf34faba075b4f2316c2cae0559dac2d883","modified":1711981604894},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/qwen1.5_moe_perf.png","hash":"b79ad1a909081fd0537bd9d44cfac2dc2133de6c","modified":1713691095074},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_load_function.png","hash":"644684f21f85d565328d98334d41bbe019acbbfc","modified":1712050094734},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_more_add_bias.png","hash":"f0de5347918e4928dbcbc59a897c3f3227c3d30f","modified":1713856613597},{"_id":"source/_posts/cs/nlp/2024/05/-5/bfloat16.jpeg","hash":"8678b705b0d6e0b7deb230bf28f2d92ce0d42088","modified":1714809868152},{"_id":"source/_posts/cs/nlp/2024/05/-/acce_draft_model_param.png","hash":"2e5b1852eaf4745f3d9bfc9b0fcccbd37621bb93","modified":1715691438739},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_choose_gamma.png","hash":"65be032bf276290ca97b7d983bc4e1e2deaa95fc","modified":1715673837432},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_speed_and_op_table.png","hash":"416238792292bff7178830267d53941da202eadc","modified":1716471229174},{"_id":"source/_posts/cs/nlp/2024/05//add_money.jpg","hash":"0b00f9f1dd128e5601f0c7502dd2cf9233898f0f","modified":1714982519785},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_ppl_200m.png","hash":"7b232b7bf3c7238836f71b4b99e492d9f6c285f0","modified":1715241848803},{"_id":"source/_posts/cs/nlp/2024/05/-/stremingllm_kv_cache.png","hash":"11b09e96662feb7cc246e60e1b21d7ffceb47ae6","modified":1714393259685},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/ablation_1.png","hash":"821874e7f72e2cfffd971a88a2391ea9738ae2a5","modified":1719838205612},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/ablation_3.png","hash":"e24a99a137b7a9375859772056fd40e97d16dcf5","modified":1719838375526},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/cv_layernorm.jpeg","hash":"f0874ecc4b9d8da8bf3bff0e13a6313ed19a7b15","modified":1712494304517},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/eval1.png","hash":"4ae5db8cf55139a708804797ebb8f0789a02f44c","modified":1719838839577},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/ablation_6.png","hash":"1899cec30c0a2bd2fec2dc611ac779b5cdbe1a57","modified":1719839017972},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/ablation_2.png","hash":"9f3686e6478c43fdb04e4e2c84a628c4ed20ec8c","modified":1719838283324},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/eval2.png","hash":"7f0c08d17c4d381a3ac8669298ad6bc540fad2e4","modified":1719838885092},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/format.png","hash":"af7622d7cdfdd24858750502eb084eb22d2236c7","modified":1719837625840},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/MLA_formula.png","hash":"4828e9a6496e9ccf53bbf7cd57743be3d30a59e2","modified":1720839747693},{"_id":"source/_posts/cs/nlp/2024/07/Llama-3-1-/scaling_law_exp.png","hash":"097206df0cabb28e017489e79620a110b0d1fccf","modified":1721917823686},{"_id":"source/_posts/cs/nlp/2024/07/MoE-expert-choice-routing/intro.png","hash":"47bc8d61f20b0e79e35c9ec34291f98b7d2d8392","modified":1721549599997},{"_id":"source/_posts/cs/nlp/2024/07/MoE-expert-choice-routing/expert_num.png","hash":"595b9231b589235236f3398fb59a5239f5ea688d","modified":1721574436013},{"_id":"source/_posts/cs/nlp/2024/07/MoE-expert-choice-routing/efficiency.png","hash":"5c418161fa8bcce6cae77f564ab417431cd89176","modified":1721574189651},{"_id":"source/_posts/cs/nlp/2024/07/MoE-expert-choice-routing/capped.png","hash":"6375daf2798ae89bd456cdf9c725886a198c6f67","modified":1721658832966},{"_id":"source/_posts/cs/nlp/2024/07/Qwen2/eval_chat_small.png","hash":"1f94215d3367d59d983b721c66cf1fa0e83adbc4","modified":1721225702489},{"_id":"source/_posts/cs/nlp/2024/07/Qwen2/eval_long.png","hash":"9b3141566ae461e7ca25bd66e4347e23afd333aa","modified":1721226007075},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/lfa_conv.png","hash":"1dbfdde181f9adc5647b1cf9e15afe2062d315f0","modified":1720081824671},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/yuan2_sft_hp.png","hash":"8e1669d6c366737f4db99d036559a8aaeb9ab700","modified":1720083710115},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/router_eval.png","hash":"cf2a0fce5e092f214d935368341eb3e65bc61315","modified":1720098494603},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/yuan2_intro.png","hash":"a313198b675d2b41ef6ff294fd79c325910b3ae2","modified":1720081256288},{"_id":"source/_posts/cs/nlp/2024/07/model-soup/algo.png","hash":"cdccfde708ddda8df67bd061294422186e1f0c33","modified":1722343519360},{"_id":"source/_posts/cs/nlp/2024/07/model-soup/swa_1.png","hash":"4408daa48c5fd8647ec41cbf172c7bea476671b4","modified":1722343173471},{"_id":"source/_posts/cs/nlp/2024/07/routing/diff_p.png","hash":"6d8acebbfac87d268798cb804dea84eda1f9636a","modified":1721047465727},{"_id":"source/_posts/cs/nlp/2024/07/model-soup/swa_2.png","hash":"a6a627d314bb4dd6572ff0d5c8a79e37486afbfc","modified":1722343195090},{"_id":"source/_posts/cs/nlp/2024/07/routing/task_expert.png","hash":"1f1dd0d013485d0bd8dd6c9cbde71c43891bba03","modified":1721047534513},{"_id":"source/_posts/cs/nlp/2024/07/routing/active_num.png","hash":"364d6bc9b77761a18eb7bac8b3309bd687465f73","modified":1721047498768},{"_id":"source/_posts/cs/nlp/2024/07/bilibiliindex-1-9B/add_instruct_data.png","hash":"f0b2304664a91d6ce6cbb22d7462ce96e45f4039","modified":1720179171344},{"_id":"source/_posts/cs/nlp/2024/07/bilibiliindex-1-9B/wsd_quality.png","hash":"bf688703fa4fb38f050058dffbcbb07312962845","modified":1720171693029},{"_id":"source/_posts/cs/nlp/2024/07/MoE/gating_1.png","hash":"81db2d88499fd38eebce949218f49153710a6ed4","modified":1721132634611},{"_id":"source/_posts/cs/nlp/2024/07/MoE/gating_2.png","hash":"f8f8fa62cf29d9fae40ccaf098481ed12218a23a","modified":1721132687742},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/data.png","hash":"a8db4256f648dba59ad62f7304b3958d2044deb8","modified":1719924567051},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/sft_hp.png","hash":"38c35794b1fd1a6ce3eea712f4fea9a0864c3447","modified":1719997835649},{"_id":"source/_posts/cs/nlp/2024/07/AFM/afm.png","hash":"294ccd4e500c7cb4dac9b2dc37bbeb481c46a10f","modified":1722515411926},{"_id":"source/_posts/cs/nlp/2024/07/AFM/distill.png","hash":"fd713a906b29c74220806378f665b0d3f8f32846","modified":1722515550354},{"_id":"source/_posts/cs/nlp/2024/07/AFM/recover.png","hash":"9b80bba89dd37bd6843d7119870597330d8d1fa6","modified":1722515809442},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/emb.png","hash":"00573632286f7969ee67fc0069dc930fc1f200b2","modified":1722610948039},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/model.png","hash":"e1409cafb709e2a7527c5a6b2e14886723416d78","modified":1722611230760},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_2.png","hash":"b4516bdd5f5c1ede1c9ec8ee655256e51ea5b857","modified":1723554425744},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_3_sparse.png","hash":"800bfb947e3383faf5a074d97c1836e41208eb53","modified":1723554492406},{"_id":"source/images/avatar/Picasso_Elephant.png","hash":"6d38d4b24c13a3fd506ee37a562126b76a24de83","modified":1707118741657},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/data.png","hash":"86cde89044da233481994e8728ccb256146b3ae6","modified":1718956268617},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/learning_rate.png","hash":"e53cbb5a0d395e77bdc09779eb003134b7bbbe18","modified":1718801481150},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/layers.png","hash":"fc52ceb86ba2af1a3898f7afc18ab138d63c529d","modified":1718955962053},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/exp_model.png","hash":"39ea7fc867afbeeaa1e764513ca21019f4076cc3","modified":1718958829569},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/moe_result.png","hash":"e31fb8b893615de6f8a46c79d4cc7b813c7e9509","modified":1718958779005},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/tokenizer.png","hash":"c4e4116baccb9bdf88048b5e38827937ad48c045","modified":1718955432935},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/rope.png","hash":"5535c87dbcfee3ed646db93662a0b6a1ccd3b31b","modified":1708958930811},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/wsd_exp2.png","hash":"86256736d0e7b22c1f57778d3973202e4ac00f69","modified":1718874313485},{"_id":"source/_posts/cs/nlp/2024/06/GLM4/all_tools.png","hash":"c6a831b6334ad229e219e0342264acb6522a8196","modified":1719494379740},{"_id":"source/_posts/cs/nlp/2024/06/SkyworkMoE/100B.png","hash":"c9cc7f93992b0288d219a5926ba764860ed40c76","modified":1717509019859},{"_id":"source/_posts/cs/nlp/2024/06/SkyworkMoE/diff_dense.png","hash":"541feaefbc8790820c45a5bf0317e69c69087c24","modified":1717576882837},{"_id":"source/_posts/cs/nlp/2024/06/SkyworkMoE/structure.png","hash":"b71c6bfe51757237c56124d801bf0409c5d34b19","modified":1717506299219},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/rolling_buffer.png","hash":"34d4db9f4855926db561faa80e934dd971c0974e","modified":1710516051198},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/longformer_attention.png","hash":"64860379955872ecac5835b3f9d8c6d130c7e485","modified":1710560038203},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/deepnorm.png","hash":"4726d8a40d1d0db397005408295e1ba54809a7e4","modified":1711207769375},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/deepnorm_result.png","hash":"138cafc159f1e2e02455c540b4754f7cbb7f521d","modified":1711208592246},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_algo.png","hash":"56f1ab55c0e94814e6e37c30421012ed82098d62","modified":1711028736211},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ics_define.png","hash":"6bf3240ef78bad2cf76897a29c05428f4c195fba","modified":1711116459766},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/9B.png","hash":"7de19972e48b1f43c501d60a1c43a28c46b198e8","modified":1711618166808},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/long_context_result.png","hash":"daea6f734d64bdf5d24c6e17a640f23b1bd35b5f","modified":1711531324567},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/mqa_result_1.png","hash":"a052b57f71eb78e3158ed2ee06ff0e5597607a2f","modified":1709975039443},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/margin_dist.png","hash":"cf82f48158e4ba3e503cbe25cc811910804489cf","modified":1717257492586},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_long_perf_1.png","hash":"bc9c40bde860e78882965a25056a848aa4a89c77","modified":1713699132210},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_model_param.png","hash":"7b838274937cf45d73e59ac1fb5c2034e46586ae","modified":1713683464457},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_less_activated_expert.png","hash":"ac7ad86dabe94564bdb17399231c6ddc7da83962","modified":1712806182926},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_expert_specialization.png","hash":"64947872486dd083a7076bfdfe67cb0626208579","modified":1712805838590},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_compare_gpt3.png","hash":"311b21079599473054378e339885e0b87719e63e","modified":1712848303496},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/mistral_8_7b_active_perf.png","hash":"0c67d935657e62b9e8eeabc6403c269e09016626","modified":1713700314192},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_137b.png","hash":"fc11cfc87b2994956a9adbee76621fe5d964dc30","modified":1713447317172},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe.png","hash":"2a2ee095b8cc0727daa2cdd0c63891e4a470eae8","modified":1712042970123},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_more_add_noise.png","hash":"12d13a9ee6c28553626e469ed18d022e0176a873","modified":1713856976557},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_z_loss_result.png","hash":"3edd8e9eb069c9557c98fd21600c02b3a1978cc5","modified":1713858286214},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_remove_multiplications.png","hash":"52bc94bfe726dfc832534dde409154efe0ce7b0f","modified":1713856140610},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/vanilla_moe.png","hash":"7da0a6e9d529256107b5f6b287737ac47513a797","modified":1711962655018},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/alpha.png","hash":"4e3ad45447f5757d2cfcdf9d9555351233456c3b","modified":1717157185206},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/summarization.png","hash":"4934f3e42f20bc3a44ad07267e91a14c4c005543","modified":1717155934841},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_expected_token_num.png","hash":"52be2409ca9513de2e5ce10a0e77d8aa98dfc328","modified":1715672443581},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_walltime.png","hash":"b645987bec587e743021bc330de277416ef36d5e","modified":1715674129472},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_speed_and_op.png","hash":"9b5e3a6c9276309e7aa5a8848d52ef361e62bb36","modified":1715674286748},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_quality.png","hash":"fab44d68fc27f7bb2c06f758e537b9b249be0699","modified":1714913630381},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_structure.png","hash":"35f958d9ba50460689727c7038bf3a344000fa52","modified":1714288831777},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_downstream.png","hash":"192160ca6972003a61678e2f7f2467f5bbd94451","modified":1715242314359},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_process.png","hash":"259ea90040b7a5b98a27afcb992a6bee31707ab9","modified":1714289830786},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/cv_batchnorm.png","hash":"d9e8d897c36125fddcf1cbcfa5c237a37158a939","modified":1712503708413},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/intro.png","hash":"ea39460a533562e1bb3cb87e4b083d230b4099c8","modified":1719825323889},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/MLA.png","hash":"18fad673827880185f8fdfdf60bf759ecb70fd53","modified":1720795292816},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/GQA_compare_MHA.png","hash":"d1836492efcad8eb01f0c610db6f833dea52f565","modified":1720795017364},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/needle.png","hash":"5196cd7771b84b66a28073111ad5a899e48bc72a","modified":1720847545262},{"_id":"source/_posts/cs/nlp/2024/07/Llama3-1-post-training/code_sample.png","hash":"cb15e79a41f9c45426d90ccbb8a97354384e2f50","modified":1722003209787},{"_id":"source/_posts/cs/nlp/2024/07/Llama3-1-post-training/post_training.png","hash":"8f02f967b378a22f067d5d7fb67a313624fdea68","modified":1721999723778},{"_id":"source/_posts/cs/nlp/2024/07/Llama-3-1-/llama3.png","hash":"19935286a22c8072320836e2c6675fcdbdf785d3","modified":1721917785162},{"_id":"source/_posts/cs/nlp/2024/07/Llama-3-1-/model.png","hash":"238262c4ef4ff2087b014c326de75fdc2151c1f2","modified":1721917652735},{"_id":"source/_posts/cs/nlp/2024/07/MoE-expert-choice-routing/model.png","hash":"b24ba505952a804939e7e63f7676a20176609bed","modified":1721573996315},{"_id":"source/_posts/cs/nlp/2024/07/Qwen2/eval_needle.png","hash":"ca90761b48b146b4596267505f483f630980fe2b","modified":1721225914444},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/eval2.png","hash":"3bb02a3242fa75c34446178d4535a5ac391b7b31","modified":1720099667046},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/eval3.png","hash":"a10cdbe9b06a9d2b18b4a3ab28a2e25253ddbaf8","modified":1720099684326},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/eval4.png","hash":"1bd8cd0e7c2a612a9039a0c2274f353cf53a2c7a","modified":1720099699226},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/eval1.png","hash":"209e715214321dbd5879358fd4c390c2ec9e4a76","modified":1720099627180},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/yuan2_chat_data.png","hash":"d69c7854d75ebd7751fbeaf0a54cf029120dd7f5","modified":1720083565392},{"_id":"source/_posts/cs/nlp/2024/07/model-soup/swa_3.png","hash":"1918cc7b7d5f1110a3160ce80962316659424abb","modified":1722343202248},{"_id":"source/_posts/cs/nlp/2024/07/model-soup/method_soup.png","hash":"2356fc6694f2860802080383174efebc00d7ef23","modified":1722343474691},{"_id":"source/_posts/cs/nlp/2024/07/routing/perf.png","hash":"3e30d83a8bf735f3cd52566a3c2fd49947608c87","modified":1721047415575},{"_id":"source/_posts/cs/nlp/2024/07/bilibiliindex-1-9B/layer_num.png","hash":"315b4bb529fe45846d624a77bf23f199e041b59a","modified":1720169701737},{"_id":"source/_posts/cs/nlp/2024/07/bilibiliindex-1-9B/sft.png","hash":"f263860e110bd216de9296353a6e8cd56a5e3d2c","modified":1720179785866},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/intro.png","hash":"1e9401f1b85dc3387404472ecbd99bd9b0defed3","modified":1721394507448},{"_id":"source/_posts/cs/nlp/2024/07/MoE/models.png","hash":"7d7cc95caf7ee900d27616f24c61a54bd3104c3e","modified":1721132473650},{"_id":"source/_posts/cs/nlp/2024/07/MoE/norm.png","hash":"c8df60632519a86b3d0d5e719d1e98ae13762f6b","modified":1721132776026},{"_id":"source/_posts/cs/nlp/2024/07/-8/1.png","hash":"7d4bc4e879e853edefdee49bc33f71a4c87b31db","modified":1721305109364},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/intro.png","hash":"553dcf79eaab0fd25a64088f4e2054da038362d5","modified":1719923764392},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/eval_1.png","hash":"1ec3d8dec7bd24bf84e71902747ad36eee8b91d0","modified":1719926075476},{"_id":"source/_posts/cs/nlp/2024/07/-/efficiency.png","hash":"f8b65167254c4f67b4f8d521fa957550614c5517","modified":1721825324908},{"_id":"source/_posts/cs/nlp/2024/07/-/buckets.png","hash":"a80c242c4904a5c833819646cb5b41fdf6a1a3e6","modified":1721744820381},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/device.png","hash":"7dac5432f851444e63b4a50fbe5267eb4d6d48c7","modified":1722610693452},{"_id":"source/_posts/cs/nlp/2024/07/-/scaling.png","hash":"cf485430efb7e14fe412f435266d84c7311815b5","modified":1721829371960},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/share_2.png","hash":"96d02323048ab12d5d85d7ba2e83c983d2d1bfaa","modified":1722611074601},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/repeat.png","hash":"a71018f5c9a41981946db03d58636cd65afbf163","modified":1722611111947},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/kd.png","hash":"7e7595e30d05dfbcc3b5b387800baf71ce01a58f","modified":1722611318660},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_15_result.png","hash":"6affba40c95f658fa3a15edead4b97c3e6a98da2","modified":1723554054957},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_1_example_1.png","hash":"8776dbb337b27badc563e3c291dc27c55a7e6cb9","modified":1723553799352},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_1_example_2.png","hash":"959e4fce596299f6cf5c9a59c06d2d040d119bdd","modified":1723553871320},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_2_1.png","hash":"2245f0e52eb229591a8becee9d21d99d8247d594","modified":1723554307833},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_2_3.png","hash":"cf8dd9735a77c25f5b9e8c32f3652cd0bf6e71fb","modified":1723554398098},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/train_loss.png","hash":"ac1261a27009436fae1bf50412017e8f25ea7d38","modified":1718956787257},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/wsd_update.png","hash":"bdee7c642152f421d099748d5afd6570b7c99a5a","modified":1718875127763},{"_id":"source/_posts/cs/nlp/2024/06/loss/downstream_dataset_num.png","hash":"ac409ad0cd39c968f3d59a0ab7d4e75f9922d682","modified":1718524405587},{"_id":"source/_posts/cs/nlp/2024/06/SkyworkMoE/lr_result.png","hash":"96674fe834c8223d29136a4c8d36abf28cb3c195","modified":1717576132058},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/prefill_and_chunking.png","hash":"0c706e0728ea462b2b00c59a97c79ccf5f05b598","modified":1710516401027},{"_id":"source/_posts/cs/nlp/2024/06/10wJetMoE/data1.png","hash":"050a2ebcc9784a20ca811c99215b137c8257c0c7","modified":1719386642602},{"_id":"source/_posts/cs/nlp/2024/06/10wJetMoE/structure.png","hash":"61969133bdb9770d5139d27374eb3e3f4cc44d0e","modified":1719383798123},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bs_bn.png","hash":"aa28241d75f914603b9f7f67cc54db4e61bac668","modified":1711113379148},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/base_model_eval.png","hash":"9b4e65d246865683f8e3348d74bfad03c937b65f","modified":1711616539832},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/rmsnorm_eff.png","hash":"350a7a2703eef1ee9357609ae5820bfc30835681","modified":1711166117196},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/perf.png","hash":"3c068a423dcd32cac7f1630bd69fbe5a4c6789af","modified":1711614561095},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/sft.png","hash":"ea9aea143af836012f44d21956ab5455487e9bfb","modified":1711615463552},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/GQA_result_1.png","hash":"87f2c3632fdf83828e8bd07a95cd8e7bf277fc88","modified":1709982952107},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/gpu_cache.png","hash":"edb6b1abdecd3099f2d68c2a729c0ca9b1fb0db7","modified":1709970717456},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Scaled-dot-product-self-attention.pbm","hash":"03da711b1547c944deea60d9bf345eb30e7c566f","modified":1709638849226},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/transformer_structure.png","hash":"87f0258e43922eface0277e13167a4ba8c1402bd","modified":1709802508932},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/ablation.png","hash":"4c73c83eb527141e17d1109b6c2cff3488de6259","modified":1717250998001},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/GQA.png","hash":"d6b0cd20ae5f68a96f0865b3d0b6b129969caa4b","modified":1709982190361},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/intro.png","hash":"3ff9cce772cd825ec8b88591d576a5f52982d679","modified":1717231737607},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/benchmark.png","hash":"83084a5f64006000898d5252b3f8afccb635b3f0","modified":1717246623701},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/result_4.png","hash":"6f554d1b6911c3db211a7e57e886e62f03b46ffd","modified":1716985505197},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_vs_closed_models.png","hash":"6211c05b7d69194f2d820622525273110467a0d5","modified":1713698929131},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/cover.jpeg","hash":"6226a5276377816b37a20572a8b725af3ddf5760","modified":1714102101607},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_2b_less_expert.png","hash":"e03a00a194efd33890517b4ad642bca5566cf9df","modified":1713688653303},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_vs_open_models.png","hash":"ba0348d3fe68a27f8c6435c4c3a6d08d9c8869c6","modified":1713698848988},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_ablation.png","hash":"108dc8d66ae0e370e7969403efd85178b9a8523a","modified":1712805107241},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_family.png","hash":"9d813f12f82d8702886f7ede72c5a25151390ba9","modified":1712932424673},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_related_model.png","hash":"0f109231fc1b425c7364401c41ce5f3aecfd76c7","modified":1713709711124},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/mistral_8_22b_multiling.png","hash":"4af49ffc09a0de3793ec7137d3dfbddc9c309d38","modified":1713706946046},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_moe_family.png","hash":"21a6c80f1dac39eb364fb417ed83afde2b212675","modified":1713449413788},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_more_dense_layer.png","hash":"67c37482d73cd7ece7e384c0bd73e6891fc752e1","modified":1713856367887},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_capacity_factor_speed.png","hash":"60b624b7595e1f90763ea745ea358b6852eeefb0","modified":1713881294917},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_distill_sft.png","hash":"7a59660f367634d68aa392698042f3d9cfe190da","modified":1712979135172},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_init.png","hash":"f9fb36a0defac7a5990b5c58a614f3165af0ae3e","modified":1712934593938},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_dropout.png","hash":"63f36ca61aaa71e88ddf23339e63a9ccd898a6ce","modified":1712934811239},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_t5_result.png","hash":"83b63aafceeb8f2bc3f89ee6a1e3caec7987a1c9","modified":1715674742164},{"_id":"source/_posts/cs/nlp/2024/05//eng_ppl.png","hash":"fccca1509ebab2e89a3ceaad0dfeedc700de2691","modified":1714841526104},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_lost.png","hash":"b9d73b8022266af17789ab049c7adda621729cc9","modified":1714913979224},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_dataset_dist.png","hash":"e5754afcb70a45c0d11ee5db43c724350ec64257","modified":1714913484177},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_booksum.png","hash":"276dfa014d35f7d3375fbb7cee6eed21127f9955","modified":1715160163895},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_language_modeling.png","hash":"a49a7cc02694e7017c2a20dc0666046108b3c4c5","modified":1715159444656},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_passkey.png","hash":"811c5c677f7616b6625a0b86f2004f6d3ebeefe9","modified":1715160050601},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_ablation.png","hash":"babe57024a1c2212405220124dfd376a8a2bcfb6","modified":1715242576296},{"_id":"source/_posts/cs/nlp/2024/05/-/stremingllm_attention_sink.png","hash":"933f34f3bc1d04e2b36f3305ac9fe2acd8bc9939","modified":1714383008964},{"_id":"source/_posts/cs/nlp/2024/04/-4/transformer.png","hash":"9dddf171ca51f2ed1218baa9b84f4b98e9b911cf","modified":1713605151549},{"_id":"source/_posts/cs/nlp/2024/05/-/xl_vanilla_sw.png","hash":"3259a751066a0083ef249a0412f43fb582e6544c","modified":1715138182854},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/example.png","hash":"71cbc6c7f81bb102a23a5faa1d21f3908c7a3513","modified":1719837686942},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/MLA_cache.png","hash":"5fc118f53be86ddca4d7dc3783bc975eefd4e7c0","modified":1720839932880},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/model.png","hash":"e78cbe64d1c56722e7be47a4da30ba240100623c","modified":1719834617079},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/MLA_perf.png","hash":"b4aca26e5d41a48a7135fa9c7ca787a634847a01","modified":1720840225525},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/GQA.png","hash":"d6b0cd20ae5f68a96f0865b3d0b6b129969caa4b","modified":1709982190361},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/lite_eval_1.png","hash":"58696a02bf5b8c9c535ea4d6020a9f342553a303","modified":1720848437174},{"_id":"source/_posts/cs/nlp/2024/07/Llama3-1-post-training/sft_data.png","hash":"4e0572823d184079fd67e6937fe28127b0c72402","modified":1721999837233},{"_id":"source/_posts/cs/nlp/2024/07/Llama3-1-post-training/preference_data.png","hash":"197c3f37d5bf8684e1f8e60fc469794b2b78697d","modified":1721999796333},{"_id":"source/_posts/cs/nlp/2024/07/Llama-3-1-/scaling_law.png","hash":"fb2483e67bc86a0c17de9ab662ad7f92abdbf6aa","modified":1721917873848},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/lite_eval_2.png","hash":"dc8a1babc93ab353af6083cb960e7fa06bd48e41","modified":1720848476847},{"_id":"source/_posts/cs/nlp/2024/07/Qwen2/eval_base_small.png","hash":"c7d77e2bbba4785f385a61c92f276b9e8a5ea4c1","modified":1721225477103},{"_id":"source/_posts/cs/nlp/2024/07/Qwen2/eval_chat_large.png","hash":"9b97aa7c0ce7dbeaccb2d7b0e01324a959c1da0e","modified":1721225796668},{"_id":"source/_posts/cs/nlp/2024/07/Qwen2/eval_chat_7B.png","hash":"38e13d8d0c4f20fc0578405bd7036ad4432eabcf","modified":1721225746675},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/lfa.png","hash":"a332fcea93d56e4832cbeb35349b3ade8d02be28","modified":1720081723686},{"_id":"source/_posts/cs/nlp/2024/07/Qwen2/model.png","hash":"3df6aa4cd1c5e1ad95443ec86babd64cc1ac9567","modified":1721225386170},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/lfa_result.png","hash":"07c79f825aee4660d2463c67d010c5e5b63d7b24","modified":1720082536762},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/router.png","hash":"5f4b3283dfb386ea62f77e902b03ace2e416c515","modified":1720098275527},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/pretrain.png","hash":"489a86abddbf895fde11c7bf1a4467eb58d5d6bf","modified":1720098943551},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/yuan2_train_curve.png","hash":"ca231c0756f06345f1d4d4082dcf07ce5ef646dd","modified":1720085057543},{"_id":"source/_posts/cs/nlp/2024/07/model-soup/angle_2.png","hash":"ba71676087d1d7aa5d9ed9d62f88059c8fe66c9c","modified":1722343593458},{"_id":"source/_posts/cs/nlp/2024/07/routing/top-p.png","hash":"fbbbbe2b9eae52191cdba27e719c69e02142c4f6","modified":1721047341755},{"_id":"source/_posts/cs/nlp/2024/07/bilibiliindex-1-9B/dpo.png","hash":"04dc7be1370c5cdbc5111e92e2ca2bc23d211d5e","modified":1720180109561},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/3.png","hash":"a23ad2ad4d57e25c08ba703854d57844e504b63f","modified":1721395873986},{"_id":"source/_posts/cs/nlp/2024/07/bilibiliindex-1-9B/pt_data.png","hash":"d0f2a86f5aecec1fa98182e29aeeadb220d5a3b9","modified":1720168977046},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/1.png","hash":"dcf85b619aa60b05d620fd8e678214bb0d019144","modified":1721395598649},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/4.png","hash":"f0a158f0265c03c02f47e7233217f9adc8529346","modified":1721396208668},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/2.png","hash":"bf630562a1d0eeccb04c30a4e67e77f5317f5b90","modified":1721395695767},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/models.png","hash":"f8e96129c9c6bcc6a2a58ed11f40823b5987aaf3","modified":1721395483625},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/a6.png","hash":"7a14072bc7f92c53020d4313979663a08b55a971","modified":1721398691185},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/pretrain_hp.png","hash":"bb930adb3aeb440983e7e30f373e80ba59187690","modified":1719925962774},{"_id":"source/_posts/cs/nlp/2024/08//xiaomi_1.png","hash":"f1cdd75d772a8954f06ed17fb794654e3cddbb8d","modified":1723833716241},{"_id":"source/_posts/cs/nlp/2024/07/AFM/core_ablation.png","hash":"668ab6946ee526e5cacd5f8e33f1013a7a872f9d","modified":1722515499766},{"_id":"source/_posts/cs/nlp/2024/07/AFM/pretrain_2.png","hash":"89df5a5e4edb32a9c07efdced175841f230845a6","modified":1722515606241},{"_id":"source/_posts/cs/nlp/2024/07/AFM/intelligence.png","hash":"d4a34111d688ddf8b0a90ecc7a718586d5201d9e","modified":1722515675625},{"_id":"source/_posts/cs/nlp/2024/07/-/curriculum.png","hash":"f6dab28a9f334ec95bf4d49832d5c1bbc9eb70d1","modified":1721828899074},{"_id":"source/_posts/cs/nlp/2024/07/-/bias.png","hash":"3930d3dff31e035529184319260775244fa6d009","modified":1721825950423},{"_id":"source/_posts/cs/nlp/2024/07/-/mixture.png","hash":"cdaf52015913c3710c9124e586fe8d0a68777df4","modified":1721827541110},{"_id":"source/_posts/cs/nlp/2024/07/-/sota.png","hash":"4d9a25476bee1b70550a28d9da5f9d4ff610d381","modified":1721830152801},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/mobilellm.png","hash":"3dd06feb9934a7397d234e1121fb6ad6ef96fe31","modified":1722610648657},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/share.png","hash":"d003ad24bed7c72dbf3178c71d019db5c9f2bd17","modified":1722611033832},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_15_bench_3.png","hash":"75184881a16d9f3294b1c02f136ffb47f0caca14","modified":1723554181125},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_15_bench_1.png","hash":"20f6e497f733737f3df51d2981976b7899e835f5","modified":1723554033174},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_15_bench_2.png","hash":"d33d4349718c81882c7785b708fd96897f49c0b8","modified":1723554153840},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_2_2.png","hash":"e91aad82f0d366705bfe79f5f362c67316e40aa4","modified":1723554365713},{"_id":"source/images/background/wallhaven-2ywymm.png","hash":"aeaf089b64472a1c72cdd9778c5deac4086eb38f","modified":1707045207190},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_3_result.png","hash":"146a28ca3d66b673a84ca47c94eb98d820222d74","modified":1723554553241},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/128k_result.png","hash":"97987dc9ae6ef342ce5bb4a34072a558b11a8770","modified":1718958174001},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/cos_loss.png","hash":"2b5a94aea83ca359aac95432298a8b32b29672b0","modified":1718801980800},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/wsd_exp1.png","hash":"2c556e50353e6b1528918f310f3150afdfd2f549","modified":1718873769123},{"_id":"source/_posts/cs/nlp/2024/06/-MEDUSA/construct_tree.png","hash":"ba3dc50c0c35e13fb15e18ad7748494d7e20f532","modified":1718336888466},{"_id":"source/_posts/cs/nlp/2024/06/-MEDUSA/threshold.png","hash":"fb52a6f66fddabc8ae9f72d027f4605178f65fb1","modified":1718357328544},{"_id":"source/_posts/cs/nlp/2024/06/-MEDUSA/exp1.png","hash":"1d2454407467f240c2fa549a73589bcfa462a78b","modified":1718356038625},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi_explanation.png","hash":"df22d646e5a47e6223b7722007cfb7dde7c27280","modified":1709198077742},{"_id":"source/_posts/cs/nlp/2024/06/-MEDUSA/tree_attention.png","hash":"c3c0b10e0bd043235b307ebddb730b43a84fded4","modified":1718265969822},{"_id":"source/_posts/cs/nlp/2024/06/loss/exp3_plot.png","hash":"5bf2e800c8583d545e30a084400ec34eca436025","modified":1718509738469},{"_id":"source/_posts/cs/nlp/2024/06/LLM/4.png","hash":"f9c2e248f1416369b19e434fd28e7160f85c9d22","modified":1718713634456},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/warmup_effect.png","hash":"3e936786065e1ab9cbad17f5b86a5b8129720270","modified":1711204451931},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/eval.png","hash":"e78d1d820de4c455b9301124d6016a19762eae1f","modified":1711446016442},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/multimodal.png","hash":"b14a4eb4d377101acf7b50904b9ee0f1d473aacc","modified":1711614412507},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/pretrain_data_dist.png","hash":"8c66a625723cb87ee67a9ff60d3614b369f50592","modified":1711463822846},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_infer_efficiency.png","hash":"34245e99c2b29dbd54104ccbbb3d8c15706307b9","modified":1713699575045},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/result_2.png","hash":"6baa634147220fed9edfff7c70e83c56a2b24913","modified":1716985357034},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_capacity_factor.png","hash":"a960540d3419de77c3823d343247abfddeadde1c","modified":1713881000881},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_perf.png","hash":"ca3c95d4b1c1c8f986d7adcacbf04da444e91610","modified":1712049369889},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_specilized.png","hash":"138ad5a388c77cc02202de794ec4cb734d633065","modified":1712043657680},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_models.png","hash":"bc59770bc8ea44bfe480484a99aa9143acbaa6fa","modified":1713795616340},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_multiling_specialization.png","hash":"bac636d8da61768adb5a9b7c5bd75547267ae470","modified":1714048322261},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/odpo_intro.png","hash":"e32faada4824a2654e32d125cb7dded9895f87dc","modified":1717141374659},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_structure.png","hash":"d906a9148e035025683e8da1eee5fa3d87164aa5","modified":1713585604066},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/toxicity_control.png","hash":"42173484be1fd33e29244f43d658ba03ec9bacb2","modified":1717155371840},{"_id":"source/_posts/cs/nlp/2024/05/-5/ntk_by_parts.png","hash":"5b49750dc6a2d1b878f34bc71e3961d96282499a","modified":1714809199814},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_sd_algo.png","hash":"576ffae274518c5a4e6c049d199e0714b06bba86","modified":1715671754784},{"_id":"source/_posts/cs/nlp/2024/05//pose_ppl.png","hash":"fded043b94c97c1a782869963f5dea371e257b80","modified":1714918324529},{"_id":"source/_posts/cs/nlp/2024/05//eng_config.png","hash":"8edac537bd1aefea28406c414e0c0a4c888234be","modified":1714840521116},{"_id":"source/_posts/cs/nlp/2024/05//pose_method.png","hash":"db8784c9e4b14c5963f62f072df6a4c3c5405874","modified":1714916547117},{"_id":"source/_posts/cs/nlp/2024/05//pose_passkey.png","hash":"6202690a895f0114c90f6821c8cf1ad7388e1592","modified":1714918508416},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_design.png","hash":"e96faad18cb526d201ce069f9ce09dc3c9c0d16e","modified":1715245088507},{"_id":"source/_posts/cs/nlp/2024/05/-/stremingllm_init_token_num.png","hash":"ffe78c31d901311249530e786bc5ed321e2e242f","modified":1714392839583},{"_id":"source/_posts/cs/nlp/2024/05/-/stremingllm_exp.png","hash":"cdd21419055ca9bed86b46675e118ad4bdf55544","modified":1714394556802},{"_id":"source/_posts/cs/nlp/2024/05/-/stremingllm_perf_4m.png","hash":"c4545d7f0bbd0c5f6baa85dd64b747a3723fdf2e","modified":1714394913980},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/intro.png","hash":"0317287e02c626e3bae72f5af4100306df572329","modified":1720792089146},{"_id":"source/_posts/cs/nlp/2024/07/Qwen2/eval_base_7B.png","hash":"6ecd155b46dd98bf0931b210a1ada116d6a214b5","modified":1721225529436},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/yuan2_pretrain_data.png","hash":"dc700e56e4f28f0a6f42be843625b7cfd9181568","modified":1720082924963},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/a2.png","hash":"38a3c1b163ab3783dde39921808686bf108ec14f","modified":1721397468256},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/a5.png","hash":"6d442fca2b0393119dfdefbeaa72798b6e156491","modified":1721398426448},{"_id":"source/_posts/cs/nlp/2024/07/MoE/dynamic.png","hash":"eecce3203dbe70ad732f13c706b6ac8812de8cde","modified":1721132727791},{"_id":"source/_posts/cs/nlp/2024/07/MoE/matrix_level.png","hash":"11ca695a551c3b8357c5b3af88dcec86c2cacb6d","modified":1721132544228},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/peft_eval.png","hash":"e2af71ff2fb5ae2565c51e05bbf20e293838e5dc","modified":1719926472543},{"_id":"source/_posts/cs/nlp/2024/08//xiaomi_2.png","hash":"fa82b0d446a2be5a9d1075880c8358b5c6f6fd23","modified":1723833790166},{"_id":"source/_posts/cs/nlp/2024/07/AFM/pretrain_1.png","hash":"18ad360b603d6fa8bda0d2c1f6b7e2533f181efd","modified":1722515588208},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/structure.png","hash":"bbd51c9725e0c7378018c98204955ddb2bdc1502","modified":1722610725646},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/param_search_2.png","hash":"601bf31d9a7aaac8d1e1e60f1a4c2d40224160e5","modified":1718798191336},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/param_search.png","hash":"82785189286d4e7b69d24998d14ca9b78fb4d890","modified":1718797744783},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi.png","hash":"193df6c392d71dd4b90202d9be97f000f37b68ec","modified":1709197025669},{"_id":"source/_posts/cs/nlp/2024/06/GLM4/glm.png","hash":"127b31e5722081b17f680deb6c7052103cdf1158","modified":1719491115553},{"_id":"source/_posts/cs/nlp/2024/06/-MEDUSA/tree_attention_exp.png","hash":"0b516ebc1df1418eb6f5734b05f3975b9bc71d18","modified":1718357514716},{"_id":"source/_posts/cs/nlp/2024/06/LLM/3.png","hash":"d0bbe995cc75de29aaa21f1abc144d3bedf5556f","modified":1718713442312},{"_id":"source/_posts/cs/nlp/2024/06/LLM/6.png","hash":"c22d52b720cc97628669ba318f13c39692ff8c52","modified":1718714705043},{"_id":"source/_posts/cs/nlp/2024/06/SkyworkMoE/perf.png","hash":"02a9e32039679d6f1249e0bd6bfbc3cff00228c8","modified":1717564274115},{"_id":"source/_posts/cs/nlp/2024/06/loss/metrics.png","hash":"7bb73cea5404b78b78799e7069c914c50f847b39","modified":1718520589707},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_perf.png","hash":"c9d7ce0a301920c4e722e341200f311995923735","modified":1710558943189},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_swa.png","hash":"59037b91ba8f256fd89b3d60b8ce477e4c8f4b3a","modified":1710252446580},{"_id":"source/_posts/cs/nlp/2024/06/10wJetMoE/mtbench.png","hash":"3411a742f62a58e4ff435ca06641e8830c2c80f6","modified":1719391072662},{"_id":"source/_posts/cs/nlp/2024/06/10wJetMoE/evaluation.png","hash":"ee53ffd386d5ba9b276dc3f55b87e68b5f8dc378","modified":1719391004272},{"_id":"source/_posts/cs/nlp/2024/06/10wJetMoE/data2.png","hash":"29418adf51a48bca2340e506dd86dbb8882f913e","modified":1719386681496},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ics_measure.png","hash":"e9fe87cfea7dcef7cb66e1d76c17d883cbbc3cbd","modified":1711115709830},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/28.png","hash":"d438e857378575809c880b78ca715dc69e50b364","modified":1710643355076},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/mqa_result_3.png","hash":"12e310102ace1f9e89c0e9a352cf4a3462335a60","modified":1709986493059},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/hyperparameters.png","hash":"b3595e75eff0cb8ea8f86fd9e2f8c6ae5f7892bc","modified":1717242348046},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/dpo_loss_code.png","hash":"279b32cc1c4dfefa8790fcbb597659e8b974ac61","modified":1716975072126},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_perf.png","hash":"035a992ef2e7e5a9165c9488e2696e38fa29165c","modified":1713699612482},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_16b_perf_1.png","hash":"d540e8a58e166c0ba894708c9b0c277c31107487","modified":1713690316202},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_16b_perf_2.png","hash":"6886e18a2e7601202e8721b83f998faf028e19eb","modified":1713690412061},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_upper_bound_13b.png","hash":"cec21f0cf3809011ce210dffda35da3671147008","modified":1712803976611},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_model.png","hash":"63c95ea5ae77528f7d94a94b21cf12ed63e0bfdb","modified":1712849528728},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_compare_gpt3_2.png","hash":"89172dda5ac569780ea47e85bc43eaef1d6918ae","modified":1712848396056},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_perf.png","hash":"b6ed751d2f171dac971bcf1d7f12e8a2d7fad388","modified":1712672178340},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_structure.png","hash":"b0564913ecc1f78e5052dbc07eb65b3f048846e3","modified":1712752326556},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/result_1.png","hash":"725c6be46c42e8fc8184304bb0cdf5071b09e8b2","modified":1716985304979},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_pretrain_result.png","hash":"880f70f371b8392e3021bf56d282be5640c232f7","modified":1712135713134},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_distill.png","hash":"be5cae81fafbda6b638ac185421bd04e00b7a60d","modified":1712978653926},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_scaling_dense.png","hash":"b8de8c427de51d62e69915da7a97bf4a9b505317","modified":1712976996880},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_scaling_time.png","hash":"b56568665d2680cc0723269ae39dc4b60de1b01c","modified":1712976813025},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/sentiment_control.png","hash":"fc200cc3802fdee9d80e4bc259f4baca7b425ae7","modified":1717154380930},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/scaling_function.png","hash":"b554e0bd697e72bb2e5e24a16c678d80c2efcc52","modified":1717156880112},{"_id":"source/_posts/cs/nlp/2024/05/-/acce_k.png","hash":"ca37e1983347a1a835389de4d17047e3b0d02af4","modified":1715691856143},{"_id":"source/_posts/cs/nlp/2024/05/-/acce_alog.png","hash":"61f4653292bbd93debee66cebfb44ac7198e5818","modified":1715937599067},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_perf.png","hash":"5cf407e2e2ee61bb2b6bdae0570c3b8c4a3a9374","modified":1714913782279},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_compare.png","hash":"9b272eba596a790b1d40ef3a8b041bdffe1660d3","modified":1715159194851},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_ppl_figure.png","hash":"34397c88e268b769c97a2b6e2ad63bf6ad5270ef","modified":1715241210130},{"_id":"source/_posts/cs/nlp/2024/05/-/streamingllm_model_ppl.png","hash":"2fc1c11d7cfa2598b414e5e4c145181ec9e10648","modified":1714381636336},{"_id":"source/_posts/cs/nlp/2024/05/-/xl_attention.png","hash":"395424d6c048880e143b9b2f93585597fbebebd7","modified":1715138841538},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/model.png","hash":"3b312a78f5664334694a18cd1fb6a1419bb7db73","modified":1720792285144},{"_id":"source/_posts/cs/nlp/2024/07/Llama3-1-post-training/steerability.png","hash":"58a2e99e8b23814a53563b71159fa7118f63a106","modified":1722069263667},{"_id":"source/_posts/cs/nlp/2024/07/Llama3-1-post-training/file_upload.png","hash":"bc74f106e2202b647ad06f8a40dcce617cc17d27","modified":1722067685146},{"_id":"source/_posts/cs/nlp/2024/07/Qwen2/eval_base_large.png","hash":"0548563b9aabe77bf92969ac7435fe94f94819be","modified":1721225599070},{"_id":"source/_posts/cs/nlp/2024/07/model-soup/compare.png","hash":"b226e1c4aab1103479faef5071ff1d81cfd288e3","modified":1722343621570},{"_id":"source/_posts/cs/nlp/2024/07/bilibiliindex-1-9B/norm_head.png","hash":"b5a6e95dec33597b0e492c3cb27acf2ce6b08fc7","modified":1720170295809},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/a1.png","hash":"b12067baa44113a9a5a813a6342680602e14afaf","modified":1721396953513},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/a3.png","hash":"8fef339f6148d85ca2e476c1e31d02ba1c0a4051","modified":1721397726657},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/eval_2.png","hash":"87d2cbaef2f915e03f03efa30ad6d853988d2ddc","modified":1719926186791},{"_id":"source/_posts/cs/nlp/2024/08//bfm.png","hash":"61a3b1d748826cda9791eb31554b959cf8e431fe","modified":1723835975032},{"_id":"source/_posts/cs/nlp/2024/08//system_1.png","hash":"59d47bc561ae6364870b410a9b30bb3f942ad683","modified":1723835285728},{"_id":"source/_posts/cs/nlp/2024/08//task_emb.png","hash":"f9a24501fc342029ad08bbe0f1e954614b9a4f3c","modified":1723864724613},{"_id":"source/_posts/cs/nlp/2024/07/-/dist.png","hash":"2c032b4da26c6377b72bb68f7f5b4ab41ae9c403","modified":1721745027299},{"_id":"source/_posts/cs/nlp/2024/07/-/model.png","hash":"c1d3c58aba2ed3099bf7218722e8a2cd451547d2","modified":1721824546827},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/result.png","hash":"60f4f0a3cb4a817146c9d33235653cc41c2382f8","modified":1722611159631},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_1_code_case.png","hash":"d8ddb0204e7807ce235dd5ebab5d1fc8c4a87a58","modified":1723553562841},{"_id":"source/_posts/cs/nlp/2024/06/loss/exp1_compute.png","hash":"4b2bb4c651292fa438292ea1adcd5fb6530fec78","modified":1718523259881},{"_id":"source/_posts/cs/nlp/2024/06/SkyworkMoE/exp_1.png","hash":"31f27f3659a4c3e0af9894e6a9b85cccec611889","modified":1717509313256},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/realformer.png","hash":"3bc805db3177c7e6521362b063543941da8d2bd3","modified":1711206089667},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/21.png","hash":"fb2577b5fa73b06b786484b3723f7aa3819638a0","modified":1710643325115},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/ln.png","hash":"4d4c831fc95c591ea0415e07dd5f46d1b1494e60","modified":1717251987860},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/intro.png","hash":"c5175eef020ec3499aa67163813eab1c4c13a84a","modified":1716887862324},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_sft.png","hash":"8bc26fcfef1aab448d0db0b28666852f62961a3b","modified":1712821943440},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_perf.png","hash":"01fb9d4f232e9f0b86abb91dbe5d8fb9fac456ce","modified":1712933127383},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_algo_1.png","hash":"aa8bcd982c78e4304c63f81e44b20519dc04f18f","modified":1712070648688},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_result.png","hash":"32ce9bba1b65ceb2e69c079e113d8b4c524bc479","modified":1712067349025},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_hierarchical_gating.png","hash":"067160c735e9c0b8cff777df60d52dfca21ea783","modified":1712045417559},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_perf.png","hash":"27157f620e2b7c4be60b2a58f7857a888794cde1","modified":1713968695747},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_perf.png","hash":"58927fa39b5e56e8da00144b417bbae5256d6bdf","modified":1714048423893},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_diff_expert_capacity.png","hash":"23f2234b45a2a05ff8b098e68a361a71f46816e9","modified":1712133650991},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_distill_diff_model.png","hash":"05c4d885f5563e3dbd56c055a52df5c1531677a7","modified":1712978973713},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_example.png","hash":"edbdb72af30cac5f036e47b3d0d426919f336e62","modified":1715609386236},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_dataset.png","hash":"b85a9077cffcace583a7dbcdbd235ab646086ea1","modified":1714913285654},{"_id":"source/_posts/cs/nlp/2024/05//eng_tokens.png","hash":"b6c43c289004164e90de39c30e170de5ac1088aa","modified":1714890071917},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_starting_tokens.png","hash":"18e3a03213a7275617201b71eb274cd8fd8b0bf9","modified":1715178569704},{"_id":"source/_posts/cs/nlp/2024/07/Llama-3-1-/eval.png","hash":"55fc95a59d33a688a4fb6ce0cc94c1e74480477b","modified":1721917747223},{"_id":"source/_posts/cs/nlp/2024/07/bilibiliindex-1-9B/scheduler.png","hash":"2ecb96391be5f52692147199916123dc8b26aa76","modified":1720171083752},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/a4.png","hash":"2491aacfa7722f017f9a673e95e96f5957282d4f","modified":1721398085384},{"_id":"source/_posts/cs/nlp/2024/07/-8/2.png","hash":"23f2234b45a2a05ff8b098e68a361a71f46816e9","modified":1721305135342},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/head.png","hash":"82379ab5e71e15f3e1b8690f9bc20b8ca928d8a6","modified":1722610992830},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/zero_shot.png","hash":"ef3354533df8ff1053ba06800294cf5be67d0576","modified":1722611282889},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/scaling_law.png","hash":"19067e52c8d6e073c86fbf4c4fee258574f9b348","modified":1718893969072},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_rope_ext.png","hash":"43e1131d146b8104e6936d73aca51d8ace4aff61","modified":1709206562025},{"_id":"source/_posts/cs/nlp/2024/06/-MEDUSA/intro.png","hash":"ab8abb35042127509f264d1f0e8ce838350b08ff","modified":1718249943493},{"_id":"source/_posts/cs/nlp/2024/06/loss/downstream_dataset.png","hash":"a6cda82185bcb7c1f70931d792e10583825c2d01","modified":1718508214406},{"_id":"source/_posts/cs/nlp/2024/06/loss/exp1_plot.png","hash":"27b2613a83f5873538e0e5832ea957e47625d719","modified":1718508482972},{"_id":"source/_posts/cs/nlp/2024/06/loss/exp2_plot.png","hash":"e65d2e11a5a28b4b1ff8b54ffff65be2298d7c1f","modified":1718509536128},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/cover.png","hash":"0493fd58fd2dad33394399d960924dbff6b386b1","modified":1711459395465},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/20.png","hash":"5d42628c8dac91c9671a58535b730e91966c0cbc","modified":1710643321378},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/ln_effect.png","hash":"714ba6cdf67ea33d507e3358b113a94bcd24e1ce","modified":1717252559659},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_comparison.png","hash":"20c7e90a390604d2146b4984678524046ad941b8","modified":1712803733203},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_model.png","hash":"5179df7e42bb7dc49fb6f92f4ec68ed820aeaae2","modified":1712068764148},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/mistral_8_22b_code.png","hash":"f9b78e0669e0c83f716c8e4abfa4d97b6f9b8143","modified":1713707063629},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/mistral_8_22b_reasoning.png","hash":"ce00d9ba7b8a65eb238c99f79a17ca7a3cd2238a","modified":1713706908014},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_scaling_step.png","hash":"dbfa55ab1c94e266344315fd215f2d646eaefda1","modified":1712976642477},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_encoder_specialization.png","hash":"382cb53fcc2e9172ffd7554a04714feed4d9706b","modified":1713882897561},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_alpha.png","hash":"f7f5a24106f1b9d16fd805b3ed3d3c2efb4a8c03","modified":1715674930516},{"_id":"source/_posts/cs/nlp/2024/05//eng_data.png","hash":"9262b3bbc1b415d9c776723fe85c0a43f9fb562a","modified":1714835715477},{"_id":"source/_posts/cs/nlp/2024/05//eng_data_dist.png","hash":"157286ce140bd5acc7c45fb12785649cc7214472","modified":1714837490239},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/m32_intro.png","hash":"dbd36b878a094f3db0dd328d1f451c545c108cb0","modified":1720097892725},{"_id":"source/_posts/cs/nlp/2024/07/model-soup/result.png","hash":"6c5d190fff97ddce11a3e9b870c20c73e484aac0","modified":1722343666627},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/deep_ablation.png","hash":"828f515fe97190eecb6eb0b84e318819340d2e34","modified":1722610892406},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/sft_result.png","hash":"599e02f21987d2f50b32d774fbcf3d9aae07792c","modified":1719926429383},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/deep.png","hash":"1071405517d42ae5e5b9ec42f206accbe1013021","modified":1722610806416},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/structure_ablation.png","hash":"e821acb21030415364edeae99d894619bcb0e17b","modified":1722610756935},{"_id":"source/_posts/cs/nlp/2024/08/phi/overfit.png","hash":"a091387862e564bb8ce30d1fe0b903a65fa74422","modified":1723554595629},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_1_compare.png","hash":"ad246d961a39997e773d7ced10c50de6ad565f2b","modified":1723553676564},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/batch_size.png","hash":"8e754e7b97b0290618e90e499d33f300e149b39c","modified":1718799768529},{"_id":"source/_posts/cs/nlp/2024/06/loss/exp2_param.png","hash":"1412eccf1c2cf485bde50b1cbf9223ee5a5ff2cf","modified":1718509235904},{"_id":"source/_posts/cs/nlp/2024/06/SkyworkMoE/normaization.png","hash":"8c4ce11d34256acc7e3800355186ab00f2236293","modified":1717556835290},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/lihongyi_self_attention.png","hash":"39db6256143fd9a494e848240a8daa434aaddea5","modified":1709965340148},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/main_results.png","hash":"a7388a503f0157c2ebe9ef765d63daab358b67d7","modified":1717249875414},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_145b.png","hash":"502a377d8625d78d2e3ba7281bfb11732c14a61c","modified":1712822142201},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_upper_bound_2b.png","hash":"4338539fe123371c5512c730fc8a35864236323b","modified":1712803847753},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_capacity_effect.png","hash":"16268929d0ec965d3771fccbb595b75d82f05912","modified":1712134554223},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/vanilla_moe_result.png","hash":"e491e44cfff384422f3d9cf87cd5e52fd976aed7","modified":1711963546083},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_sft_result.png","hash":"0dd7cfaac788c5d112dd10fe98f0052b369420bf","modified":1712978414761},{"_id":"source/_posts/cs/nlp/2024/05//eng_needle_comp.png","hash":"e4c9f5c51548faf11a7ff64d23aae5bd2927ab0e","modified":1714830276600},{"_id":"source/_posts/cs/nlp/2024/05//eng_sample.png","hash":"6b0d7ed89b16a3e9c6297219f33e59f204923828","modified":1714890968069},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_gating.png","hash":"5e875c3de6bde62ea8e15ea4995a56f9fd28d67c","modified":1715159702016},{"_id":"source/_posts/cs/nlp/2024/05/-/streamingllm_compare.png","hash":"40394890082b1666d1221f302ed52a80fc358477","modified":1714317603622},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/align_eval.png","hash":"882ae5f89d06999a7113d6345a2e275ffd947eb1","modified":1720847960346},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/pt_eval.png","hash":"68b8a0f9d7a49509ab10fcf083bf292b86556db8","modified":1720847649150},{"_id":"source/_posts/cs/nlp/2024/07/Llama3-1-post-training/multi_step_tool.png","hash":"922228b41af7043faad376a0d5988a2478151af6","modified":1722067561949},{"_id":"source/_posts/cs/nlp/2024/07/model-soup/angle.png","hash":"e9ab1ed585133183ebe85d017eb816d5624edf64","modified":1722343556920},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_1_result.png","hash":"2a039d232b1ba349eefe922746b0d74b2850e237","modified":1723553371271},{"_id":"source/images/avatar/shadow.png","hash":"da67a14e556fb3223c1c4ec8b9f33eb52c612f50","modified":1706779539112},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_ics.png","hash":"f92751ea20430f25caa3d6bb892c5894bf7509d6","modified":1711115173958},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/ict.png","hash":"2445ecbbf5ec6a96695f21c03a5fcbf67640b9f0","modified":1711615224209},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/17.png","hash":"8ea1c5d90f3da5c469eb17aea50f377cb9c28ba0","modified":1710643306201},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/18.png","hash":"fe0a8e7005110abca19bc7ae506f3e35042b70ec","modified":1710643312126},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/26.png","hash":"f74d03dae65109740f48924f30b52a742b5e4273","modified":1710643346427},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/gradient.png","hash":"58524a8fd10f4d76e8e419e1ed46bd9a99cf58d5","modified":1717234900278},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_intro.png","hash":"6b810c88945281a9cdf9749941cdbe08346fd42b","modified":1714914076240},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_example.png","hash":"946cdbb0a425b9d11b0ff587007885990abc9f99","modified":1714912943693},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/eval.png","hash":"982f977c073703051fdecb951c6231f7b74aa58a","modified":1718956887234},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/pretrain_data_pipeline.png","hash":"91218a2272eab9284904c91bacd8d8a40e3c1580","modified":1711462239524},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/10.png","hash":"2c52d4f90dd9356eeb4c9a39f1df1038ccec4693","modified":1710643262247},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/3.png","hash":"4c2c2a30d9ac8db03bab56da5d16ce2042ef73bc","modified":1710643220743},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/6.png","hash":"4b32a49bfead98f5238871b81076176e38168333","modified":1710643241274},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/eval_3.png","hash":"078ad89992f2cd4bc20b2cc0fd412ceda987e231","modified":1719926362787},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/1.png","hash":"a8898b3f3b7c64fabc5fad9bf8ef5524501d2aeb","modified":1710643881936},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/19.png","hash":"1d7b929e709657c9b7d7ca4da8eadc8c4ca4b3ca","modified":1710643317015},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/24.png","hash":"b89b0a0ca774a4efc1ece628fb20379b5f6a0b69","modified":1710643338042},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Scaled-dot-product-self-attention.png","hash":"983eae2b767df413ef3211ddaf31f1b833d7c86f","modified":1709986303475},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/8.png","hash":"92b0e3b75ce97bbaf4aa69e484216702293589ee","modified":1710643252580},{"_id":"source/_posts/cs/nlp/2024/07/bilibiliindex-1-9B/dedup.png","hash":"417cb6d188decbbf3f25e6595dddca213a6d3376","modified":1720169373099},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/27.png","hash":"557c04a2134b6ae147e076cdb80de1730e937d9b","modified":1710643350772},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/9.png","hash":"2a0fb56563b13411035ed41a3ad882f66f948b26","modified":1710643257459},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/11.png","hash":"9b964f3aa6f82a09eb2f2f944508bf0a9d29efb3","modified":1710643267156},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/22.png","hash":"3b27321ef8d76844f6720e1a27d65d1946d48ea7","modified":1710643330423},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/23.png","hash":"9a9af6308620b59f2ee00a3d0da4e942d953e406","modified":1710643334116},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/25.png","hash":"aa259b58be90eed6af0c4ba800a991b9464453d0","modified":1710643342514},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/15.png","hash":"15abcbcf7340941e98dac7a0ab42d922e7fea1b4","modified":1710643288058},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/4.png","hash":"4dacbfb89079d528da1208773961e1366debde9b","modified":1710643230331},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/2.png","hash":"768421239ad7c838dd86714fd9f17b3c73cbb887","modified":1710643214870},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/12.png","hash":"c6c493b14e0a1cc4863a912c4ccc998de194bfc0","modified":1710643275198},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/14.png","hash":"71f75960246f7528b3b83b84f8f91775f9e2fb45","modified":1710643284450},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/16.png","hash":"ba7b2bc65e10389cf9a87ddef69f462e806304f5","modified":1710643293307},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/MQA.png","hash":"7e3f3037311be60e79a7b5388338febc9f3b6d7c","modified":1709986434286},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/7.png","hash":"95640525b0706d3118eeb88c6c4c6217a96c39d0","modified":1710643246812},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/13.png","hash":"68b7da3e3074e4d6995eaf96c7d8cf622eadffb7","modified":1710643279584},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/5.png","hash":"7ae786b309a757c5f61a713c7ceef4d2824b024e","modified":1710643235878},{"_id":"source/_posts/cs/nlp/2024/05/-/speculative_decoding.png","hash":"fe277fa76f9f9c71e2030a41ca9eab458c33826a","modified":1716543143782},{"_id":"source/images/background/wallhaven-p97q73.png","hash":"e83875b9cd1dcfa87f0e76dc1a327cbe2921c5da","modified":1707045618160},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/ms_invest_mistral.png","hash":"faf324c0b57843516a0b256750e6475ec0c2ce93","modified":1710316114714},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ellipse_1.png","hash":"ef2470f6bf1511dc9aac9f1c6489b9d2ffdcb45f","modified":1711006814272},{"_id":"source/_posts/cs/nlp/2024/05/-/digimon.png","hash":"247f4059dd9671047f5d6707d8cef75a93d93f40","modified":1715070986892},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/norm_in_nlp.png","hash":"7be79b0e55d7d00ff6c16c247d0e506771453380","modified":1712575607757},{"_id":"source/images/background/wallhaven-gpxpg3.png","hash":"d798eaa0d679ed8a14022b0741098711c3402ca8","modified":1707045245660},{"_id":"source/images/background/wallhaven-x636oz.png","hash":"d419359164da44691e2bc8641256ca7c8111e227","modified":1706875075740},{"_id":"public/baidusitemap.xml","hash":"2dfaae180d3127fbc9c1f9b46ec338eedd30feaf","modified":1723898785076},{"_id":"public/search.xml","hash":"c5b85877943f0fae0f454dee412e08a115eb4ab0","modified":1723898785076},{"_id":"public/sitemap.xml","hash":"8904218856909d308b12a1983cd34bbfacbf13d9","modified":1723898785076},{"_id":"public/sitemap.txt","hash":"1c808f238a647fb97bc1b23ac30a22a03b8afc36","modified":1723898785076},{"_id":"public/categories/index.html","hash":"cea7739c59161c63b0b67421673dfec25b4993e3","modified":1723898785076},{"_id":"public/about/index.html","hash":"3a6d6bd4af4e12222b9b580f3eae19dec3dabfa8","modified":1723898785076},{"_id":"public/tags/index.html","hash":"0297d55aa5583b4aacbf1829d0cb737f1747bead","modified":1723898785076},{"_id":"public/9c593ccd.html","hash":"8620476c1cdf450539b8ddb04728501f6cfe026e","modified":1723898785076},{"_id":"public/fe13b56f.html","hash":"ad7104ef11a0b382ba8e1bbe4acce5101c50ed77","modified":1723898785076},{"_id":"public/fb9c8882.html","hash":"c3f948f6e70f8ca80c76d90da537bdd0fc739700","modified":1723898785076},{"_id":"public/5ac36d34.html","hash":"05bb5861e81c9f2454602288fde46e462890545f","modified":1723898785076},{"_id":"public/1e34e252.html","hash":"c7736b2444723462477019f910ce478a423d922e","modified":1723898785076},{"_id":"public/bb8fcf21.html","hash":"4cad11cfbcc7606cce958ef0a7c644d6375b4e71","modified":1723898785076},{"_id":"public/93328a2a.html","hash":"3111d6942638408b6e3f0bfdeaee150aee122b33","modified":1723898785076},{"_id":"public/210dbccd.html","hash":"6199ed0bade1f932da1d48e0e9d24501c67103ca","modified":1723898785076},{"_id":"public/2c8bbc7.html","hash":"47cbc3f75407d025427878a128fd2bf378c2a909","modified":1723898785076},{"_id":"public/a0824e29.html","hash":"2490d7cf3994efb19ed89befb2f874a444458766","modified":1723898785076},{"_id":"public/e287b9c3.html","hash":"bf62ca9123ef982e1e6973db015330eadf845180","modified":1723898785076},{"_id":"public/7d7294cb.html","hash":"bf09a17022c5d1644c08877682e90b499b7d0d11","modified":1723898785076},{"_id":"public/5e1d14b3.html","hash":"509e35b613d9fe433c41bf33f82c52667f2292fa","modified":1723898785076},{"_id":"public/224c42da.html","hash":"6e9368ea4a8820db2c3b66846ac5b37e7b315a02","modified":1723898785076},{"_id":"public/83c49df0.html","hash":"4ab2350eb8553ef4f9bea054d10134d25b33e376","modified":1723898785076},{"_id":"public/a8f8b641.html","hash":"252c25e6f14e3d4f8e9f41f9ee3c0e5fca62d885","modified":1723898785076},{"_id":"public/770b63e1.html","hash":"488e7d98c5f2ce303901ee19c24031eb86369c31","modified":1723898785076},{"_id":"public/3df0cd42.html","hash":"d7adccafeaaf492918921f88c74aa8df27524b66","modified":1723898785076},{"_id":"public/f845f3e4.html","hash":"bca584e1c43cdf80b2de2fa213633106cb21047f","modified":1723898785076},{"_id":"public/cf3f1f81.html","hash":"33a92741a5f93aaaad846ef7f597da1b891b36c9","modified":1723898785076},{"_id":"public/a5206abd.html","hash":"3cfc7db31e080302d031c243fa90e379a83796b7","modified":1723898785076},{"_id":"public/f3acf042.html","hash":"b7570a6a5bc66ad9c62eff2a485fd057e209841e","modified":1723898785076},{"_id":"public/f0902f1a.html","hash":"ecf0dc1345a5e3ebc28c7978a639bd754cdfc99f","modified":1723898785076},{"_id":"public/376db710.html","hash":"24d44edf6cd2a1dd987efc4005700354b36b781b","modified":1723898785076},{"_id":"public/7381cae3.html","hash":"4e78eb782c28df12f80ef05b3bf0528efc36f29a","modified":1723898785076},{"_id":"public/f5fb75e4.html","hash":"96e4e2beaf43d03a576023498b6bbc785cc7e8d1","modified":1723898785076},{"_id":"public/dd614e12.html","hash":"04c67c2fe87a87c5aa479aaf243933c2108f37a0","modified":1723898785076},{"_id":"public/7bbe2df6.html","hash":"4e6e618559ec94826de662df18a76fc03d17e031","modified":1723898785076},{"_id":"public/1d5bcd45.html","hash":"df7519f3d2fa102ca71a20aaa5f48c567ecd9b48","modified":1723898785076},{"_id":"public/4fe7b810.html","hash":"44568451bc6ccfb22d976c3a6c8076b69050639e","modified":1723898785076},{"_id":"public/280fa97a.html","hash":"67dce14e5ba308d5eeb0f1e360e72eb43fd2ee76","modified":1723898785076},{"_id":"public/da871ebe.html","hash":"2ab1ef8e09ad70769aec7d032efbcc4a533a5060","modified":1723898785076},{"_id":"public/7c04944d.html","hash":"cded27b59c62d57e2968ea6fdb1da977cfc624dd","modified":1723898785076},{"_id":"public/f5c015c.html","hash":"5840c41d9e5147e2bf370d57aa279de6a42e6780","modified":1723898785076},{"_id":"public/45ee1a6d.html","hash":"6534b0cbf7a6a6d975eba23321e9bddf9fcc9e1e","modified":1723898785076},{"_id":"public/cc852861.html","hash":"4090d500a5aa5d88230da113fb3980e76b20a52a","modified":1723898785076},{"_id":"public/473f2b43.html","hash":"0cc1288269bd6629dccde543e44e4efa9a6f58c7","modified":1723898785076},{"_id":"public/336f2f3e.html","hash":"97f8cd06da7e8562a0bfee46f6d0e02405a5eaf0","modified":1723898785076},{"_id":"public/1736008.html","hash":"376e4ec4d274d2089faeeb0363c58ebb5b3140ff","modified":1723898785076},{"_id":"public/b70b4a2d.html","hash":"f8e30ee064c63b8757b507d6a14225a613c0b8ec","modified":1723898785076},{"_id":"public/44e38c1b.html","hash":"7bb499608bef682dfcd7de5d40b496ed20fc4fd1","modified":1723898785076},{"_id":"public/41b6a819.html","hash":"251a0655702164336918491941a0d3508dcf36a6","modified":1723898785076},{"_id":"public/ad0bba9d.html","hash":"63346a5c63ddb1fabf34206d84d8be939bff8606","modified":1723898785076},{"_id":"public/6a40bfa5.html","hash":"c41457c12cd4945873ba2a6d1339b3981121b705","modified":1723898785076},{"_id":"public/3345028a.html","hash":"4f2dbd1510361f31cfa38e809f2dc22df0531e15","modified":1723898785076},{"_id":"public/c61d17e3.html","hash":"b9b806f522487bec22f3cabf83dc445641da4d26","modified":1723898785076},{"_id":"public/3dc22f96.html","hash":"0ba370d2732f72523c4059e906aaa9834fd25e23","modified":1723898785076},{"_id":"public/c4da56c0.html","hash":"fc64e2232362589874da175cbd73bde1c014a16e","modified":1723898785076},{"_id":"public/a051710f.html","hash":"24690fd87f1b96c094654ea65283946af545b57a","modified":1723898785076},{"_id":"public/14e576c.html","hash":"639a918ce07659538d740a8293e932210a6a6e16","modified":1723898785076},{"_id":"public/archives/index.html","hash":"b59efe868617ccd444786a46b0e4ef9f8687c8c4","modified":1723898785076},{"_id":"public/archives/page/2/index.html","hash":"bd39602b0f998d2cc786a52cdabefed8bd97e38a","modified":1723898785076},{"_id":"public/archives/page/3/index.html","hash":"b33425d5ce674f6452b1ba1a5a62ffaeaaba0f48","modified":1723898785076},{"_id":"public/archives/page/4/index.html","hash":"02b64e6d83d79e90a68b6409f26a4bb51ca3d4be","modified":1723898785076},{"_id":"public/archives/page/5/index.html","hash":"fc1ad71c635a4b0138716dcb6b909ac8cc4c51ff","modified":1723898785076},{"_id":"public/archives/page/6/index.html","hash":"655e6f084e2e8c7ffc58e2b979969fc8ecd51565","modified":1723898785076},{"_id":"public/archives/2023/index.html","hash":"022e0be7e85506ee57f752bf57d6956864b105ba","modified":1723898785076},{"_id":"public/archives/2024/index.html","hash":"d98348c6b10d7ee95111d7f56f4e7371861294d9","modified":1723898785076},{"_id":"public/archives/2023/03/index.html","hash":"396dd1393dbe3c5999b3471a4c7b5a8a9f3b1189","modified":1723898785076},{"_id":"public/archives/2024/page/2/index.html","hash":"4558c62dfad9e8c4a756e0a09b54899439c10f59","modified":1723898785076},{"_id":"public/archives/2024/page/3/index.html","hash":"146ac41cb82c21258186d3038f7d8116555c27b2","modified":1723898785076},{"_id":"public/archives/2024/page/4/index.html","hash":"1d27d620f3dc0cdb614d50f96a825f765ecdac78","modified":1723898785076},{"_id":"public/archives/2024/02/index.html","hash":"d4f7aa1b18ef064a93f8808a3b7e4d749f5216eb","modified":1723898785076},{"_id":"public/archives/2024/page/5/index.html","hash":"b7c6c7066b2136b3151736ed5144f12605f6de72","modified":1723898785076},{"_id":"public/archives/2024/03/index.html","hash":"99b4d95b45d6767cb9b9fd91ca12c30c4da7226d","modified":1723898785076},{"_id":"public/archives/2024/04/index.html","hash":"fdeae7819c80adbff509ba5222e06b23f1300595","modified":1723898785076},{"_id":"public/archives/2024/05/index.html","hash":"913da0f26230651be21e286b9517a321e69bbe7f","modified":1723898785076},{"_id":"public/archives/2024/06/index.html","hash":"3550eb749f0dbd8774c5c6697d85a84eab7db847","modified":1723898785076},{"_id":"public/archives/2024/07/index.html","hash":"4cfdcddc022a21bbad1bb7221331ac50ed1d6c9d","modified":1723898785076},{"_id":"public/archives/2024/07/page/2/index.html","hash":"c6cb9aef94044820e6c095c1c49b5edcb965cb7e","modified":1723898785076},{"_id":"public/archives/2024/08/index.html","hash":"22aa5e6e5a5e76ae2c263cc5bd0c442d545847de","modified":1723898785076},{"_id":"public/categories/CS/index.html","hash":"566749bc27082ab1296f43884f7bb7f381f95bc3","modified":1723898785076},{"_id":"public/categories/CS/page/2/index.html","hash":"8dd526176a4e9d52331b2210db040a65b734094c","modified":1723898785076},{"_id":"public/categories/CS/page/3/index.html","hash":"be426dabe58ef89a8ffff82da4a461dc0d81ce31","modified":1723898785076},{"_id":"public/categories/CS/page/4/index.html","hash":"4fe651d8d51170f635c72c7dd437900b45b0dbe0","modified":1723898785076},{"_id":"public/categories/CS/page/5/index.html","hash":"77aebc6efaa52935a617b7d5844c256bb05e7a26","modified":1723898785076},{"_id":"public/categories/CS/page/6/index.html","hash":"5d7f5148ed2f9d743d27f45d8b55740c3644c151","modified":1723898785076},{"_id":"public/categories/CS/NLP/index.html","hash":"2f5f5b73ac50f8f40aab6514013a2998dfc1a0ee","modified":1723898785076},{"_id":"public/categories/CS/NLP/page/2/index.html","hash":"c48bdd0f2060b81c9f0b8f0b17263e66cefc4a44","modified":1723898785076},{"_id":"public/categories/CS/NLP/page/3/index.html","hash":"9aaa58bbde44087d814aa0f4feb9472354b09bb3","modified":1723898785076},{"_id":"public/categories/CS/NLP/page/4/index.html","hash":"9ce788bdf2cdd33cb0887c4236923ec2e9c6a1b2","modified":1723898785076},{"_id":"public/categories/CS/NLP/page/5/index.html","hash":"836059fe3f5d0b22386ea6eecfb8ada793abb25e","modified":1723898785076},{"_id":"public/categories/CS/NLP/page/6/index.html","hash":"0b6b0d3c11da42a3505c0abec9fccfbce34d3db6","modified":1723898785076},{"_id":"public/categories/CS/NLP/LLM/index.html","hash":"73b281b788e93843c7277387bc492d60753aae23","modified":1723898785076},{"_id":"public/categories/CS/NLP/LLM/page/2/index.html","hash":"723cb4192ee77328d285c868f21ce2d69d406831","modified":1723898785076},{"_id":"public/categories/CS/NLP/LLM/page/3/index.html","hash":"3b64589edf8dfff7696874e4f543f005928d3d0f","modified":1723898785076},{"_id":"public/categories/CS/NLP/LLM/page/4/index.html","hash":"a3a51e17a442dac673155f6593f16951fd504da8","modified":1723898785076},{"_id":"public/categories/CS/NLP/LLM/page/5/index.html","hash":"e28e91baa1766e2f9fc95066268555011dd3d202","modified":1723898785076},{"_id":"public/categories/CS/NLP/LLM/page/6/index.html","hash":"804cf76eeb28ab97bc638512765be8bc66122fc3","modified":1723898785076},{"_id":"public/index.html","hash":"b66d5d2b73d48e9077dbf9bcc54325f8cb29dc82","modified":1723898785076},{"_id":"public/page/2/index.html","hash":"83816c636de12aacc47b05499102bb77dd82307e","modified":1723898785076},{"_id":"public/page/3/index.html","hash":"34095a037423a7c4ff2d7055a8d183202f65a21f","modified":1723898785076},{"_id":"public/tags/NLP/index.html","hash":"449eb56d255d3877262ac0c7c136eab6261ee5a5","modified":1723898785076},{"_id":"public/tags/NLP/page/2/index.html","hash":"fc8106626b00c5368e9fb8a49d609c217a30b8b5","modified":1723898785076},{"_id":"public/tags/NLP/page/3/index.html","hash":"875d836893a835d06ba280125f64efcc9f823964","modified":1723898785076},{"_id":"public/tags/NLP/page/4/index.html","hash":"dfcb7df2f65c367f487bf708bba2f10e0b8ee303","modified":1723898785076},{"_id":"public/tags/NLP/page/5/index.html","hash":"bdee835b17ac5dc996b1c81a6bc6e22713ce9553","modified":1723898785076},{"_id":"public/tags/NLP/page/6/index.html","hash":"fd1227b02287f10c5e21f58c68cbba5d18c9f5cb","modified":1723898785076},{"_id":"public/tags/LLM/index.html","hash":"2364684a3cae5e27620e61e0439ed1453485cc38","modified":1723898785076},{"_id":"public/tags/LLM/page/2/index.html","hash":"41aee47e821e2a47d0547a103c57826f0c68c389","modified":1723898785076},{"_id":"public/tags/LLM/page/3/index.html","hash":"43e3f17bfdea6e2226545ec45b3811f315617100","modified":1723898785076},{"_id":"public/tags/LLM/page/4/index.html","hash":"b6d07df4c6206d3af6705461cc306628645501aa","modified":1723898785076},{"_id":"public/tags/LLM/page/5/index.html","hash":"aa221bfdbf03599f9d260c67d536445d3e2b5c43","modified":1723898785076},{"_id":"public/tags/LLM/page/6/index.html","hash":"ab72791712f90d43158994ffdbbb05a2fe082f2c","modified":1723898785076},{"_id":"public/tags/transformer/index.html","hash":"59a9550b85b2ca575930995f16ddc5850e03f8a4","modified":1723898785076},{"_id":"public/tags/transformer/page/2/index.html","hash":"2159e38c70251787a3b9e5e5e107c1dd012b181b","modified":1723898785076},{"_id":"public/tags/transformer/page/3/index.html","hash":"8849ff53f8d2288fdfbbe111488dc275704b3634","modified":1723898785076},{"_id":"public/tags/transformer/page/4/index.html","hash":"bfe200a84196737be95c700a9b2eb974fc262285","modified":1723898785076},{"_id":"public/tags/transformer/page/5/index.html","hash":"42ada57b5f1c0e9c1de5a8cf5b7427a57d14ca92","modified":1723898785076},{"_id":"public/tags/positional-encoding/index.html","hash":"bcc75940bc41ae11c87ba98f6aa354a3cfd865d5","modified":1723898785076},{"_id":"public/tags/RoPE/index.html","hash":"521fff6f56bfe3cdbb92cc62620ca78c4a5397cc","modified":1723898785076},{"_id":"public/tags//index.html","hash":"eff74a460dca3c16f6ae9caed01428a48ebd984d","modified":1723898785076},{"_id":"public/tags//index.html","hash":"c983166aea4dc2ebe89ef575ecc7f0198c76f298","modified":1723898785076},{"_id":"public/tags//index.html","hash":"38d32133388714ef071e20921b83fc5b6d03f22f","modified":1723898785076},{"_id":"public/tags//page/2/index.html","hash":"8449280240f450cae90d59be03ee6c5d037fa9fc","modified":1723898785076},{"_id":"public/tags//index.html","hash":"9063891dab3e40f9ee32114cd1a564887ecd2066","modified":1723898785076},{"_id":"public/tags//index.html","hash":"3a11502f6ec0353524705ac5b1b4c83971c88852","modified":1723898785076},{"_id":"public/tags//index.html","hash":"27c1a531dd164fb4c642ce4bbb23ca5edc1c5561","modified":1723898785076},{"_id":"public/tags/agent/index.html","hash":"bfeb47f9cfecd3993652b13408fa0c8980d6ade8","modified":1723898785076},{"_id":"public/tags//index.html","hash":"c1c269041a1f91f3f47805b82f29e78ae340ef42","modified":1723898785076},{"_id":"public/tags//index.html","hash":"0a0c6cc836b70d9b76fa8aa98867ea7210cdcef6","modified":1723898785076},{"_id":"public/tags//index.html","hash":"641d7cdd5e77f85a79b7fa25cc303b8305e41e30","modified":1723898785076},{"_id":"public/tags//index.html","hash":"2c9e3dc796079e132afa744cbe9337cb9fd95988","modified":1723898785076},{"_id":"public/tags/SFT/index.html","hash":"937503cd7da1f10b981294f843609a1cae829dd9","modified":1723898785076},{"_id":"public/tags//index.html","hash":"bab0a7581cc6b10a38667cc1e2d6bf7a693c55da","modified":1723898785076},{"_id":"public/tags//index.html","hash":"9bf441a0b77bbcddb87e12c01a58094f02806d85","modified":1723898785076},{"_id":"public/tags//index.html","hash":"b0fb7c733d0def00aa884f95c204fe067b117d0c","modified":1723898785076},{"_id":"public/tags/MoE/index.html","hash":"565a36816851e8c1300bbf94a8b21ded0d1c9b89","modified":1723898785076},{"_id":"public/tags/attention/index.html","hash":"4641d9a31635f1d0815762959d4c849b791bfcc1","modified":1723898785076},{"_id":"public/tags/sliding-window-attention/index.html","hash":"b0a773ac180fd2511c60f570369e4fd6e4be80d3","modified":1723898785076},{"_id":"public/tags/sparse-attention/index.html","hash":"cfb8024c7ace0693770dffec8829c77c33fd2b0d","modified":1723898785076},{"_id":"public/tags/layernorm/index.html","hash":"38fd96d6ce7349053e7a72c5ddfcbd4e43d697a9","modified":1723898785076},{"_id":"public/tags/post-norm/index.html","hash":"33658a1228f96e6e06edd4eca9f298db80c2b97a","modified":1723898785076},{"_id":"public/tags/pre-norm/index.html","hash":"b0d38bbc13ca7d14b1dfc74dd47007292e8e7866","modified":1723898785076},{"_id":"public/tags/normalization/index.html","hash":"92db05343ea48f468224966ccd4b6b61ca6cfdff","modified":1723898785076},{"_id":"public/tags/batchnorm/index.html","hash":"9114736bb3e37cfe1fc573d0450487a0a04635b4","modified":1723898785076},{"_id":"public/tags//index.html","hash":"edbc7388644dd86de21997bbaa0f66c1658e5a7d","modified":1723898785076},{"_id":"public/tags/ChatGPT/index.html","hash":"88f274da11e80a38d72be644ed2fc7479714bc56","modified":1723898785076},{"_id":"public/tags/Sparrow/index.html","hash":"b136d67dae5ba368dc92812d6b384bb0972ac99c","modified":1723898785076},{"_id":"public/tags/LaMDA/index.html","hash":"c3f843da447edd62fe12f96a0b452e7cbbef9eee","modified":1723898785076},{"_id":"public/tags/GopherCite/index.html","hash":"4d88aafbc12df88d486e753496f141a600435542","modified":1723898785076},{"_id":"public/tags/WebGPT/index.html","hash":"0c3401177148ef8b38c95904e26a79416dc5da03","modified":1723898785076},{"_id":"public/tags/InstructGPT/index.html","hash":"d45316e57c2f3b53b894879117541b6305484dc4","modified":1723898785076},{"_id":"public/tags/KV-Cache/index.html","hash":"ed1668a69d35e3a539b49718e366baee5caab4d8","modified":1723898785076},{"_id":"public/tags//index.html","hash":"9cd6a332ae33c990bc81879cf4cc62242794ac8b","modified":1723898785076},{"_id":"public/tags/DeepSeek/index.html","hash":"2333cd94e07e2c2b0d7d954417f1f6ecf071ce94","modified":1723898785076},{"_id":"public/tags/MLA/index.html","hash":"bd3a14dabf469a140bfe54dc5fde51b8eac2e205","modified":1723898785076},{"_id":"public/tags/GQA/index.html","hash":"82be7b9a335c2b0a9ce98dddf0f145e4b6da4d5d","modified":1723898785076},{"_id":"public/tags/Gemma2/index.html","hash":"1a9a0231718f8e7ae86e7d49c4c11cc1e116201d","modified":1723898785076},{"_id":"public/tags/Meta/index.html","hash":"a52b4a4730f08790bb10a6ff5e3f241fc71ce98f","modified":1723898785076},{"_id":"public/tags/Llama/index.html","hash":"1a4020524087e7dfe5cf1a43473889e977f66b66","modified":1723898785076},{"_id":"public/tags/post-training/index.html","hash":"d9160721dc4b3c62515ad3a397ad1b7ff93cadc0","modified":1723898785076},{"_id":"public/tags/DPO/index.html","hash":"64bb569a09952687820027f20a4955ca80721c0b","modified":1723898785076},{"_id":"public/tags/RM/index.html","hash":"5c0030403b741e4250d0b2961e35a542c05a278e","modified":1723898785076},{"_id":"public/tags/RS/index.html","hash":"d9c5abf57aaeff682642ca5962ccf3ea99a84688","modified":1723898785076},{"_id":"public/tags/routing/index.html","hash":"c3260846e46c35a4ef9cc95ca104629874df0d46","modified":1723898785076},{"_id":"public/tags/Qwen/index.html","hash":"5319c2bacccac9eb3db19472249de894bd5b5609","modified":1723898785076},{"_id":"public/tags//index.html","hash":"f6fbff4948e694abd1c37131108055ecaea09890","modified":1723898785076},{"_id":"public/tags//index.html","hash":"2deb66dc51d043ad6b47c537aa1191756c345a9e","modified":1723898785076},{"_id":"public/tags//index.html","hash":"4b7b1fbaeeb4319bd9a6a88d6f30afa4bd5e725f","modified":1723898785076},{"_id":"public/tags//index.html","hash":"cd7a8ae6e699b68d714e92e2bedbbfbf14655952","modified":1723898785076},{"_id":"public/tags//index.html","hash":"7a7154cf67692967d0a2a2520d769efa64ccc24c","modified":1723898785076},{"_id":"public/tags//index.html","hash":"d1b93f5b82f52f8f52f710b28a06a05fb1676194","modified":1723898785076},{"_id":"public/tags/Bert/index.html","hash":"313a27c73728a7b539b50d33465f72e554bcbd08","modified":1723898785076},{"_id":"public/tags//index.html","hash":"9bdc4ba20060d17812c452d647238234ccc677cd","modified":1723898785076},{"_id":"public/tags//index.html","hash":"d936287e7979aee03973ae072bac9a29e318fdab","modified":1723898785076},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1723898785076},{"_id":"public/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1723898785076},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1723898785076},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1723898785076},{"_id":"public/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1723898785076},{"_id":"public/CNAME","hash":"07ad8fed7f1b810bf3715a0e68324e5cbb1d47ac","modified":1723898785076},{"_id":"public/images/avatar/20180303210737_XsJVr.jpeg","hash":"2ebbbcead4446a69b0ac0698ab227a1af49735ce","modified":1723898785076},{"_id":"public/images/cover.png","hash":"2f2aa6173619dd38425673ba110b50b9156d4d10","modified":1723898785076},{"_id":"public/images/qrcode.jpg","hash":"eab6c8893d0c79cf705cf64d738f560563365b0c","modified":1723898785076},{"_id":"public/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1723898785076},{"_id":"public/images/avatar/v2-d3367d08d5311ce843d5569776e5942e_1440w.webp","hash":"3e7db51e59a8261de04f55814475556494d153f6","modified":1723898785076},{"_id":"public/images/favicon/favicon_io/about.txt","hash":"d1f084751c81e98b1675b8c0745f6f306a1eaf66","modified":1723898785076},{"_id":"public/images/favicon/favicon_io/android-chrome-192x192.png","hash":"2164889376f2bd6f9dd2066ebdfe4abd547f49d9","modified":1723898785076},{"_id":"public/images/favicon/favicon_io/favicon-16x16.png","hash":"fb63ee2fc88b9ce42dec5bf68088da14be9350f5","modified":1723898785076},{"_id":"public/images/favicon/favicon_io/android-chrome-512x512.png","hash":"419f0b0317ad2a31acfa7dc19e1502731e92e05f","modified":1723898785076},{"_id":"public/images/favicon/favicon_io/favicon.ico","hash":"fceaa7158ae89a98fe5b93152dd7dec2d36c1ff4","modified":1723898785076},{"_id":"public/images/favicon/favicon_io/site.webmanifest","hash":"bf31baf91bdd2fcde24a45e3f2a1be33733c6f69","modified":1723898785076},{"_id":"public/images/favicon/favicon_io/apple-touch-icon.png","hash":"5e36b0bfe67bcf96f4f0e92366a5de509fe9a78a","modified":1723898785076},{"_id":"public/images/favicon/favicon_io/favicon-32x32.png","hash":"f42c950146a171afca4ad70ff425b5b4ff02f926","modified":1723898785076},{"_id":"public/a051710f/complex_number.png","hash":"255aa16a44913d53784b09d0a08762df6d21dc16","modified":1723898785076},{"_id":"public/f0902f1a/1.png","hash":"cb86a4ce4589b590aa1b220dbdf32f9db559a1b8","modified":1723898785076},{"_id":"public/f0902f1a/4.png","hash":"491537b72fcab5676e6322c47d9ae7e1054f3387","modified":1723898785076},{"_id":"public/f0902f1a/5.png","hash":"982701399546708e62d974938e35f17a6aa5abb5","modified":1723898785076},{"_id":"public/f0902f1a/2.png","hash":"8c8c02a78f8e874162574caa267c32f469665110","modified":1723898785076},{"_id":"public/f0902f1a/3.png","hash":"8bff531083b17baa6de7d44ecdf464c29d175a31","modified":1723898785076},{"_id":"public/c4da56c0/mix_precision_fp16.png","hash":"4da22fa80039f8271ffd839aba70f6b1aca7afe4","modified":1723898785076},{"_id":"public/c61d17e3/dilated_conv.png","hash":"bbc2ff2e9f891da4bfaf6d535ab8545acc18e8a6","modified":1723898785076},{"_id":"public/c61d17e3/mistral_large_performance.jpeg","hash":"54e3ed874802ac9465580d6b5fcc5d6c1de96244","modified":1723898785076},{"_id":"public/c61d17e3/receptive_field_cnn.png","hash":"46515aa3bce1eb0fc244f62ceee7b899c28183e8","modified":1723898785076},{"_id":"public/6a40bfa5/bn_and_ln.png","hash":"ff10e75b9d2c95aa713f013b104ed307f2121e57","modified":1723898785076},{"_id":"public/6a40bfa5/bn_ln_gn_in.png","hash":"9783c818f5e0eaea33d169718476bbe8874cf945","modified":1723898785076},{"_id":"public/6a40bfa5/lossfunc_surface.jpeg","hash":"c78a8df335da0d507963fb73a62fe2c3d145c91f","modified":1723898785076},{"_id":"public/6a40bfa5/realformer_attention.png","hash":"e9a92e5c07c8ca6873ea70671ad54eb2f1a13332","modified":1723898785076},{"_id":"public/6a40bfa5/rmsnorm.png","hash":"55bbcb42145011f7b5adf90cc613e22e2b94f060","modified":1723898785076},{"_id":"public/3dc22f96/MQA.webp","hash":"456a8ab19cc1564912034c375e8c3c5a42be6837","modified":1723898785076},{"_id":"public/3dc22f96/attention_calculation.png","hash":"1f020c39c78221e41c5c6953ef97239e9f42aa3c","modified":1723898785076},{"_id":"public/3dc22f96/cnn_heatmap.png","hash":"cb0bde73c9c4d0646133947ebaab16c44c753667","modified":1723898785076},{"_id":"public/3dc22f96/encoder.png","hash":"d6a3a39c420d90e50f02f8b34f127bfe34177331","modified":1723898785076},{"_id":"public/3dc22f96/decoder.png","hash":"28ee3d1ab68bd325ecb9d2066bc264a63d7de081","modified":1723898785076},{"_id":"public/3dc22f96/seq2seq.png","hash":"9baa57cc8000a918d0adca6dceaac3ea54791ea8","modified":1723898785076},{"_id":"public/3dc22f96/softmax.png","hash":"de80ba20e55abf7457cac958aa87627d0a7e5d77","modified":1723898785076},{"_id":"public/3dc22f96/seq2seq_attention.png","hash":"b95046eee028b45dd9734639ecde8189e93b2374","modified":1723898785076},{"_id":"public/f5c015c/formula.png","hash":"65fe200098b51d1712b6c38d039aa8be22d38e82","modified":1723898785076},{"_id":"public/336f2f3e/yarn.png","hash":"ee124a0823b429842082acebe78a7162915cc11c","modified":1723898785076},{"_id":"public/280fa97a/contingency_table.png","hash":"94b25e2d4803d9802d3c5455aed84911fe506089","modified":1723898785076},{"_id":"public/45ee1a6d/lm_infinite_attention_entropy.png","hash":"be91b6a49cfe30dd51ed4f8eb258eb4715a70e37","modified":1723898785076},{"_id":"public/45ee1a6d/lm_infinite_attention_logits_explode.png","hash":"3cc54ee973126c1ca3bcd85d75039e490efc9acf","modified":1723898785076},{"_id":"public/45ee1a6d/lm_infinite_middle_k.png","hash":"869d4b687714409a5a4b89c85dc5ee2c1f0c2c86","modified":1723898785076},{"_id":"public/280fa97a/reward_accuracy_compare.png","hash":"0b5526d61a1cbb3188e8e53ea858b1d9d1953660","modified":1723898785076},{"_id":"public/45ee1a6d/lm_infinite_starting_tokens_num.png","hash":"6a2c841a3d3fd354f57757aa7e663e83585545d6","modified":1723898785076},{"_id":"public/44e38c1b/qwen1.5_moe_tps.png","hash":"5478a6583a6c6fb68f1bc9429c103e84fe39efaf","modified":1723898785076},{"_id":"public/44e38c1b/softplus.png","hash":"bdc66c39227441390f2241b4f26c0b1fbab331d9","modified":1723898785076},{"_id":"public/44e38c1b/st_moe_round_error.png","hash":"0172ba008837b3490a3e456306aa72be65636d90","modified":1723898785076},{"_id":"public/44e38c1b/xiaomi_moe.jpg","hash":"d898ba33f1ee70efa136dbff3cb38983b461524f","modified":1723898785076},{"_id":"public/b70b4a2d/bn_and_ln.png","hash":"ff10e75b9d2c95aa713f013b104ed307f2121e57","modified":1723898785076},{"_id":"public/cf3f1f81/ablation_5.png","hash":"edf1a94db2e64c8b0122a8a3d2e2041784084b64","modified":1723898785076},{"_id":"public/cf3f1f81/ablation_4.png","hash":"f358308ba8c262ce443af4bcbd7643d32c729a6d","modified":1723898785076},{"_id":"public/2c8bbc7/cf.png","hash":"950b15a49298c1add50dcb13af2ad862dc895733","modified":1723898785076},{"_id":"public/2c8bbc7/dense.png","hash":"90a2d4c58443fbc6fe920ba25db4d4a5aba9e1aa","modified":1723898785076},{"_id":"public/2c8bbc7/dist.png","hash":"26b518e55aede62e9af5b91847ee2a0aa5e3c7d1","modified":1723898785076},{"_id":"public/3df0cd42/scalability.png","hash":"50158badd662c759906125f6ed46753ffe4b846e","modified":1723898785076},{"_id":"public/3df0cd42/train_hp.png","hash":"c81fbe8950dfafca0d814ed77c4a7d55cee935fd","modified":1723898785076},{"_id":"public/224c42da/diff_layer.png","hash":"21c3838f4b9efb0dbd14d5cf853061ab5f65f27b","modified":1723898785076},{"_id":"public/210dbccd/base_freq.png","hash":"9b4dd13f3dd6152011b2db58824826fb5d70ea82","modified":1723898785076},{"_id":"public/5e1d14b3/t2.png","hash":"901faf24fd46677982d4b6191fc05a2a1b39740f","modified":1723898785076},{"_id":"public/f845f3e4/formula.png","hash":"7a6299ae119624a92b0c2a198631673024924952","modified":1723898785076},{"_id":"public/fe13b56f/phi_2_0.png","hash":"6e820cf29184fb7cbe780be7058df8a5f23774b0","modified":1723898785076},{"_id":"public/images/avatar/v2-7775c13704e96ccf9f57a71451f40464_r.jpg","hash":"0629f15db217c8b7037a5b46ef187d79b9a66a09","modified":1723898785076},{"_id":"public/a051710f/remote_attenuation.png","hash":"90aa3d8f76c9b5c18de8b7740649b499241cab0c","modified":1723898785076},{"_id":"public/c4da56c0/meta_pi_nosft.png","hash":"d3b46ed19e458973c3586f3bf2ef906deb4080a8","modified":1723898785076},{"_id":"public/c4da56c0/rope_matrix.png","hash":"84372b91204098bab2ff09d2d865ad869f93bdbd","modified":1723898785076},{"_id":"public/f5fb75e4/eng_data.png","hash":"3d16e16eb4a57436ff9d2c0b64e7042944dbfab8","modified":1723898785076},{"_id":"public/images/avatar/v2-0ee0da4c6747bc62b9ea5b4c7b37d75b_r.jpg","hash":"6a3863489ed0982c7fe19d575eae28ba8c286ccf","modified":1723898785076},{"_id":"public/4fe7b810/curve.png","hash":"f81e68f050653f66ca087c2f13a222aca426384a","modified":1723898785076},{"_id":"public/f5fb75e4/exp1_param.png","hash":"5f940193d379f18bb36ac38f2bfa5d1aaef65b15","modified":1723898785076},{"_id":"public/7381cae3/5.png","hash":"8fa2b214c490a5262951301bbd4d0510d5199b3c","modified":1723898785076},{"_id":"public/dd614e12/lora.png","hash":"c044aa39cff915b64ee576c558756cb1093e5075","modified":1723898785076},{"_id":"public/7381cae3/ditto_1.png","hash":"bfe3f56dee05d35b5b816a356ec7688d81c33b92","modified":1723898785076},{"_id":"public/7381cae3/ditto_2.png","hash":"abd8e2173d83ff20cc0adc636163a97f7fc46de2","modified":1723898785076},{"_id":"public/f3acf042/model_param.png","hash":"a053d5204998fb025425706c16409d01f6589548","modified":1723898785076},{"_id":"public/7bbe2df6/speed.png","hash":"402da175e2e365db3cc0e6c377b89df5b8615b6c","modified":1723898785076},{"_id":"public/c61d17e3/mistral_architechture.png","hash":"5e4c347dc41d7f070f54b386fdccf675cfeb8f10","modified":1723898785076},{"_id":"public/c61d17e3/big_bird_attention.png","hash":"ed6c76b9bb77b98d34c429333498d04dac8e3ed9","modified":1723898785076},{"_id":"public/6a40bfa5/ellipse_2.png","hash":"ee20ecce8c3470d17b1a4bd43df811d16269ffd3","modified":1723898785076},{"_id":"public/6a40bfa5/prmsnorm.png","hash":"d5826342f665f4cb04fdbb2e3d83e0b2607355c9","modified":1723898785076},{"_id":"public/6a40bfa5/postnorm_prenorm.png","hash":"d8830735e89c73ca416baabf4a195d7891d9f0ed","modified":1723898785076},{"_id":"public/6a40bfa5/sigmoid.png","hash":"f1ced5f06861a2e0296050aff17eebfe3d023a6f","modified":1723898785076},{"_id":"public/41b6a819/model.png","hash":"631efc6d4e92da128d7a10a4dc6af307ee4ddcbf","modified":1723898785076},{"_id":"public/41b6a819/third_party.png","hash":"673fe2b2cad3b1f40c0fcfd190c0034d8dbe7f31","modified":1723898785076},{"_id":"public/3dc22f96/llama2_qga.png","hash":"5e0dea7d03de9144eb524a0a9adb102e91b52aaa","modified":1723898785076},{"_id":"public/3dc22f96/multihead_attention.png","hash":"6f8ee285f2646dc163b6b3164a0639aa9ddd7f27","modified":1723898785076},{"_id":"public/376db710/2_stage.png","hash":"ca7a6d52aa5dbf3afa1e00de6548386ad0ea738b","modified":1723898785076},{"_id":"public/3dc22f96/sram_dram.png","hash":"ae7a9296b67c02608460a334fbbad3781b890302","modified":1723898785076},{"_id":"public/376db710/batch_size_2.png","hash":"d2f8c192c90acd0ccba2ec814bd36fc2e646e6af","modified":1723898785076},{"_id":"public/376db710/cos_lr.png","hash":"951b0fc36a8b0e7b9c58a39345950546f090fb3a","modified":1723898785076},{"_id":"public/473f2b43/gradient.png","hash":"0b72939cfd4770fc21727bb203efb9c5dd2491d1","modified":1723898785076},{"_id":"public/473f2b43/result_3.png","hash":"df5861c846176c90bd9a90bd5836919ef023b13e","modified":1723898785076},{"_id":"public/f5c015c/acce_draft_model_param.png","hash":"2e5b1852eaf4745f3d9bfc9b0fcccbd37621bb93","modified":1723898785076},{"_id":"public/f5c015c/fi_choose_gamma.png","hash":"65be032bf276290ca97b7d983bc4e1e2deaa95fc","modified":1723898785076},{"_id":"public/f5c015c/fi_speed_and_op_table.png","hash":"416238792292bff7178830267d53941da202eadc","modified":1723898785076},{"_id":"public/336f2f3e/bfloat16.jpeg","hash":"8678b705b0d6e0b7deb230bf28f2d92ce0d42088","modified":1723898785076},{"_id":"public/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1723898785076},{"_id":"public/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1723898785076},{"_id":"public/css/main.css","hash":"6aea217c0462e6970606601a9fe39183cf15614c","modified":1723898785076},{"_id":"public/css/noscript.css","hash":"4cd5301e478e0e0d4b176740ec314087ec5cb707","modified":1723898785076},{"_id":"public/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1723898785076},{"_id":"public/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1723898785076},{"_id":"public/js/motion.js","hash":"770d63c26f22705311028a36b52e999cc8a2da82","modified":1723898785076},{"_id":"public/js/pjax.js","hash":"b03ba78c6916ad2f390d55bc1bc18fafb64b0ebf","modified":1723898785076},{"_id":"public/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1723898785076},{"_id":"public/js/schemes/muse.js","hash":"ba7ba2c129d1f240c6a22cec3e53f3f22af64b6b","modified":1723898785076},{"_id":"public/js/next-boot.js","hash":"745bd828205da7e5fbd3f860cc3697097630f5f3","modified":1723898785076},{"_id":"public/js/utils.js","hash":"d775148c2bf20c028622af5609a788167352bf1e","modified":1723898785076},{"_id":"public/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1723898785076},{"_id":"public/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1723898785076},{"_id":"public/js/third-party/addtoany.js","hash":"5276c8f78ee562a8965216dc67d762e59cb4a9f2","modified":1723898785076},{"_id":"public/js/third-party/fancybox.js","hash":"819f382c561fe5ec23c67cc5fabd63dd1cc22dc1","modified":1723898785076},{"_id":"public/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1723898785076},{"_id":"public/js/third-party/analytics/google-analytics.js","hash":"def07bcc7c17d8a0caad177fb1dd2f3a5e5b3536","modified":1723898785076},{"_id":"public/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1723898785076},{"_id":"public/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1723898785076},{"_id":"public/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1723898785076},{"_id":"public/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1723898785076},{"_id":"public/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1723898785076},{"_id":"public/js/third-party/comments/disqus.js","hash":"da361917d65e5dca8362f8cdeb6c8cc0e8316cec","modified":1723898785076},{"_id":"public/js/third-party/comments/disqusjs.js","hash":"1e826dea3f684c0515f362dc1352447a1f0eae71","modified":1723898785076},{"_id":"public/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1723898785076},{"_id":"public/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1723898785076},{"_id":"public/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1723898785076},{"_id":"public/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1723898785076},{"_id":"public/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1723898785076},{"_id":"public/js/third-party/search/algolia-search.js","hash":"fdb7b7cef1a147d897e7f7cd8903b58368ec2062","modified":1723898785076},{"_id":"public/js/third-party/search/local-search.js","hash":"4536cb6d0a9bbaaa86fab3fa0101f6a3a3ec5a76","modified":1723898785076},{"_id":"public/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1723898785076},{"_id":"public/js/third-party/statistics/lean-analytics.js","hash":"835cbf54c49ef1327f47df70ff2636ad36b6f57d","modified":1723898785076},{"_id":"public/js/third-party/statistics/firestore.js","hash":"6e0682bb42170d61b13b786295f45f9c785f8b73","modified":1723898785076},{"_id":"public/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1723898785076},{"_id":"public/js/third-party/tags/mermaid.js","hash":"6bf821310342c5b87a631873e7650a475a0765f1","modified":1723898785076},{"_id":"public/js/third-party/tags/wavedrom.js","hash":"40dcd10df6edf124088c329346e0cc0bdac74ef1","modified":1723898785076},{"_id":"public/1d5bcd45/gate_dist.png","hash":"912b7bf9c26345d2c1513050ac484efde65580b2","modified":1723898785076},{"_id":"public/1d5bcd45/lr_exp.png","hash":"32b3dda778f6dd42388ec8ad6f8782738068f54f","modified":1723898785076},{"_id":"public/280fa97a/dpo_correlation.png","hash":"2c1dafd42b7ffa3318395a4934df87692ba5fd62","modified":1723898785076},{"_id":"public/280fa97a/reward_accuracy.png","hash":"7fb3d4dd64e3013534bd77eb1f2def23ec57c8cd","modified":1723898785076},{"_id":"public/280fa97a/simpo_hyperparameters.png","hash":"f976162c0882c49b43b503beb1384b447e2d5d00","modified":1723898785076},{"_id":"public/280fa97a/simpo_contingency.png","hash":"773700b1d041aba8b3912deee9c8bc7886fec099","modified":1723898785076},{"_id":"public/45ee1a6d/lm_infinite_ppl_200m.png","hash":"7b232b7bf3c7238836f71b4b99e492d9f6c285f0","modified":1723898785076},{"_id":"public/45ee1a6d/stremingllm_kv_cache.png","hash":"11b09e96662feb7cc246e60e1b21d7ffceb47ae6","modified":1723898785076},{"_id":"public/44e38c1b/dbrx_long_perf_2.png","hash":"2b6be0099f1eeb0ee2d2056eae7cf541e146b636","modified":1723898785076},{"_id":"public/44e38c1b/dbrx_train_efficiency.png","hash":"c117407ada2adab8d97250268e2eafa533bb9083","modified":1723898785076},{"_id":"public/44e38c1b/mistral_8_7b_perf.png","hash":"f90d9ff5b14326b0eef2a0026b3f5940e0d42f0a","modified":1723898785076},{"_id":"public/44e38c1b/modular_connectionist.png","hash":"b5865cf34faba075b4f2316c2cae0559dac2d883","modified":1723898785076},{"_id":"public/44e38c1b/qwen1.5_moe_perf.png","hash":"b79ad1a909081fd0537bd9d44cfac2dc2133de6c","modified":1723898785076},{"_id":"public/44e38c1b/qwen1.5_moe_params.png","hash":"93d14a2645969b08a4fb80a31aa75fd8e5201ff8","modified":1723898785076},{"_id":"public/44e38c1b/rnn_moe_load_function.png","hash":"644684f21f85d565328d98334d41bbe019acbbfc","modified":1723898785076},{"_id":"public/44e38c1b/st_moe_more_add_bias.png","hash":"f0de5347918e4928dbcbc59a897c3f3227c3d30f","modified":1723898785076},{"_id":"public/cc852861/add_money.jpg","hash":"0b00f9f1dd128e5601f0c7502dd2cf9233898f0f","modified":1723898785076},{"_id":"public/b70b4a2d/cv_layernorm.jpeg","hash":"f0874ecc4b9d8da8bf3bff0e13a6313ed19a7b15","modified":1723898785076},{"_id":"public/7d7294cb/scaling_law_exp.png","hash":"097206df0cabb28e017489e79620a110b0d1fccf","modified":1723898785076},{"_id":"public/cf3f1f81/ablation_1.png","hash":"821874e7f72e2cfffd971a88a2391ea9738ae2a5","modified":1723898785076},{"_id":"public/cf3f1f81/ablation_2.png","hash":"9f3686e6478c43fdb04e4e2c84a628c4ed20ec8c","modified":1723898785076},{"_id":"public/cf3f1f81/ablation_3.png","hash":"e24a99a137b7a9375859772056fd40e97d16dcf5","modified":1723898785076},{"_id":"public/cf3f1f81/ablation_6.png","hash":"1899cec30c0a2bd2fec2dc611ac779b5cdbe1a57","modified":1723898785076},{"_id":"public/cf3f1f81/eval1.png","hash":"4ae5db8cf55139a708804797ebb8f0789a02f44c","modified":1723898785076},{"_id":"public/cf3f1f81/eval2.png","hash":"7f0c08d17c4d381a3ac8669298ad6bc540fad2e4","modified":1723898785076},{"_id":"public/cf3f1f81/format.png","hash":"af7622d7cdfdd24858750502eb084eb22d2236c7","modified":1723898785076},{"_id":"public/83c49df0/MLA_formula.png","hash":"4828e9a6496e9ccf53bbf7cd57743be3d30a59e2","modified":1723898785076},{"_id":"public/2c8bbc7/capped.png","hash":"6375daf2798ae89bd456cdf9c725886a198c6f67","modified":1723898785076},{"_id":"public/2c8bbc7/efficiency.png","hash":"5c418161fa8bcce6cae77f564ab417431cd89176","modified":1723898785076},{"_id":"public/2c8bbc7/intro.png","hash":"47bc8d61f20b0e79e35c9ec34291f98b7d2d8392","modified":1723898785076},{"_id":"public/2c8bbc7/expert_num.png","hash":"595b9231b589235236f3398fb59a5239f5ea688d","modified":1723898785076},{"_id":"public/a8f8b641/eval_chat_small.png","hash":"1f94215d3367d59d983b721c66cf1fa0e83adbc4","modified":1723898785076},{"_id":"public/a8f8b641/eval_long.png","hash":"9b3141566ae461e7ca25bd66e4347e23afd333aa","modified":1723898785076},{"_id":"public/3df0cd42/lfa_conv.png","hash":"1dbfdde181f9adc5647b1cf9e15afe2062d315f0","modified":1723898785076},{"_id":"public/3df0cd42/router_eval.png","hash":"cf2a0fce5e092f214d935368341eb3e65bc61315","modified":1723898785076},{"_id":"public/3df0cd42/yuan2_sft_hp.png","hash":"8e1669d6c366737f4db99d036559a8aaeb9ab700","modified":1723898785076},{"_id":"public/3df0cd42/yuan2_intro.png","hash":"a313198b675d2b41ef6ff294fd79c325910b3ae2","modified":1723898785076},{"_id":"public/770b63e1/add_instruct_data.png","hash":"f0b2304664a91d6ce6cbb22d7462ce96e45f4039","modified":1723898785076},{"_id":"public/224c42da/active_num.png","hash":"364d6bc9b77761a18eb7bac8b3309bd687465f73","modified":1723898785076},{"_id":"public/770b63e1/wsd_quality.png","hash":"bf688703fa4fb38f050058dffbcbb07312962845","modified":1723898785076},{"_id":"public/224c42da/diff_p.png","hash":"6d8acebbfac87d268798cb804dea84eda1f9636a","modified":1723898785076},{"_id":"public/224c42da/task_expert.png","hash":"1f1dd0d013485d0bd8dd6c9cbde71c43891bba03","modified":1723898785076},{"_id":"public/bb8fcf21/algo.png","hash":"cdccfde708ddda8df67bd061294422186e1f0c33","modified":1723898785076},{"_id":"public/bb8fcf21/swa_1.png","hash":"4408daa48c5fd8647ec41cbf172c7bea476671b4","modified":1723898785076},{"_id":"public/bb8fcf21/swa_2.png","hash":"a6a627d314bb4dd6572ff0d5c8a79e37486afbfc","modified":1723898785076},{"_id":"public/1e34e252/afm.png","hash":"294ccd4e500c7cb4dac9b2dc37bbeb481c46a10f","modified":1723898785076},{"_id":"public/1e34e252/distill.png","hash":"fd713a906b29c74220806378f665b0d3f8f32846","modified":1723898785076},{"_id":"public/1e34e252/recover.png","hash":"9b80bba89dd37bd6843d7119870597330d8d1fa6","modified":1723898785076},{"_id":"public/5e1d14b3/gating_2.png","hash":"f8f8fa62cf29d9fae40ccaf098481ed12218a23a","modified":1723898785076},{"_id":"public/5e1d14b3/gating_1.png","hash":"81db2d88499fd38eebce949218f49153710a6ed4","modified":1723898785076},{"_id":"public/f845f3e4/data.png","hash":"a8db4256f648dba59ad62f7304b3958d2044deb8","modified":1723898785076},{"_id":"public/f845f3e4/sft_hp.png","hash":"38c35794b1fd1a6ce3eea712f4fea9a0864c3447","modified":1723898785076},{"_id":"public/fe13b56f/phi_2.png","hash":"b4516bdd5f5c1ede1c9ec8ee655256e51ea5b857","modified":1723898785076},{"_id":"public/fe13b56f/phi_3_sparse.png","hash":"800bfb947e3383faf5a074d97c1836e41208eb53","modified":1723898785076},{"_id":"public/5ac36d34/emb.png","hash":"00573632286f7969ee67fc0069dc930fc1f200b2","modified":1723898785076},{"_id":"public/5ac36d34/model.png","hash":"e1409cafb709e2a7527c5a6b2e14886723416d78","modified":1723898785076},{"_id":"public/images/avatar/Picasso_Elephant.png","hash":"6d38d4b24c13a3fd506ee37a562126b76a24de83","modified":1723898785076},{"_id":"public/a051710f/rope.png","hash":"5535c87dbcfee3ed646db93662a0b6a1ccd3b31b","modified":1723898785076},{"_id":"public/c61d17e3/longformer_attention.png","hash":"64860379955872ecac5835b3f9d8c6d130c7e485","modified":1723898785076},{"_id":"public/c61d17e3/rolling_buffer.png","hash":"34d4db9f4855926db561faa80e934dd971c0974e","modified":1723898785076},{"_id":"public/a5206abd/all_tools.png","hash":"c6a831b6334ad229e219e0342264acb6522a8196","modified":1723898785076},{"_id":"public/6a40bfa5/bn_algo.png","hash":"56f1ab55c0e94814e6e37c30421012ed82098d62","modified":1723898785076},{"_id":"public/6a40bfa5/deepnorm.png","hash":"4726d8a40d1d0db397005408295e1ba54809a7e4","modified":1723898785076},{"_id":"public/6a40bfa5/deepnorm_result.png","hash":"138cafc159f1e2e02455c540b4754f7cbb7f521d","modified":1723898785076},{"_id":"public/6a40bfa5/ics_define.png","hash":"6bf3240ef78bad2cf76897a29c05428f4c195fba","modified":1723898785076},{"_id":"public/41b6a819/9B.png","hash":"7de19972e48b1f43c501d60a1c43a28c46b198e8","modified":1723898785076},{"_id":"public/41b6a819/long_context_result.png","hash":"daea6f734d64bdf5d24c6e17a640f23b1bd35b5f","modified":1723898785076},{"_id":"public/3dc22f96/mqa_result_1.png","hash":"a052b57f71eb78e3158ed2ee06ff0e5597607a2f","modified":1723898785076},{"_id":"public/376db710/exp_model.png","hash":"39ea7fc867afbeeaa1e764513ca21019f4076cc3","modified":1723898785076},{"_id":"public/376db710/data.png","hash":"86cde89044da233481994e8728ccb256146b3ae6","modified":1723898785076},{"_id":"public/376db710/learning_rate.png","hash":"e53cbb5a0d395e77bdc09779eb003134b7bbbe18","modified":1723898785076},{"_id":"public/376db710/layers.png","hash":"fc52ceb86ba2af1a3898f7afc18ab138d63c529d","modified":1723898785076},{"_id":"public/376db710/moe_result.png","hash":"e31fb8b893615de6f8a46c79d4cc7b813c7e9509","modified":1723898785076},{"_id":"public/376db710/tokenizer.png","hash":"c4e4116baccb9bdf88048b5e38827937ad48c045","modified":1723898785076},{"_id":"public/376db710/wsd_exp2.png","hash":"86256736d0e7b22c1f57778d3973202e4ac00f69","modified":1723898785076},{"_id":"public/da871ebe/alpha.png","hash":"4e3ad45447f5757d2cfcdf9d9555351233456c3b","modified":1723898785076},{"_id":"public/da871ebe/summarization.png","hash":"4934f3e42f20bc3a44ad07267e91a14c4c005543","modified":1723898785076},{"_id":"public/f5c015c/fi_expected_token_num.png","hash":"52be2409ca9513de2e5ce10a0e77d8aa98dfc328","modified":1723898785076},{"_id":"public/f5c015c/fi_speed_and_op.png","hash":"9b5e3a6c9276309e7aa5a8848d52ef361e62bb36","modified":1723898785076},{"_id":"public/f5c015c/fi_walltime.png","hash":"b645987bec587e743021bc330de277416ef36d5e","modified":1723898785076},{"_id":"public/1d5bcd45/100B.png","hash":"c9cc7f93992b0288d219a5926ba764860ed40c76","modified":1723898785076},{"_id":"public/1d5bcd45/diff_dense.png","hash":"541feaefbc8790820c45a5bf0317e69c69087c24","modified":1723898785076},{"_id":"public/1d5bcd45/structure.png","hash":"b71c6bfe51757237c56124d801bf0409c5d34b19","modified":1723898785076},{"_id":"public/280fa97a/margin_dist.png","hash":"cf82f48158e4ba3e503cbe25cc811910804489cf","modified":1723898785076},{"_id":"public/45ee1a6d/infini_attention_structure.png","hash":"35f958d9ba50460689727c7038bf3a344000fa52","modified":1723898785076},{"_id":"public/45ee1a6d/infini_attention_process.png","hash":"259ea90040b7a5b98a27afcb992a6bee31707ab9","modified":1723898785076},{"_id":"public/45ee1a6d/lm_infinite_downstream.png","hash":"192160ca6972003a61678e2f7f2467f5bbd94451","modified":1723898785076},{"_id":"public/44e38c1b/dbrx_long_perf_1.png","hash":"bc9c40bde860e78882965a25056a848aa4a89c77","modified":1723898785076},{"_id":"public/44e38c1b/ds_model_param.png","hash":"7b838274937cf45d73e59ac1fb5c2034e46586ae","modified":1723898785076},{"_id":"public/44e38c1b/ds_moe_expert_specialization.png","hash":"64947872486dd083a7076bfdfe67cb0626208579","modified":1723898785076},{"_id":"public/44e38c1b/ds_moe_less_activated_expert.png","hash":"ac7ad86dabe94564bdb17399231c6ddc7da83962","modified":1723898785076},{"_id":"public/44e38c1b/glam_compare_gpt3.png","hash":"311b21079599473054378e339885e0b87719e63e","modified":1723898785076},{"_id":"public/44e38c1b/mistral_8_7b_active_perf.png","hash":"0c67d935657e62b9e8eeabc6403c269e09016626","modified":1723898785076},{"_id":"public/44e38c1b/rnn_moe.png","hash":"2a2ee095b8cc0727daa2cdd0c63891e4a470eae8","modified":1723898785076},{"_id":"public/44e38c1b/rnn_moe_137b.png","hash":"fc11cfc87b2994956a9adbee76621fe5d964dc30","modified":1723898785076},{"_id":"public/44e38c1b/st_moe_more_add_noise.png","hash":"12d13a9ee6c28553626e469ed18d022e0176a873","modified":1723898785076},{"_id":"public/44e38c1b/st_moe_remove_multiplications.png","hash":"52bc94bfe726dfc832534dde409154efe0ce7b0f","modified":1723898785076},{"_id":"public/44e38c1b/st_moe_z_loss_result.png","hash":"3edd8e9eb069c9557c98fd21600c02b3a1978cc5","modified":1723898785076},{"_id":"public/44e38c1b/vanilla_moe.png","hash":"7da0a6e9d529256107b5f6b287737ac47513a797","modified":1723898785076},{"_id":"public/cc852861/paraphrasing_quality.png","hash":"fab44d68fc27f7bb2c06f758e537b9b249be0699","modified":1723898785076},{"_id":"public/b70b4a2d/cv_batchnorm.png","hash":"d9e8d897c36125fddcf1cbcfa5c237a37158a939","modified":1723898785076},{"_id":"public/7d7294cb/llama3.png","hash":"19935286a22c8072320836e2c6675fcdbdf785d3","modified":1723898785076},{"_id":"public/7d7294cb/model.png","hash":"238262c4ef4ff2087b014c326de75fdc2151c1f2","modified":1723898785076},{"_id":"public/cf3f1f81/intro.png","hash":"ea39460a533562e1bb3cb87e4b083d230b4099c8","modified":1723898785076},{"_id":"public/83c49df0/GQA_compare_MHA.png","hash":"d1836492efcad8eb01f0c610db6f833dea52f565","modified":1723898785076},{"_id":"public/83c49df0/MLA.png","hash":"18fad673827880185f8fdfdf60bf759ecb70fd53","modified":1723898785076},{"_id":"public/83c49df0/needle.png","hash":"5196cd7771b84b66a28073111ad5a899e48bc72a","modified":1723898785076},{"_id":"public/93328a2a/code_sample.png","hash":"cb15e79a41f9c45426d90ccbb8a97354384e2f50","modified":1723898785076},{"_id":"public/93328a2a/post_training.png","hash":"8f02f967b378a22f067d5d7fb67a313624fdea68","modified":1723898785076},{"_id":"public/2c8bbc7/model.png","hash":"b24ba505952a804939e7e63f7676a20176609bed","modified":1723898785076},{"_id":"public/a8f8b641/eval_needle.png","hash":"ca90761b48b146b4596267505f483f630980fe2b","modified":1723898785076},{"_id":"public/3df0cd42/eval1.png","hash":"209e715214321dbd5879358fd4c390c2ec9e4a76","modified":1723898785076},{"_id":"public/3df0cd42/eval2.png","hash":"3bb02a3242fa75c34446178d4535a5ac391b7b31","modified":1723898785076},{"_id":"public/3df0cd42/eval4.png","hash":"1bd8cd0e7c2a612a9039a0c2274f353cf53a2c7a","modified":1723898785076},{"_id":"public/3df0cd42/eval3.png","hash":"a10cdbe9b06a9d2b18b4a3ab28a2e25253ddbaf8","modified":1723898785076},{"_id":"public/3df0cd42/yuan2_chat_data.png","hash":"d69c7854d75ebd7751fbeaf0a54cf029120dd7f5","modified":1723898785076},{"_id":"public/770b63e1/layer_num.png","hash":"315b4bb529fe45846d624a77bf23f199e041b59a","modified":1723898785076},{"_id":"public/770b63e1/sft.png","hash":"f263860e110bd216de9296353a6e8cd56a5e3d2c","modified":1723898785076},{"_id":"public/224c42da/perf.png","hash":"3e30d83a8bf735f3cd52566a3c2fd49947608c87","modified":1723898785076},{"_id":"public/bb8fcf21/method_soup.png","hash":"2356fc6694f2860802080383174efebc00d7ef23","modified":1723898785076},{"_id":"public/bb8fcf21/swa_3.png","hash":"1918cc7b7d5f1110a3160ce80962316659424abb","modified":1723898785076},{"_id":"public/a0824e29/intro.png","hash":"1e9401f1b85dc3387404472ecbd99bd9b0defed3","modified":1723898785076},{"_id":"public/e287b9c3/1.png","hash":"7d4bc4e879e853edefdee49bc33f71a4c87b31db","modified":1723898785076},{"_id":"public/5e1d14b3/norm.png","hash":"c8df60632519a86b3d0d5e719d1e98ae13762f6b","modified":1723898785076},{"_id":"public/5e1d14b3/models.png","hash":"7d7cc95caf7ee900d27616f24c61a54bd3104c3e","modified":1723898785076},{"_id":"public/210dbccd/efficiency.png","hash":"f8b65167254c4f67b4f8d521fa957550614c5517","modified":1723898785076},{"_id":"public/210dbccd/buckets.png","hash":"a80c242c4904a5c833819646cb5b41fdf6a1a3e6","modified":1723898785076},{"_id":"public/210dbccd/scaling.png","hash":"cf485430efb7e14fe412f435266d84c7311815b5","modified":1723898785076},{"_id":"public/f845f3e4/eval_1.png","hash":"1ec3d8dec7bd24bf84e71902747ad36eee8b91d0","modified":1723898785076},{"_id":"public/f845f3e4/intro.png","hash":"553dcf79eaab0fd25a64088f4e2054da038362d5","modified":1723898785076},{"_id":"public/fe13b56f/phi_15_result.png","hash":"6affba40c95f658fa3a15edead4b97c3e6a98da2","modified":1723898785076},{"_id":"public/fe13b56f/phi_1_example_1.png","hash":"8776dbb337b27badc563e3c291dc27c55a7e6cb9","modified":1723898785076},{"_id":"public/fe13b56f/phi_1_example_2.png","hash":"959e4fce596299f6cf5c9a59c06d2d040d119bdd","modified":1723898785076},{"_id":"public/fe13b56f/phi_2_1.png","hash":"2245f0e52eb229591a8becee9d21d99d8247d594","modified":1723898785076},{"_id":"public/fe13b56f/phi_2_3.png","hash":"cf8dd9735a77c25f5b9e8c32f3652cd0bf6e71fb","modified":1723898785076},{"_id":"public/5ac36d34/device.png","hash":"7dac5432f851444e63b4a50fbe5267eb4d6d48c7","modified":1723898785076},{"_id":"public/5ac36d34/repeat.png","hash":"a71018f5c9a41981946db03d58636cd65afbf163","modified":1723898785076},{"_id":"public/5ac36d34/kd.png","hash":"7e7595e30d05dfbcc3b5b387800baf71ce01a58f","modified":1723898785076},{"_id":"public/5ac36d34/share_2.png","hash":"96d02323048ab12d5d85d7ba2e83c983d2d1bfaa","modified":1723898785076},{"_id":"public/f5fb75e4/downstream_dataset_num.png","hash":"ac409ad0cd39c968f3d59a0ab7d4e75f9922d682","modified":1723898785076},{"_id":"public/f3acf042/data1.png","hash":"050a2ebcc9784a20ca811c99215b137c8257c0c7","modified":1723898785076},{"_id":"public/f3acf042/structure.png","hash":"61969133bdb9770d5139d27374eb3e3f4cc44d0e","modified":1723898785076},{"_id":"public/c61d17e3/prefill_and_chunking.png","hash":"0c706e0728ea462b2b00c59a97c79ccf5f05b598","modified":1723898785076},{"_id":"public/6a40bfa5/bs_bn.png","hash":"aa28241d75f914603b9f7f67cc54db4e61bac668","modified":1723898785076},{"_id":"public/6a40bfa5/rmsnorm_eff.png","hash":"350a7a2703eef1ee9357609ae5820bfc30835681","modified":1723898785076},{"_id":"public/41b6a819/base_model_eval.png","hash":"9b4e65d246865683f8e3348d74bfad03c937b65f","modified":1723898785076},{"_id":"public/41b6a819/perf.png","hash":"3c068a423dcd32cac7f1630bd69fbe5a4c6789af","modified":1723898785076},{"_id":"public/41b6a819/sft.png","hash":"ea9aea143af836012f44d21956ab5455487e9bfb","modified":1723898785076},{"_id":"public/3dc22f96/GQA.png","hash":"d6b0cd20ae5f68a96f0865b3d0b6b129969caa4b","modified":1723898785076},{"_id":"public/3dc22f96/Scaled-dot-product-self-attention.pbm","hash":"03da711b1547c944deea60d9bf345eb30e7c566f","modified":1723898785076},{"_id":"public/3dc22f96/GQA_result_1.png","hash":"87f2c3632fdf83828e8bd07a95cd8e7bf277fc88","modified":1723898785076},{"_id":"public/3dc22f96/gpu_cache.png","hash":"edb6b1abdecd3099f2d68c2a729c0ca9b1fb0db7","modified":1723898785076},{"_id":"public/3dc22f96/transformer_structure.png","hash":"87f0258e43922eface0277e13167a4ba8c1402bd","modified":1723898785076},{"_id":"public/376db710/train_loss.png","hash":"ac1261a27009436fae1bf50412017e8f25ea7d38","modified":1723898785076},{"_id":"public/376db710/wsd_update.png","hash":"bdee7c642152f421d099748d5afd6570b7c99a5a","modified":1723898785076},{"_id":"public/473f2b43/result_4.png","hash":"6f554d1b6911c3db211a7e57e886e62f03b46ffd","modified":1723898785076},{"_id":"public/f5c015c/fi_t5_result.png","hash":"83b63aafceeb8f2bc3f89ee6a1e3caec7987a1c9","modified":1723898785076},{"_id":"public/1d5bcd45/lr_result.png","hash":"96674fe834c8223d29136a4c8d36abf28cb3c195","modified":1723898785076},{"_id":"public/280fa97a/benchmark.png","hash":"83084a5f64006000898d5252b3f8afccb635b3f0","modified":1723898785076},{"_id":"public/280fa97a/ablation.png","hash":"4c73c83eb527141e17d1109b6c2cff3488de6259","modified":1723898785076},{"_id":"public/280fa97a/intro.png","hash":"3ff9cce772cd825ec8b88591d576a5f52982d679","modified":1723898785076},{"_id":"public/45ee1a6d/infini_attention_booksum.png","hash":"276dfa014d35f7d3375fbb7cee6eed21127f9955","modified":1723898785076},{"_id":"public/45ee1a6d/infini_attention_language_modeling.png","hash":"a49a7cc02694e7017c2a20dc0666046108b3c4c5","modified":1723898785076},{"_id":"public/45ee1a6d/infini_attention_passkey.png","hash":"811c5c677f7616b6625a0b86f2004f6d3ebeefe9","modified":1723898785076},{"_id":"public/45ee1a6d/lm_infinite_ablation.png","hash":"babe57024a1c2212405220124dfd376a8a2bcfb6","modified":1723898785076},{"_id":"public/45ee1a6d/stremingllm_attention_sink.png","hash":"933f34f3bc1d04e2b36f3305ac9fe2acd8bc9939","modified":1723898785076},{"_id":"public/45ee1a6d/xl_vanilla_sw.png","hash":"3259a751066a0083ef249a0412f43fb582e6544c","modified":1723898785076},{"_id":"public/44e38c1b/cover.jpeg","hash":"6226a5276377816b37a20572a8b725af3ddf5760","modified":1723898785076},{"_id":"public/44e38c1b/dbrx_vs_closed_models.png","hash":"6211c05b7d69194f2d820622525273110467a0d5","modified":1723898785076},{"_id":"public/44e38c1b/dbrx_vs_open_models.png","hash":"ba0348d3fe68a27f8c6435c4c3a6d08d9c8869c6","modified":1723898785076},{"_id":"public/44e38c1b/ds_2b_less_expert.png","hash":"e03a00a194efd33890517b4ad642bca5566cf9df","modified":1723898785076},{"_id":"public/44e38c1b/ds_moe_ablation.png","hash":"108dc8d66ae0e370e7969403efd85178b9a8523a","modified":1723898785076},{"_id":"public/44e38c1b/glam_family.png","hash":"9d813f12f82d8702886f7ede72c5a25151390ba9","modified":1723898785076},{"_id":"public/44e38c1b/glam_related_model.png","hash":"0f109231fc1b425c7364401c41ce5f3aecfd76c7","modified":1723898785076},{"_id":"public/44e38c1b/gshard_moe_family.png","hash":"21a6c80f1dac39eb364fb417ed83afde2b212675","modified":1723898785076},{"_id":"public/44e38c1b/mistral_8_22b_multiling.png","hash":"4af49ffc09a0de3793ec7137d3dfbddc9c309d38","modified":1723898785076},{"_id":"public/44e38c1b/st_moe_capacity_factor_speed.png","hash":"60b624b7595e1f90763ea745ea358b6852eeefb0","modified":1723898785076},{"_id":"public/44e38c1b/st_moe_more_dense_layer.png","hash":"67c37482d73cd7ece7e384c0bd73e6891fc752e1","modified":1723898785076},{"_id":"public/44e38c1b/switch_transformer_distill_sft.png","hash":"7a59660f367634d68aa392698042f3d9cfe190da","modified":1723898785076},{"_id":"public/44e38c1b/switch_transformer_dropout.png","hash":"63f36ca61aaa71e88ddf23339e63a9ccd898a6ce","modified":1723898785076},{"_id":"public/44e38c1b/switch_transformer_init.png","hash":"f9fb36a0defac7a5990b5c58a614f3165af0ae3e","modified":1723898785076},{"_id":"public/cc852861/eng_ppl.png","hash":"fccca1509ebab2e89a3ceaad0dfeedc700de2691","modified":1723898785076},{"_id":"public/cc852861/paraphrasing_lost.png","hash":"b9d73b8022266af17789ab049c7adda621729cc9","modified":1723898785076},{"_id":"public/cc852861/paraphrasing_dataset_dist.png","hash":"e5754afcb70a45c0d11ee5db43c724350ec64257","modified":1723898785076},{"_id":"public/1736008/transformer.png","hash":"9dddf171ca51f2ed1218baa9b84f4b98e9b911cf","modified":1723898785076},{"_id":"public/7d7294cb/scaling_law.png","hash":"fb2483e67bc86a0c17de9ab662ad7f92abdbf6aa","modified":1723898785076},{"_id":"public/cf3f1f81/model.png","hash":"e78cbe64d1c56722e7be47a4da30ba240100623c","modified":1723898785076},{"_id":"public/83c49df0/GQA.png","hash":"d6b0cd20ae5f68a96f0865b3d0b6b129969caa4b","modified":1723898785076},{"_id":"public/cf3f1f81/example.png","hash":"71cbc6c7f81bb102a23a5faa1d21f3908c7a3513","modified":1723898785076},{"_id":"public/83c49df0/MLA_perf.png","hash":"b4aca26e5d41a48a7135fa9c7ca787a634847a01","modified":1723898785076},{"_id":"public/83c49df0/lite_eval_1.png","hash":"58696a02bf5b8c9c535ea4d6020a9f342553a303","modified":1723898785076},{"_id":"public/83c49df0/MLA_cache.png","hash":"5fc118f53be86ddca4d7dc3783bc975eefd4e7c0","modified":1723898785076},{"_id":"public/83c49df0/lite_eval_2.png","hash":"dc8a1babc93ab353af6083cb960e7fa06bd48e41","modified":1723898785076},{"_id":"public/93328a2a/preference_data.png","hash":"197c3f37d5bf8684e1f8e60fc469794b2b78697d","modified":1723898785076},{"_id":"public/93328a2a/sft_data.png","hash":"4e0572823d184079fd67e6937fe28127b0c72402","modified":1723898785076},{"_id":"public/a8f8b641/eval_base_small.png","hash":"c7d77e2bbba4785f385a61c92f276b9e8a5ea4c1","modified":1723898785076},{"_id":"public/a8f8b641/eval_chat_7B.png","hash":"38e13d8d0c4f20fc0578405bd7036ad4432eabcf","modified":1723898785076},{"_id":"public/a8f8b641/eval_chat_large.png","hash":"9b97aa7c0ce7dbeaccb2d7b0e01324a959c1da0e","modified":1723898785076},{"_id":"public/3df0cd42/lfa.png","hash":"a332fcea93d56e4832cbeb35349b3ade8d02be28","modified":1723898785076},{"_id":"public/a8f8b641/model.png","hash":"3df6aa4cd1c5e1ad95443ec86babd64cc1ac9567","modified":1723898785076},{"_id":"public/3df0cd42/router.png","hash":"5f4b3283dfb386ea62f77e902b03ace2e416c515","modified":1723898785076},{"_id":"public/3df0cd42/lfa_result.png","hash":"07c79f825aee4660d2463c67d010c5e5b63d7b24","modified":1723898785076},{"_id":"public/3df0cd42/pretrain.png","hash":"489a86abddbf895fde11c7bf1a4467eb58d5d6bf","modified":1723898785076},{"_id":"public/3df0cd42/yuan2_train_curve.png","hash":"ca231c0756f06345f1d4d4082dcf07ce5ef646dd","modified":1723898785076},{"_id":"public/770b63e1/dpo.png","hash":"04dc7be1370c5cdbc5111e92e2ca2bc23d211d5e","modified":1723898785076},{"_id":"public/770b63e1/pt_data.png","hash":"d0f2a86f5aecec1fa98182e29aeeadb220d5a3b9","modified":1723898785076},{"_id":"public/224c42da/top-p.png","hash":"fbbbbe2b9eae52191cdba27e719c69e02142c4f6","modified":1723898785076},{"_id":"public/bb8fcf21/angle_2.png","hash":"ba71676087d1d7aa5d9ed9d62f88059c8fe66c9c","modified":1723898785076},{"_id":"public/a0824e29/1.png","hash":"dcf85b619aa60b05d620fd8e678214bb0d019144","modified":1723898785076},{"_id":"public/a0824e29/3.png","hash":"a23ad2ad4d57e25c08ba703854d57844e504b63f","modified":1723898785076},{"_id":"public/a0824e29/2.png","hash":"bf630562a1d0eeccb04c30a4e67e77f5317f5b90","modified":1723898785076},{"_id":"public/a0824e29/4.png","hash":"f0a158f0265c03c02f47e7233217f9adc8529346","modified":1723898785076},{"_id":"public/a0824e29/a6.png","hash":"7a14072bc7f92c53020d4313979663a08b55a971","modified":1723898785076},{"_id":"public/a0824e29/models.png","hash":"f8e96129c9c6bcc6a2a58ed11f40823b5987aaf3","modified":1723898785076},{"_id":"public/1e34e252/intelligence.png","hash":"d4a34111d688ddf8b0a90ecc7a718586d5201d9e","modified":1723898785076},{"_id":"public/1e34e252/pretrain_2.png","hash":"89df5a5e4edb32a9c07efdced175841f230845a6","modified":1723898785076},{"_id":"public/1e34e252/core_ablation.png","hash":"668ab6946ee526e5cacd5f8e33f1013a7a872f9d","modified":1723898785076},{"_id":"public/210dbccd/bias.png","hash":"3930d3dff31e035529184319260775244fa6d009","modified":1723898785076},{"_id":"public/210dbccd/curriculum.png","hash":"f6dab28a9f334ec95bf4d49832d5c1bbc9eb70d1","modified":1723898785076},{"_id":"public/210dbccd/mixture.png","hash":"cdaf52015913c3710c9124e586fe8d0a68777df4","modified":1723898785076},{"_id":"public/210dbccd/sota.png","hash":"4d9a25476bee1b70550a28d9da5f9d4ff610d381","modified":1723898785076},{"_id":"public/f845f3e4/pretrain_hp.png","hash":"bb930adb3aeb440983e7e30f373e80ba59187690","modified":1723898785076},{"_id":"public/9c593ccd/xiaomi_1.png","hash":"f1cdd75d772a8954f06ed17fb794654e3cddbb8d","modified":1723898785076},{"_id":"public/fe13b56f/phi_15_bench_3.png","hash":"75184881a16d9f3294b1c02f136ffb47f0caca14","modified":1723898785076},{"_id":"public/fe13b56f/phi_15_bench_1.png","hash":"20f6e497f733737f3df51d2981976b7899e835f5","modified":1723898785076},{"_id":"public/fe13b56f/phi_15_bench_2.png","hash":"d33d4349718c81882c7785b708fd96897f49c0b8","modified":1723898785076},{"_id":"public/fe13b56f/phi_2_2.png","hash":"e91aad82f0d366705bfe79f5f362c67316e40aa4","modified":1723898785076},{"_id":"public/fe13b56f/phi_3_result.png","hash":"146a28ca3d66b673a84ca47c94eb98d820222d74","modified":1723898785076},{"_id":"public/5ac36d34/mobilellm.png","hash":"3dd06feb9934a7397d234e1121fb6ad6ef96fe31","modified":1723898785076},{"_id":"public/5ac36d34/share.png","hash":"d003ad24bed7c72dbf3178c71d019db5c9f2bd17","modified":1723898785076},{"_id":"public/images/background/wallhaven-2ywymm.png","hash":"aeaf089b64472a1c72cdd9778c5deac4086eb38f","modified":1723898785076},{"_id":"public/c4da56c0/meta_pi_explanation.png","hash":"df22d646e5a47e6223b7722007cfb7dde7c27280","modified":1723898785076},{"_id":"public/f5fb75e4/exp3_plot.png","hash":"5bf2e800c8583d545e30a084400ec34eca436025","modified":1723898785076},{"_id":"public/7381cae3/4.png","hash":"f9c2e248f1416369b19e434fd28e7160f85c9d22","modified":1723898785076},{"_id":"public/7bbe2df6/construct_tree.png","hash":"ba3dc50c0c35e13fb15e18ad7748494d7e20f532","modified":1723898785076},{"_id":"public/7bbe2df6/exp1.png","hash":"1d2454407467f240c2fa549a73589bcfa462a78b","modified":1723898785076},{"_id":"public/7bbe2df6/threshold.png","hash":"fb52a6f66fddabc8ae9f72d027f4605178f65fb1","modified":1723898785076},{"_id":"public/7bbe2df6/tree_attention.png","hash":"c3c0b10e0bd043235b307ebddb730b43a84fded4","modified":1723898785076},{"_id":"public/6a40bfa5/warmup_effect.png","hash":"3e936786065e1ab9cbad17f5b86a5b8129720270","modified":1723898785076},{"_id":"public/41b6a819/eval.png","hash":"e78d1d820de4c455b9301124d6016a19762eae1f","modified":1723898785076},{"_id":"public/41b6a819/multimodal.png","hash":"b14a4eb4d377101acf7b50904b9ee0f1d473aacc","modified":1723898785076},{"_id":"public/41b6a819/pretrain_data_dist.png","hash":"8c66a625723cb87ee67a9ff60d3614b369f50592","modified":1723898785076},{"_id":"public/376db710/128k_result.png","hash":"97987dc9ae6ef342ce5bb4a34072a558b11a8770","modified":1723898785076},{"_id":"public/376db710/cos_loss.png","hash":"2b5a94aea83ca359aac95432298a8b32b29672b0","modified":1723898785076},{"_id":"public/376db710/wsd_exp1.png","hash":"2c556e50353e6b1528918f310f3150afdfd2f549","modified":1723898785076},{"_id":"public/473f2b43/result_2.png","hash":"6baa634147220fed9edfff7c70e83c56a2b24913","modified":1723898785076},{"_id":"public/da871ebe/odpo_intro.png","hash":"e32faada4824a2654e32d125cb7dded9895f87dc","modified":1723898785076},{"_id":"public/da871ebe/toxicity_control.png","hash":"42173484be1fd33e29244f43d658ba03ec9bacb2","modified":1723898785076},{"_id":"public/f5c015c/fi_sd_algo.png","hash":"576ffae274518c5a4e6c049d199e0714b06bba86","modified":1723898785076},{"_id":"public/336f2f3e/ntk_by_parts.png","hash":"5b49750dc6a2d1b878f34bc71e3961d96282499a","modified":1723898785076},{"_id":"public/45ee1a6d/lm_infinite_design.png","hash":"e96faad18cb526d201ce069f9ce09dc3c9c0d16e","modified":1723898785076},{"_id":"public/45ee1a6d/stremingllm_exp.png","hash":"cdd21419055ca9bed86b46675e118ad4bdf55544","modified":1723898785076},{"_id":"public/45ee1a6d/stremingllm_init_token_num.png","hash":"ffe78c31d901311249530e786bc5ed321e2e242f","modified":1723898785076},{"_id":"public/45ee1a6d/stremingllm_perf_4m.png","hash":"c4545d7f0bbd0c5f6baa85dd64b747a3723fdf2e","modified":1723898785076},{"_id":"public/44e38c1b/dbrx_infer_efficiency.png","hash":"34245e99c2b29dbd54104ccbbb3d8c15706307b9","modified":1723898785076},{"_id":"public/44e38c1b/rnn_moe_specilized.png","hash":"138ad5a388c77cc02202de794ec4cb734d633065","modified":1723898785076},{"_id":"public/44e38c1b/st_moe_capacity_factor.png","hash":"a960540d3419de77c3823d343247abfddeadde1c","modified":1723898785076},{"_id":"public/44e38c1b/rnn_moe_perf.png","hash":"ca3c95d4b1c1c8f986d7adcacbf04da444e91610","modified":1723898785076},{"_id":"public/44e38c1b/st_moe_models.png","hash":"bc59770bc8ea44bfe480484a99aa9143acbaa6fa","modified":1723898785076},{"_id":"public/44e38c1b/st_moe_multiling_specialization.png","hash":"bac636d8da61768adb5a9b7c5bd75547267ae470","modified":1723898785076},{"_id":"public/cc852861/eng_config.png","hash":"8edac537bd1aefea28406c414e0c0a4c888234be","modified":1723898785076},{"_id":"public/44e38c1b/switch_transformer_structure.png","hash":"d906a9148e035025683e8da1eee5fa3d87164aa5","modified":1723898785076},{"_id":"public/cc852861/pose_ppl.png","hash":"fded043b94c97c1a782869963f5dea371e257b80","modified":1723898785076},{"_id":"public/cc852861/pose_passkey.png","hash":"6202690a895f0114c90f6821c8cf1ad7388e1592","modified":1723898785076},{"_id":"public/cc852861/pose_method.png","hash":"db8784c9e4b14c5963f62f072df6a4c3c5405874","modified":1723898785076},{"_id":"public/83c49df0/intro.png","hash":"0317287e02c626e3bae72f5af4100306df572329","modified":1723898785076},{"_id":"public/a8f8b641/eval_base_7B.png","hash":"6ecd155b46dd98bf0931b210a1ada116d6a214b5","modified":1723898785076},{"_id":"public/3df0cd42/yuan2_pretrain_data.png","hash":"dc700e56e4f28f0a6f42be843625b7cfd9181568","modified":1723898785076},{"_id":"public/a0824e29/a2.png","hash":"38a3c1b163ab3783dde39921808686bf108ec14f","modified":1723898785076},{"_id":"public/a0824e29/a5.png","hash":"6d442fca2b0393119dfdefbeaa72798b6e156491","modified":1723898785076},{"_id":"public/1e34e252/pretrain_1.png","hash":"18ad360b603d6fa8bda0d2c1f6b7e2533f181efd","modified":1723898785076},{"_id":"public/5e1d14b3/dynamic.png","hash":"eecce3203dbe70ad732f13c706b6ac8812de8cde","modified":1723898785076},{"_id":"public/5e1d14b3/matrix_level.png","hash":"11ca695a551c3b8357c5b3af88dcec86c2cacb6d","modified":1723898785076},{"_id":"public/f845f3e4/peft_eval.png","hash":"e2af71ff2fb5ae2565c51e05bbf20e293838e5dc","modified":1723898785076},{"_id":"public/9c593ccd/xiaomi_2.png","hash":"fa82b0d446a2be5a9d1075880c8358b5c6f6fd23","modified":1723898785076},{"_id":"public/5ac36d34/structure.png","hash":"bbd51c9725e0c7378018c98204955ddb2bdc1502","modified":1723898785076},{"_id":"public/c4da56c0/meta_pi.png","hash":"193df6c392d71dd4b90202d9be97f000f37b68ec","modified":1723898785076},{"_id":"public/7381cae3/3.png","hash":"d0bbe995cc75de29aaa21f1abc144d3bedf5556f","modified":1723898785076},{"_id":"public/f5fb75e4/metrics.png","hash":"7bb73cea5404b78b78799e7069c914c50f847b39","modified":1723898785076},{"_id":"public/7381cae3/6.png","hash":"c22d52b720cc97628669ba318f13c39692ff8c52","modified":1723898785076},{"_id":"public/f3acf042/evaluation.png","hash":"ee53ffd386d5ba9b276dc3f55b87e68b5f8dc378","modified":1723898785076},{"_id":"public/f3acf042/data2.png","hash":"29418adf51a48bca2340e506dd86dbb8882f913e","modified":1723898785076},{"_id":"public/f3acf042/mtbench.png","hash":"3411a742f62a58e4ff435ca06641e8830c2c80f6","modified":1723898785076},{"_id":"public/7bbe2df6/tree_attention_exp.png","hash":"0b516ebc1df1418eb6f5734b05f3975b9bc71d18","modified":1723898785076},{"_id":"public/c61d17e3/mistral_perf.png","hash":"c9d7ce0a301920c4e722e341200f311995923735","modified":1723898785076},{"_id":"public/c61d17e3/mistral_swa.png","hash":"59037b91ba8f256fd89b3d60b8ce477e4c8f4b3a","modified":1723898785076},{"_id":"public/a5206abd/glm.png","hash":"127b31e5722081b17f680deb6c7052103cdf1158","modified":1723898785076},{"_id":"public/6a40bfa5/ics_measure.png","hash":"e9fe87cfea7dcef7cb66e1d76c17d883cbbc3cbd","modified":1723898785076},{"_id":"public/14e576c/28.png","hash":"d438e857378575809c880b78ca715dc69e50b364","modified":1723898785076},{"_id":"public/3dc22f96/mqa_result_3.png","hash":"12e310102ace1f9e89c0e9a352cf4a3462335a60","modified":1723898785076},{"_id":"public/376db710/param_search_2.png","hash":"601bf31d9a7aaac8d1e1e60f1a4c2d40224160e5","modified":1723898785076},{"_id":"public/376db710/param_search.png","hash":"82785189286d4e7b69d24998d14ca9b78fb4d890","modified":1723898785076},{"_id":"public/473f2b43/dpo_loss_code.png","hash":"279b32cc1c4dfefa8790fcbb597659e8b974ac61","modified":1723898785076},{"_id":"public/da871ebe/scaling_function.png","hash":"b554e0bd697e72bb2e5e24a16c678d80c2efcc52","modified":1723898785076},{"_id":"public/da871ebe/sentiment_control.png","hash":"fc200cc3802fdee9d80e4bc259f4baca7b425ae7","modified":1723898785076},{"_id":"public/473f2b43/result_1.png","hash":"725c6be46c42e8fc8184304bb0cdf5071b09e8b2","modified":1723898785076},{"_id":"public/f5c015c/acce_alog.png","hash":"61f4653292bbd93debee66cebfb44ac7198e5818","modified":1723898785076},{"_id":"public/f5c015c/acce_k.png","hash":"ca37e1983347a1a835389de4d17047e3b0d02af4","modified":1723898785076},{"_id":"public/1d5bcd45/perf.png","hash":"02a9e32039679d6f1249e0bd6bfbc3cff00228c8","modified":1723898785076},{"_id":"public/280fa97a/hyperparameters.png","hash":"b3595e75eff0cb8ea8f86fd9e2f8c6ae5f7892bc","modified":1723898785076},{"_id":"public/45ee1a6d/infini_attention_compare.png","hash":"9b272eba596a790b1d40ef3a8b041bdffe1660d3","modified":1723898785076},{"_id":"public/45ee1a6d/lm_infinite_ppl_figure.png","hash":"34397c88e268b769c97a2b6e2ad63bf6ad5270ef","modified":1723898785076},{"_id":"public/45ee1a6d/streamingllm_model_ppl.png","hash":"2fc1c11d7cfa2598b414e5e4c145181ec9e10648","modified":1723898785076},{"_id":"public/45ee1a6d/xl_attention.png","hash":"395424d6c048880e143b9b2f93585597fbebebd7","modified":1723898785076},{"_id":"public/44e38c1b/dbrx_perf.png","hash":"035a992ef2e7e5a9165c9488e2696e38fa29165c","modified":1723898785076},{"_id":"public/44e38c1b/ds_16b_perf_2.png","hash":"6886e18a2e7601202e8721b83f998faf028e19eb","modified":1723898785076},{"_id":"public/44e38c1b/ds_16b_perf_1.png","hash":"d540e8a58e166c0ba894708c9b0c277c31107487","modified":1723898785076},{"_id":"public/44e38c1b/ds_moe_perf.png","hash":"b6ed751d2f171dac971bcf1d7f12e8a2d7fad388","modified":1723898785076},{"_id":"public/44e38c1b/ds_moe_structure.png","hash":"b0564913ecc1f78e5052dbc07eb65b3f048846e3","modified":1723898785076},{"_id":"public/44e38c1b/ds_moe_upper_bound_13b.png","hash":"cec21f0cf3809011ce210dffda35da3671147008","modified":1723898785076},{"_id":"public/44e38c1b/glam_compare_gpt3_2.png","hash":"89172dda5ac569780ea47e85bc43eaef1d6918ae","modified":1723898785076},{"_id":"public/44e38c1b/glam_model.png","hash":"63c95ea5ae77528f7d94a94b21cf12ed63e0bfdb","modified":1723898785076},{"_id":"public/44e38c1b/switch_transformer_distill.png","hash":"be5cae81fafbda6b638ac185421bd04e00b7a60d","modified":1723898785076},{"_id":"public/44e38c1b/switch_transformer_scaling_dense.png","hash":"b8de8c427de51d62e69915da7a97bf4a9b505317","modified":1723898785076},{"_id":"public/44e38c1b/switch_transformer_scaling_time.png","hash":"b56568665d2680cc0723269ae39dc4b60de1b01c","modified":1723898785076},{"_id":"public/44e38c1b/switch_transformer_pretrain_result.png","hash":"880f70f371b8392e3021bf56d282be5640c232f7","modified":1723898785076},{"_id":"public/cc852861/paraphrasing_perf.png","hash":"5cf407e2e2ee61bb2b6bdae0570c3b8c4a3a9374","modified":1723898785076},{"_id":"public/83c49df0/model.png","hash":"3b312a78f5664334694a18cd1fb6a1419bb7db73","modified":1723898785076},{"_id":"public/93328a2a/file_upload.png","hash":"bc74f106e2202b647ad06f8a40dcce617cc17d27","modified":1723898785076},{"_id":"public/93328a2a/steerability.png","hash":"58a2e99e8b23814a53563b71159fa7118f63a106","modified":1723898785076},{"_id":"public/a8f8b641/eval_base_large.png","hash":"0548563b9aabe77bf92969ac7435fe94f94819be","modified":1723898785076},{"_id":"public/770b63e1/norm_head.png","hash":"b5a6e95dec33597b0e492c3cb27acf2ce6b08fc7","modified":1723898785076},{"_id":"public/bb8fcf21/compare.png","hash":"b226e1c4aab1103479faef5071ff1d81cfd288e3","modified":1723898785076},{"_id":"public/a0824e29/a1.png","hash":"b12067baa44113a9a5a813a6342680602e14afaf","modified":1723898785076},{"_id":"public/a0824e29/a3.png","hash":"8fef339f6148d85ca2e476c1e31d02ba1c0a4051","modified":1723898785076},{"_id":"public/210dbccd/dist.png","hash":"2c032b4da26c6377b72bb68f7f5b4ab41ae9c403","modified":1723898785076},{"_id":"public/210dbccd/model.png","hash":"c1d3c58aba2ed3099bf7218722e8a2cd451547d2","modified":1723898785076},{"_id":"public/f845f3e4/eval_2.png","hash":"87d2cbaef2f915e03f03efa30ad6d853988d2ddc","modified":1723898785076},{"_id":"public/9c593ccd/task_emb.png","hash":"f9a24501fc342029ad08bbe0f1e954614b9a4f3c","modified":1723898785076},{"_id":"public/9c593ccd/bfm.png","hash":"61a3b1d748826cda9791eb31554b959cf8e431fe","modified":1723898785076},{"_id":"public/9c593ccd/system_1.png","hash":"59d47bc561ae6364870b410a9b30bb3f942ad683","modified":1723898785076},{"_id":"public/fe13b56f/phi_1_code_case.png","hash":"d8ddb0204e7807ce235dd5ebab5d1fc8c4a87a58","modified":1723898785076},{"_id":"public/5ac36d34/result.png","hash":"60f4f0a3cb4a817146c9d33235653cc41c2382f8","modified":1723898785076},{"_id":"public/f5fb75e4/exp1_compute.png","hash":"4b2bb4c651292fa438292ea1adcd5fb6530fec78","modified":1723898785076},{"_id":"public/6a40bfa5/realformer.png","hash":"3bc805db3177c7e6521362b063543941da8d2bd3","modified":1723898785076},{"_id":"public/14e576c/21.png","hash":"fb2577b5fa73b06b786484b3723f7aa3819638a0","modified":1723898785076},{"_id":"public/473f2b43/intro.png","hash":"c5175eef020ec3499aa67163813eab1c4c13a84a","modified":1723898785076},{"_id":"public/f5c015c/fi_example.png","hash":"edbdb72af30cac5f036e47b3d0d426919f336e62","modified":1723898785076},{"_id":"public/1d5bcd45/exp_1.png","hash":"31f27f3659a4c3e0af9894e6a9b85cccec611889","modified":1723898785076},{"_id":"public/280fa97a/ln.png","hash":"4d4c831fc95c591ea0415e07dd5f46d1b1494e60","modified":1723898785076},{"_id":"public/45ee1a6d/lm_infinite_starting_tokens.png","hash":"18e3a03213a7275617201b71eb274cd8fd8b0bf9","modified":1723898785076},{"_id":"public/44e38c1b/ds_moe_sft.png","hash":"8bc26fcfef1aab448d0db0b28666852f62961a3b","modified":1723898785076},{"_id":"public/44e38c1b/gshard_algo_1.png","hash":"aa8bcd982c78e4304c63f81e44b20519dc04f18f","modified":1723898785076},{"_id":"public/44e38c1b/gshard_perf.png","hash":"27157f620e2b7c4be60b2a58f7857a888794cde1","modified":1723898785076},{"_id":"public/44e38c1b/glam_perf.png","hash":"01fb9d4f232e9f0b86abb91dbe5d8fb9fac456ce","modified":1723898785076},{"_id":"public/44e38c1b/gshard_result.png","hash":"32ce9bba1b65ceb2e69c079e113d8b4c524bc479","modified":1723898785076},{"_id":"public/44e38c1b/rnn_moe_hierarchical_gating.png","hash":"067160c735e9c0b8cff777df60d52dfca21ea783","modified":1723898785076},{"_id":"public/44e38c1b/st_moe_perf.png","hash":"58927fa39b5e56e8da00144b417bbae5256d6bdf","modified":1723898785076},{"_id":"public/44e38c1b/switch_transformer_diff_expert_capacity.png","hash":"23f2234b45a2a05ff8b098e68a361a71f46816e9","modified":1723898785076},{"_id":"public/44e38c1b/switch_transformer_distill_diff_model.png","hash":"05c4d885f5563e3dbd56c055a52df5c1531677a7","modified":1723898785076},{"_id":"public/cc852861/eng_tokens.png","hash":"b6c43c289004164e90de39c30e170de5ac1088aa","modified":1723898785076},{"_id":"public/cc852861/paraphrasing_dataset.png","hash":"b85a9077cffcace583a7dbcdbd235ab646086ea1","modified":1723898785076},{"_id":"public/7d7294cb/eval.png","hash":"55fc95a59d33a688a4fb6ce0cc94c1e74480477b","modified":1723898785076},{"_id":"public/770b63e1/scheduler.png","hash":"2ecb96391be5f52692147199916123dc8b26aa76","modified":1723898785076},{"_id":"public/a0824e29/a4.png","hash":"2491aacfa7722f017f9a673e95e96f5957282d4f","modified":1723898785076},{"_id":"public/e287b9c3/2.png","hash":"23f2234b45a2a05ff8b098e68a361a71f46816e9","modified":1723898785076},{"_id":"public/5ac36d34/head.png","hash":"82379ab5e71e15f3e1b8690f9bc20b8ca928d8a6","modified":1723898785076},{"_id":"public/5ac36d34/zero_shot.png","hash":"ef3354533df8ff1053ba06800294cf5be67d0576","modified":1723898785076},{"_id":"public/c4da56c0/meta_rope_ext.png","hash":"43e1131d146b8104e6936d73aca51d8ace4aff61","modified":1723898785076},{"_id":"public/f5fb75e4/downstream_dataset.png","hash":"a6cda82185bcb7c1f70931d792e10583825c2d01","modified":1723898785076},{"_id":"public/f5fb75e4/exp1_plot.png","hash":"27b2613a83f5873538e0e5832ea957e47625d719","modified":1723898785076},{"_id":"public/f5fb75e4/exp2_plot.png","hash":"e65d2e11a5a28b4b1ff8b54ffff65be2298d7c1f","modified":1723898785076},{"_id":"public/7bbe2df6/intro.png","hash":"ab8abb35042127509f264d1f0e8ce838350b08ff","modified":1723898785076},{"_id":"public/41b6a819/cover.png","hash":"0493fd58fd2dad33394399d960924dbff6b386b1","modified":1723898785076},{"_id":"public/14e576c/20.png","hash":"5d42628c8dac91c9671a58535b730e91966c0cbc","modified":1723898785076},{"_id":"public/376db710/scaling_law.png","hash":"19067e52c8d6e073c86fbf4c4fee258574f9b348","modified":1723898785076},{"_id":"public/f5c015c/fi_alpha.png","hash":"f7f5a24106f1b9d16fd805b3ed3d3c2efb4a8c03","modified":1723898785076},{"_id":"public/280fa97a/ln_effect.png","hash":"714ba6cdf67ea33d507e3358b113a94bcd24e1ce","modified":1723898785076},{"_id":"public/44e38c1b/ds_moe_comparison.png","hash":"20c7e90a390604d2146b4984678524046ad941b8","modified":1723898785076},{"_id":"public/44e38c1b/mistral_8_22b_code.png","hash":"f9b78e0669e0c83f716c8e4abfa4d97b6f9b8143","modified":1723898785076},{"_id":"public/44e38c1b/gshard_model.png","hash":"5179df7e42bb7dc49fb6f92f4ec68ed820aeaae2","modified":1723898785076},{"_id":"public/44e38c1b/mistral_8_22b_reasoning.png","hash":"ce00d9ba7b8a65eb238c99f79a17ca7a3cd2238a","modified":1723898785076},{"_id":"public/44e38c1b/st_moe_encoder_specialization.png","hash":"382cb53fcc2e9172ffd7554a04714feed4d9706b","modified":1723898785076},{"_id":"public/44e38c1b/switch_transformer_scaling_step.png","hash":"dbfa55ab1c94e266344315fd215f2d646eaefda1","modified":1723898785076},{"_id":"public/cc852861/eng_data.png","hash":"9262b3bbc1b415d9c776723fe85c0a43f9fb562a","modified":1723898785076},{"_id":"public/cc852861/eng_data_dist.png","hash":"157286ce140bd5acc7c45fb12785649cc7214472","modified":1723898785076},{"_id":"public/3df0cd42/m32_intro.png","hash":"dbd36b878a094f3db0dd328d1f451c545c108cb0","modified":1723898785076},{"_id":"public/bb8fcf21/result.png","hash":"6c5d190fff97ddce11a3e9b870c20c73e484aac0","modified":1723898785076},{"_id":"public/f845f3e4/sft_result.png","hash":"599e02f21987d2f50b32d774fbcf3d9aae07792c","modified":1723898785076},{"_id":"public/fe13b56f/overfit.png","hash":"a091387862e564bb8ce30d1fe0b903a65fa74422","modified":1723898785076},{"_id":"public/fe13b56f/phi_1_compare.png","hash":"ad246d961a39997e773d7ced10c50de6ad565f2b","modified":1723898785076},{"_id":"public/5ac36d34/deep_ablation.png","hash":"828f515fe97190eecb6eb0b84e318819340d2e34","modified":1723898785076},{"_id":"public/5ac36d34/deep.png","hash":"1071405517d42ae5e5b9ec42f206accbe1013021","modified":1723898785076},{"_id":"public/5ac36d34/structure_ablation.png","hash":"e821acb21030415364edeae99d894619bcb0e17b","modified":1723898785076},{"_id":"public/f5fb75e4/exp2_param.png","hash":"1412eccf1c2cf485bde50b1cbf9223ee5a5ff2cf","modified":1723898785076},{"_id":"public/3dc22f96/lihongyi_self_attention.png","hash":"39db6256143fd9a494e848240a8daa434aaddea5","modified":1723898785076},{"_id":"public/376db710/batch_size.png","hash":"8e754e7b97b0290618e90e499d33f300e149b39c","modified":1723898785076},{"_id":"public/1d5bcd45/normaization.png","hash":"8c4ce11d34256acc7e3800355186ab00f2236293","modified":1723898785076},{"_id":"public/280fa97a/main_results.png","hash":"a7388a503f0157c2ebe9ef765d63daab358b67d7","modified":1723898785076},{"_id":"public/45ee1a6d/streamingllm_compare.png","hash":"40394890082b1666d1221f302ed52a80fc358477","modified":1723898785076},{"_id":"public/45ee1a6d/infini_attention_gating.png","hash":"5e875c3de6bde62ea8e15ea4995a56f9fd28d67c","modified":1723898785076},{"_id":"public/44e38c1b/ds_moe_145b.png","hash":"502a377d8625d78d2e3ba7281bfb11732c14a61c","modified":1723898785076},{"_id":"public/44e38c1b/ds_moe_upper_bound_2b.png","hash":"4338539fe123371c5512c730fc8a35864236323b","modified":1723898785076},{"_id":"public/44e38c1b/switch_transformer_capacity_effect.png","hash":"16268929d0ec965d3771fccbb595b75d82f05912","modified":1723898785076},{"_id":"public/44e38c1b/switch_transformer_sft_result.png","hash":"0dd7cfaac788c5d112dd10fe98f0052b369420bf","modified":1723898785076},{"_id":"public/cc852861/eng_sample.png","hash":"6b0d7ed89b16a3e9c6297219f33e59f204923828","modified":1723898785076},{"_id":"public/cc852861/eng_needle_comp.png","hash":"e4c9f5c51548faf11a7ff64d23aae5bd2927ab0e","modified":1723898785076},{"_id":"public/44e38c1b/vanilla_moe_result.png","hash":"e491e44cfff384422f3d9cf87cd5e52fd976aed7","modified":1723898785076},{"_id":"public/93328a2a/multi_step_tool.png","hash":"922228b41af7043faad376a0d5988a2478151af6","modified":1723898785076},{"_id":"public/83c49df0/pt_eval.png","hash":"68b8a0f9d7a49509ab10fcf083bf292b86556db8","modified":1723898785076},{"_id":"public/83c49df0/align_eval.png","hash":"882ae5f89d06999a7113d6345a2e275ffd947eb1","modified":1723898785076},{"_id":"public/bb8fcf21/angle.png","hash":"e9ab1ed585133183ebe85d017eb816d5624edf64","modified":1723898785076},{"_id":"public/fe13b56f/phi_1_result.png","hash":"2a039d232b1ba349eefe922746b0d74b2850e237","modified":1723898785076},{"_id":"public/images/avatar/shadow.png","hash":"da67a14e556fb3223c1c4ec8b9f33eb52c612f50","modified":1723898785076},{"_id":"public/6a40bfa5/bn_ics.png","hash":"f92751ea20430f25caa3d6bb892c5894bf7509d6","modified":1723898785076},{"_id":"public/41b6a819/ict.png","hash":"2445ecbbf5ec6a96695f21c03a5fcbf67640b9f0","modified":1723898785076},{"_id":"public/14e576c/17.png","hash":"8ea1c5d90f3da5c469eb17aea50f377cb9c28ba0","modified":1723898785076},{"_id":"public/14e576c/18.png","hash":"fe0a8e7005110abca19bc7ae506f3e35042b70ec","modified":1723898785076},{"_id":"public/14e576c/26.png","hash":"f74d03dae65109740f48924f30b52a742b5e4273","modified":1723898785076},{"_id":"public/280fa97a/gradient.png","hash":"58524a8fd10f4d76e8e419e1ed46bd9a99cf58d5","modified":1723898785076},{"_id":"public/cc852861/paraphrasing_intro.png","hash":"6b810c88945281a9cdf9749941cdbe08346fd42b","modified":1723898785076},{"_id":"public/cc852861/paraphrasing_example.png","hash":"946cdbb0a425b9d11b0ff587007885990abc9f99","modified":1723898785076},{"_id":"public/41b6a819/pretrain_data_pipeline.png","hash":"91218a2272eab9284904c91bacd8d8a40e3c1580","modified":1723898785076},{"_id":"public/14e576c/10.png","hash":"2c52d4f90dd9356eeb4c9a39f1df1038ccec4693","modified":1723898785076},{"_id":"public/14e576c/3.png","hash":"4c2c2a30d9ac8db03bab56da5d16ce2042ef73bc","modified":1723898785076},{"_id":"public/14e576c/6.png","hash":"4b32a49bfead98f5238871b81076176e38168333","modified":1723898785076},{"_id":"public/376db710/eval.png","hash":"982f977c073703051fdecb951c6231f7b74aa58a","modified":1723898785076},{"_id":"public/f845f3e4/eval_3.png","hash":"078ad89992f2cd4bc20b2cc0fd412ceda987e231","modified":1723898785076},{"_id":"public/14e576c/1.png","hash":"a8898b3f3b7c64fabc5fad9bf8ef5524501d2aeb","modified":1723898785076},{"_id":"public/14e576c/19.png","hash":"1d7b929e709657c9b7d7ca4da8eadc8c4ca4b3ca","modified":1723898785076},{"_id":"public/14e576c/24.png","hash":"b89b0a0ca774a4efc1ece628fb20379b5f6a0b69","modified":1723898785076},{"_id":"public/3dc22f96/Scaled-dot-product-self-attention.png","hash":"983eae2b767df413ef3211ddaf31f1b833d7c86f","modified":1723898785076},{"_id":"public/14e576c/8.png","hash":"92b0e3b75ce97bbaf4aa69e484216702293589ee","modified":1723898785076},{"_id":"public/770b63e1/dedup.png","hash":"417cb6d188decbbf3f25e6595dddca213a6d3376","modified":1723898785076},{"_id":"public/14e576c/27.png","hash":"557c04a2134b6ae147e076cdb80de1730e937d9b","modified":1723898785076},{"_id":"public/14e576c/9.png","hash":"2a0fb56563b13411035ed41a3ad882f66f948b26","modified":1723898785076},{"_id":"public/14e576c/11.png","hash":"9b964f3aa6f82a09eb2f2f944508bf0a9d29efb3","modified":1723898785076},{"_id":"public/14e576c/23.png","hash":"9a9af6308620b59f2ee00a3d0da4e942d953e406","modified":1723898785076},{"_id":"public/14e576c/22.png","hash":"3b27321ef8d76844f6720e1a27d65d1946d48ea7","modified":1723898785076},{"_id":"public/14e576c/25.png","hash":"aa259b58be90eed6af0c4ba800a991b9464453d0","modified":1723898785076},{"_id":"public/14e576c/2.png","hash":"768421239ad7c838dd86714fd9f17b3c73cbb887","modified":1723898785076},{"_id":"public/14e576c/4.png","hash":"4dacbfb89079d528da1208773961e1366debde9b","modified":1723898785076},{"_id":"public/14e576c/15.png","hash":"15abcbcf7340941e98dac7a0ab42d922e7fea1b4","modified":1723898785076},{"_id":"public/14e576c/14.png","hash":"71f75960246f7528b3b83b84f8f91775f9e2fb45","modified":1723898785076},{"_id":"public/14e576c/12.png","hash":"c6c493b14e0a1cc4863a912c4ccc998de194bfc0","modified":1723898785076},{"_id":"public/14e576c/16.png","hash":"ba7b2bc65e10389cf9a87ddef69f462e806304f5","modified":1723898785076},{"_id":"public/3dc22f96/MQA.png","hash":"7e3f3037311be60e79a7b5388338febc9f3b6d7c","modified":1723898785076},{"_id":"public/14e576c/7.png","hash":"95640525b0706d3118eeb88c6c4c6217a96c39d0","modified":1723898785076},{"_id":"public/14e576c/13.png","hash":"68b7da3e3074e4d6995eaf96c7d8cf622eadffb7","modified":1723898785076},{"_id":"public/14e576c/5.png","hash":"7ae786b309a757c5f61a713c7ceef4d2824b024e","modified":1723898785076},{"_id":"public/f5c015c/speculative_decoding.png","hash":"fe277fa76f9f9c71e2030a41ca9eab458c33826a","modified":1723898785076},{"_id":"public/images/background/wallhaven-p97q73.png","hash":"e83875b9cd1dcfa87f0e76dc1a327cbe2921c5da","modified":1723898785076},{"_id":"public/c61d17e3/ms_invest_mistral.png","hash":"faf324c0b57843516a0b256750e6475ec0c2ce93","modified":1723898785076},{"_id":"public/6a40bfa5/ellipse_1.png","hash":"ef2470f6bf1511dc9aac9f1c6489b9d2ffdcb45f","modified":1723898785076},{"_id":"public/45ee1a6d/digimon.png","hash":"247f4059dd9671047f5d6707d8cef75a93d93f40","modified":1723898785076},{"_id":"public/b70b4a2d/norm_in_nlp.png","hash":"7be79b0e55d7d00ff6c16c247d0e506771453380","modified":1723898785076},{"_id":"public/images/background/wallhaven-gpxpg3.png","hash":"d798eaa0d679ed8a14022b0741098711c3402ca8","modified":1723898785076},{"_id":"public/images/background/wallhaven-x636oz.png","hash":"d419359164da44691e2bc8641256ca7c8111e227","modified":1723898785076}],"Category":[{"name":"CS","_id":"clzy4tsmk00040p4k7t9c6gj9"},{"name":"NLP","parent":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsmr000i0p4k2xfhcwq7"},{"name":"LLM","parent":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsmx001p0p4k0s1z5pt2"}],"Data":[{"_id":"styles","data":".post-toc .nav .nav-child {\n  display: block;\n}\n.post-toc ol {\n  font-size: 13px;\n}\nbody {\n  background: url(\"/images/background/wallhaven-p97q73.png\");\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-size: cover;\n  background-position: 50% 50%;\n}\n:root {\n  --content-bg-color: rgba(32,32,32,0.816);\n}\n"}],"Page":[{"title":"categories","date":"2024-01-31T10:57:57.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2024-01-31 18:57:57\ntype: \"categories\"\ncomments: false\n---\n","updated":"2024-01-31T10:58:37.077Z","path":"categories/index.html","layout":"page","_id":"clzy4tsmf00000p4kdhu7bw8v","content":"\n","length":0,"excerpt":"","more":"\n"},{"title":"about","date":"2024-01-31T10:57:44.000Z","type":"about","comments":0,"_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2024-01-31 18:57:44\ntype: \"about\"\ncomments: false\n---\n","updated":"2024-01-31T10:58:21.349Z","path":"about/index.html","layout":"page","_id":"clzy4tsmj00020p4ka00ld77m","content":"\n","length":0,"excerpt":"","more":"\n"},{"title":"tags","date":"2024-01-31T10:50:02.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2024-01-31 18:50:02\ntype: \"tags\"\ncomments: false\n---\n","updated":"2024-01-31T10:57:24.396Z","path":"tags/index.html","layout":"page","_id":"clzy4tsml00060p4kgf3z3pli","content":"\n","length":0,"excerpt":"","more":"\n"}],"Post":[{"title":"LLM:RoPE","abbrlink":"a051710f","date":"2024-02-21T13:18:13.000Z","mathjax":true,"_content":"\nLLMRoPE\n\n# RoPE\n\nRoPERotary Position Embedding2021TransformerRoPE<big><u>****</u></big>\n\n2023AlibiRoPE20232024RoPEAlibiAlibi\n\nRoPERoPE  \n\n# \n\nRoPE\n\n  \n\nBert256/512token  \n  \n<u>****</u>token-2token-1token-10002token-10001  \n<u>****</u><u>****</u>self-attention<u>****</u>  \n<u>****</u><u>****</u><u>****</u><u>****</u>  \n\n  \n\n3  \n\n## \n\nself-attention  \n\n$$\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}$$  \n\n\n$x_i$  $x_j$  $i$  $j$ $p$   \n\n$p$ $x$  $p$ attentionsoftmaxelement-wise addition\n\n $x + p$   $x * p$ \n\n## \n\n $x$  $p$   \n\n1 $e_1 = x_ + p_1$ 18 $e_8 = x_ + p_8$  $e_1$  $e_8$ <u>****</u>  \n\n15121512=512handle\n\n1 $q_{i}k_{j}^{T}$  \n\n$$\\begin{align*}q_ik_j^\\top&=\\left(x_i+p_i\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{\\color{red}W_K^\\top p_j^\\top}\\right)\\end{align*}\\tag{2}$$  \n\n $p_iW_\\mathbb{Q}$  $W_K^\\top p_j^\\top$   \n\n### Google\n\nGoogleSelf-Attention with Relative Position Representations $p_iW_\\mathbb{Q}$  $j$ $W_K^\\top p_j^\\top$  $i$$j$  $R_{ij}^K$attention<u>**input projection**</u>  \n\n$$\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n$$  \n\n$R_{ij}^K$ clip  \n\n$$\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n$$\n\n $p_\\mathrm{K}$  \n\nclip****tokentoken256>256\n\nGoogleinput\n\n$$\\begin{align*}\no_\\mathrm{i}&=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} + {\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}$$\n\nGoogle $p_{j}W_{\\mathrm{V}}$ \n\n$$\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}$$\n\n$R_{\\mathrm{i,j}}^{\\mathrm{V}}$  $R_{ij}^K$  + clip\n\n### XLNET\n\nXLNETGoogle  \n\n2  \n\n$$\\begin{align*}\nq_ik_j^T\n&= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&= \nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n$$  \n\n\n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n$$    \n\n $p_i$  $u$  $\\nu$  $p_j$  $R_{i-j}^\\top$   \n\n $u$  $\\nu$  $u$  $\\nu$ \n\n$$\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n$$   \n\nXLNET  \n\n$$\\begin{align*}\no_\\mathrm{i}\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}$$\n\nGoogleXLNET $\\mathrm{a_{i,j}}$ 2 $i$  $j$ clip\n\nT5  \n\n### T5\n\n6 $i$  $j$   \n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n$$    \n\nXLNETDeBertaT5\n\n## \n\nattention  \n\n1softmax33\n\n8433  \n\n\n\n\n\nself-attentionlinear attention  \n\n# RoPE\n\n## attention\n\nRoPE\n\n  \n\nself-attention1 =  + softmaxsoftmax  \n\n  \n\n$$\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n$$  \n\n $q_m$  $m$ query$k_n$  $n$ key$f_q$  $f_k$ querykey  \n\n $f_q$  $f_k$  $g$ 11  \n\nRoPE  \n\n## \n\n11 $g$ \n\n2  \n\n{% asset_img complex_number.png 282 401  %}\n\nquerykey2  \n hidden size = 2   \n\n211Roformer  \n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta} \\\\\nf_k(\\boldsymbol{k}_n,n)&=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta} \\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n$$  \n\n $\\boldsymbol{k}_n^*$  $\\boldsymbol{k}_n$   \n\n\n\n  \n\n$$\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib \n\\end{gathered}\n\\tag{13}\n$$\n\n11  \n\n  \n\n  \n\n$$\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n$$\n\n22 $q_m$   \n\n$$\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n$$\n\n\n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m) \n&= \\boldsymbol{q}_me^{im\\theta} \\\\\n&= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&= (q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n$$  \n\n16  \n\n$$\nf_q(\\boldsymbol{q}_m,m) = \\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n$$  \n\n1transpose  \n\n\n\n$$\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n) \n&= (k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n$$  \n\n\n\n$$\nf_k(\\boldsymbol{k}_n,n) = \\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n$$  \n\n  \n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)) \\\\&+ (q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\\\\\n=&q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right) \\\\\n&+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right) \\\\\n&+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta)) \\\\\n&+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta)) \\\\\n=&q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta) \\\\\n&-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta) +q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n$$\n\n\n$$\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n$$\n\n $g$  \n\n$$\n\\begin{aligned}\n&g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n= &\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right] \\\\\n= &\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) - i(q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})](\\cos((m - n)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n$$\n\n\n\n## \n\n17\n\n$$\nf_q(\\boldsymbol{q}_m,m)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n$$\n\n  \n\n$$\nf_k(\\boldsymbol{k}_n,n)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n$$\n\n  \n\n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta) + \\sin(m\\theta)\\sin(n\\theta)}&\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n$$  \n\n2223  \n\n$$\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n$$  \n\n $f_q$  $f_k$   \n\n\n\n## 2\n\n2 $f_q$  $f_k$  $g$ 11  \n\n  \n\n$$\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\\\0&0&0&0&\\cdots&\\sin m\\theta_{d/2-1}&\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n$$  \n\n$d$  $d/2$  $d/2$  $\\theta$\n\n$$\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n$$  \n\n $m$  $n$  $R_m$  $R_n$self-attention  \n\n $\\theta$ GoogleAttention is All You Need\n\n## \n\n25  \n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n$$\n\nelement-wise  \n\nLLAMAdecoder\n\n## \n\n  \n\n\n\n $\\theta$   \n\n[Roformer](https://arxiv.org/abs/2104.09864)[](https://spaces.ac.cn/archives/8265)  \n\n $d = 128$ \n\n{% asset_img remote_attenuation.png 775 457  %}  \n\n#   \n\nRoPEtransformer\n\n# Reference\n1Transformerhttps://spaces.ac.cn/archives/8130  \n2Transformer2https://spaces.ac.cn/archives/8265  \n3RoFormer: Enhanced Transformer with Rotary Position Embedding https://arxiv.org/abs/2104.09864  \n4RoPE https://zhuanlan.zhihu.com/p/647109286  \n\n***\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)\n","source":"_posts/cs/nlp/2024/02/LLMRoPE.md","raw":"---\ntitle: LLM:RoPE\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - positional encoding\n  - RoPE\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: a051710f\ndate: 2024-02-21 21:18:13\nmathjax: true\n---\n\nLLMRoPE\n\n# RoPE\n\nRoPERotary Position Embedding2021TransformerRoPE<big><u>****</u></big>\n\n2023AlibiRoPE20232024RoPEAlibiAlibi\n\nRoPERoPE  \n\n# \n\nRoPE\n\n  \n\nBert256/512token  \n  \n<u>****</u>token-2token-1token-10002token-10001  \n<u>****</u><u>****</u>self-attention<u>****</u>  \n<u>****</u><u>****</u><u>****</u><u>****</u>  \n\n  \n\n3  \n\n## \n\nself-attention  \n\n$$\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}$$  \n\n\n$x_i$  $x_j$  $i$  $j$ $p$   \n\n$p$ $x$  $p$ attentionsoftmaxelement-wise addition\n\n $x + p$   $x * p$ \n\n## \n\n $x$  $p$   \n\n1 $e_1 = x_ + p_1$ 18 $e_8 = x_ + p_8$  $e_1$  $e_8$ <u>****</u>  \n\n15121512=512handle\n\n1 $q_{i}k_{j}^{T}$  \n\n$$\\begin{align*}q_ik_j^\\top&=\\left(x_i+p_i\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{\\color{red}W_K^\\top p_j^\\top}\\right)\\end{align*}\\tag{2}$$  \n\n $p_iW_\\mathbb{Q}$  $W_K^\\top p_j^\\top$   \n\n### Google\n\nGoogleSelf-Attention with Relative Position Representations $p_iW_\\mathbb{Q}$  $j$ $W_K^\\top p_j^\\top$  $i$$j$  $R_{ij}^K$attention<u>**input projection**</u>  \n\n$$\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n$$  \n\n$R_{ij}^K$ clip  \n\n$$\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n$$\n\n $p_\\mathrm{K}$  \n\nclip****tokentoken256>256\n\nGoogleinput\n\n$$\\begin{align*}\no_\\mathrm{i}&=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} + {\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}$$\n\nGoogle $p_{j}W_{\\mathrm{V}}$ \n\n$$\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}$$\n\n$R_{\\mathrm{i,j}}^{\\mathrm{V}}$  $R_{ij}^K$  + clip\n\n### XLNET\n\nXLNETGoogle  \n\n2  \n\n$$\\begin{align*}\nq_ik_j^T\n&= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top x_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&= \nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n$$  \n\n\n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n$$    \n\n $p_i$  $u$  $\\nu$  $p_j$  $R_{i-j}^\\top$   \n\n $u$  $\\nu$  $u$  $\\nu$ \n\n$$\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n$$   \n\nXLNET  \n\n$$\\begin{align*}\no_\\mathrm{i}\n&=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}$$\n\nGoogleXLNET $\\mathrm{a_{i,j}}$ 2 $i$  $j$ clip\n\nT5  \n\n### T5\n\n6 $i$  $j$   \n\n$$\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n$$    \n\nXLNETDeBertaT5\n\n## \n\nattention  \n\n1softmax33\n\n8433  \n\n\n\n\n\nself-attentionlinear attention  \n\n# RoPE\n\n## attention\n\nRoPE\n\n  \n\nself-attention1 =  + softmaxsoftmax  \n\n  \n\n$$\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n$$  \n\n $q_m$  $m$ query$k_n$  $n$ key$f_q$  $f_k$ querykey  \n\n $f_q$  $f_k$  $g$ 11  \n\nRoPE  \n\n## \n\n11 $g$ \n\n2  \n\n{% asset_img complex_number.png 282 401  %}\n\nquerykey2  \n hidden size = 2   \n\n211Roformer  \n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta} \\\\\nf_k(\\boldsymbol{k}_n,n)&=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta} \\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n$$  \n\n $\\boldsymbol{k}_n^*$  $\\boldsymbol{k}_n$   \n\n\n\n  \n\n$$\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib \n\\end{gathered}\n\\tag{13}\n$$\n\n11  \n\n  \n\n  \n\n$$\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n$$\n\n22 $q_m$   \n\n$$\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n$$\n\n\n\n$$\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m) \n&= \\boldsymbol{q}_me^{im\\theta} \\\\\n&= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&= (q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n$$  \n\n16  \n\n$$\nf_q(\\boldsymbol{q}_m,m) = \\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n$$  \n\n1transpose  \n\n\n\n$$\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n) \n&= (k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n$$  \n\n\n\n$$\nf_k(\\boldsymbol{k}_n,n) = \\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n$$  \n\n  \n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)) \\\\&+ (q_m^{(1)}\\sin(m\\theta) + q_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) + k_n^{(2)}\\cos(n\\theta))\\\\\n=&q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right) \\\\\n&+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right) \\\\\n&+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta)) \\\\\n&+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta)) \\\\\n=&q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta) \\\\\n&-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta) +q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n$$\n\n\n$$\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n$$\n\n $g$  \n\n$$\n\\begin{aligned}\n&g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n= &\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right] \\\\\n= &\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) - i(q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})](\\cos((m - n)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) + (q_m^{(1)}k_n^{{2}} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n$$\n\n\n\n## \n\n17\n\n$$\nf_q(\\boldsymbol{q}_m,m)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n$$\n\n  \n\n$$\nf_k(\\boldsymbol{k}_n,n)^\\top = \n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n$$\n\n  \n\n$$\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta) + \\sin(m\\theta)\\sin(n\\theta)}&\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n$$  \n\n2223  \n\n$$\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n$$  \n\n $f_q$  $f_k$   \n\n\n\n## 2\n\n2 $f_q$  $f_k$  $g$ 11  \n\n  \n\n$$\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\\\0&0&0&0&\\cdots&\\sin m\\theta_{d/2-1}&\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n$$  \n\n$d$  $d/2$  $d/2$  $\\theta$\n\n$$\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n$$  \n\n $m$  $n$  $R_m$  $R_n$self-attention  \n\n $\\theta$ GoogleAttention is All You Need\n\n## \n\n25  \n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n$$\n\nelement-wise  \n\nLLAMAdecoder\n\n## \n\n  \n\n\n\n $\\theta$   \n\n[Roformer](https://arxiv.org/abs/2104.09864)[](https://spaces.ac.cn/archives/8265)  \n\n $d = 128$ \n\n{% asset_img remote_attenuation.png 775 457  %}  \n\n#   \n\nRoPEtransformer\n\n# Reference\n1Transformerhttps://spaces.ac.cn/archives/8130  \n2Transformer2https://spaces.ac.cn/archives/8265  \n3RoFormer: Enhanced Transformer with Rotary Position Embedding https://arxiv.org/abs/2104.09864  \n4RoPE https://zhuanlan.zhihu.com/p/647109286  \n\n***\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)\n","slug":"cs/nlp/2024/02/LLMRoPE","published":1,"updated":"2024-04-05T06:44:27.271Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmh00010p4k4cqqejre","content":"<p>LLMRoPE</p>\n<h1 id=\"rope\">RoPE</h1>\n<p>RoPERotary Position\nEmbedding2021TransformerRoPE<big><u><strong></strong></u></big></p>\n<p>2023AlibiRoPE20232024RoPEAlibiAlibi</p>\n<p>RoPERoPE</p>\n<h1 id=\"\"></h1>\n<p>RoPE</p>\n<p></p>\n<p>Bert256/512token<br>\n<br>\n<u><strong></strong></u>token-2token-1token-10002token-10001<br>\n<u><strong></strong></u><u><strong></strong></u>self-attention<u><strong></strong></u><br>\n<u><strong></strong></u><u><strong></strong></u><u><strong></strong></u><u><strong></strong></u></p>\n<p></p>\n<p>3</p>\n<h2 id=\"\"></h2>\n<p>self-attention</p>\n<p><span class=\"math display\">\\[\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}\\]</span></p>\n<p><span class=\"math inline\">\\(x_i\\)</span>  <span class=\"math inline\">\\(x_j\\)</span>  <span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(p\\)</span> </p>\n<p><span class=\"math inline\">\\(p\\)</span>\n<span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(p\\)</span>\nattentionsoftmaxelement-wise\naddition</p>\n<p>\n<span class=\"math inline\">\\(x + p\\)</span>  <span class=\"math inline\">\\(x * p\\)</span> </p>\n<h2 id=\"\"></h2>\n<p> <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(p\\)</span>\n</p>\n<p>1 <span class=\"math inline\">\\(e_1 =\nx_ + p_1\\)</span>\n18\n<span class=\"math inline\">\\(e_8 = x_ + p_8\\)</span>  <span class=\"math inline\">\\(e_1\\)</span>  <span class=\"math inline\">\\(e_8\\)</span>\n<u><strong></strong></u></p>\n<p>15121512=512handle</p>\n<p>1 <span class=\"math inline\">\\(q_{i}k_{j}^{T}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{align*}q_ik_j^\\top&amp;=\\left(x_i+p_i\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&amp;=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{\\color{red}W_K^\\top\np_j^\\top}\\right)\\end{align*}\\tag{2}\\]</span></p>\n<p> <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span> </p>\n<h3 id=\"google\">Google</h3>\n<p>GoogleSelf-Attention with Relative\nPosition Representations <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n\n<span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span>  <span class=\"math inline\">\\(i\\)</span><span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(R_{ij}^K\\)</span>attention<u><strong>input\nprojection</strong></u></p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n\\]</span></p>\n<p><span class=\"math inline\">\\(R_{ij}^K\\)</span>\nclip</p>\n<p><span class=\"math display\">\\[\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n\\]</span></p>\n<p> <span class=\"math inline\">\\(p_\\mathrm{K}\\)</span>\n</p>\n<p>clip<strong></strong>tokentoken256&gt;256</p>\n<p>Googleinput</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}&amp;=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} +\n{\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}\\]</span></p>\n<p>Google <span class=\"math inline\">\\(p_{j}W_{\\mathrm{V}}\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}\\]</span></p>\n<p><span class=\"math inline\">\\(R_{\\mathrm{i,j}}^{\\mathrm{V}}\\)</span> \n<span class=\"math inline\">\\(R_{ij}^K\\)</span> \n+ clip</p>\n<h3 id=\"xlnet\">XLNET</h3>\n<p>XLNETGoogle</p>\n<p>2</p>\n<p><span class=\"math display\">\\[\\begin{align*}\nq_ik_j^T\n&amp;= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&amp;=\nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu}\nW_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(p_i\\)</span> \n<span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span>  <span class=\"math inline\">\\(p_j\\)</span>  <span class=\"math inline\">\\(R_{i-j}^\\top\\)</span> </p>\n<p> <span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span>\n <span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span> </p>\n<p><span class=\"math display\">\\[\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n\\]</span></p>\n<p>XLNET</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}\\]</span></p>\n<p>GoogleXLNET <span class=\"math inline\">\\(\\mathrm{a_{i,j}}\\)</span>\n2\n<span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span>\nclip</p>\n<p>T5</p>\n<h3 id=\"t5\">T5</h3>\n<p>6\n<span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span>\n</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n\\]</span></p>\n<p>XLNETDeBertaT5</p>\n<h2 id=\"\"></h2>\n<p>attention</p>\n<p>1softmax33</p>\n<p>8433</p>\n<p></p>\n<p></p>\n<p>self-attentionlinear\nattention</p>\n<h1 id=\"rope\">RoPE</h1>\n<h2 id=\"attention\">attention</h2>\n<p>RoPE</p>\n<p></p>\n<p>self-attention1\n=  +\nsoftmaxsoftmax</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(q_m\\)</span>  <span class=\"math inline\">\\(m\\)</span> query<span class=\"math inline\">\\(k_n\\)</span>  <span class=\"math inline\">\\(n\\)</span> key<span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>\nquerykey</p>\n<p> <span class=\"math inline\">\\(f_q\\)</span> \n<span class=\"math inline\">\\(f_k\\)</span>  <span class=\"math inline\">\\(g\\)</span> 11</p>\n<p>RoPE</p>\n<h2 id=\"\"></h2>\n<p>11 <span class=\"math inline\">\\(g\\)</span>\n</p>\n<p>2</p>\n<img src=\"/a051710f/complex_number.png\" class width=\"282\" height=\"401\" title=\"\">\n<p>querykey2<br>\n hidden size = 2 </p>\n<p>211Roformer</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&amp;=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta}\n\\\\\nf_k(\\boldsymbol{k}_n,n)&amp;=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta}\n\\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&amp;=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\boldsymbol{k}_n^*\\)</span>  <span class=\"math inline\">\\(\\boldsymbol{k}_n\\)</span> </p>\n<p></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib\n\\end{gathered}\n\\tag{13}\n\\]</span></p>\n<p>11</p>\n<p></p>\n<p></p>\n<p><span class=\"math display\">\\[\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n\\]</span></p>\n<p>22 <span class=\"math inline\">\\(q_m\\)</span> </p>\n<p><span class=\"math display\">\\[\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)\n&amp;= \\boldsymbol{q}_me^{im\\theta} \\\\\n&amp;= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&amp;=\n(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n\\]</span></p>\n<p>16</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m) =\n\\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n\\]</span></p>\n<p>1transpose</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n)\n&amp;=\n(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n) =\n\\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n\\]</span></p>\n<p><br>\n<span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&amp;(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))\n\\\\&amp;+ (q_m^{(1)}\\sin(m\\theta) +\nq_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) +\nk_n^{(2)}\\cos(n\\theta))\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta))\n\\\\\n&amp;+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta))\n\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta)\n\\\\\n&amp;-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta)\n+q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n\\]</span></p>\n<p> <span class=\"math display\">\\[\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(g\\)</span></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n=\n&amp;\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\\\\n= &amp;\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) -\ni(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})](\\cos((m -\nn)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 -\nq_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n\\]</span></p>\n<p></p>\n<h2 id=\"\"></h2>\n<p>17</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta)\n+ \\sin(m\\theta)\\sin(n\\theta)}&amp;\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&amp;\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&amp;{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&amp;{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n\\]</span></p>\n<p>2223</p>\n<p><span class=\"math display\">\\[\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n\\]</span></p>\n<p> <span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>\n</p>\n<p></p>\n<h2 id=\"2\">2</h2>\n<p>2 <span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>  <span class=\"math inline\">\\(g\\)</span>\n11</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&amp;-\\sin\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\\\sin m\\theta_0&amp;\\cos\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\cos\nm\\theta_1&amp;-\\sin m\\theta_1&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\sin\nm\\theta_1&amp;\\cos\nm\\theta_1&amp;\\cdots&amp;0&amp;0\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\cos\nm\\theta_{d/2-1}&amp;-\\sin\nm\\theta_{d/2-1}\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\sin\nm\\theta_{d/2-1}&amp;\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n\\]</span></p>\n<p><span class=\"math inline\">\\(d\\)</span>\n <span class=\"math inline\">\\(d/2\\)</span>  <span class=\"math inline\">\\(d/2\\)</span> \n<span class=\"math inline\">\\(\\theta\\)</span></p>\n<p><span class=\"math display\">\\[\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(m\\)</span>\n <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(R_m\\)</span>  <span class=\"math inline\">\\(R_n\\)</span>self-attention</p>\n<p> <span class=\"math inline\">\\(\\theta\\)</span>\nGoogleAttention is All You\nNeed</p>\n<h2 id=\"\"></h2>\n<p>25</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n\\]</span></p>\n<p>element-wise</p>\n<p>LLAMAdecoder</p>\n<h2 id=\"\"></h2>\n<p></p>\n<p></p>\n<p>\n<span class=\"math inline\">\\(\\theta\\)</span>\n</p>\n<p><a href=\"https://arxiv.org/abs/2104.09864\">Roformer</a><a href=\"https://spaces.ac.cn/archives/8265\"></a></p>\n<p> <span class=\"math inline\">\\(d = 128\\)</span>\n</p>\n<img src=\"/a051710f/remote_attenuation.png\" class width=\"775\" height=\"457\" title=\"\">\n<h1 id=\"\"></h1>\n<p>RoPEtransformer</p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Transformerhttps://spaces.ac.cn/archives/8130<br>\n2Transformer2https://spaces.ac.cn/archives/8265<br>\n3RoFormer: Enhanced Transformer with Rotary Position Embedding\nhttps://arxiv.org/abs/2104.09864<br>\n4RoPE\nhttps://zhuanlan.zhihu.com/p/647109286</p>\n<hr>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n","length":14264,"excerpt":"","more":"<p>LLMRoPE</p>\n<h1 id=\"rope\">RoPE</h1>\n<p>RoPERotary Position\nEmbedding2021TransformerRoPE<big><u><strong></strong></u></big></p>\n<p>2023AlibiRoPE20232024RoPEAlibiAlibi</p>\n<p>RoPERoPE</p>\n<h1 id=\"\"></h1>\n<p>RoPE</p>\n<p></p>\n<p>Bert256/512token<br>\n<br>\n<u><strong></strong></u>token-2token-1token-10002token-10001<br>\n<u><strong></strong></u><u><strong></strong></u>self-attention<u><strong></strong></u><br>\n<u><strong></strong></u><u><strong></strong></u><u><strong></strong></u><u><strong></strong></u></p>\n<p></p>\n<p>3</p>\n<h2 id=\"\"></h2>\n<p>self-attention</p>\n<p><span class=\"math display\">\\[\\left.\\left\\{\\begin{array}{l}q_\\mathrm{i}=(x_\\mathrm{i}+p_\\mathrm{i})W_\\mathrm{Q}\\\\k_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{K}\\\\\\nu_\\mathrm{j}=(x_\\mathrm{j}+p_\\mathrm{j})W_\\mathrm{V}\\\\\\mathrm{a_\\mathrm{i,j}}=\\mathrm{softmax}\\left(q_\\mathrm{i}k_\\mathrm{j}^\\top\\right)\\\\o_\\mathrm{i}=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\end{array}\\right.\\right.\\tag{1}\\]</span></p>\n<p><span class=\"math inline\">\\(x_i\\)</span>  <span class=\"math inline\">\\(x_j\\)</span>  <span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(p\\)</span> </p>\n<p><span class=\"math inline\">\\(p\\)</span>\n<span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(p\\)</span>\nattentionsoftmaxelement-wise\naddition</p>\n<p>\n<span class=\"math inline\">\\(x + p\\)</span>  <span class=\"math inline\">\\(x * p\\)</span> </p>\n<h2 id=\"\"></h2>\n<p> <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(p\\)</span>\n</p>\n<p>1 <span class=\"math inline\">\\(e_1 =\nx_ + p_1\\)</span>\n18\n<span class=\"math inline\">\\(e_8 = x_ + p_8\\)</span>  <span class=\"math inline\">\\(e_1\\)</span>  <span class=\"math inline\">\\(e_8\\)</span>\n<u><strong></strong></u></p>\n<p>15121512=512handle</p>\n<p>1 <span class=\"math inline\">\\(q_{i}k_{j}^{T}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{align*}q_ik_j^\\top&amp;=\\left(x_i+p_i\\right)W_\\mathbb{Q}W_K^\\top\\left(x_j+p_j\\right)^\\top\\\\&amp;=\\left(x_iW_\\mathbb{Q}+{\\color{red}p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{\\color{red}W_K^\\top\np_j^\\top}\\right)\\end{align*}\\tag{2}\\]</span></p>\n<p> <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span> </p>\n<h3 id=\"google\">Google</h3>\n<p>GoogleSelf-Attention with Relative\nPosition Representations <span class=\"math inline\">\\(p_iW_\\mathbb{Q}\\)</span>\n\n<span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(W_K^\\top p_j^\\top\\)</span>  <span class=\"math inline\">\\(i\\)</span><span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(R_{ij}^K\\)</span>attention<u><strong>input\nprojection</strong></u></p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left(x_{i}W_{\\mathbb{Q}}\\left(x_{j}W_{\\mathbb{K}}+R_{\\mathbf{i,j}}^{\\mathbf{K}}\\right)^{\\top}\\right)\\tag{3}\n\\]</span></p>\n<p><span class=\"math inline\">\\(R_{ij}^K\\)</span>\nclip</p>\n<p><span class=\"math display\">\\[\nR_{\\mathrm{i,j}}^\\mathrm{K}=p_\\mathrm{K}\\left[\\mathrm{clip(i-j,p_{min},p_{max})}\\right]\n\\]</span></p>\n<p> <span class=\"math inline\">\\(p_\\mathrm{K}\\)</span>\n</p>\n<p>clip<strong></strong>tokentoken256&gt;256</p>\n<p>Googleinput</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}&amp;=\\sum_\\mathrm{j}a_\\mathrm{i,j}\\nu_\\mathrm{j}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\\mathrm{V}}\\\\\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}(x_{j}W_{\\mathrm{V}} +\n{\\color{red}p_{j}W_{\\mathrm{V}}})\\\\\n\\end{align*}\\tag{4}\\]</span></p>\n<p>Google <span class=\"math inline\">\\(p_{j}W_{\\mathrm{V}}\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_{\\mathrm{i}}=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}\\left(x_{j}W_{\\mathrm{V}}+R_{\\mathrm{i,j}}^{\\mathrm{V}}\\right)\\tag{5}\n\\end{align*}\\]</span></p>\n<p><span class=\"math inline\">\\(R_{\\mathrm{i,j}}^{\\mathrm{V}}\\)</span> \n<span class=\"math inline\">\\(R_{ij}^K\\)</span> \n+ clip</p>\n<h3 id=\"xlnet\">XLNET</h3>\n<p>XLNETGoogle</p>\n<p>2</p>\n<p><span class=\"math display\">\\[\\begin{align*}\nq_ik_j^T\n&amp;= \\left(x_iW_\\mathbb{Q}+{p_iW_\\mathbb{Q}}\\right)\\left(W_K^\\top\nx_j^\\top+{W_K^\\top p_j^\\top}\\right)\\\\\n&amp;=\nx_iW_\\mathbb{Q}W_\\mathbb{K}^Tx_j^T\n+x_iW_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{x_j^T}\n+{\\color{red}p_i}W_\\mathbb{Q}W_\\mathbb{K}^T{\\color{red}p_j^T}\\\\\n\\end{align*}\\tag{6}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu}\nW_\\mathrm{Q}W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\\right)\n\\tag{7}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(p_i\\)</span> \n<span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span>  <span class=\"math inline\">\\(p_j\\)</span>  <span class=\"math inline\">\\(R_{i-j}^\\top\\)</span> </p>\n<p> <span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span>\n <span class=\"math inline\">\\(u\\)</span>  <span class=\"math inline\">\\(\\nu\\)</span> </p>\n<p><span class=\"math display\">\\[\nx_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top {\\color{red}R_\\mathrm{i-j}^\\top}\n+{\\color{red}u}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+{\\color{red}\\nu} W_\\mathrm{K}^\\top{\\color{red}R_\\mathrm{i-j}^\\top}\n\\tag{8}\n\\]</span></p>\n<p>XLNET</p>\n<p><span class=\"math display\">\\[\\begin{align*}\no_\\mathrm{i}\n&amp;=\\sum_{\\mathrm{j}}\\mathrm{a_{i,j}}x_{j}W_{\\mathrm{V}}\\\\\n\\end{align*}\\tag{9}\\]</span></p>\n<p>GoogleXLNET <span class=\"math inline\">\\(\\mathrm{a_{i,j}}\\)</span>\n2\n<span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span>\nclip</p>\n<p>T5</p>\n<h3 id=\"t5\">T5</h3>\n<p>6\n<span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(j\\)</span>\n</p>\n<p><span class=\"math display\">\\[\n\\mathrm{a_{ij}=softmax}\\left\n(x_iW_\\mathrm{Q}W_\\mathrm{K}^\\top x_\\mathrm{j}^\\top\n+ \\beta_{i,j}\\right)\n\\tag{10}\n\\]</span></p>\n<p>XLNETDeBertaT5</p>\n<h2 id=\"\"></h2>\n<p>attention</p>\n<p>1softmax33</p>\n<p>8433</p>\n<p></p>\n<p></p>\n<p>self-attentionlinear\nattention</p>\n<h1 id=\"rope\">RoPE</h1>\n<h2 id=\"attention\">attention</h2>\n<p>RoPE</p>\n<p></p>\n<p>self-attention1\n=  +\nsoftmaxsoftmax</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\n\\tag{11}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(q_m\\)</span>  <span class=\"math inline\">\\(m\\)</span> query<span class=\"math inline\">\\(k_n\\)</span>  <span class=\"math inline\">\\(n\\)</span> key<span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>\nquerykey</p>\n<p> <span class=\"math inline\">\\(f_q\\)</span> \n<span class=\"math inline\">\\(f_k\\)</span>  <span class=\"math inline\">\\(g\\)</span> 11</p>\n<p>RoPE</p>\n<h2 id=\"\"></h2>\n<p>11 <span class=\"math inline\">\\(g\\)</span>\n</p>\n<p>2</p>\n<img src=\"/a051710f/complex_number.png\" class width=\"282\" height=\"401\" title=\"\">\n<p>querykey2<br>\n hidden size = 2 </p>\n<p>211Roformer</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)&amp;=\\boldsymbol{q}_me^{im\\theta}=\\left(\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)e^{im\\theta}\n\\\\\nf_k(\\boldsymbol{k}_n,n)&amp;=\\boldsymbol{k}_ne^{in\\theta}=(\\boldsymbol{W}_k\\boldsymbol{x}_n)e^{in\\theta}\n\\\\\ng(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)&amp;=\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n=\\mathrm{Re}\\left[(\\boldsymbol{W}_q\\boldsymbol{x}_m)(\\boldsymbol{W}_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}\\right]\\\\\n\\end{aligned} \\\\\n\\tag{12}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\boldsymbol{k}_n^*\\)</span>  <span class=\"math inline\">\\(\\boldsymbol{k}_n\\)</span> </p>\n<p></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{gathered}\nz=a+ib \\\\\nz^*=a-ib\n\\end{gathered}\n\\tag{13}\n\\]</span></p>\n<p>11</p>\n<p></p>\n<p></p>\n<p><span class=\"math display\">\\[\ne^{ix}=\\cos x+i\\sin x\n\\tag{14}\n\\]</span></p>\n<p>22 <span class=\"math inline\">\\(q_m\\)</span> </p>\n<p><span class=\"math display\">\\[\nq_m = q_m^{(1)} + iq_m^{(2)}\n\\tag{15}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_q(\\boldsymbol{q}_m,m)\n&amp;= \\boldsymbol{q}_me^{im\\theta} \\\\\n&amp;= (q_m^{(1)} + iq_m^{(2)})(\\cos (m\\theta)+i\\sin (m\\theta)) \\\\\n&amp;=\n(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))+i(q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta))\n\\end{aligned}\n\\tag{16}\n\\]</span></p>\n<p>16</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m) =\n\\left.\\left[\\begin{matrix}{q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta)}\\\\{q_m^{(1)}\\sin(m\\theta)\n+ q_m^{(2)}\\cos(m\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{17}\n\\]</span></p>\n<p>1transpose</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\nf_k(\\boldsymbol{k}_n,n)\n&amp;=\n(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))+i(k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta))\n\\end{aligned}\n\\tag{18}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n) =\n\\left.\\left[\\begin{matrix}{k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta)}\\\\{k_n^{(1)}\\sin(n\\theta)\n+ k_n^{(2)}\\cos(n\\theta)}\\end{matrix}\\right.\\right]^\\top\n\\tag{19}\n\\]</span></p>\n<p><br>\n<span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\=&amp;(q_m^{(1)}cos(m\\theta)-q_m^{(2)}\\sin(m\\theta))(k_n^{(1)}cos(n\\theta)-k_n^{(2)}\\sin(n\\theta))\n\\\\&amp;+ (q_m^{(1)}\\sin(m\\theta) +\nq_m^{(2)}\\cos(m\\theta))(k_n^{(1)}\\sin(n\\theta) +\nk_n^{(2)}\\cos(n\\theta))\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\left(\\cos(m\\theta)\\cos(n\\theta)+\\sin(m\\theta)\\sin(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(1)}k_n^{(2)}\\left(-\\cos(m\\theta)\\sin(n\\theta)+\\sin(m\\theta)\\cos(n\\theta)\\right)\n\\\\\n&amp;+q_m^{(2)}k_n^{(1)}(-\\sin(m\\theta)\\cos(n\\theta)+\\cos(m\\theta)\\sin(n\\theta))\n\\\\\n&amp;+q_m^{(2)}k_n^{(2)}(\\sin(m\\theta)\\sin(n\\theta)+\\cos(m\\theta)\\cos(n\\theta))\n\\\\\n=&amp;q_m^{(1)}k_n^{(1)}\\cos((m-n)\\theta)+q_m^{(1)}k_n^{(2)}\\sin((m-n)\\theta)\n\\\\\n&amp;-\\left.q_m^{(2)}k_n^{(1)}\\right.\\sin((m-n)\\theta)\n+q_m^{(2)}k_n^{(2)}\\cos((m-n)\\theta)\\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\n\\end{aligned}\n\\tag{20}\n\\]</span></p>\n<p> <span class=\"math display\">\\[\n\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\\n{\\cos(\\alpha\\pm\\beta)=\\cos\\alpha\\cos\\beta\\mp\\sin\\alpha\\sin\\beta}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(g\\)</span></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;g(\\boldsymbol{q}_m,\\boldsymbol{k}_n,m-n)\\\\\n=\n&amp;\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\\\\n= &amp;\\mathrm{Re}\\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) -\ni(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})](\\cos((m -\nn)\\theta) + i\\sin((m-n)\\theta))\\right] \\\\\n= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m - n)\\theta) +\n(q_m^{(1)}k_n^2 -\nq_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta)\\\\\n= &amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\n\\end{aligned}\n\\tag{21}\n\\]</span></p>\n<p></p>\n<h2 id=\"\"></h2>\n<p>17</p>\n<p><span class=\"math display\">\\[\nf_q(\\boldsymbol{q}_m,m)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{q_m^{(1)}}\\\\{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{22}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\nf_k(\\boldsymbol{k}_n,n)^\\top =\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\tag{23}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos(m\\theta)}&amp;{\\sin(m\\theta)}\\\\{-\\sin(m\\theta)}&amp;{\\cos(m\\theta)}\\end{matrix}\\right.\\right]\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\\left.\\left[\\begin{matrix}{\\cos(m\\theta)\\cos(n\\theta)\n+ \\sin(m\\theta)\\sin(n\\theta)}&amp;\n{-\\cos(m\\theta)\\sin(n\\theta) + \\sin(m\\theta)\\cos(n\\theta)}\\\\\n{-\\cos(n\\theta)\\sin(m\\theta) + \\cos(m\\theta)\\sin(n\\theta)}&amp;\n{\\sin(m\\theta)\\sin(n\\theta) + \\cos(m\\theta)\\cos(n\\theta)}\n\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\\\\\n=&amp;{\\left.\\left[\\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\\end{matrix}\\right.\\right]}\n\\left.\\left[\\begin{matrix}{\\cos((m-n)\\theta)}&amp;{\\sin((m-n)\\theta)}\\\\{-\\sin((m-n)\\theta)}&amp;{\\cos((m-n)\\theta)}\\end{matrix}\\right.\\right]\n{\\left.\\left[\\begin{matrix}{k_n^{(1)}}\\\\{k_n^{(2)}}\\end{matrix}\\right.\\right]}\n\\end{aligned}\n\\tag{24}\n\\]</span></p>\n<p>2223</p>\n<p><span class=\"math display\">\\[\n\\left.\\left[\\begin{matrix}{\\cos(n\\theta)}&amp;{-\\sin(n\\theta)}\\\\{\\sin(n\\theta)}&amp;{\\cos(n\\theta)}\\end{matrix}\\right.\\right]\n\\]</span></p>\n<p> <span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>\n</p>\n<p></p>\n<h2 id=\"2\">2</h2>\n<p>2 <span class=\"math inline\">\\(f_q\\)</span>  <span class=\"math inline\">\\(f_k\\)</span>  <span class=\"math inline\">\\(g\\)</span>\n11</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&amp;-\\sin\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\\\sin m\\theta_0&amp;\\cos\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\cos\nm\\theta_1&amp;-\\sin m\\theta_1&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\sin\nm\\theta_1&amp;\\cos\nm\\theta_1&amp;\\cdots&amp;0&amp;0\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\cos\nm\\theta_{d/2-1}&amp;-\\sin\nm\\theta_{d/2-1}\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\sin\nm\\theta_{d/2-1}&amp;\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\tag{25}\n\\]</span></p>\n<p><span class=\"math inline\">\\(d\\)</span>\n <span class=\"math inline\">\\(d/2\\)</span>  <span class=\"math inline\">\\(d/2\\)</span> \n<span class=\"math inline\">\\(\\theta\\)</span></p>\n<p><span class=\"math display\">\\[\n\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}\n\\tag{26}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(m\\)</span>\n <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(R_m\\)</span>  <span class=\"math inline\">\\(R_n\\)</span>self-attention</p>\n<p> <span class=\"math inline\">\\(\\theta\\)</span>\nGoogleAttention is All You\nNeed</p>\n<h2 id=\"\"></h2>\n<p>25</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\tag{27}\n\\]</span></p>\n<p>element-wise</p>\n<p>LLAMAdecoder</p>\n<h2 id=\"\"></h2>\n<p></p>\n<p></p>\n<p>\n<span class=\"math inline\">\\(\\theta\\)</span>\n</p>\n<p><a href=\"https://arxiv.org/abs/2104.09864\">Roformer</a><a href=\"https://spaces.ac.cn/archives/8265\"></a></p>\n<p> <span class=\"math inline\">\\(d = 128\\)</span>\n</p>\n<img src=\"/a051710f/remote_attenuation.png\" class width=\"775\" height=\"457\" title=\"\">\n<h1 id=\"\"></h1>\n<p>RoPEtransformer</p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Transformerhttps://spaces.ac.cn/archives/8130<br>\n2Transformer2https://spaces.ac.cn/archives/8265<br>\n3RoFormer: Enhanced Transformer with Rotary Position Embedding\nhttps://arxiv.org/abs/2104.09864<br>\n4RoPE\nhttps://zhuanlan.zhihu.com/p/647109286</p>\n<hr>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n"},{"title":"LLM","abbrlink":"c4da56c0","date":"2024-02-28T07:19:28.000Z","_content":"\n  \n\nRoPERoPE[](http://www.linsight.cn/a051710f.html) [](https://zhuanlan.zhihu.com/p/684072868) [](https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&mid=2247483759&idx=1&sn=f7b59b879476b8687a340606b5568eae&chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&token=88551061&lang=zh_CN#rd)\n\n#   \n\n2023LLM20235Claude100k tokens67ChatGPT3.516kChatGLM2-B32k  \n\nChatGLMAgentChatGLM3ChatGLM4  \n\nLM-SYSLongChatMosaicLMMPT16k\n\nQwen-1.532k  \n\n<center>\n\n|  |  |\n| :----: | :----: |\n| Baichuan2 | 192k |\n| GPT4-turbo  | 128k |\n| Yi | 200k |\n| Kimi Chat | 128k(20) |\n| Claude2 | 200k |  \n\n</center>\n\n  \n\n  \n\n#   \n\ntokenizertokentoken>1.5tokenizer2200ktoken30w  \n\n27  \n\n<big><u>****</u></big>  \n\nRAGRetrieval-augmented generationRAG  \n\n<big><u>****</u></big>prompt  \n\nprompt  \n\n1ppl2attention\n\n# \n\n  \n\n2k/4k8k16kPPLRoPE<u>****</u>  \n\n## \n\n2k/4k8k/16k/32k+  \n\n  \n\n1.  \n\n32k  \n\n4k8attention maskattention mask  \n\n>  \n\n2.  \n\ntransformer  \n\n $l$  $V$ hidden size $h$ batch size $b$  $s$ Adam1  \n\n(1) \n\n $\\Phi$  =  + $l$ * decoder = $Vh + l(12h^2 + 13h)$  \n\n $s$   \n\n(2)   \n\n = logits + $l$ *  $\\approx2bshV + l*(24bsh^2+4bs^2h)$\n\nsoftmaxsoftmax $s$ \n\n\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\frac{}{}\n&=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n$$\n\n $s << h$  $h$ 1k1w $s$  $sh$   \n\n(3)   \n\noptimizer\n\n$\\Phi$$\\Phi$$2\\Phi$ $(\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi + 2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]$ \n\n{% asset_img mix_precision_fp16.png  %}  \n\n\n\n  \n\nsoftmaxdropout  \n\nattention $x$  $QKV$  $x$  $QK$  $QK$ softmax $QK^T$  $11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2$  $l$  $l$   \n\n $s$ 4k32k64GPUbatch sizegradient accumulation<big><u>****</u></big>  \n\n2B7B16k32k200k34B70B+  \n\n2k4k  \n\n  \n\n##  Position Interpolation\n\n236Meta[EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION](https://arxiv.org/pdf/2306.15595.pdf)RoPEPIPosition Interpolation2k32k1kstep\n\n{% asset_img meta_pi.png PI %}  \n{% asset_img LLM/meta_pi.png PI %}  \n\n>In contrast, LLaMA models that are extended via direct fine-tuning only saw a minimal increase of the effective context window size kmax from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear indication of an acceleration in the increase of window size.  \n\n1w20482560\n\nRoPE  \n\nRoPERoPE $\\left|m-n \\right|$ <2048attention $\\left|m-n \\right|$ \n\n{% asset_img meta_rope_ext.png RoPE %}  \n\n3000attention score\n\n\n\nPI  \n\n{% asset_img meta_pi_nosft.png PI %}  \n\n2k2k2k4k\n\n{% asset_img meta_pi_explanation.png PI %}  \n\n123...11.522.5...0.5  \n\nattention score $\\tilde{a}(s)=a(Ls/L^{\\prime})$ $L$ 2048$L^{\\prime}$ 8k/16k/32k\n\nRoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}'(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L'}\\right)\n\\end{aligned}\n\\end{equation}\n$$\n\n $m$ 1 ${L}/{L'}$  \n\n  \n\n\n\n\n\n## NTK-Aware Interpolation \n\ncosNTK-Aware InterpolationRoPE<u>****</u>NTK-Aware Scaled RoPECodeLlama1M  \n\nNTKNeural Tangent KernelGLM4  \n\n>Neural Tangent Kernel (NTK) NTK   \n Neural Tangent Kernel  \nNTK   \nNTK \n\nNTK  \n\n  \n\nRoPE $m$   \n\n{% asset_img rope_matrix.png RoPE %}  \n\n22 $d/2$  $\\theta_j=10000^{-2j/d}$  $j$ $j$  $base=10000$  $base$   \n\n  \n\n  \n\n[RoPE](https://www.zhihu.com/people/us4ever)2  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle= \\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n$$  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n $h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*$ $s=m-n$   \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n $\\alpha=L'/L>1$  $s$   \n\nNTK-Aware Scaled RoPE $\\theta_j$ baseRoPE10000  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n $\\theta$  $\\alpha^{\\frac{-2j}{d-2}}$  $j$  $\\alpha^{\\frac{-2j}{d-2}}$ 1 $j$  $j$ 0 $d/2 - 1$$\\alpha^{\\frac{-2j}{d-2}}$  $\\alpha^{-1}$ \n\n[](https://zhuanlan.zhihu.com/p/645770522)NTK-Aware Interpolation  \n\n>RoPE 12 3 60  RoPE 1/60  1/60 4 RoPE NTK-Aware RoPE  1.5  2  90  24  129.6k  43.2k   \n\nRoPE[](https://kexue.fm/archives/9675)  \n\nYaRN[](https://arxiv.org/pdf/2309.00071.pdf)NTK  \n\n>Given the results from [6], this method performs much better at extending the context size of non-finetuned models compared to PI [9]. However, one major disadvantage of this method is that given it is not just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\" interpolation [6] yields inferior results to PI [9]. Furthermore, due to the \"out-of-bound\" values, the theoretical scale factor s does not accurately describe the true context extension scale. In practice, the scale value s has to be set higher than the expected scale for a given context length extension.  \n\nNTK4k32k $\\alpha=L'/L$ 816\n\n## NTK-by-parts\n\nNTK-by-partsNTKNTK-awareRoPENTK-by-parts  \n\n $j$ RoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n$\\lambda_j$  $j$   \n\n $j$  $L$ RoPE $sin$ 1/40~1-1~0 $j$   \n\nNTK-by-parts  \n\n-  $j$  $\\lambda_j$    \n-  $\\lambda_j\\geq$   \n- NTK-aware interpolation  \n\n $r(j)=\\frac{L}{\\lambda_j}$  $\\beta_1\\beta_2$  $r(j)<\\beta_1$  $r(j)\\geq \\beta_2$   \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&if&r(j)<\\beta_1\\\\1&if&r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n$$  \n\nNTK-by-parts $\\theta_j$   \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n$$\n\n $\\beta_1\\beta_2$  $\\beta_1=1\\beta_2=32$ 1/32   \n\n## Dynamically NTK Scaled RoPE  \n\nNTK-Aware InterpolationRoPEattention score $l$  $L$  $\\alpha$ baseDynamically NTK Scaled RoPENTK  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n$$  \n\n $l$  $l>L$  $\\alpha$ 1 $l\\leq L$   \n\nkv-cacheRoPE  \n\n## YaRN  \n\ntokensoftmaxRoPEtoken  \n\nRoPEsoftmaxlogitsoftmax $t>1$ RoPE $\\sqrt{t}$ RoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n$$  \n\nLlama 1Llama 2$\\begin{aligned}\\sqrt{\\frac1t}&=0.1\\ln(\\alpha)+1.\\end{aligned}$Llama  \n\nYaRNNTK-by-partsattention score  \n\nYaRN\n\n## logn  \n\nlognattention $\\sqrt{d}$ logn[](https://zhuanlan.zhihu.com/p/678755776)YaRN  \n\ntokentokentokenattention score  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L'}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n$$  \n\n $L'>L$ YaRN\n\n## \n\nwindow attentionstreaming LLMLongLoRAFocus Transformer\n\n#   \n\n2k4k  \n\n-   \n- token  \n\nattention score  \n\nPINTKNTKlognYaRN  \n\n# Reference  \n1transformerKV cache https://zhuanlan.zhihu.com/p/624740065  \n2EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION https://arxiv.org/pdf/2306.15595.pdf  \n3Transformer10RoPE https://kexue.fm/archives/9675  \n4YaRN: Efficient Context Window Extension of Large\nLanguage Models https://arxiv.org/pdf/2309.00071.pdf  \n5RoPE https://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ  \n6LLM https://zhuanlan.zhihu.com/p/645770522\n7prompt https://cloud.tencent.com/developer/article/2330611  \n8Transformer8 https://spaces.ac.cn/archives/9444  \n9RoPE192K https://zhuanlan.zhihu.com/p/678755776\n***\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)\n","source":"_posts/cs/nlp/2024/02/LLM.md","raw":"---\ntitle: LLM\nabbrlink: c4da56c0\ndate: 2024-02-28 15:19:28\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  \n\nRoPERoPE[](http://www.linsight.cn/a051710f.html) [](https://zhuanlan.zhihu.com/p/684072868) [](https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&mid=2247483759&idx=1&sn=f7b59b879476b8687a340606b5568eae&chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&token=88551061&lang=zh_CN#rd)\n\n#   \n\n2023LLM20235Claude100k tokens67ChatGPT3.516kChatGLM2-B32k  \n\nChatGLMAgentChatGLM3ChatGLM4  \n\nLM-SYSLongChatMosaicLMMPT16k\n\nQwen-1.532k  \n\n<center>\n\n|  |  |\n| :----: | :----: |\n| Baichuan2 | 192k |\n| GPT4-turbo  | 128k |\n| Yi | 200k |\n| Kimi Chat | 128k(20) |\n| Claude2 | 200k |  \n\n</center>\n\n  \n\n  \n\n#   \n\ntokenizertokentoken>1.5tokenizer2200ktoken30w  \n\n27  \n\n<big><u>****</u></big>  \n\nRAGRetrieval-augmented generationRAG  \n\n<big><u>****</u></big>prompt  \n\nprompt  \n\n1ppl2attention\n\n# \n\n  \n\n2k/4k8k16kPPLRoPE<u>****</u>  \n\n## \n\n2k/4k8k/16k/32k+  \n\n  \n\n1.  \n\n32k  \n\n4k8attention maskattention mask  \n\n>  \n\n2.  \n\ntransformer  \n\n $l$  $V$ hidden size $h$ batch size $b$  $s$ Adam1  \n\n(1) \n\n $\\Phi$  =  + $l$ * decoder = $Vh + l(12h^2 + 13h)$  \n\n $s$   \n\n(2)   \n\n = logits + $l$ *  $\\approx2bshV + l*(24bsh^2+4bs^2h)$\n\nsoftmaxsoftmax $s$ \n\n\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\frac{}{}\n&=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n$$\n\n $s << h$  $h$ 1k1w $s$  $sh$   \n\n(3)   \n\noptimizer\n\n$\\Phi$$\\Phi$$2\\Phi$ $(\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi + 2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]$ \n\n{% asset_img mix_precision_fp16.png  %}  \n\n\n\n  \n\nsoftmaxdropout  \n\nattention $x$  $QKV$  $x$  $QK$  $QK$ softmax $QK^T$  $11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2$  $l$  $l$   \n\n $s$ 4k32k64GPUbatch sizegradient accumulation<big><u>****</u></big>  \n\n2B7B16k32k200k34B70B+  \n\n2k4k  \n\n  \n\n##  Position Interpolation\n\n236Meta[EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION](https://arxiv.org/pdf/2306.15595.pdf)RoPEPIPosition Interpolation2k32k1kstep\n\n{% asset_img meta_pi.png PI %}  \n{% asset_img LLM/meta_pi.png PI %}  \n\n>In contrast, LLaMA models that are extended via direct fine-tuning only saw a minimal increase of the effective context window size kmax from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear indication of an acceleration in the increase of window size.  \n\n1w20482560\n\nRoPE  \n\nRoPERoPE $\\left|m-n \\right|$ <2048attention $\\left|m-n \\right|$ \n\n{% asset_img meta_rope_ext.png RoPE %}  \n\n3000attention score\n\n\n\nPI  \n\n{% asset_img meta_pi_nosft.png PI %}  \n\n2k2k2k4k\n\n{% asset_img meta_pi_explanation.png PI %}  \n\n123...11.522.5...0.5  \n\nattention score $\\tilde{a}(s)=a(Ls/L^{\\prime})$ $L$ 2048$L^{\\prime}$ 8k/16k/32k\n\nRoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}'(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L'}\\right)\n\\end{aligned}\n\\end{equation}\n$$\n\n $m$ 1 ${L}/{L'}$  \n\n  \n\n\n\n\n\n## NTK-Aware Interpolation \n\ncosNTK-Aware InterpolationRoPE<u>****</u>NTK-Aware Scaled RoPECodeLlama1M  \n\nNTKNeural Tangent KernelGLM4  \n\n>Neural Tangent Kernel (NTK) NTK   \n Neural Tangent Kernel  \nNTK   \nNTK \n\nNTK  \n\n  \n\nRoPE $m$   \n\n{% asset_img rope_matrix.png RoPE %}  \n\n22 $d/2$  $\\theta_j=10000^{-2j/d}$  $j$ $j$  $base=10000$  $base$   \n\n  \n\n  \n\n[RoPE](https://www.zhihu.com/people/us4ever)2  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n&\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle= \\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n$$  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n $h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*$ $s=m-n$   \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n$$  \n\n $\\alpha=L'/L>1$  $s$   \n\nNTK-Aware Scaled RoPE $\\theta_j$ baseRoPE10000  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n $\\theta$  $\\alpha^{\\frac{-2j}{d-2}}$  $j$  $\\alpha^{\\frac{-2j}{d-2}}$ 1 $j$  $j$ 0 $d/2 - 1$$\\alpha^{\\frac{-2j}{d-2}}$  $\\alpha^{-1}$ \n\n[](https://zhuanlan.zhihu.com/p/645770522)NTK-Aware Interpolation  \n\n>RoPE 12 3 60  RoPE 1/60  1/60 4 RoPE NTK-Aware RoPE  1.5  2  90  24  129.6k  43.2k   \n\nRoPE[](https://kexue.fm/archives/9675)  \n\nYaRN[](https://arxiv.org/pdf/2309.00071.pdf)NTK  \n\n>Given the results from [6], this method performs much better at extending the context size of non-finetuned models compared to PI [9]. However, one major disadvantage of this method is that given it is not just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\" interpolation [6] yields inferior results to PI [9]. Furthermore, due to the \"out-of-bound\" values, the theoretical scale factor s does not accurately describe the true context extension scale. In practice, the scale value s has to be set higher than the expected scale for a given context length extension.  \n\nNTK4k32k $\\alpha=L'/L$ 816\n\n## NTK-by-parts\n\nNTK-by-partsNTKNTK-awareRoPENTK-by-parts  \n\n $j$ RoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n$$  \n\n$\\lambda_j$  $j$   \n\n $j$  $L$ RoPE $sin$ 1/40~1-1~0 $j$   \n\nNTK-by-parts  \n\n-  $j$  $\\lambda_j$    \n-  $\\lambda_j\\geq$   \n- NTK-aware interpolation  \n\n $r(j)=\\frac{L}{\\lambda_j}$  $\\beta_1\\beta_2$  $r(j)<\\beta_1$  $r(j)\\geq \\beta_2$   \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&if&r(j)<\\beta_1\\\\1&if&r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n$$  \n\nNTK-by-parts $\\theta_j$   \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n$$\n\n $\\beta_1\\beta_2$  $\\beta_1=1\\beta_2=32$ 1/32   \n\n## Dynamically NTK Scaled RoPE  \n\nNTK-Aware InterpolationRoPEattention score $l$  $L$  $\\alpha$ baseDynamically NTK Scaled RoPENTK  \n\n  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n$$  \n\n $l$  $l>L$  $\\alpha$ 1 $l\\leq L$   \n\nkv-cacheRoPE  \n\n## YaRN  \n\ntokensoftmaxRoPEtoken  \n\nRoPEsoftmaxlogitsoftmax $t>1$ RoPE $\\sqrt{t}$ RoPE  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n$$  \n\nLlama 1Llama 2$\\begin{aligned}\\sqrt{\\frac1t}&=0.1\\ln(\\alpha)+1.\\end{aligned}$Llama  \n\nYaRNNTK-by-partsattention score  \n\nYaRN\n\n## logn  \n\nlognattention $\\sqrt{d}$ logn[](https://zhuanlan.zhihu.com/p/678755776)YaRN  \n\ntokentokentokenattention score  \n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L'}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n$$  \n\n $L'>L$ YaRN\n\n## \n\nwindow attentionstreaming LLMLongLoRAFocus Transformer\n\n#   \n\n2k4k  \n\n-   \n- token  \n\nattention score  \n\nPINTKNTKlognYaRN  \n\n# Reference  \n1transformerKV cache https://zhuanlan.zhihu.com/p/624740065  \n2EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION https://arxiv.org/pdf/2306.15595.pdf  \n3Transformer10RoPE https://kexue.fm/archives/9675  \n4YaRN: Efficient Context Window Extension of Large\nLanguage Models https://arxiv.org/pdf/2309.00071.pdf  \n5RoPE https://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ  \n6LLM https://zhuanlan.zhihu.com/p/645770522\n7prompt https://cloud.tencent.com/developer/article/2330611  \n8Transformer8 https://spaces.ac.cn/archives/9444  \n9RoPE192K https://zhuanlan.zhihu.com/p/678755776\n***\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)\n","slug":"cs/nlp/2024/02/LLM","published":1,"updated":"2024-03-13T07:23:38.942Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmj00030p4ka4mbe3g6","content":"<p></p>\n<p>RoPERoPE<a href=\"http://www.linsight.cn/a051710f.html\"></a> <a href=\"https://zhuanlan.zhihu.com/p/684072868\"></a> <a href=\"https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&amp;mid=2247483759&amp;idx=1&amp;sn=f7b59b879476b8687a340606b5568eae&amp;chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&amp;token=88551061&amp;lang=zh_CN#rd\"></a></p>\n<h1 id=\"\"></h1>\n<p>2023LLM20235Claude100k\ntokens67ChatGPT3.516kChatGLM2-B32k</p>\n<p>ChatGLMAgentChatGLM3ChatGLM4</p>\n<p>LM-SYSLongChatMosaicLMMPT16k</p>\n<p>Qwen-1.532k</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Baichuan2</td>\n<td style=\"text-align: center;\">192k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">GPT4-turbo</td>\n<td style=\"text-align: center;\">128k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Yi</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Kimi Chat</td>\n<td style=\"text-align: center;\">128k(20)</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Claude2</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p></p>\n<p></p>\n<h1 id=\"\"></h1>\n<p>tokenizertokentoken&gt;1.5tokenizer2200ktoken30w</p>\n<p>27</p>\n<p><big><u><strong></strong></u></big></p>\n<p>RAGRetrieval-augmented\ngenerationRAG</p>\n<p><big><u><strong></strong></u></big>prompt</p>\n<p>prompt</p>\n<p>1ppl2attention</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>2k/4k8k16kPPLRoPE<u><strong></strong></u></p>\n<h2 id=\"\"></h2>\n<p>2k/4k8k/16k/32k+</p>\n<p></p>\n<p>1.</p>\n<p>32k</p>\n<p>4k8attention\nmaskattention\nmask</p>\n<p>&gt;</p>\n<p>2.</p>\n<p>transformer</p>\n<p> <span class=\"math inline\">\\(l\\)</span> \n<span class=\"math inline\">\\(V\\)</span> hidden size <span class=\"math inline\">\\(h\\)</span> batch size <span class=\"math inline\">\\(b\\)</span>  <span class=\"math inline\">\\(s\\)</span>\nAdam1</p>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p> <span class=\"math inline\">\\(\\Phi\\)</span> = \n+ <span class=\"math inline\">\\(l\\)</span> * decoder = <span class=\"math inline\">\\(Vh + l(12h^2 + 13h)\\)</span></p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p> = logits + <span class=\"math inline\">\\(l\\)</span> *  <span class=\"math inline\">\\(\\approx2bshV + l*(24bsh^2+4bs^2h)\\)</span></p>\n<p>softmaxsoftmax <span class=\"math inline\">\\(s\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{}{}\n&amp;=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&amp;\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(s &lt;&lt; h\\)</span>\n <span class=\"math inline\">\\(h\\)</span> 1k1w\n<span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(sh\\)</span>\n</p>\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>optimizer</p>\n<p><span class=\"math inline\">\\(\\Phi\\)</span><span class=\"math inline\">\\(\\Phi\\)</span><span class=\"math inline\">\\(2\\Phi\\)</span>\n<span class=\"math inline\">\\((\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi +\n2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]\\)</span>\n</p>\n<img src=\"/c4da56c0/mix_precision_fp16.png\" class title=\"\">\n<p></p>\n<p></p>\n<p>softmaxdropout</p>\n<p>attention <span class=\"math inline\">\\(x\\)</span>\n <span class=\"math inline\">\\(QKV\\)</span>  <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(QK\\)</span>  <span class=\"math inline\">\\(QK\\)</span> softmax\n<span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2\\)</span> \n<span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(l\\)</span> </p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n4k32k64GPUbatch\nsizegradient\naccumulation<big><u><strong></strong></u></big></p>\n<p>2B7B16k32k200k34B70B+</p>\n<p>2k4k</p>\n<p></p>\n<h2 id=\"-position-interpolation\"> Position\nInterpolation</h2>\n<p>236Meta<a href=\"https://arxiv.org/pdf/2306.15595.pdf\">EXTENDING CONTEXT WINDOW\nOF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION</a>RoPEPIPosition\nInterpolation2k32k1kstep</p>\n<img src=\"/c4da56c0/meta_pi.png\" class title=\"PI\">\n\n<blockquote>\n<p>In contrast, LLaMA models that are extended via direct fine-tuning\nonly saw a minimal increase of the effective context window size kmax\nfrom 2048 to 2560, even after fine-tuning for more than 10000 steps,\nwith no clear indication of an acceleration in the increase of window\nsize.</p>\n</blockquote>\n<p>1w20482560</p>\n<p>RoPE</p>\n<p>RoPERoPE\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n&lt;2048attention\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n</p>\n<img src=\"/c4da56c0/meta_rope_ext.png\" class title=\"RoPE\">\n<p>3000attention\nscore</p>\n<p></p>\n<p>PI</p>\n<img src=\"/c4da56c0/meta_pi_nosft.png\" class title=\"PI\">\n<p>2k2k2k4k</p>\n<img src=\"/c4da56c0/meta_pi_explanation.png\" class title=\"PI\">\n<p>123...11.522.5...0.5</p>\n<p>attention\nscore <span class=\"math inline\">\\(\\tilde{a}(s)=a(Ls/L^{\\prime})\\)</span> <span class=\"math inline\">\\(L\\)</span> 2048<span class=\"math inline\">\\(L^{\\prime}\\)</span>\n8k/16k/32k</p>\n<p>RoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}&#39;(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L&#39;}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(m\\)</span> 1\n<span class=\"math inline\">\\({L}/{L&#39;}\\)</span></p>\n<p></p>\n<p></p>\n<p></p>\n<h2 id=\"ntk-aware-interpolation\">NTK-Aware Interpolation</h2>\n<p>cosNTK-Aware\nInterpolationRoPE<u><strong></strong></u>NTK-Aware\nScaled RoPECodeLlama1M</p>\n<p>NTKNeural Tangent\nKernelGLM4</p>\n<blockquote>\n<p>Neural Tangent Kernel (NTK)\nNTK\n<br>\n\nNeural Tangent\nKernel<br>\nNTK\n<br>\nNTK\n</p>\n</blockquote>\n<p>NTK</p>\n<p></p>\n<p>RoPE <span class=\"math inline\">\\(m\\)</span>\n</p>\n<img src=\"/c4da56c0/rope_matrix.png\" class title=\"RoPE\">\n<p>22 <span class=\"math inline\">\\(d/2\\)</span>\n\n<span class=\"math inline\">\\(\\theta_j=10000^{-2j/d}\\)</span> \n<span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(base=10000\\)</span>  <span class=\"math inline\">\\(base\\)</span>\n</p>\n<p></p>\n<p></p>\n<p><a href=\"https://www.zhihu.com/people/us4ever\">RoPE</a>2</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=\n\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&amp;\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*\\)</span>\n<span class=\"math inline\">\\(s=m-n\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha=L&#39;/L&gt;1\\)</span>\n <span class=\"math inline\">\\(s\\)</span> </p>\n<p>NTK-Aware Scaled RoPE <span class=\"math inline\">\\(\\theta_j\\)</span>\nbaseRoPE10000</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\theta\\)</span>  <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>  <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>\n1 <span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(j\\)</span> 0\n<span class=\"math inline\">\\(d/2 - 1\\)</span><span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>  <span class=\"math inline\">\\(\\alpha^{-1}\\)</span> </p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/645770522\"></a>NTK-Aware\nInterpolation</p>\n<blockquote>\n<p>RoPE\n12 3 60 \nRoPE 1/60 \n1/60 4 RoPE\nNTK-Aware\nRoPE  1.5\n 2  90  24\n 129.6k  43.2k\n</p>\n</blockquote>\n<p>RoPE<a href=\"https://kexue.fm/archives/9675\"></a></p>\n<p>YaRN<a href=\"https://arxiv.org/pdf/2309.00071.pdf\"></a>NTK</p>\n<blockquote>\n<p>Given the results from [6], this method performs much better at\nextending the context size of non-finetuned models compared to PI [9].\nHowever, one major disadvantage of this method is that given it is not\njust an interpolation scheme, some dimensions are slightly extrapolated\nto \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\"\ninterpolation [6] yields inferior results to PI [9]. Furthermore, due to\nthe \"out-of-bound\" values, the theoretical scale factor s does not\naccurately describe the true context extension scale. In practice, the\nscale value s has to be set higher than the expected scale for a given\ncontext length extension.</p>\n</blockquote>\n<p>NTK4k32k\n<span class=\"math inline\">\\(\\alpha=L&#39;/L\\)</span>\n816</p>\n<h2 id=\"ntk-by-parts\">NTK-by-parts</h2>\n<p>NTK-by-partsNTKNTK-awareRoPENTK-by-parts</p>\n<p> <span class=\"math inline\">\\(j\\)</span> RoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p><span class=\"math inline\">\\(\\lambda_j\\)</span>\n <span class=\"math inline\">\\(j\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(L\\)</span>\nRoPE\n<span class=\"math inline\">\\(sin\\)</span>\n1/40<sub>1-1</sub>0\n<span class=\"math inline\">\\(j\\)</span>\n</p>\n<p>NTK-by-parts</p>\n<ul>\n<li> <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(\\lambda_j\\)</span> \n<br>\n</li>\n<li> <span class=\"math inline\">\\(\\lambda_j\\geq\\)</span>\n<br>\n</li>\n<li>NTK-aware interpolation</li>\n</ul>\n<p> <span class=\"math inline\">\\(r(j)=\\frac{L}{\\lambda_j}\\)</span>\n <span class=\"math inline\">\\(\\beta_1\\beta_2\\)</span> \n<span class=\"math inline\">\\(r(j)&lt;\\beta_1\\)</span>\n <span class=\"math inline\">\\(r(j)\\geq\n\\beta_2\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&amp;if&amp;r(j)&lt;\\beta_1\\\\1&amp;if&amp;r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&amp;otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>NTK-by-parts <span class=\"math inline\">\\(\\theta_j\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\beta_1\\beta_2\\)</span>\n <span class=\"math inline\">\\(\\beta_1=1\\beta_2=32\\)</span>\n1/32</p>\n<h2 id=\"dynamically-ntk-scaled-rope\">Dynamically NTK Scaled RoPE</h2>\n<p>NTK-Aware\nInterpolationRoPEattention\nscore <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(L\\)</span>  <span class=\"math inline\">\\(\\alpha\\)</span>\nbaseDynamically NTK Scaled\nRoPENTK</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(l\\)</span> \n<span class=\"math inline\">\\(l&gt;L\\)</span>  <span class=\"math inline\">\\(\\alpha\\)</span> 1 <span class=\"math inline\">\\(l\\leq L\\)</span> </p>\n<p>kv-cacheRoPE</p>\n<h2 id=\"yarn\">YaRN</h2>\n<p>tokensoftmaxRoPEtoken</p>\n<p>RoPEsoftmaxlogitsoftmax\n<span class=\"math inline\">\\(t&gt;1\\)</span>\nRoPE <span class=\"math inline\">\\(\\sqrt{t}\\)</span>\nRoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>Llama 1Llama 2<span class=\"math inline\">\\(\\begin{aligned}\\sqrt{\\frac1t}&amp;=0.1\\ln(\\alpha)+1.\\end{aligned}\\)</span>Llama</p>\n<p>YaRNNTK-by-partsattention\nscore</p>\n<p>YaRN</p>\n<h2 id=\"logn\">logn</h2>\n<p>lognattention <span class=\"math inline\">\\(\\sqrt{d}\\)</span>\nlogn<a href=\"https://zhuanlan.zhihu.com/p/678755776\"></a>YaRN</p>\n<p>tokentokentokenattention\nscore</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L&#39;}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(L&#39;&gt;L\\)</span>\nYaRN</p>\n<h2 id=\"\"></h2>\n<p>window\nattentionstreaming LLMLongLoRAFocus\nTransformer</p>\n<h1 id=\"\"></h1>\n<p>2k4k</p>\n<ul>\n<li><br>\n</li>\n<li>token</li>\n</ul>\n<p>attention score</p>\n<p>PINTKNTKlognYaRN</p>\n<h1 id=\"reference\">Reference</h1>\n<p>1transformerKV cache\nhttps://zhuanlan.zhihu.com/p/624740065<br>\n2EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION https://arxiv.org/pdf/2306.15595.pdf<br>\n3Transformer10RoPE\nhttps://kexue.fm/archives/9675<br>\n4YaRN: Efficient Context Window Extension of Large Language Models\nhttps://arxiv.org/pdf/2309.00071.pdf<br>\n5RoPE\nhttps://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ<br>\n6LLM https://zhuanlan.zhihu.com/p/645770522\n7prompt\nhttps://cloud.tencent.com/developer/article/2330611<br>\n8Transformer8\nhttps://spaces.ac.cn/archives/9444<br>\n9RoPE192K\nhttps://zhuanlan.zhihu.com/p/678755776 ***</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n","length":12689,"excerpt":"","more":"<p></p>\n<p>RoPERoPE<a href=\"http://www.linsight.cn/a051710f.html\"></a> <a href=\"https://zhuanlan.zhihu.com/p/684072868\"></a> <a href=\"https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&amp;mid=2247483759&amp;idx=1&amp;sn=f7b59b879476b8687a340606b5568eae&amp;chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&amp;token=88551061&amp;lang=zh_CN#rd\"></a></p>\n<h1 id=\"\"></h1>\n<p>2023LLM20235Claude100k\ntokens67ChatGPT3.516kChatGLM2-B32k</p>\n<p>ChatGLMAgentChatGLM3ChatGLM4</p>\n<p>LM-SYSLongChatMosaicLMMPT16k</p>\n<p>Qwen-1.532k</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Baichuan2</td>\n<td style=\"text-align: center;\">192k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">GPT4-turbo</td>\n<td style=\"text-align: center;\">128k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Yi</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Kimi Chat</td>\n<td style=\"text-align: center;\">128k(20)</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Claude2</td>\n<td style=\"text-align: center;\">200k</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p></p>\n<p></p>\n<h1 id=\"\"></h1>\n<p>tokenizertokentoken&gt;1.5tokenizer2200ktoken30w</p>\n<p>27</p>\n<p><big><u><strong></strong></u></big></p>\n<p>RAGRetrieval-augmented\ngenerationRAG</p>\n<p><big><u><strong></strong></u></big>prompt</p>\n<p>prompt</p>\n<p>1ppl2attention</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>2k/4k8k16kPPLRoPE<u><strong></strong></u></p>\n<h2 id=\"\"></h2>\n<p>2k/4k8k/16k/32k+</p>\n<p></p>\n<p>1.</p>\n<p>32k</p>\n<p>4k8attention\nmaskattention\nmask</p>\n<p>&gt;</p>\n<p>2.</p>\n<p>transformer</p>\n<p> <span class=\"math inline\">\\(l\\)</span> \n<span class=\"math inline\">\\(V\\)</span> hidden size <span class=\"math inline\">\\(h\\)</span> batch size <span class=\"math inline\">\\(b\\)</span>  <span class=\"math inline\">\\(s\\)</span>\nAdam1</p>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p> <span class=\"math inline\">\\(\\Phi\\)</span> = \n+ <span class=\"math inline\">\\(l\\)</span> * decoder = <span class=\"math inline\">\\(Vh + l(12h^2 + 13h)\\)</span></p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p> = logits + <span class=\"math inline\">\\(l\\)</span> *  <span class=\"math inline\">\\(\\approx2bshV + l*(24bsh^2+4bs^2h)\\)</span></p>\n<p>softmaxsoftmax <span class=\"math inline\">\\(s\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{}{}\n&amp;=\\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\\\\n&amp;\\rightarrow bs\\frac{6h+s}{3h}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(s &lt;&lt; h\\)</span>\n <span class=\"math inline\">\\(h\\)</span> 1k1w\n<span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(sh\\)</span>\n</p>\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>optimizer</p>\n<p><span class=\"math inline\">\\(\\Phi\\)</span><span class=\"math inline\">\\(\\Phi\\)</span><span class=\"math inline\">\\(2\\Phi\\)</span>\n<span class=\"math inline\">\\((\\Phi + \\Phi) \\times 2 + (\\Phi + \\Phi +\n2\\Phi) \\times 4 = 20\\Phi = 20[Vh + l(12h^2 + 13h)]\\)</span>\n</p>\n<img src=\"/c4da56c0/mix_precision_fp16.png\" class title=\"\">\n<p></p>\n<p></p>\n<p>softmaxdropout</p>\n<p>attention <span class=\"math inline\">\\(x\\)</span>\n <span class=\"math inline\">\\(QKV\\)</span>  <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(QK\\)</span>  <span class=\"math inline\">\\(QK\\)</span> softmax\n<span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2\\)</span> \n<span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(l\\)</span> </p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n4k32k64GPUbatch\nsizegradient\naccumulation<big><u><strong></strong></u></big></p>\n<p>2B7B16k32k200k34B70B+</p>\n<p>2k4k</p>\n<p></p>\n<h2 id=\"-position-interpolation\"> Position\nInterpolation</h2>\n<p>236Meta<a href=\"https://arxiv.org/pdf/2306.15595.pdf\">EXTENDING CONTEXT WINDOW\nOF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION</a>RoPEPIPosition\nInterpolation2k32k1kstep</p>\n<img src=\"/c4da56c0/meta_pi.png\" class title=\"PI\">\n\n<blockquote>\n<p>In contrast, LLaMA models that are extended via direct fine-tuning\nonly saw a minimal increase of the effective context window size kmax\nfrom 2048 to 2560, even after fine-tuning for more than 10000 steps,\nwith no clear indication of an acceleration in the increase of window\nsize.</p>\n</blockquote>\n<p>1w20482560</p>\n<p>RoPE</p>\n<p>RoPERoPE\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n&lt;2048attention\n<span class=\"math inline\">\\(\\left|m-n \\right|\\)</span>\n</p>\n<img src=\"/c4da56c0/meta_rope_ext.png\" class title=\"RoPE\">\n<p>3000attention\nscore</p>\n<p></p>\n<p>PI</p>\n<img src=\"/c4da56c0/meta_pi_nosft.png\" class title=\"PI\">\n<p>2k2k2k4k</p>\n<img src=\"/c4da56c0/meta_pi_explanation.png\" class title=\"PI\">\n<p>123...11.522.5...0.5</p>\n<p>attention\nscore <span class=\"math inline\">\\(\\tilde{a}(s)=a(Ls/L^{\\prime})\\)</span> <span class=\"math inline\">\\(L\\)</span> 2048<span class=\"math inline\">\\(L^{\\prime}\\)</span>\n8k/16k/32k</p>\n<p>RoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathbf{f}&#39;(\\mathbf{x},m)=\\mathbf{f}\\left(\\mathbf{x},\\frac{mL}{L&#39;}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(m\\)</span> 1\n<span class=\"math inline\">\\({L}/{L&#39;}\\)</span></p>\n<p></p>\n<p></p>\n<p></p>\n<h2 id=\"ntk-aware-interpolation\">NTK-Aware Interpolation</h2>\n<p>cosNTK-Aware\nInterpolationRoPE<u><strong></strong></u>NTK-Aware\nScaled RoPECodeLlama1M</p>\n<p>NTKNeural Tangent\nKernelGLM4</p>\n<blockquote>\n<p>Neural Tangent Kernel (NTK)\nNTK\n<br>\n\nNeural Tangent\nKernel<br>\nNTK\n<br>\nNTK\n</p>\n</blockquote>\n<p>NTK</p>\n<p></p>\n<p>RoPE <span class=\"math inline\">\\(m\\)</span>\n</p>\n<img src=\"/c4da56c0/rope_matrix.png\" class title=\"RoPE\">\n<p>22 <span class=\"math inline\">\\(d/2\\)</span>\n\n<span class=\"math inline\">\\(\\theta_j=10000^{-2j/d}\\)</span> \n<span class=\"math inline\">\\(j\\)</span> <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(base=10000\\)</span>  <span class=\"math inline\">\\(base\\)</span>\n</p>\n<p></p>\n<p></p>\n<p><a href=\"https://www.zhihu.com/people/us4ever\">RoPE</a>2</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n&amp;\\langle f_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=\n\\mathrm{Re}\\left[\\boldsymbol{q}_m\\boldsymbol{k}_n^*e^{i(m-n)\\theta}\\right]\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\langle\nf_q(\\boldsymbol{q}_m,m),f_k(\\boldsymbol{k}_n,n)\\rangle=&amp;\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(h_j=\\boldsymbol{q}_m\\boldsymbol{k}_n^*\\)</span>\n<span class=\"math inline\">\\(s=m-n\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{is\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{Re}[\\sum_j^{d/2}h_je^{i\\frac{s}{\\alpha}\\theta_j}]\\\\\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha=L&#39;/L&gt;1\\)</span>\n <span class=\"math inline\">\\(s\\)</span> </p>\n<p>NTK-Aware Scaled RoPE <span class=\"math inline\">\\(\\theta_j\\)</span>\nbaseRoPE10000</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{base}=base\\times\\alpha^{\\frac{d}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\hat{base}^{-2j/d}=base^{-2j/d}\\times\\alpha^{\\frac{-2j}{d-2}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\theta\\)</span>  <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>  <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>\n1 <span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(j\\)</span> 0\n<span class=\"math inline\">\\(d/2 - 1\\)</span><span class=\"math inline\">\\(\\alpha^{\\frac{-2j}{d-2}}\\)</span>  <span class=\"math inline\">\\(\\alpha^{-1}\\)</span> </p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/645770522\"></a>NTK-Aware\nInterpolation</p>\n<blockquote>\n<p>RoPE\n12 3 60 \nRoPE 1/60 \n1/60 4 RoPE\nNTK-Aware\nRoPE  1.5\n 2  90  24\n 129.6k  43.2k\n</p>\n</blockquote>\n<p>RoPE<a href=\"https://kexue.fm/archives/9675\"></a></p>\n<p>YaRN<a href=\"https://arxiv.org/pdf/2309.00071.pdf\"></a>NTK</p>\n<blockquote>\n<p>Given the results from [6], this method performs much better at\nextending the context size of non-finetuned models compared to PI [9].\nHowever, one major disadvantage of this method is that given it is not\njust an interpolation scheme, some dimensions are slightly extrapolated\nto \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\"\ninterpolation [6] yields inferior results to PI [9]. Furthermore, due to\nthe \"out-of-bound\" values, the theoretical scale factor s does not\naccurately describe the true context extension scale. In practice, the\nscale value s has to be set higher than the expected scale for a given\ncontext length extension.</p>\n</blockquote>\n<p>NTK4k32k\n<span class=\"math inline\">\\(\\alpha=L&#39;/L\\)</span>\n816</p>\n<h2 id=\"ntk-by-parts\">NTK-by-parts</h2>\n<p>NTK-by-partsNTKNTK-awareRoPENTK-by-parts</p>\n<p> <span class=\"math inline\">\\(j\\)</span> RoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\lambda_j=\\frac{2\\pi}{\\theta_j}=2\\pi\\cdot base^{\\frac{2j}{d}}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p><span class=\"math inline\">\\(\\lambda_j\\)</span>\n <span class=\"math inline\">\\(j\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(j\\)</span>\n <span class=\"math inline\">\\(L\\)</span>\nRoPE\n<span class=\"math inline\">\\(sin\\)</span>\n1/40<sub>1-1</sub>0\n<span class=\"math inline\">\\(j\\)</span>\n</p>\n<p>NTK-by-parts</p>\n<ul>\n<li> <span class=\"math inline\">\\(j\\)</span>  <span class=\"math inline\">\\(\\lambda_j\\)</span> \n<br>\n</li>\n<li> <span class=\"math inline\">\\(\\lambda_j\\geq\\)</span>\n<br>\n</li>\n<li>NTK-aware interpolation</li>\n</ul>\n<p> <span class=\"math inline\">\\(r(j)=\\frac{L}{\\lambda_j}\\)</span>\n <span class=\"math inline\">\\(\\beta_1\\beta_2\\)</span> \n<span class=\"math inline\">\\(r(j)&lt;\\beta_1\\)</span>\n <span class=\"math inline\">\\(r(j)\\geq\n\\beta_2\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\left.\\gamma(r)=\\left\\{\\begin{matrix}0&amp;if&amp;r(j)&lt;\\beta_1\\\\1&amp;if&amp;r(j)\\geq\\beta_2\\\\\\frac{r-\\beta_1}{\\beta_2-\\beta_1}&amp;otherwise\\end{matrix}\\right.\\right.\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>NTK-by-parts <span class=\"math inline\">\\(\\theta_j\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\theta_j}=\\left(1-\\gamma(r(j))\\right)\\frac{\\theta_j}s+\\gamma(r(j))\\theta_j\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\beta_1\\beta_2\\)</span>\n <span class=\"math inline\">\\(\\beta_1=1\\beta_2=32\\)</span>\n1/32</p>\n<h2 id=\"dynamically-ntk-scaled-rope\">Dynamically NTK Scaled RoPE</h2>\n<p>NTK-Aware\nInterpolationRoPEattention\nscore <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(L\\)</span>  <span class=\"math inline\">\\(\\alpha\\)</span>\nbaseDynamically NTK Scaled\nRoPENTK</p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}=max(1,\\frac{l}{L})\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(l\\)</span> \n<span class=\"math inline\">\\(l&gt;L\\)</span>  <span class=\"math inline\">\\(\\alpha\\)</span> 1 <span class=\"math inline\">\\(l\\leq L\\)</span> </p>\n<p>kv-cacheRoPE</p>\n<h2 id=\"yarn\">YaRN</h2>\n<p>tokensoftmaxRoPEtoken</p>\n<p>RoPEsoftmaxlogitsoftmax\n<span class=\"math inline\">\\(t&gt;1\\)</span>\nRoPE <span class=\"math inline\">\\(\\sqrt{t}\\)</span>\nRoPE</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{softmax}\\left(\\frac{\\mathbf{q}_m^T\\mathbf{k}_n}{t\\sqrt{d}}\\right)\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p>Llama 1Llama 2<span class=\"math inline\">\\(\\begin{aligned}\\sqrt{\\frac1t}&amp;=0.1\\ln(\\alpha)+1.\\end{aligned}\\)</span>Llama</p>\n<p>YaRNNTK-by-partsattention\nscore</p>\n<p>YaRN</p>\n<h2 id=\"logn\">logn</h2>\n<p>lognattention <span class=\"math inline\">\\(\\sqrt{d}\\)</span>\nlogn<a href=\"https://zhuanlan.zhihu.com/p/678755776\"></a>YaRN</p>\n<p>tokentokentokenattention\nscore</p>\n<p><span class=\"math display\">\\[\n\\begin{equation}\n\\begin{aligned}\n\\text{Attention}_E(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\log_{L}{L&#39;}}{\\sqrt{d}}\\boldsymbol{Q}\\boldsymbol{K}^\\mathrm{T}\\right)\\boldsymbol{V}\n\\end{aligned}\n\\end{equation}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(L&#39;&gt;L\\)</span>\nYaRN</p>\n<h2 id=\"\"></h2>\n<p>window\nattentionstreaming LLMLongLoRAFocus\nTransformer</p>\n<h1 id=\"\"></h1>\n<p>2k4k</p>\n<ul>\n<li><br>\n</li>\n<li>token</li>\n</ul>\n<p>attention score</p>\n<p>PINTKNTKlognYaRN</p>\n<h1 id=\"reference\">Reference</h1>\n<p>1transformerKV cache\nhttps://zhuanlan.zhihu.com/p/624740065<br>\n2EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION\nINTERPOLATION https://arxiv.org/pdf/2306.15595.pdf<br>\n3Transformer10RoPE\nhttps://kexue.fm/archives/9675<br>\n4YaRN: Efficient Context Window Extension of Large Language Models\nhttps://arxiv.org/pdf/2309.00071.pdf<br>\n5RoPE\nhttps://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ<br>\n6LLM https://zhuanlan.zhihu.com/p/645770522\n7prompt\nhttps://cloud.tencent.com/developer/article/2330611<br>\n8Transformer8\nhttps://spaces.ac.cn/archives/9444<br>\n9RoPE192K\nhttps://zhuanlan.zhihu.com/p/678755776 ***</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n"},{"title":"GLM4","abbrlink":"a5206abd","date":"2024-06-27T06:51:38.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nGLMGLM-4  \n\nGLM-4-9Bagent  \n\n{% asset_img glm.png GLM %}  \n\n  \n\n#   \n\n  \n- deduplication, filteringtokenization  \n- deduplicationexact deduplicationfuzzy deduplication  \n-   \n- tokenizationbyte-level BPEtiktokencl100k_base150k  \n- re-weightwikibooks  \n- 10Ttoken  \n\n#   \n\n  \n- No Bias Except QKVQKVbiasbias  \n- RMSNormSwiGLU  \n- 2DRoPE  \n- GQAKV cacheGQAMHAFFN10/3hidden size  \n\n# Alignment  \n\nSFTprompttemplate-based  \n\n# ChatGLM Techniques  \n\nChatGLM  \n- LongAlignLongalign: A recipe for long context alignment of large language modelsGLM-4128kClaude 2GPT-4 Turbo (1106)  \n- ChatGLM-MathChatglm-math: Improving math problem-solving in large language models with a self-critique pipelineself-critique  \n- ChatGLM-RLHFChatglm-rlhf: Practices of aligning large language models with human feedbackPPODPO  \n- Self-ContrastExtensive self-contrast enables feedback-free language model alignmentSelf-ContrastRLHF  \n- AgentTuningAgenttuning: Enabling generalized agent abilities for llmsagentAgentInstruct instruction-tuning  \n- APARApar: Llms can do auto-parallel auto-regressive decodingauto-parallel auto-regressive  \n\n# GLM-4 All Tools  \n\nGLM-4 All Tools  \n\n{% asset_img all_tools.png All Tools %}  \n\n#   \n\nGLMagentfunction callall tools  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1ChatGLM: A Family of Large Language Models\nfrom GLM-130B to GLM-4 All Tools https://arxiv.org/abs/2406.12793  \n","source":"_posts/cs/nlp/2024/06/GLM4.md","raw":"---\ntitle: GLM4\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \n  - \n  - \n  - agent\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: a5206abd\ndate: 2024-06-27 14:51:38\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nGLMGLM-4  \n\nGLM-4-9Bagent  \n\n{% asset_img glm.png GLM %}  \n\n  \n\n#   \n\n  \n- deduplication, filteringtokenization  \n- deduplicationexact deduplicationfuzzy deduplication  \n-   \n- tokenizationbyte-level BPEtiktokencl100k_base150k  \n- re-weightwikibooks  \n- 10Ttoken  \n\n#   \n\n  \n- No Bias Except QKVQKVbiasbias  \n- RMSNormSwiGLU  \n- 2DRoPE  \n- GQAKV cacheGQAMHAFFN10/3hidden size  \n\n# Alignment  \n\nSFTprompttemplate-based  \n\n# ChatGLM Techniques  \n\nChatGLM  \n- LongAlignLongalign: A recipe for long context alignment of large language modelsGLM-4128kClaude 2GPT-4 Turbo (1106)  \n- ChatGLM-MathChatglm-math: Improving math problem-solving in large language models with a self-critique pipelineself-critique  \n- ChatGLM-RLHFChatglm-rlhf: Practices of aligning large language models with human feedbackPPODPO  \n- Self-ContrastExtensive self-contrast enables feedback-free language model alignmentSelf-ContrastRLHF  \n- AgentTuningAgenttuning: Enabling generalized agent abilities for llmsagentAgentInstruct instruction-tuning  \n- APARApar: Llms can do auto-parallel auto-regressive decodingauto-parallel auto-regressive  \n\n# GLM-4 All Tools  \n\nGLM-4 All Tools  \n\n{% asset_img all_tools.png All Tools %}  \n\n#   \n\nGLMagentfunction callall tools  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1ChatGLM: A Family of Large Language Models\nfrom GLM-130B to GLM-4 All Tools https://arxiv.org/abs/2406.12793  \n","slug":"cs/nlp/2024/06/GLM4","published":1,"updated":"2024-06-27T13:47:55.409Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmm00070p4k05cz5zt9","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>GLMGLM-4</p>\n<p>GLM-4-9Bagent</p>\n<img src=\"/a5206abd/glm.png\" class title=\"GLM\">\n<p></p>\n<h1 id=\"\"></h1>\n<p><br>\n- deduplication, filteringtokenization<br>\n- deduplicationexact deduplicationfuzzy deduplication<br>\n- <br>\n- tokenizationbyte-level\nBPEtiktokencl100k_base150k<br>\n-\nre-weightwikibooks<br>\n- 10Ttoken</p>\n<h1 id=\"\"></h1>\n<p><br>\n- No Bias Except\nQKVQKVbiasbias<br>\n- RMSNormSwiGLU<br>\n- 2DRoPE<br>\n- GQAKV\ncacheGQAMHAFFN10/3hidden\nsize</p>\n<h1 id=\"alignment\">Alignment</h1>\n<p>SFTprompttemplate-based</p>\n<h1 id=\"chatglm-techniques\">ChatGLM Techniques</h1>\n<p>ChatGLM<br>\n- LongAlignLongalign: A recipe for long context alignment of large\nlanguage modelsGLM-4128kClaude\n2GPT-4 Turbo (1106)<br>\n- ChatGLM-MathChatglm-math: Improving math problem-solving in large\nlanguage models with a self-critique\npipelineself-critique<br>\n- ChatGLM-RLHFChatglm-rlhf: Practices of aligning large language\nmodels with human feedbackPPODPO<br>\n- Self-ContrastExtensive self-contrast enables feedback-free\nlanguage model\nalignmentSelf-ContrastRLHF<br>\n- AgentTuningAgenttuning: Enabling generalized agent abilities for\nllmsagentAgentInstruct\ninstruction-tuning<br>\n- APARApar: Llms can do auto-parallel auto-regressive\ndecodingauto-parallel auto-regressive</p>\n<h1 id=\"glm-4-all-tools\">GLM-4 All Tools</h1>\n<p>GLM-4 All Tools</p>\n<img src=\"/a5206abd/all_tools.png\" class title=\"All Tools\">\n<h1 id=\"\"></h1>\n<p>GLMagentfunction callall\ntools</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1ChatGLM: A Family of Large Language Models from GLM-130B to\nGLM-4 All Tools https://arxiv.org/abs/2406.12793</p>\n","length":2130,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>GLMGLM-4</p>\n<p>GLM-4-9Bagent</p>\n<img src=\"/a5206abd/glm.png\" class title=\"GLM\">\n<p></p>\n<h1 id=\"\"></h1>\n<p><br>\n- deduplication, filteringtokenization<br>\n- deduplicationexact deduplicationfuzzy deduplication<br>\n- <br>\n- tokenizationbyte-level\nBPEtiktokencl100k_base150k<br>\n-\nre-weightwikibooks<br>\n- 10Ttoken</p>\n<h1 id=\"\"></h1>\n<p><br>\n- No Bias Except\nQKVQKVbiasbias<br>\n- RMSNormSwiGLU<br>\n- 2DRoPE<br>\n- GQAKV\ncacheGQAMHAFFN10/3hidden\nsize</p>\n<h1 id=\"alignment\">Alignment</h1>\n<p>SFTprompttemplate-based</p>\n<h1 id=\"chatglm-techniques\">ChatGLM Techniques</h1>\n<p>ChatGLM<br>\n- LongAlignLongalign: A recipe for long context alignment of large\nlanguage modelsGLM-4128kClaude\n2GPT-4 Turbo (1106)<br>\n- ChatGLM-MathChatglm-math: Improving math problem-solving in large\nlanguage models with a self-critique\npipelineself-critique<br>\n- ChatGLM-RLHFChatglm-rlhf: Practices of aligning large language\nmodels with human feedbackPPODPO<br>\n- Self-ContrastExtensive self-contrast enables feedback-free\nlanguage model\nalignmentSelf-ContrastRLHF<br>\n- AgentTuningAgenttuning: Enabling generalized agent abilities for\nllmsagentAgentInstruct\ninstruction-tuning<br>\n- APARApar: Llms can do auto-parallel auto-regressive\ndecodingauto-parallel auto-regressive</p>\n<h1 id=\"glm-4-all-tools\">GLM-4 All Tools</h1>\n<p>GLM-4 All Tools</p>\n<img src=\"/a5206abd/all_tools.png\" class title=\"All Tools\">\n<h1 id=\"\"></h1>\n<p>GLMagentfunction callall\ntools</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1ChatGLM: A Family of Large Language Models from GLM-130B to\nGLM-4 All Tools https://arxiv.org/abs/2406.12793</p>\n"},{"title":"MiniCPM","abbrlink":"376db710","date":"2024-06-18T13:51:22.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nMiniCPMMiniCPMembedding1.2B2.4BMiniCPM-DPOMiniCPM-MoEMiniCPM-128K  \n\nMiniCPM  \n\n#   \n\nSLMPhiTinyLlamaMobileLLMGemma  \n\nMiniCPMSLM  \n\n#   \n\nMiniCPMModel Wind Tunnel Experiments  \n\n12batch sizescaling3learning rate\n\n  \n\n{% asset_img exp_model.png  %}  \n\n##   \n\nSLM  \n\nTensor ProgramTensor programs v: Tuning large neural networks via zero-shot hyperparameter transferTensor programs vi: Feature learning in infinite-depth neural networks  \n\nattention softmaxscaling  \n\n{% asset_img param_search_2.png  %}  \n\n  \n- Maximal Update Parametrization  \n- N=0.009B10N20ND=10N=0.09Btoken  \n- QK-NormQuerykey normalization for transformersindependent weight decayDecoupled weight decay regularizationlearning ratelearning rateQK-Normindependent weight decay  \n\nhyper-parameters  \n- scale depth = 1.4  \n- scale emb = 12  \n- init std = 0.1  \n- lr = 0.01  \n\n{% asset_img param_search.png  %}  \n\n## Optimal Batch Size  \n\nbatch size  \n\nbatch sizeupdatebatch sizeupdate stepbatch sizeloss  \n\nOpenAIScaling laws for neural language modelsbatch size  \n\nScaling laws for neural language modelsloss functiontokenstep=OpenAIcritical batch sizesteptokenloss  \n\nGPUGPUbatch sizestepstepGPUbatch sizestepbatch sizestep  \n\nMiniCPMnot consuming too many stepsminimizing the token quantity to achieve the lowest loss  \n\noptimal batch sizelossachievable loss  \n\noptimal batch sizeoptimal learning rateMiniCPMlearning ratelearning ratebatch sizebatch sizelearning rateCoordinate Descent optimization method  \n\nMiniCPM0.009B0.03B0.17B6batch sizeglobal learning rate=0.01cosine learning rate schedulerC4optimal batch sizeloss  \n\n{% asset_img batch_size.png  %}  \n\nlog  \n\n{% asset_img batch_size_2.png  %}  \n\nC4lossoptimal batch size  \n\n$$bs=\\frac{1.21\\times10^9}{L^{6.24}}$$  \n\n## Optimal Learning Rate  \n\nTensor Programoptimal learning rateMiniCPM0.04B0.1B0.3B0.5Blearning rate  \n\noptimal learning rate0.01  \n\n{% asset_img learning_rate.png  %}  \n\nMiniCPM2.1B0.01learning rateloss  \n\n# WSD  \n\n## cosine learning rate scheduler  \n\ncosine schedulerlearning rateTstepS0.036Bcosinecosine loopscheduler\n\n{% asset_img cos_lr.png LR %}  \n\nloss  \n\n{% asset_img cos_loss.png LR %}  \n\nT=S  \n- T<SschedulerT=Sschedulerlearning ratelearning rateglobal optimum  \n- T>SschedulerT=Sschedulerlearning rate decaytraining dynamics local optimum  \n\n## Warmup-Stable-Decay  \n\nMiniCPMhigh learning rate stagelearning decay stageschedulerWarmup-Stable-Decay scheduler  \n\n$$\\left.WSD(T;s)=\\begin{cases}&\\frac{s}{W}\\eta,\\quad s<W\\\\&\\eta,\\quad W<s<T\\\\&f(s-T)\\eta,\\quad T<s<S\\end{cases}\\right.$$  \n\nWwarmupstepTstable training step$\\eta$ maximum learning rate$f\\left(s-T\\right)$ s decreasing function01  \n\nWW  \n\nWSD  \n\n1Loss Decreases Dramatically in Decay Stage  \n\n0.036BWSDTSdecaydecaylearning ratelossT=SCosine LRSlossloss  \n\n{% asset_img wsd_exp1.png WSD %}  \n\nstable traininglearningdecaycheckpointlearning ratestable training steplearning rateCosine LRSsteplossstabledecay  \n\n210% Steps are Enough  \n\n40N60N80Ntoken10%learning rate decay10%10%steplearning rate decay  \n\n3Effective Data Scaling with WSD LRS  \n\nWSDWSDMiniCPM0.036B40N0.17Bloss  \n\n{% asset_img wsd_exp2.png WSD %}  \n\n0.036BChinchilla Optimalmatch 0.17Bloss  \n\n## Measuring the Scaling Law with WSD LRS  \n\nWSDmodel sizedata sizescalingstable stagelearningdecaystep  \n\n0.04B2B6SLMscaling law10N60N6decay  \n\n365tokenizerGPT-4 technical reportbytetokenscipy curvefit functionmodel size N and data size D  \n\n$$L(N,D)=C_NN^{-\\alpha}+C_DD^{-\\beta}+L_0$$  \n\n  \n\n{% asset_img scaling_law.png scaling law %}  \n\nScaling language models: Methods, analysis & insights from training gopherTraining compute-optimal large language modelsScaling laws for neural language modelstoken192Training compute-optimal large language models20  \n\nMiniCPMLLAMA2LLAMA2token70~10020\n\nWSD  \n\n# Two Stage Pre-training Strategy  \n\nWSDlossMiniCPMlearning rateSFTSFT  \n- SFTSFTloss  \n- SFTlearning rate decay  \n\n  \n- A-1: 2.4Bdecay4BSFT  \n- A-2: 2.4Bdecay+SFT4BSFT  \n- B-1: 1.2Bdecay6BSFT  \n- B-2: 1.2Bdecay12BSFT  \n- B-3: 1.2Bdecay+SFT6BSFT  \n\n+SFT  \n\n{% asset_img 2_stage.png 2 %}  \n\nlearning rateSFT  \n\n# MiniCPM  \n\n##   \n\nMiniCPM2.4B1.2B2.4B122,7531.2B73,440BPEMiniCPMtokenizer  \n\n{% asset_img tokenizer.png tokenizer %}  \n\nMiniCPM\n\nhidden stateMiniCPMPhi-2SLMMobilellm: Optimizing sub-billion parameter language models for on-device use cases  \n\n{% asset_img layers.png  %}  \n\n1.2BGQA  \n\n##   \n\nWSDstable1Tbatch size=3.93Mmax lr=0.01  \n\ndecaydecay $f(s-T)=0.5^{(s-S)/T}$T=5000 steps (20B tokens)  \n\nSFT6Blearning ratelearning rateWSD  \n\n  \n\n{% asset_img data.png  %}  \n\n1.2B2.4Bloss  \n\n{% asset_img train_loss.png training loss %}  \n\nlossbatch sizelearning rate  \n\nSFT  \n\n{% asset_img eval.png evaluation %}  \n\n## MiniCPM-DPO  \n\nSFTMiniCPMUltraFeedbackDPO  \n\nDPOCosine LRS, max learning rate=1e-5epoch  \n\nDPOMT-bench6.897.25benchmark  \n\n## MiniCPM-128k  \n\nMiniCPM4k128kstablecheckpoint  \n\nMiniCPM44% 56%  \n\n128kcurriculum learning32k128k4k-32kABF32K128KNTK-Aware RoPE scaling  \n\nYiZebra: Extending context window with layerwise grouped local-global attentionQAMiniCPMQA  \n\nMiniCPM-128kBenchbench: Extending long context evaluation beyond 100k tokens  \n\n{% asset_img 128k_result.png 128k evaluation %}  \n\n## MiniCPM-MoE  \n\nMiniCPM-MoESparse UpcyclingSparse upcycling: Training mixture-of-experts from dense checkpointsstablecheckpointrouter00.01  \n\nMiniCPM-MoE13.6B24B\n\nswitch transformer0.01\n\nlearning rateWSD4Mbatch size130kSFTbatch size2M  \n\nMiniCPM-MoE  \n\n{% asset_img moe_result.png moe evaluation %}  \n\n#   \n\nMiniCPM1B/2Bscaling law  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies https://arxiv.org/abs/2404.06395  \n","source":"_posts/cs/nlp/2024/06/MiniCPM.md","raw":"---\ntitle: MiniCPM\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 376db710\ndate: 2024-06-18 21:51:22\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nMiniCPMMiniCPMembedding1.2B2.4BMiniCPM-DPOMiniCPM-MoEMiniCPM-128K  \n\nMiniCPM  \n\n#   \n\nSLMPhiTinyLlamaMobileLLMGemma  \n\nMiniCPMSLM  \n\n#   \n\nMiniCPMModel Wind Tunnel Experiments  \n\n12batch sizescaling3learning rate\n\n  \n\n{% asset_img exp_model.png  %}  \n\n##   \n\nSLM  \n\nTensor ProgramTensor programs v: Tuning large neural networks via zero-shot hyperparameter transferTensor programs vi: Feature learning in infinite-depth neural networks  \n\nattention softmaxscaling  \n\n{% asset_img param_search_2.png  %}  \n\n  \n- Maximal Update Parametrization  \n- N=0.009B10N20ND=10N=0.09Btoken  \n- QK-NormQuerykey normalization for transformersindependent weight decayDecoupled weight decay regularizationlearning ratelearning rateQK-Normindependent weight decay  \n\nhyper-parameters  \n- scale depth = 1.4  \n- scale emb = 12  \n- init std = 0.1  \n- lr = 0.01  \n\n{% asset_img param_search.png  %}  \n\n## Optimal Batch Size  \n\nbatch size  \n\nbatch sizeupdatebatch sizeupdate stepbatch sizeloss  \n\nOpenAIScaling laws for neural language modelsbatch size  \n\nScaling laws for neural language modelsloss functiontokenstep=OpenAIcritical batch sizesteptokenloss  \n\nGPUGPUbatch sizestepstepGPUbatch sizestepbatch sizestep  \n\nMiniCPMnot consuming too many stepsminimizing the token quantity to achieve the lowest loss  \n\noptimal batch sizelossachievable loss  \n\noptimal batch sizeoptimal learning rateMiniCPMlearning ratelearning ratebatch sizebatch sizelearning rateCoordinate Descent optimization method  \n\nMiniCPM0.009B0.03B0.17B6batch sizeglobal learning rate=0.01cosine learning rate schedulerC4optimal batch sizeloss  \n\n{% asset_img batch_size.png  %}  \n\nlog  \n\n{% asset_img batch_size_2.png  %}  \n\nC4lossoptimal batch size  \n\n$$bs=\\frac{1.21\\times10^9}{L^{6.24}}$$  \n\n## Optimal Learning Rate  \n\nTensor Programoptimal learning rateMiniCPM0.04B0.1B0.3B0.5Blearning rate  \n\noptimal learning rate0.01  \n\n{% asset_img learning_rate.png  %}  \n\nMiniCPM2.1B0.01learning rateloss  \n\n# WSD  \n\n## cosine learning rate scheduler  \n\ncosine schedulerlearning rateTstepS0.036Bcosinecosine loopscheduler\n\n{% asset_img cos_lr.png LR %}  \n\nloss  \n\n{% asset_img cos_loss.png LR %}  \n\nT=S  \n- T<SschedulerT=Sschedulerlearning ratelearning rateglobal optimum  \n- T>SschedulerT=Sschedulerlearning rate decaytraining dynamics local optimum  \n\n## Warmup-Stable-Decay  \n\nMiniCPMhigh learning rate stagelearning decay stageschedulerWarmup-Stable-Decay scheduler  \n\n$$\\left.WSD(T;s)=\\begin{cases}&\\frac{s}{W}\\eta,\\quad s<W\\\\&\\eta,\\quad W<s<T\\\\&f(s-T)\\eta,\\quad T<s<S\\end{cases}\\right.$$  \n\nWwarmupstepTstable training step$\\eta$ maximum learning rate$f\\left(s-T\\right)$ s decreasing function01  \n\nWW  \n\nWSD  \n\n1Loss Decreases Dramatically in Decay Stage  \n\n0.036BWSDTSdecaydecaylearning ratelossT=SCosine LRSlossloss  \n\n{% asset_img wsd_exp1.png WSD %}  \n\nstable traininglearningdecaycheckpointlearning ratestable training steplearning rateCosine LRSsteplossstabledecay  \n\n210% Steps are Enough  \n\n40N60N80Ntoken10%learning rate decay10%10%steplearning rate decay  \n\n3Effective Data Scaling with WSD LRS  \n\nWSDWSDMiniCPM0.036B40N0.17Bloss  \n\n{% asset_img wsd_exp2.png WSD %}  \n\n0.036BChinchilla Optimalmatch 0.17Bloss  \n\n## Measuring the Scaling Law with WSD LRS  \n\nWSDmodel sizedata sizescalingstable stagelearningdecaystep  \n\n0.04B2B6SLMscaling law10N60N6decay  \n\n365tokenizerGPT-4 technical reportbytetokenscipy curvefit functionmodel size N and data size D  \n\n$$L(N,D)=C_NN^{-\\alpha}+C_DD^{-\\beta}+L_0$$  \n\n  \n\n{% asset_img scaling_law.png scaling law %}  \n\nScaling language models: Methods, analysis & insights from training gopherTraining compute-optimal large language modelsScaling laws for neural language modelstoken192Training compute-optimal large language models20  \n\nMiniCPMLLAMA2LLAMA2token70~10020\n\nWSD  \n\n# Two Stage Pre-training Strategy  \n\nWSDlossMiniCPMlearning rateSFTSFT  \n- SFTSFTloss  \n- SFTlearning rate decay  \n\n  \n- A-1: 2.4Bdecay4BSFT  \n- A-2: 2.4Bdecay+SFT4BSFT  \n- B-1: 1.2Bdecay6BSFT  \n- B-2: 1.2Bdecay12BSFT  \n- B-3: 1.2Bdecay+SFT6BSFT  \n\n+SFT  \n\n{% asset_img 2_stage.png 2 %}  \n\nlearning rateSFT  \n\n# MiniCPM  \n\n##   \n\nMiniCPM2.4B1.2B2.4B122,7531.2B73,440BPEMiniCPMtokenizer  \n\n{% asset_img tokenizer.png tokenizer %}  \n\nMiniCPM\n\nhidden stateMiniCPMPhi-2SLMMobilellm: Optimizing sub-billion parameter language models for on-device use cases  \n\n{% asset_img layers.png  %}  \n\n1.2BGQA  \n\n##   \n\nWSDstable1Tbatch size=3.93Mmax lr=0.01  \n\ndecaydecay $f(s-T)=0.5^{(s-S)/T}$T=5000 steps (20B tokens)  \n\nSFT6Blearning ratelearning rateWSD  \n\n  \n\n{% asset_img data.png  %}  \n\n1.2B2.4Bloss  \n\n{% asset_img train_loss.png training loss %}  \n\nlossbatch sizelearning rate  \n\nSFT  \n\n{% asset_img eval.png evaluation %}  \n\n## MiniCPM-DPO  \n\nSFTMiniCPMUltraFeedbackDPO  \n\nDPOCosine LRS, max learning rate=1e-5epoch  \n\nDPOMT-bench6.897.25benchmark  \n\n## MiniCPM-128k  \n\nMiniCPM4k128kstablecheckpoint  \n\nMiniCPM44% 56%  \n\n128kcurriculum learning32k128k4k-32kABF32K128KNTK-Aware RoPE scaling  \n\nYiZebra: Extending context window with layerwise grouped local-global attentionQAMiniCPMQA  \n\nMiniCPM-128kBenchbench: Extending long context evaluation beyond 100k tokens  \n\n{% asset_img 128k_result.png 128k evaluation %}  \n\n## MiniCPM-MoE  \n\nMiniCPM-MoESparse UpcyclingSparse upcycling: Training mixture-of-experts from dense checkpointsstablecheckpointrouter00.01  \n\nMiniCPM-MoE13.6B24B\n\nswitch transformer0.01\n\nlearning rateWSD4Mbatch size130kSFTbatch size2M  \n\nMiniCPM-MoE  \n\n{% asset_img moe_result.png moe evaluation %}  \n\n#   \n\nMiniCPM1B/2Bscaling law  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies https://arxiv.org/abs/2404.06395  \n","slug":"cs/nlp/2024/06/MiniCPM","published":1,"updated":"2024-06-24T04:09:56.529Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmm00080p4k7vhx37f3","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>MiniCPMMiniCPMembedding1.2B2.4BMiniCPM-DPOMiniCPM-MoEMiniCPM-128K</p>\n<p>MiniCPM</p>\n<h1 id=\"\"></h1>\n<p>SLMPhiTinyLlamaMobileLLMGemma</p>\n<p>MiniCPMSLM</p>\n<h1 id=\"\"></h1>\n<p>MiniCPMModel Wind\nTunnel Experiments</p>\n<p>12batch\nsizescaling3learning rate</p>\n<p></p>\n<img src=\"/376db710/exp_model.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p>SLM</p>\n<p>Tensor ProgramTensor programs v: Tuning large\nneural networks via zero-shot hyperparameter transferTensor\nprograms vi: Feature learning in infinite-depth neural\nnetworks</p>\n<p>attention\nsoftmaxscaling</p>\n<img src=\"/376db710/param_search_2.png\" class title=\"\">\n<p><br>\n- Maximal Update Parametrization<br>\n-\nN=0.009B10N20ND=10N=0.09Btoken<br>\n- QK-NormQuerykey normalization for\ntransformersindependent weight decayDecoupled weight decay\nregularizationlearning\nratelearning\nrateQK-Normindependent\nweight decay</p>\n<p>hyper-parameters<br>\n- scale depth = 1.4<br>\n- scale emb = 12<br>\n- init std = 0.1<br>\n- lr = 0.01</p>\n<img src=\"/376db710/param_search.png\" class title=\"\">\n<h2 id=\"optimal-batch-size\">Optimal Batch Size</h2>\n<p>batch size</p>\n<p>batch\nsizeupdatebatch\nsizeupdate stepbatch\nsizeloss</p>\n<p>OpenAIScaling laws for neural language\nmodelsbatch size</p>\n<p>Scaling laws for neural language modelsloss\nfunctiontokenstep=OpenAIcritical\nbatch\nsizesteptokenloss</p>\n<p>GPUGPUbatch\nsizestepstepGPUbatch\nsizestepbatch\nsizestep</p>\n<p>MiniCPMnot consuming too many\nstepsminimizing the token quantity to achieve the\nlowest loss</p>\n<p>optimal batch\nsizelossachievable\nloss</p>\n<p>optimal batch sizeoptimal learning\nrateMiniCPMlearning\nratelearning ratebatch\nsizebatch sizelearning\nrateCoordinate Descent optimization method</p>\n<p>MiniCPM0.009B0.03B0.17B6batch\nsizeglobal learning rate=0.01cosine learning rate\nschedulerC4optimal batch sizeloss</p>\n<img src=\"/376db710/batch_size.png\" class title=\"\">\n<p>log</p>\n<img src=\"/376db710/batch_size_2.png\" class title=\"\">\n<p>C4lossoptimal batch size</p>\n<p><span class=\"math display\">\\[bs=\\frac{1.21\\times10^9}{L^{6.24}}\\]</span></p>\n<h2 id=\"optimal-learning-rate\">Optimal Learning Rate</h2>\n<p>Tensor Programoptimal learning\nrateMiniCPM0.04B0.1B0.3B0.5Blearning\nrate</p>\n<p>optimal learning\nrate0.01</p>\n<img src=\"/376db710/learning_rate.png\" class title=\"\">\n<p>MiniCPM2.1B0.01learning\nrateloss</p>\n<h1 id=\"wsd\">WSD</h1>\n<h2 id=\"cosine-learning-rate-scheduler\">cosine learning rate\nscheduler</h2>\n<p>cosine schedulerlearning\nrateTstepS0.036Bcosinecosine\nloopscheduler</p>\n<img src=\"/376db710/cos_lr.png\" class title=\"LR\">\n<p>loss</p>\n<img src=\"/376db710/cos_loss.png\" class title=\"LR\">\n<p>T=S<br>\n- T&lt;SschedulerT=Sschedulerlearning\nratelearning rateglobal\noptimum<br>\n- T&gt;SschedulerT=Sschedulerlearning rate\ndecaytraining dynamics local\noptimum</p>\n<h2 id=\"warmup-stable-decay\">Warmup-Stable-Decay</h2>\n<p>MiniCPMhigh learning rate\nstagelearning decay stageschedulerWarmup-Stable-Decay\nscheduler</p>\n<p><span class=\"math display\">\\[\\left.WSD(T;s)=\\begin{cases}&amp;\\frac{s}{W}\\eta,\\quad\ns&lt;W\\\\&amp;\\eta,\\quad W&lt;s&lt;T\\\\&amp;f(s-T)\\eta,\\quad\nT&lt;s&lt;S\\end{cases}\\right.\\]</span></p>\n<p>WwarmupstepTstable training step<span class=\"math inline\">\\(\\eta\\)</span> maximum learning rate<span class=\"math inline\">\\(f\\left(s-T\\right)\\)</span> s decreasing\nfunction01</p>\n<p>WW</p>\n<p>WSD</p>\n<p>1Loss Decreases Dramatically in Decay Stage</p>\n<p>0.036BWSDTSdecaydecaylearning\nratelossT=SCosine\nLRSlossloss</p>\n<img src=\"/376db710/wsd_exp1.png\" class title=\"WSD\">\n<p>stable\ntraininglearningdecaycheckpointlearning\nratestable training\nsteplearning rateCosine\nLRSsteplossstabledecay</p>\n<p>210% Steps are Enough</p>\n<p>40N60N80Ntoken10%learning\nrate\ndecay10%10%steplearning\nrate decay</p>\n<p>3Effective Data Scaling with WSD LRS</p>\n<p>WSDWSDMiniCPM0.036B40N0.17Bloss</p>\n<img src=\"/376db710/wsd_exp2.png\" class title=\"WSD\">\n<p>0.036BChinchilla\nOptimalmatch\n0.17Bloss</p>\n<h2 id=\"measuring-the-scaling-law-with-wsd-lrs\">Measuring the Scaling\nLaw with WSD LRS</h2>\n<p>WSDmodel sizedata\nsizescalingstable\nstagelearningdecaystep</p>\n<p>0.04B2B6SLMscaling\nlaw10N60N6decay</p>\n<p>365tokenizerGPT-4\ntechnical\nreportbytetokenscipy\ncurvefit functionmodel size N and data size\nD</p>\n<p><span class=\"math display\">\\[L(N,D)=C_NN^{-\\alpha}+C_DD^{-\\beta}+L_0\\]</span></p>\n<p></p>\n<img src=\"/376db710/scaling_law.png\" class title=\"scaling law\">\n<p>Scaling language models: Methods, analysis &amp; insights\nfrom training gopherTraining compute-optimal large language\nmodelsScaling laws for neural language\nmodelstoken192Training\ncompute-optimal large language models20</p>\n<p>MiniCPMLLAMA2LLAMA2token70~10020</p>\n<p>WSD</p>\n<h1 id=\"two-stage-pre-training-strategy\">Two Stage Pre-training\nStrategy</h1>\n<p>WSDlossMiniCPMlearning\nrateSFTSFT<br>\n- SFTSFTloss<br>\n- SFTlearning rate\ndecay</p>\n<p><br>\n- A-1: 2.4Bdecay4BSFT<br>\n- A-2:\n2.4Bdecay+SFT4BSFT<br>\n- B-1: 1.2Bdecay6BSFT<br>\n- B-2: 1.2Bdecay12BSFT<br>\n- B-3:\n1.2Bdecay+SFT6BSFT</p>\n<p>+SFT</p>\n<img src=\"/376db710/2_stage.png\" class title=\"2\">\n<p>learning rateSFT</p>\n<h1 id=\"minicpm\">MiniCPM</h1>\n<h2 id=\"\"></h2>\n<p>MiniCPM2.4B1.2B2.4B122,7531.2B73,440BPEMiniCPMtokenizer</p>\n<img src=\"/376db710/tokenizer.png\" class title=\"tokenizer\">\n<p>MiniCPM</p>\n<p>hidden\nstateMiniCPMPhi-2SLMMobilellm:\nOptimizing sub-billion parameter language models for on-device use\ncases</p>\n<img src=\"/376db710/layers.png\" class title=\"\">\n<p>1.2BGQA</p>\n<h2 id=\"\"></h2>\n<p>WSDstable1Tbatch size=3.93Mmax\nlr=0.01</p>\n<p>decaydecay <span class=\"math inline\">\\(f(s-T)=0.5^{(s-S)/T}\\)</span>T=5000 steps\n(20B tokens)</p>\n<p>SFT6Blearning ratelearning\nrateWSD</p>\n<p></p>\n<img src=\"/376db710/data.png\" class title=\"\">\n<p>1.2B2.4Bloss</p>\n<img src=\"/376db710/train_loss.png\" class title=\"training loss\">\n<p>lossbatch sizelearning\nrate</p>\n<p>SFT</p>\n<img src=\"/376db710/eval.png\" class title=\"evaluation\">\n<h2 id=\"minicpm-dpo\">MiniCPM-DPO</h2>\n<p>SFTMiniCPMUltraFeedbackDPO</p>\n<p>DPOCosine LRS, max learning\nrate=1e-5epoch</p>\n<p>DPOMT-bench6.897.25benchmark</p>\n<h2 id=\"minicpm-128k\">MiniCPM-128k</h2>\n<p>MiniCPM4k128kstablecheckpoint</p>\n<p>MiniCPM44%\n56%</p>\n<p>128kcurriculum\nlearning32k128k4k-32kABF32K128KNTK-Aware\nRoPE scaling</p>\n<p>YiZebra: Extending context window with layerwise\ngrouped local-global\nattentionQAMiniCPMQA</p>\n<p>MiniCPM-128kBenchbench: Extending long context evaluation\nbeyond 100k tokens</p>\n<img src=\"/376db710/128k_result.png\" class title=\"128k evaluation\">\n<h2 id=\"minicpm-moe\">MiniCPM-MoE</h2>\n<p>MiniCPM-MoESparse UpcyclingSparse upcycling: Training\nmixture-of-experts from dense\ncheckpointsstablecheckpointrouter00.01</p>\n<p>MiniCPM-MoE13.6B24B</p>\n<p>switch transformer0.01</p>\n<p>learning rateWSD4Mbatch\nsize130kSFTbatch size2M</p>\n<p>MiniCPM-MoE</p>\n<img src=\"/376db710/moe_result.png\" class title=\"moe evaluation\">\n<h1 id=\"\"></h1>\n<p>MiniCPM1B/2Bscaling\nlaw</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1MiniCPM: Unveiling the Potential of Small Language Models with\nScalable Training Strategies https://arxiv.org/abs/2404.06395</p>\n","length":7850,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>MiniCPMMiniCPMembedding1.2B2.4BMiniCPM-DPOMiniCPM-MoEMiniCPM-128K</p>\n<p>MiniCPM</p>\n<h1 id=\"\"></h1>\n<p>SLMPhiTinyLlamaMobileLLMGemma</p>\n<p>MiniCPMSLM</p>\n<h1 id=\"\"></h1>\n<p>MiniCPMModel Wind\nTunnel Experiments</p>\n<p>12batch\nsizescaling3learning rate</p>\n<p></p>\n<img src=\"/376db710/exp_model.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p>SLM</p>\n<p>Tensor ProgramTensor programs v: Tuning large\nneural networks via zero-shot hyperparameter transferTensor\nprograms vi: Feature learning in infinite-depth neural\nnetworks</p>\n<p>attention\nsoftmaxscaling</p>\n<img src=\"/376db710/param_search_2.png\" class title=\"\">\n<p><br>\n- Maximal Update Parametrization<br>\n-\nN=0.009B10N20ND=10N=0.09Btoken<br>\n- QK-NormQuerykey normalization for\ntransformersindependent weight decayDecoupled weight decay\nregularizationlearning\nratelearning\nrateQK-Normindependent\nweight decay</p>\n<p>hyper-parameters<br>\n- scale depth = 1.4<br>\n- scale emb = 12<br>\n- init std = 0.1<br>\n- lr = 0.01</p>\n<img src=\"/376db710/param_search.png\" class title=\"\">\n<h2 id=\"optimal-batch-size\">Optimal Batch Size</h2>\n<p>batch size</p>\n<p>batch\nsizeupdatebatch\nsizeupdate stepbatch\nsizeloss</p>\n<p>OpenAIScaling laws for neural language\nmodelsbatch size</p>\n<p>Scaling laws for neural language modelsloss\nfunctiontokenstep=OpenAIcritical\nbatch\nsizesteptokenloss</p>\n<p>GPUGPUbatch\nsizestepstepGPUbatch\nsizestepbatch\nsizestep</p>\n<p>MiniCPMnot consuming too many\nstepsminimizing the token quantity to achieve the\nlowest loss</p>\n<p>optimal batch\nsizelossachievable\nloss</p>\n<p>optimal batch sizeoptimal learning\nrateMiniCPMlearning\nratelearning ratebatch\nsizebatch sizelearning\nrateCoordinate Descent optimization method</p>\n<p>MiniCPM0.009B0.03B0.17B6batch\nsizeglobal learning rate=0.01cosine learning rate\nschedulerC4optimal batch sizeloss</p>\n<img src=\"/376db710/batch_size.png\" class title=\"\">\n<p>log</p>\n<img src=\"/376db710/batch_size_2.png\" class title=\"\">\n<p>C4lossoptimal batch size</p>\n<p><span class=\"math display\">\\[bs=\\frac{1.21\\times10^9}{L^{6.24}}\\]</span></p>\n<h2 id=\"optimal-learning-rate\">Optimal Learning Rate</h2>\n<p>Tensor Programoptimal learning\nrateMiniCPM0.04B0.1B0.3B0.5Blearning\nrate</p>\n<p>optimal learning\nrate0.01</p>\n<img src=\"/376db710/learning_rate.png\" class title=\"\">\n<p>MiniCPM2.1B0.01learning\nrateloss</p>\n<h1 id=\"wsd\">WSD</h1>\n<h2 id=\"cosine-learning-rate-scheduler\">cosine learning rate\nscheduler</h2>\n<p>cosine schedulerlearning\nrateTstepS0.036Bcosinecosine\nloopscheduler</p>\n<img src=\"/376db710/cos_lr.png\" class title=\"LR\">\n<p>loss</p>\n<img src=\"/376db710/cos_loss.png\" class title=\"LR\">\n<p>T=S<br>\n- T&lt;SschedulerT=Sschedulerlearning\nratelearning rateglobal\noptimum<br>\n- T&gt;SschedulerT=Sschedulerlearning rate\ndecaytraining dynamics local\noptimum</p>\n<h2 id=\"warmup-stable-decay\">Warmup-Stable-Decay</h2>\n<p>MiniCPMhigh learning rate\nstagelearning decay stageschedulerWarmup-Stable-Decay\nscheduler</p>\n<p><span class=\"math display\">\\[\\left.WSD(T;s)=\\begin{cases}&amp;\\frac{s}{W}\\eta,\\quad\ns&lt;W\\\\&amp;\\eta,\\quad W&lt;s&lt;T\\\\&amp;f(s-T)\\eta,\\quad\nT&lt;s&lt;S\\end{cases}\\right.\\]</span></p>\n<p>WwarmupstepTstable training step<span class=\"math inline\">\\(\\eta\\)</span> maximum learning rate<span class=\"math inline\">\\(f\\left(s-T\\right)\\)</span> s decreasing\nfunction01</p>\n<p>WW</p>\n<p>WSD</p>\n<p>1Loss Decreases Dramatically in Decay Stage</p>\n<p>0.036BWSDTSdecaydecaylearning\nratelossT=SCosine\nLRSlossloss</p>\n<img src=\"/376db710/wsd_exp1.png\" class title=\"WSD\">\n<p>stable\ntraininglearningdecaycheckpointlearning\nratestable training\nsteplearning rateCosine\nLRSsteplossstabledecay</p>\n<p>210% Steps are Enough</p>\n<p>40N60N80Ntoken10%learning\nrate\ndecay10%10%steplearning\nrate decay</p>\n<p>3Effective Data Scaling with WSD LRS</p>\n<p>WSDWSDMiniCPM0.036B40N0.17Bloss</p>\n<img src=\"/376db710/wsd_exp2.png\" class title=\"WSD\">\n<p>0.036BChinchilla\nOptimalmatch\n0.17Bloss</p>\n<h2 id=\"measuring-the-scaling-law-with-wsd-lrs\">Measuring the Scaling\nLaw with WSD LRS</h2>\n<p>WSDmodel sizedata\nsizescalingstable\nstagelearningdecaystep</p>\n<p>0.04B2B6SLMscaling\nlaw10N60N6decay</p>\n<p>365tokenizerGPT-4\ntechnical\nreportbytetokenscipy\ncurvefit functionmodel size N and data size\nD</p>\n<p><span class=\"math display\">\\[L(N,D)=C_NN^{-\\alpha}+C_DD^{-\\beta}+L_0\\]</span></p>\n<p></p>\n<img src=\"/376db710/scaling_law.png\" class title=\"scaling law\">\n<p>Scaling language models: Methods, analysis &amp; insights\nfrom training gopherTraining compute-optimal large language\nmodelsScaling laws for neural language\nmodelstoken192Training\ncompute-optimal large language models20</p>\n<p>MiniCPMLLAMA2LLAMA2token70~10020</p>\n<p>WSD</p>\n<h1 id=\"two-stage-pre-training-strategy\">Two Stage Pre-training\nStrategy</h1>\n<p>WSDlossMiniCPMlearning\nrateSFTSFT<br>\n- SFTSFTloss<br>\n- SFTlearning rate\ndecay</p>\n<p><br>\n- A-1: 2.4Bdecay4BSFT<br>\n- A-2:\n2.4Bdecay+SFT4BSFT<br>\n- B-1: 1.2Bdecay6BSFT<br>\n- B-2: 1.2Bdecay12BSFT<br>\n- B-3:\n1.2Bdecay+SFT6BSFT</p>\n<p>+SFT</p>\n<img src=\"/376db710/2_stage.png\" class title=\"2\">\n<p>learning rateSFT</p>\n<h1 id=\"minicpm\">MiniCPM</h1>\n<h2 id=\"\"></h2>\n<p>MiniCPM2.4B1.2B2.4B122,7531.2B73,440BPEMiniCPMtokenizer</p>\n<img src=\"/376db710/tokenizer.png\" class title=\"tokenizer\">\n<p>MiniCPM</p>\n<p>hidden\nstateMiniCPMPhi-2SLMMobilellm:\nOptimizing sub-billion parameter language models for on-device use\ncases</p>\n<img src=\"/376db710/layers.png\" class title=\"\">\n<p>1.2BGQA</p>\n<h2 id=\"\"></h2>\n<p>WSDstable1Tbatch size=3.93Mmax\nlr=0.01</p>\n<p>decaydecay <span class=\"math inline\">\\(f(s-T)=0.5^{(s-S)/T}\\)</span>T=5000 steps\n(20B tokens)</p>\n<p>SFT6Blearning ratelearning\nrateWSD</p>\n<p></p>\n<img src=\"/376db710/data.png\" class title=\"\">\n<p>1.2B2.4Bloss</p>\n<img src=\"/376db710/train_loss.png\" class title=\"training loss\">\n<p>lossbatch sizelearning\nrate</p>\n<p>SFT</p>\n<img src=\"/376db710/eval.png\" class title=\"evaluation\">\n<h2 id=\"minicpm-dpo\">MiniCPM-DPO</h2>\n<p>SFTMiniCPMUltraFeedbackDPO</p>\n<p>DPOCosine LRS, max learning\nrate=1e-5epoch</p>\n<p>DPOMT-bench6.897.25benchmark</p>\n<h2 id=\"minicpm-128k\">MiniCPM-128k</h2>\n<p>MiniCPM4k128kstablecheckpoint</p>\n<p>MiniCPM44%\n56%</p>\n<p>128kcurriculum\nlearning32k128k4k-32kABF32K128KNTK-Aware\nRoPE scaling</p>\n<p>YiZebra: Extending context window with layerwise\ngrouped local-global\nattentionQAMiniCPMQA</p>\n<p>MiniCPM-128kBenchbench: Extending long context evaluation\nbeyond 100k tokens</p>\n<img src=\"/376db710/128k_result.png\" class title=\"128k evaluation\">\n<h2 id=\"minicpm-moe\">MiniCPM-MoE</h2>\n<p>MiniCPM-MoESparse UpcyclingSparse upcycling: Training\nmixture-of-experts from dense\ncheckpointsstablecheckpointrouter00.01</p>\n<p>MiniCPM-MoE13.6B24B</p>\n<p>switch transformer0.01</p>\n<p>learning rateWSD4Mbatch\nsize130kSFTbatch size2M</p>\n<p>MiniCPM-MoE</p>\n<img src=\"/376db710/moe_result.png\" class title=\"moe evaluation\">\n<h1 id=\"\"></h1>\n<p>MiniCPM1B/2Bscaling\nlaw</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1MiniCPM: Unveiling the Potential of Small Language Models with\nScalable Training Strategies https://arxiv.org/abs/2404.06395</p>\n"},{"title":"RoPE","abbrlink":"f0902f1a","date":"2024-06-25T11:12:38.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nRoPE  \n\nRoPE [LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n\n#   \n\nRoPERoPEqk  \n\npositionmqk  \n\n$$\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\\\0&0&0&0&\\cdots&\\sin m\\theta_{d/2-1}&\\cos n\\theta_{d/2-1}\\end{pmatrix}\n$$  \n\n  \n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n$$  \n\n\n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\-\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\-\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\-\\sin m\\theta_{d/2-1}\\end{pmatrix}\n$$  \n\nhuggingfacecossin  \n\n#   \n\nqktokenattentiontoken  \n\n  \n\nbasewindow sizehead size  \n\n```python  \n\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef apply_rope(input_vec, position, base=10000):\n    # \n    d = input_vec.shape[0]\n    \n    # theta\n    i = np.arange(1, d // 2 + 1)\n    theta = base ** (-2 * (i - 1) / d)\n    theta = np.repeat(theta, 2)\n    \n    # \n    reranged_vec = np.empty_like(input_vec)\n    reranged_vec[0::2] = -input_vec[1::2]\n    reranged_vec[1::2] = input_vec[:-1:2]\n    output_vec = input_vec * np.cos(position * theta) + reranged_vec * np.sin(position * theta)\n    \n    return output_vec\n\n\ndef plot(x, y, name=''):\n    plt.plot(x, y, label=name)\n    plt.legend()\n    # \n    plt.show()\n    \nbase = 10000\nwindow_size = 4096\nd = 512\n\nq = np.ones(d)\nk = np.ones(d)\n\nrotated_q = apply_rope(input_vec=q, position=0, base=base)\n\ninner_products = []\nfor i in range(window_size):\n    rotated_k = apply_rope(input_vec=k, position=i, base=base)\n    product = np.dot(rotated_q, rotated_k)\n    inner_products.append(product)\n    \nplot(x=range(window_size), y=inner_products, name=f'base={base},window size={window_size},d={d}')\n\n```  \n\n1q = k = 1  \n\nqk1q0k0~4096q  \n\n{% asset_img 1.png  %}  \n\nbase=10000d=512  \n\n\n\n409665536  \n\n{% asset_img 2.png  %}  \n\n15000  \n\n1/4  \n\nbasebasebase=5M10M  \n\nbase5M  \n\n{% asset_img 3.png  %}  \n\n  \n\nq0k0~4096/65536q  \n\n{% asset_img 4.png  %}  \n\nq  \n\n2qk  \n\nqk1qk\n\n{% asset_img 5.png  %}  \n\n1  \n\n#   \n\n- RoPEbase  \n- qk  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1Transformer2https://spaces.ac.cn/archives/8265  \n2RoFormer: Enhanced Transformer with Rotary Position Embedding https://arxiv.org/abs/2104.09864  \n3LLM:RoPE http://www.linsight.cn/c4da56c0.html  \n","source":"_posts/cs/nlp/2024/06/RoPE.md","raw":"---\ntitle: RoPE\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - positional encoding\n  - RoPE\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: f0902f1a\ndate: 2024-06-25 19:12:38\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nRoPE  \n\nRoPE [LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n\n#   \n\nRoPERoPEqk  \n\npositionmqk  \n\n$$\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\\\0&0&0&0&\\cdots&\\sin m\\theta_{d/2-1}&\\cos n\\theta_{d/2-1}\\end{pmatrix}\n$$  \n\n  \n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n$$  \n\n\n\n$$\n\\boldsymbol{R}_{ m}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_0\\\\-\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\-\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\-\\sin m\\theta_{d/2-1}\\end{pmatrix}\n$$  \n\nhuggingfacecossin  \n\n#   \n\nqktokenattentiontoken  \n\n  \n\nbasewindow sizehead size  \n\n```python  \n\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef apply_rope(input_vec, position, base=10000):\n    # \n    d = input_vec.shape[0]\n    \n    # theta\n    i = np.arange(1, d // 2 + 1)\n    theta = base ** (-2 * (i - 1) / d)\n    theta = np.repeat(theta, 2)\n    \n    # \n    reranged_vec = np.empty_like(input_vec)\n    reranged_vec[0::2] = -input_vec[1::2]\n    reranged_vec[1::2] = input_vec[:-1:2]\n    output_vec = input_vec * np.cos(position * theta) + reranged_vec * np.sin(position * theta)\n    \n    return output_vec\n\n\ndef plot(x, y, name=''):\n    plt.plot(x, y, label=name)\n    plt.legend()\n    # \n    plt.show()\n    \nbase = 10000\nwindow_size = 4096\nd = 512\n\nq = np.ones(d)\nk = np.ones(d)\n\nrotated_q = apply_rope(input_vec=q, position=0, base=base)\n\ninner_products = []\nfor i in range(window_size):\n    rotated_k = apply_rope(input_vec=k, position=i, base=base)\n    product = np.dot(rotated_q, rotated_k)\n    inner_products.append(product)\n    \nplot(x=range(window_size), y=inner_products, name=f'base={base},window size={window_size},d={d}')\n\n```  \n\n1q = k = 1  \n\nqk1q0k0~4096q  \n\n{% asset_img 1.png  %}  \n\nbase=10000d=512  \n\n\n\n409665536  \n\n{% asset_img 2.png  %}  \n\n15000  \n\n1/4  \n\nbasebasebase=5M10M  \n\nbase5M  \n\n{% asset_img 3.png  %}  \n\n  \n\nq0k0~4096/65536q  \n\n{% asset_img 4.png  %}  \n\nq  \n\n2qk  \n\nqk1qk\n\n{% asset_img 5.png  %}  \n\n1  \n\n#   \n\n- RoPEbase  \n- qk  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1Transformer2https://spaces.ac.cn/archives/8265  \n2RoFormer: Enhanced Transformer with Rotary Position Embedding https://arxiv.org/abs/2104.09864  \n3LLM:RoPE http://www.linsight.cn/c4da56c0.html  \n","slug":"cs/nlp/2024/06/RoPE","published":1,"updated":"2024-06-26T02:59:16.930Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmn00090p4k39vfat2y","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>RoPE</p>\n<p>RoPE <a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<h1 id=\"\"></h1>\n<p>RoPERoPEqk</p>\n<p>positionmqk</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&amp;-\\sin\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\\\sin m\\theta_0&amp;\\cos\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\cos\nm\\theta_1&amp;-\\sin m\\theta_1&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\sin\nm\\theta_1&amp;\\cos\nm\\theta_1&amp;\\cdots&amp;0&amp;0\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\cos\nm\\theta_{d/2-1}&amp;-\\sin\nm\\theta_{d/2-1}\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\sin\nm\\theta_{d/2-1}&amp;\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\-\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\-\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\-\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\]</span></p>\n<p>huggingfacecossin</p>\n<h1 id=\"\"></h1>\n<p>qktokenattentiontoken</p>\n<p></p>\n<p>basewindow sizehead\nsize</p>\n<div class=\"sourceCode\" id=\"cb1\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span id=\"cb1-1\"><a href=\"#cb1-1\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-2\"><a href=\"#cb1-2\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"im\">import</span> random</span>\n<span id=\"cb1-3\"><a href=\"#cb1-3\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"im\">import</span> numpy <span class=\"im\">as</span> np</span>\n<span id=\"cb1-4\"><a href=\"#cb1-4\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"im\">import</span> matplotlib.pyplot <span class=\"im\">as</span> plt</span>\n<span id=\"cb1-5\"><a href=\"#cb1-5\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-6\"><a href=\"#cb1-6\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"kw\">def</span> apply_rope(input_vec, position, base<span class=\"op\">=</span><span class=\"dv\">10000</span>):</span>\n<span id=\"cb1-7\"><a href=\"#cb1-7\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"co\"># </span></span>\n<span id=\"cb1-8\"><a href=\"#cb1-8\" aria-hidden=\"true\" tabindex=\"-1\"></a>    d <span class=\"op\">=</span> input_vec.shape[<span class=\"dv\">0</span>]</span>\n<span id=\"cb1-9\"><a href=\"#cb1-9\" aria-hidden=\"true\" tabindex=\"-1\"></a>    </span>\n<span id=\"cb1-10\"><a href=\"#cb1-10\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"co\"># theta</span></span>\n<span id=\"cb1-11\"><a href=\"#cb1-11\" aria-hidden=\"true\" tabindex=\"-1\"></a>    i <span class=\"op\">=</span> np.arange(<span class=\"dv\">1</span>, d <span class=\"op\">//</span> <span class=\"dv\">2</span> <span class=\"op\">+</span> <span class=\"dv\">1</span>)</span>\n<span id=\"cb1-12\"><a href=\"#cb1-12\" aria-hidden=\"true\" tabindex=\"-1\"></a>    theta <span class=\"op\">=</span> base <span class=\"op\">**</span> (<span class=\"op\">-</span><span class=\"dv\">2</span> <span class=\"op\">*</span> (i <span class=\"op\">-</span> <span class=\"dv\">1</span>) <span class=\"op\">/</span> d)</span>\n<span id=\"cb1-13\"><a href=\"#cb1-13\" aria-hidden=\"true\" tabindex=\"-1\"></a>    theta <span class=\"op\">=</span> np.repeat(theta, <span class=\"dv\">2</span>)</span>\n<span id=\"cb1-14\"><a href=\"#cb1-14\" aria-hidden=\"true\" tabindex=\"-1\"></a>    </span>\n<span id=\"cb1-15\"><a href=\"#cb1-15\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"co\"># </span></span>\n<span id=\"cb1-16\"><a href=\"#cb1-16\" aria-hidden=\"true\" tabindex=\"-1\"></a>    reranged_vec <span class=\"op\">=</span> np.empty_like(input_vec)</span>\n<span id=\"cb1-17\"><a href=\"#cb1-17\" aria-hidden=\"true\" tabindex=\"-1\"></a>    reranged_vec[<span class=\"dv\">0</span>::<span class=\"dv\">2</span>] <span class=\"op\">=</span> <span class=\"op\">-</span>input_vec[<span class=\"dv\">1</span>::<span class=\"dv\">2</span>]</span>\n<span id=\"cb1-18\"><a href=\"#cb1-18\" aria-hidden=\"true\" tabindex=\"-1\"></a>    reranged_vec[<span class=\"dv\">1</span>::<span class=\"dv\">2</span>] <span class=\"op\">=</span> input_vec[:<span class=\"op\">-</span><span class=\"dv\">1</span>:<span class=\"dv\">2</span>]</span>\n<span id=\"cb1-19\"><a href=\"#cb1-19\" aria-hidden=\"true\" tabindex=\"-1\"></a>    output_vec <span class=\"op\">=</span> input_vec <span class=\"op\">*</span> np.cos(position <span class=\"op\">*</span> theta) <span class=\"op\">+</span> reranged_vec <span class=\"op\">*</span> np.sin(position <span class=\"op\">*</span> theta)</span>\n<span id=\"cb1-20\"><a href=\"#cb1-20\" aria-hidden=\"true\" tabindex=\"-1\"></a>    </span>\n<span id=\"cb1-21\"><a href=\"#cb1-21\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"cf\">return</span> output_vec</span>\n<span id=\"cb1-22\"><a href=\"#cb1-22\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-23\"><a href=\"#cb1-23\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-24\"><a href=\"#cb1-24\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"kw\">def</span> plot(x, y, name<span class=\"op\">=</span><span class=\"st\">&#39;&#39;</span>):</span>\n<span id=\"cb1-25\"><a href=\"#cb1-25\" aria-hidden=\"true\" tabindex=\"-1\"></a>    plt.plot(x, y, label<span class=\"op\">=</span>name)</span>\n<span id=\"cb1-26\"><a href=\"#cb1-26\" aria-hidden=\"true\" tabindex=\"-1\"></a>    plt.legend()</span>\n<span id=\"cb1-27\"><a href=\"#cb1-27\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"co\"># </span></span>\n<span id=\"cb1-28\"><a href=\"#cb1-28\" aria-hidden=\"true\" tabindex=\"-1\"></a>    plt.show()</span>\n<span id=\"cb1-29\"><a href=\"#cb1-29\" aria-hidden=\"true\" tabindex=\"-1\"></a>    </span>\n<span id=\"cb1-30\"><a href=\"#cb1-30\" aria-hidden=\"true\" tabindex=\"-1\"></a>base <span class=\"op\">=</span> <span class=\"dv\">10000</span></span>\n<span id=\"cb1-31\"><a href=\"#cb1-31\" aria-hidden=\"true\" tabindex=\"-1\"></a>window_size <span class=\"op\">=</span> <span class=\"dv\">4096</span></span>\n<span id=\"cb1-32\"><a href=\"#cb1-32\" aria-hidden=\"true\" tabindex=\"-1\"></a>d <span class=\"op\">=</span> <span class=\"dv\">512</span></span>\n<span id=\"cb1-33\"><a href=\"#cb1-33\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-34\"><a href=\"#cb1-34\" aria-hidden=\"true\" tabindex=\"-1\"></a>q <span class=\"op\">=</span> np.ones(d)</span>\n<span id=\"cb1-35\"><a href=\"#cb1-35\" aria-hidden=\"true\" tabindex=\"-1\"></a>k <span class=\"op\">=</span> np.ones(d)</span>\n<span id=\"cb1-36\"><a href=\"#cb1-36\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-37\"><a href=\"#cb1-37\" aria-hidden=\"true\" tabindex=\"-1\"></a>rotated_q <span class=\"op\">=</span> apply_rope(input_vec<span class=\"op\">=</span>q, position<span class=\"op\">=</span><span class=\"dv\">0</span>, base<span class=\"op\">=</span>base)</span>\n<span id=\"cb1-38\"><a href=\"#cb1-38\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-39\"><a href=\"#cb1-39\" aria-hidden=\"true\" tabindex=\"-1\"></a>inner_products <span class=\"op\">=</span> []</span>\n<span id=\"cb1-40\"><a href=\"#cb1-40\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(window_size):</span>\n<span id=\"cb1-41\"><a href=\"#cb1-41\" aria-hidden=\"true\" tabindex=\"-1\"></a>    rotated_k <span class=\"op\">=</span> apply_rope(input_vec<span class=\"op\">=</span>k, position<span class=\"op\">=</span>i, base<span class=\"op\">=</span>base)</span>\n<span id=\"cb1-42\"><a href=\"#cb1-42\" aria-hidden=\"true\" tabindex=\"-1\"></a>    product <span class=\"op\">=</span> np.dot(rotated_q, rotated_k)</span>\n<span id=\"cb1-43\"><a href=\"#cb1-43\" aria-hidden=\"true\" tabindex=\"-1\"></a>    inner_products.append(product)</span>\n<span id=\"cb1-44\"><a href=\"#cb1-44\" aria-hidden=\"true\" tabindex=\"-1\"></a>    </span>\n<span id=\"cb1-45\"><a href=\"#cb1-45\" aria-hidden=\"true\" tabindex=\"-1\"></a>plot(x<span class=\"op\">=</span><span class=\"bu\">range</span>(window_size), y<span class=\"op\">=</span>inner_products, name<span class=\"op\">=</span><span class=\"ss\">f&#39;base=</span><span class=\"sc\">&#123;</span>base<span class=\"sc\">&#125;</span><span class=\"ss\">,window size=</span><span class=\"sc\">&#123;</span>window_size<span class=\"sc\">&#125;</span><span class=\"ss\">,d=</span><span class=\"sc\">&#123;</span>d<span class=\"sc\">&#125;</span><span class=\"ss\">&#39;</span>)</span></code></pre></div>\n<p>1q = k = 1</p>\n<p>qk1q0k0~4096q</p>\n<img src=\"/f0902f1a/1.png\" class title=\"\">\n<p>base=10000d=512</p>\n<p></p>\n<p>409665536</p>\n<img src=\"/f0902f1a/2.png\" class title=\"\">\n<p>15000</p>\n<p>1/4</p>\n<p>basebasebase=5M10M</p>\n<p>base5M</p>\n<img src=\"/f0902f1a/3.png\" class title=\"\">\n<p></p>\n<p>q0k0~4096/65536q</p>\n<img src=\"/f0902f1a/4.png\" class title=\"\">\n<p>q</p>\n<p>2qk</p>\n<p>qk1qk</p>\n<img src=\"/f0902f1a/5.png\" class title=\"\">\n<p>1</p>\n<h1 id=\"\"></h1>\n<ul>\n<li>RoPEbase<br>\n</li>\n<li>qk</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Transformer2https://spaces.ac.cn/archives/8265<br>\n2RoFormer: Enhanced Transformer with Rotary Position Embedding\nhttps://arxiv.org/abs/2104.09864<br>\n3LLM:RoPE http://www.linsight.cn/c4da56c0.html</p>\n","length":4209,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>RoPE</p>\n<p>RoPE <a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<h1 id=\"\"></h1>\n<p>RoPERoPEqk</p>\n<p>positionmqk</p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\\Theta,m}^d=\\begin{pmatrix}\\cos m\\theta_0&amp;-\\sin\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\\\sin m\\theta_0&amp;\\cos\nm\\theta_0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\cos\nm\\theta_1&amp;-\\sin m\\theta_1&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;\\sin\nm\\theta_1&amp;\\cos\nm\\theta_1&amp;\\cdots&amp;0&amp;0\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\cos\nm\\theta_{d/2-1}&amp;-\\sin\nm\\theta_{d/2-1}\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;\\sin\nm\\theta_{d/2-1}&amp;\\cos n\\theta_{d/2-1}\\end{pmatrix}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}-q_1\\\\q_0\\\\-q_3\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\boldsymbol{R}_{\nm}\\boldsymbol{q}=\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos\nm\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\cos\nm\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos\nm\\theta_{d/2-1}\\end{pmatrix}\n+\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\q_4\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin\nm\\theta_0\\\\-\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\-\\sin m\\theta_1\\\\\\sin\nm\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\-\\sin\nm\\theta_{d/2-1}\\end{pmatrix}\n\\]</span></p>\n<p>huggingfacecossin</p>\n<h1 id=\"\"></h1>\n<p>qktokenattentiontoken</p>\n<p></p>\n<p>basewindow sizehead\nsize</p>\n<div class=\"sourceCode\" id=\"cb1\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span id=\"cb1-1\"><a href=\"#cb1-1\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-2\"><a href=\"#cb1-2\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"im\">import</span> random</span>\n<span id=\"cb1-3\"><a href=\"#cb1-3\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"im\">import</span> numpy <span class=\"im\">as</span> np</span>\n<span id=\"cb1-4\"><a href=\"#cb1-4\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"im\">import</span> matplotlib.pyplot <span class=\"im\">as</span> plt</span>\n<span id=\"cb1-5\"><a href=\"#cb1-5\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-6\"><a href=\"#cb1-6\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"kw\">def</span> apply_rope(input_vec, position, base<span class=\"op\">=</span><span class=\"dv\">10000</span>):</span>\n<span id=\"cb1-7\"><a href=\"#cb1-7\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"co\"># </span></span>\n<span id=\"cb1-8\"><a href=\"#cb1-8\" aria-hidden=\"true\" tabindex=\"-1\"></a>    d <span class=\"op\">=</span> input_vec.shape[<span class=\"dv\">0</span>]</span>\n<span id=\"cb1-9\"><a href=\"#cb1-9\" aria-hidden=\"true\" tabindex=\"-1\"></a>    </span>\n<span id=\"cb1-10\"><a href=\"#cb1-10\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"co\"># theta</span></span>\n<span id=\"cb1-11\"><a href=\"#cb1-11\" aria-hidden=\"true\" tabindex=\"-1\"></a>    i <span class=\"op\">=</span> np.arange(<span class=\"dv\">1</span>, d <span class=\"op\">//</span> <span class=\"dv\">2</span> <span class=\"op\">+</span> <span class=\"dv\">1</span>)</span>\n<span id=\"cb1-12\"><a href=\"#cb1-12\" aria-hidden=\"true\" tabindex=\"-1\"></a>    theta <span class=\"op\">=</span> base <span class=\"op\">**</span> (<span class=\"op\">-</span><span class=\"dv\">2</span> <span class=\"op\">*</span> (i <span class=\"op\">-</span> <span class=\"dv\">1</span>) <span class=\"op\">/</span> d)</span>\n<span id=\"cb1-13\"><a href=\"#cb1-13\" aria-hidden=\"true\" tabindex=\"-1\"></a>    theta <span class=\"op\">=</span> np.repeat(theta, <span class=\"dv\">2</span>)</span>\n<span id=\"cb1-14\"><a href=\"#cb1-14\" aria-hidden=\"true\" tabindex=\"-1\"></a>    </span>\n<span id=\"cb1-15\"><a href=\"#cb1-15\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"co\"># </span></span>\n<span id=\"cb1-16\"><a href=\"#cb1-16\" aria-hidden=\"true\" tabindex=\"-1\"></a>    reranged_vec <span class=\"op\">=</span> np.empty_like(input_vec)</span>\n<span id=\"cb1-17\"><a href=\"#cb1-17\" aria-hidden=\"true\" tabindex=\"-1\"></a>    reranged_vec[<span class=\"dv\">0</span>::<span class=\"dv\">2</span>] <span class=\"op\">=</span> <span class=\"op\">-</span>input_vec[<span class=\"dv\">1</span>::<span class=\"dv\">2</span>]</span>\n<span id=\"cb1-18\"><a href=\"#cb1-18\" aria-hidden=\"true\" tabindex=\"-1\"></a>    reranged_vec[<span class=\"dv\">1</span>::<span class=\"dv\">2</span>] <span class=\"op\">=</span> input_vec[:<span class=\"op\">-</span><span class=\"dv\">1</span>:<span class=\"dv\">2</span>]</span>\n<span id=\"cb1-19\"><a href=\"#cb1-19\" aria-hidden=\"true\" tabindex=\"-1\"></a>    output_vec <span class=\"op\">=</span> input_vec <span class=\"op\">*</span> np.cos(position <span class=\"op\">*</span> theta) <span class=\"op\">+</span> reranged_vec <span class=\"op\">*</span> np.sin(position <span class=\"op\">*</span> theta)</span>\n<span id=\"cb1-20\"><a href=\"#cb1-20\" aria-hidden=\"true\" tabindex=\"-1\"></a>    </span>\n<span id=\"cb1-21\"><a href=\"#cb1-21\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"cf\">return</span> output_vec</span>\n<span id=\"cb1-22\"><a href=\"#cb1-22\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-23\"><a href=\"#cb1-23\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-24\"><a href=\"#cb1-24\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"kw\">def</span> plot(x, y, name<span class=\"op\">=</span><span class=\"st\">&#39;&#39;</span>):</span>\n<span id=\"cb1-25\"><a href=\"#cb1-25\" aria-hidden=\"true\" tabindex=\"-1\"></a>    plt.plot(x, y, label<span class=\"op\">=</span>name)</span>\n<span id=\"cb1-26\"><a href=\"#cb1-26\" aria-hidden=\"true\" tabindex=\"-1\"></a>    plt.legend()</span>\n<span id=\"cb1-27\"><a href=\"#cb1-27\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"co\"># </span></span>\n<span id=\"cb1-28\"><a href=\"#cb1-28\" aria-hidden=\"true\" tabindex=\"-1\"></a>    plt.show()</span>\n<span id=\"cb1-29\"><a href=\"#cb1-29\" aria-hidden=\"true\" tabindex=\"-1\"></a>    </span>\n<span id=\"cb1-30\"><a href=\"#cb1-30\" aria-hidden=\"true\" tabindex=\"-1\"></a>base <span class=\"op\">=</span> <span class=\"dv\">10000</span></span>\n<span id=\"cb1-31\"><a href=\"#cb1-31\" aria-hidden=\"true\" tabindex=\"-1\"></a>window_size <span class=\"op\">=</span> <span class=\"dv\">4096</span></span>\n<span id=\"cb1-32\"><a href=\"#cb1-32\" aria-hidden=\"true\" tabindex=\"-1\"></a>d <span class=\"op\">=</span> <span class=\"dv\">512</span></span>\n<span id=\"cb1-33\"><a href=\"#cb1-33\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-34\"><a href=\"#cb1-34\" aria-hidden=\"true\" tabindex=\"-1\"></a>q <span class=\"op\">=</span> np.ones(d)</span>\n<span id=\"cb1-35\"><a href=\"#cb1-35\" aria-hidden=\"true\" tabindex=\"-1\"></a>k <span class=\"op\">=</span> np.ones(d)</span>\n<span id=\"cb1-36\"><a href=\"#cb1-36\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-37\"><a href=\"#cb1-37\" aria-hidden=\"true\" tabindex=\"-1\"></a>rotated_q <span class=\"op\">=</span> apply_rope(input_vec<span class=\"op\">=</span>q, position<span class=\"op\">=</span><span class=\"dv\">0</span>, base<span class=\"op\">=</span>base)</span>\n<span id=\"cb1-38\"><a href=\"#cb1-38\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-39\"><a href=\"#cb1-39\" aria-hidden=\"true\" tabindex=\"-1\"></a>inner_products <span class=\"op\">=</span> []</span>\n<span id=\"cb1-40\"><a href=\"#cb1-40\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(window_size):</span>\n<span id=\"cb1-41\"><a href=\"#cb1-41\" aria-hidden=\"true\" tabindex=\"-1\"></a>    rotated_k <span class=\"op\">=</span> apply_rope(input_vec<span class=\"op\">=</span>k, position<span class=\"op\">=</span>i, base<span class=\"op\">=</span>base)</span>\n<span id=\"cb1-42\"><a href=\"#cb1-42\" aria-hidden=\"true\" tabindex=\"-1\"></a>    product <span class=\"op\">=</span> np.dot(rotated_q, rotated_k)</span>\n<span id=\"cb1-43\"><a href=\"#cb1-43\" aria-hidden=\"true\" tabindex=\"-1\"></a>    inner_products.append(product)</span>\n<span id=\"cb1-44\"><a href=\"#cb1-44\" aria-hidden=\"true\" tabindex=\"-1\"></a>    </span>\n<span id=\"cb1-45\"><a href=\"#cb1-45\" aria-hidden=\"true\" tabindex=\"-1\"></a>plot(x<span class=\"op\">=</span><span class=\"bu\">range</span>(window_size), y<span class=\"op\">=</span>inner_products, name<span class=\"op\">=</span><span class=\"ss\">f&#39;base=</span><span class=\"sc\">&#123;</span>base<span class=\"sc\">&#125;</span><span class=\"ss\">,window size=</span><span class=\"sc\">&#123;</span>window_size<span class=\"sc\">&#125;</span><span class=\"ss\">,d=</span><span class=\"sc\">&#123;</span>d<span class=\"sc\">&#125;</span><span class=\"ss\">&#39;</span>)</span></code></pre></div>\n<p>1q = k = 1</p>\n<p>qk1q0k0~4096q</p>\n<img src=\"/f0902f1a/1.png\" class title=\"\">\n<p>base=10000d=512</p>\n<p></p>\n<p>409665536</p>\n<img src=\"/f0902f1a/2.png\" class title=\"\">\n<p>15000</p>\n<p>1/4</p>\n<p>basebasebase=5M10M</p>\n<p>base5M</p>\n<img src=\"/f0902f1a/3.png\" class title=\"\">\n<p></p>\n<p>q0k0~4096/65536q</p>\n<img src=\"/f0902f1a/4.png\" class title=\"\">\n<p>q</p>\n<p>2qk</p>\n<p>qk1qk</p>\n<img src=\"/f0902f1a/5.png\" class title=\"\">\n<p>1</p>\n<h1 id=\"\"></h1>\n<ul>\n<li>RoPEbase<br>\n</li>\n<li>qk</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Transformer2https://spaces.ac.cn/archives/8265<br>\n2RoFormer: Enhanced Transformer with Rotary Position Embedding\nhttps://arxiv.org/abs/2104.09864<br>\n3LLM:RoPE http://www.linsight.cn/c4da56c0.html</p>\n"},{"title":"loss","abbrlink":"f5fb75e4","date":"2024-06-15T08:13:55.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nUnderstanding Emergent Abilities of Language Models from the Loss Perspective -- loss  \n\n#   \n\nEmergent abilities of large language modelsemergent ability  \n\n  \n- LLaMA3GPT-3  \n- Are emergent abilities of large language models a mirage?  \n\nTraining compute-optimal large language modelsindicatorloss\n\nlosslossemergent ability  \n\n# pretraining loss  \n\n##   \n\n  \n- BPE  \n- LLaMAGQARoPEQ/K  \n- AdamW$\\beta_1=0.9$$\\beta_2=0.95$  \n- 2048  \n- 1:4  \n\n{% asset_img eng_data.png  %}  \n\n  \n\n612  \n\n{% asset_img downstream_dataset.png  %}  \n\n{% asset_img downstream_dataset_num.png  %}  \n\n## pretraining loss vs. performance  \n\n31.5B6B32B3T3T2.5T  \n\n{% asset_img exp1_param.png  %}  \n\n43B tokencheckpoint3checkpointloss  \n\n{% asset_img exp1_plot.png loss vs. performance %}  \n\n3  \n- loss  \n- loss  \n- losstokenlearning dynamics  \n\n  \n\n{% asset_img exp1_compute.png  %}  \n\n  \n\n## training token count vs. performance  \n\n28  \n\n{% asset_img exp2_param.png  %}  \n\ntokencheckpointtokendecay\n\n28checkpointloss  \n\n{% asset_img exp2_plot.png token count vs. performance %}  \n\nloss  \n\n28  \n\n## LLaMAs loss vs. performance  \n\nLLaMA  \n\nLLaMAcheckpointLLaMA6\n\n{% asset_img exp3_plot.png loss vs. performance %}  \n\nLLaMA  \n\n> pre-training loss is a good indicator of LMs performance on downstream tasks, independent of model sizes, training tokens, languages, and pretraining frameworks  \n\n#   \n\n##   \n\n122  \n- TriviaQA, HellaSwag, RACE, WinoGrande, NLPCC-KBQA, ClozeT, CLUEWSC, C3loss  \n- MMLU, C-Eval, GSM8K, GSM8K-Chineselossloss2.2prompt  \n\nGrokking: Generalization beyond overfitting on small algorithmic datasetsgrokking  \n\ngrokkingperfect generalizationimprovementimprovement  \n\nemergent abilityscaling lawtokenlossloss  \n\n##   \n\nemergent ability MMLU0  \n\n  \n- probability of the correct answer (CorrectChoiceProb)  \n- Are emergent abilities of large language models a mirage?Brier Score  \n\n$$\\text{BrierScore}=\\frac1N\\sum_{i=1}^N\\sum_{j=1}^C(y_{ij}-\\hat{y}_{ij})^2$$  \n\nNC  \n\nMMLUC-Eval  \n\n{% asset_img metrics.png  %}  \n\n  \n\nBrier Score  \n\nA/B/C/DA10000.250.250.250.250.25  \n\nBrier Score1.50.750.75Brier ScoreBrier Score1.51.0  \n\nTraining trajectories of language models across scalesperplexity of correct optionsperplexity of correct options  \n\nperplexityperplexityperplexityperplexity  \n\n# lossemergent abilities  \n\nlossemergent ability  \n\n> Definition. An ability is emergent if it is not present in models with higher pre-training loss but is present in models with lower pre-training loss.  \n\nemergent abilitynormalized performance0.250.25normalized performance0loss $L$   \n\n$$\\begin{cases}f(L)&\\mathrm{if~}L<\\eta\\\\0&\\mathrm{otherwise}&\\end{cases}$$  \n\nf$\\eta$   \n\nScaling laws for autoregressive generative modelingtoken $D$  $N$   \n\n$$L(N)=L_\\infty+\\left(\\frac{N_0}N\\right)^{\\alpha_N}$$  \n\n $L_{\\infty}$ irreducible loss$\\alpha_{N}$   \n\n  \n\n$$\\begin{cases}f\\left(L_\\infty+\\left(\\frac{N_0}N\\right)^{\\alpha_N}\\right)&\\text{if }N\\geq N_0\\cdot\\left(\\eta-L_\\infty\\right)^{-\\frac1{\\alpha_N}}\\\\0&\\text{otherwise}&\\end{cases}$$  \n\n $N_0\\cdot(\\eta-L_\\infty)^{-1/\\alpha_N}$ normalized performance0lossnormalized performance  \n\n#   \n\nlossloss  \n\nmodel familyloss  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1Understanding Emergent Abilities of Language Models from the Loss Perspective https://arxiv.org/abs/2403.15796  ","source":"_posts/cs/nlp/2024/06/loss.md","raw":"---\ntitle: loss\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: f5fb75e4\ndate: 2024-06-15 16:13:55\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nUnderstanding Emergent Abilities of Language Models from the Loss Perspective -- loss  \n\n#   \n\nEmergent abilities of large language modelsemergent ability  \n\n  \n- LLaMA3GPT-3  \n- Are emergent abilities of large language models a mirage?  \n\nTraining compute-optimal large language modelsindicatorloss\n\nlosslossemergent ability  \n\n# pretraining loss  \n\n##   \n\n  \n- BPE  \n- LLaMAGQARoPEQ/K  \n- AdamW$\\beta_1=0.9$$\\beta_2=0.95$  \n- 2048  \n- 1:4  \n\n{% asset_img eng_data.png  %}  \n\n  \n\n612  \n\n{% asset_img downstream_dataset.png  %}  \n\n{% asset_img downstream_dataset_num.png  %}  \n\n## pretraining loss vs. performance  \n\n31.5B6B32B3T3T2.5T  \n\n{% asset_img exp1_param.png  %}  \n\n43B tokencheckpoint3checkpointloss  \n\n{% asset_img exp1_plot.png loss vs. performance %}  \n\n3  \n- loss  \n- loss  \n- losstokenlearning dynamics  \n\n  \n\n{% asset_img exp1_compute.png  %}  \n\n  \n\n## training token count vs. performance  \n\n28  \n\n{% asset_img exp2_param.png  %}  \n\ntokencheckpointtokendecay\n\n28checkpointloss  \n\n{% asset_img exp2_plot.png token count vs. performance %}  \n\nloss  \n\n28  \n\n## LLaMAs loss vs. performance  \n\nLLaMA  \n\nLLaMAcheckpointLLaMA6\n\n{% asset_img exp3_plot.png loss vs. performance %}  \n\nLLaMA  \n\n> pre-training loss is a good indicator of LMs performance on downstream tasks, independent of model sizes, training tokens, languages, and pretraining frameworks  \n\n#   \n\n##   \n\n122  \n- TriviaQA, HellaSwag, RACE, WinoGrande, NLPCC-KBQA, ClozeT, CLUEWSC, C3loss  \n- MMLU, C-Eval, GSM8K, GSM8K-Chineselossloss2.2prompt  \n\nGrokking: Generalization beyond overfitting on small algorithmic datasetsgrokking  \n\ngrokkingperfect generalizationimprovementimprovement  \n\nemergent abilityscaling lawtokenlossloss  \n\n##   \n\nemergent ability MMLU0  \n\n  \n- probability of the correct answer (CorrectChoiceProb)  \n- Are emergent abilities of large language models a mirage?Brier Score  \n\n$$\\text{BrierScore}=\\frac1N\\sum_{i=1}^N\\sum_{j=1}^C(y_{ij}-\\hat{y}_{ij})^2$$  \n\nNC  \n\nMMLUC-Eval  \n\n{% asset_img metrics.png  %}  \n\n  \n\nBrier Score  \n\nA/B/C/DA10000.250.250.250.250.25  \n\nBrier Score1.50.750.75Brier ScoreBrier Score1.51.0  \n\nTraining trajectories of language models across scalesperplexity of correct optionsperplexity of correct options  \n\nperplexityperplexityperplexityperplexity  \n\n# lossemergent abilities  \n\nlossemergent ability  \n\n> Definition. An ability is emergent if it is not present in models with higher pre-training loss but is present in models with lower pre-training loss.  \n\nemergent abilitynormalized performance0.250.25normalized performance0loss $L$   \n\n$$\\begin{cases}f(L)&\\mathrm{if~}L<\\eta\\\\0&\\mathrm{otherwise}&\\end{cases}$$  \n\nf$\\eta$   \n\nScaling laws for autoregressive generative modelingtoken $D$  $N$   \n\n$$L(N)=L_\\infty+\\left(\\frac{N_0}N\\right)^{\\alpha_N}$$  \n\n $L_{\\infty}$ irreducible loss$\\alpha_{N}$   \n\n  \n\n$$\\begin{cases}f\\left(L_\\infty+\\left(\\frac{N_0}N\\right)^{\\alpha_N}\\right)&\\text{if }N\\geq N_0\\cdot\\left(\\eta-L_\\infty\\right)^{-\\frac1{\\alpha_N}}\\\\0&\\text{otherwise}&\\end{cases}$$  \n\n $N_0\\cdot(\\eta-L_\\infty)^{-1/\\alpha_N}$ normalized performance0lossnormalized performance  \n\n#   \n\nlossloss  \n\nmodel familyloss  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1Understanding Emergent Abilities of Language Models from the Loss Perspective https://arxiv.org/abs/2403.15796  ","slug":"cs/nlp/2024/06/loss","published":1,"updated":"2024-06-16T09:16:55.156Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmp000c0p4k9p4u62qk","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>Understanding Emergent Abilities of Language Models from the\nLoss Perspective --\nloss</p>\n<h1 id=\"\"></h1>\n<p>Emergent abilities of large language modelsemergent\nability</p>\n<p><br>\n-\nLLaMA3GPT-3<br>\n- Are emergent abilities of large language models a\nmirage?</p>\n<p>Training compute-optimal large language\nmodelsindicatorloss</p>\n<p>losslossemergent\nability</p>\n<h1 id=\"pretraining-loss\">pretraining\nloss</h1>\n<h2 id=\"\"></h2>\n<p><br>\n- BPE<br>\n- LLaMAGQARoPEQ/K<br>\n- AdamW<span class=\"math inline\">\\(\\beta_1=0.9\\)</span><span class=\"math inline\">\\(\\beta_2=0.95\\)</span><br>\n- 2048<br>\n-\n1:4</p>\n<img src=\"/f5fb75e4/eng_data.png\" class title=\"\">\n<p></p>\n<p>612</p>\n<img src=\"/f5fb75e4/downstream_dataset.png\" class title=\"\">\n<img src=\"/f5fb75e4/downstream_dataset_num.png\" class title=\"\">\n<h2 id=\"pretraining-loss-vs.-performance\">pretraining loss\nvs. performance</h2>\n<p>31.5B6B32B3T3T2.5T</p>\n<img src=\"/f5fb75e4/exp1_param.png\" class title=\"\">\n<p>43B\ntokencheckpoint3checkpointloss</p>\n<img src=\"/f5fb75e4/exp1_plot.png\" class title=\"loss vs. performance\">\n<p>3<br>\n-\nloss<br>\n-\nloss<br>\n-\nlosstokenlearning\ndynamics</p>\n<p></p>\n<img src=\"/f5fb75e4/exp1_compute.png\" class title=\"\">\n<p></p>\n<h2 id=\"training-token-count-vs.-performance\">training\ntoken count vs. performance</h2>\n<p>28</p>\n<img src=\"/f5fb75e4/exp2_param.png\" class title=\"\">\n<p>tokencheckpointtokendecay</p>\n<p>28checkpointloss</p>\n<img src=\"/f5fb75e4/exp2_plot.png\" class title=\"token count vs. performance\">\n<p>loss</p>\n<p>28</p>\n<h2 id=\"llamas-loss-vs.-performance\">LLaMAs loss vs. performance</h2>\n<p>LLaMA</p>\n<p>LLaMAcheckpointLLaMA6</p>\n<img src=\"/f5fb75e4/exp3_plot.png\" class title=\"loss vs. performance\">\n<p>LLaMA</p>\n<blockquote>\n<p>pre-training loss is a good indicator of LMs performance on\ndownstream tasks, independent of model sizes, training tokens,\nlanguages, and pretraining frameworks</p>\n</blockquote>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>122<br>\n- TriviaQA, HellaSwag, RACE, WinoGrande, NLPCC-KBQA, ClozeT,\nCLUEWSC, C3loss<br>\n- MMLU, C-Eval, GSM8K,\nGSM8K-Chineselossloss2.2prompt</p>\n<p>Grokking: Generalization beyond overfitting on\nsmall algorithmic datasetsgrokking</p>\n<p>grokkingperfect\ngeneralizationimprovementimprovement</p>\n<p>emergent abilityscaling\nlawtokenlossloss</p>\n<h2 id=\"\"></h2>\n<p>emergent\nability\nMMLU0</p>\n<p><br>\n- probability of the correct answer (CorrectChoiceProb)<br>\n- Are emergent abilities of large language models a\nmirage?Brier Score</p>\n<p><span class=\"math display\">\\[\\text{BrierScore}=\\frac1N\\sum_{i=1}^N\\sum_{j=1}^C(y_{ij}-\\hat{y}_{ij})^2\\]</span></p>\n<p>NC</p>\n<p>MMLUC-Eval</p>\n<img src=\"/f5fb75e4/metrics.png\" class title=\"\">\n<p></p>\n<p>Brier Score</p>\n<p>A/B/C/DA10000.250.250.250.250.25</p>\n<p>Brier\nScore1.50.750.75Brier\nScoreBrier\nScore1.51.0</p>\n<p>Training trajectories of language models across\nscalesperplexity of correct\noptionsperplexity of correct\noptions</p>\n<p>perplexityperplexityperplexityperplexity</p>\n<h1 id=\"lossemergent-abilities\">lossemergent\nabilities</h1>\n<p>lossemergent\nability</p>\n<blockquote>\n<p>Definition. An ability is emergent if it is not present in models\nwith higher pre-training loss but is present in models with lower\npre-training loss.</p>\n</blockquote>\n<p>emergent abilitynormalized\nperformance0.250.25normalized\nperformance0loss <span class=\"math inline\">\\(L\\)</span> </p>\n<p><span class=\"math display\">\\[\\begin{cases}f(L)&amp;\\mathrm{if~}L&lt;\\eta\\\\0&amp;\\mathrm{otherwise}&amp;\\end{cases}\\]</span></p>\n<p>f<span class=\"math inline\">\\(\\eta\\)</span>\n</p>\n<p>Scaling laws for autoregressive generative\nmodelingtoken <span class=\"math inline\">\\(D\\)</span>  <span class=\"math inline\">\\(N\\)</span> </p>\n<p><span class=\"math display\">\\[L(N)=L_\\infty+\\left(\\frac{N_0}N\\right)^{\\alpha_N}\\]</span></p>\n<p> <span class=\"math inline\">\\(L_{\\infty}\\)</span> irreducible\nloss<span class=\"math inline\">\\(\\alpha_{N}\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{cases}f\\left(L_\\infty+\\left(\\frac{N_0}N\\right)^{\\alpha_N}\\right)&amp;\\text{if\n}N\\geq\nN_0\\cdot\\left(\\eta-L_\\infty\\right)^{-\\frac1{\\alpha_N}}\\\\0&amp;\\text{otherwise}&amp;\\end{cases}\\]</span></p>\n<p> <span class=\"math inline\">\\(N_0\\cdot(\\eta-L_\\infty)^{-1/\\alpha_N}\\)</span>\nnormalized\nperformance0lossnormalized\nperformance</p>\n<h1 id=\"\"></h1>\n<p>lossloss</p>\n<p>model\nfamilyloss</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Understanding Emergent Abilities of Language Models from the\nLoss Perspective https://arxiv.org/abs/2403.15796</p>\n","length":5448,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>Understanding Emergent Abilities of Language Models from the\nLoss Perspective --\nloss</p>\n<h1 id=\"\"></h1>\n<p>Emergent abilities of large language modelsemergent\nability</p>\n<p><br>\n-\nLLaMA3GPT-3<br>\n- Are emergent abilities of large language models a\nmirage?</p>\n<p>Training compute-optimal large language\nmodelsindicatorloss</p>\n<p>losslossemergent\nability</p>\n<h1 id=\"pretraining-loss\">pretraining\nloss</h1>\n<h2 id=\"\"></h2>\n<p><br>\n- BPE<br>\n- LLaMAGQARoPEQ/K<br>\n- AdamW<span class=\"math inline\">\\(\\beta_1=0.9\\)</span><span class=\"math inline\">\\(\\beta_2=0.95\\)</span><br>\n- 2048<br>\n-\n1:4</p>\n<img src=\"/f5fb75e4/eng_data.png\" class title=\"\">\n<p></p>\n<p>612</p>\n<img src=\"/f5fb75e4/downstream_dataset.png\" class title=\"\">\n<img src=\"/f5fb75e4/downstream_dataset_num.png\" class title=\"\">\n<h2 id=\"pretraining-loss-vs.-performance\">pretraining loss\nvs. performance</h2>\n<p>31.5B6B32B3T3T2.5T</p>\n<img src=\"/f5fb75e4/exp1_param.png\" class title=\"\">\n<p>43B\ntokencheckpoint3checkpointloss</p>\n<img src=\"/f5fb75e4/exp1_plot.png\" class title=\"loss vs. performance\">\n<p>3<br>\n-\nloss<br>\n-\nloss<br>\n-\nlosstokenlearning\ndynamics</p>\n<p></p>\n<img src=\"/f5fb75e4/exp1_compute.png\" class title=\"\">\n<p></p>\n<h2 id=\"training-token-count-vs.-performance\">training\ntoken count vs. performance</h2>\n<p>28</p>\n<img src=\"/f5fb75e4/exp2_param.png\" class title=\"\">\n<p>tokencheckpointtokendecay</p>\n<p>28checkpointloss</p>\n<img src=\"/f5fb75e4/exp2_plot.png\" class title=\"token count vs. performance\">\n<p>loss</p>\n<p>28</p>\n<h2 id=\"llamas-loss-vs.-performance\">LLaMAs loss vs. performance</h2>\n<p>LLaMA</p>\n<p>LLaMAcheckpointLLaMA6</p>\n<img src=\"/f5fb75e4/exp3_plot.png\" class title=\"loss vs. performance\">\n<p>LLaMA</p>\n<blockquote>\n<p>pre-training loss is a good indicator of LMs performance on\ndownstream tasks, independent of model sizes, training tokens,\nlanguages, and pretraining frameworks</p>\n</blockquote>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>122<br>\n- TriviaQA, HellaSwag, RACE, WinoGrande, NLPCC-KBQA, ClozeT,\nCLUEWSC, C3loss<br>\n- MMLU, C-Eval, GSM8K,\nGSM8K-Chineselossloss2.2prompt</p>\n<p>Grokking: Generalization beyond overfitting on\nsmall algorithmic datasetsgrokking</p>\n<p>grokkingperfect\ngeneralizationimprovementimprovement</p>\n<p>emergent abilityscaling\nlawtokenlossloss</p>\n<h2 id=\"\"></h2>\n<p>emergent\nability\nMMLU0</p>\n<p><br>\n- probability of the correct answer (CorrectChoiceProb)<br>\n- Are emergent abilities of large language models a\nmirage?Brier Score</p>\n<p><span class=\"math display\">\\[\\text{BrierScore}=\\frac1N\\sum_{i=1}^N\\sum_{j=1}^C(y_{ij}-\\hat{y}_{ij})^2\\]</span></p>\n<p>NC</p>\n<p>MMLUC-Eval</p>\n<img src=\"/f5fb75e4/metrics.png\" class title=\"\">\n<p></p>\n<p>Brier Score</p>\n<p>A/B/C/DA10000.250.250.250.250.25</p>\n<p>Brier\nScore1.50.750.75Brier\nScoreBrier\nScore1.51.0</p>\n<p>Training trajectories of language models across\nscalesperplexity of correct\noptionsperplexity of correct\noptions</p>\n<p>perplexityperplexityperplexityperplexity</p>\n<h1 id=\"lossemergent-abilities\">lossemergent\nabilities</h1>\n<p>lossemergent\nability</p>\n<blockquote>\n<p>Definition. An ability is emergent if it is not present in models\nwith higher pre-training loss but is present in models with lower\npre-training loss.</p>\n</blockquote>\n<p>emergent abilitynormalized\nperformance0.250.25normalized\nperformance0loss <span class=\"math inline\">\\(L\\)</span> </p>\n<p><span class=\"math display\">\\[\\begin{cases}f(L)&amp;\\mathrm{if~}L&lt;\\eta\\\\0&amp;\\mathrm{otherwise}&amp;\\end{cases}\\]</span></p>\n<p>f<span class=\"math inline\">\\(\\eta\\)</span>\n</p>\n<p>Scaling laws for autoregressive generative\nmodelingtoken <span class=\"math inline\">\\(D\\)</span>  <span class=\"math inline\">\\(N\\)</span> </p>\n<p><span class=\"math display\">\\[L(N)=L_\\infty+\\left(\\frac{N_0}N\\right)^{\\alpha_N}\\]</span></p>\n<p> <span class=\"math inline\">\\(L_{\\infty}\\)</span> irreducible\nloss<span class=\"math inline\">\\(\\alpha_{N}\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{cases}f\\left(L_\\infty+\\left(\\frac{N_0}N\\right)^{\\alpha_N}\\right)&amp;\\text{if\n}N\\geq\nN_0\\cdot\\left(\\eta-L_\\infty\\right)^{-\\frac1{\\alpha_N}}\\\\0&amp;\\text{otherwise}&amp;\\end{cases}\\]</span></p>\n<p> <span class=\"math inline\">\\(N_0\\cdot(\\eta-L_\\infty)^{-1/\\alpha_N}\\)</span>\nnormalized\nperformance0lossnormalized\nperformance</p>\n<h1 id=\"\"></h1>\n<p>lossloss</p>\n<p>model\nfamilyloss</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Understanding Emergent Abilities of Language Models from the\nLoss Perspective https://arxiv.org/abs/2403.15796</p>\n"},{"title":"LLMICL","abbrlink":"7381cae3","date":"2024-06-17T11:22:22.000Z","_content":"\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nLLM  \n\nin-context learning  \n\n# \n\n  \n\nChatGPTLLM  \n\n  \n\n/  \n\n  \n\n  \n\n# induction heads  \n\nAnthropic22In-context Learning and Induction HeadstransformerICLinduction heads  \n\ninduction headscircuittokentokentoken\n\n  \n\n<center>  \n... [token A] [token B] [token C] ... [toekn A]  \n</center>  \n\ntokeninduction heads [toekn A]  [toekn A] [token A] [token B] [token C] pattern  \n\n<center>  \n... [token A] [token B] [token C] ... [toekn A] [toekn B] [toekn C]  \n</center>  \n\ninduction headstokentokeninduction headstokentokenpatternpattern  \n\n<center>  \n... [token A] [token B] [token C] ... [toekn A]  \n</center>  \n\n [toekn A]  [toekn A] induction heads  \n\n<center>  \n... [token A] [token B] [token C] ... [toekn A] [toekn B] [toekn C]  \n</center>  \n\n [toekn B][toekn C][token B][token C]  \n\ninduction heads2attention headAnthropic2transformertransformer\n\ninduction headsICL[token A] [token B] [toekn A]  [toekn B]ICL  \n\ninduction headsLLMnext token predictiontokentoken/patterntoken  \n\n# ICL  \n\nLearning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation2022  \n\nDITTOpseuDo-repetITion penalizaTiOn NEural contextualiZed representation for cHinese lAnguage understanding  \n\nmaximization-based decodinggreedy decoding  \n\n  \n\ninduction heads  \n\nself-reinforcement  \n\ntoken  \n\n{% asset_img ditto_1.png  %}  \n\nSentences with higher initial probabilities usually have a\nstronger self-reinforcement effect  \n\ntokentoken  \n\n{% asset_img ditto_2.png  %}  \n\nDITTOtoken  \n\nUNDERSTANDING IN-CONTEXT LEARNING FROM REPETITIONSself-reinforcement  \n- self-reinforcementLLM  \n- self-reinforcementtoken  \n\n{% asset_img 3.png  %}  \n\n- token1self-reinforcementself-reinforcement  \n\n{% asset_img 4.png  %}  \n\ntokentokennext token predictionself-reinforcement  \n\n{% asset_img 5.png  %}  \n\n  \n\nconstraining output space  \n\nICLdemonstrationself-reinforcementICLABCD  \n\nICLdemonstrationmaskABCDAnswer  \n\n{% asset_img 6.png  %}  \n\nmaskABCDABCD/Answerdemonstration  \n\nlearning to follow patterns  \n\nCoT  \n\nspurious connections\n\nICLself-reinforcementprompt  \n\n#     \n\nself-reinforcementICL  \n\n  \n\ndecoding-base  \n- stochastic samplingtokentop-ktop-p  \n- tokentoken  \n- contrastive decodingtoken  \n- beam search  \n- locally typical sampling  \n- no repeat ngramngram  \n\ntraining-based\n- token  \n-   \n\n#   \n\n  \n\nICL  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1 https://www.zhihu.com/question/616130636  \n2Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation https://arxiv.org/abs/2206.02369  \n3Understanding In-Context Learning from Repetitions https://arxiv.org/abs/2310.00297  \n4In-context Learning and Induction Heads https://arxiv.org/abs/2209.11895  \n5https://zhuanlan.zhihu.com/p/671697479","source":"_posts/cs/nlp/2024/06/LLM.md","raw":"---\ntitle: LLMICL\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 7381cae3\ndate: 2024-06-17 19:22:22\n---\n\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nLLM  \n\nin-context learning  \n\n# \n\n  \n\nChatGPTLLM  \n\n  \n\n/  \n\n  \n\n  \n\n# induction heads  \n\nAnthropic22In-context Learning and Induction HeadstransformerICLinduction heads  \n\ninduction headscircuittokentokentoken\n\n  \n\n<center>  \n... [token A] [token B] [token C] ... [toekn A]  \n</center>  \n\ntokeninduction heads [toekn A]  [toekn A] [token A] [token B] [token C] pattern  \n\n<center>  \n... [token A] [token B] [token C] ... [toekn A] [toekn B] [toekn C]  \n</center>  \n\ninduction headstokentokeninduction headstokentokenpatternpattern  \n\n<center>  \n... [token A] [token B] [token C] ... [toekn A]  \n</center>  \n\n [toekn A]  [toekn A] induction heads  \n\n<center>  \n... [token A] [token B] [token C] ... [toekn A] [toekn B] [toekn C]  \n</center>  \n\n [toekn B][toekn C][token B][token C]  \n\ninduction heads2attention headAnthropic2transformertransformer\n\ninduction headsICL[token A] [token B] [toekn A]  [toekn B]ICL  \n\ninduction headsLLMnext token predictiontokentoken/patterntoken  \n\n# ICL  \n\nLearning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation2022  \n\nDITTOpseuDo-repetITion penalizaTiOn NEural contextualiZed representation for cHinese lAnguage understanding  \n\nmaximization-based decodinggreedy decoding  \n\n  \n\ninduction heads  \n\nself-reinforcement  \n\ntoken  \n\n{% asset_img ditto_1.png  %}  \n\nSentences with higher initial probabilities usually have a\nstronger self-reinforcement effect  \n\ntokentoken  \n\n{% asset_img ditto_2.png  %}  \n\nDITTOtoken  \n\nUNDERSTANDING IN-CONTEXT LEARNING FROM REPETITIONSself-reinforcement  \n- self-reinforcementLLM  \n- self-reinforcementtoken  \n\n{% asset_img 3.png  %}  \n\n- token1self-reinforcementself-reinforcement  \n\n{% asset_img 4.png  %}  \n\ntokentokennext token predictionself-reinforcement  \n\n{% asset_img 5.png  %}  \n\n  \n\nconstraining output space  \n\nICLdemonstrationself-reinforcementICLABCD  \n\nICLdemonstrationmaskABCDAnswer  \n\n{% asset_img 6.png  %}  \n\nmaskABCDABCD/Answerdemonstration  \n\nlearning to follow patterns  \n\nCoT  \n\nspurious connections\n\nICLself-reinforcementprompt  \n\n#     \n\nself-reinforcementICL  \n\n  \n\ndecoding-base  \n- stochastic samplingtokentop-ktop-p  \n- tokentoken  \n- contrastive decodingtoken  \n- beam search  \n- locally typical sampling  \n- no repeat ngramngram  \n\ntraining-based\n- token  \n-   \n\n#   \n\n  \n\nICL  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1 https://www.zhihu.com/question/616130636  \n2Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation https://arxiv.org/abs/2206.02369  \n3Understanding In-Context Learning from Repetitions https://arxiv.org/abs/2310.00297  \n4In-context Learning and Induction Heads https://arxiv.org/abs/2209.11895  \n5https://zhuanlan.zhihu.com/p/671697479","slug":"cs/nlp/2024/06/LLM","published":1,"updated":"2024-06-18T13:36:13.750Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmp000d0p4k8ivqhhwc","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>LLM</p>\n<p>in-context learning</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>ChatGPTLLM</p>\n<p></p>\n<p>/</p>\n<p></p>\n<p></p>\n<h1 id=\"induction-heads\">induction heads</h1>\n<p>Anthropic22In-context Learning and Induction\nHeadstransformerICLinduction\nheads</p>\n<p>induction\nheadscircuittokentokentoken</p>\n<p></p>\n<center>\n... [token A] [token B] [token C] ... [toekn A]\n</center>\n<p>tokeninduction heads [toekn A]\n [toekn\nA] [token A] [token B] [token\nC] pattern</p>\n<center>\n... [token A] [token B] [token C] ... [toekn A] [toekn B] [toekn C]\n</center>\n<p>induction\nheadstokentokeninduction\nheadstokentokenpatternpattern</p>\n<center>\n... [token A] [token B] [token C] ... [toekn A]\n</center>\n<p> [toekn A]  [toekn A] induction\nheads</p>\n<center>\n... [token A] [token B] [token C] ... [toekn A] [toekn B] [toekn C]\n</center>\n<p> [toekn B][toekn C][token B][token\nC]</p>\n<p>induction heads2attention\nheadAnthropic2transformertransformer</p>\n<p>induction\nheadsICL[token\nA] [token B] [toekn A]  [toekn B]ICL</p>\n<p>induction\nheadsLLMnext\ntoken\npredictiontokentoken/patterntoken</p>\n<h1 id=\"icl\">ICL</h1>\n<p>Learning to Break the Loop: Analyzing and Mitigating\nRepetitions for Neural Text Generation2022</p>\n<p>DITTOpseuDo-repetITion\npenalizaTiOn NEural\ncontextualiZed representation for cHinese lAnguage understanding</p>\n<p>maximization-based decodinggreedy\ndecoding</p>\n<p></p>\n<p>induction\nheads</p>\n<p>self-reinforcement</p>\n<p>token</p>\n<img src=\"/7381cae3/ditto_1.png\" class title=\"\">\n<p>Sentences with higher initial probabilities usually have a\nstronger self-reinforcement effect</p>\n<p>tokentoken</p>\n<img src=\"/7381cae3/ditto_2.png\" class title=\"\">\n<p>DITTOtoken</p>\n<p>UNDERSTANDING IN-CONTEXT LEARNING FROM\nREPETITIONSself-reinforcement<br>\n- self-reinforcementLLM<br>\n- self-reinforcementtoken</p>\n<img src=\"/7381cae3/3.png\" class title=\"\">\n<ul>\n<li>token1self-reinforcementself-reinforcement</li>\n</ul>\n<img src=\"/7381cae3/4.png\" class title=\"\">\n<p>tokentokennext token\npredictionself-reinforcement</p>\n<img src=\"/7381cae3/5.png\" class title=\"\">\n<p></p>\n<p>constraining output space</p>\n<p>ICLdemonstrationself-reinforcementICLABCD</p>\n<p>ICLdemonstrationmaskABCDAnswer</p>\n<img src=\"/7381cae3/6.png\" class title=\"\">\n<p>maskABCDABCD/Answerdemonstration</p>\n<p>learning to follow patterns</p>\n<p>CoT</p>\n<p>spurious connections</p>\n<p>ICLself-reinforcementprompt</p>\n<h1 id=\"\"></h1>\n<p>self-reinforcementICL</p>\n<p></p>\n<p>decoding-base<br>\n- stochastic\nsamplingtokentop-ktop-p<br>\n- tokentoken<br>\n- contrastive\ndecodingtoken<br>\n- beam search<br>\n- locally typical sampling<br>\n- no repeat ngramngram</p>\n<p>training-based -\ntoken<br>\n-\n</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>ICL</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1\nhttps://www.zhihu.com/question/616130636<br>\n2Learning to Break the Loop: Analyzing and Mitigating Repetitions\nfor Neural Text Generation https://arxiv.org/abs/2206.02369<br>\n3Understanding In-Context Learning from Repetitions\nhttps://arxiv.org/abs/2310.00297<br>\n4In-context Learning and Induction Heads\nhttps://arxiv.org/abs/2209.11895<br>\n5https://zhuanlan.zhihu.com/p/671697479</p>\n","length":4666,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>LLM</p>\n<p>in-context learning</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>ChatGPTLLM</p>\n<p></p>\n<p>/</p>\n<p></p>\n<p></p>\n<h1 id=\"induction-heads\">induction heads</h1>\n<p>Anthropic22In-context Learning and Induction\nHeadstransformerICLinduction\nheads</p>\n<p>induction\nheadscircuittokentokentoken</p>\n<p></p>\n<center>\n... [token A] [token B] [token C] ... [toekn A]\n</center>\n<p>tokeninduction heads [toekn A]\n [toekn\nA] [token A] [token B] [token\nC] pattern</p>\n<center>\n... [token A] [token B] [token C] ... [toekn A] [toekn B] [toekn C]\n</center>\n<p>induction\nheadstokentokeninduction\nheadstokentokenpatternpattern</p>\n<center>\n... [token A] [token B] [token C] ... [toekn A]\n</center>\n<p> [toekn A]  [toekn A] induction\nheads</p>\n<center>\n... [token A] [token B] [token C] ... [toekn A] [toekn B] [toekn C]\n</center>\n<p> [toekn B][toekn C][token B][token\nC]</p>\n<p>induction heads2attention\nheadAnthropic2transformertransformer</p>\n<p>induction\nheadsICL[token\nA] [token B] [toekn A]  [toekn B]ICL</p>\n<p>induction\nheadsLLMnext\ntoken\npredictiontokentoken/patterntoken</p>\n<h1 id=\"icl\">ICL</h1>\n<p>Learning to Break the Loop: Analyzing and Mitigating\nRepetitions for Neural Text Generation2022</p>\n<p>DITTOpseuDo-repetITion\npenalizaTiOn NEural\ncontextualiZed representation for cHinese lAnguage understanding</p>\n<p>maximization-based decodinggreedy\ndecoding</p>\n<p></p>\n<p>induction\nheads</p>\n<p>self-reinforcement</p>\n<p>token</p>\n<img src=\"/7381cae3/ditto_1.png\" class title=\"\">\n<p>Sentences with higher initial probabilities usually have a\nstronger self-reinforcement effect</p>\n<p>tokentoken</p>\n<img src=\"/7381cae3/ditto_2.png\" class title=\"\">\n<p>DITTOtoken</p>\n<p>UNDERSTANDING IN-CONTEXT LEARNING FROM\nREPETITIONSself-reinforcement<br>\n- self-reinforcementLLM<br>\n- self-reinforcementtoken</p>\n<img src=\"/7381cae3/3.png\" class title=\"\">\n<ul>\n<li>token1self-reinforcementself-reinforcement</li>\n</ul>\n<img src=\"/7381cae3/4.png\" class title=\"\">\n<p>tokentokennext token\npredictionself-reinforcement</p>\n<img src=\"/7381cae3/5.png\" class title=\"\">\n<p></p>\n<p>constraining output space</p>\n<p>ICLdemonstrationself-reinforcementICLABCD</p>\n<p>ICLdemonstrationmaskABCDAnswer</p>\n<img src=\"/7381cae3/6.png\" class title=\"\">\n<p>maskABCDABCD/Answerdemonstration</p>\n<p>learning to follow patterns</p>\n<p>CoT</p>\n<p>spurious connections</p>\n<p>ICLself-reinforcementprompt</p>\n<h1 id=\"\"></h1>\n<p>self-reinforcementICL</p>\n<p></p>\n<p>decoding-base<br>\n- stochastic\nsamplingtokentop-ktop-p<br>\n- tokentoken<br>\n- contrastive\ndecodingtoken<br>\n- beam search<br>\n- locally typical sampling<br>\n- no repeat ngramngram</p>\n<p>training-based -\ntoken<br>\n-\n</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>ICL</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1\nhttps://www.zhihu.com/question/616130636<br>\n2Learning to Break the Loop: Analyzing and Mitigating Repetitions\nfor Neural Text Generation https://arxiv.org/abs/2206.02369<br>\n3Understanding In-Context Learning from Repetitions\nhttps://arxiv.org/abs/2310.00297<br>\n4In-context Learning and Induction Heads\nhttps://arxiv.org/abs/2209.11895<br>\n5https://zhuanlan.zhihu.com/p/671697479</p>\n"},{"title":"-IPO","abbrlink":"4fe7b810","date":"2024-06-02T03:58:52.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nDPOODPOsimPO[-DPO](http://www.linsight.cn/473f2b43.html)[-ODPO](http://www.linsight.cn/da871ebe.html)[-simPO](http://www.linsight.cn/280fa97a.html)  \n\nA General Theoretical Paradigm to Understand Learning from Human PreferencesRLHFDPOgeneralPOPOIdentity-PO (IPO)  \n\n# PO  \n\nRLHF  \n\n$$\\mathbb{E}_\\pi[r(x,y)]-\\beta D_{\\text{KL}}(\\pi\\mid\\mid\\pi_{\\text{ref}})$$  \n\nDPODPO  \n\n$$\\begin{aligned}\\min_{\\pi}\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\Bigg[-\\log\\sigma\\Bigg(\\beta\\log\\Bigg(\\frac{\\pi(y_w|x)}{\\pi(y_l|x)}\\Bigg)-\\beta\\log\\left(\\frac{\\pi_{\\mathrm{ref}}(y_w|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}\\Bigg)\\Bigg)\\right]\\end{aligned}$$  \n\nIPOgeneralpreference probabilitynon-decreasing function   \n\n$$\\Psi:\\begin{bmatrix}0,1\\end{bmatrix}\\to\\mathbb{R}$$  \n\n-preference optimisation objective  \n\n$$\\max_\\pi\\quad\\mathbb{E}_{x\\thicksim\\rho}\\quad[\\Psi(p^*(y\\succ y'|x))]-\\beta D_{\\mathrm{KL}}(\\pi\\mid\\mid\\pi_{\\mathrm{ref}})$$  \n\n  \n\n$$\\Psi(q)=\\log(q/(1-q))$$  \n\nBradley-Terry model  \n\n$$\\begin{aligned}\n\\mathbb{E}_{y'\\thicksim\\mu}[\\Psi(p^*(y\\succ y'))]& =\\underset{y'\\thicksim\\mu}{\\operatorname*{\\mathbb{E}}}\\left[\\Psi\\left(\\frac{e^{r(y)}}{e^{r(y)}+e^{r(y')}}\\right)\\right]  \\\\\n&=\\mathbb{E}_{y^{\\prime}\\thicksim\\mu}[\\log(e^{r(y)}/e^{r(y^{\\prime})})] \\\\\n&=\\mathbb{E}_{y'\\thicksim\\mu}[r(y)-r(y')] \\\\\n&=r(y)-\\underset{y'\\thicksim\\mu}{\\mathbb{E}}[r(y')]\n\\end{aligned}$$  \n\nPORLHFDPO  \n\nDPOPOBradley-Terry model  \n\n$$\\pi^*(y)\\propto\\pi_{\\mathrm{ref}}(y)\\exp\\left(\\beta^{-1}\\mathbb{E}_{y^{\\prime}\\thicksim\\mu}[\\Psi(p^*(y\\succ y^{\\prime}))]\\right)$$  \n\n(q)  \n\n{% asset_img curve.png log %}  \n\n(q)  \n\n  \n\n$$p^*(y\\succ y')=1$$  \n\nBT  \n\n$$(r(y)-r(y'))\\to+\\infty$$  \n\n $(r(y)-r(y'))\\to+\\infty$ PO  \n\n$$\\begin{aligned}\n&\\frac{\\pi^*(y_l)}{\\pi^*(y_w)}\\\\\n=&\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}\\left(\\beta^{-1}\\sum_{y^{\\prime}}[\\Psi(p(y_l\\succ y^{\\prime}))-\\Psi(p(y_w\\succ y^{\\prime}))]\\right)\\\\\n=&\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}(\\beta^{-1}\\sum_{y^{\\prime}}[r(y_l)-r(y_w)])\\\\\n=&\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}(\\beta^{-1}\\sum_{y^{\\prime}}[-\\infty])\\\\\n=&0\n\\end{aligned}$$  \n\n $\\beta$  $\\pi^*(y_l)=0$KLKLreward  \n\nRLHFrewardDPOreward  \n\n# IPO  \n\n(q)DPO(q)identity mappingIPO  \n\n$$\\max_\\pi\\quad\\mathbb{E}_{x\\thicksim\\rho}\\quad[p^*(y\\succ y'|x)]-\\beta D_{\\mathrm{KL}}(\\pi\\mid\\mid\\pi_{\\mathrm{ref}})$$  \n\nIPO  \n\n$$\\mathbb{E}_{(y_w,y_l,x)\\thicksim D}\\left(h_\\pi(y_w,y_l,x)-\\frac{\\beta^{-1}}2\\right)^2$$  \n\n$$h_\\pi(y,y',x)=\\log\\left(\\frac{\\pi(y|x)\\pi_{\\text{ref}}(y'|x)}{\\pi(y'|x)\\pi_{\\text{ref}}(y|x)}\\right)$$  \n\n#   \n\nPO/IPODPODPO  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n1A General Theoretical Paradigm to Understand Learning from Human Preferences https://arxiv.org/abs/2310.12036  \n","source":"_posts/cs/nlp/2024/06/-IPO.md","raw":"---\ntitle: -IPO\nabbrlink: 4fe7b810\ndate: 2024-06-02 11:58:52\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \n  - SFT\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nDPOODPOsimPO[-DPO](http://www.linsight.cn/473f2b43.html)[-ODPO](http://www.linsight.cn/da871ebe.html)[-simPO](http://www.linsight.cn/280fa97a.html)  \n\nA General Theoretical Paradigm to Understand Learning from Human PreferencesRLHFDPOgeneralPOPOIdentity-PO (IPO)  \n\n# PO  \n\nRLHF  \n\n$$\\mathbb{E}_\\pi[r(x,y)]-\\beta D_{\\text{KL}}(\\pi\\mid\\mid\\pi_{\\text{ref}})$$  \n\nDPODPO  \n\n$$\\begin{aligned}\\min_{\\pi}\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\Bigg[-\\log\\sigma\\Bigg(\\beta\\log\\Bigg(\\frac{\\pi(y_w|x)}{\\pi(y_l|x)}\\Bigg)-\\beta\\log\\left(\\frac{\\pi_{\\mathrm{ref}}(y_w|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}\\Bigg)\\Bigg)\\right]\\end{aligned}$$  \n\nIPOgeneralpreference probabilitynon-decreasing function   \n\n$$\\Psi:\\begin{bmatrix}0,1\\end{bmatrix}\\to\\mathbb{R}$$  \n\n-preference optimisation objective  \n\n$$\\max_\\pi\\quad\\mathbb{E}_{x\\thicksim\\rho}\\quad[\\Psi(p^*(y\\succ y'|x))]-\\beta D_{\\mathrm{KL}}(\\pi\\mid\\mid\\pi_{\\mathrm{ref}})$$  \n\n  \n\n$$\\Psi(q)=\\log(q/(1-q))$$  \n\nBradley-Terry model  \n\n$$\\begin{aligned}\n\\mathbb{E}_{y'\\thicksim\\mu}[\\Psi(p^*(y\\succ y'))]& =\\underset{y'\\thicksim\\mu}{\\operatorname*{\\mathbb{E}}}\\left[\\Psi\\left(\\frac{e^{r(y)}}{e^{r(y)}+e^{r(y')}}\\right)\\right]  \\\\\n&=\\mathbb{E}_{y^{\\prime}\\thicksim\\mu}[\\log(e^{r(y)}/e^{r(y^{\\prime})})] \\\\\n&=\\mathbb{E}_{y'\\thicksim\\mu}[r(y)-r(y')] \\\\\n&=r(y)-\\underset{y'\\thicksim\\mu}{\\mathbb{E}}[r(y')]\n\\end{aligned}$$  \n\nPORLHFDPO  \n\nDPOPOBradley-Terry model  \n\n$$\\pi^*(y)\\propto\\pi_{\\mathrm{ref}}(y)\\exp\\left(\\beta^{-1}\\mathbb{E}_{y^{\\prime}\\thicksim\\mu}[\\Psi(p^*(y\\succ y^{\\prime}))]\\right)$$  \n\n(q)  \n\n{% asset_img curve.png log %}  \n\n(q)  \n\n  \n\n$$p^*(y\\succ y')=1$$  \n\nBT  \n\n$$(r(y)-r(y'))\\to+\\infty$$  \n\n $(r(y)-r(y'))\\to+\\infty$ PO  \n\n$$\\begin{aligned}\n&\\frac{\\pi^*(y_l)}{\\pi^*(y_w)}\\\\\n=&\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}\\left(\\beta^{-1}\\sum_{y^{\\prime}}[\\Psi(p(y_l\\succ y^{\\prime}))-\\Psi(p(y_w\\succ y^{\\prime}))]\\right)\\\\\n=&\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}(\\beta^{-1}\\sum_{y^{\\prime}}[r(y_l)-r(y_w)])\\\\\n=&\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}(\\beta^{-1}\\sum_{y^{\\prime}}[-\\infty])\\\\\n=&0\n\\end{aligned}$$  \n\n $\\beta$  $\\pi^*(y_l)=0$KLKLreward  \n\nRLHFrewardDPOreward  \n\n# IPO  \n\n(q)DPO(q)identity mappingIPO  \n\n$$\\max_\\pi\\quad\\mathbb{E}_{x\\thicksim\\rho}\\quad[p^*(y\\succ y'|x)]-\\beta D_{\\mathrm{KL}}(\\pi\\mid\\mid\\pi_{\\mathrm{ref}})$$  \n\nIPO  \n\n$$\\mathbb{E}_{(y_w,y_l,x)\\thicksim D}\\left(h_\\pi(y_w,y_l,x)-\\frac{\\beta^{-1}}2\\right)^2$$  \n\n$$h_\\pi(y,y',x)=\\log\\left(\\frac{\\pi(y|x)\\pi_{\\text{ref}}(y'|x)}{\\pi(y'|x)\\pi_{\\text{ref}}(y|x)}\\right)$$  \n\n#   \n\nPO/IPODPODPO  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n1A General Theoretical Paradigm to Understand Learning from Human Preferences https://arxiv.org/abs/2310.12036  \n","slug":"cs/nlp/2024/06/-IPO","published":1,"updated":"2024-06-06T14:52:00.000Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmq000g0p4kf42o7x9n","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>DPOODPOsimPO<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a></p>\n<p>A General Theoretical Paradigm to Understand Learning from Human\nPreferencesRLHFDPOgeneralPOPOIdentity-PO\n(IPO)</p>\n<h1 id=\"po\">PO</h1>\n<p>RLHF</p>\n<p><span class=\"math display\">\\[\\mathbb{E}_\\pi[r(x,y)]-\\beta\nD_{\\text{KL}}(\\pi\\mid\\mid\\pi_{\\text{ref}})\\]</span></p>\n<p>DPODPO</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\min_{\\pi}\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\Bigg[-\\log\\sigma\\Bigg(\\beta\\log\\Bigg(\\frac{\\pi(y_w|x)}{\\pi(y_l|x)}\\Bigg)-\\beta\\log\\left(\\frac{\\pi_{\\mathrm{ref}}(y_w|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}\\Bigg)\\Bigg)\\right]\\end{aligned}\\]</span></p>\n<p>IPOgeneralpreference\nprobabilitynon-decreasing function </p>\n<p><span class=\"math display\">\\[\\Psi:\\begin{bmatrix}0,1\\end{bmatrix}\\to\\mathbb{R}\\]</span></p>\n<p>-preference optimisation objective</p>\n<p><span class=\"math display\">\\[\\max_\\pi\\quad\\mathbb{E}_{x\\thicksim\\rho}\\quad[\\Psi(p^*(y\\succ\ny&#39;|x))]-\\beta\nD_{\\mathrm{KL}}(\\pi\\mid\\mid\\pi_{\\mathrm{ref}})\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\Psi(q)=\\log(q/(1-q))\\]</span></p>\n<p>Bradley-Terry model</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathbb{E}_{y&#39;\\thicksim\\mu}[\\Psi(p^*(y\\succ y&#39;))]&amp;\n=\\underset{y&#39;\\thicksim\\mu}{\\operatorname*{\\mathbb{E}}}\\left[\\Psi\\left(\\frac{e^{r(y)}}{e^{r(y)}+e^{r(y&#39;)}}\\right)\\right]  \\\\\n&amp;=\\mathbb{E}_{y^{\\prime}\\thicksim\\mu}[\\log(e^{r(y)}/e^{r(y^{\\prime})})]\n\\\\\n&amp;=\\mathbb{E}_{y&#39;\\thicksim\\mu}[r(y)-r(y&#39;)] \\\\\n&amp;=r(y)-\\underset{y&#39;\\thicksim\\mu}{\\mathbb{E}}[r(y&#39;)]\n\\end{aligned}\\]</span></p>\n<p>PORLHFDPO</p>\n<p>DPOPOBradley-Terry\nmodel</p>\n<p><span class=\"math display\">\\[\\pi^*(y)\\propto\\pi_{\\mathrm{ref}}(y)\\exp\\left(\\beta^{-1}\\mathbb{E}_{y^{\\prime}\\thicksim\\mu}[\\Psi(p^*(y\\succ\ny^{\\prime}))]\\right)\\]</span></p>\n<p>(q)</p>\n<img src=\"/4fe7b810/curve.png\" class title=\"log\">\n<p>(q)</p>\n<p></p>\n<p><span class=\"math display\">\\[p^*(y\\succ y&#39;)=1\\]</span></p>\n<p>BT</p>\n<p><span class=\"math display\">\\[(r(y)-r(y&#39;))\\to+\\infty\\]</span></p>\n<p> <span class=\"math inline\">\\((r(y)-r(y&#39;))\\to+\\infty\\)</span>\nPO</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\frac{\\pi^*(y_l)}{\\pi^*(y_w)}\\\\\n=&amp;\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}\\left(\\beta^{-1}\\sum_{y^{\\prime}}[\\Psi(p(y_l\\succ\ny^{\\prime}))-\\Psi(p(y_w\\succ y^{\\prime}))]\\right)\\\\\n=&amp;\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}(\\beta^{-1}\\sum_{y^{\\prime}}[r(y_l)-r(y_w)])\\\\\n=&amp;\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}(\\beta^{-1}\\sum_{y^{\\prime}}[-\\infty])\\\\\n=&amp;0\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span>\n <span class=\"math inline\">\\(\\pi^*(y_l)=0\\)</span>KLKLreward</p>\n<p>RLHFrewardDPOreward</p>\n<h1 id=\"ipo\">IPO</h1>\n<p>(q)DPO(q)identity\nmappingIPO</p>\n<p><span class=\"math display\">\\[\\max_\\pi\\quad\\mathbb{E}_{x\\thicksim\\rho}\\quad[p^*(y\\succ\ny&#39;|x)]-\\beta\nD_{\\mathrm{KL}}(\\pi\\mid\\mid\\pi_{\\mathrm{ref}})\\]</span></p>\n<p>IPO</p>\n<p><span class=\"math display\">\\[\\mathbb{E}_{(y_w,y_l,x)\\thicksim\nD}\\left(h_\\pi(y_w,y_l,x)-\\frac{\\beta^{-1}}2\\right)^2\\]</span></p>\n<p><span class=\"math display\">\\[h_\\pi(y,y&#39;,x)=\\log\\left(\\frac{\\pi(y|x)\\pi_{\\text{ref}}(y&#39;|x)}{\\pi(y&#39;|x)\\pi_{\\text{ref}}(y|x)}\\right)\\]</span></p>\n<h1 id=\"\"></h1>\n<p>PO/IPODPODPO</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1A General Theoretical Paradigm to Understand Learning from Human\nPreferences https://arxiv.org/abs/2310.12036</p>\n","length":3544,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>DPOODPOsimPO<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a></p>\n<p>A General Theoretical Paradigm to Understand Learning from Human\nPreferencesRLHFDPOgeneralPOPOIdentity-PO\n(IPO)</p>\n<h1 id=\"po\">PO</h1>\n<p>RLHF</p>\n<p><span class=\"math display\">\\[\\mathbb{E}_\\pi[r(x,y)]-\\beta\nD_{\\text{KL}}(\\pi\\mid\\mid\\pi_{\\text{ref}})\\]</span></p>\n<p>DPODPO</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\min_{\\pi}\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\Bigg[-\\log\\sigma\\Bigg(\\beta\\log\\Bigg(\\frac{\\pi(y_w|x)}{\\pi(y_l|x)}\\Bigg)-\\beta\\log\\left(\\frac{\\pi_{\\mathrm{ref}}(y_w|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}\\Bigg)\\Bigg)\\right]\\end{aligned}\\]</span></p>\n<p>IPOgeneralpreference\nprobabilitynon-decreasing function </p>\n<p><span class=\"math display\">\\[\\Psi:\\begin{bmatrix}0,1\\end{bmatrix}\\to\\mathbb{R}\\]</span></p>\n<p>-preference optimisation objective</p>\n<p><span class=\"math display\">\\[\\max_\\pi\\quad\\mathbb{E}_{x\\thicksim\\rho}\\quad[\\Psi(p^*(y\\succ\ny&#39;|x))]-\\beta\nD_{\\mathrm{KL}}(\\pi\\mid\\mid\\pi_{\\mathrm{ref}})\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\Psi(q)=\\log(q/(1-q))\\]</span></p>\n<p>Bradley-Terry model</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathbb{E}_{y&#39;\\thicksim\\mu}[\\Psi(p^*(y\\succ y&#39;))]&amp;\n=\\underset{y&#39;\\thicksim\\mu}{\\operatorname*{\\mathbb{E}}}\\left[\\Psi\\left(\\frac{e^{r(y)}}{e^{r(y)}+e^{r(y&#39;)}}\\right)\\right]  \\\\\n&amp;=\\mathbb{E}_{y^{\\prime}\\thicksim\\mu}[\\log(e^{r(y)}/e^{r(y^{\\prime})})]\n\\\\\n&amp;=\\mathbb{E}_{y&#39;\\thicksim\\mu}[r(y)-r(y&#39;)] \\\\\n&amp;=r(y)-\\underset{y&#39;\\thicksim\\mu}{\\mathbb{E}}[r(y&#39;)]\n\\end{aligned}\\]</span></p>\n<p>PORLHFDPO</p>\n<p>DPOPOBradley-Terry\nmodel</p>\n<p><span class=\"math display\">\\[\\pi^*(y)\\propto\\pi_{\\mathrm{ref}}(y)\\exp\\left(\\beta^{-1}\\mathbb{E}_{y^{\\prime}\\thicksim\\mu}[\\Psi(p^*(y\\succ\ny^{\\prime}))]\\right)\\]</span></p>\n<p>(q)</p>\n<img src=\"/4fe7b810/curve.png\" class title=\"log\">\n<p>(q)</p>\n<p></p>\n<p><span class=\"math display\">\\[p^*(y\\succ y&#39;)=1\\]</span></p>\n<p>BT</p>\n<p><span class=\"math display\">\\[(r(y)-r(y&#39;))\\to+\\infty\\]</span></p>\n<p> <span class=\"math inline\">\\((r(y)-r(y&#39;))\\to+\\infty\\)</span>\nPO</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\frac{\\pi^*(y_l)}{\\pi^*(y_w)}\\\\\n=&amp;\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}\\left(\\beta^{-1}\\sum_{y^{\\prime}}[\\Psi(p(y_l\\succ\ny^{\\prime}))-\\Psi(p(y_w\\succ y^{\\prime}))]\\right)\\\\\n=&amp;\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}(\\beta^{-1}\\sum_{y^{\\prime}}[r(y_l)-r(y_w)])\\\\\n=&amp;\\frac{\\pi_{\\mathrm{ref}}(y_l)}{\\pi_{\\mathrm{ref}}(y_w)}\\mathrm{exp}(\\beta^{-1}\\sum_{y^{\\prime}}[-\\infty])\\\\\n=&amp;0\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span>\n <span class=\"math inline\">\\(\\pi^*(y_l)=0\\)</span>KLKLreward</p>\n<p>RLHFrewardDPOreward</p>\n<h1 id=\"ipo\">IPO</h1>\n<p>(q)DPO(q)identity\nmappingIPO</p>\n<p><span class=\"math display\">\\[\\max_\\pi\\quad\\mathbb{E}_{x\\thicksim\\rho}\\quad[p^*(y\\succ\ny&#39;|x)]-\\beta\nD_{\\mathrm{KL}}(\\pi\\mid\\mid\\pi_{\\mathrm{ref}})\\]</span></p>\n<p>IPO</p>\n<p><span class=\"math display\">\\[\\mathbb{E}_{(y_w,y_l,x)\\thicksim\nD}\\left(h_\\pi(y_w,y_l,x)-\\frac{\\beta^{-1}}2\\right)^2\\]</span></p>\n<p><span class=\"math display\">\\[h_\\pi(y,y&#39;,x)=\\log\\left(\\frac{\\pi(y|x)\\pi_{\\text{ref}}(y&#39;|x)}{\\pi(y&#39;|x)\\pi_{\\text{ref}}(y|x)}\\right)\\]</span></p>\n<h1 id=\"\"></h1>\n<p>PO/IPODPODPO</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1A General Theoretical Paradigm to Understand Learning from Human\nPreferences https://arxiv.org/abs/2310.12036</p>\n"},{"title":"(7)","abbrlink":"dd614e12","date":"2024-06-12T14:13:46.000Z","_content":"\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~  \n\n~\n\n***  \n\n# 1.MoE  \n\nMoEexpertexpert  \n\nrouting collapse  \n\n  \n\nlossloss  \n\n# 2.BertMLM[Mask] token[Mask] tokenattention mask  \n\nattention mask[Mask] token  \n\n[Mask] tokenmodeling  \n\n\n# 3.warmup  \n\nlosslearning ratestep  \n\nResNet  \n\n  \n\nstep  \n\n# 4.RobertaBert  \n\n1. BertMLMmasktokenepochepochsamplemaskRobertadynamic maskmaskepochmask  \n\n2. BertMLMNSPRobertaNSPNSP  \n\n3. Robertabatch size  \n\n4. Robertastep  \n\n5. BertWordPieceRobertaBBPE  \n\n6. GoogleWWMmaskmaskRoberta-WWM  \n\n# 5.LoRA  \n\nLoRAABAB0LoRALoRAAB0  \n\n{% asset_img lora.png lora %}  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  ","source":"_posts/cs/nlp/2024/06/-7.md","raw":"---\ntitle: (7)\ntags:\n  - NLP\n  - LLM\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: dd614e12\ndate: 2024-06-12 22:13:46\n---\n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~  \n\n~\n\n***  \n\n# 1.MoE  \n\nMoEexpertexpert  \n\nrouting collapse  \n\n  \n\nlossloss  \n\n# 2.BertMLM[Mask] token[Mask] tokenattention mask  \n\nattention mask[Mask] token  \n\n[Mask] tokenmodeling  \n\n\n# 3.warmup  \n\nlosslearning ratestep  \n\nResNet  \n\n  \n\nstep  \n\n# 4.RobertaBert  \n\n1. BertMLMmasktokenepochepochsamplemaskRobertadynamic maskmaskepochmask  \n\n2. BertMLMNSPRobertaNSPNSP  \n\n3. Robertabatch size  \n\n4. Robertastep  \n\n5. BertWordPieceRobertaBBPE  \n\n6. GoogleWWMmaskmaskRoberta-WWM  \n\n# 5.LoRA  \n\nLoRAABAB0LoRALoRAAB0  \n\n{% asset_img lora.png lora %}  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  ","slug":"cs/nlp/2024/06/-7","published":1,"updated":"2024-06-12T14:21:01.594Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmq000h0p4kf1bk55ty","content":"<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM~</p>\n<p>~</p>\n<hr>\n<h1 id=\"moe\">1.MoE</h1>\n<p>MoEexpertexpert</p>\n<p>routing\ncollapse</p>\n<p></p>\n<p>lossloss</p>\n<h1 id=\"bertmlmmask-tokenmask-tokenattention-mask\">2.BertMLM[Mask]\ntoken[Mask]\ntokenattention mask</h1>\n<p>attention\nmask[Mask]\ntoken</p>\n<p>[Mask]\ntokenmodeling</p>\n<h1 id=\"warmup\">3.warmup</h1>\n<p>losslearning\nratestep</p>\n<p>ResNet</p>\n<p></p>\n<p>step</p>\n<h1 id=\"robertabert\">4.RobertaBert</h1>\n<ol type=\"1\">\n<li><p>BertMLMmasktokenepochepochsamplemaskRobertadynamic\nmaskmaskepochmask</p></li>\n<li><p>BertMLMNSPRobertaNSPNSP</p></li>\n<li><p>Robertabatch\nsize</p></li>\n<li><p>Robertastep</p></li>\n<li><p>BertWordPieceRobertaBBPE</p></li>\n<li><p>GoogleWWMmaskmaskRoberta-WWM</p></li>\n</ol>\n<h1 id=\"lora\">5.LoRA</h1>\n<p>LoRAABAB0LoRALoRAAB0</p>\n<img src=\"/dd614e12/lora.png\" class title=\"lora\">\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n","length":2131,"excerpt":"","more":"<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM~</p>\n<p>~</p>\n<hr>\n<h1 id=\"moe\">1.MoE</h1>\n<p>MoEexpertexpert</p>\n<p>routing\ncollapse</p>\n<p></p>\n<p>lossloss</p>\n<h1 id=\"bertmlmmask-tokenmask-tokenattention-mask\">2.BertMLM[Mask]\ntoken[Mask]\ntokenattention mask</h1>\n<p>attention\nmask[Mask]\ntoken</p>\n<p>[Mask]\ntokenmodeling</p>\n<h1 id=\"warmup\">3.warmup</h1>\n<p>losslearning\nratestep</p>\n<p>ResNet</p>\n<p></p>\n<p>step</p>\n<h1 id=\"robertabert\">4.RobertaBert</h1>\n<ol type=\"1\">\n<li><p>BertMLMmasktokenepochepochsamplemaskRobertadynamic\nmaskmaskepochmask</p></li>\n<li><p>BertMLMNSPRobertaNSPNSP</p></li>\n<li><p>Robertabatch\nsize</p></li>\n<li><p>Robertastep</p></li>\n<li><p>BertWordPieceRobertaBBPE</p></li>\n<li><p>GoogleWWMmaskmaskRoberta-WWM</p></li>\n</ol>\n<h1 id=\"lora\">5.LoRA</h1>\n<p>LoRAABAB0LoRALoRAAB0</p>\n<img src=\"/dd614e12/lora.png\" class title=\"lora\">\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n"},{"title":"-MEDUSA","abbrlink":"7bbe2df6","date":"2024-06-11T14:13:04.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\nspeculative decoding[-](http://www.linsight.cn/f5c015c.html)  \n\nMEDUSAMEDUSA2.2~2.8  \n\n#   \n\ntokenGPUmemory-bandwidth-bound  \n\nincreasing the arithmetic intensity/reducing the number of decoding steps  \n\n  \n- draft modeldraftdistribution shift  \n-   \n\nMEDUSA  \n\n# MEDUSA  \n\nMEDUSA  \n- tokenMEDUSAtoken  \n- tokenMEDUSAtree attention  \n- typical acceptance  \n\nMEDUSApipeline  \n\n{% asset_img intro.png introduction %}  \n\nMEDUSA  \n- MEDUSA-1backboneQLoRAmemory  \n- MEDUSA-2MEDUSAMEDUSA-1MEDUSA-2MEDUSAMEDUSA-2BaseSFT  \n\nSFTavailableSFTRLHFself-distillationMEDUSA head  \n\n# MEDUSA HEADS  \n\nMEDUSAtoken  \n\nhidden state $t$  $h_{t}$ $K$  $k$  $t+k+1$ token $k$  $1$ ~ $K$ $t+1$  $k=0$  \n\n $k$ vocabulary $p_t^{(k)}$  \n\n$$\\begin{aligned}p_t^{(k)}=\\text{softmax}\\left(W_2^{(k)}\\cdot\\left(\\text{SiLU}(W_1^{(k)}\\cdot h_t)+h_t\\right)\\right),\\\\\\mathrm{where~}W_2^{(k)}\\in\\mathbb{R}^{d\\times V},W_1^{(k)}\\in\\mathbb{R}^{d\\times d}.\\end{aligned}$$  \n\n$d$ hidden state$V$ FFN  \n\n $W_2^{(k)}$  $W_1^{(k)}$ 0  \n\n $K$ draft modelMEDUSAdistribution shift  \n\n# TREE ATTENTION  \n\n## Cartesian product  \n\n $K+1$ token  \n\ndraft model  \n\ntokenacceleration ratedecoding steptokenMEDUSAtree attention  \n\n $k$ token $s_k$ Cartesian producttree attention  \n\ntree attention  \n\n{% asset_img tree_attention.png tree attention %}  \n\nmaskbatch sizetoken  \n\n## tree attention  \n\nCartesian product  \n\ntoken  \n\ntree attention  \n\ncalibration datasetAlpaca-eval datasettoken $k$  $i$ token $a_k^{(i)}$  \n\ntoken $[i_1,i_2,\\cdots,i_k]$  $\\prod_{j=1}^ka_j^{(i_j)}$  \n\ntreenodeleaf nodetree attentiontoken $I$ expectation of acceptance length  \n\n$$\\sum_{[i_1,i_2,\\cdots,i_k]\\in I}\\prod_{j=1}^ka_j^{(i_j)}$$  \n\ntreetreeexpectation of acceptance lengthacceleration rate  \n\ntreetoken  \n\n{% asset_img construct_tree.png tree attention %}  \n\n#   \n\nMEDUSA  \n\n##   \n\nMEDUSA-1  \n\n $k$ loss  \n\n$$\\mathcal{L}_k=-\\log p_t^{(k)}(y_{t+k+1})$$  \n\nloss  \n\n$$\\mathcal{L}_{\\text{MEDUSA-l}}=\\sum_{k=1}^K-\\lambda_k\\log p_t^{(k)}(y_{t+k+1})$$  \n\n $\\lambda_{k}$  $k$ loss  \n\n$\\lambda_{k}=0.8^{k}$  \n\nQLoRA  \n\nMEDUSA-1MEDUSAMEDUSA-2  \n\nMEDUSA-2acceleration rate  \n\n1Combined loss  \n\nnext-token predictionlossloss  \n\n$$\\mathcal{L}_{\\text{MEDUSA-}2}=\\mathcal{L}_{\\text{LM}}+\\lambda_0\\mathcal{L}_{\\text{MEDUSA-}1}$$  \n\n$$\\mathcal{L}_{\\text{LM}}=-\\log p_t^{(0)}(y_{t+1})$$  \n\n $\\lambda_0=0.2$self-distillation$\\lambda_0=0.01$  \n\n2Differential learning rates  \n\n42e-35e-4  \n\n3Heads warmup  \n\nloss  \n\ntwo-stage trainingMEDUSA-1MEDUSA-2 $\\lambda_0$ two-stage training $\\lambda_0$   \n\n## SELF-DISTILLATION  \n\nSFTRLHFSFT  \n\nself-distillationMEDUSA  \n\ntarget modeldomainpromptself-talk  \n\nMEDUSA-1MEDUSA-2  \n \nMEDUSAMEDUSA-2ground truth  \n\n$$\\mathcal{L}_{\\text{LM-distill}}=KL(p_{\\text{original},t}^{(0)}||p_t^{(0)})$$  \n\n# TYPICAL ACCEPTANCE  \n\ntemperaturetemperaturedraft modeltokentoken  \n\ntemperaturecreativity  \n\nMEDUSAmatchtypical  \n\ncontext $x_1,x_2,\\cdots,x_n$ $(x_{n+1},x_{n+2},\\cdots,x_{n+K+1})$token  \n\n$$\\begin{aligned}p_{\\text{original}}(x_{n+k}|x_1,x_2,\\cdots,x_{n+k-1})&>\\\\\\min\\left(\\epsilon,\\delta\\exp\\left(-H(p_{\\text{original}}(\\cdot|x_1,x_2,\\cdots,x_{n+k-1})))\\right)\\right),\\end{aligned}$$  \n\n $H(\\cdot)$ entropy function$\\epsilon,\\delta$ hard thresholdentropy-dependent threshold  \n\nthreshold1$\\epsilon$ token2tokenentropyreasonable $\\delta$ exp(entropy)token  \n\ntemperatrue0token0tokentoken  \n\n  \n\n#   \n\n## CONFIGURATION OF TREE ATTENTION  \n\ntree attentiontree attention  \n\n{% asset_img tree_attention_exp.png  %}  \n\ntree attentionacceleration rate  \n\ntokentoken  \n\n## THRESHOLDS OF TYPICAL ACCEPTANCE  \n\n $\\epsilon $ acceleration rate  \n\n{% asset_img threshold.png  %}  \n\n##   \n\n  \n\n{% asset_img speed.png  %}  \n\n#   \n\nMEDUSAtree attentiontypical acceptance  \n\nMEDUSA  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n1MEDUSA: Simple LLM Inference Acceleration Framework with Multiple\nDecoding Heads https://arxiv.org/abs/2401.10774  \n","source":"_posts/cs/nlp/2024/06/-MEDUSA.md","raw":"---\ntitle: -MEDUSA\nabbrlink: 7bbe2df6\ndate: 2024-06-11 22:13:04\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\nspeculative decoding[-](http://www.linsight.cn/f5c015c.html)  \n\nMEDUSAMEDUSA2.2~2.8  \n\n#   \n\ntokenGPUmemory-bandwidth-bound  \n\nincreasing the arithmetic intensity/reducing the number of decoding steps  \n\n  \n- draft modeldraftdistribution shift  \n-   \n\nMEDUSA  \n\n# MEDUSA  \n\nMEDUSA  \n- tokenMEDUSAtoken  \n- tokenMEDUSAtree attention  \n- typical acceptance  \n\nMEDUSApipeline  \n\n{% asset_img intro.png introduction %}  \n\nMEDUSA  \n- MEDUSA-1backboneQLoRAmemory  \n- MEDUSA-2MEDUSAMEDUSA-1MEDUSA-2MEDUSAMEDUSA-2BaseSFT  \n\nSFTavailableSFTRLHFself-distillationMEDUSA head  \n\n# MEDUSA HEADS  \n\nMEDUSAtoken  \n\nhidden state $t$  $h_{t}$ $K$  $k$  $t+k+1$ token $k$  $1$ ~ $K$ $t+1$  $k=0$  \n\n $k$ vocabulary $p_t^{(k)}$  \n\n$$\\begin{aligned}p_t^{(k)}=\\text{softmax}\\left(W_2^{(k)}\\cdot\\left(\\text{SiLU}(W_1^{(k)}\\cdot h_t)+h_t\\right)\\right),\\\\\\mathrm{where~}W_2^{(k)}\\in\\mathbb{R}^{d\\times V},W_1^{(k)}\\in\\mathbb{R}^{d\\times d}.\\end{aligned}$$  \n\n$d$ hidden state$V$ FFN  \n\n $W_2^{(k)}$  $W_1^{(k)}$ 0  \n\n $K$ draft modelMEDUSAdistribution shift  \n\n# TREE ATTENTION  \n\n## Cartesian product  \n\n $K+1$ token  \n\ndraft model  \n\ntokenacceleration ratedecoding steptokenMEDUSAtree attention  \n\n $k$ token $s_k$ Cartesian producttree attention  \n\ntree attention  \n\n{% asset_img tree_attention.png tree attention %}  \n\nmaskbatch sizetoken  \n\n## tree attention  \n\nCartesian product  \n\ntoken  \n\ntree attention  \n\ncalibration datasetAlpaca-eval datasettoken $k$  $i$ token $a_k^{(i)}$  \n\ntoken $[i_1,i_2,\\cdots,i_k]$  $\\prod_{j=1}^ka_j^{(i_j)}$  \n\ntreenodeleaf nodetree attentiontoken $I$ expectation of acceptance length  \n\n$$\\sum_{[i_1,i_2,\\cdots,i_k]\\in I}\\prod_{j=1}^ka_j^{(i_j)}$$  \n\ntreetreeexpectation of acceptance lengthacceleration rate  \n\ntreetoken  \n\n{% asset_img construct_tree.png tree attention %}  \n\n#   \n\nMEDUSA  \n\n##   \n\nMEDUSA-1  \n\n $k$ loss  \n\n$$\\mathcal{L}_k=-\\log p_t^{(k)}(y_{t+k+1})$$  \n\nloss  \n\n$$\\mathcal{L}_{\\text{MEDUSA-l}}=\\sum_{k=1}^K-\\lambda_k\\log p_t^{(k)}(y_{t+k+1})$$  \n\n $\\lambda_{k}$  $k$ loss  \n\n$\\lambda_{k}=0.8^{k}$  \n\nQLoRA  \n\nMEDUSA-1MEDUSAMEDUSA-2  \n\nMEDUSA-2acceleration rate  \n\n1Combined loss  \n\nnext-token predictionlossloss  \n\n$$\\mathcal{L}_{\\text{MEDUSA-}2}=\\mathcal{L}_{\\text{LM}}+\\lambda_0\\mathcal{L}_{\\text{MEDUSA-}1}$$  \n\n$$\\mathcal{L}_{\\text{LM}}=-\\log p_t^{(0)}(y_{t+1})$$  \n\n $\\lambda_0=0.2$self-distillation$\\lambda_0=0.01$  \n\n2Differential learning rates  \n\n42e-35e-4  \n\n3Heads warmup  \n\nloss  \n\ntwo-stage trainingMEDUSA-1MEDUSA-2 $\\lambda_0$ two-stage training $\\lambda_0$   \n\n## SELF-DISTILLATION  \n\nSFTRLHFSFT  \n\nself-distillationMEDUSA  \n\ntarget modeldomainpromptself-talk  \n\nMEDUSA-1MEDUSA-2  \n \nMEDUSAMEDUSA-2ground truth  \n\n$$\\mathcal{L}_{\\text{LM-distill}}=KL(p_{\\text{original},t}^{(0)}||p_t^{(0)})$$  \n\n# TYPICAL ACCEPTANCE  \n\ntemperaturetemperaturedraft modeltokentoken  \n\ntemperaturecreativity  \n\nMEDUSAmatchtypical  \n\ncontext $x_1,x_2,\\cdots,x_n$ $(x_{n+1},x_{n+2},\\cdots,x_{n+K+1})$token  \n\n$$\\begin{aligned}p_{\\text{original}}(x_{n+k}|x_1,x_2,\\cdots,x_{n+k-1})&>\\\\\\min\\left(\\epsilon,\\delta\\exp\\left(-H(p_{\\text{original}}(\\cdot|x_1,x_2,\\cdots,x_{n+k-1})))\\right)\\right),\\end{aligned}$$  \n\n $H(\\cdot)$ entropy function$\\epsilon,\\delta$ hard thresholdentropy-dependent threshold  \n\nthreshold1$\\epsilon$ token2tokenentropyreasonable $\\delta$ exp(entropy)token  \n\ntemperatrue0token0tokentoken  \n\n  \n\n#   \n\n## CONFIGURATION OF TREE ATTENTION  \n\ntree attentiontree attention  \n\n{% asset_img tree_attention_exp.png  %}  \n\ntree attentionacceleration rate  \n\ntokentoken  \n\n## THRESHOLDS OF TYPICAL ACCEPTANCE  \n\n $\\epsilon $ acceleration rate  \n\n{% asset_img threshold.png  %}  \n\n##   \n\n  \n\n{% asset_img speed.png  %}  \n\n#   \n\nMEDUSAtree attentiontypical acceptance  \n\nMEDUSA  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n1MEDUSA: Simple LLM Inference Acceleration Framework with Multiple\nDecoding Heads https://arxiv.org/abs/2401.10774  \n","slug":"cs/nlp/2024/06/-MEDUSA","published":1,"updated":"2024-06-15T08:19:24.061Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmr000k0p4k1ioxckic","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>speculative decoding<a href=\"http://www.linsight.cn/f5c015c.html\">-</a></p>\n<p>MEDUSAMEDUSA2.2~2.8</p>\n<h1 id=\"\"></h1>\n<p>tokenGPUmemory-bandwidth-bound</p>\n<p>increasing the arithmetic\nintensity/reducing\nthe number of decoding steps</p>\n<p><br>\n- draft modeldraftdistribution\nshift<br>\n- </p>\n<p>MEDUSA</p>\n<h1 id=\"medusa\">MEDUSA</h1>\n<p>MEDUSA<br>\n-\ntokenMEDUSAtoken<br>\n-\ntokenMEDUSAtree\nattention<br>\n- typical acceptance</p>\n<p>MEDUSApipeline</p>\n<img src=\"/7bbe2df6/intro.png\" class title=\"introduction\">\n<p>MEDUSA<br>\n-\nMEDUSA-1backboneQLoRAmemory<br>\n-\nMEDUSA-2MEDUSAMEDUSA-1MEDUSA-2MEDUSAMEDUSA-2BaseSFT</p>\n<p>SFTavailableSFTRLHFself-distillationMEDUSA\nhead</p>\n<h1 id=\"medusa-heads\">MEDUSA HEADS</h1>\n<p>MEDUSAtoken</p>\n<p>hidden state <span class=\"math inline\">\\(t\\)</span>  <span class=\"math inline\">\\(h_{t}\\)</span> <span class=\"math inline\">\\(K\\)</span>  <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(t+k+1\\)</span> token <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(1\\)</span> ~ <span class=\"math inline\">\\(K\\)</span>\n<span class=\"math inline\">\\(t+1\\)</span>  <span class=\"math inline\">\\(k=0\\)</span></p>\n<p> <span class=\"math inline\">\\(k\\)</span>\nvocabulary <span class=\"math inline\">\\(p_t^{(k)}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}p_t^{(k)}=\\text{softmax}\\left(W_2^{(k)}\\cdot\\left(\\text{SiLU}(W_1^{(k)}\\cdot\nh_t)+h_t\\right)\\right),\\\\\\mathrm{where~}W_2^{(k)}\\in\\mathbb{R}^{d\\times\nV},W_1^{(k)}\\in\\mathbb{R}^{d\\times d}.\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(d\\)</span> hidden\nstate<span class=\"math inline\">\\(V\\)</span>\nFFN</p>\n<p> <span class=\"math inline\">\\(W_2^{(k)}\\)</span>\n <span class=\"math inline\">\\(W_1^{(k)}\\)</span>\n0</p>\n<p> <span class=\"math inline\">\\(K\\)</span>\ndraft\nmodelMEDUSAdistribution shift</p>\n<h1 id=\"tree-attention\">TREE ATTENTION</h1>\n<h2 id=\"cartesian-product\">Cartesian product</h2>\n<p> <span class=\"math inline\">\\(K+1\\)</span> token</p>\n<p>draft\nmodel</p>\n<p>tokenacceleration\nratedecoding\nsteptokenMEDUSAtree\nattention</p>\n<p> <span class=\"math inline\">\\(k\\)</span>\ntoken <span class=\"math inline\">\\(s_k\\)</span>\nCartesian\nproducttree\nattention</p>\n<p>tree attention</p>\n<img src=\"/7bbe2df6/tree_attention.png\" class title=\"tree attention\">\n<p>maskbatch\nsizetoken</p>\n<h2 id=\"tree-attention\">tree attention</h2>\n<p>Cartesian\nproduct</p>\n<p>token</p>\n<p>tree\nattention</p>\n<p>calibration datasetAlpaca-eval\ndatasettoken <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(i\\)</span> token <span class=\"math inline\">\\(a_k^{(i)}\\)</span></p>\n<p>token <span class=\"math inline\">\\([i_1,i_2,\\cdots,i_k]\\)</span>\n <span class=\"math inline\">\\(\\prod_{j=1}^ka_j^{(i_j)}\\)</span></p>\n<p>treenodeleaf\nnodetree attentiontoken\n<span class=\"math inline\">\\(I\\)</span>\nexpectation of acceptance\nlength</p>\n<p><span class=\"math display\">\\[\\sum_{[i_1,i_2,\\cdots,i_k]\\in\nI}\\prod_{j=1}^ka_j^{(i_j)}\\]</span></p>\n<p>treetreeexpectation\nof acceptance lengthacceleration rate</p>\n<p>treetoken</p>\n<img src=\"/7bbe2df6/construct_tree.png\" class title=\"tree attention\">\n<h1 id=\"\"></h1>\n<p>MEDUSA</p>\n<h2 id=\"\"></h2>\n<p>MEDUSA-1</p>\n<p> <span class=\"math inline\">\\(k\\)</span>\nloss</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_k=-\\log\np_t^{(k)}(y_{t+k+1})\\]</span></p>\n<p>loss</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{MEDUSA-l}}=\\sum_{k=1}^K-\\lambda_k\\log\np_t^{(k)}(y_{t+k+1})\\]</span></p>\n<p> <span class=\"math inline\">\\(\\lambda_{k}\\)</span>\n <span class=\"math inline\">\\(k\\)</span>\nloss</p>\n<p><span class=\"math inline\">\\(\\lambda_{k}=0.8^{k}\\)</span></p>\n<p>QLoRA</p>\n<p>MEDUSA-1MEDUSAMEDUSA-2</p>\n<p>MEDUSA-2acceleration\nrate</p>\n<p>1Combined loss</p>\n<p>next-token\npredictionlossloss</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{MEDUSA-}2}=\\mathcal{L}_{\\text{LM}}+\\lambda_0\\mathcal{L}_{\\text{MEDUSA-}1}\\]</span></p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{LM}}=-\\log\np_t^{(0)}(y_{t+1})\\]</span></p>\n<p> <span class=\"math inline\">\\(\\lambda_0=0.2\\)</span>self-distillation<span class=\"math inline\">\\(\\lambda_0=0.01\\)</span></p>\n<p>2Differential learning rates</p>\n<p>42e-35e-4</p>\n<p>3Heads warmup</p>\n<p>loss</p>\n<p>two-stage\ntrainingMEDUSA-1MEDUSA-2\n<span class=\"math inline\">\\(\\lambda_0\\)</span>\ntwo-stage training <span class=\"math inline\">\\(\\lambda_0\\)</span> </p>\n<h2 id=\"self-distillation\">SELF-DISTILLATION</h2>\n<p>SFTRLHFSFT</p>\n<p>self-distillationMEDUSA</p>\n<p>target\nmodeldomainpromptself-talk</p>\n<p>MEDUSA-1MEDUSA-2</p>\n<p>MEDUSAMEDUSA-2ground\ntruth</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{LM-distill}}=KL(p_{\\text{original},t}^{(0)}||p_t^{(0)})\\]</span></p>\n<h1 id=\"typical-acceptance\">TYPICAL ACCEPTANCE</h1>\n<p>temperaturetemperaturedraft\nmodeltokentoken</p>\n<p>temperaturecreativity</p>\n<p>MEDUSAmatchtypical</p>\n<p>context <span class=\"math inline\">\\(x_1,x_2,\\cdots,x_n\\)</span> <span class=\"math inline\">\\((x_{n+1},x_{n+2},\\cdots,x_{n+K+1})\\)</span>token</p>\n<p><span class=\"math display\">\\[\\begin{aligned}p_{\\text{original}}(x_{n+k}|x_1,x_2,\\cdots,x_{n+k-1})&amp;&gt;\\\\\\min\\left(\\epsilon,\\delta\\exp\\left(-H(p_{\\text{original}}(\\cdot|x_1,x_2,\\cdots,x_{n+k-1})))\\right)\\right),\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(H(\\cdot)\\)</span> entropy\nfunction<span class=\"math inline\">\\(\\epsilon,\\delta\\)</span>\nhard thresholdentropy-dependent threshold</p>\n<p>threshold1<span class=\"math inline\">\\(\\epsilon\\)</span>\ntoken2tokenentropyreasonable\n<span class=\"math inline\">\\(\\delta\\)</span>\nexp(entropy)token</p>\n<p>temperatrue0token0tokentoken</p>\n<p></p>\n<h1 id=\"\"></h1>\n<h2 id=\"configuration-of-tree-attention\">CONFIGURATION OF TREE\nATTENTION</h2>\n<p>tree attentiontree\nattention</p>\n<img src=\"/7bbe2df6/tree_attention_exp.png\" class title=\"\">\n<p>tree attentionacceleration rate</p>\n<p>tokentoken</p>\n<h2 id=\"thresholds-of-typical-acceptance\">THRESHOLDS OF TYPICAL\nACCEPTANCE</h2>\n<p> $$ acceleration\nrate</p>\n<img src=\"/7bbe2df6/threshold.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p></p>\n<img src=\"/7bbe2df6/speed.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<p>MEDUSAtree attentiontypical\nacceptance</p>\n<p>MEDUSA</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1MEDUSA: Simple LLM Inference Acceleration Framework with\nMultiple Decoding Heads https://arxiv.org/abs/2401.10774</p>\n","length":6717,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>speculative decoding<a href=\"http://www.linsight.cn/f5c015c.html\">-</a></p>\n<p>MEDUSAMEDUSA2.2~2.8</p>\n<h1 id=\"\"></h1>\n<p>tokenGPUmemory-bandwidth-bound</p>\n<p>increasing the arithmetic\nintensity/reducing\nthe number of decoding steps</p>\n<p><br>\n- draft modeldraftdistribution\nshift<br>\n- </p>\n<p>MEDUSA</p>\n<h1 id=\"medusa\">MEDUSA</h1>\n<p>MEDUSA<br>\n-\ntokenMEDUSAtoken<br>\n-\ntokenMEDUSAtree\nattention<br>\n- typical acceptance</p>\n<p>MEDUSApipeline</p>\n<img src=\"/7bbe2df6/intro.png\" class title=\"introduction\">\n<p>MEDUSA<br>\n-\nMEDUSA-1backboneQLoRAmemory<br>\n-\nMEDUSA-2MEDUSAMEDUSA-1MEDUSA-2MEDUSAMEDUSA-2BaseSFT</p>\n<p>SFTavailableSFTRLHFself-distillationMEDUSA\nhead</p>\n<h1 id=\"medusa-heads\">MEDUSA HEADS</h1>\n<p>MEDUSAtoken</p>\n<p>hidden state <span class=\"math inline\">\\(t\\)</span>  <span class=\"math inline\">\\(h_{t}\\)</span> <span class=\"math inline\">\\(K\\)</span>  <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(t+k+1\\)</span> token <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(1\\)</span> ~ <span class=\"math inline\">\\(K\\)</span>\n<span class=\"math inline\">\\(t+1\\)</span>  <span class=\"math inline\">\\(k=0\\)</span></p>\n<p> <span class=\"math inline\">\\(k\\)</span>\nvocabulary <span class=\"math inline\">\\(p_t^{(k)}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}p_t^{(k)}=\\text{softmax}\\left(W_2^{(k)}\\cdot\\left(\\text{SiLU}(W_1^{(k)}\\cdot\nh_t)+h_t\\right)\\right),\\\\\\mathrm{where~}W_2^{(k)}\\in\\mathbb{R}^{d\\times\nV},W_1^{(k)}\\in\\mathbb{R}^{d\\times d}.\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(d\\)</span> hidden\nstate<span class=\"math inline\">\\(V\\)</span>\nFFN</p>\n<p> <span class=\"math inline\">\\(W_2^{(k)}\\)</span>\n <span class=\"math inline\">\\(W_1^{(k)}\\)</span>\n0</p>\n<p> <span class=\"math inline\">\\(K\\)</span>\ndraft\nmodelMEDUSAdistribution shift</p>\n<h1 id=\"tree-attention\">TREE ATTENTION</h1>\n<h2 id=\"cartesian-product\">Cartesian product</h2>\n<p> <span class=\"math inline\">\\(K+1\\)</span> token</p>\n<p>draft\nmodel</p>\n<p>tokenacceleration\nratedecoding\nsteptokenMEDUSAtree\nattention</p>\n<p> <span class=\"math inline\">\\(k\\)</span>\ntoken <span class=\"math inline\">\\(s_k\\)</span>\nCartesian\nproducttree\nattention</p>\n<p>tree attention</p>\n<img src=\"/7bbe2df6/tree_attention.png\" class title=\"tree attention\">\n<p>maskbatch\nsizetoken</p>\n<h2 id=\"tree-attention\">tree attention</h2>\n<p>Cartesian\nproduct</p>\n<p>token</p>\n<p>tree\nattention</p>\n<p>calibration datasetAlpaca-eval\ndatasettoken <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(i\\)</span> token <span class=\"math inline\">\\(a_k^{(i)}\\)</span></p>\n<p>token <span class=\"math inline\">\\([i_1,i_2,\\cdots,i_k]\\)</span>\n <span class=\"math inline\">\\(\\prod_{j=1}^ka_j^{(i_j)}\\)</span></p>\n<p>treenodeleaf\nnodetree attentiontoken\n<span class=\"math inline\">\\(I\\)</span>\nexpectation of acceptance\nlength</p>\n<p><span class=\"math display\">\\[\\sum_{[i_1,i_2,\\cdots,i_k]\\in\nI}\\prod_{j=1}^ka_j^{(i_j)}\\]</span></p>\n<p>treetreeexpectation\nof acceptance lengthacceleration rate</p>\n<p>treetoken</p>\n<img src=\"/7bbe2df6/construct_tree.png\" class title=\"tree attention\">\n<h1 id=\"\"></h1>\n<p>MEDUSA</p>\n<h2 id=\"\"></h2>\n<p>MEDUSA-1</p>\n<p> <span class=\"math inline\">\\(k\\)</span>\nloss</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_k=-\\log\np_t^{(k)}(y_{t+k+1})\\]</span></p>\n<p>loss</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{MEDUSA-l}}=\\sum_{k=1}^K-\\lambda_k\\log\np_t^{(k)}(y_{t+k+1})\\]</span></p>\n<p> <span class=\"math inline\">\\(\\lambda_{k}\\)</span>\n <span class=\"math inline\">\\(k\\)</span>\nloss</p>\n<p><span class=\"math inline\">\\(\\lambda_{k}=0.8^{k}\\)</span></p>\n<p>QLoRA</p>\n<p>MEDUSA-1MEDUSAMEDUSA-2</p>\n<p>MEDUSA-2acceleration\nrate</p>\n<p>1Combined loss</p>\n<p>next-token\npredictionlossloss</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{MEDUSA-}2}=\\mathcal{L}_{\\text{LM}}+\\lambda_0\\mathcal{L}_{\\text{MEDUSA-}1}\\]</span></p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{LM}}=-\\log\np_t^{(0)}(y_{t+1})\\]</span></p>\n<p> <span class=\"math inline\">\\(\\lambda_0=0.2\\)</span>self-distillation<span class=\"math inline\">\\(\\lambda_0=0.01\\)</span></p>\n<p>2Differential learning rates</p>\n<p>42e-35e-4</p>\n<p>3Heads warmup</p>\n<p>loss</p>\n<p>two-stage\ntrainingMEDUSA-1MEDUSA-2\n<span class=\"math inline\">\\(\\lambda_0\\)</span>\ntwo-stage training <span class=\"math inline\">\\(\\lambda_0\\)</span> </p>\n<h2 id=\"self-distillation\">SELF-DISTILLATION</h2>\n<p>SFTRLHFSFT</p>\n<p>self-distillationMEDUSA</p>\n<p>target\nmodeldomainpromptself-talk</p>\n<p>MEDUSA-1MEDUSA-2</p>\n<p>MEDUSAMEDUSA-2ground\ntruth</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{LM-distill}}=KL(p_{\\text{original},t}^{(0)}||p_t^{(0)})\\]</span></p>\n<h1 id=\"typical-acceptance\">TYPICAL ACCEPTANCE</h1>\n<p>temperaturetemperaturedraft\nmodeltokentoken</p>\n<p>temperaturecreativity</p>\n<p>MEDUSAmatchtypical</p>\n<p>context <span class=\"math inline\">\\(x_1,x_2,\\cdots,x_n\\)</span> <span class=\"math inline\">\\((x_{n+1},x_{n+2},\\cdots,x_{n+K+1})\\)</span>token</p>\n<p><span class=\"math display\">\\[\\begin{aligned}p_{\\text{original}}(x_{n+k}|x_1,x_2,\\cdots,x_{n+k-1})&amp;&gt;\\\\\\min\\left(\\epsilon,\\delta\\exp\\left(-H(p_{\\text{original}}(\\cdot|x_1,x_2,\\cdots,x_{n+k-1})))\\right)\\right),\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(H(\\cdot)\\)</span> entropy\nfunction<span class=\"math inline\">\\(\\epsilon,\\delta\\)</span>\nhard thresholdentropy-dependent threshold</p>\n<p>threshold1<span class=\"math inline\">\\(\\epsilon\\)</span>\ntoken2tokenentropyreasonable\n<span class=\"math inline\">\\(\\delta\\)</span>\nexp(entropy)token</p>\n<p>temperatrue0token0tokentoken</p>\n<p></p>\n<h1 id=\"\"></h1>\n<h2 id=\"configuration-of-tree-attention\">CONFIGURATION OF TREE\nATTENTION</h2>\n<p>tree attentiontree\nattention</p>\n<img src=\"/7bbe2df6/tree_attention_exp.png\" class title=\"\">\n<p>tree attentionacceleration rate</p>\n<p>tokentoken</p>\n<h2 id=\"thresholds-of-typical-acceptance\">THRESHOLDS OF TYPICAL\nACCEPTANCE</h2>\n<p> $$ acceleration\nrate</p>\n<img src=\"/7bbe2df6/threshold.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p></p>\n<img src=\"/7bbe2df6/speed.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<p>MEDUSAtree attentiontypical\nacceptance</p>\n<p>MEDUSA</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1MEDUSA: Simple LLM Inference Acceleration Framework with\nMultiple Decoding Heads https://arxiv.org/abs/2401.10774</p>\n"},{"title":"10wJetMoE","abbrlink":"f3acf042","date":"2024-06-26T03:22:35.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nJetMoEMITPrincetonMoE8B2B  \n\nJetMoE10wJetMoEbenchmark  \n\nMoE [MoE](http://www.linsight.cn/44e38c1b.html)  \n\n#   \n\n##   \n\nDeepseek MoEMixtral 8x7BQwen-MoEJetMoEFFNSparsely-gated Mixtureof-ExpertsSMOEModuleformer: Learning modular large language models from uncurated dataattention  \n\n{% asset_img structure.png  %}  \n\nattentionMoA Mixture of Attention heads (MoA)Mixture of Attention Heads: Selecting Attention Heads Per Token  \n\nMoAFFNMoEattentionattention expertattention expert e4 $\\mathbf{R}^{D_{emb}\\times D_{att}}$  $\\mathbf{W}_q^e,\\mathbf{W}_k,\\mathbf{W}_v,\\mathbf{W}_o^e$ $D_{att}=H\\times D_{head}$Hattention expertattention headattention expert  \n\nattention expert $\\mathbf{W}_k$  $\\mathbf{W}_v$ attentionexpertattention expert $\\mathbf{W}_q^e$  $\\mathbf{W}_o^e$  \n\nvector x2kv  \n\n$$\\begin{aligned}\\mathbf{k}&=\\mathbf{W}_{k}\\mathbf{x}\\\\\\mathbf{v}&=\\mathbf{W}_{v}\\mathbf{x}\\end{aligned}$$  \n\ngating functionexpertattention expertattention  \n\n$$\\begin{aligned}&\\mathbf{q}_{e}=\\mathbf{W}_{q}^{e}\\mathbf{x}\\\\&\\mathbf{a}_{e}=\\mathrm{}\\left(\\mathbf{q}_{e},\\mathbf{k},\\mathbf{v}\\right)\\\\&\\mathbf{o}_{e}=\\mathbf{W}_{o}^{e}\\mathbf{a}\\end{aligned}$$  \n\nJetMoEFFNgatingtop-k gating MoE  \n\nJetMoE  \n\n{% asset_img model_param.png  %}  \n\n8experttoken2expert  \n\n##   \n\nSwitch Transformerfrequency-based auxiliary loss  \n\n$$loss_b=N\\sum_{i=1}^Nf_iP_i$$  \n\nNexpert$f_i$ expert itoken$P_i$ routerexpert i  \n\nST-MoEz-loss  \n\n$$loss_z=\\frac1B\\sum_{i=1}^B\\left(\\log\\sum_{j=1}^N\\exp(x_j^i)\\right)^2$$  \n\nxrouterlogitsBtoken  \n\nlossloss  \n\n$$loss=loss_{lm}+\\alpha loss_b+\\beta loss_z$$  \n\n $\\alpha=0.01$$\\beta=0.001$  \n\n#   \n\nJetMoE  \n\n  \n- RefinedWeb5Ttoken600B  \n- StarCoder86  \n- Dolma3T token  \n- The Pile825GB  \n- Proof-Pile-2OpenWebMathStackMathQAOpenAssistantxP3xCommitPackFT  \n\n  \n- OpenHermes 2.5  \n- UltraTextbooks  \n- UltraChat 200k  \n- TemplateGSMMagicoder-Evol-110KEvol-Code AlpacaCode-290k-ShareGPT  \n\n#   \n\nJetMoEMegatronpipeline parallelismexpert parallelism96H10030,000GPU hour1.25T token  \n\n  \n- AdamW  \n- maximum learning rate = 5e-4  \n- batch size = 4M  \n- sequence length = 4096  \n- learning rate schedule = WSDwarmup = 10B tokendecay = 250B token  \n\nMiniCPM\n- phase1warmup and stable learning rateRefinedWeb, Starcoder, The Pile, peS2o from Dolma, and OpenWebMath  \n- phase2:decay learning rate  \n\nphase1phase2  \n\n{% asset_img data1.png  %}  \n\n{% asset_img data2.png  %}  \n\n# Alignment  \n\nJetMoEDistilled Supervised Fine-TuningdSFTdSFTprompt  \n\nJetMoEZephyr-7b-betachat templateGPT-4ClaudeJetMoE  \n- UltraChat 200k  \n- Airoboros-3.2  \n- Code-Feedback  \n- Orca-math-word-problems-200k  \n- SystemChat  \n- Capybara  \n\n  \n- lr = 2e-5  \n- batch size = 128  \n- epoch = 3  \n\nSFTDistilled Direct Preference Optimization (dDPO)  \n\nUltraFeedbackpreference  \n\n  \n- lr = 5e-7  \n- batch size = 128  \n- epoch = 1  \n\n#   \n\nbenchmark  \n\n{% asset_img evaluation.png  %}  \n\n{% asset_img mtbench.png  %}  \n\n#   \n\nJetMoEMoE  \n\nattention expert  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1JetMoE: Reaching Llama2 Performance with 0.1M Dollars https://arxiv.org/abs/2404.07413  \n","source":"_posts/cs/nlp/2024/06/10wJetMoE.md","raw":"---\ntitle: 10wJetMoE\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - MoE\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: f3acf042\ndate: 2024-06-26 11:22:35\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nJetMoEMITPrincetonMoE8B2B  \n\nJetMoE10wJetMoEbenchmark  \n\nMoE [MoE](http://www.linsight.cn/44e38c1b.html)  \n\n#   \n\n##   \n\nDeepseek MoEMixtral 8x7BQwen-MoEJetMoEFFNSparsely-gated Mixtureof-ExpertsSMOEModuleformer: Learning modular large language models from uncurated dataattention  \n\n{% asset_img structure.png  %}  \n\nattentionMoA Mixture of Attention heads (MoA)Mixture of Attention Heads: Selecting Attention Heads Per Token  \n\nMoAFFNMoEattentionattention expertattention expert e4 $\\mathbf{R}^{D_{emb}\\times D_{att}}$  $\\mathbf{W}_q^e,\\mathbf{W}_k,\\mathbf{W}_v,\\mathbf{W}_o^e$ $D_{att}=H\\times D_{head}$Hattention expertattention headattention expert  \n\nattention expert $\\mathbf{W}_k$  $\\mathbf{W}_v$ attentionexpertattention expert $\\mathbf{W}_q^e$  $\\mathbf{W}_o^e$  \n\nvector x2kv  \n\n$$\\begin{aligned}\\mathbf{k}&=\\mathbf{W}_{k}\\mathbf{x}\\\\\\mathbf{v}&=\\mathbf{W}_{v}\\mathbf{x}\\end{aligned}$$  \n\ngating functionexpertattention expertattention  \n\n$$\\begin{aligned}&\\mathbf{q}_{e}=\\mathbf{W}_{q}^{e}\\mathbf{x}\\\\&\\mathbf{a}_{e}=\\mathrm{}\\left(\\mathbf{q}_{e},\\mathbf{k},\\mathbf{v}\\right)\\\\&\\mathbf{o}_{e}=\\mathbf{W}_{o}^{e}\\mathbf{a}\\end{aligned}$$  \n\nJetMoEFFNgatingtop-k gating MoE  \n\nJetMoE  \n\n{% asset_img model_param.png  %}  \n\n8experttoken2expert  \n\n##   \n\nSwitch Transformerfrequency-based auxiliary loss  \n\n$$loss_b=N\\sum_{i=1}^Nf_iP_i$$  \n\nNexpert$f_i$ expert itoken$P_i$ routerexpert i  \n\nST-MoEz-loss  \n\n$$loss_z=\\frac1B\\sum_{i=1}^B\\left(\\log\\sum_{j=1}^N\\exp(x_j^i)\\right)^2$$  \n\nxrouterlogitsBtoken  \n\nlossloss  \n\n$$loss=loss_{lm}+\\alpha loss_b+\\beta loss_z$$  \n\n $\\alpha=0.01$$\\beta=0.001$  \n\n#   \n\nJetMoE  \n\n  \n- RefinedWeb5Ttoken600B  \n- StarCoder86  \n- Dolma3T token  \n- The Pile825GB  \n- Proof-Pile-2OpenWebMathStackMathQAOpenAssistantxP3xCommitPackFT  \n\n  \n- OpenHermes 2.5  \n- UltraTextbooks  \n- UltraChat 200k  \n- TemplateGSMMagicoder-Evol-110KEvol-Code AlpacaCode-290k-ShareGPT  \n\n#   \n\nJetMoEMegatronpipeline parallelismexpert parallelism96H10030,000GPU hour1.25T token  \n\n  \n- AdamW  \n- maximum learning rate = 5e-4  \n- batch size = 4M  \n- sequence length = 4096  \n- learning rate schedule = WSDwarmup = 10B tokendecay = 250B token  \n\nMiniCPM\n- phase1warmup and stable learning rateRefinedWeb, Starcoder, The Pile, peS2o from Dolma, and OpenWebMath  \n- phase2:decay learning rate  \n\nphase1phase2  \n\n{% asset_img data1.png  %}  \n\n{% asset_img data2.png  %}  \n\n# Alignment  \n\nJetMoEDistilled Supervised Fine-TuningdSFTdSFTprompt  \n\nJetMoEZephyr-7b-betachat templateGPT-4ClaudeJetMoE  \n- UltraChat 200k  \n- Airoboros-3.2  \n- Code-Feedback  \n- Orca-math-word-problems-200k  \n- SystemChat  \n- Capybara  \n\n  \n- lr = 2e-5  \n- batch size = 128  \n- epoch = 3  \n\nSFTDistilled Direct Preference Optimization (dDPO)  \n\nUltraFeedbackpreference  \n\n  \n- lr = 5e-7  \n- batch size = 128  \n- epoch = 1  \n\n#   \n\nbenchmark  \n\n{% asset_img evaluation.png  %}  \n\n{% asset_img mtbench.png  %}  \n\n#   \n\nJetMoEMoE  \n\nattention expert  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1JetMoE: Reaching Llama2 Performance with 0.1M Dollars https://arxiv.org/abs/2404.07413  \n","slug":"cs/nlp/2024/06/10wJetMoE","published":1,"updated":"2024-06-26T12:49:15.240Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmr000l0p4k37uv5eye","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>JetMoEMITPrincetonMoE8B2B</p>\n<p>JetMoE10wJetMoEbenchmark</p>\n<p>MoE <a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a></p>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>Deepseek MoEMixtral\n8x7BQwen-MoEJetMoEFFNSparsely-gated\nMixtureof-ExpertsSMOEModuleformer: Learning\nmodular large language models from uncurated\ndataattention</p>\n<img src=\"/f3acf042/structure.png\" class title=\"\">\n<p>attentionMoA Mixture of Attention heads\n(MoA)Mixture of Attention Heads: Selecting Attention Heads Per\nToken</p>\n<p>MoAFFNMoEattentionattention\nexpertattention expert e4 <span class=\"math inline\">\\(\\mathbf{R}^{D_{emb}\\times D_{att}}\\)</span>\n <span class=\"math inline\">\\(\\mathbf{W}_q^e,\\mathbf{W}_k,\\mathbf{W}_v,\\mathbf{W}_o^e\\)</span>\n<span class=\"math inline\">\\(D_{att}=H\\times\nD_{head}\\)</span>Hattention expertattention\nheadattention expert</p>\n<p>attention expert <span class=\"math inline\">\\(\\mathbf{W}_k\\)</span>  <span class=\"math inline\">\\(\\mathbf{W}_v\\)</span>\nattentionexpertattention\nexpert <span class=\"math inline\">\\(\\mathbf{W}_q^e\\)</span> \n<span class=\"math inline\">\\(\\mathbf{W}_o^e\\)</span></p>\n<p>vector x2kv</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\mathbf{k}&amp;=\\mathbf{W}_{k}\\mathbf{x}\\\\\\mathbf{v}&amp;=\\mathbf{W}_{v}\\mathbf{x}\\end{aligned}\\]</span></p>\n<p>gating functionexpertattention\nexpertattention</p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;\\mathbf{q}_{e}=\\mathbf{W}_{q}^{e}\\mathbf{x}\\\\&amp;\\mathbf{a}_{e}=\\mathrm{}\\left(\\mathbf{q}_{e},\\mathbf{k},\\mathbf{v}\\right)\\\\&amp;\\mathbf{o}_{e}=\\mathbf{W}_{o}^{e}\\mathbf{a}\\end{aligned}\\]</span></p>\n<p>JetMoEFFNgatingtop-k gating\nMoE</p>\n<p>JetMoE</p>\n<img src=\"/f3acf042/model_param.png\" class title=\"\">\n<p>8experttoken2expert</p>\n<h2 id=\"\"></h2>\n<p>Switch Transformerfrequency-based auxiliary\nloss</p>\n<p><span class=\"math display\">\\[loss_b=N\\sum_{i=1}^Nf_iP_i\\]</span></p>\n<p>Nexpert<span class=\"math inline\">\\(f_i\\)</span>\nexpert itoken<span class=\"math inline\">\\(P_i\\)</span>\nrouterexpert i</p>\n<p>ST-MoEz-loss</p>\n<p><span class=\"math display\">\\[loss_z=\\frac1B\\sum_{i=1}^B\\left(\\log\\sum_{j=1}^N\\exp(x_j^i)\\right)^2\\]</span></p>\n<p>xrouterlogitsBtoken</p>\n<p>lossloss</p>\n<p><span class=\"math display\">\\[loss=loss_{lm}+\\alpha loss_b+\\beta\nloss_z\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha=0.01\\)</span><span class=\"math inline\">\\(\\beta=0.001\\)</span></p>\n<h1 id=\"\"></h1>\n<p>JetMoE</p>\n<p><br>\n- RefinedWeb5Ttoken600B<br>\n- StarCoder86<br>\n- Dolma3T token<br>\n- The Pile825GB<br>\n-\nProof-Pile-2OpenWebMathStackMathQAOpenAssistantxP3xCommitPackFT</p>\n<p><br>\n- OpenHermes 2.5<br>\n- UltraTextbooks<br>\n- UltraChat 200k<br>\n- TemplateGSMMagicoder-Evol-110KEvol-Code\nAlpacaCode-290k-ShareGPT</p>\n<h1 id=\"\"></h1>\n<p>JetMoEMegatronpipeline parallelismexpert\nparallelism96H10030,000GPU\nhour1.25T token</p>\n<p><br>\n- AdamW<br>\n- maximum learning rate = 5e-4<br>\n- batch size = 4M<br>\n- sequence length = 4096<br>\n- learning rate schedule = WSDwarmup = 10B tokendecay = 250B\ntoken</p>\n<p>MiniCPM - phase1warmup and stable\nlearning rateRefinedWeb, Starcoder, The Pile, peS2o\nfrom Dolma, and OpenWebMath<br>\n- phase2:decay learning rate</p>\n<p>phase1phase2</p>\n<img src=\"/f3acf042/data1.png\" class title=\"\">\n<img src=\"/f3acf042/data2.png\" class title=\"\">\n<h1 id=\"alignment\">Alignment</h1>\n<p>JetMoEDistilled Supervised\nFine-TuningdSFTdSFTprompt</p>\n<p>JetMoEZephyr-7b-betachat\ntemplateGPT-4ClaudeJetMoE<br>\n- UltraChat 200k<br>\n- Airoboros-3.2<br>\n- Code-Feedback<br>\n- Orca-math-word-problems-200k<br>\n- SystemChat<br>\n- Capybara</p>\n<p><br>\n- lr = 2e-5<br>\n- batch size = 128<br>\n- epoch = 3</p>\n<p>SFTDistilled Direct Preference Optimization\n(dDPO)</p>\n<p>UltraFeedbackpreference</p>\n<p><br>\n- lr = 5e-7<br>\n- batch size = 128<br>\n- epoch = 1</p>\n<h1 id=\"\"></h1>\n<p>benchmark</p>\n<img src=\"/f3acf042/evaluation.png\" class title=\"\">\n<img src=\"/f3acf042/mtbench.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<p>JetMoEMoE</p>\n<p>attention expert</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1JetMoE: Reaching Llama2 Performance with 0.1M Dollars\nhttps://arxiv.org/abs/2404.07413</p>\n","length":3801,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>JetMoEMITPrincetonMoE8B2B</p>\n<p>JetMoE10wJetMoEbenchmark</p>\n<p>MoE <a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a></p>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>Deepseek MoEMixtral\n8x7BQwen-MoEJetMoEFFNSparsely-gated\nMixtureof-ExpertsSMOEModuleformer: Learning\nmodular large language models from uncurated\ndataattention</p>\n<img src=\"/f3acf042/structure.png\" class title=\"\">\n<p>attentionMoA Mixture of Attention heads\n(MoA)Mixture of Attention Heads: Selecting Attention Heads Per\nToken</p>\n<p>MoAFFNMoEattentionattention\nexpertattention expert e4 <span class=\"math inline\">\\(\\mathbf{R}^{D_{emb}\\times D_{att}}\\)</span>\n <span class=\"math inline\">\\(\\mathbf{W}_q^e,\\mathbf{W}_k,\\mathbf{W}_v,\\mathbf{W}_o^e\\)</span>\n<span class=\"math inline\">\\(D_{att}=H\\times\nD_{head}\\)</span>Hattention expertattention\nheadattention expert</p>\n<p>attention expert <span class=\"math inline\">\\(\\mathbf{W}_k\\)</span>  <span class=\"math inline\">\\(\\mathbf{W}_v\\)</span>\nattentionexpertattention\nexpert <span class=\"math inline\">\\(\\mathbf{W}_q^e\\)</span> \n<span class=\"math inline\">\\(\\mathbf{W}_o^e\\)</span></p>\n<p>vector x2kv</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\mathbf{k}&amp;=\\mathbf{W}_{k}\\mathbf{x}\\\\\\mathbf{v}&amp;=\\mathbf{W}_{v}\\mathbf{x}\\end{aligned}\\]</span></p>\n<p>gating functionexpertattention\nexpertattention</p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;\\mathbf{q}_{e}=\\mathbf{W}_{q}^{e}\\mathbf{x}\\\\&amp;\\mathbf{a}_{e}=\\mathrm{}\\left(\\mathbf{q}_{e},\\mathbf{k},\\mathbf{v}\\right)\\\\&amp;\\mathbf{o}_{e}=\\mathbf{W}_{o}^{e}\\mathbf{a}\\end{aligned}\\]</span></p>\n<p>JetMoEFFNgatingtop-k gating\nMoE</p>\n<p>JetMoE</p>\n<img src=\"/f3acf042/model_param.png\" class title=\"\">\n<p>8experttoken2expert</p>\n<h2 id=\"\"></h2>\n<p>Switch Transformerfrequency-based auxiliary\nloss</p>\n<p><span class=\"math display\">\\[loss_b=N\\sum_{i=1}^Nf_iP_i\\]</span></p>\n<p>Nexpert<span class=\"math inline\">\\(f_i\\)</span>\nexpert itoken<span class=\"math inline\">\\(P_i\\)</span>\nrouterexpert i</p>\n<p>ST-MoEz-loss</p>\n<p><span class=\"math display\">\\[loss_z=\\frac1B\\sum_{i=1}^B\\left(\\log\\sum_{j=1}^N\\exp(x_j^i)\\right)^2\\]</span></p>\n<p>xrouterlogitsBtoken</p>\n<p>lossloss</p>\n<p><span class=\"math display\">\\[loss=loss_{lm}+\\alpha loss_b+\\beta\nloss_z\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha=0.01\\)</span><span class=\"math inline\">\\(\\beta=0.001\\)</span></p>\n<h1 id=\"\"></h1>\n<p>JetMoE</p>\n<p><br>\n- RefinedWeb5Ttoken600B<br>\n- StarCoder86<br>\n- Dolma3T token<br>\n- The Pile825GB<br>\n-\nProof-Pile-2OpenWebMathStackMathQAOpenAssistantxP3xCommitPackFT</p>\n<p><br>\n- OpenHermes 2.5<br>\n- UltraTextbooks<br>\n- UltraChat 200k<br>\n- TemplateGSMMagicoder-Evol-110KEvol-Code\nAlpacaCode-290k-ShareGPT</p>\n<h1 id=\"\"></h1>\n<p>JetMoEMegatronpipeline parallelismexpert\nparallelism96H10030,000GPU\nhour1.25T token</p>\n<p><br>\n- AdamW<br>\n- maximum learning rate = 5e-4<br>\n- batch size = 4M<br>\n- sequence length = 4096<br>\n- learning rate schedule = WSDwarmup = 10B tokendecay = 250B\ntoken</p>\n<p>MiniCPM - phase1warmup and stable\nlearning rateRefinedWeb, Starcoder, The Pile, peS2o\nfrom Dolma, and OpenWebMath<br>\n- phase2:decay learning rate</p>\n<p>phase1phase2</p>\n<img src=\"/f3acf042/data1.png\" class title=\"\">\n<img src=\"/f3acf042/data2.png\" class title=\"\">\n<h1 id=\"alignment\">Alignment</h1>\n<p>JetMoEDistilled Supervised\nFine-TuningdSFTdSFTprompt</p>\n<p>JetMoEZephyr-7b-betachat\ntemplateGPT-4ClaudeJetMoE<br>\n- UltraChat 200k<br>\n- Airoboros-3.2<br>\n- Code-Feedback<br>\n- Orca-math-word-problems-200k<br>\n- SystemChat<br>\n- Capybara</p>\n<p><br>\n- lr = 2e-5<br>\n- batch size = 128<br>\n- epoch = 3</p>\n<p>SFTDistilled Direct Preference Optimization\n(dDPO)</p>\n<p>UltraFeedbackpreference</p>\n<p><br>\n- lr = 5e-7<br>\n- batch size = 128<br>\n- epoch = 1</p>\n<h1 id=\"\"></h1>\n<p>benchmark</p>\n<img src=\"/f3acf042/evaluation.png\" class title=\"\">\n<img src=\"/f3acf042/mtbench.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<p>JetMoEMoE</p>\n<p>attention expert</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1JetMoE: Reaching Llama2 Performance with 0.1M Dollars\nhttps://arxiv.org/abs/2404.07413</p>\n"},{"title":"-SkyworkMoE","abbrlink":"1d5bcd45","date":"2024-06-04T12:51:02.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nMoE[MoE](http://www.linsight.cn/44e38c1b.html)  \n\nSkywork-MoE146B22BMoE  \n\nSkywork-MoE  \n\n# Skywork-MoE  \n\nSkywork-MoE  \n- Llama-like architecture  \n- RoPE  \n- RMSNorm  \n- SwiGLU activation function  \n\n  \n\n{% asset_img structure.png  %}  \n\nSkywork-MoE146B1622B  \n\n192NVIDIAHGX-A8001536A800-80G  \n\nMegatrondata parallelismZeRO-1690token/GPU/secondGPU38%  \n\n#   \n\nMoE  \n- upcyclingdenseMoEMoEdensedense  \n- from scratchMoEupcyclingupcycling  \n\ndensedenseMoEdenseMoE  \n\nupcyclingfrom scratchdenseMoEMoEupcyclingfrom scratchdensefrom scratch  \n\nfrom scratch  \n\n300B token0.3Bdense100B300Bcheckpointcheckpoint\"checkpoint-100B\"\"checkpoint-300B\"  \n\ndense8MoE3from-scratch / checkpoint-100B / checkpoint-300B  \n\nMoE100B300Btoken  \n\n100B  \n\n{% asset_img 100B.png 100B %}  \n\n300Binit_scratch-decay_300binit_100b-decay_300binit_300b-3xLRinit_300b-const3  \n\n  \n\n{% asset_img exp_1.png  %}  \n\n100Bfrom scratchdenseMoElossinit_300b-constinit_300b-const  \n\n300Bfrom scratch  \n\nexpert similarityexpert similarityupcyclingexpert similarityfrom scratchexpert similarity0densesuboptimal  \n\n $C_{\\mathrm{dense}}$ dense$C_{\\mathrm{MoE}}$ MoE  \n-  $C_{\\mathrm{MoE}}\\ll C_{\\mathrm{dense}}$upcyclingupcyclingdense  \n-  $C_{\\mathrm{MoE}}\\geq2C_{\\mathrm{dense}}$from scratch  \n\n  \n\n#   \n\nSkywork-MoEgating logit normalizationadaptive auxiliary loss coefficients  \n\n## gating logit normalization  \n\ngating layerMoEweighted average  \n\ngating layer  \n\nSkywork-MoEgating layersoftmaxnormalization step  \n\n$$\\begin{aligned}&z=Wx+b\\\\&\\tilde{z}=\\lambda\\cdot\\frac{z-\\mu}{\\sigma}\\\\&g=\\operatorname{softmax}(\\tilde{z})\\end{aligned}$$  \n\n $\\lambda$   \n\n0 $\\lambda$  $\\lambda$ softmaxsoftmax  \n\nSkywork-MoE2.5B16MoEgating logit normalization  \n\ngatingnormalization  \n\n{% asset_img gate_dist.png gating distribution %}  \n\nnormalizationtraining losstoken drop rate  \n\n{% asset_img normaization.png gating logit normalization %}  \n\ngating layerMax1/Max2Max2/Max3expert  \n\nSkywork-MoE $\\lambda=1$  \n\n## adaptive auxiliary loss coefficients  \n\nMoEauxiliary lossMMoEloss  \n\n$$\\mathcal{L}_{\\mathrm{total}}=\\mathcal{L}_{\\mathrm{ce}}+\\sum_{l=1}^M\\alpha\\mathcal{L}_{\\mathrm{aux}}^{(l)}$$  \n\nMoEauxiliary loss  \n\nSkywork-MoEauxiliary loss $\\alpha$ gatingauxiliary loss  \n\nSkywork-MoEadaptive auxiliary loss coefficients  \n\nMoEauxiliary lossMoEtoken drop ratetoken drop rategatingauxiliary loss  \n\nlMoEistepauxiliary loss  \n\n$$\\begin{array}{rcl}\\hat\\alpha_{i+1}^{(l)}&=&f(d_i^{(l)})\\\\\\alpha_{i+1}^{(l)}&=&\\beta\\alpha_i^{(l)}+(1-\\beta)\\hat\\alpha_{i+1}^{(l)}\\end{array}$$  \n\ndtoken drop ratef$\\alpha$ moving average$\\beta$ moving average  \n\nf  \n\n$$f(d)=\\left\\{\\begin{array}{ll}\\xi d&\\text{if }d\\leq\\alpha_{\\text{max}}/\\xi\\\\\\alpha_{\\text{max}}&\\text{if }d>\\alpha_{\\text{max}}/\\xi\\end{array}\\right.$$  \n\n$\\xi$ auxiliary loss coefficienttoken drop rate  \n\n  \n- $\\xi=1/5$  \n- $\\alpha_{\\max}=0.01$  \n- $\\beta=0.99$  \n\n#   \n\n  \n\n##   \n\nMoEtokenglobal batch size  \n\nnkk/n  \n\nbatch sizenoiselearning ratelinear scaling$k/n$square root scaling$\\sqrt{k/n}$  \n\nlearning rateSkywork-MoE1.8B322square root scaling3  \n\n{% asset_img lr_exp.png lr %}  \n\n300Blrpeak lr10%10Blr0  \n\nloss  \n\n{% asset_img lr_result.png lr %}  \n\n300Blr10Blossloss300Blr decay  \n\nMoE  \n\n##   \n\ndenseMoEdenseMoE  \n\nSkywork-MoEdense100BdensedenseMoE  \n\ndense $M_{\\mathrm{base}}$100B $M_{\\mathrm{cn}},M_{\\mathrm{en}},M_{\\mathrm{code}}$ dense $M_{\\mathrm{cn}}$ 3$M_{\\mathrm{en}}$ 3$M_{\\mathrm{code}}$ 1$M_{\\mathrm{base}}$ 18MoE  \n\nloss  \n\n{% asset_img diff_dense.png  %}  \n\n  \n\n90Bloss0.01denseSkywork-MoE  \n\n#   \n\n146BSkywork-MoESkywork-13B  \n\nSkyPile  \n\n7:2:1  \n\nSkywork-MoEbenchmark  \n\n{% asset_img perf.png  %}  \n\n  \n\n#   \n\nSkywork-MoEMoE  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n1Skywork-MoE: A Deep Dive into Training Techniques for\nMixture-of-Experts Language Models https://github.com/SkyworkAI/Skywork-MoE/blob/main/skywork-moe-tech-report.pdf  \n","source":"_posts/cs/nlp/2024/06/SkyworkMoE.md","raw":"---\ntitle: -SkyworkMoE\nabbrlink: 1d5bcd45\ndate: 2024-06-04 20:51:02\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - MoE\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nMoE[MoE](http://www.linsight.cn/44e38c1b.html)  \n\nSkywork-MoE146B22BMoE  \n\nSkywork-MoE  \n\n# Skywork-MoE  \n\nSkywork-MoE  \n- Llama-like architecture  \n- RoPE  \n- RMSNorm  \n- SwiGLU activation function  \n\n  \n\n{% asset_img structure.png  %}  \n\nSkywork-MoE146B1622B  \n\n192NVIDIAHGX-A8001536A800-80G  \n\nMegatrondata parallelismZeRO-1690token/GPU/secondGPU38%  \n\n#   \n\nMoE  \n- upcyclingdenseMoEMoEdensedense  \n- from scratchMoEupcyclingupcycling  \n\ndensedenseMoEdenseMoE  \n\nupcyclingfrom scratchdenseMoEMoEupcyclingfrom scratchdensefrom scratch  \n\nfrom scratch  \n\n300B token0.3Bdense100B300Bcheckpointcheckpoint\"checkpoint-100B\"\"checkpoint-300B\"  \n\ndense8MoE3from-scratch / checkpoint-100B / checkpoint-300B  \n\nMoE100B300Btoken  \n\n100B  \n\n{% asset_img 100B.png 100B %}  \n\n300Binit_scratch-decay_300binit_100b-decay_300binit_300b-3xLRinit_300b-const3  \n\n  \n\n{% asset_img exp_1.png  %}  \n\n100Bfrom scratchdenseMoElossinit_300b-constinit_300b-const  \n\n300Bfrom scratch  \n\nexpert similarityexpert similarityupcyclingexpert similarityfrom scratchexpert similarity0densesuboptimal  \n\n $C_{\\mathrm{dense}}$ dense$C_{\\mathrm{MoE}}$ MoE  \n-  $C_{\\mathrm{MoE}}\\ll C_{\\mathrm{dense}}$upcyclingupcyclingdense  \n-  $C_{\\mathrm{MoE}}\\geq2C_{\\mathrm{dense}}$from scratch  \n\n  \n\n#   \n\nSkywork-MoEgating logit normalizationadaptive auxiliary loss coefficients  \n\n## gating logit normalization  \n\ngating layerMoEweighted average  \n\ngating layer  \n\nSkywork-MoEgating layersoftmaxnormalization step  \n\n$$\\begin{aligned}&z=Wx+b\\\\&\\tilde{z}=\\lambda\\cdot\\frac{z-\\mu}{\\sigma}\\\\&g=\\operatorname{softmax}(\\tilde{z})\\end{aligned}$$  \n\n $\\lambda$   \n\n0 $\\lambda$  $\\lambda$ softmaxsoftmax  \n\nSkywork-MoE2.5B16MoEgating logit normalization  \n\ngatingnormalization  \n\n{% asset_img gate_dist.png gating distribution %}  \n\nnormalizationtraining losstoken drop rate  \n\n{% asset_img normaization.png gating logit normalization %}  \n\ngating layerMax1/Max2Max2/Max3expert  \n\nSkywork-MoE $\\lambda=1$  \n\n## adaptive auxiliary loss coefficients  \n\nMoEauxiliary lossMMoEloss  \n\n$$\\mathcal{L}_{\\mathrm{total}}=\\mathcal{L}_{\\mathrm{ce}}+\\sum_{l=1}^M\\alpha\\mathcal{L}_{\\mathrm{aux}}^{(l)}$$  \n\nMoEauxiliary loss  \n\nSkywork-MoEauxiliary loss $\\alpha$ gatingauxiliary loss  \n\nSkywork-MoEadaptive auxiliary loss coefficients  \n\nMoEauxiliary lossMoEtoken drop ratetoken drop rategatingauxiliary loss  \n\nlMoEistepauxiliary loss  \n\n$$\\begin{array}{rcl}\\hat\\alpha_{i+1}^{(l)}&=&f(d_i^{(l)})\\\\\\alpha_{i+1}^{(l)}&=&\\beta\\alpha_i^{(l)}+(1-\\beta)\\hat\\alpha_{i+1}^{(l)}\\end{array}$$  \n\ndtoken drop ratef$\\alpha$ moving average$\\beta$ moving average  \n\nf  \n\n$$f(d)=\\left\\{\\begin{array}{ll}\\xi d&\\text{if }d\\leq\\alpha_{\\text{max}}/\\xi\\\\\\alpha_{\\text{max}}&\\text{if }d>\\alpha_{\\text{max}}/\\xi\\end{array}\\right.$$  \n\n$\\xi$ auxiliary loss coefficienttoken drop rate  \n\n  \n- $\\xi=1/5$  \n- $\\alpha_{\\max}=0.01$  \n- $\\beta=0.99$  \n\n#   \n\n  \n\n##   \n\nMoEtokenglobal batch size  \n\nnkk/n  \n\nbatch sizenoiselearning ratelinear scaling$k/n$square root scaling$\\sqrt{k/n}$  \n\nlearning rateSkywork-MoE1.8B322square root scaling3  \n\n{% asset_img lr_exp.png lr %}  \n\n300Blrpeak lr10%10Blr0  \n\nloss  \n\n{% asset_img lr_result.png lr %}  \n\n300Blr10Blossloss300Blr decay  \n\nMoE  \n\n##   \n\ndenseMoEdenseMoE  \n\nSkywork-MoEdense100BdensedenseMoE  \n\ndense $M_{\\mathrm{base}}$100B $M_{\\mathrm{cn}},M_{\\mathrm{en}},M_{\\mathrm{code}}$ dense $M_{\\mathrm{cn}}$ 3$M_{\\mathrm{en}}$ 3$M_{\\mathrm{code}}$ 1$M_{\\mathrm{base}}$ 18MoE  \n\nloss  \n\n{% asset_img diff_dense.png  %}  \n\n  \n\n90Bloss0.01denseSkywork-MoE  \n\n#   \n\n146BSkywork-MoESkywork-13B  \n\nSkyPile  \n\n7:2:1  \n\nSkywork-MoEbenchmark  \n\n{% asset_img perf.png  %}  \n\n  \n\n#   \n\nSkywork-MoEMoE  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n1Skywork-MoE: A Deep Dive into Training Techniques for\nMixture-of-Experts Language Models https://github.com/SkyworkAI/Skywork-MoE/blob/main/skywork-moe-tech-report.pdf  \n","slug":"cs/nlp/2024/06/SkyworkMoE","published":1,"updated":"2024-06-05T12:41:50.292Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsms000o0p4kcl5g8m2j","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>MoE<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a></p>\n<p>Skywork-MoE146B22BMoE</p>\n<p>Skywork-MoE</p>\n<h1 id=\"skywork-moe\">Skywork-MoE</h1>\n<p>Skywork-MoE<br>\n- Llama-like architecture<br>\n- RoPE<br>\n- RMSNorm<br>\n- SwiGLU activation function</p>\n<p></p>\n<img src=\"/1d5bcd45/structure.png\" class title=\"\">\n<p>Skywork-MoE146B1622B</p>\n<p>192NVIDIAHGX-A8001536A800-80G</p>\n<p>Megatrondata\nparallelismZeRO-1690token/GPU/secondGPU38%</p>\n<h1 id=\"\"></h1>\n<p>MoE<br>\n-\nupcyclingdenseMoEMoEdensedense<br>\n- from\nscratchMoEupcyclingupcycling</p>\n<p>densedenseMoEdenseMoE</p>\n<p>upcyclingfrom\nscratchdenseMoEMoEupcyclingfrom\nscratchdensefrom scratch</p>\n<p>from\nscratch</p>\n<p>300B\ntoken0.3Bdense100B300Bcheckpointcheckpoint\"checkpoint-100B\"\"checkpoint-300B\"</p>\n<p>dense8MoE3from-scratch\n/ checkpoint-100B / checkpoint-300B</p>\n<p>MoE100B300Btoken</p>\n<p>100B</p>\n<img src=\"/1d5bcd45/100B.png\" class title=\"100B\">\n<p>300Binit_scratch-decay_300binit_100b-decay_300binit_300b-3xLRinit_300b-const3</p>\n<p></p>\n<img src=\"/1d5bcd45/exp_1.png\" class title=\"\">\n<p>100Bfrom\nscratchdenseMoElossinit_300b-constinit_300b-const</p>\n<p>300Bfrom\nscratch</p>\n<p>expert similarityexpert\nsimilarityupcyclingexpert\nsimilarityfrom\nscratchexpert\nsimilarity0densesuboptimal</p>\n<p> <span class=\"math inline\">\\(C_{\\mathrm{dense}}\\)</span>\ndense<span class=\"math inline\">\\(C_{\\mathrm{MoE}}\\)</span>\nMoE<br>\n-  <span class=\"math inline\">\\(C_{\\mathrm{MoE}}\\ll\nC_{\\mathrm{dense}}\\)</span>upcyclingupcyclingdense<br>\n-  <span class=\"math inline\">\\(C_{\\mathrm{MoE}}\\geq2C_{\\mathrm{dense}}\\)</span>from\nscratch</p>\n<p></p>\n<h1 id=\"\"></h1>\n<p>Skywork-MoEgating logit\nnormalizationadaptive auxiliary loss coefficients</p>\n<h2 id=\"gating-logit-normalization\">gating logit normalization</h2>\n<p>gating\nlayerMoEweighted\naverage</p>\n<p>gating\nlayer</p>\n<p>Skywork-MoEgating\nlayersoftmaxnormalization step</p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;z=Wx+b\\\\&amp;\\tilde{z}=\\lambda\\cdot\\frac{z-\\mu}{\\sigma}\\\\&amp;g=\\operatorname{softmax}(\\tilde{z})\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\lambda\\)</span> </p>\n<p>0 <span class=\"math inline\">\\(\\lambda\\)</span>  <span class=\"math inline\">\\(\\lambda\\)</span>\nsoftmaxsoftmax</p>\n<p>Skywork-MoE2.5B16MoEgating\nlogit normalization</p>\n<p>gatingnormalization</p>\n<img src=\"/1d5bcd45/gate_dist.png\" class title=\"gating distribution\">\n<p>normalizationtraining losstoken drop\nrate</p>\n<img src=\"/1d5bcd45/normaization.png\" class title=\"gating logit normalization\">\n<p>gating\nlayerMax1/Max2Max2/Max3expert</p>\n<p>Skywork-MoE <span class=\"math inline\">\\(\\lambda=1\\)</span></p>\n<h2 id=\"adaptive-auxiliary-loss-coefficients\">adaptive auxiliary loss\ncoefficients</h2>\n<p>MoEauxiliary\nlossMMoEloss</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\mathrm{total}}=\\mathcal{L}_{\\mathrm{ce}}+\\sum_{l=1}^M\\alpha\\mathcal{L}_{\\mathrm{aux}}^{(l)}\\]</span></p>\n<p>MoEauxiliary loss</p>\n<p>Skywork-MoEauxiliary loss <span class=\"math inline\">\\(\\alpha\\)</span>\ngatingauxiliary\nloss</p>\n<p>Skywork-MoEadaptive auxiliary loss\ncoefficients</p>\n<p>MoEauxiliary\nlossMoEtoken drop\nratetoken drop\nrategatingauxiliary\nloss</p>\n<p>lMoEistepauxiliary loss</p>\n<p><span class=\"math display\">\\[\\begin{array}{rcl}\\hat\\alpha_{i+1}^{(l)}&amp;=&amp;f(d_i^{(l)})\\\\\\alpha_{i+1}^{(l)}&amp;=&amp;\\beta\\alpha_i^{(l)}+(1-\\beta)\\hat\\alpha_{i+1}^{(l)}\\end{array}\\]</span></p>\n<p>dtoken drop ratef<span class=\"math inline\">\\(\\alpha\\)</span> moving\naverage<span class=\"math inline\">\\(\\beta\\)</span> moving\naverage</p>\n<p>f</p>\n<p><span class=\"math display\">\\[f(d)=\\left\\{\\begin{array}{ll}\\xi\nd&amp;\\text{if\n}d\\leq\\alpha_{\\text{max}}/\\xi\\\\\\alpha_{\\text{max}}&amp;\\text{if\n}d&gt;\\alpha_{\\text{max}}/\\xi\\end{array}\\right.\\]</span></p>\n<p><span class=\"math inline\">\\(\\xi\\)</span> auxiliary loss\ncoefficienttoken drop rate</p>\n<p><br>\n- <span class=\"math inline\">\\(\\xi=1/5\\)</span><br>\n- <span class=\"math inline\">\\(\\alpha_{\\max}=0.01\\)</span><br>\n- <span class=\"math inline\">\\(\\beta=0.99\\)</span></p>\n<h1 id=\"\"></h1>\n<p></p>\n<h2 id=\"\"></h2>\n<p>MoEtokenglobal\nbatch size</p>\n<p>nkk/n</p>\n<p>batch\nsizenoiselearning\nratelinear scaling<span class=\"math inline\">\\(k/n\\)</span>square root scaling<span class=\"math inline\">\\(\\sqrt{k/n}\\)</span></p>\n<p>learning\nrateSkywork-MoE1.8B322square\nroot scaling3</p>\n<img src=\"/1d5bcd45/lr_exp.png\" class title=\"lr\">\n<p>300Blrpeak\nlr10%10Blr0</p>\n<p>loss</p>\n<img src=\"/1d5bcd45/lr_result.png\" class title=\"lr\">\n<p>300Blr10Blossloss300Blr\ndecay</p>\n<p>MoE</p>\n<h2 id=\"\"></h2>\n<p>denseMoEdenseMoE</p>\n<p>Skywork-MoEdense100BdensedenseMoE</p>\n<p>dense <span class=\"math inline\">\\(M_{\\mathrm{base}}\\)</span>100B\n<span class=\"math inline\">\\(M_{\\mathrm{cn}},M_{\\mathrm{en}},M_{\\mathrm{code}}\\)</span>\ndense <span class=\"math inline\">\\(M_{\\mathrm{cn}}\\)</span> 3<span class=\"math inline\">\\(M_{\\mathrm{en}}\\)</span> 3<span class=\"math inline\">\\(M_{\\mathrm{code}}\\)</span> 1<span class=\"math inline\">\\(M_{\\mathrm{base}}\\)</span>\n18MoE</p>\n<p>loss</p>\n<img src=\"/1d5bcd45/diff_dense.png\" class title=\"\">\n<p></p>\n<p>90Bloss0.01denseSkywork-MoE</p>\n<h1 id=\"\"></h1>\n<p>146BSkywork-MoESkywork-13B</p>\n<p>SkyPile</p>\n<p>7:2:1</p>\n<p>Skywork-MoEbenchmark</p>\n<img src=\"/1d5bcd45/perf.png\" class title=\"\">\n<p></p>\n<h1 id=\"\"></h1>\n<p>Skywork-MoEMoE</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Skywork-MoE: A Deep Dive into Training Techniques for\nMixture-of-Experts Language Models\nhttps://github.com/SkyworkAI/Skywork-MoE/blob/main/skywork-moe-tech-report.pdf</p>\n","length":5954,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>MoE<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a></p>\n<p>Skywork-MoE146B22BMoE</p>\n<p>Skywork-MoE</p>\n<h1 id=\"skywork-moe\">Skywork-MoE</h1>\n<p>Skywork-MoE<br>\n- Llama-like architecture<br>\n- RoPE<br>\n- RMSNorm<br>\n- SwiGLU activation function</p>\n<p></p>\n<img src=\"/1d5bcd45/structure.png\" class title=\"\">\n<p>Skywork-MoE146B1622B</p>\n<p>192NVIDIAHGX-A8001536A800-80G</p>\n<p>Megatrondata\nparallelismZeRO-1690token/GPU/secondGPU38%</p>\n<h1 id=\"\"></h1>\n<p>MoE<br>\n-\nupcyclingdenseMoEMoEdensedense<br>\n- from\nscratchMoEupcyclingupcycling</p>\n<p>densedenseMoEdenseMoE</p>\n<p>upcyclingfrom\nscratchdenseMoEMoEupcyclingfrom\nscratchdensefrom scratch</p>\n<p>from\nscratch</p>\n<p>300B\ntoken0.3Bdense100B300Bcheckpointcheckpoint\"checkpoint-100B\"\"checkpoint-300B\"</p>\n<p>dense8MoE3from-scratch\n/ checkpoint-100B / checkpoint-300B</p>\n<p>MoE100B300Btoken</p>\n<p>100B</p>\n<img src=\"/1d5bcd45/100B.png\" class title=\"100B\">\n<p>300Binit_scratch-decay_300binit_100b-decay_300binit_300b-3xLRinit_300b-const3</p>\n<p></p>\n<img src=\"/1d5bcd45/exp_1.png\" class title=\"\">\n<p>100Bfrom\nscratchdenseMoElossinit_300b-constinit_300b-const</p>\n<p>300Bfrom\nscratch</p>\n<p>expert similarityexpert\nsimilarityupcyclingexpert\nsimilarityfrom\nscratchexpert\nsimilarity0densesuboptimal</p>\n<p> <span class=\"math inline\">\\(C_{\\mathrm{dense}}\\)</span>\ndense<span class=\"math inline\">\\(C_{\\mathrm{MoE}}\\)</span>\nMoE<br>\n-  <span class=\"math inline\">\\(C_{\\mathrm{MoE}}\\ll\nC_{\\mathrm{dense}}\\)</span>upcyclingupcyclingdense<br>\n-  <span class=\"math inline\">\\(C_{\\mathrm{MoE}}\\geq2C_{\\mathrm{dense}}\\)</span>from\nscratch</p>\n<p></p>\n<h1 id=\"\"></h1>\n<p>Skywork-MoEgating logit\nnormalizationadaptive auxiliary loss coefficients</p>\n<h2 id=\"gating-logit-normalization\">gating logit normalization</h2>\n<p>gating\nlayerMoEweighted\naverage</p>\n<p>gating\nlayer</p>\n<p>Skywork-MoEgating\nlayersoftmaxnormalization step</p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;z=Wx+b\\\\&amp;\\tilde{z}=\\lambda\\cdot\\frac{z-\\mu}{\\sigma}\\\\&amp;g=\\operatorname{softmax}(\\tilde{z})\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\lambda\\)</span> </p>\n<p>0 <span class=\"math inline\">\\(\\lambda\\)</span>  <span class=\"math inline\">\\(\\lambda\\)</span>\nsoftmaxsoftmax</p>\n<p>Skywork-MoE2.5B16MoEgating\nlogit normalization</p>\n<p>gatingnormalization</p>\n<img src=\"/1d5bcd45/gate_dist.png\" class title=\"gating distribution\">\n<p>normalizationtraining losstoken drop\nrate</p>\n<img src=\"/1d5bcd45/normaization.png\" class title=\"gating logit normalization\">\n<p>gating\nlayerMax1/Max2Max2/Max3expert</p>\n<p>Skywork-MoE <span class=\"math inline\">\\(\\lambda=1\\)</span></p>\n<h2 id=\"adaptive-auxiliary-loss-coefficients\">adaptive auxiliary loss\ncoefficients</h2>\n<p>MoEauxiliary\nlossMMoEloss</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\mathrm{total}}=\\mathcal{L}_{\\mathrm{ce}}+\\sum_{l=1}^M\\alpha\\mathcal{L}_{\\mathrm{aux}}^{(l)}\\]</span></p>\n<p>MoEauxiliary loss</p>\n<p>Skywork-MoEauxiliary loss <span class=\"math inline\">\\(\\alpha\\)</span>\ngatingauxiliary\nloss</p>\n<p>Skywork-MoEadaptive auxiliary loss\ncoefficients</p>\n<p>MoEauxiliary\nlossMoEtoken drop\nratetoken drop\nrategatingauxiliary\nloss</p>\n<p>lMoEistepauxiliary loss</p>\n<p><span class=\"math display\">\\[\\begin{array}{rcl}\\hat\\alpha_{i+1}^{(l)}&amp;=&amp;f(d_i^{(l)})\\\\\\alpha_{i+1}^{(l)}&amp;=&amp;\\beta\\alpha_i^{(l)}+(1-\\beta)\\hat\\alpha_{i+1}^{(l)}\\end{array}\\]</span></p>\n<p>dtoken drop ratef<span class=\"math inline\">\\(\\alpha\\)</span> moving\naverage<span class=\"math inline\">\\(\\beta\\)</span> moving\naverage</p>\n<p>f</p>\n<p><span class=\"math display\">\\[f(d)=\\left\\{\\begin{array}{ll}\\xi\nd&amp;\\text{if\n}d\\leq\\alpha_{\\text{max}}/\\xi\\\\\\alpha_{\\text{max}}&amp;\\text{if\n}d&gt;\\alpha_{\\text{max}}/\\xi\\end{array}\\right.\\]</span></p>\n<p><span class=\"math inline\">\\(\\xi\\)</span> auxiliary loss\ncoefficienttoken drop rate</p>\n<p><br>\n- <span class=\"math inline\">\\(\\xi=1/5\\)</span><br>\n- <span class=\"math inline\">\\(\\alpha_{\\max}=0.01\\)</span><br>\n- <span class=\"math inline\">\\(\\beta=0.99\\)</span></p>\n<h1 id=\"\"></h1>\n<p></p>\n<h2 id=\"\"></h2>\n<p>MoEtokenglobal\nbatch size</p>\n<p>nkk/n</p>\n<p>batch\nsizenoiselearning\nratelinear scaling<span class=\"math inline\">\\(k/n\\)</span>square root scaling<span class=\"math inline\">\\(\\sqrt{k/n}\\)</span></p>\n<p>learning\nrateSkywork-MoE1.8B322square\nroot scaling3</p>\n<img src=\"/1d5bcd45/lr_exp.png\" class title=\"lr\">\n<p>300Blrpeak\nlr10%10Blr0</p>\n<p>loss</p>\n<img src=\"/1d5bcd45/lr_result.png\" class title=\"lr\">\n<p>300Blr10Blossloss300Blr\ndecay</p>\n<p>MoE</p>\n<h2 id=\"\"></h2>\n<p>denseMoEdenseMoE</p>\n<p>Skywork-MoEdense100BdensedenseMoE</p>\n<p>dense <span class=\"math inline\">\\(M_{\\mathrm{base}}\\)</span>100B\n<span class=\"math inline\">\\(M_{\\mathrm{cn}},M_{\\mathrm{en}},M_{\\mathrm{code}}\\)</span>\ndense <span class=\"math inline\">\\(M_{\\mathrm{cn}}\\)</span> 3<span class=\"math inline\">\\(M_{\\mathrm{en}}\\)</span> 3<span class=\"math inline\">\\(M_{\\mathrm{code}}\\)</span> 1<span class=\"math inline\">\\(M_{\\mathrm{base}}\\)</span>\n18MoE</p>\n<p>loss</p>\n<img src=\"/1d5bcd45/diff_dense.png\" class title=\"\">\n<p></p>\n<p>90Bloss0.01denseSkywork-MoE</p>\n<h1 id=\"\"></h1>\n<p>146BSkywork-MoESkywork-13B</p>\n<p>SkyPile</p>\n<p>7:2:1</p>\n<p>Skywork-MoEbenchmark</p>\n<img src=\"/1d5bcd45/perf.png\" class title=\"\">\n<p></p>\n<h1 id=\"\"></h1>\n<p>Skywork-MoEMoE</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Skywork-MoE: A Deep Dive into Training Techniques for\nMixture-of-Experts Language Models\nhttps://github.com/SkyworkAI/Skywork-MoE/blob/main/skywork-moe-tech-report.pdf</p>\n"},{"title":":sliding window attention","abbrlink":"c61d17e3","date":"2024-03-12T09:26:00.000Z","_content":"\n//  \n\nLLM  \n\n[LLM](http://www.linsight.cn/c4da56c0.html)32k+/128k+[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)MQAGQAKV  \n\nSWAsliding window attention  \n\nQwenMistralSWA  \n\nMistral  \n\nMistral AIAI202352023912Mistral 7BMoEMistral 8x7B  \n\n20242  \n\n{% asset_img ms_invest_mistral.png MS %}  \n\n20242Mistral Large & 32kMMLUGPT4  \n\n{% asset_img mistral_large_performance.jpeg Mistral Large MMLU Performance %}  \n\nMistralOPENAIMETA  \n\n# SWA\n\nSWAMistralMistral 7BSWA  \n\n## Mistral 7B\n\n202310MistralMistral 7B[](https://arxiv.org/pdf/2310.06825.pdf)LlamaMistralGQASWA  \n\nMistral 7B  \n\n{% asset_img mistral_architechture.png Mistral Architechture %}  \n\nMistralkv=8GQAintermediate sizeLlama211008  \n\n## \n\ncausal attentiontokentoken  \n\n $s$ 1 $s^2$ KV Cache  \n\n/ $s$ \n\n1\n\n $[m,n]\\times[n,p]$  $[m,p]$ $m\\times p$  $n$  $n$  $2mpn$ floating point operationsFLOPs  \n\n[Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)  \n\nMHA $s$  $L$ hidden size $d_{model}$  $d_{q}$   $n_{q}$ $d_{model} = n_{q}\\times d_{q}$ operationFLOPs  \n\n<center>\n\n| Operation | FLOPsMHA |\n| :---- | :----: |\n| Attention: QKV | $6\\times s\\times h_{model}^{2}$  |\n| Attention: QK logits ( $QK^T$ ) | $n_{q}\\times 2\\times s^2\\times h_{q}$ |\n| Attention: Softmax | $n_{q}\\times 3\\times s^2$ |\n| Attention: Reduction (apply to $V$) | $n_{q}\\times 2\\times s^2\\times h_{q}$ |\n| Attention: Outupt Linear Project | $2\\times s\\times h_{model}^{2}$ |\n\n</center>\n\nSoftmax $[1,s]$ softmax $3s$  $s$ exp $s$  $s$  $[s,s]$ softmax  $3s^2$  $n_{q}$ \n\noperationscalingdropout\n\nMistral 7BGQA  \n\nKVkv $n_{kv}$\n\n<center>\n\n| Operation | FLOPsGQA |\n| :---- | :----: |\n| Attention: QKV | $2\\times s\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times n_{kv})$  |\n\n</center>\n\nQK logitsSoftmaxReduction $s$   \n\n2\n\nKV Cache  \n\n$$\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\n\n\n$$\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\nMistral 7B16kKV_Cache2G  \n\nGQA16k+\n\n## SWA\n\nattention $s$  $s$ \n\nCNN  \n\n{% asset_img receptive_field_cnn.png CNN Receptive Field %}  \n\n3 $3\\times 3$ CNNsliding window  \n\nlayer 3layer 2 $3\\times 3$ layer 2  \n\nlayer 2layer 1 $3\\times 3$ layer 2 $3\\times 3$ layer 1 $5\\times 5$ layer 3<u>****</u> $5\\times 5$   \n\nlayer 4layer 4layer 1  $7\\times 7$   \n\n  \n\n  \n\nCNN  \n\n  \n\n  \n\n\n\nMistralSWA  \n\n{% asset_img mistral_swa.png Mistral SWA %}  \n\ncausal attentionattention mask  \n\nSWAattention mask33  \n\nCNNLLM  \n\nMistral 7B409632 $4096\\times 32=131,072$ 131k  \n\nattentionQK logitsSoftmaxReduction $s$ SWAoperation4k131k131k $32\\times 32=1024$   \n\n $s$ 131k $31/32$   \n\nSWA4kcausal attention4k\n\n>In practice, for a sequence length of 16K and W = 4096, changes made to FlashAttention [11] and xFormers [18] yield a 2x speed improvement over a vanilla attention baseline.\n\nMistralSWAFlashAttentionxFormers16k2  \n\n## KV Cache\n\nsliding windowKV Cache  \n\nSWAkv  \n\n $W=4$ 5token1tokenkv  \n\n{% asset_img rolling_buffer.png swa rolling buffer %}  \n\nthroughputcase  \n\n## Prompt\n\nRAGfunciton callprompt  \n\nGPT4system promptOPENAI  \n\n>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\" and the user's locale is \"en-US\"\nYour knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07.\nImage input capabilities: Enabled\n>\n>Tools\n>\n>python\n>\n>When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 60.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is disabled. Do not make external web requests or API calls as they will fail.\n>\n>dalle\n>\n>Whenever a description of an image is given, create a prompt that dalle can use to generate the image and abide to the following policy:\n>1. The prompt must be in English. Translate to English if needed.\n>2. DO NOT ask for permission to generate the image, just do it!\n>3. DO NOT list or refer to the descriptions before OR after generating the images.\n>4. Do not create more than 1 image, even if the user requests more.\n>5. Do not create images in the style of artists, creative professionals or studios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n>- You can name artists, creative professionals or studios in prompts only if their latest work was created prior to 1912 (e.g. Van Gogh, Goya)\n>- If asked to generate an image that would violate this policy, instead apply the following procedure: (a) substitute the artist's name with three adjectives that capture key aspects of the style; (b) include an associated artistic movement or era to provide context; and (c) mention the primary medium used by the artist\n>6. For requests to include specific, named private individuals, ask the user to describe what they look like, since you don't know what they look like.\n>7. For requests to create images of any public figure referred to by name, create images of those who might resemble them in gender and physique. But they shouldn't look like them. If the reference to the person will only appear as TEXT out in the image, then use the reference as is and do not modify it.\n>8. Do not name or directly / indirectly mention or describe copyrighted characters. Rewrite prompts to describe in detail a specific different character with a different specific color, hair style, or other defining visual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around 100 words long.\nExample dalle invocation:\n>{\n>\"prompt\": \"<insert prompt here>\"\n>}\n>namespace dalle {\n>\n>Create images from a text-only prompt.\ntype text2im = (_: {\nThe size of the requested image. Use 1024x1024 (square) as the default, 1792x1024 if the user requests a wide image, and 1024x1792 for full-body portraits. Always include this parameter in the request.\nn?: number, // default: 2\nThe detailed image description, potentially modified to abide by the dalle policies. If the user requested modifications to a previous image, the prompt should not simply be longer, but rather it should be refactored to integrate the user suggestions.\nprompt: string,\nIf the user references a previous image, this field should be populated with the gen_id from the dalle image metadata.\nreferenced_image_ids?: string[],\n}) => any;\n} // namespace dalle\n>\n>voice_mode\n>Voice mode functions are not available in text conversations.\n>namespace voice_mode {   } // namespace voice_mode\n>\n>browser\n>\n>You have the tool `browser`. Use `browser` in the following circumstances:\n>    - User is asking about current events or something that requires real-time information (weather, sports scores, etc.)\n>    - User is asking about some term you are totally unfamiliar with (it might be new)\n>    - User explicitly asks you to browse or provide links to references\n>\n>Given a query that requires retrieval, your turn will consist of three steps:\n>1. Call the search function to get a list of results.\n>2. Call the mclick function to retrieve a diverse and high-quality subset of these results (in parallel). Remember to SELECT AT LEAST 3 sources when using `mclick`.\n>3. Write a response to the user based on these results. In your response, cite sources using the citation format below.\n>\n>In some cases, you should repeat step 1 twice, if the initial results are unsatisfactory, and you believe that you can refine the query to get better results.\n>\n>You can also open a url directly if one is provided by the user. Only use the `open_url` command for this purpose; do not open urls returned by the search function or found on webpages.\n>\n>The `browser` tool has the following commands:\n\t`search(query: str, recency_days: int)` Issues a query to a search engine and displays the results.\n         `mclick(ids: list[str])`. Retrieves the contents of the webpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST 3 and at most 10 pages. Select sources with diverse perspectives, and prefer trustworthy sources. Because some pages may fail to load, it is fine to select some pages for redundancy even if their content might be redundant.\n\t`open_url(url: str)` Opens the given URL and displays it.\n>\n>For citing quotes from the 'browser' tool: please render in this format: {message idx}{link text}.\nFor long citations: please render in this format: [link text](message idx).\nOtherwise do not render links.\n\nsystem promptkvsystem promptsliding windowsystem promptkv\n\n $W=4$system prompt9system promptkv [4,4,1]   \n\nwindowattention mask0  \n\nattention mask  \n\n  \n\n  \n\nprompt  \n\n{% asset_img prefill_and_chunking.png prefill and chunking %}  \n\nFlashAttention/PagedAttention\n\nMistral 7BLlamaLlama 34B  \n\n{% asset_img mistral_perf.png mistral performance %}  \n\nMistral7B  \n\n# Sparse Attention\n\nSWAsparse attentionsparse attention  \n\nsparse attention  \n\n## Longformer\n\nMistralSWA  \n\n2020[Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150.pdf)SWAsparse attention  \n\nLongformer  \n\n{% asset_img longformer_attention.png longformer %}  \n\nbSWABert  \n\nSWAdilated sliding window    \n\n{% asset_img dilated_conv.png dilated convolution %}  \n\nattentionSWAdilated sliding window  \n\n  \n\nBert[CLS] tokentoken  \n\nGPTinstructionprompt  \n\ntokenglobal attentionsliding windowd  \n\n## Big Bird\n\n2020Longformersparse attention[Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)  \n\nsliding windowglobal attentionLongformerBig Birdrandom attention  \n\n{% asset_img big_bird_attention.png big bird attention %}  \n\n $r=2$ 2\n\n#   \n\nSWA  \n\nSWAsparse attention<big>****</big>global + local attentionflash attentionrandom attention\n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n1Mistral 7B https://arxiv.org/pdf/2310.06825.pdf  \n2Longformer: The Long-Document Transformer \nhttps://arxiv.org/pdf/2004.05150.pdf  \n3Training Compute-Optimal Large Language Models https://arxiv.org/pdf/2203.15556.pdf  \n4GPT-4 System Prompt Revealed https://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed  \n5Big Bird: Transformers for Longer Sequences https://arxiv.org/abs/2007.14062  ","source":"_posts/cs/nlp/2024/03/LLM-sliding-window-attention.md","raw":"---\ntitle: ':sliding window attention'\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - attention\n  - sliding window attention\n  - sparse attention\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: c61d17e3\ndate: 2024-03-12 17:26:00\n---\n\n//  \n\nLLM  \n\n[LLM](http://www.linsight.cn/c4da56c0.html)32k+/128k+[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)MQAGQAKV  \n\nSWAsliding window attention  \n\nQwenMistralSWA  \n\nMistral  \n\nMistral AIAI202352023912Mistral 7BMoEMistral 8x7B  \n\n20242  \n\n{% asset_img ms_invest_mistral.png MS %}  \n\n20242Mistral Large & 32kMMLUGPT4  \n\n{% asset_img mistral_large_performance.jpeg Mistral Large MMLU Performance %}  \n\nMistralOPENAIMETA  \n\n# SWA\n\nSWAMistralMistral 7BSWA  \n\n## Mistral 7B\n\n202310MistralMistral 7B[](https://arxiv.org/pdf/2310.06825.pdf)LlamaMistralGQASWA  \n\nMistral 7B  \n\n{% asset_img mistral_architechture.png Mistral Architechture %}  \n\nMistralkv=8GQAintermediate sizeLlama211008  \n\n## \n\ncausal attentiontokentoken  \n\n $s$ 1 $s^2$ KV Cache  \n\n/ $s$ \n\n1\n\n $[m,n]\\times[n,p]$  $[m,p]$ $m\\times p$  $n$  $n$  $2mpn$ floating point operationsFLOPs  \n\n[Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)  \n\nMHA $s$  $L$ hidden size $d_{model}$  $d_{q}$   $n_{q}$ $d_{model} = n_{q}\\times d_{q}$ operationFLOPs  \n\n<center>\n\n| Operation | FLOPsMHA |\n| :---- | :----: |\n| Attention: QKV | $6\\times s\\times h_{model}^{2}$  |\n| Attention: QK logits ( $QK^T$ ) | $n_{q}\\times 2\\times s^2\\times h_{q}$ |\n| Attention: Softmax | $n_{q}\\times 3\\times s^2$ |\n| Attention: Reduction (apply to $V$) | $n_{q}\\times 2\\times s^2\\times h_{q}$ |\n| Attention: Outupt Linear Project | $2\\times s\\times h_{model}^{2}$ |\n\n</center>\n\nSoftmax $[1,s]$ softmax $3s$  $s$ exp $s$  $s$  $[s,s]$ softmax  $3s^2$  $n_{q}$ \n\noperationscalingdropout\n\nMistral 7BGQA  \n\nKVkv $n_{kv}$\n\n<center>\n\n| Operation | FLOPsGQA |\n| :---- | :----: |\n| Attention: QKV | $2\\times s\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times n_{kv})$  |\n\n</center>\n\nQK logitsSoftmaxReduction $s$   \n\n2\n\nKV Cache  \n\n$$\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\n\n\n$$\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n$$  \n\nMistral 7B16kKV_Cache2G  \n\nGQA16k+\n\n## SWA\n\nattention $s$  $s$ \n\nCNN  \n\n{% asset_img receptive_field_cnn.png CNN Receptive Field %}  \n\n3 $3\\times 3$ CNNsliding window  \n\nlayer 3layer 2 $3\\times 3$ layer 2  \n\nlayer 2layer 1 $3\\times 3$ layer 2 $3\\times 3$ layer 1 $5\\times 5$ layer 3<u>****</u> $5\\times 5$   \n\nlayer 4layer 4layer 1  $7\\times 7$   \n\n  \n\n  \n\nCNN  \n\n  \n\n  \n\n\n\nMistralSWA  \n\n{% asset_img mistral_swa.png Mistral SWA %}  \n\ncausal attentionattention mask  \n\nSWAattention mask33  \n\nCNNLLM  \n\nMistral 7B409632 $4096\\times 32=131,072$ 131k  \n\nattentionQK logitsSoftmaxReduction $s$ SWAoperation4k131k131k $32\\times 32=1024$   \n\n $s$ 131k $31/32$   \n\nSWA4kcausal attention4k\n\n>In practice, for a sequence length of 16K and W = 4096, changes made to FlashAttention [11] and xFormers [18] yield a 2x speed improvement over a vanilla attention baseline.\n\nMistralSWAFlashAttentionxFormers16k2  \n\n## KV Cache\n\nsliding windowKV Cache  \n\nSWAkv  \n\n $W=4$ 5token1tokenkv  \n\n{% asset_img rolling_buffer.png swa rolling buffer %}  \n\nthroughputcase  \n\n## Prompt\n\nRAGfunciton callprompt  \n\nGPT4system promptOPENAI  \n\n>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\" and the user's locale is \"en-US\"\nYour knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07.\nImage input capabilities: Enabled\n>\n>Tools\n>\n>python\n>\n>When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 60.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is disabled. Do not make external web requests or API calls as they will fail.\n>\n>dalle\n>\n>Whenever a description of an image is given, create a prompt that dalle can use to generate the image and abide to the following policy:\n>1. The prompt must be in English. Translate to English if needed.\n>2. DO NOT ask for permission to generate the image, just do it!\n>3. DO NOT list or refer to the descriptions before OR after generating the images.\n>4. Do not create more than 1 image, even if the user requests more.\n>5. Do not create images in the style of artists, creative professionals or studios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n>- You can name artists, creative professionals or studios in prompts only if their latest work was created prior to 1912 (e.g. Van Gogh, Goya)\n>- If asked to generate an image that would violate this policy, instead apply the following procedure: (a) substitute the artist's name with three adjectives that capture key aspects of the style; (b) include an associated artistic movement or era to provide context; and (c) mention the primary medium used by the artist\n>6. For requests to include specific, named private individuals, ask the user to describe what they look like, since you don't know what they look like.\n>7. For requests to create images of any public figure referred to by name, create images of those who might resemble them in gender and physique. But they shouldn't look like them. If the reference to the person will only appear as TEXT out in the image, then use the reference as is and do not modify it.\n>8. Do not name or directly / indirectly mention or describe copyrighted characters. Rewrite prompts to describe in detail a specific different character with a different specific color, hair style, or other defining visual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around 100 words long.\nExample dalle invocation:\n>{\n>\"prompt\": \"<insert prompt here>\"\n>}\n>namespace dalle {\n>\n>Create images from a text-only prompt.\ntype text2im = (_: {\nThe size of the requested image. Use 1024x1024 (square) as the default, 1792x1024 if the user requests a wide image, and 1024x1792 for full-body portraits. Always include this parameter in the request.\nn?: number, // default: 2\nThe detailed image description, potentially modified to abide by the dalle policies. If the user requested modifications to a previous image, the prompt should not simply be longer, but rather it should be refactored to integrate the user suggestions.\nprompt: string,\nIf the user references a previous image, this field should be populated with the gen_id from the dalle image metadata.\nreferenced_image_ids?: string[],\n}) => any;\n} // namespace dalle\n>\n>voice_mode\n>Voice mode functions are not available in text conversations.\n>namespace voice_mode {   } // namespace voice_mode\n>\n>browser\n>\n>You have the tool `browser`. Use `browser` in the following circumstances:\n>    - User is asking about current events or something that requires real-time information (weather, sports scores, etc.)\n>    - User is asking about some term you are totally unfamiliar with (it might be new)\n>    - User explicitly asks you to browse or provide links to references\n>\n>Given a query that requires retrieval, your turn will consist of three steps:\n>1. Call the search function to get a list of results.\n>2. Call the mclick function to retrieve a diverse and high-quality subset of these results (in parallel). Remember to SELECT AT LEAST 3 sources when using `mclick`.\n>3. Write a response to the user based on these results. In your response, cite sources using the citation format below.\n>\n>In some cases, you should repeat step 1 twice, if the initial results are unsatisfactory, and you believe that you can refine the query to get better results.\n>\n>You can also open a url directly if one is provided by the user. Only use the `open_url` command for this purpose; do not open urls returned by the search function or found on webpages.\n>\n>The `browser` tool has the following commands:\n\t`search(query: str, recency_days: int)` Issues a query to a search engine and displays the results.\n         `mclick(ids: list[str])`. Retrieves the contents of the webpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST 3 and at most 10 pages. Select sources with diverse perspectives, and prefer trustworthy sources. Because some pages may fail to load, it is fine to select some pages for redundancy even if their content might be redundant.\n\t`open_url(url: str)` Opens the given URL and displays it.\n>\n>For citing quotes from the 'browser' tool: please render in this format: {message idx}{link text}.\nFor long citations: please render in this format: [link text](message idx).\nOtherwise do not render links.\n\nsystem promptkvsystem promptsliding windowsystem promptkv\n\n $W=4$system prompt9system promptkv [4,4,1]   \n\nwindowattention mask0  \n\nattention mask  \n\n  \n\n  \n\nprompt  \n\n{% asset_img prefill_and_chunking.png prefill and chunking %}  \n\nFlashAttention/PagedAttention\n\nMistral 7BLlamaLlama 34B  \n\n{% asset_img mistral_perf.png mistral performance %}  \n\nMistral7B  \n\n# Sparse Attention\n\nSWAsparse attentionsparse attention  \n\nsparse attention  \n\n## Longformer\n\nMistralSWA  \n\n2020[Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150.pdf)SWAsparse attention  \n\nLongformer  \n\n{% asset_img longformer_attention.png longformer %}  \n\nbSWABert  \n\nSWAdilated sliding window    \n\n{% asset_img dilated_conv.png dilated convolution %}  \n\nattentionSWAdilated sliding window  \n\n  \n\nBert[CLS] tokentoken  \n\nGPTinstructionprompt  \n\ntokenglobal attentionsliding windowd  \n\n## Big Bird\n\n2020Longformersparse attention[Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)  \n\nsliding windowglobal attentionLongformerBig Birdrandom attention  \n\n{% asset_img big_bird_attention.png big bird attention %}  \n\n $r=2$ 2\n\n#   \n\nSWA  \n\nSWAsparse attention<big>****</big>global + local attentionflash attentionrandom attention\n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n1Mistral 7B https://arxiv.org/pdf/2310.06825.pdf  \n2Longformer: The Long-Document Transformer \nhttps://arxiv.org/pdf/2004.05150.pdf  \n3Training Compute-Optimal Large Language Models https://arxiv.org/pdf/2203.15556.pdf  \n4GPT-4 System Prompt Revealed https://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed  \n5Big Bird: Transformers for Longer Sequences https://arxiv.org/abs/2007.14062  ","slug":"cs/nlp/2024/03/LLM-sliding-window-attention","published":1,"updated":"2024-03-20T11:38:30.908Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsms000p0p4kcps7453c","content":"<p>//</p>\n<p>LLM</p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a>32k+/128k+<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a>MQAGQAKV</p>\n<p>SWAsliding\nwindow attention</p>\n<p>QwenMistralSWA</p>\n<p>Mistral</p>\n<p>Mistral\nAIAI202352023912Mistral\n7BMoEMistral 8x7B</p>\n<p>20242</p>\n<img src=\"/c61d17e3/ms_invest_mistral.png\" class title=\"MS\">\n<p>20242Mistral Large &amp;\n32kMMLUGPT4</p>\n<img src=\"/c61d17e3/mistral_large_performance.jpeg\" class title=\"Mistral Large MMLU Performance\">\n<p>MistralOPENAIMETA</p>\n<h1 id=\"swa\">SWA</h1>\n<p>SWAMistralMistral\n7BSWA</p>\n<h2 id=\"mistral-7b\">Mistral 7B</h2>\n<p>202310MistralMistral 7B<a href=\"https://arxiv.org/pdf/2310.06825.pdf\"></a>LlamaMistralGQASWA</p>\n<p>Mistral 7B</p>\n<img src=\"/c61d17e3/mistral_architechture.png\" class title=\"Mistral Architechture\">\n<p>Mistralkv=8GQAintermediate\nsizeLlama211008</p>\n<h2 id=\"\"></h2>\n<p>causal\nattentiontokentoken</p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n1 <span class=\"math inline\">\\(s^2\\)</span> KV\nCache</p>\n<p>/ <span class=\"math inline\">\\(s\\)</span>\n</p>\n<p>1</p>\n<p> <span class=\"math inline\">\\([m,n]\\times[n,p]\\)</span>  <span class=\"math inline\">\\([m,p]\\)</span> <span class=\"math inline\">\\(m\\times p\\)</span>  <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(2mpn\\)</span> floating point\noperationsFLOPs</p>\n<p><a href=\"https://arxiv.org/pdf/2203.15556.pdf\">Training\nCompute-Optimal Large Language Models</a></p>\n<p>MHA <span class=\"math inline\">\\(s\\)</span>\n <span class=\"math inline\">\\(L\\)</span> hidden size\n<span class=\"math inline\">\\(d_{model}\\)</span>  <span class=\"math inline\">\\(d_{q}\\)</span>   <span class=\"math inline\">\\(n_{q}\\)</span> <span class=\"math inline\">\\(d_{model} = n_{q}\\times d_{q}\\)</span>\noperationFLOPs</p>\n<center>\n<table>\n<colgroup>\n<col style=\"width: 45%\">\n<col style=\"width: 54%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPsMHA</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(6\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: QK logits ( <span class=\"math inline\">\\(QK^T\\)</span> )</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n2\\times s^2\\times h_{q}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Softmax</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n3\\times s^2\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: Reduction (apply to <span class=\"math inline\">\\(V\\)</span>)</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n2\\times s^2\\times h_{q}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Outupt Linear Project</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>Softmax <span class=\"math inline\">\\([1,s]\\)</span>\nsoftmax <span class=\"math inline\">\\(3s\\)</span> \n<span class=\"math inline\">\\(s\\)</span> exp <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\([s,s]\\)</span> softmax <span class=\"math inline\">\\(3s^2\\)</span> \n<span class=\"math inline\">\\(n_{q}\\)</span> </p>\n<p>operationscalingdropout</p>\n<p>Mistral 7BGQA</p>\n<p>KVkv <span class=\"math inline\">\\(n_{kv}\\)</span></p>\n<center>\n<table>\n<colgroup>\n<col style=\"width: 45%\">\n<col style=\"width: 54%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPsGQA</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times\nn_{kv})\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>QK logitsSoftmaxReduction <span class=\"math inline\">\\(s\\)</span> </p>\n<p>2</p>\n<p>KV Cache</p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p>Mistral 7B16kKV_Cache2G</p>\n<p>GQA16k+</p>\n<h2 id=\"swa\">SWA</h2>\n<p>attention <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(s\\)</span>\n</p>\n<p>CNN</p>\n<img src=\"/c61d17e3/receptive_field_cnn.png\" class title=\"CNN Receptive Field\">\n<p>3 <span class=\"math inline\">\\(3\\times 3\\)</span>\nCNNsliding window</p>\n<p>layer 3layer 2 <span class=\"math inline\">\\(3\\times 3\\)</span> layer\n2</p>\n<p>layer 2layer 1 <span class=\"math inline\">\\(3\\times 3\\)</span> layer 2 <span class=\"math inline\">\\(3\\times 3\\)</span> layer 1\n<span class=\"math inline\">\\(5\\times 5\\)</span> layer\n3<u><strong></strong></u> <span class=\"math inline\">\\(5\\times 5\\)</span> </p>\n<p>layer 4layer\n4layer 1  <span class=\"math inline\">\\(7\\times 7\\)</span> </p>\n<p></p>\n<p></p>\n<p>CNN</p>\n<p></p>\n<p></p>\n<p></p>\n<p>MistralSWA</p>\n<img src=\"/c61d17e3/mistral_swa.png\" class title=\"Mistral SWA\">\n<p>causal\nattentionattention\nmask</p>\n<p>SWAattention\nmask33</p>\n<p>CNNLLM</p>\n<p>Mistral 7B409632\n<span class=\"math inline\">\\(4096\\times 32=131,072\\)</span>\n131k</p>\n<p>attentionQK\nlogitsSoftmaxReduction <span class=\"math inline\">\\(s\\)</span>\nSWAoperation4k131k131k\n<span class=\"math inline\">\\(32\\times 32=1024\\)</span> </p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n131k <span class=\"math inline\">\\(31/32\\)</span> </p>\n<p>SWA4kcausal\nattention4k</p>\n<blockquote>\n<p>In practice, for a sequence length of 16K and W = 4096, changes made\nto FlashAttention [11] and xFormers [18] yield a 2x speed improvement\nover a vanilla attention baseline.</p>\n</blockquote>\n<p>MistralSWAFlashAttentionxFormers16k2</p>\n<h2 id=\"kv-cache\">KV Cache</h2>\n<p>sliding windowKV\nCache</p>\n<p>SWAkv</p>\n<p> <span class=\"math inline\">\\(W=4\\)</span>\n5token1tokenkv</p>\n<img src=\"/c61d17e3/rolling_buffer.png\" class title=\"swa rolling buffer\">\n<p>throughputcase</p>\n<h2 id=\"prompt\">Prompt</h2>\n<p>RAGfunciton\ncallprompt</p>\n<p>GPT4system\npromptOPENAI</p>\n<blockquote>\n<p>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\nAppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\nand the user's locale is \"en-US\" Your knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07. Image input capabilities: Enabled</p>\n<p>Tools</p>\n<p>python</p>\n<p>When you send a message containing Python code to python, it will be\nexecuted in a stateful Jupyter notebook environment. python will respond\nwith the output of the execution or time out after 60.0 seconds. The\ndrive at '/mnt/data' can be used to save and persist user files.\nInternet access for this session is disabled. Do not make external web\nrequests or API calls as they will fail.</p>\n<p>dalle</p>\n<p>Whenever a description of an image is given, create a prompt that\ndalle can use to generate the image and abide to the following policy:\n1. The prompt must be in English. Translate to English if needed. 2. DO\nNOT ask for permission to generate the image, just do it! 3. DO NOT list\nor refer to the descriptions before OR after generating the images. 4.\nDo not create more than 1 image, even if the user requests more. 5. Do\nnot create images in the style of artists, creative professionals or\nstudios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n- You can name artists, creative professionals or studios in prompts\nonly if their latest work was created prior to 1912 (e.g. Van Gogh,\nGoya) - If asked to generate an image that would violate this policy,\ninstead apply the following procedure: (a) substitute the artist's name\nwith three adjectives that capture key aspects of the style; (b) include\nan associated artistic movement or era to provide context; and (c)\nmention the primary medium used by the artist 6. For requests to include\nspecific, named private individuals, ask the user to describe what they\nlook like, since you don't know what they look like. 7. For requests to\ncreate images of any public figure referred to by name, create images of\nthose who might resemble them in gender and physique. But they shouldn't\nlook like them. If the reference to the person will only appear as TEXT\nout in the image, then use the reference as is and do not modify it. 8.\nDo not name or directly / indirectly mention or describe copyrighted\ncharacters. Rewrite prompts to describe in detail a specific different\ncharacter with a different specific color, hair style, or other defining\nvisual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around\n100 words long. Example dalle invocation: { \"prompt\":\n\"<insert prompt here>\" } namespace dalle {</insert></p>\n<p>Create images from a text-only prompt. type text2im = (_: { The size\nof the requested image. Use 1024x1024 (square) as the default, 1792x1024\nif the user requests a wide image, and 1024x1792 for full-body\nportraits. Always include this parameter in the request. n?: number, //\ndefault: 2 The detailed image description, potentially modified to abide\nby the dalle policies. If the user requested modifications to a previous\nimage, the prompt should not simply be longer, but rather it should be\nrefactored to integrate the user suggestions. prompt: string, If the\nuser references a previous image, this field should be populated with\nthe gen_id from the dalle image metadata. referenced_image_ids?:\nstring[], }) =&gt; any; } // namespace dalle</p>\n<p>voice_mode Voice mode functions are not available in text\nconversations. namespace voice_mode { } // namespace voice_mode</p>\n<p>browser</p>\n<p>You have the tool <code>browser</code>. Use <code>browser</code> in\nthe following circumstances: - User is asking about current events or\nsomething that requires real-time information (weather, sports scores,\netc.) - User is asking about some term you are totally unfamiliar with\n(it might be new) - User explicitly asks you to browse or provide links\nto references</p>\n<p>Given a query that requires retrieval, your turn will consist of\nthree steps: 1. Call the search function to get a list of results. 2.\nCall the mclick function to retrieve a diverse and high-quality subset\nof these results (in parallel). Remember to SELECT AT LEAST 3 sources\nwhen using <code>mclick</code>. 3. Write a response to the user based on\nthese results. In your response, cite sources using the citation format\nbelow.</p>\n<p>In some cases, you should repeat step 1 twice, if the initial results\nare unsatisfactory, and you believe that you can refine the query to get\nbetter results.</p>\n<p>You can also open a url directly if one is provided by the user. Only\nuse the <code>open_url</code> command for this purpose; do not open urls\nreturned by the search function or found on webpages.</p>\n<p>The <code>browser</code> tool has the following commands:\n<code>search(query: str, recency_days: int)</code> Issues a query to a\nsearch engine and displays the results.\n<code>mclick(ids: list[str])</code>. Retrieves the contents of the\nwebpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST\n3 and at most 10 pages. Select sources with diverse perspectives, and\nprefer trustworthy sources. Because some pages may fail to load, it is\nfine to select some pages for redundancy even if their content might be\nredundant. <code>open_url(url: str)</code> Opens the given URL and\ndisplays it.</p>\n<p>For citing quotes from the 'browser' tool: please render in this\nformat: {message idx}{link text}. For long citations: please render\nin this format: <a href=\"message%20idx\">link text</a>. Otherwise do not\nrender links.</p>\n</blockquote>\n<p>system\npromptkvsystem\npromptsliding windowsystem\npromptkv</p>\n<p> <span class=\"math inline\">\\(W=4\\)</span>system\nprompt9system promptkv [4,4,1] </p>\n<p>windowattention\nmask0</p>\n<p>attention\nmask</p>\n<p></p>\n<p></p>\n<p>prompt</p>\n<img src=\"/c61d17e3/prefill_and_chunking.png\" class title=\"prefill and chunking\">\n<p>FlashAttention/PagedAttention</p>\n<p>Mistral\n7BLlamaLlama\n34B</p>\n<img src=\"/c61d17e3/mistral_perf.png\" class title=\"mistral performance\">\n<p>Mistral7B</p>\n<h1 id=\"sparse-attention\">Sparse Attention</h1>\n<p>SWAsparse attentionsparse\nattention</p>\n<p>sparse\nattention</p>\n<h2 id=\"longformer\">Longformer</h2>\n<p>MistralSWA</p>\n<p>2020<a href=\"https://arxiv.org/pdf/2004.05150.pdf\">Longformer:\nThe Long-Document Transformer</a>SWAsparse\nattention</p>\n<p>Longformer</p>\n<img src=\"/c61d17e3/longformer_attention.png\" class title=\"longformer\">\n<p>bSWABert</p>\n<p>SWAdilated sliding\nwindow</p>\n<img src=\"/c61d17e3/dilated_conv.png\" class title=\"dilated convolution\">\n<p>attentionSWAdilated sliding\nwindow</p>\n<p></p>\n<p>Bert[CLS]\ntokentoken</p>\n<p>GPTinstructionprompt</p>\n<p>tokenglobal\nattentionsliding windowd</p>\n<h2 id=\"big-bird\">Big Bird</h2>\n<p>2020Longformersparse\nattention<a href=\"https://arxiv.org/abs/2007.14062\">Big Bird: Transformers for\nLonger Sequences</a></p>\n<p>sliding windowglobal attentionLongformerBig\nBirdrandom attention</p>\n<img src=\"/c61d17e3/big_bird_attention.png\" class title=\"big bird attention\">\n<p> <span class=\"math inline\">\\(r=2\\)</span>\n2</p>\n<h1 id=\"\"></h1>\n<p>SWA</p>\n<p>SWAsparse\nattention<big><strong></strong></big>global\n+ local attentionflash attentionrandom\nattention</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Mistral 7B https://arxiv.org/pdf/2310.06825.pdf<br>\n2Longformer: The Long-Document Transformer\nhttps://arxiv.org/pdf/2004.05150.pdf<br>\n3Training Compute-Optimal Large Language Models\nhttps://arxiv.org/pdf/2203.15556.pdf<br>\n4GPT-4 System Prompt Revealed\nhttps://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed<br>\n5Big Bird: Transformers for Longer Sequences\nhttps://arxiv.org/abs/2007.14062</p>\n","length":10783,"excerpt":"","more":"<p>//</p>\n<p>LLM</p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a>32k+/128k+<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a>MQAGQAKV</p>\n<p>SWAsliding\nwindow attention</p>\n<p>QwenMistralSWA</p>\n<p>Mistral</p>\n<p>Mistral\nAIAI202352023912Mistral\n7BMoEMistral 8x7B</p>\n<p>20242</p>\n<img src=\"/c61d17e3/ms_invest_mistral.png\" class title=\"MS\">\n<p>20242Mistral Large &amp;\n32kMMLUGPT4</p>\n<img src=\"/c61d17e3/mistral_large_performance.jpeg\" class title=\"Mistral Large MMLU Performance\">\n<p>MistralOPENAIMETA</p>\n<h1 id=\"swa\">SWA</h1>\n<p>SWAMistralMistral\n7BSWA</p>\n<h2 id=\"mistral-7b\">Mistral 7B</h2>\n<p>202310MistralMistral 7B<a href=\"https://arxiv.org/pdf/2310.06825.pdf\"></a>LlamaMistralGQASWA</p>\n<p>Mistral 7B</p>\n<img src=\"/c61d17e3/mistral_architechture.png\" class title=\"Mistral Architechture\">\n<p>Mistralkv=8GQAintermediate\nsizeLlama211008</p>\n<h2 id=\"\"></h2>\n<p>causal\nattentiontokentoken</p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n1 <span class=\"math inline\">\\(s^2\\)</span> KV\nCache</p>\n<p>/ <span class=\"math inline\">\\(s\\)</span>\n</p>\n<p>1</p>\n<p> <span class=\"math inline\">\\([m,n]\\times[n,p]\\)</span>  <span class=\"math inline\">\\([m,p]\\)</span> <span class=\"math inline\">\\(m\\times p\\)</span>  <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(2mpn\\)</span> floating point\noperationsFLOPs</p>\n<p><a href=\"https://arxiv.org/pdf/2203.15556.pdf\">Training\nCompute-Optimal Large Language Models</a></p>\n<p>MHA <span class=\"math inline\">\\(s\\)</span>\n <span class=\"math inline\">\\(L\\)</span> hidden size\n<span class=\"math inline\">\\(d_{model}\\)</span>  <span class=\"math inline\">\\(d_{q}\\)</span>   <span class=\"math inline\">\\(n_{q}\\)</span> <span class=\"math inline\">\\(d_{model} = n_{q}\\times d_{q}\\)</span>\noperationFLOPs</p>\n<center>\n<table>\n<colgroup>\n<col style=\"width: 45%\">\n<col style=\"width: 54%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPsMHA</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(6\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: QK logits ( <span class=\"math inline\">\\(QK^T\\)</span> )</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n2\\times s^2\\times h_{q}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Softmax</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n3\\times s^2\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Attention: Reduction (apply to <span class=\"math inline\">\\(V\\)</span>)</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(n_{q}\\times\n2\\times s^2\\times h_{q}\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: Outupt Linear Project</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>Softmax <span class=\"math inline\">\\([1,s]\\)</span>\nsoftmax <span class=\"math inline\">\\(3s\\)</span> \n<span class=\"math inline\">\\(s\\)</span> exp <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\([s,s]\\)</span> softmax <span class=\"math inline\">\\(3s^2\\)</span> \n<span class=\"math inline\">\\(n_{q}\\)</span> </p>\n<p>operationscalingdropout</p>\n<p>Mistral 7BGQA</p>\n<p>KVkv <span class=\"math inline\">\\(n_{kv}\\)</span></p>\n<center>\n<table>\n<colgroup>\n<col style=\"width: 45%\">\n<col style=\"width: 54%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Operation</th>\n<th style=\"text-align: center;\">FLOPsGQA</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Attention: QKV</td>\n<td style=\"text-align: center;\"><span class=\"math inline\">\\(2\\times\ns\\times h_{model}^{2}\\\\+4\\times s\\times h_{model}\\times (h_{q}\\times\nn_{kv})\\)</span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>QK logitsSoftmaxReduction <span class=\"math inline\">\\(s\\)</span> </p>\n<p>2</p>\n<p>KV Cache</p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d_{q}\\times n_{kv}\n\\]</span></p>\n<p>Mistral 7B16kKV_Cache2G</p>\n<p>GQA16k+</p>\n<h2 id=\"swa\">SWA</h2>\n<p>attention <span class=\"math inline\">\\(s\\)</span>  <span class=\"math inline\">\\(s\\)</span>\n</p>\n<p>CNN</p>\n<img src=\"/c61d17e3/receptive_field_cnn.png\" class title=\"CNN Receptive Field\">\n<p>3 <span class=\"math inline\">\\(3\\times 3\\)</span>\nCNNsliding window</p>\n<p>layer 3layer 2 <span class=\"math inline\">\\(3\\times 3\\)</span> layer\n2</p>\n<p>layer 2layer 1 <span class=\"math inline\">\\(3\\times 3\\)</span> layer 2 <span class=\"math inline\">\\(3\\times 3\\)</span> layer 1\n<span class=\"math inline\">\\(5\\times 5\\)</span> layer\n3<u><strong></strong></u> <span class=\"math inline\">\\(5\\times 5\\)</span> </p>\n<p>layer 4layer\n4layer 1  <span class=\"math inline\">\\(7\\times 7\\)</span> </p>\n<p></p>\n<p></p>\n<p>CNN</p>\n<p></p>\n<p></p>\n<p></p>\n<p>MistralSWA</p>\n<img src=\"/c61d17e3/mistral_swa.png\" class title=\"Mistral SWA\">\n<p>causal\nattentionattention\nmask</p>\n<p>SWAattention\nmask33</p>\n<p>CNNLLM</p>\n<p>Mistral 7B409632\n<span class=\"math inline\">\\(4096\\times 32=131,072\\)</span>\n131k</p>\n<p>attentionQK\nlogitsSoftmaxReduction <span class=\"math inline\">\\(s\\)</span>\nSWAoperation4k131k131k\n<span class=\"math inline\">\\(32\\times 32=1024\\)</span> </p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n131k <span class=\"math inline\">\\(31/32\\)</span> </p>\n<p>SWA4kcausal\nattention4k</p>\n<blockquote>\n<p>In practice, for a sequence length of 16K and W = 4096, changes made\nto FlashAttention [11] and xFormers [18] yield a 2x speed improvement\nover a vanilla attention baseline.</p>\n</blockquote>\n<p>MistralSWAFlashAttentionxFormers16k2</p>\n<h2 id=\"kv-cache\">KV Cache</h2>\n<p>sliding windowKV\nCache</p>\n<p>SWAkv</p>\n<p> <span class=\"math inline\">\\(W=4\\)</span>\n5token1tokenkv</p>\n<img src=\"/c61d17e3/rolling_buffer.png\" class title=\"swa rolling buffer\">\n<p>throughputcase</p>\n<h2 id=\"prompt\">Prompt</h2>\n<p>RAGfunciton\ncallprompt</p>\n<p>GPT4system\npromptOPENAI</p>\n<blockquote>\n<p>Your user's user agent is \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\nAppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\nand the user's locale is \"en-US\" Your knowledge cutoff date is 2023-04.\nThe current date is 2024-02-07. Image input capabilities: Enabled</p>\n<p>Tools</p>\n<p>python</p>\n<p>When you send a message containing Python code to python, it will be\nexecuted in a stateful Jupyter notebook environment. python will respond\nwith the output of the execution or time out after 60.0 seconds. The\ndrive at '/mnt/data' can be used to save and persist user files.\nInternet access for this session is disabled. Do not make external web\nrequests or API calls as they will fail.</p>\n<p>dalle</p>\n<p>Whenever a description of an image is given, create a prompt that\ndalle can use to generate the image and abide to the following policy:\n1. The prompt must be in English. Translate to English if needed. 2. DO\nNOT ask for permission to generate the image, just do it! 3. DO NOT list\nor refer to the descriptions before OR after generating the images. 4.\nDo not create more than 1 image, even if the user requests more. 5. Do\nnot create images in the style of artists, creative professionals or\nstudios whose latest work was created after 1912 (e.g. Picasso, Kahlo).\n- You can name artists, creative professionals or studios in prompts\nonly if their latest work was created prior to 1912 (e.g. Van Gogh,\nGoya) - If asked to generate an image that would violate this policy,\ninstead apply the following procedure: (a) substitute the artist's name\nwith three adjectives that capture key aspects of the style; (b) include\nan associated artistic movement or era to provide context; and (c)\nmention the primary medium used by the artist 6. For requests to include\nspecific, named private individuals, ask the user to describe what they\nlook like, since you don't know what they look like. 7. For requests to\ncreate images of any public figure referred to by name, create images of\nthose who might resemble them in gender and physique. But they shouldn't\nlook like them. If the reference to the person will only appear as TEXT\nout in the image, then use the reference as is and do not modify it. 8.\nDo not name or directly / indirectly mention or describe copyrighted\ncharacters. Rewrite prompts to describe in detail a specific different\ncharacter with a different specific color, hair style, or other defining\nvisual characteristic. Do not discuss copyright policies in responses.\nThe generated prompt sent to dalle should be very detailed, and around\n100 words long. Example dalle invocation: { \"prompt\":\n\"<insert prompt here>\" } namespace dalle {</insert></p>\n<p>Create images from a text-only prompt. type text2im = (_: { The size\nof the requested image. Use 1024x1024 (square) as the default, 1792x1024\nif the user requests a wide image, and 1024x1792 for full-body\nportraits. Always include this parameter in the request. n?: number, //\ndefault: 2 The detailed image description, potentially modified to abide\nby the dalle policies. If the user requested modifications to a previous\nimage, the prompt should not simply be longer, but rather it should be\nrefactored to integrate the user suggestions. prompt: string, If the\nuser references a previous image, this field should be populated with\nthe gen_id from the dalle image metadata. referenced_image_ids?:\nstring[], }) =&gt; any; } // namespace dalle</p>\n<p>voice_mode Voice mode functions are not available in text\nconversations. namespace voice_mode { } // namespace voice_mode</p>\n<p>browser</p>\n<p>You have the tool <code>browser</code>. Use <code>browser</code> in\nthe following circumstances: - User is asking about current events or\nsomething that requires real-time information (weather, sports scores,\netc.) - User is asking about some term you are totally unfamiliar with\n(it might be new) - User explicitly asks you to browse or provide links\nto references</p>\n<p>Given a query that requires retrieval, your turn will consist of\nthree steps: 1. Call the search function to get a list of results. 2.\nCall the mclick function to retrieve a diverse and high-quality subset\nof these results (in parallel). Remember to SELECT AT LEAST 3 sources\nwhen using <code>mclick</code>. 3. Write a response to the user based on\nthese results. In your response, cite sources using the citation format\nbelow.</p>\n<p>In some cases, you should repeat step 1 twice, if the initial results\nare unsatisfactory, and you believe that you can refine the query to get\nbetter results.</p>\n<p>You can also open a url directly if one is provided by the user. Only\nuse the <code>open_url</code> command for this purpose; do not open urls\nreturned by the search function or found on webpages.</p>\n<p>The <code>browser</code> tool has the following commands:\n<code>search(query: str, recency_days: int)</code> Issues a query to a\nsearch engine and displays the results.\n<code>mclick(ids: list[str])</code>. Retrieves the contents of the\nwebpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST\n3 and at most 10 pages. Select sources with diverse perspectives, and\nprefer trustworthy sources. Because some pages may fail to load, it is\nfine to select some pages for redundancy even if their content might be\nredundant. <code>open_url(url: str)</code> Opens the given URL and\ndisplays it.</p>\n<p>For citing quotes from the 'browser' tool: please render in this\nformat: {message idx}{link text}. For long citations: please render\nin this format: <a href=\"message%20idx\">link text</a>. Otherwise do not\nrender links.</p>\n</blockquote>\n<p>system\npromptkvsystem\npromptsliding windowsystem\npromptkv</p>\n<p> <span class=\"math inline\">\\(W=4\\)</span>system\nprompt9system promptkv [4,4,1] </p>\n<p>windowattention\nmask0</p>\n<p>attention\nmask</p>\n<p></p>\n<p></p>\n<p>prompt</p>\n<img src=\"/c61d17e3/prefill_and_chunking.png\" class title=\"prefill and chunking\">\n<p>FlashAttention/PagedAttention</p>\n<p>Mistral\n7BLlamaLlama\n34B</p>\n<img src=\"/c61d17e3/mistral_perf.png\" class title=\"mistral performance\">\n<p>Mistral7B</p>\n<h1 id=\"sparse-attention\">Sparse Attention</h1>\n<p>SWAsparse attentionsparse\nattention</p>\n<p>sparse\nattention</p>\n<h2 id=\"longformer\">Longformer</h2>\n<p>MistralSWA</p>\n<p>2020<a href=\"https://arxiv.org/pdf/2004.05150.pdf\">Longformer:\nThe Long-Document Transformer</a>SWAsparse\nattention</p>\n<p>Longformer</p>\n<img src=\"/c61d17e3/longformer_attention.png\" class title=\"longformer\">\n<p>bSWABert</p>\n<p>SWAdilated sliding\nwindow</p>\n<img src=\"/c61d17e3/dilated_conv.png\" class title=\"dilated convolution\">\n<p>attentionSWAdilated sliding\nwindow</p>\n<p></p>\n<p>Bert[CLS]\ntokentoken</p>\n<p>GPTinstructionprompt</p>\n<p>tokenglobal\nattentionsliding windowd</p>\n<h2 id=\"big-bird\">Big Bird</h2>\n<p>2020Longformersparse\nattention<a href=\"https://arxiv.org/abs/2007.14062\">Big Bird: Transformers for\nLonger Sequences</a></p>\n<p>sliding windowglobal attentionLongformerBig\nBirdrandom attention</p>\n<img src=\"/c61d17e3/big_bird_attention.png\" class title=\"big bird attention\">\n<p> <span class=\"math inline\">\\(r=2\\)</span>\n2</p>\n<h1 id=\"\"></h1>\n<p>SWA</p>\n<p>SWAsparse\nattention<big><strong></strong></big>global\n+ local attentionflash attentionrandom\nattention</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Mistral 7B https://arxiv.org/pdf/2310.06825.pdf<br>\n2Longformer: The Long-Document Transformer\nhttps://arxiv.org/pdf/2004.05150.pdf<br>\n3Training Compute-Optimal Large Language Models\nhttps://arxiv.org/pdf/2203.15556.pdf<br>\n4GPT-4 System Prompt Revealed\nhttps://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed<br>\n5Big Bird: Transformers for Longer Sequences\nhttps://arxiv.org/abs/2007.14062</p>\n"},{"title":"transformernormalization","abbrlink":"6a40bfa5","date":"2024-03-19T13:06:12.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nNormalizationattention  \n\nnormalizationtransformer  \n\n  \n\n# why normalization\n\nnormalization  \n\n  \n\n## normalization\n\n $Loss(x_1,x_2)=x_1^2+x_2^2+b$   \n\n{% asset_img lossfunc_surface.jpeg loss function surface %}  \n\n  \n\n  \n\nminimum  \n\n  \n\n{% asset_img ellipse_1.png ellipse %}  \n\n  \n\n  \n\n{% asset_img ellipse_2.png ellipse %}  \n\n  \n\n  \n\n $x_{1}$ $x_{2}$->  \n\n0.x~2.x0  \n\n  \n\n $x_{2}$   \n\n  \n\n  \n\nnormalization  \n\nnormalization  \n$$x^{\\prime}=\\frac{x-\\min(\\mathrm{x})}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}$$  \n\n$$x^{\\prime}=\\frac{x-\\mu}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}$$  \n\n$$x^{\\prime}=\\frac{x-\\mu}\\sigma$$  \n\n $\\mu$ $\\sigma$   \n\nZ-score normalization  \n\n  \n\nPCA  \n\n## ICS...\n\ni.i.d.independent and identical distribution  \n\ni.i.d.  \n\n  \n\n  \n\n  \n\ni.i.d.i.i.d.  \n\nPCAi.i.d.  \n\n  \n\ni.i.d.  \n\nICSinternal covariate shift  \n\n  \n\nnormalization  \n\nnormalizationbatchnormICSbatchnorm  \n\nICS  \n\n## \n\nsigmoid\n\n{% asset_img sigmoid.png sigmoid %}  \n\n > 6  < -6 sigmoid  \n\n  \n\nICSnormalization  \n\nnormalization  \n\n# batchnorm\n\nnormalizationbatchnormlayernorm  \n\n## batchnorm\n\n $[B,C]$  $B$ batch size$C$   \n\n $C$ normalization $C$   \n\n $i$ batch  \n\n$$\n\\mu_{i}=\\frac{1}{B}\\sum_{j=1}^{B}x_{i,j}\n$$  \n\n  \n\n$$\n\\sigma_{i}^{2}=\\frac{1}{B}\\sum_{j=1}^{B}(x_{i,j}-\\mu_{i})^2\n$$  \n\nbatchZ-score normalization  \n\n$$\nx_{i,j}'=\\frac{x_{i,j}-\\mu_{i}}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\n$$  \n\n $\\epsilon$ 0  \n\n $C$ 01  \n\n  \n\nsigmoid  \n\n  \n\n  $\\gamma$  $\\beta$   \n\n$$\ny_{i,j} = \\gamma_{i} x_{i,j}' + \\beta_{i}\n$$  \n\n  \n\nbatchnorm $\\gamma$  $\\beta$ \n\n[Batch Normalization: Accelerating Deep Network](https://zhuanlan.zhihu.com/p/340856414)  \n\n{% asset_img bn_algo.png batch norm %}  \n\n## CNNbatchnorm  \n\nbatchnormCNN  \n\nCNNfeature mapsize $[B,C,H,W]$  $B$ batch size$C$ channel$H$  $W$   \n\nbatchnorm $C\\times H\\times W$  $B$  $B$   \n\nCNNchannel $H\\times W$  $H\\times W$ batch  \n\nbatch $B\\times H\\times W$  $C$  $C$  $\\gamma$  $\\beta$   \n\nbatchnormbatchnormrelufcbatchnormfcbias  \n\nbtwbatchnorm $\\gamma$ 1 $\\beta$ 0batchnorm  \n\n##   \n\nbatchnormmini-batch  \n\n$\\gamma$  $\\beta$   \n\nsamplesamplebatch  \n\ni.i.d.  \n\n\n\nmoving_mean = momentum  moving_mean + (1.0  momentum)  mean  \n\nmoving_var = momentum  moving_var + (1.0  momentum)  var  \n\nbatch  \n\nmomentum TF/Keras 0.99 Pytorch 0.9  \n\nmomentum  \n \nmomentum  \n\nbatch sizemini batchmomentum  \n\nbatch\n\nbatch sizemini batchbatchnormbatch size  \n\n{% asset_img bs_bn.png batch size %}  \n\nbatchnorm    \n\nbatch\n\nmomentumbatch sizebatch size\n\n## batchnorm\n\nbatchnormbatchnormICS2018How Does Batch Normalization Help Optimization?  \n\nbatchnormICSICSbatchnormcovariate shiftbatchnorm  \n\n{% asset_img bn_ics.png ICS %}  \n\nICSbatchnormICS  \n\nICSbatchnorm  \n\nbatchnormICS  \n\nICSICS  \n\nICS\n\n{% asset_img ics_define.png ICS  %}  \n\niL2\n\n$G_{t,i}$ t  \n\n$G_{t,i}'$ t  \n\n  \n\nICS\n\n{% asset_img ics_measure.png ICS measure %}  \n\nbatchnorm  \n\nbatchnormICS  \n\nbatchnorm  \n\nbatchnorm  \n\n# layernorm\n\nbatchnormlayernorm  \n\n## layernorm\n\nlayernormlayerlayer  \n\nbatchnormbatchlayernorm  \n\n{% asset_img bn_and_ln.png bnln %}  \n\nNLPbatchnormlayernormbatchnormlayernorm  \n\nlayernormNLPRNNtransfomrer  \n\ntransformer $[B,S,H]$ $S$ paddingzero-paddingbatch  \n\n\n\n```\n      \n  \n    \n```\n\npad\n\n```\n      \n    [P] [P]\n      [P]\n```\n\npaddingbatchnorm  \n\n[P] [P] batch size2 [P] normalization  \n\nbatch  \n\n [P] token  \n\nlayernorm $H$ normalization $H$ $\\gamma$  $\\beta$   \n\ntoken  \n\n## transformerlayernorm  \n\nbatchnormlayernormbatch  \n\nlayernormbatchbatch size  \n\nnlplayernormbatchnorm  \n\nPowerNorm: Rethinking Batch Normalization in TransformerstransformerBN  \n\nbatchbatchrunning statisticsNLPIWSLT14batchrunning statisticsCV  \n\nmagnitudeCV  \n\ntransformerBNCVNLPNLPbatch  \n\nlayernormNLPbatchnorm  \n\n## RMSnorm\n\n19Root Mean Square Layer NormalizationnormalizationRMSnormlayernorm  \n\nRMSnormlayernorm  \n\n{% asset_img rmsnorm.png RMSnorm %}  \n\nlayernorm  \n\n{% asset_img rmsnorm_eff.png RMSnorm %}  \n\nGRUlayernormlayernormlayernorm  \n\nlayernorm  \n\npRMSnormp%  \n\n{% asset_img prmsnorm.png prmsnorm %}  \n\nRMSnorm  \n\n# post-norm & pre-norm\n\n## \nlayernorm  \n\ntransformerpost-normOn Layer Normalization in the Transformer Architecturepre-norm  \n\npost-normpre-norm\n\n{% asset_img postnorm_prenorm.png postnorm and prenorm %}  \n\npost-normpre-norm  \n\npost-normpre-normpre-normpost-normpre-norm  \n\n $l$  $l+1$ post-norm  \n\n$$\nx_{l+1}=\\mathrm{Norm}(x_l+\\mathrm{F}_t(x_l))\n$$\n\npre-norm  \n\n$$\nx_{l+1}=x_l+\\mathrm{F}_l(\\mathrm{Norm}(x_l))\n$$\n\nPre NormPost Norm $\\mathrm{F}_l(\\mathrm{Norm}(x_l))$ norm  \n\n $l$ $x_{l}x_{l+1}$  $\\mathrm{F}_l(\\mathrm{Norm}(x_l))$  $\\mathrm{F}_{l+1}(\\mathrm{Norm}(x_{l+1}))$   \n\n$$\\begin{aligned}\n&\\mathrm{F}_l(\\operatorname{Norm}(x_l))+\\operatorname{F}_{l+1}(\\operatorname{Norm}(x_{l+1})) \\\\\n&{\\approx}\\mathrm{F}_l(\\mathrm{Norm}(x_l))+\\mathrm{F}_{l+1}\\left(\\mathrm{Norm}(x_l)\\right) \\\\\n&=(\\mathrm{F}_l\\oplus\\mathrm{F}_{l+1})(\\mathrm{Norm}(\\pmb{x_l}))\n\\end{aligned}$$\n\n $l$  $l+1$  $l$   \n\npre-norm $l$ post-norm  \n\nnormalizationpre-norm--  \n\n  \n\npost-normlossnormpost-normpre-norm  \n\n## warmup  \n\nOn Layer Normalization in the Transformer Architecturepre-normpost-normtransformerwarmup  \n\nwarmup  \n\npre-normtransformerwarmuppost-norm+warmuppost-normwarmup  \n\n{% asset_img warmup_effect.png warmup %}  \n\n## Deepnorm  \n\n2022DeepNet: Scaling Transformers to 1,000 Layerstransformer  \n\nDeepnorm  \n\n{% asset_img deepnorm.png deepnorm %}  \n\n $\\alpha>1$ post-norm  \n\n $\\beta$  $G_{l}$   \n\ndeepnormpre-normpost-norm  \n\n{% asset_img deepnorm_result.png deepnorm result %}  \n\npost-norm  \n\n## Realformer--residual attention  \n\npost-normpre-normRealFormer: Transformer Likes Residual Attention  \n\n{% asset_img realformer.png realformer %}  \n\nRealFormerTransformerSoftmax\n\n\n\n{% asset_img realformer_attention.png realformer attention %}  \n\n $Prev'$ softmax $\\frac{Q^{\\prime}K^{\\prime T}}{\\sqrt{d_k}}+Prev'$ attention  \n\n# \n\nnormalizationbatchnormlayernormtransformer\n\nrmsnorm + prenorm  \n\nnormalization    \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n***\n\n# Reference  \n1https://www.zhihu.com/question/487766088  \n2Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization https://arxiv.org/abs/2001.06838  \n3Transformer()& https://zhuanlan.zhihu.com/p/476102712  \n4Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://arxiv.org/abs/1502.03167  \n5How Does Batch Normalization Help Optimization? https://arxiv.org/abs/1805.11604  \n6Batch Normalization: Accelerating Deep Network https://zhuanlan.zhihu.com/p/340856414  \n7Layer Normalization https://arxiv.org/abs/1607.06450  \n8NormalizationBN/LN/WN https://zhuanlan.zhihu.com/p/33173246  \n9Transformer()BatchNormalization https://zhuanlan.zhihu.com/p/481277619  \n10Layer Normalization https://arxiv.org/abs/1607.06450  \n11PowerNorm: Rethinking Batch Normalization in Transformers https://arxiv.org/abs/2003.07845  \n12Root Mean Square Layer Normalization https://arxiv.org/abs/1910.07467  \n13On Layer Normalization in the Transformer Architecture https://arxiv.org/abs/2002.04745  \n14Pre NormPost Norm https://spaces.ac.cn/archives/9009  \n15Understanding the Difficulty of Training Transformers https://arxiv.org/abs/2004.08249  \n16RealFormer: Transformer Likes Residual Attention https://arxiv.org/abs/2012.11747  \n17DeepNet: Scaling Transformers to 1,000 Layers https://arxiv.org/abs/2203.00555  \n\n","source":"_posts/cs/nlp/2024/03/Transformernormalization.md","raw":"---\ntitle: transformernormalization\nabbrlink: 6a40bfa5\ndate: 2024-03-19 21:06:12\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - layernorm\n  - post-norm\n  - pre-norm\n  - normalization\n  - batchnorm\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nNormalizationattention  \n\nnormalizationtransformer  \n\n  \n\n# why normalization\n\nnormalization  \n\n  \n\n## normalization\n\n $Loss(x_1,x_2)=x_1^2+x_2^2+b$   \n\n{% asset_img lossfunc_surface.jpeg loss function surface %}  \n\n  \n\n  \n\nminimum  \n\n  \n\n{% asset_img ellipse_1.png ellipse %}  \n\n  \n\n  \n\n{% asset_img ellipse_2.png ellipse %}  \n\n  \n\n  \n\n $x_{1}$ $x_{2}$->  \n\n0.x~2.x0  \n\n  \n\n $x_{2}$   \n\n  \n\n  \n\nnormalization  \n\nnormalization  \n$$x^{\\prime}=\\frac{x-\\min(\\mathrm{x})}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}$$  \n\n$$x^{\\prime}=\\frac{x-\\mu}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}$$  \n\n$$x^{\\prime}=\\frac{x-\\mu}\\sigma$$  \n\n $\\mu$ $\\sigma$   \n\nZ-score normalization  \n\n  \n\nPCA  \n\n## ICS...\n\ni.i.d.independent and identical distribution  \n\ni.i.d.  \n\n  \n\n  \n\n  \n\ni.i.d.i.i.d.  \n\nPCAi.i.d.  \n\n  \n\ni.i.d.  \n\nICSinternal covariate shift  \n\n  \n\nnormalization  \n\nnormalizationbatchnormICSbatchnorm  \n\nICS  \n\n## \n\nsigmoid\n\n{% asset_img sigmoid.png sigmoid %}  \n\n > 6  < -6 sigmoid  \n\n  \n\nICSnormalization  \n\nnormalization  \n\n# batchnorm\n\nnormalizationbatchnormlayernorm  \n\n## batchnorm\n\n $[B,C]$  $B$ batch size$C$   \n\n $C$ normalization $C$   \n\n $i$ batch  \n\n$$\n\\mu_{i}=\\frac{1}{B}\\sum_{j=1}^{B}x_{i,j}\n$$  \n\n  \n\n$$\n\\sigma_{i}^{2}=\\frac{1}{B}\\sum_{j=1}^{B}(x_{i,j}-\\mu_{i})^2\n$$  \n\nbatchZ-score normalization  \n\n$$\nx_{i,j}'=\\frac{x_{i,j}-\\mu_{i}}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\n$$  \n\n $\\epsilon$ 0  \n\n $C$ 01  \n\n  \n\nsigmoid  \n\n  \n\n  $\\gamma$  $\\beta$   \n\n$$\ny_{i,j} = \\gamma_{i} x_{i,j}' + \\beta_{i}\n$$  \n\n  \n\nbatchnorm $\\gamma$  $\\beta$ \n\n[Batch Normalization: Accelerating Deep Network](https://zhuanlan.zhihu.com/p/340856414)  \n\n{% asset_img bn_algo.png batch norm %}  \n\n## CNNbatchnorm  \n\nbatchnormCNN  \n\nCNNfeature mapsize $[B,C,H,W]$  $B$ batch size$C$ channel$H$  $W$   \n\nbatchnorm $C\\times H\\times W$  $B$  $B$   \n\nCNNchannel $H\\times W$  $H\\times W$ batch  \n\nbatch $B\\times H\\times W$  $C$  $C$  $\\gamma$  $\\beta$   \n\nbatchnormbatchnormrelufcbatchnormfcbias  \n\nbtwbatchnorm $\\gamma$ 1 $\\beta$ 0batchnorm  \n\n##   \n\nbatchnormmini-batch  \n\n$\\gamma$  $\\beta$   \n\nsamplesamplebatch  \n\ni.i.d.  \n\n\n\nmoving_mean = momentum  moving_mean + (1.0  momentum)  mean  \n\nmoving_var = momentum  moving_var + (1.0  momentum)  var  \n\nbatch  \n\nmomentum TF/Keras 0.99 Pytorch 0.9  \n\nmomentum  \n \nmomentum  \n\nbatch sizemini batchmomentum  \n\nbatch\n\nbatch sizemini batchbatchnormbatch size  \n\n{% asset_img bs_bn.png batch size %}  \n\nbatchnorm    \n\nbatch\n\nmomentumbatch sizebatch size\n\n## batchnorm\n\nbatchnormbatchnormICS2018How Does Batch Normalization Help Optimization?  \n\nbatchnormICSICSbatchnormcovariate shiftbatchnorm  \n\n{% asset_img bn_ics.png ICS %}  \n\nICSbatchnormICS  \n\nICSbatchnorm  \n\nbatchnormICS  \n\nICSICS  \n\nICS\n\n{% asset_img ics_define.png ICS  %}  \n\niL2\n\n$G_{t,i}$ t  \n\n$G_{t,i}'$ t  \n\n  \n\nICS\n\n{% asset_img ics_measure.png ICS measure %}  \n\nbatchnorm  \n\nbatchnormICS  \n\nbatchnorm  \n\nbatchnorm  \n\n# layernorm\n\nbatchnormlayernorm  \n\n## layernorm\n\nlayernormlayerlayer  \n\nbatchnormbatchlayernorm  \n\n{% asset_img bn_and_ln.png bnln %}  \n\nNLPbatchnormlayernormbatchnormlayernorm  \n\nlayernormNLPRNNtransfomrer  \n\ntransformer $[B,S,H]$ $S$ paddingzero-paddingbatch  \n\n\n\n```\n      \n  \n    \n```\n\npad\n\n```\n      \n    [P] [P]\n      [P]\n```\n\npaddingbatchnorm  \n\n[P] [P] batch size2 [P] normalization  \n\nbatch  \n\n [P] token  \n\nlayernorm $H$ normalization $H$ $\\gamma$  $\\beta$   \n\ntoken  \n\n## transformerlayernorm  \n\nbatchnormlayernormbatch  \n\nlayernormbatchbatch size  \n\nnlplayernormbatchnorm  \n\nPowerNorm: Rethinking Batch Normalization in TransformerstransformerBN  \n\nbatchbatchrunning statisticsNLPIWSLT14batchrunning statisticsCV  \n\nmagnitudeCV  \n\ntransformerBNCVNLPNLPbatch  \n\nlayernormNLPbatchnorm  \n\n## RMSnorm\n\n19Root Mean Square Layer NormalizationnormalizationRMSnormlayernorm  \n\nRMSnormlayernorm  \n\n{% asset_img rmsnorm.png RMSnorm %}  \n\nlayernorm  \n\n{% asset_img rmsnorm_eff.png RMSnorm %}  \n\nGRUlayernormlayernormlayernorm  \n\nlayernorm  \n\npRMSnormp%  \n\n{% asset_img prmsnorm.png prmsnorm %}  \n\nRMSnorm  \n\n# post-norm & pre-norm\n\n## \nlayernorm  \n\ntransformerpost-normOn Layer Normalization in the Transformer Architecturepre-norm  \n\npost-normpre-norm\n\n{% asset_img postnorm_prenorm.png postnorm and prenorm %}  \n\npost-normpre-norm  \n\npost-normpre-normpre-normpost-normpre-norm  \n\n $l$  $l+1$ post-norm  \n\n$$\nx_{l+1}=\\mathrm{Norm}(x_l+\\mathrm{F}_t(x_l))\n$$\n\npre-norm  \n\n$$\nx_{l+1}=x_l+\\mathrm{F}_l(\\mathrm{Norm}(x_l))\n$$\n\nPre NormPost Norm $\\mathrm{F}_l(\\mathrm{Norm}(x_l))$ norm  \n\n $l$ $x_{l}x_{l+1}$  $\\mathrm{F}_l(\\mathrm{Norm}(x_l))$  $\\mathrm{F}_{l+1}(\\mathrm{Norm}(x_{l+1}))$   \n\n$$\\begin{aligned}\n&\\mathrm{F}_l(\\operatorname{Norm}(x_l))+\\operatorname{F}_{l+1}(\\operatorname{Norm}(x_{l+1})) \\\\\n&{\\approx}\\mathrm{F}_l(\\mathrm{Norm}(x_l))+\\mathrm{F}_{l+1}\\left(\\mathrm{Norm}(x_l)\\right) \\\\\n&=(\\mathrm{F}_l\\oplus\\mathrm{F}_{l+1})(\\mathrm{Norm}(\\pmb{x_l}))\n\\end{aligned}$$\n\n $l$  $l+1$  $l$   \n\npre-norm $l$ post-norm  \n\nnormalizationpre-norm--  \n\n  \n\npost-normlossnormpost-normpre-norm  \n\n## warmup  \n\nOn Layer Normalization in the Transformer Architecturepre-normpost-normtransformerwarmup  \n\nwarmup  \n\npre-normtransformerwarmuppost-norm+warmuppost-normwarmup  \n\n{% asset_img warmup_effect.png warmup %}  \n\n## Deepnorm  \n\n2022DeepNet: Scaling Transformers to 1,000 Layerstransformer  \n\nDeepnorm  \n\n{% asset_img deepnorm.png deepnorm %}  \n\n $\\alpha>1$ post-norm  \n\n $\\beta$  $G_{l}$   \n\ndeepnormpre-normpost-norm  \n\n{% asset_img deepnorm_result.png deepnorm result %}  \n\npost-norm  \n\n## Realformer--residual attention  \n\npost-normpre-normRealFormer: Transformer Likes Residual Attention  \n\n{% asset_img realformer.png realformer %}  \n\nRealFormerTransformerSoftmax\n\n\n\n{% asset_img realformer_attention.png realformer attention %}  \n\n $Prev'$ softmax $\\frac{Q^{\\prime}K^{\\prime T}}{\\sqrt{d_k}}+Prev'$ attention  \n\n# \n\nnormalizationbatchnormlayernormtransformer\n\nrmsnorm + prenorm  \n\nnormalization    \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n***\n\n# Reference  \n1https://www.zhihu.com/question/487766088  \n2Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization https://arxiv.org/abs/2001.06838  \n3Transformer()& https://zhuanlan.zhihu.com/p/476102712  \n4Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://arxiv.org/abs/1502.03167  \n5How Does Batch Normalization Help Optimization? https://arxiv.org/abs/1805.11604  \n6Batch Normalization: Accelerating Deep Network https://zhuanlan.zhihu.com/p/340856414  \n7Layer Normalization https://arxiv.org/abs/1607.06450  \n8NormalizationBN/LN/WN https://zhuanlan.zhihu.com/p/33173246  \n9Transformer()BatchNormalization https://zhuanlan.zhihu.com/p/481277619  \n10Layer Normalization https://arxiv.org/abs/1607.06450  \n11PowerNorm: Rethinking Batch Normalization in Transformers https://arxiv.org/abs/2003.07845  \n12Root Mean Square Layer Normalization https://arxiv.org/abs/1910.07467  \n13On Layer Normalization in the Transformer Architecture https://arxiv.org/abs/2002.04745  \n14Pre NormPost Norm https://spaces.ac.cn/archives/9009  \n15Understanding the Difficulty of Training Transformers https://arxiv.org/abs/2004.08249  \n16RealFormer: Transformer Likes Residual Attention https://arxiv.org/abs/2012.11747  \n17DeepNet: Scaling Transformers to 1,000 Layers https://arxiv.org/abs/2203.00555  \n\n","slug":"cs/nlp/2024/03/Transformernormalization","published":1,"updated":"2024-04-07T06:36:14.847Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsms000r0p4k1fhha0b5","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>Normalizationattention</p>\n<p>normalizationtransformer</p>\n<p></p>\n<h1 id=\"why-normalization\">why normalization</h1>\n<p>normalization</p>\n<p></p>\n<h2 id=\"normalization\">normalization</h2>\n<p> <span class=\"math inline\">\\(Loss(x_1,x_2)=x_1^2+x_2^2+b\\)</span>\n</p>\n<img src=\"/6a40bfa5/lossfunc_surface.jpeg\" class title=\"loss function surface\">\n<p></p>\n<p></p>\n<p>minimum</p>\n<p></p>\n<img src=\"/6a40bfa5/ellipse_1.png\" class title=\"ellipse\">\n<p></p>\n<p></p>\n<img src=\"/6a40bfa5/ellipse_2.png\" class title=\"ellipse\">\n<p></p>\n<p></p>\n<p> <span class=\"math inline\">\\(x_{1}\\)</span> <span class=\"math inline\">\\(x_{2}\\)</span>-&gt;</p>\n<p>0.x~2.x0</p>\n<p></p>\n<p> <span class=\"math inline\">\\(x_{2}\\)</span>\n</p>\n<p></p>\n<p></p>\n<p>normalization</p>\n<p>normalization<br>\n<span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\min(\\mathrm{x})}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}\\]</span></p>\n<p><span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\mu}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}\\]</span></p>\n<p><span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\mu}\\sigma\\]</span></p>\n<p> <span class=\"math inline\">\\(\\mu\\)</span> <span class=\"math inline\">\\(\\sigma\\)</span> </p>\n<p>Z-score\nnormalization</p>\n<p></p>\n<p>PCA</p>\n<h2 id=\"ics...\">ICS...</h2>\n<p>i.i.d.independent and identical\ndistribution</p>\n<p>i.i.d.</p>\n<p></p>\n<p></p>\n<p></p>\n<p>i.i.d.i.i.d.</p>\n<p>PCAi.i.d.</p>\n<p></p>\n<p>i.i.d.</p>\n<p>ICSinternal covariate shift</p>\n<p></p>\n<p>normalization</p>\n<p>normalizationbatchnormICSbatchnorm</p>\n<p>ICS</p>\n<h2 id=\"\"></h2>\n<p>sigmoid</p>\n<img src=\"/6a40bfa5/sigmoid.png\" class title=\"sigmoid\">\n<p> &gt; 6  &lt; -6\nsigmoid</p>\n<p></p>\n<p>ICSnormalization</p>\n<p>normalization</p>\n<h1 id=\"batchnorm\">batchnorm</h1>\n<p>normalizationbatchnormlayernorm</p>\n<h2 id=\"batchnorm\">batchnorm</h2>\n<p> <span class=\"math inline\">\\([B,C]\\)</span>\n <span class=\"math inline\">\\(B\\)</span> batch size<span class=\"math inline\">\\(C\\)</span> </p>\n<p> <span class=\"math inline\">\\(C\\)</span>\nnormalization\n<span class=\"math inline\">\\(C\\)</span> </p>\n<p> <span class=\"math inline\">\\(i\\)</span>\nbatch</p>\n<p><span class=\"math display\">\\[\n\\mu_{i}=\\frac{1}{B}\\sum_{j=1}^{B}x_{i,j}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\sigma_{i}^{2}=\\frac{1}{B}\\sum_{j=1}^{B}(x_{i,j}-\\mu_{i})^2\n\\]</span></p>\n<p>batchZ-score\nnormalization</p>\n<p><span class=\"math display\">\\[\nx_{i,j}&#39;=\\frac{x_{i,j}-\\mu_{i}}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\epsilon\\)</span>\n0</p>\n<p> <span class=\"math inline\">\\(C\\)</span>\n01</p>\n<p></p>\n<p>sigmoid</p>\n<p></p>\n<p>\n <span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p><span class=\"math display\">\\[\ny_{i,j} = \\gamma_{i} x_{i,j}&#39; + \\beta_{i}\n\\]</span></p>\n<p></p>\n<p>batchnorm\n<span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span>\n</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/340856414\">Batch\nNormalization: Accelerating Deep Network</a></p>\n<img src=\"/6a40bfa5/bn_algo.png\" class title=\"batch norm\">\n<h2 id=\"cnnbatchnorm\">CNNbatchnorm</h2>\n<p>batchnormCNN</p>\n<p>CNNfeature mapsize <span class=\"math inline\">\\([B,C,H,W]\\)</span>  <span class=\"math inline\">\\(B\\)</span> batch size<span class=\"math inline\">\\(C\\)</span> channel<span class=\"math inline\">\\(H\\)</span>  <span class=\"math inline\">\\(W\\)</span> </p>\n<p>batchnorm <span class=\"math inline\">\\(C\\times H\\times W\\)</span> \n<span class=\"math inline\">\\(B\\)</span>  <span class=\"math inline\">\\(B\\)</span> </p>\n<p>CNNchannel <span class=\"math inline\">\\(H\\times W\\)</span>\n <span class=\"math inline\">\\(H\\times W\\)</span>\nbatch</p>\n<p>batch <span class=\"math inline\">\\(B\\times H\\times W\\)</span>  <span class=\"math inline\">\\(C\\)</span>  <span class=\"math inline\">\\(C\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p>batchnormbatchnormrelufcbatchnormfcbias</p>\n<p>btwbatchnorm <span class=\"math inline\">\\(\\gamma\\)</span> 1 <span class=\"math inline\">\\(\\beta\\)</span>\n0batchnorm</p>\n<h2 id=\"\"></h2>\n<p>batchnormmini-batch</p>\n<p><span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span>\n</p>\n<p>samplesamplebatch</p>\n<p>i.i.d.</p>\n<p></p>\n<p>moving_mean = momentum  moving_mean + (1.0  momentum)  mean</p>\n<p>moving_var = momentum  moving_var + (1.0  momentum)  var</p>\n<p>batch</p>\n<p>momentum TF/Keras 0.99 Pytorch\n0.9</p>\n<p>momentum</p>\n<p>momentum</p>\n<p>batch sizemini\nbatchmomentum</p>\n<p>batch</p>\n<p>batch sizemini\nbatchbatchnormbatch\nsize</p>\n<img src=\"/6a40bfa5/bs_bn.png\" class title=\"batch size\">\n<p>batchnorm</p>\n<p>batch</p>\n<p>momentumbatch\nsizebatch size</p>\n<h2 id=\"batchnorm\">batchnorm</h2>\n<p>batchnormbatchnormICS2018How\nDoes Batch Normalization Help Optimization?</p>\n<p>batchnormICSICSbatchnormcovariate\nshiftbatchnorm</p>\n<img src=\"/6a40bfa5/bn_ics.png\" class title=\"ICS\">\n<p>ICSbatchnormICS</p>\n<p>ICSbatchnorm</p>\n<p>batchnormICS</p>\n<p>ICSICS</p>\n<p>ICS</p>\n<img src=\"/6a40bfa5/ics_define.png\" class title=\"ICS \">\n<p>iL2</p>\n<p><span class=\"math inline\">\\(G_{t,i}\\)</span>\nt</p>\n<p><span class=\"math inline\">\\(G_{t,i}&#39;\\)</span>\nt</p>\n<p></p>\n<p>ICS</p>\n<img src=\"/6a40bfa5/ics_measure.png\" class title=\"ICS measure\">\n<p>batchnorm</p>\n<p>batchnormICS</p>\n<p>batchnorm</p>\n<p>batchnorm</p>\n<h1 id=\"layernorm\">layernorm</h1>\n<p>batchnormlayernorm</p>\n<h2 id=\"layernorm\">layernorm</h2>\n<p>layernormlayerlayer</p>\n<p>batchnormbatchlayernorm</p>\n<img src=\"/6a40bfa5/bn_and_ln.png\" class title=\"bnln\">\n<p>NLPbatchnormlayernormbatchnormlayernorm</p>\n<p>layernormNLPRNNtransfomrer</p>\n<p>transformer <span class=\"math inline\">\\([B,S,H]\\)</span> <span class=\"math inline\">\\(S\\)</span>\npaddingzero-paddingbatch</p>\n<p></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">  </span><br><span class=\"line\">    </span><br></pre></td></tr></table></figure>\n<p>pad</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">    [P] [P]</span><br><span class=\"line\">      [P]</span><br></pre></td></tr></table></figure>\n<p>paddingbatchnorm</p>\n<p>[P] [P]\nbatch size2 [P]\nnormalization</p>\n<p>batch</p>\n<p> [P]\ntoken</p>\n<p>layernorm <span class=\"math inline\">\\(H\\)</span>\nnormalization <span class=\"math inline\">\\(H\\)</span>\n<span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p>token</p>\n<h2 id=\"transformerlayernorm\">transformerlayernorm</h2>\n<p>batchnormlayernormbatch</p>\n<p>layernormbatchbatch size</p>\n<p>nlplayernormbatchnorm</p>\n<p>PowerNorm: Rethinking Batch Normalization in\nTransformerstransformerBN</p>\n<p>batchbatchrunning\nstatisticsNLPIWSLT14batchrunning\nstatisticsCV</p>\n<p>magnitudeCV</p>\n<p>transformerBNCVNLPNLPbatch</p>\n<p>layernormNLPbatchnorm</p>\n<h2 id=\"rmsnorm\">RMSnorm</h2>\n<p>19Root Mean Square Layer\nNormalizationnormalizationRMSnormlayernorm</p>\n<p>RMSnormlayernorm</p>\n<img src=\"/6a40bfa5/rmsnorm.png\" class title=\"RMSnorm\">\n<p>layernorm</p>\n<img src=\"/6a40bfa5/rmsnorm_eff.png\" class title=\"RMSnorm\">\n<p>GRUlayernormlayernormlayernorm</p>\n<p>layernorm</p>\n<p>pRMSnormp%</p>\n<img src=\"/6a40bfa5/prmsnorm.png\" class title=\"prmsnorm\">\n<p>RMSnorm</p>\n<h1 id=\"post-norm-pre-norm\">post-norm &amp; pre-norm</h1>\n<h2 id=\"\"></h2>\n<p>layernorm</p>\n<p>transformerpost-normOn Layer Normalization in\nthe Transformer Architecturepre-norm</p>\n<p>post-normpre-norm</p>\n<img src=\"/6a40bfa5/postnorm_prenorm.png\" class title=\"postnorm and prenorm\">\n<p>post-normpre-norm</p>\n<p>post-normpre-normpre-normpost-normpre-norm</p>\n<p> <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(l+1\\)</span> post-norm</p>\n<p><span class=\"math display\">\\[\nx_{l+1}=\\mathrm{Norm}(x_l+\\mathrm{F}_t(x_l))\n\\]</span></p>\n<p>pre-norm</p>\n<p><span class=\"math display\">\\[\nx_{l+1}=x_l+\\mathrm{F}_l(\\mathrm{Norm}(x_l))\n\\]</span></p>\n<p>Pre NormPost Norm\n<span class=\"math inline\">\\(\\mathrm{F}_l(\\mathrm{Norm}(x_l))\\)</span>\nnorm</p>\n<p> <span class=\"math inline\">\\(l\\)</span> <span class=\"math inline\">\\(x_{l}x_{l+1}\\)</span>  <span class=\"math inline\">\\(\\mathrm{F}_l(\\mathrm{Norm}(x_l))\\)</span>  <span class=\"math inline\">\\(\\mathrm{F}_{l+1}(\\mathrm{Norm}(x_{l+1}))\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\mathrm{F}_l(\\operatorname{Norm}(x_l))+\\operatorname{F}_{l+1}(\\operatorname{Norm}(x_{l+1}))\n\\\\\n&amp;{\\approx}\\mathrm{F}_l(\\mathrm{Norm}(x_l))+\\mathrm{F}_{l+1}\\left(\\mathrm{Norm}(x_l)\\right)\n\\\\\n&amp;=(\\mathrm{F}_l\\oplus\\mathrm{F}_{l+1})(\\mathrm{Norm}(\\pmb{x_l}))\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(l+1\\)</span>  <span class=\"math inline\">\\(l\\)</span> </p>\n<p>pre-norm <span class=\"math inline\">\\(l\\)</span>\npost-norm</p>\n<p>normalizationpre-norm--</p>\n<p></p>\n<p>post-normlossnormpost-normpre-norm</p>\n<h2 id=\"warmup\">warmup</h2>\n<p>On Layer Normalization in the Transformer\nArchitecturepre-normpost-normtransformerwarmup</p>\n<p>warmup</p>\n<p>pre-normtransformerwarmuppost-norm+warmuppost-normwarmup</p>\n<img src=\"/6a40bfa5/warmup_effect.png\" class title=\"warmup\">\n<h2 id=\"deepnorm\">Deepnorm</h2>\n<p>2022DeepNet: Scaling Transformers to 1,000\nLayerstransformer</p>\n<p>Deepnorm</p>\n<img src=\"/6a40bfa5/deepnorm.png\" class title=\"deepnorm\">\n<p> <span class=\"math inline\">\\(\\alpha&gt;1\\)</span>\npost-norm</p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span> \n<span class=\"math inline\">\\(G_{l}\\)</span>\n</p>\n<p>deepnormpre-normpost-norm</p>\n<img src=\"/6a40bfa5/deepnorm_result.png\" class title=\"deepnorm result\">\n<p>post-norm</p>\n<h2 id=\"realformer--residual-attention\">Realformer--residual\nattention</h2>\n<p>post-normpre-normRealFormer:\nTransformer Likes Residual Attention</p>\n<img src=\"/6a40bfa5/realformer.png\" class title=\"realformer\">\n<p>RealFormerTransformerSoftmax</p>\n<p></p>\n<img src=\"/6a40bfa5/realformer_attention.png\" class title=\"realformer attention\">\n<p> <span class=\"math inline\">\\(Prev&#39;\\)</span>\nsoftmax\n<span class=\"math inline\">\\(\\frac{Q^{\\prime}K^{\\prime\nT}}{\\sqrt{d_k}}+Prev&#39;\\)</span> attention</p>\n<h1 id=\"\"></h1>\n<p>normalizationbatchnormlayernormtransformer</p>\n<p>rmsnorm + prenorm</p>\n<p>normalization</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1https://www.zhihu.com/question/487766088<br>\n2Towards Stabilizing Batch Statistics in Backward Propagation of\nBatch Normalization https://arxiv.org/abs/2001.06838<br>\n3Transformer()&amp;\nhttps://zhuanlan.zhihu.com/p/476102712<br>\n4Batch Normalization: Accelerating Deep Network Training by Reducing\nInternal Covariate Shift https://arxiv.org/abs/1502.03167<br>\n5How Does Batch Normalization Help Optimization?\nhttps://arxiv.org/abs/1805.11604<br>\n6Batch Normalization: Accelerating Deep Network\nhttps://zhuanlan.zhihu.com/p/340856414<br>\n7Layer Normalization https://arxiv.org/abs/1607.06450<br>\n8NormalizationBN/LN/WN\nhttps://zhuanlan.zhihu.com/p/33173246<br>\n9Transformer()BatchNormalization\nhttps://zhuanlan.zhihu.com/p/481277619<br>\n10Layer Normalization https://arxiv.org/abs/1607.06450<br>\n11PowerNorm: Rethinking Batch Normalization in Transformers\nhttps://arxiv.org/abs/2003.07845<br>\n12Root Mean Square Layer Normalization\nhttps://arxiv.org/abs/1910.07467<br>\n13On Layer Normalization in the Transformer Architecture\nhttps://arxiv.org/abs/2002.04745<br>\n14Pre NormPost Norm\nhttps://spaces.ac.cn/archives/9009<br>\n15Understanding the Difficulty of Training Transformers\nhttps://arxiv.org/abs/2004.08249<br>\n16RealFormer: Transformer Likes Residual Attention\nhttps://arxiv.org/abs/2012.11747<br>\n17DeepNet: Scaling Transformers to 1,000 Layers\nhttps://arxiv.org/abs/2203.00555</p>\n","length":11986,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>Normalizationattention</p>\n<p>normalizationtransformer</p>\n<p></p>\n<h1 id=\"why-normalization\">why normalization</h1>\n<p>normalization</p>\n<p></p>\n<h2 id=\"normalization\">normalization</h2>\n<p> <span class=\"math inline\">\\(Loss(x_1,x_2)=x_1^2+x_2^2+b\\)</span>\n</p>\n<img src=\"/6a40bfa5/lossfunc_surface.jpeg\" class title=\"loss function surface\">\n<p></p>\n<p></p>\n<p>minimum</p>\n<p></p>\n<img src=\"/6a40bfa5/ellipse_1.png\" class title=\"ellipse\">\n<p></p>\n<p></p>\n<img src=\"/6a40bfa5/ellipse_2.png\" class title=\"ellipse\">\n<p></p>\n<p></p>\n<p> <span class=\"math inline\">\\(x_{1}\\)</span> <span class=\"math inline\">\\(x_{2}\\)</span>-&gt;</p>\n<p>0.x~2.x0</p>\n<p></p>\n<p> <span class=\"math inline\">\\(x_{2}\\)</span>\n</p>\n<p></p>\n<p></p>\n<p>normalization</p>\n<p>normalization<br>\n<span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\min(\\mathrm{x})}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}\\]</span></p>\n<p><span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\mu}{\\max(\\mathrm{x})-\\min(\\mathrm{x})}\\]</span></p>\n<p><span class=\"math display\">\\[x^{\\prime}=\\frac{x-\\mu}\\sigma\\]</span></p>\n<p> <span class=\"math inline\">\\(\\mu\\)</span> <span class=\"math inline\">\\(\\sigma\\)</span> </p>\n<p>Z-score\nnormalization</p>\n<p></p>\n<p>PCA</p>\n<h2 id=\"ics...\">ICS...</h2>\n<p>i.i.d.independent and identical\ndistribution</p>\n<p>i.i.d.</p>\n<p></p>\n<p></p>\n<p></p>\n<p>i.i.d.i.i.d.</p>\n<p>PCAi.i.d.</p>\n<p></p>\n<p>i.i.d.</p>\n<p>ICSinternal covariate shift</p>\n<p></p>\n<p>normalization</p>\n<p>normalizationbatchnormICSbatchnorm</p>\n<p>ICS</p>\n<h2 id=\"\"></h2>\n<p>sigmoid</p>\n<img src=\"/6a40bfa5/sigmoid.png\" class title=\"sigmoid\">\n<p> &gt; 6  &lt; -6\nsigmoid</p>\n<p></p>\n<p>ICSnormalization</p>\n<p>normalization</p>\n<h1 id=\"batchnorm\">batchnorm</h1>\n<p>normalizationbatchnormlayernorm</p>\n<h2 id=\"batchnorm\">batchnorm</h2>\n<p> <span class=\"math inline\">\\([B,C]\\)</span>\n <span class=\"math inline\">\\(B\\)</span> batch size<span class=\"math inline\">\\(C\\)</span> </p>\n<p> <span class=\"math inline\">\\(C\\)</span>\nnormalization\n<span class=\"math inline\">\\(C\\)</span> </p>\n<p> <span class=\"math inline\">\\(i\\)</span>\nbatch</p>\n<p><span class=\"math display\">\\[\n\\mu_{i}=\\frac{1}{B}\\sum_{j=1}^{B}x_{i,j}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\sigma_{i}^{2}=\\frac{1}{B}\\sum_{j=1}^{B}(x_{i,j}-\\mu_{i})^2\n\\]</span></p>\n<p>batchZ-score\nnormalization</p>\n<p><span class=\"math display\">\\[\nx_{i,j}&#39;=\\frac{x_{i,j}-\\mu_{i}}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\n\\]</span></p>\n<p> <span class=\"math inline\">\\(\\epsilon\\)</span>\n0</p>\n<p> <span class=\"math inline\">\\(C\\)</span>\n01</p>\n<p></p>\n<p>sigmoid</p>\n<p></p>\n<p>\n <span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p><span class=\"math display\">\\[\ny_{i,j} = \\gamma_{i} x_{i,j}&#39; + \\beta_{i}\n\\]</span></p>\n<p></p>\n<p>batchnorm\n<span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span>\n</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/340856414\">Batch\nNormalization: Accelerating Deep Network</a></p>\n<img src=\"/6a40bfa5/bn_algo.png\" class title=\"batch norm\">\n<h2 id=\"cnnbatchnorm\">CNNbatchnorm</h2>\n<p>batchnormCNN</p>\n<p>CNNfeature mapsize <span class=\"math inline\">\\([B,C,H,W]\\)</span>  <span class=\"math inline\">\\(B\\)</span> batch size<span class=\"math inline\">\\(C\\)</span> channel<span class=\"math inline\">\\(H\\)</span>  <span class=\"math inline\">\\(W\\)</span> </p>\n<p>batchnorm <span class=\"math inline\">\\(C\\times H\\times W\\)</span> \n<span class=\"math inline\">\\(B\\)</span>  <span class=\"math inline\">\\(B\\)</span> </p>\n<p>CNNchannel <span class=\"math inline\">\\(H\\times W\\)</span>\n <span class=\"math inline\">\\(H\\times W\\)</span>\nbatch</p>\n<p>batch <span class=\"math inline\">\\(B\\times H\\times W\\)</span>  <span class=\"math inline\">\\(C\\)</span>  <span class=\"math inline\">\\(C\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p>batchnormbatchnormrelufcbatchnormfcbias</p>\n<p>btwbatchnorm <span class=\"math inline\">\\(\\gamma\\)</span> 1 <span class=\"math inline\">\\(\\beta\\)</span>\n0batchnorm</p>\n<h2 id=\"\"></h2>\n<p>batchnormmini-batch</p>\n<p><span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span>\n</p>\n<p>samplesamplebatch</p>\n<p>i.i.d.</p>\n<p></p>\n<p>moving_mean = momentum  moving_mean + (1.0  momentum)  mean</p>\n<p>moving_var = momentum  moving_var + (1.0  momentum)  var</p>\n<p>batch</p>\n<p>momentum TF/Keras 0.99 Pytorch\n0.9</p>\n<p>momentum</p>\n<p>momentum</p>\n<p>batch sizemini\nbatchmomentum</p>\n<p>batch</p>\n<p>batch sizemini\nbatchbatchnormbatch\nsize</p>\n<img src=\"/6a40bfa5/bs_bn.png\" class title=\"batch size\">\n<p>batchnorm</p>\n<p>batch</p>\n<p>momentumbatch\nsizebatch size</p>\n<h2 id=\"batchnorm\">batchnorm</h2>\n<p>batchnormbatchnormICS2018How\nDoes Batch Normalization Help Optimization?</p>\n<p>batchnormICSICSbatchnormcovariate\nshiftbatchnorm</p>\n<img src=\"/6a40bfa5/bn_ics.png\" class title=\"ICS\">\n<p>ICSbatchnormICS</p>\n<p>ICSbatchnorm</p>\n<p>batchnormICS</p>\n<p>ICSICS</p>\n<p>ICS</p>\n<img src=\"/6a40bfa5/ics_define.png\" class title=\"ICS \">\n<p>iL2</p>\n<p><span class=\"math inline\">\\(G_{t,i}\\)</span>\nt</p>\n<p><span class=\"math inline\">\\(G_{t,i}&#39;\\)</span>\nt</p>\n<p></p>\n<p>ICS</p>\n<img src=\"/6a40bfa5/ics_measure.png\" class title=\"ICS measure\">\n<p>batchnorm</p>\n<p>batchnormICS</p>\n<p>batchnorm</p>\n<p>batchnorm</p>\n<h1 id=\"layernorm\">layernorm</h1>\n<p>batchnormlayernorm</p>\n<h2 id=\"layernorm\">layernorm</h2>\n<p>layernormlayerlayer</p>\n<p>batchnormbatchlayernorm</p>\n<img src=\"/6a40bfa5/bn_and_ln.png\" class title=\"bnln\">\n<p>NLPbatchnormlayernormbatchnormlayernorm</p>\n<p>layernormNLPRNNtransfomrer</p>\n<p>transformer <span class=\"math inline\">\\([B,S,H]\\)</span> <span class=\"math inline\">\\(S\\)</span>\npaddingzero-paddingbatch</p>\n<p></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">  </span><br><span class=\"line\">    </span><br></pre></td></tr></table></figure>\n<p>pad</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">    [P] [P]</span><br><span class=\"line\">      [P]</span><br></pre></td></tr></table></figure>\n<p>paddingbatchnorm</p>\n<p>[P] [P]\nbatch size2 [P]\nnormalization</p>\n<p>batch</p>\n<p> [P]\ntoken</p>\n<p>layernorm <span class=\"math inline\">\\(H\\)</span>\nnormalization <span class=\"math inline\">\\(H\\)</span>\n<span class=\"math inline\">\\(\\gamma\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p>token</p>\n<h2 id=\"transformerlayernorm\">transformerlayernorm</h2>\n<p>batchnormlayernormbatch</p>\n<p>layernormbatchbatch size</p>\n<p>nlplayernormbatchnorm</p>\n<p>PowerNorm: Rethinking Batch Normalization in\nTransformerstransformerBN</p>\n<p>batchbatchrunning\nstatisticsNLPIWSLT14batchrunning\nstatisticsCV</p>\n<p>magnitudeCV</p>\n<p>transformerBNCVNLPNLPbatch</p>\n<p>layernormNLPbatchnorm</p>\n<h2 id=\"rmsnorm\">RMSnorm</h2>\n<p>19Root Mean Square Layer\nNormalizationnormalizationRMSnormlayernorm</p>\n<p>RMSnormlayernorm</p>\n<img src=\"/6a40bfa5/rmsnorm.png\" class title=\"RMSnorm\">\n<p>layernorm</p>\n<img src=\"/6a40bfa5/rmsnorm_eff.png\" class title=\"RMSnorm\">\n<p>GRUlayernormlayernormlayernorm</p>\n<p>layernorm</p>\n<p>pRMSnormp%</p>\n<img src=\"/6a40bfa5/prmsnorm.png\" class title=\"prmsnorm\">\n<p>RMSnorm</p>\n<h1 id=\"post-norm-pre-norm\">post-norm &amp; pre-norm</h1>\n<h2 id=\"\"></h2>\n<p>layernorm</p>\n<p>transformerpost-normOn Layer Normalization in\nthe Transformer Architecturepre-norm</p>\n<p>post-normpre-norm</p>\n<img src=\"/6a40bfa5/postnorm_prenorm.png\" class title=\"postnorm and prenorm\">\n<p>post-normpre-norm</p>\n<p>post-normpre-normpre-normpost-normpre-norm</p>\n<p> <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(l+1\\)</span> post-norm</p>\n<p><span class=\"math display\">\\[\nx_{l+1}=\\mathrm{Norm}(x_l+\\mathrm{F}_t(x_l))\n\\]</span></p>\n<p>pre-norm</p>\n<p><span class=\"math display\">\\[\nx_{l+1}=x_l+\\mathrm{F}_l(\\mathrm{Norm}(x_l))\n\\]</span></p>\n<p>Pre NormPost Norm\n<span class=\"math inline\">\\(\\mathrm{F}_l(\\mathrm{Norm}(x_l))\\)</span>\nnorm</p>\n<p> <span class=\"math inline\">\\(l\\)</span> <span class=\"math inline\">\\(x_{l}x_{l+1}\\)</span>  <span class=\"math inline\">\\(\\mathrm{F}_l(\\mathrm{Norm}(x_l))\\)</span>  <span class=\"math inline\">\\(\\mathrm{F}_{l+1}(\\mathrm{Norm}(x_{l+1}))\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\mathrm{F}_l(\\operatorname{Norm}(x_l))+\\operatorname{F}_{l+1}(\\operatorname{Norm}(x_{l+1}))\n\\\\\n&amp;{\\approx}\\mathrm{F}_l(\\mathrm{Norm}(x_l))+\\mathrm{F}_{l+1}\\left(\\mathrm{Norm}(x_l)\\right)\n\\\\\n&amp;=(\\mathrm{F}_l\\oplus\\mathrm{F}_{l+1})(\\mathrm{Norm}(\\pmb{x_l}))\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(l+1\\)</span>  <span class=\"math inline\">\\(l\\)</span> </p>\n<p>pre-norm <span class=\"math inline\">\\(l\\)</span>\npost-norm</p>\n<p>normalizationpre-norm--</p>\n<p></p>\n<p>post-normlossnormpost-normpre-norm</p>\n<h2 id=\"warmup\">warmup</h2>\n<p>On Layer Normalization in the Transformer\nArchitecturepre-normpost-normtransformerwarmup</p>\n<p>warmup</p>\n<p>pre-normtransformerwarmuppost-norm+warmuppost-normwarmup</p>\n<img src=\"/6a40bfa5/warmup_effect.png\" class title=\"warmup\">\n<h2 id=\"deepnorm\">Deepnorm</h2>\n<p>2022DeepNet: Scaling Transformers to 1,000\nLayerstransformer</p>\n<p>Deepnorm</p>\n<img src=\"/6a40bfa5/deepnorm.png\" class title=\"deepnorm\">\n<p> <span class=\"math inline\">\\(\\alpha&gt;1\\)</span>\npost-norm</p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span> \n<span class=\"math inline\">\\(G_{l}\\)</span>\n</p>\n<p>deepnormpre-normpost-norm</p>\n<img src=\"/6a40bfa5/deepnorm_result.png\" class title=\"deepnorm result\">\n<p>post-norm</p>\n<h2 id=\"realformer--residual-attention\">Realformer--residual\nattention</h2>\n<p>post-normpre-normRealFormer:\nTransformer Likes Residual Attention</p>\n<img src=\"/6a40bfa5/realformer.png\" class title=\"realformer\">\n<p>RealFormerTransformerSoftmax</p>\n<p></p>\n<img src=\"/6a40bfa5/realformer_attention.png\" class title=\"realformer attention\">\n<p> <span class=\"math inline\">\\(Prev&#39;\\)</span>\nsoftmax\n<span class=\"math inline\">\\(\\frac{Q^{\\prime}K^{\\prime\nT}}{\\sqrt{d_k}}+Prev&#39;\\)</span> attention</p>\n<h1 id=\"\"></h1>\n<p>normalizationbatchnormlayernormtransformer</p>\n<p>rmsnorm + prenorm</p>\n<p>normalization</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1https://www.zhihu.com/question/487766088<br>\n2Towards Stabilizing Batch Statistics in Backward Propagation of\nBatch Normalization https://arxiv.org/abs/2001.06838<br>\n3Transformer()&amp;\nhttps://zhuanlan.zhihu.com/p/476102712<br>\n4Batch Normalization: Accelerating Deep Network Training by Reducing\nInternal Covariate Shift https://arxiv.org/abs/1502.03167<br>\n5How Does Batch Normalization Help Optimization?\nhttps://arxiv.org/abs/1805.11604<br>\n6Batch Normalization: Accelerating Deep Network\nhttps://zhuanlan.zhihu.com/p/340856414<br>\n7Layer Normalization https://arxiv.org/abs/1607.06450<br>\n8NormalizationBN/LN/WN\nhttps://zhuanlan.zhihu.com/p/33173246<br>\n9Transformer()BatchNormalization\nhttps://zhuanlan.zhihu.com/p/481277619<br>\n10Layer Normalization https://arxiv.org/abs/1607.06450<br>\n11PowerNorm: Rethinking Batch Normalization in Transformers\nhttps://arxiv.org/abs/2003.07845<br>\n12Root Mean Square Layer Normalization\nhttps://arxiv.org/abs/1910.07467<br>\n13On Layer Normalization in the Transformer Architecture\nhttps://arxiv.org/abs/2002.04745<br>\n14Pre NormPost Norm\nhttps://spaces.ac.cn/archives/9009<br>\n15Understanding the Difficulty of Training Transformers\nhttps://arxiv.org/abs/2004.08249<br>\n16RealFormer: Transformer Likes Residual Attention\nhttps://arxiv.org/abs/2012.11747<br>\n17DeepNet: Scaling Transformers to 1,000 Layers\nhttps://arxiv.org/abs/2203.00555</p>\n"},{"title":"(1)","abbrlink":"3345028a","date":"2024-03-17T02:46:09.000Z","_content":"\n![](/images/cover.png)  \n\n\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n\n***\n\n//  \n\nLLM~~\n\n# 1Transformerscaled dot-product attentionQKd  \n\nsoftmaxsoftmaxattentionscalingd softmax\n\n# 2TransformerQK  \n\n1QK  \n\n2QK  \n\n3  \n\n# 3TransformerFFNFFNFFN  \n\n1SVM kernel  \n\n2  \n\n3  \n\n# 4MQA(Multi-Query Attention)GQA(Grouped-Query Attention)MHA(Multi-Head Attention)  \n\n1MQAGQAMHAMHAKV  \n\n2Decoder-onlycausal attentionKVMQAGQAKVKV  \n\n# 5LLMDecoder-only  \n\n1AttentionDecoder-only  \n\n2  \n\n3Causal AttentionAttentiontoken  \n\n4KV Cache  \n\n5  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***\n\n\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  ","source":"_posts/cs/nlp/2024/03/-1.md","raw":"---\ntitle: (1)\ntags:\n  - NLP\n  - LLM\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 3345028a\ndate: 2024-03-17 10:46:09\n---\n\n![](/images/cover.png)  \n\n\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n\n***\n\n//  \n\nLLM~~\n\n# 1Transformerscaled dot-product attentionQKd  \n\nsoftmaxsoftmaxattentionscalingd softmax\n\n# 2TransformerQK  \n\n1QK  \n\n2QK  \n\n3  \n\n# 3TransformerFFNFFNFFN  \n\n1SVM kernel  \n\n2  \n\n3  \n\n# 4MQA(Multi-Query Attention)GQA(Grouped-Query Attention)MHA(Multi-Head Attention)  \n\n1MQAGQAMHAMHAKV  \n\n2Decoder-onlycausal attentionKVMQAGQAKVKV  \n\n# 5LLMDecoder-only  \n\n1AttentionDecoder-only  \n\n2  \n\n3Causal AttentionAttentiontoken  \n\n4KV Cache  \n\n5  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***\n\n\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  ","slug":"cs/nlp/2024/03/-1","published":1,"updated":"2024-03-17T14:16:49.511Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmt000u0p4kenajh0xm","content":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<hr>\n<p>//</p>\n<p>LLM<sub></sub></p>\n<h1 id=\"transformerscaled-dot-product-attentionqkd\">1Transformerscaled\ndot-product attentionQKd</h1>\n<p>softmaxsoftmaxattentionscalingd\nsoftmax</p>\n<h1 id=\"transformerqk\">2TransformerQK</h1>\n<p>1QK</p>\n<p>2QK</p>\n<p>3</p>\n<h1 id=\"transformerffnffnffn\">3TransformerFFNFFNFFN</h1>\n<p>1SVM\nkernel</p>\n<p>2</p>\n<p>3</p>\n<h1 id=\"mqamulti-query-attentiongqagrouped-query-attentionmhamulti-head-attention\">4MQA(Multi-Query\nAttention)GQA(Grouped-Query Attention)MHA(Multi-Head\nAttention)</h1>\n<p>1MQAGQAMHAMHAKV</p>\n<p>2Decoder-onlycausal\nattentionKVMQAGQAKVKV</p>\n<h1 id=\"llmdecoder-only\">5LLMDecoder-only</h1>\n<p>1AttentionDecoder-only</p>\n<p>2</p>\n<p>3Causal\nAttentionAttentiontoken</p>\n<p>4KV Cache</p>\n<p>5</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n","length":1412,"excerpt":"","more":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<hr>\n<p>//</p>\n<p>LLM<sub></sub></p>\n<h1 id=\"transformerscaled-dot-product-attentionqkd\">1Transformerscaled\ndot-product attentionQKd</h1>\n<p>softmaxsoftmaxattentionscalingd\nsoftmax</p>\n<h1 id=\"transformerqk\">2TransformerQK</h1>\n<p>1QK</p>\n<p>2QK</p>\n<p>3</p>\n<h1 id=\"transformerffnffnffn\">3TransformerFFNFFNFFN</h1>\n<p>1SVM\nkernel</p>\n<p>2</p>\n<p>3</p>\n<h1 id=\"mqamulti-query-attentiongqagrouped-query-attentionmhamulti-head-attention\">4MQA(Multi-Query\nAttention)GQA(Grouped-Query Attention)MHA(Multi-Head\nAttention)</h1>\n<p>1MQAGQAMHAMHAKV</p>\n<p>2Decoder-onlycausal\nattentionKVMQAGQAKVKV</p>\n<h1 id=\"llmdecoder-only\">5LLMDecoder-only</h1>\n<p>1AttentionDecoder-only</p>\n<p>2</p>\n<p>3Causal\nAttentionAttentiontoken</p>\n<p>4KV Cache</p>\n<p>5</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n"},{"title":"Yi-","abbrlink":"41b6a819","date":"2024-03-26T08:51:08.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n01.AIAI20231101.AIYi-6BYi-34B baseYi-6B-200KYi-34B-200K baseYi01.AIchatYi-9B  \n\n202311YiSuperCLUE/CMMLUYiYi  \n\n20243Yi  \n\n# TL;DR\n\n  \n\n- Yi-34Bint4float16<1%RTX409024G\n- LLAMA2\n- 3.1Tscaling law1T\n- <10k\n- 4k\n- \n\n# \n\n##   \n\nYi6B9B34B34B  \n\n34B24GRTX4090  \n\nint434B24GGPU  \n\n[Understanding INT4 Quantization for Language Models: Latency Speedup, Composability, and Failure Cases](https://arxiv.org/abs/2301.12017)Yi-34Bint8bf16<1%int4  \n\n{% asset_img eval.png Yi %}  \n\n3.1T tokenDeepMindscaling law1TB<2T  \n\nChinchilla[Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)scaling law  \n\nscaling lawYiovertrain  \n\n<big>**Yi+--34B+70B**</big>  \n\n##   \n\nLLAMA2  \n\n- LLAMA270BGQAYiGQA  \n- RoPERoPE ABFEffective long-context scaling of foundation modelsbase10M  \n- SwiGLUGLU Variants Improve Transformer  \n\nactivation size4h8/3hGQA  \n\n> We use SwiGLU as Yis post-attention layer, reducing its activation size from 4h to 8/3h (h denotes hidden size) to be consistent with the normal post-attention layer. This adjustment also compensates for the reduction in parameter resulted from GQA, making the overall parameter count comparible of existing 7B and 34B models.  \n\n{% asset_img model.png Yi %}  \n\n<big>****</big>  \n\n## tokenizer  \n\n- BPE64000  \n- digit  \n- OOVunicode  \n-   \n  \nLLMtokenizerYi  \n\n#   \n\nLLMYi  \n\n{% asset_img cover.png  %}  \n\n##   \n\n  \n\n{% asset_img pretrain_data_pipeline.png  %}  \n\n1.  &   \n\n  \n\nCCNeTCCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data  \n\n2.  Heuristic Rule Filters  \n\n  \n\n- URL  \n-   \n- n-gramScaling Language Models: Methods, Analysis & Insights from Training GopherCulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages\n- Personal Identifiable InformationPII\n\n3.  Learned Filters  \n\n4scorer  \n\n- Perplexity ScorerCCNet: Extracting High Quality Monolingual Datasets from Web Crawl Datakenlmperplexity\n- Quality Scorer\n- Document Coherence Scorer\n- Safety Scorer\n\n4.  Cluster-based Filters  \n\n  \n\n5. \n\nThe RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Onlyminhash  \n\n  \n\n{% asset_img pretrain_data_dist.png  %}  \n\ngarbage ingarbage out\n\n> we prefer 3T tokens over sophasticated engineering over 10T tokens without extensive filtering\n\n10T3  \n\n##  \n\nQuality is All You Need  \n\n<10kSFT  \n\nGemini: A family of highly capable multimodal models.Llama 2: Open Foundation and Fine-Tuned Chat ModelsLima: Less is more for alignmentFLANScaling instruction-finetuned language modelsUltraChatEnhancing chat language models by scaling high-quality instructional conversations  \n\n  \n\n- <big>**prompt distribution selection**</big>Wizardlm: Empowering large language models to follow complex instructionsSFT  \n- <big>**CoT data formatting**</big>Take a step back: Evoking reasoning via abstraction in large language modelsStep-Back  \n- <big>**response formatting**</big>Lima: Less is more for alignmentresponseintroduction-body-conclusionwhere the body is usually a list of bullet point  \n- <big>****</big>responseresponse  \n- <big>****</big>response  \n- <big>****</big>#instag: Instruction tagging for analyzing supervised fine-tuning of large language models  \n- <big>****</big>How abilities in large language models are affected by supervised fine-tuning data compositionapproximate grid search{1, 1/2, 1/4, 1/8, 1/16, 1/32, 1/64}  \n- <big>****</big>OPENAIChatML[https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md](https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md)system promptuser inputbot response\n\nSFT\n\n{% asset_img sft.png SFT %}  \n\n#   \n\n## infra\n\nYi  \n\n(1) \n(2) \n(3) DPOMegatronDeepSpeed\n(4) LLMcontinuous batching  paged attention\n\nUI  \n\n##   \n\n4k  \n\n##   \n\n  \n\n- AdamWbeta=[0.9,0.999]epsilon = 1e-8  \n- seq_len = 4096  \n- batch size = 64  \n- constant lr = 1e-5weight decay = 0.1  \n- gradient clip = 1.0  \n- max step = 300\n- Neftune: Noisy embeddings improve instruction finetuning6B noise scale = 534B noise scale = 45\n\n# \n\n## \n\n1.   \n\nYi  \n\n{% asset_img base_model_eval.png Base %}  \n\nGPT3.5GPT4Yi  \n\nYi  \n\n2.   \n\n- Yi-34BYi-6BYi-34BYi-6B  \n- Yi-34BQwen-14BFalcon-180B\n- GPT-4LLMLLMGPT-4GPT-3.5LLMQwen-14BYi-34BC-EvalCMMLUGaokaoGPT-4BBHHumanEvalMATH  \n\n\n3. In-Context Learning  \n\nYiin-context learning-underlying function  \n\n y = w1x1 + w2x2 + ... + wnxn  \n\n x1, x2, ..., xn, y x  y  \n\n w1, w2, ..., wn  \n\na y  y  |y  y| b y == y   \n\nunderlying function  \n\n[1,-1]Yi-34BLLAMA-70B  \n\n[11111]LLAMA-70BMistral 8*7B  \n\n{% asset_img ict.png ICT %}  \n\n## Chat  \n\n1. \n\nbasezero-shotfew-shot\n\n{% asset_img eval.png Yi %}  \n\nGoodharts principle  \n\nYi-34B-ChatYi-6B-ChatSFT  \n\n2. \n\n{% asset_img third_party.png  %}  \n\n#   \n\nbase3  \n\n##   \n\n4kbase200kSFT  \n\nattentionattentionsparse attention  \n\n12length-upsampled long-context data3  \n\nrecitation  \n\nData engineering for scaling language models to 128k contextParaphrasing the original text makes high accuracy long-context qa  \n\n5B tokenbatch size=4Mtoken100step1005B/4M=1250  \n\n> We continue pretrain the model on 5B tokens with 4M batch size, which translate to 100 optimization steps. Aligning with the concurrent work from Fu et al. [22], we observe that such light-weight continue pretraining is already able to enable a strong performance on Needle-in-a-Haystack test, as we will show in Figure 6.\n\nData engineering for scaling language models to 128k context  \n\nSFT  \n\n  \n\n  \n\n\n\n  \n\n{% asset_img long_context_result.png  %}  \n\n##   \n\nViTCLIP ViT-H/14 modeltransformerYi-Chat  \n\n{% asset_img multimodal.png  %}  \n\n3  \n\n1224^2ViTprojection1-LAION-400MViTViTLLM  \n\n2ViT448^2LAION-400M2000-480-CLLaVALLaVARFlickrVQAv2RefCOCOVisual7w  \n\n3100-GQAVizWiz VQATextCapsOCR-VQAVisual GenomeShareGPT4V50,000  \n\n128A1006B334B10  \n\n## Depth Upscaling \n\n326B489B  \n\nScaling large language models with simple yet effective depth up-scaling12-281648  \n\ncosine similarity  \n\n  \n\n{% asset_img 9B.png 9B %}  \n\n  \n\nDepth Upscaling  \n\n800B token  \n\n70%  \n\nconstant lr = 3e-54M tokenbatch size  \n\nbatch sizeYi-6B  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n","source":"_posts/cs/nlp/2024/03/Yi-.md","raw":"---\ntitle: Yi-\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 41b6a819\ndate: 2024-03-26 16:51:08\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n01.AIAI20231101.AIYi-6BYi-34B baseYi-6B-200KYi-34B-200K baseYi01.AIchatYi-9B  \n\n202311YiSuperCLUE/CMMLUYiYi  \n\n20243Yi  \n\n# TL;DR\n\n  \n\n- Yi-34Bint4float16<1%RTX409024G\n- LLAMA2\n- 3.1Tscaling law1T\n- <10k\n- 4k\n- \n\n# \n\n##   \n\nYi6B9B34B34B  \n\n34B24GRTX4090  \n\nint434B24GGPU  \n\n[Understanding INT4 Quantization for Language Models: Latency Speedup, Composability, and Failure Cases](https://arxiv.org/abs/2301.12017)Yi-34Bint8bf16<1%int4  \n\n{% asset_img eval.png Yi %}  \n\n3.1T tokenDeepMindscaling law1TB<2T  \n\nChinchilla[Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)scaling law  \n\nscaling lawYiovertrain  \n\n<big>**Yi+--34B+70B**</big>  \n\n##   \n\nLLAMA2  \n\n- LLAMA270BGQAYiGQA  \n- RoPERoPE ABFEffective long-context scaling of foundation modelsbase10M  \n- SwiGLUGLU Variants Improve Transformer  \n\nactivation size4h8/3hGQA  \n\n> We use SwiGLU as Yis post-attention layer, reducing its activation size from 4h to 8/3h (h denotes hidden size) to be consistent with the normal post-attention layer. This adjustment also compensates for the reduction in parameter resulted from GQA, making the overall parameter count comparible of existing 7B and 34B models.  \n\n{% asset_img model.png Yi %}  \n\n<big>****</big>  \n\n## tokenizer  \n\n- BPE64000  \n- digit  \n- OOVunicode  \n-   \n  \nLLMtokenizerYi  \n\n#   \n\nLLMYi  \n\n{% asset_img cover.png  %}  \n\n##   \n\n  \n\n{% asset_img pretrain_data_pipeline.png  %}  \n\n1.  &   \n\n  \n\nCCNeTCCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data  \n\n2.  Heuristic Rule Filters  \n\n  \n\n- URL  \n-   \n- n-gramScaling Language Models: Methods, Analysis & Insights from Training GopherCulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages\n- Personal Identifiable InformationPII\n\n3.  Learned Filters  \n\n4scorer  \n\n- Perplexity ScorerCCNet: Extracting High Quality Monolingual Datasets from Web Crawl Datakenlmperplexity\n- Quality Scorer\n- Document Coherence Scorer\n- Safety Scorer\n\n4.  Cluster-based Filters  \n\n  \n\n5. \n\nThe RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Onlyminhash  \n\n  \n\n{% asset_img pretrain_data_dist.png  %}  \n\ngarbage ingarbage out\n\n> we prefer 3T tokens over sophasticated engineering over 10T tokens without extensive filtering\n\n10T3  \n\n##  \n\nQuality is All You Need  \n\n<10kSFT  \n\nGemini: A family of highly capable multimodal models.Llama 2: Open Foundation and Fine-Tuned Chat ModelsLima: Less is more for alignmentFLANScaling instruction-finetuned language modelsUltraChatEnhancing chat language models by scaling high-quality instructional conversations  \n\n  \n\n- <big>**prompt distribution selection**</big>Wizardlm: Empowering large language models to follow complex instructionsSFT  \n- <big>**CoT data formatting**</big>Take a step back: Evoking reasoning via abstraction in large language modelsStep-Back  \n- <big>**response formatting**</big>Lima: Less is more for alignmentresponseintroduction-body-conclusionwhere the body is usually a list of bullet point  \n- <big>****</big>responseresponse  \n- <big>****</big>response  \n- <big>****</big>#instag: Instruction tagging for analyzing supervised fine-tuning of large language models  \n- <big>****</big>How abilities in large language models are affected by supervised fine-tuning data compositionapproximate grid search{1, 1/2, 1/4, 1/8, 1/16, 1/32, 1/64}  \n- <big>****</big>OPENAIChatML[https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md](https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md)system promptuser inputbot response\n\nSFT\n\n{% asset_img sft.png SFT %}  \n\n#   \n\n## infra\n\nYi  \n\n(1) \n(2) \n(3) DPOMegatronDeepSpeed\n(4) LLMcontinuous batching  paged attention\n\nUI  \n\n##   \n\n4k  \n\n##   \n\n  \n\n- AdamWbeta=[0.9,0.999]epsilon = 1e-8  \n- seq_len = 4096  \n- batch size = 64  \n- constant lr = 1e-5weight decay = 0.1  \n- gradient clip = 1.0  \n- max step = 300\n- Neftune: Noisy embeddings improve instruction finetuning6B noise scale = 534B noise scale = 45\n\n# \n\n## \n\n1.   \n\nYi  \n\n{% asset_img base_model_eval.png Base %}  \n\nGPT3.5GPT4Yi  \n\nYi  \n\n2.   \n\n- Yi-34BYi-6BYi-34BYi-6B  \n- Yi-34BQwen-14BFalcon-180B\n- GPT-4LLMLLMGPT-4GPT-3.5LLMQwen-14BYi-34BC-EvalCMMLUGaokaoGPT-4BBHHumanEvalMATH  \n\n\n3. In-Context Learning  \n\nYiin-context learning-underlying function  \n\n y = w1x1 + w2x2 + ... + wnxn  \n\n x1, x2, ..., xn, y x  y  \n\n w1, w2, ..., wn  \n\na y  y  |y  y| b y == y   \n\nunderlying function  \n\n[1,-1]Yi-34BLLAMA-70B  \n\n[11111]LLAMA-70BMistral 8*7B  \n\n{% asset_img ict.png ICT %}  \n\n## Chat  \n\n1. \n\nbasezero-shotfew-shot\n\n{% asset_img eval.png Yi %}  \n\nGoodharts principle  \n\nYi-34B-ChatYi-6B-ChatSFT  \n\n2. \n\n{% asset_img third_party.png  %}  \n\n#   \n\nbase3  \n\n##   \n\n4kbase200kSFT  \n\nattentionattentionsparse attention  \n\n12length-upsampled long-context data3  \n\nrecitation  \n\nData engineering for scaling language models to 128k contextParaphrasing the original text makes high accuracy long-context qa  \n\n5B tokenbatch size=4Mtoken100step1005B/4M=1250  \n\n> We continue pretrain the model on 5B tokens with 4M batch size, which translate to 100 optimization steps. Aligning with the concurrent work from Fu et al. [22], we observe that such light-weight continue pretraining is already able to enable a strong performance on Needle-in-a-Haystack test, as we will show in Figure 6.\n\nData engineering for scaling language models to 128k context  \n\nSFT  \n\n  \n\n  \n\n\n\n  \n\n{% asset_img long_context_result.png  %}  \n\n##   \n\nViTCLIP ViT-H/14 modeltransformerYi-Chat  \n\n{% asset_img multimodal.png  %}  \n\n3  \n\n1224^2ViTprojection1-LAION-400MViTViTLLM  \n\n2ViT448^2LAION-400M2000-480-CLLaVALLaVARFlickrVQAv2RefCOCOVisual7w  \n\n3100-GQAVizWiz VQATextCapsOCR-VQAVisual GenomeShareGPT4V50,000  \n\n128A1006B334B10  \n\n## Depth Upscaling \n\n326B489B  \n\nScaling large language models with simple yet effective depth up-scaling12-281648  \n\ncosine similarity  \n\n  \n\n{% asset_img 9B.png 9B %}  \n\n  \n\nDepth Upscaling  \n\n800B token  \n\n70%  \n\nconstant lr = 3e-54M tokenbatch size  \n\nbatch sizeYi-6B  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n","slug":"cs/nlp/2024/03/Yi-","published":1,"updated":"2024-03-29T11:53:37.115Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmt000x0p4k0a8r1fh9","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>01.AIAI20231101.AIYi-6BYi-34B\nbaseYi-6B-200KYi-34B-200K\nbaseYi01.AIchatYi-9B</p>\n<p>202311YiSuperCLUE/CMMLUYiYi</p>\n<p>20243Yi</p>\n<h1 id=\"tldr\">TL;DR</h1>\n<p></p>\n<ul>\n<li>Yi-34Bint4float16&lt;1%RTX409024G</li>\n<li>LLAMA2</li>\n<li>3.1Tscaling\nlaw1T</li>\n<li>&lt;10k</li>\n<li>4k</li>\n<li></li>\n</ul>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>Yi6B9B34B34B</p>\n<p>34B24GRTX4090</p>\n<p>int434B24GGPU</p>\n<p><a href=\"https://arxiv.org/abs/2301.12017\">Understanding INT4\nQuantization for Language Models: Latency Speedup, Composability, and\nFailure\nCases</a>Yi-34Bint8bf16&lt;1%int4</p>\n<img src=\"/41b6a819/eval.png\" class title=\"Yi\">\n<p>3.1T tokenDeepMindscaling\nlaw1TB&lt;2T</p>\n<p>Chinchilla<a href=\"https://arxiv.org/abs/2203.15556\">Training Compute-Optimal Large\nLanguage Models</a>scaling law</p>\n<p>scaling lawYiovertrain</p>\n<p><big><strong>Yi+--34B+70B</strong></big></p>\n<h2 id=\"\"></h2>\n<p>LLAMA2</p>\n<ul>\n<li>LLAMA270BGQAYiGQA<br>\n</li>\n<li>RoPERoPE ABFEffective long-context scaling of\nfoundation modelsbase10M<br>\n</li>\n<li>SwiGLUGLU Variants Improve Transformer</li>\n</ul>\n<p>activation\nsize4h8/3hGQA</p>\n<blockquote>\n<p>We use SwiGLU as Yis post-attention layer, reducing its activation\nsize from 4h to 8/3h (h denotes hidden size) to be consistent with the\nnormal post-attention layer. This adjustment also compensates for the\nreduction in parameter resulted from GQA, making the overall parameter\ncount comparible of existing 7B and 34B models.</p>\n</blockquote>\n<img src=\"/41b6a819/model.png\" class title=\"Yi\">\n<p><big><strong></strong></big></p>\n<h2 id=\"tokenizer\">tokenizer</h2>\n<ul>\n<li>BPE64000<br>\n</li>\n<li>digit<br>\n</li>\n<li>OOVunicode </li>\n<li></li>\n</ul>\n<p>LLMtokenizerYi</p>\n<h1 id=\"\"></h1>\n<p>LLMYi</p>\n<img src=\"/41b6a819/cover.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p></p>\n<img src=\"/41b6a819/pretrain_data_pipeline.png\" class title=\"\">\n<ol type=\"1\">\n<li> &amp; </li>\n</ol>\n<p></p>\n<p>CCNeTCCNet: Extracting High Quality Monolingual Datasets\nfrom Web Crawl Data</p>\n<ol start=\"2\" type=\"1\">\n<li> Heuristic Rule Filters</li>\n</ol>\n<p></p>\n<ul>\n<li>URL<br>\n</li>\n<li><br>\n</li>\n<li>n-gramScaling Language Models:\nMethods, Analysis &amp; Insights from Training\nGopherCulturaX: A Cleaned, Enormous, and\nMultilingual Dataset for Large Language Models in 167 Languages</li>\n<li>Personal Identifiable\nInformationPII</li>\n</ul>\n<ol start=\"3\" type=\"1\">\n<li> Learned Filters</li>\n</ol>\n<p>4scorer</p>\n<ul>\n<li>Perplexity ScorerCCNet: Extracting High Quality Monolingual\nDatasets from Web Crawl\nDatakenlmperplexity</li>\n<li>Quality\nScorer</li>\n<li>Document Coherence\nScorer</li>\n<li>Safety Scorer</li>\n</ul>\n<ol start=\"4\" type=\"1\">\n<li> Cluster-based Filters</li>\n</ol>\n<p></p>\n<ol start=\"5\" type=\"1\">\n<li></li>\n</ol>\n<p>The RefinedWeb Dataset for Falcon LLM: Outperforming Curated\nCorpora with Web Data, and Web Data\nOnlyminhash</p>\n<p></p>\n<img src=\"/41b6a819/pretrain_data_dist.png\" class title=\"\">\n<p>garbage\ningarbage out</p>\n<blockquote>\n<p>we prefer 3T tokens over sophasticated engineering over 10T tokens\nwithout extensive filtering</p>\n</blockquote>\n<p>10T3</p>\n<h2 id=\"\"></h2>\n<p>Quality is All You Need</p>\n<p>&lt;10kSFT</p>\n<p>Gemini: A family of highly capable multimodal\nmodels.Llama 2: Open Foundation and Fine-Tuned Chat\nModelsLima: Less is more for alignmentFLANScaling\ninstruction-finetuned language modelsUltraChatEnhancing chat\nlanguage models by scaling high-quality instructional\nconversations</p>\n<p></p>\n<ul>\n<li><big><strong>prompt distribution\nselection</strong></big>Wizardlm: Empowering large language\nmodels to follow complex\ninstructionsSFT<br>\n</li>\n<li><big><strong>CoT data formatting</strong></big>Take a\nstep back: Evoking reasoning via abstraction in large language\nmodelsStep-Back<br>\n</li>\n<li><big><strong>response formatting</strong></big>Lima:\nLess is more for\nalignmentresponseintroduction-body-conclusionwhere\nthe body is usually a list of bullet point<br>\n</li>\n<li><big><strong></strong></big>responseresponse<br>\n</li>\n<li><big><strong></strong></big>response<br>\n</li>\n<li><big><strong></strong></big>#instag:\nInstruction tagging for analyzing supervised fine-tuning of large\nlanguage\nmodels<br>\n</li>\n<li><big><strong></strong></big>How\nabilities in large language models are affected by supervised\nfine-tuning data compositionapproximate grid\nsearch{1, 1/2, 1/4, 1/8, 1/16, 1/32,\n1/64}<br>\n</li>\n<li><big><strong></strong></big>OPENAIChatML<a href=\"https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md\">https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md</a>system\npromptuser inputbot response</li>\n</ul>\n<p>SFT</p>\n<img src=\"/41b6a819/sft.png\" class title=\"SFT\">\n<h1 id=\"\"></h1>\n<h2 id=\"infra\">infra</h2>\n<p>Yi</p>\n<ol type=\"1\">\n<li></li>\n<li></li>\n<li>DPOMegatronDeepSpeed</li>\n<li>LLMcontinuous batching  paged\nattention</li>\n</ol>\n<p>UI</p>\n<h2 id=\"\"></h2>\n<p>4k</p>\n<h2 id=\"\"></h2>\n<p></p>\n<ul>\n<li>AdamWbeta=[0.9,0.999]epsilon = 1e-8<br>\n</li>\n<li>seq_len = 4096<br>\n</li>\n<li>batch size = 64<br>\n</li>\n<li>constant lr = 1e-5weight decay = 0.1<br>\n</li>\n<li>gradient clip = 1.0<br>\n</li>\n<li>max step = 300</li>\n<li>Neftune: Noisy embeddings improve instruction\nfinetuning6B noise scale = 534B noise scale =\n45</li>\n</ul>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>Yi</p>\n<img src=\"/41b6a819/base_model_eval.png\" class title=\"Base\">\n<p>GPT3.5GPT4Yi</p>\n<p>Yi</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<ul>\n<li>Yi-34BYi-6BYi-34BYi-6B<br>\n</li>\n<li>Yi-34BQwen-14BFalcon-180B</li>\n<li>GPT-4LLMLLMGPT-4GPT-3.5LLMQwen-14BYi-34BC-EvalCMMLUGaokaoGPT-4BBHHumanEvalMATH</li>\n</ul>\n<ol start=\"3\" type=\"1\">\n<li>In-Context Learning</li>\n</ol>\n<p>Yiin-context\nlearning-underlying\nfunction</p>\n<p> y = w1x1 + w2x2 +\n... + wnxn</p>\n<p> x1, x2, ..., xn, y x \ny</p>\n<p> w1, w2, ..., wn</p>\n<p>a y  y  |y  y|\nb y == y </p>\n<p>underlying\nfunction</p>\n<p>[1,-1]Yi-34BLLAMA-70B</p>\n<p>[11111]LLAMA-70BMistral\n8*7B</p>\n<img src=\"/41b6a819/ict.png\" class title=\"ICT\">\n<h2 id=\"chat\">Chat</h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>basezero-shotfew-shot</p>\n<img src=\"/41b6a819/eval.png\" class title=\"Yi\">\n<p>Goodharts\nprinciple</p>\n<p>Yi-34B-ChatYi-6B-ChatSFT</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<img src=\"/41b6a819/third_party.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<p>base3</p>\n<h2 id=\"\"></h2>\n<p>4kbase200kSFT</p>\n<p>attentionattentionsparse\nattention</p>\n<p>12length-upsampled\nlong-context\ndata3</p>\n<p>recitation</p>\n<p>Data engineering for scaling language\nmodels to 128k contextParaphrasing the original text makes high\naccuracy long-context qa</p>\n<p>5B tokenbatch\nsize=4Mtoken100step1005B/4M=1250</p>\n<blockquote>\n<p>We continue pretrain the model on 5B tokens with 4M batch size, which\ntranslate to 100 optimization steps. Aligning with the concurrent work\nfrom Fu et al. [22], we observe that such light-weight continue\npretraining is already able to enable a strong performance on\nNeedle-in-a-Haystack test, as we will show in Figure 6.</p>\n</blockquote>\n<p>Data engineering for scaling language models to 128k\ncontext</p>\n<p>SFT</p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<img src=\"/41b6a819/long_context_result.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p>ViTCLIP ViT-H/14\nmodeltransformerYi-Chat</p>\n<img src=\"/41b6a819/multimodal.png\" class title=\"\">\n<p>3</p>\n<p>1224^2ViTprojection1-LAION-400MViTViTLLM</p>\n<p>2ViT448^2LAION-400M2000-480-CLLaVALLaVARFlickrVQAv2RefCOCOVisual7w</p>\n<p>3100-GQAVizWiz\nVQATextCapsOCR-VQAVisual\nGenomeShareGPT4V50,000</p>\n<p>128A1006B334B10</p>\n<h2 id=\"depth-upscaling-\">Depth Upscaling </h2>\n<p>326B489B</p>\n<p>Scaling large language models with simple yet effective depth\nup-scaling12-281648</p>\n<p>cosine\nsimilarity</p>\n<p></p>\n<img src=\"/41b6a819/9B.png\" class title=\"9B\">\n<p></p>\n<p>Depth Upscaling</p>\n<p>800B token</p>\n<p>70%</p>\n<p>constant lr = 3e-54M\ntokenbatch size</p>\n<p>batch\nsizeYi-6B</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<p><a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n","length":8965,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>01.AIAI20231101.AIYi-6BYi-34B\nbaseYi-6B-200KYi-34B-200K\nbaseYi01.AIchatYi-9B</p>\n<p>202311YiSuperCLUE/CMMLUYiYi</p>\n<p>20243Yi</p>\n<h1 id=\"tldr\">TL;DR</h1>\n<p></p>\n<ul>\n<li>Yi-34Bint4float16&lt;1%RTX409024G</li>\n<li>LLAMA2</li>\n<li>3.1Tscaling\nlaw1T</li>\n<li>&lt;10k</li>\n<li>4k</li>\n<li></li>\n</ul>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>Yi6B9B34B34B</p>\n<p>34B24GRTX4090</p>\n<p>int434B24GGPU</p>\n<p><a href=\"https://arxiv.org/abs/2301.12017\">Understanding INT4\nQuantization for Language Models: Latency Speedup, Composability, and\nFailure\nCases</a>Yi-34Bint8bf16&lt;1%int4</p>\n<img src=\"/41b6a819/eval.png\" class title=\"Yi\">\n<p>3.1T tokenDeepMindscaling\nlaw1TB&lt;2T</p>\n<p>Chinchilla<a href=\"https://arxiv.org/abs/2203.15556\">Training Compute-Optimal Large\nLanguage Models</a>scaling law</p>\n<p>scaling lawYiovertrain</p>\n<p><big><strong>Yi+--34B+70B</strong></big></p>\n<h2 id=\"\"></h2>\n<p>LLAMA2</p>\n<ul>\n<li>LLAMA270BGQAYiGQA<br>\n</li>\n<li>RoPERoPE ABFEffective long-context scaling of\nfoundation modelsbase10M<br>\n</li>\n<li>SwiGLUGLU Variants Improve Transformer</li>\n</ul>\n<p>activation\nsize4h8/3hGQA</p>\n<blockquote>\n<p>We use SwiGLU as Yis post-attention layer, reducing its activation\nsize from 4h to 8/3h (h denotes hidden size) to be consistent with the\nnormal post-attention layer. This adjustment also compensates for the\nreduction in parameter resulted from GQA, making the overall parameter\ncount comparible of existing 7B and 34B models.</p>\n</blockquote>\n<img src=\"/41b6a819/model.png\" class title=\"Yi\">\n<p><big><strong></strong></big></p>\n<h2 id=\"tokenizer\">tokenizer</h2>\n<ul>\n<li>BPE64000<br>\n</li>\n<li>digit<br>\n</li>\n<li>OOVunicode </li>\n<li></li>\n</ul>\n<p>LLMtokenizerYi</p>\n<h1 id=\"\"></h1>\n<p>LLMYi</p>\n<img src=\"/41b6a819/cover.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p></p>\n<img src=\"/41b6a819/pretrain_data_pipeline.png\" class title=\"\">\n<ol type=\"1\">\n<li> &amp; </li>\n</ol>\n<p></p>\n<p>CCNeTCCNet: Extracting High Quality Monolingual Datasets\nfrom Web Crawl Data</p>\n<ol start=\"2\" type=\"1\">\n<li> Heuristic Rule Filters</li>\n</ol>\n<p></p>\n<ul>\n<li>URL<br>\n</li>\n<li><br>\n</li>\n<li>n-gramScaling Language Models:\nMethods, Analysis &amp; Insights from Training\nGopherCulturaX: A Cleaned, Enormous, and\nMultilingual Dataset for Large Language Models in 167 Languages</li>\n<li>Personal Identifiable\nInformationPII</li>\n</ul>\n<ol start=\"3\" type=\"1\">\n<li> Learned Filters</li>\n</ol>\n<p>4scorer</p>\n<ul>\n<li>Perplexity ScorerCCNet: Extracting High Quality Monolingual\nDatasets from Web Crawl\nDatakenlmperplexity</li>\n<li>Quality\nScorer</li>\n<li>Document Coherence\nScorer</li>\n<li>Safety Scorer</li>\n</ul>\n<ol start=\"4\" type=\"1\">\n<li> Cluster-based Filters</li>\n</ol>\n<p></p>\n<ol start=\"5\" type=\"1\">\n<li></li>\n</ol>\n<p>The RefinedWeb Dataset for Falcon LLM: Outperforming Curated\nCorpora with Web Data, and Web Data\nOnlyminhash</p>\n<p></p>\n<img src=\"/41b6a819/pretrain_data_dist.png\" class title=\"\">\n<p>garbage\ningarbage out</p>\n<blockquote>\n<p>we prefer 3T tokens over sophasticated engineering over 10T tokens\nwithout extensive filtering</p>\n</blockquote>\n<p>10T3</p>\n<h2 id=\"\"></h2>\n<p>Quality is All You Need</p>\n<p>&lt;10kSFT</p>\n<p>Gemini: A family of highly capable multimodal\nmodels.Llama 2: Open Foundation and Fine-Tuned Chat\nModelsLima: Less is more for alignmentFLANScaling\ninstruction-finetuned language modelsUltraChatEnhancing chat\nlanguage models by scaling high-quality instructional\nconversations</p>\n<p></p>\n<ul>\n<li><big><strong>prompt distribution\nselection</strong></big>Wizardlm: Empowering large language\nmodels to follow complex\ninstructionsSFT<br>\n</li>\n<li><big><strong>CoT data formatting</strong></big>Take a\nstep back: Evoking reasoning via abstraction in large language\nmodelsStep-Back<br>\n</li>\n<li><big><strong>response formatting</strong></big>Lima:\nLess is more for\nalignmentresponseintroduction-body-conclusionwhere\nthe body is usually a list of bullet point<br>\n</li>\n<li><big><strong></strong></big>responseresponse<br>\n</li>\n<li><big><strong></strong></big>response<br>\n</li>\n<li><big><strong></strong></big>#instag:\nInstruction tagging for analyzing supervised fine-tuning of large\nlanguage\nmodels<br>\n</li>\n<li><big><strong></strong></big>How\nabilities in large language models are affected by supervised\nfine-tuning data compositionapproximate grid\nsearch{1, 1/2, 1/4, 1/8, 1/16, 1/32,\n1/64}<br>\n</li>\n<li><big><strong></strong></big>OPENAIChatML<a href=\"https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md\">https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md</a>system\npromptuser inputbot response</li>\n</ul>\n<p>SFT</p>\n<img src=\"/41b6a819/sft.png\" class title=\"SFT\">\n<h1 id=\"\"></h1>\n<h2 id=\"infra\">infra</h2>\n<p>Yi</p>\n<ol type=\"1\">\n<li></li>\n<li></li>\n<li>DPOMegatronDeepSpeed</li>\n<li>LLMcontinuous batching  paged\nattention</li>\n</ol>\n<p>UI</p>\n<h2 id=\"\"></h2>\n<p>4k</p>\n<h2 id=\"\"></h2>\n<p></p>\n<ul>\n<li>AdamWbeta=[0.9,0.999]epsilon = 1e-8<br>\n</li>\n<li>seq_len = 4096<br>\n</li>\n<li>batch size = 64<br>\n</li>\n<li>constant lr = 1e-5weight decay = 0.1<br>\n</li>\n<li>gradient clip = 1.0<br>\n</li>\n<li>max step = 300</li>\n<li>Neftune: Noisy embeddings improve instruction\nfinetuning6B noise scale = 534B noise scale =\n45</li>\n</ul>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>Yi</p>\n<img src=\"/41b6a819/base_model_eval.png\" class title=\"Base\">\n<p>GPT3.5GPT4Yi</p>\n<p>Yi</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<ul>\n<li>Yi-34BYi-6BYi-34BYi-6B<br>\n</li>\n<li>Yi-34BQwen-14BFalcon-180B</li>\n<li>GPT-4LLMLLMGPT-4GPT-3.5LLMQwen-14BYi-34BC-EvalCMMLUGaokaoGPT-4BBHHumanEvalMATH</li>\n</ul>\n<ol start=\"3\" type=\"1\">\n<li>In-Context Learning</li>\n</ol>\n<p>Yiin-context\nlearning-underlying\nfunction</p>\n<p> y = w1x1 + w2x2 +\n... + wnxn</p>\n<p> x1, x2, ..., xn, y x \ny</p>\n<p> w1, w2, ..., wn</p>\n<p>a y  y  |y  y|\nb y == y </p>\n<p>underlying\nfunction</p>\n<p>[1,-1]Yi-34BLLAMA-70B</p>\n<p>[11111]LLAMA-70BMistral\n8*7B</p>\n<img src=\"/41b6a819/ict.png\" class title=\"ICT\">\n<h2 id=\"chat\">Chat</h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>basezero-shotfew-shot</p>\n<img src=\"/41b6a819/eval.png\" class title=\"Yi\">\n<p>Goodharts\nprinciple</p>\n<p>Yi-34B-ChatYi-6B-ChatSFT</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<img src=\"/41b6a819/third_party.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<p>base3</p>\n<h2 id=\"\"></h2>\n<p>4kbase200kSFT</p>\n<p>attentionattentionsparse\nattention</p>\n<p>12length-upsampled\nlong-context\ndata3</p>\n<p>recitation</p>\n<p>Data engineering for scaling language\nmodels to 128k contextParaphrasing the original text makes high\naccuracy long-context qa</p>\n<p>5B tokenbatch\nsize=4Mtoken100step1005B/4M=1250</p>\n<blockquote>\n<p>We continue pretrain the model on 5B tokens with 4M batch size, which\ntranslate to 100 optimization steps. Aligning with the concurrent work\nfrom Fu et al. [22], we observe that such light-weight continue\npretraining is already able to enable a strong performance on\nNeedle-in-a-Haystack test, as we will show in Figure 6.</p>\n</blockquote>\n<p>Data engineering for scaling language models to 128k\ncontext</p>\n<p>SFT</p>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<img src=\"/41b6a819/long_context_result.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p>ViTCLIP ViT-H/14\nmodeltransformerYi-Chat</p>\n<img src=\"/41b6a819/multimodal.png\" class title=\"\">\n<p>3</p>\n<p>1224^2ViTprojection1-LAION-400MViTViTLLM</p>\n<p>2ViT448^2LAION-400M2000-480-CLLaVALLaVARFlickrVQAv2RefCOCOVisual7w</p>\n<p>3100-GQAVizWiz\nVQATextCapsOCR-VQAVisual\nGenomeShareGPT4V50,000</p>\n<p>128A1006B334B10</p>\n<h2 id=\"depth-upscaling-\">Depth Upscaling </h2>\n<p>326B489B</p>\n<p>Scaling large language models with simple yet effective depth\nup-scaling12-281648</p>\n<p>cosine\nsimilarity</p>\n<p></p>\n<img src=\"/41b6a819/9B.png\" class title=\"9B\">\n<p></p>\n<p>Depth Upscaling</p>\n<p>800B token</p>\n<p>70%</p>\n<p>constant lr = 3e-54M\ntokenbatch size</p>\n<p>batch\nsizeYi-6B</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<p><a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n"},{"title":"(2)","abbrlink":"ad0bba9d","date":"2024-03-24T03:24:47.000Z","_content":"\n![](/images/cover.png)  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~~\n\n***  \n\n# 1Berttoken embedding()position encoding  \n\n1concat+  \n\n2(768)250076810007681-1  \n\n3one-hotone-hot  \n\n4  \n\n# 2LoRALoRA  \n\n1LoRALoRA  \n\n2//LoRA  \n\n3LoRA1batch23optionalint8/int4  \n\n# 3normalizationbatchnorm/layernorm  \n\n118018normalization/  \n\n2batchnormICSinternal covariate shifti.i.d.normalization  \n\n3.How Does Batch Normalization Help Optimization?batchnormICSbatchnormICSbatchnorm  \n\n# 4Transformerpre-normpost-norm?  \n\n1.Transformerpost-normadd & normpost-norm  \n\n2.Pre-normpost-normL+1L  \n\n3.post-normpre-normpost-normpre-norm  \n\n# 5Multi-Head Attentionhidden size=DhdD=dhsbatch size=1self-attentionFloat Operations  \n\n1.QKV6  s  D^22D  \n\n2.QKh  2  d  s^2h  \n\n3.scalingh  s^2  \n\n4.softmaxh  3  s^2softmaxsexpsexps  \n\n5.reductionVh  2  d  s^2  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)  \n","source":"_posts/cs/nlp/2024/03/-2.md","raw":"---\ntitle: (2)\ntags:\n  - NLP\n  - LLM\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: ad0bba9d\ndate: 2024-03-24 11:24:47\n---\n\n![](/images/cover.png)  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)\n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~~\n\n***  \n\n# 1Berttoken embedding()position encoding  \n\n1concat+  \n\n2(768)250076810007681-1  \n\n3one-hotone-hot  \n\n4  \n\n# 2LoRALoRA  \n\n1LoRALoRA  \n\n2//LoRA  \n\n3LoRA1batch23optionalint8/int4  \n\n# 3normalizationbatchnorm/layernorm  \n\n118018normalization/  \n\n2batchnormICSinternal covariate shifti.i.d.normalization  \n\n3.How Does Batch Normalization Help Optimization?batchnormICSbatchnormICSbatchnorm  \n\n# 4Transformerpre-normpost-norm?  \n\n1.Transformerpost-normadd & normpost-norm  \n\n2.Pre-normpost-normL+1L  \n\n3.post-normpre-normpost-normpre-norm  \n\n# 5Multi-Head Attentionhidden size=DhdD=dhsbatch size=1self-attentionFloat Operations  \n\n1.QKV6  s  D^22D  \n\n2.QKh  2  d  s^2h  \n\n3.scalingh  s^2  \n\n4.softmaxh  3  s^2softmaxsexpsexps  \n\n5.reductionVh  2  d  s^2  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)\n\n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)\n\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)\n\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n\n[LLM:RoPE](http://www.linsight.cn/a051710f.html)\n\n[(1)](http://www.linsight.cn/3345028a.html)  \n","slug":"cs/nlp/2024/03/-2","published":1,"updated":"2024-03-24T04:16:09.176Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmt00100p4ketyogyon","content":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM<sub></sub></p>\n<hr>\n<h1 id=\"berttoken-embeddingposition-encoding\">1Berttoken\nembedding()position encoding</h1>\n<p>1concat+</p>\n<p>2(768)250076810007681-1</p>\n<p>3one-hotone-hot</p>\n<p>4</p>\n<h1 id=\"loralora\">2LoRALoRA</h1>\n<p>1LoRALoRA</p>\n<p>2//LoRA</p>\n<p>3LoRA1batch23optionalint8/int4</p>\n<h1 id=\"normalizationbatchnormlayernorm\">3normalizationbatchnorm/layernorm</h1>\n<p>118018normalization/</p>\n<p>2batchnormICSinternal\ncovariate\nshifti.i.d.normalization</p>\n<p>3.How Does Batch Normalization Help\nOptimization?batchnormICSbatchnormICSbatchnorm</p>\n<h1 id=\"transformerpre-normpost-norm\">4Transformerpre-normpost-norm?</h1>\n<p>1.Transformerpost-normadd &amp;\nnormpost-norm</p>\n<p>2.Pre-normpost-normL+1L</p>\n<p>3.post-normpre-normpost-normpre-norm</p>\n<h1 id=\"multi-head-attentionhidden-sizedhdddhsbatch-size1self-attentionfloat-operations\">5Multi-Head\nAttentionhidden\nsize=DhdD=dhsbatch\nsize=1self-attentionFloat\nOperations</h1>\n<p>1.QKV6  s \nD^22D</p>\n<p>2.QKh  2  d  s^2h</p>\n<p>3.scalingh  s^2</p>\n<p>4.softmaxh  3 \ns^2softmaxsexpsexps</p>\n<p>5.reductionVh  2  d  s^2</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n","length":2220,"excerpt":"","more":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM<sub></sub></p>\n<hr>\n<h1 id=\"berttoken-embeddingposition-encoding\">1Berttoken\nembedding()position encoding</h1>\n<p>1concat+</p>\n<p>2(768)250076810007681-1</p>\n<p>3one-hotone-hot</p>\n<p>4</p>\n<h1 id=\"loralora\">2LoRALoRA</h1>\n<p>1LoRALoRA</p>\n<p>2//LoRA</p>\n<p>3LoRA1batch23optionalint8/int4</p>\n<h1 id=\"normalizationbatchnormlayernorm\">3normalizationbatchnorm/layernorm</h1>\n<p>118018normalization/</p>\n<p>2batchnormICSinternal\ncovariate\nshifti.i.d.normalization</p>\n<p>3.How Does Batch Normalization Help\nOptimization?batchnormICSbatchnormICSbatchnorm</p>\n<h1 id=\"transformerpre-normpost-norm\">4Transformerpre-normpost-norm?</h1>\n<p>1.Transformerpost-normadd &amp;\nnormpost-norm</p>\n<p>2.Pre-normpost-normL+1L</p>\n<p>3.post-normpre-normpost-normpre-norm</p>\n<h1 id=\"multi-head-attentionhidden-sizedhdddhsbatch-size1self-attentionfloat-operations\">5Multi-Head\nAttentionhidden\nsize=DhdD=dhsbatch\nsize=1self-attentionFloat\nOperations</h1>\n<p>1.QKV6  s \nD^22D</p>\n<p>2.QKh  2  d  s^2h</p>\n<p>3.scalingh  s^2</p>\n<p>4.softmaxh  3 \ns^2softmaxsexpsexps</p>\n<p>5.reductionVh  2  d  s^2</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a></p>\n<p><a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p><a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a></p>\n<p><a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a></p>\n<p><a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a></p>\n<p><a href=\"http://www.linsight.cn/3345028a.html\">(1)</a></p>\n"},{"title":"ChatGPT","abbrlink":"14e576c","date":"2023-03-17T02:36:53.000Z","_content":"\n{% asset_img 1.png page_1 %}  \n\n{% asset_img 2.png page_2 %}  \n\n{% asset_img 3.png page_3 %}  \n\n{% asset_img 4.png page_4 %}  \n\n{% asset_img 5.png page_5 %}  \n\n{% asset_img 6.png page_6 %}  \n\n{% asset_img 7.png page_7 %}  \n\n{% asset_img 8.png page_8 %}  \n\n{% asset_img 9.png page_9 %}  \n\n{% asset_img 10.png page_10 %}  \n\n{% asset_img 11.png page_11 %}  \n\n{% asset_img 12.png page_12 %}  \n\n{% asset_img 13.png page_13 %}  \n\n{% asset_img 14.png page_14 %}  \n\n{% asset_img 15.png page_15 %}  \n\n{% asset_img 16.png page_16 %}  \n\n{% asset_img 17.png page_17 %}  \n\n{% asset_img 18.png page_18 %}  \n\n{% asset_img 19.png page_19 %}  \n\n{% asset_img 20.png page_20 %}  \n\n{% asset_img 21.png page_21 %}  \n\n{% asset_img 22.png page_22 %}  \n\n{% asset_img 23.png page_23 %}  \n\n{% asset_img 24.png page_24 %}  \n\n{% asset_img 25.png page_25 %}  \n\n{% asset_img 26.png page_26 %}  \n\n{% asset_img 27.png page_27 %}  \n\n{% asset_img 28.png page_28 %}  ","source":"_posts/cs/nlp/2024/03/ChatGPT.md","raw":"---\ntitle: ChatGPT\ntags:\n  - NLP\n  - LLM\n  - ChatGPT\n  - Sparrow\n  - LaMDA\n  - GopherCite\n  - WebGPT\n  - InstructGPT\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 14e576c\ndate: 2023-03-17 10:36:53\n---\n\n{% asset_img 1.png page_1 %}  \n\n{% asset_img 2.png page_2 %}  \n\n{% asset_img 3.png page_3 %}  \n\n{% asset_img 4.png page_4 %}  \n\n{% asset_img 5.png page_5 %}  \n\n{% asset_img 6.png page_6 %}  \n\n{% asset_img 7.png page_7 %}  \n\n{% asset_img 8.png page_8 %}  \n\n{% asset_img 9.png page_9 %}  \n\n{% asset_img 10.png page_10 %}  \n\n{% asset_img 11.png page_11 %}  \n\n{% asset_img 12.png page_12 %}  \n\n{% asset_img 13.png page_13 %}  \n\n{% asset_img 14.png page_14 %}  \n\n{% asset_img 15.png page_15 %}  \n\n{% asset_img 16.png page_16 %}  \n\n{% asset_img 17.png page_17 %}  \n\n{% asset_img 18.png page_18 %}  \n\n{% asset_img 19.png page_19 %}  \n\n{% asset_img 20.png page_20 %}  \n\n{% asset_img 21.png page_21 %}  \n\n{% asset_img 22.png page_22 %}  \n\n{% asset_img 23.png page_23 %}  \n\n{% asset_img 24.png page_24 %}  \n\n{% asset_img 25.png page_25 %}  \n\n{% asset_img 26.png page_26 %}  \n\n{% asset_img 27.png page_27 %}  \n\n{% asset_img 28.png page_28 %}  ","slug":"cs/nlp/2024/03/ChatGPT","published":1,"updated":"2024-03-17T03:28:29.054Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmu00130p4k6fjxdg86","content":"<img src=\"/14e576c/1.png\" class title=\"page_1\">\n<img src=\"/14e576c/2.png\" class title=\"page_2\">\n<img src=\"/14e576c/3.png\" class title=\"page_3\">\n<img src=\"/14e576c/4.png\" class title=\"page_4\">\n<img src=\"/14e576c/5.png\" class title=\"page_5\">\n<img src=\"/14e576c/6.png\" class title=\"page_6\">\n<img src=\"/14e576c/7.png\" class title=\"page_7\">\n<img src=\"/14e576c/8.png\" class title=\"page_8\">\n<img src=\"/14e576c/9.png\" class title=\"page_9\">\n<img src=\"/14e576c/10.png\" class title=\"page_10\">\n<img src=\"/14e576c/11.png\" class title=\"page_11\">\n<img src=\"/14e576c/12.png\" class title=\"page_12\">\n<img src=\"/14e576c/13.png\" class title=\"page_13\">\n<img src=\"/14e576c/14.png\" class title=\"page_14\">\n<img src=\"/14e576c/15.png\" class title=\"page_15\">\n<img src=\"/14e576c/16.png\" class title=\"page_16\">\n<img src=\"/14e576c/17.png\" class title=\"page_17\">\n<img src=\"/14e576c/18.png\" class title=\"page_18\">\n<img src=\"/14e576c/19.png\" class title=\"page_19\">\n<img src=\"/14e576c/20.png\" class title=\"page_20\">\n<img src=\"/14e576c/21.png\" class title=\"page_21\">\n<img src=\"/14e576c/22.png\" class title=\"page_22\">\n<img src=\"/14e576c/23.png\" class title=\"page_23\">\n<img src=\"/14e576c/24.png\" class title=\"page_24\">\n<img src=\"/14e576c/25.png\" class title=\"page_25\">\n<img src=\"/14e576c/26.png\" class title=\"page_26\">\n<img src=\"/14e576c/27.png\" class title=\"page_27\">\n<img src=\"/14e576c/28.png\" class title=\"page_28\">\n","length":0,"excerpt":"","more":"<img src=\"/14e576c/1.png\" class title=\"page_1\">\n<img src=\"/14e576c/2.png\" class title=\"page_2\">\n<img src=\"/14e576c/3.png\" class title=\"page_3\">\n<img src=\"/14e576c/4.png\" class title=\"page_4\">\n<img src=\"/14e576c/5.png\" class title=\"page_5\">\n<img src=\"/14e576c/6.png\" class title=\"page_6\">\n<img src=\"/14e576c/7.png\" class title=\"page_7\">\n<img src=\"/14e576c/8.png\" class title=\"page_8\">\n<img src=\"/14e576c/9.png\" class title=\"page_9\">\n<img src=\"/14e576c/10.png\" class title=\"page_10\">\n<img src=\"/14e576c/11.png\" class title=\"page_11\">\n<img src=\"/14e576c/12.png\" class title=\"page_12\">\n<img src=\"/14e576c/13.png\" class title=\"page_13\">\n<img src=\"/14e576c/14.png\" class title=\"page_14\">\n<img src=\"/14e576c/15.png\" class title=\"page_15\">\n<img src=\"/14e576c/16.png\" class title=\"page_16\">\n<img src=\"/14e576c/17.png\" class title=\"page_17\">\n<img src=\"/14e576c/18.png\" class title=\"page_18\">\n<img src=\"/14e576c/19.png\" class title=\"page_19\">\n<img src=\"/14e576c/20.png\" class title=\"page_20\">\n<img src=\"/14e576c/21.png\" class title=\"page_21\">\n<img src=\"/14e576c/22.png\" class title=\"page_22\">\n<img src=\"/14e576c/23.png\" class title=\"page_23\">\n<img src=\"/14e576c/24.png\" class title=\"page_24\">\n<img src=\"/14e576c/25.png\" class title=\"page_25\">\n<img src=\"/14e576c/26.png\" class title=\"page_26\">\n<img src=\"/14e576c/27.png\" class title=\"page_27\">\n<img src=\"/14e576c/28.png\" class title=\"page_28\">\n"},{"title":"MoE","abbrlink":"44e38c1b","date":"2024-03-30T01:56:05.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n202434MoEQwen1.5-MoEDBRXJambaMistral  \n\nMoE  \n\n<center>\n\n|  |  |  |\n| :----: | :----: | :----: |\n| GPT4 | 20233 | 236George HotzGPT48220B |\n| Mistral-87B | 202312 | Mistral AI |\n| LLAMA-MoE | 202312 | github |\n| DeepSeek-MoE | 20241 | MoE |\n| abab6 |20241 | MiniMaxMoE |\n| 2.0 | 20242 |  |\n| Step-2 | 20243 |  |\n| MM1 | 20243 | MoE |\n| Grok-1 | 20243 | X |\n| Qwen1.5-MoE-A2.7B| 20243 |  |\n| DBRX | 20243 | Databricks |\n| Jamba | 20243 | AI21 |\n| Mistral-822B | 20244 | Mistral AI |\n| WizardLM-2-822B | 20244 |  |\n| 3.0 | 20244 | 400BMoE |\n| Arctic | 20244 | Snowflake480BDense-MoE Hybrid |\n\n</center>  \n\nMoEMoE  \n\n{% asset_img xiaomi_moe.jpg MoE %}  \n\nMoE  \n\nMoEMoEMoE  \n\n20244DeepSeek-MoEQwen1.5-MoE\n\n#   \n\nMoE  \n\nMoEGoogle\n\n##   \n\nMoE1991[Adaptive Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)Geoffrey HintonMichael I. JordanMoE1988  \n\n>This idea was first presented by Jacobs and Hinton at the Connectionist Summer School in Pittsburg in 1988.  \n\nMoEMoE  \n\n## RNN  \n\nGoogle20171[Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)MoELSTM137B128kLSTM  \n\n## Transformer  \n\n1. 20206Google[GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)MoEencoder-decodertransformerFFNMoE12.5B600BMoE2048  \n\n2. 20211Google[Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) T5encoder-decoderFFNMoErouting1.6Tswitch transformerSwitch Transformersscaling  \n\n3. 20222Google[ST-MoE: Designing Stable and Transferable Sparse Expert Models](https://arxiv.org/abs/2202.08906)encoder-decoderMoE269B32BST-MoEMoESwitch Transformer  \n\n## GPT  \n\n1. 202112GoogleGLaM[GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://arxiv.org/abs/2112.06905)1.2Tdecoder-onlyencoder-decoderdecoder-onlyGoogle  \n\n2. 20241[DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/abs/2401.06066)2312DeepSeekMoE  \n\n3. 2024DatabricksDBRXQwen1.5-MoE-A2.7BMistral AIMistral-8x22B  \n\n#   \n\nGeoffrey HintonMichael I. Jordan[Adaptive Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)MoE  \n\n1.   \n\n  \n\n  \n\nMoEexpert  \n\nMoEvowel discrimination taskMoEaeiou  \n\n2.   \n\nMoEexpert networkgating networkexpertgating networkexpertexpertstochastictruefalse  \n\n{% asset_img vanilla_moe.png Vanilla MoE %}  \n\n3.   \n\nMoEideaJacobsHinton1988lossensembleexpertexpertexpertresidual  \n\ncase $c$ $d^c$ ground truth $i$ expert $o_{i}^c$$p_{i}^c$ gating network $i$ expert $E^{c}$ \n\n$$E^{c}=\\left|\\left|d^{c}-\\sum_{i}p_{i}^{c}o_{i}^{c}\\right|\\right|^{2}$$\n\nexpert  \n\nexpertexpertexpert  \n\nexpertexpertexpertexpertgating network  \n\n  \n\nHintonJordanlossexpert  \n\ngating networkexpert  \n\n$$E^{c}=\\langle\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2\\rangle=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}$$\n\nexpertexpertexpert  \n\nlosslocalizationcasegating networkexpertgating networkexpert  \n\nlocalizationexpert  \n\nexpertexpertgating networkexpert error+-  \n\nexpert0  \n\n4. \n\nlosslossloss  \n\n$$\\text{loss}E^{c}=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}$$  \n\n$$\\text{loss}E^c=-log\\sum_ip_i^ce^{-\\frac12\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}$$  \n\nlossloss  \n\n$$\\text{loss}\\frac{\\partial E^c}{\\partial\\mathbf{o}_i^c}=-2p_i^c(\\mathbf{d}^c-\\mathbf{o}_i^c)$$  \n\n$$\\text{loss}\\frac{\\partial E^c}{\\partial\\mathbf{o}_i^c}=-\\left[\\frac{p_i^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}}{\\sum_jp_j^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_j^c\\|^2}}\\right](\\mathbf{d}^c-\\mathbf{o}_i^c)$$  \n\nlossloss $i$ expertexpertexpert $i$ casegating networklosscaseexpertlosslossexpertlocalizationexpert  \n\nBTWloss  \n\nMoE  \n\n# LSTM MoE  \n\nGoogle20171\n[OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER](https://arxiv.org/abs/1701.06538)MoELSTM137BLSTM7  \n\n1991  \n\n## \n\nTransformer  \n\nconditional computationconditional computationMoE  \n\n  \n\n- MoEexpertbatch sizebatch size  \nbatch size3216expertexpert2batch sizebatch sizebatch size  \n-   \nNLP  \n-   \n  \n-   \nGPU  \n- GPU  \nGPUbranchingif/elseMoEgating network  \n\n  \n\n## \n\n1.   \n\n  \n\nLSTMMoEembedding  \n\n{% asset_img rnn_moe.png LSTM MoE %}  \n\nexpertfeed-forward neural networknexpertgating networkn  \n\n$$\\begin{aligned}y=\\sum_{i=1}^nG(x)_iE_i(x)\\end{aligned}$$  \n\n$E_{i}(x)$  $i$ expert$G(x)_{i}$ gating network $i$ expert  \n\n $G(x)_{i}$ 0expert  \n\nexperttwo-level hierarchical MoEgating networkgating networkexpertgating networkexpertword2vechierarchical softmax  \n\n2. gating network  \n\ngating network  \n\nsoftmaxgating function  \n\n$$\\begin{aligned}G_\\sigma(x)=Softmax(x\\cdot W_g)\\end{aligned}$$  \n\ntopkksoftmax0expert  \n\nsparsitytopkgating function  \n\nGaussian noisenoise  \n\n$$\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}$$  \n\n$$KeepTopK(v,k)_i=\\begin{cases}v_i&\\text{if }v_i\\text{ is in the top }k\\text{ elements of }v.\\\\-\\infty&\\text{otherwise.}\\end{cases}$$  \n\n$$\\begin{aligned}H(x)_i=(x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\end{aligned}$$  \n\nnoisesoftplusReLU  \n\n{% asset_img softplus.png softplus %}  \n\n  \n\n##   \n\nMoEgating networkexpertexpert  \n\n  \n\nhard constraintexperthard constraintsoft constraint  \n\nexpert  \n\n$$Importance(X)=\\sum_{x\\in X}G(x)$$  \n\n$G(x)$ gating networkexpert  \n\n $L_{importance}$$L_{importance}$   \n\n$$L_{importance}(X)=w_{importance}\\cdot CV(Importance(X))^2$$  \n\n $w_{importance}$ 0.1CVcoefficient of variation  \n\ncoefficient of variation $\\sigma$   $\\mu$   \n\nMoEexpertexpertgating $L_{importance}$   \n\n $L_{importance}$  $L_{importance}$  $L_{importance}$   \n\nexpertexpertgating  \n\n $L_{load}$ expert  \n\nexpertback propagation $L_{load}$ expert  \n\nMoE $H(x)$ KeepTopK  \n\n$$\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}$$  \n\n$$\\begin{aligned}H(x)_i=(x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\end{aligned}$$  \n\n $kth\\_excluding(H(x),k,i)$ $H(x)$  $i$  $k$  $P(x,i)$ noise $i$ noise $kth\\_excluding(H(x),k,i)$   \n\n$$\\begin{aligned}P(x,i)=Pr\\Big((x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\\\>kth\\_excluding(H(x),k,i)\\Big)\\end{aligned}$$  \n\nnoise $i$  $i$ $P(x,i)$   \n\n$$\\begin{aligned}P(x,i)&=\\Phi\\Big(\\frac{(x\\cdot W_g)_i-kth\\_excluding(H(x),k,i)}{Softplus((x\\cdot W_{noise})_i)}\\Big)\\end{aligned}$$  \n\n $\\Phi$ CDF  \n\n $i$ expert  \n\n$$\\begin{aligned}Load(X)_i=\\sum_{x\\in X}P(x,i)\\end{aligned}$$  \n\nexpert  \n\n$$L_{load}(X)=w_{load}\\cdot CV(Load(X))^2$$  \n\n$w_{load}$ 0.1  \n\n $L_{importance}(X)$$Load(X)$   \n\nexpert $W_g$   $W_{noise}$ 0  \n\n  \n\n{% asset_img rnn_moe_load_function.png  %}  \n\n  \n\n##   \n\n1.   \n\n  \n\n1batch size  \n\nexpertbatch sizenexpertkbatch sizebexpertbatch sizekb/nexpertbatch size  \n- batchbatchMoEexpertdexpertkbd/nbatch size\n- LSTMbatch size\n\n2  \n\n  \n\nexpertinputoutput[input_size, hidden_size][hidden_size, output_size]GPU1000hidden_sizeexpert1000  \n\n2.  &   \n\nMoEdense4/32/256expertflat MoE256/1024/4096experthierarchical MoEexpert1Mflat4experthierarchical MoEgating2  \n\nppldenseMoEMoE\n\n{% asset_img rnn_moe_perf.png  %}  \n\n3.   \n\n4Bdiminishing returns  \n\n + 100B token32, 256, 10244096, 16384, 65536, 131072expertMoE137B  \n\n  \n\n{% asset_img rnn_moe_137b.png 137 %}  \n\n  \n\n4. Expert Specialization  \n\nMoE\n\ntokenspecialization  \n\n{% asset_img rnn_moe_specilized.png RNN MoE  %}  \n\n# GShard\n\n1. \n\n2018Berttransformer20206GoogleGShard: Scaling Giant Models with Conditional Computation and Automatic ShardingMoEencoder-decodertransformerMoE  \n\nGShardMoE600B  \n\n{% asset_img gshard_moe_family.png GShard MoE family %}  \n\nexpertLSMT MoE -- expert24expertChatGPTBertGPT  \n\nGShardMoE  \n\n2. \n\n  \n\nGoogleencoder-decoder transfomerGShardencoder-decoder transfomer  \n\nGShardencoderdecoderFFNMoENN/2MoE  \n\n{% asset_img gshard_model.png GShard %}  \n\ntop-2 expert  \n\nGShardLSTM MoEgating functionauxiliary loss function  \n\nMoE\n\n$$\\begin{aligned}\n\\mathcal{G}_{s,E}& =\\mathrm{GATE}(x_s)  \\\\\n\\mathrm{FFN}_e(x_s)& =wo_e\\cdot\\text{ReLU}(wi_e\\cdot x_s)  \\\\\ny_{s}& =\\sum_{e=1}^E\\mathcal{G}_{s,e}\\cdot\\mathrm{FFN}_e(x_s) \n\\end{aligned}$$\n\n $x_s$ MoEtoken$w_i$  $w_o$ $\\mathcal{G}_{s}$ gating function\n\nGShardgating function12  \n\nNtokenEexpertNEgating function  \n\ngating function  \n\n1 expert capacity  \n\nexperttokenexperttoken2N/E  \n\nexpert capacityGATE()expert $c_e$ tokentokenexpert  \n\n2 Local group dispatching  \n\ntokenG2N/EG  \n\nbatchbatchbatchgroupall2allgroup  \n\ngroupgradient accumulation  \n\n3 Auxiliary loss  \n\ngatingLSTM MoE  \n\n$$\\ell_{aux}=\\frac1E\\sum_{e=1}^E\\frac{c_e}S\\cdot m_e$$  \n\n$S$ token$E$ $c_e$  $e$ token$m_e$  $e$ expert $S$ token  \n\n $\\frac{c_e}S$  $\\frac{c_e}S$  $m_e$  $m_e$  $e$ expert $S$ tokenloss  \n\nloss  \n\ngating  \n\n{% asset_img gshard_algo_1.png GShard gating  %}  \n\n4 Random routing  \n\ntop-2 experttop-1  \n\ntop-1g2  \n\n3.   \n\n\n\n{% asset_img gshard_perf.png GShard %}  \n\n# Switch Transformer\n\n20224ChatGPTGoogleSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity2021GoogleSwitch Transformer  \n\nSwitch TransformerGShardencoder-decoderT51.6T2048expert  \n\nSwitch Transformer  \n\nSwitch TransformerSwitch TransformerFLOPS/token  \n\nSwitch Transformer  \n\n1TransformerMoESwitch Transformer  \n\n2MoE to denseMoEdenseMoE99%dense  \n\n3  \n- bf16MoE  \n- MoE  \n-   \n\n41TMoE  \n\n5101  \n\n6FLOPS/tokenSwitch Transformer  \n\n##   \n\nSwitch TransformerGShardtransformerFFNMoE  \n\n{% asset_img switch_transformer_structure.png Switch Transformer  %}  \n\nSwitch Transformergating functionSwitch Transformerrouting  \n\nkexpertSwitch Transformergating1expertk=1MoESwitch layer  \n\nroutingrouter  \n\n##   \n\nGShardSwitch Transformerexpert capacityexpertbatchtoken  \n\ntokenexpertoverflowtokenGShard  \n\nSwitch Transformercapacity factor  \n\n$$\\text{expert capacity}=\\left(\\frac{\\text{tokens per batch}}{\\text{number of experts}}\\right)\\times\\text{capacity factor}.$$  \n\ncapacity factorexperttokenoverflow\n\ncapacity factor  \n\n{% asset_img switch_transformer_diff_expert_capacity.png expert capacity %}  \n\nexpert capacity\n\ncapacity factor1overflowcapacity factor  \n\nexpertoverflowMoESwitch Transformer128  \n\ncapacity factoroverflow  \n\n{% asset_img switch_transformer_capacity_effect.png expert capacity %}  \n\ntokenscalingoverflow  \n\n  \n\n $N$ expert $T$ tokenbatch $\\mathcal{B}$ \n\n$$\\begin{aligned}\\text{loss}&=\\alpha\\cdot N\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}$$  \n\n$f_{i}$  $i$ experttoken  \n\n$$\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax }p(x)=i\\}\\end{aligned}$$  \n\n$P_i$ batchtoken$i$ expert  \n\n$$\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}$$  \n\nGShard  \n\n$f$  $P$  $1/N$  \n\n$\\alpha$ 1e-51e-11e-2  \n\n $\\sum_{i=1}^N(f_i\\cdot P_i)=\\sum_{i=1}^N(\\frac1N\\cdot\\frac1N)=\\frac1N$loss $N$expertloss  \n\n## \n\n1. trick  \n\n1bf16  \n\nbf16routing function  \n\nroutingsoftmaxexponentialrounding errorrouting  \n\n2  \n\n $\\mu=0$$\\sigma=\\sqrt{s}/n$sne.g. fan-in  \n\nTransformers=1.010  \n\n{% asset_img switch_transformer_init.png  %}  \n\n3dropout  \n\nSwitch Transformerdropout  \n\n{% asset_img switch_transformer_dropout.png dropout %}  \n\ndropoutdense0.1expertdropout  \n\n2. scaling  \n\nSwitch Transformerscaling  \n\n1Step-Basis  \n\nstepexpert  \n\nstepstep  \n\n{% asset_img switch_transformer_scaling_step.png step scaling %}  \n\n2Time-Basis  \n\nSwitch TransformerstepSwitch Transformerdense  \n\nSwitch TransformerdenseSwitch Transformerdensedense1/7  \n\n{% asset_img switch_transformer_scaling_time.png time scaling %}  \n\n3dense\n\nSwitch TransformerdenseSwitch Transformerdense  \n\nStep-BasisTime-Basis64Switch TransformerT5-LargestepSwitch Transformer  \n\n{% asset_img switch_transformer_scaling_dense.png dense %}  \n\n3. SFT  \n\nGLUESuperGLUEdense  \n\neval  \n\n{% asset_img switch_transformer_sft_result.png sft %}  \n\n4.   \n\nSwitch TransformerBTdense  \n\n  \n- Switch Transformerdense  \n- label25%75%ground truth  \n\ndensedensedenseSwitch Transformer30%  \n\n{% asset_img switch_transformer_distill.png  %}  \n\n99%  \n\n{% asset_img switch_transformer_distill_diff_model.png  %}  \n\nSuperGLUE  \n\n{% asset_img switch_transformer_distill_sft.png sft %}  \n\n# GLaM\n\n1. \n\n202112GoogleGLaM: Efficient Scaling of Language Models with Mixture-of-Experts1.2T64token96.6BMoE  \n\nSwitch TransformerGLaM1.6T token  \n\n  \n\n{% asset_img glam_related_model.png glam %}  \n\nGPT-3175BGPT-3NLPGPT-3  \n\n{% asset_img glam_compare_gpt3.png glamgpt3 %}  \n\n{% asset_img glam_compare_gpt3_2.png glamgpt3 %}  \n\n2. \n\nSwitch TransformerFFNMoESwitch TransformerGLaMexpert  \n\n{% asset_img glam_model.png glam %}  \n\n  \n\n1  \n\nXLNET  \n\n2\n\n> In the non-MoE Transformer feed-forward sub-layers, we replace the first linear projection and the activation function with the Gated Linear Unitwhich computes the component-wise product of two linear transformation of the input, followed by a Gaussian Error Linear Unit.  \n\n3. \n\ntrick  \n\n1Lingvo: a modular and scalable framework for sequence-to-sequence modelingNaNInf  \n\n2BPNaNInfcheckpointNaNInf  \n\nMoE  \n\n{% asset_img glam_family.png glam %}  \n\nGLaMdense  \n\n{% asset_img glam_perf.png glam %}  \n\nGLaM MoEdense  \n\n# ST-MoE  \n\n20222GoogleST-MOE: DESIGNING STABLE AND TRANSFERABLE SPARSE EXPERT MODELSST-MoEMoEMoE  \n\nST-MoE269B32B denseStable Transferable Mixture-of-ExpertsST-MoE-32B  \n\nMoEST-MoESwitch Transformer1MoE  \n\nST-MoE4B269BST-MoE  \n\n{% asset_img st_moe_models.png ST-MoE %}  \n\n##   \n\n  \n\n1.   \n\n  \n\n  \n\n> Some architectural improvements involve more multiplications than additions or do not sum many items at once\n\n1GELU Gated Linear Units (GEGLU)  \n\nGLUcomponent-wiseGELU-Linear FFNtransformerReLU FFN  \n\n$$\\begin{aligned}FFN_{GEGLU}(x,W,V,b,c)=GELU(xW+b)\\odot(xV+c)\\end{aligned}$$  \n\n  \n\n2RMSNorm  \n\nRMSNorm $g$  \n\n$$y_i=\\frac{x_i}{\\sqrt{\\frac1d\\sum_{i=1}^dx_i^2}}\\cdot g_i$$  \n\nST-MoEGEGLURMSNorm  \n\n{% asset_img st_moe_remove_multiplications.png  %}  \n\n  \n\n3dense  \n\nST-MoEexpertdensedense\n\n{% asset_img st_moe_more_dense_layer.png dense %}  \n\n4bias\n\nFFNbias B  \n\n$$\\text{FFN}_{\\text{GEGLU}}+\\text{Add Bias}(x)=[(\\text{GELU}(xW_{11})\\odot xW_{12})+B]W_2$$  \n\n$$\\mathrm{FFN}_{\\mathrm{GEGLU}}+\\mathrm{Mult~Bias}(x)=[(\\mathrm{GELU}(xW_{11})\\odot xW_{12})\\odot B]W_2$$  \n\n  \n\n  \n\n2. noise  \n\nST-MoE  \n\ninput-jitterrouterlogits[1e-2, 1e2]  \n\n{% asset_img st_moe_more_add_noise.png noise %}  \n\nnoise  \n\n  \n\n3.   \n\nactivationgradient  \n\nST-MoE269B  \n\nST-MoErouter z-loss  \n\n$$L_z(x)=\\frac{1}{B}\\sum_{i=1}^B\\left(\\log\\sum_{j=1}^Ne^{x_j^{(i)}}\\right)^2$$  \n\n$B$ token$N$ $x\\in\\mathcal{R}^{B\\times N}$ router  \n\nz-lossrouterlogitsz-loss  \n\n{% asset_img st_moe_z_loss_result.png z-loss %}  \n\nST-MoEz-loss  \n\nz-loss $c_z$   \n\n$$L_{tot}=L_{CE}+c_BL_B+c_zL_Z$$  \n\nST-MoE$c_z=0.001$  \n\n$L_B$  auxiliary load balance lossST-MoEGShard/Switch Transformer  \n\n$$\\begin{aligned}\\text{loss}&=\\alpha\\cdot N\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}$$  \n\n$$\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax }p(x)=i\\}\\end{aligned}$$  \n\n$$\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}$$  \n\n$N$  $\\mathcal{B}$ $T$ tokenbatch$f_{i}$  $i$ experttoken$P_i$ batchtoken$i$ expert  \n\n4. \n\nfloat32bfloat16bfloat16allreducebfloat16float32  \n\nST-MoE-32BallreduceST-MoEallreducefloat32  \n\nbfloat16float32  \n\n{% asset_img st_moe_round_error.png bf16 %}  \n\nz-loss  \n\nMoErouter  \n\nST-MoE1/5token  \n\nsoftmaxMoE  \n\n## \n\ndensescaling lawMoEdense \n \n1expert  \n\n2routing  \n\n3  \n\n4  \n\nMoEscaling lawUnified scaling laws for routed language models  \n\n1. expert  \n\nST-MoE8/16/32<1%>256  \n\n>1<=1  \n\n2. routingcapacity factor  \n\ncapacity factor  \n\n{% asset_img st_moe_capacity_factor.png capacity factor %}  \n\n  \n\n1capacity factor  \n\n2capacity facotr  \n \n3expertcapacity factor  \n\ncapacity factorcapacity factor  \n\ncapacity factor  \n\n{% asset_img st_moe_capacity_factor_speed.png capacity factor %}  \n\n##   \n\n1. ST-MoE  \n\nST-MoE-32BST-MoE-32B  \n\n{% asset_img st_moe_perf.png capacity ST-MoE-32B %}  \n\n2. Expert Specialization  \n\ndecodertokenencoder  \n\n{% asset_img st_moe_encoder_specialization.png encoder %}  \n\nencodertoken  \n\n{% asset_img st_moe_multiling_specialization.png  %}  \n\n# DeepseekMoE\n\n20241DeepseekMoEMoEDeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language ModelsDeepSeekMoE  \n\nDeepSeekMoEMoE2  \n\n1expertexpert  \n\n2expertexpertshared expert  \n\nexpert(specialization)  \n\nDeepSeekMoE2BMoE16BMoEDeepSeekMoE-16B40GB  \n\nDeepSeekMoE-2B2BDeepSeekMoE-16B7B40%  \n\nDeepSeekMoE-16B  \n\n{% asset_img ds_moe_perf.png deepseek moe %}  \n\nDeepSeekMoE-2B16B  \n\nDeepSeekMoE-145BMoEDeepSeek-67B  \n\n##   \n\nMoEmixture of expertmotivationexpert  \n\n1991expert  \n\nMoEknowledge hybridityknowledge redundancy  \n\n1  \n\nexpertexpert  \n\n2  \n\nexpertexpertexpertexpert8expertexpert  \n\n(expert specialization)MoE  \n\nexpertnon-overlap & foucusd knowledge  \n\nDeepSeekMoE2  \n\n1Fine-Grained Expert Segmentation  \n\nexpertexpertexpertspecialization16expert2120expert1/464expert8 $\\binom{64}8=4,426,165,368$    \n\n2Shared Expert Isolation  \n\nexpertcommon knowledgeexpertexpertexpertexpert  \n\nMoEFine-Grained Expert SegmentationShared Expert Isolation  \n\n{% asset_img ds_moe_structure.png deepseek moe  %}  \n\nexpert isolation20221DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale  \n\nMoEexpert $N$expert $K$DeepSeekMoEexpert $1/m$DeepSeekMoE $mN$ expertexpert $mK$ $T$ $L$ $e_i^l$  $i$ expertDeepSeekMoElayernorm  \n\n$$\\mathbf{u}_{1:T}^l=\\text{Self-Att}\\left(\\mathbf{h}_{1:T}^{l-1}\\right)+\\mathbf{h}_{1:T}^{l-1}$$  \n\n$$\\mathbf{h}_t^l=\\sum_{i=1}^{mN}\\left(g_{i,t}\\text{ FFN}_i\\left(\\mathbf{u}_t^l\\right)\\right)+\\mathbf{u}_t^l$$  \n\n$$g_{i,t}=\\begin{cases}s_{i,t},&s_{i,t}\\in\\text{Topk}(\\{s_{j,t}|1\\leqslant j\\leqslant mN\\},mK)\\\\0,&\\text{otherwise,}\\end{cases}$$  \n\n$$s_{i,t}=\\mathrm{Softmax}_i\\left({\\mathbf{u}_t^l}^T\\mathbf{e}_i^l\\right)$$  \n\n## \n\nMoEgating  \n\n1routing collapsegatingexpert  \n\n2  \n\nrouting collapseDeepSeekMoEexpert-level balance loss\n\n$$\\begin{aligned}\n\\mathcal{L}_{\\mathrm{ExpBal}}& =\\alpha_1\\sum_{i=1}^{N'}f_iP_i\n\\end{aligned}$$  \n\n$$\\begin{aligned}\nf_{i}& =\\frac{N^{\\prime}}{K^{\\prime}T}\\sum_{t=1}^T\\mathbb{1}(\\text{Token }t\\text{ selects Expert }i)\n\\end{aligned}$$  \n\n$$\\begin{aligned}\nP_{i}& =\\frac1T\\sum_{t=1}^Ts_{i,t} \n\\end{aligned}$$  \n\n$\\alpha_1$ expert-level balance factor  \n\n $f_i$  $P_i$ Switch Transformer  \n\nSwitch Transformer $f_i$  $i$ experttokenDeepSeekMoE $N'/K'$  $N'=mN-K_s$$K'=mK-K_s$$K_s$ expertDeepSeekMoE $f_i$ Switch Transformer  \n\n$N'/K'$ expertloss  \n\n$P_i$ token $i$ expertSwitch Transformer  \n\n $f_i$ $P_i$   \n\nDeepSeekMoEdevice-level balance loss\n\n$$\\begin{aligned}\n\\mathcal{L}_{\\mathrm{DevBal}}& =\\alpha_2\\sum_{i=1}^Df_i'P_i'\n\\end{aligned}$$\n\n$$\\begin{aligned}\nf_i^{\\prime}& =\\frac1{|\\mathcal{E}_i|}\\sum_{j\\in\\mathcal{E}_i}f_j\n\\end{aligned}$$\n\n$$\\begin{aligned}\nP_{i}^{\\prime}& =\\sum_{j\\in\\mathcal{E}_i}P_j\n\\end{aligned}$$\n\n$\\alpha_2$ device-level balance factor  \n\n$\\mathcal{E}_i$  $i$ \n\ndevice-level balance lossexpert-level balance loss $f_i$  $P_i$ expert  \n\nexpert64expert8tokentokenexpert  \n\nexpert\n\n## \n\n1. \n\n100B tokenDeepSeekMoE-2BBPE8k\n\nDeepSeekMoE-2B0.006multi-head attention0.3B\n\n{% asset_img ds_model_param.png  %}  \n\nrelative expert sizeDeepSeekMoEexpertFFN\n\n  \n\n<center>\n\n|  |  |\n| :----: | :----: |\n| optimizer | AdamW |\n| adam_beta_1 | 0.9 |\n| adam_beta_2 | 0.95 |\n| adam_weight_decay | 0.1 |\n| warmup schedule | linear |\n| warmup step | 2000 |\n| max lr | 1.08e-3 |\n| dropout | 0 |\n| sequence length | 2k |\n| batch size | 2k |\n| total step | 25,000 |\n\n\n</center>  \n\n  \n- expertGPUdevice-level balance loss  \n- expert-level balance factor0.01  \n- 80%0.31690%0.316  \n\n100BDeepSeekMoE-2Bbenchmark4densehash layermoeHash layers for large sparse modelsSwitch TransformerGShard\n\n{% asset_img ds_moe_comparison.png deepseek moe %}  \n\n  \n- Hash LayerSwitch Transformerdense  \n- GSshardHash LayerSwitch Transformer  \n- DeepSeekMoEGShard  \n\nDeepSeekMoEdenseGShardDeepSeekMoE-2B\n\ndenseGShard161.5DeepSeekMoE-2B  \n\n{% asset_img ds_moe_upper_bound_2b.png deepseek moe upper bound %}  \n\nDeepSeekMoEDeepSeekMoE-13B, 1.21.5GShardDeepSeekMoE-13Bmatch  \n\n{% asset_img ds_moe_upper_bound_13b.png deepseek moe upper bound %}  \n\n2. DeepSeekMoE\n\nDeepSeekMoEshared expertfine-grained expertexpert\n\n{% asset_img ds_moe_ablation.png deepseek moe upper bound  %}  \n\n1\n\n2\n\n364expert1/2/4pileloss1.808,1.806,1.8111:32+6\n\n3. expert specialization\n\nDeepSeekMoEexpert specialization\n\n1DeepSeekMoE-2B1.5GShardtop  \n\n\n\n{% asset_img ds_moe_expert_specialization.png  %}  \n\nDeepSeekMoEDeepSeekMoE  \n\n2DeepSeekMoEloss\n\n3GShardDeepSeekMoE  \n\n{% asset_img ds_moe_less_activated_expert.png  %}  \n\n132b2+6GShardDeepSeekMoE\n\n{% asset_img ds_2b_less_expert.png 2B %}  \n\n1. DeepSeekMoE-16B  \n\nDeepSeekMoE-16B2TLLAMA2-7B100k  \n\n{% asset_img ds_model_param.png  %}  \n\nMoE  \n\nMoEloss  \n\nDeepSeekMoE-16B6426gating functiontoken8token16.4B2.8B  \n\ndimension  \n\n  \n- lr = 4.2e-4  \n- 80%90%lr0.316  \n- batch size = 4.5k4kbatch18M token2T10.6w  \n- pipeline parallelism\n\nexpert level balance loss0.001  \n\nDeepSeekMoE-16BDeepSeek-7B  \n\n{% asset_img ds_16b_perf_1.png DeepSeek-7B %}  \n\nDeepSeekMoE-16BLLAMA2-7B  \n\n{% asset_img ds_16b_perf_2.png LLAMA2-7B %}  \n\n5. DeepSeekMoE-145B  \n\n245BtokenDeepSeekMoE-145BDeepSeek-67B  \n\n{% asset_img ds_moe_145b.png 145b %}  \n\n# DBRX\n\n2024327DatabricksDBRX132B36BMoE\n\nDBRXRoPEGLUGQAfine-grained expert16token4MixtralGrok-182DBRX  \n\nDBRX32k12TtokenDBRX3072H100post-trainingred-team3  \n\nDBRXGPT-3.5Gemini 1.0 ProCodeLLaMA-70B  \n\n{% asset_img dbrx_perf.png DBRX %}  \n\nDBRX  \n\n{% asset_img dbrx_infer_efficiency.png  %}  \n\n# Qwen1.5-MoE \n\n2024328Qwen1.5-MoE-A2.7B2.7BQwen1.5-7B  \n\nQwen1.5-MoE-A2.7BDeepSeekMoEDBRXfine-grained expert64token84  \n\nQwen1.5-MoE-A2.7BQwen-1.8B  \n\nQwen1.5-MoE-A2.7B  \n\n{% asset_img qwen1.5_moe_perf.png Qwen1.5-MoE-A2.7B %}  \n\nQwen1.5-MoE-A2.7Bnon-embedding7B  \n\n{% asset_img qwen1.5_moe_params.png Qwen1.5-MoE-A2.7B %}  \n\nQwen1.5-MoE-A2.7BQwen1.5-7B75%  \n\nA100-80GvLLMQwen1.5-7BQwen1.5-MoE-A2.7B  \n\n/token1000token1000TPSthroughput  \n\n{% asset_img qwen1.5_moe_tps.png Qwen1.5-MoE-A2.7B TPS %}  \n\nMoEdenseQwen1.5-MoE-A2.7BQwen1.5-7B1.74  \n\n# Mistral\n\n## Mistral 8x7B\n\n20231211Mistral AIMistral-8x7Btoken82  \n\nMistral-8x7B32kLLAM2-70BGPT-3.5  \n\n{% asset_img mistral_8_7b_perf.png Mistral 8x7B %}  \n\nMistral-8x7BLLAM2-70B6  \n\nLLAM2-13B  \n\n{% asset_img mistral_8_7b_active_perf.png Mistral 8x7B %}  \n\n## Mistral 8x22B\n\n2024417Mistral AIMistral-8x22B141B39BMoE  \n\nMistral-8x22BMistral-8x7B32k64kMistral-8x22Bfunction call  \n\n  \n\n{% asset_img mistral_8_22b_reasoning.png Mistral 8x22B reasoning %}  \n\n{% asset_img mistral_8_22b_multiling.png Mistral 8x22B  %}  \n\n{% asset_img mistral_8_22b_code.png Mistral 8x22B  %}  \n\n#   \n\n- MoEdenseMoE  \n- MoEMoE  \n- denseMoE  \n-   \n- GShardSwitch Transformer  \n- MoEMoE  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n# Reference  \n1Adaptive Mixtures of Local Experts https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf  \n2Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer https://arxiv.org/abs/1701.06538  \n3GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding https://arxiv.org/abs/2006.16668  \n4Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity https://arxiv.org/abs/2101.03961  \n5GLaM: Efficient Scaling of Language Models with Mixture-of-Experts https://arxiv.org/abs/2112.06905  \n6ST-MoE: Designing Stable and Transferable Sparse Expert Models https://arxiv.org/abs/2202.08906  \n7DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models https://arxiv.org/abs/2401.06066  \n8Introducing DBRX: A New State-of-the-Art Open LLM https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm  \n9Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters https://qwenlm.github.io/zh/blog/qwen-moe/  \n","source":"_posts/cs/nlp/2024/03/MoE-.md","raw":"---\ntitle: MoE\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - MoE\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 44e38c1b\ndate: 2024-03-30 09:56:05\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n202434MoEQwen1.5-MoEDBRXJambaMistral  \n\nMoE  \n\n<center>\n\n|  |  |  |\n| :----: | :----: | :----: |\n| GPT4 | 20233 | 236George HotzGPT48220B |\n| Mistral-87B | 202312 | Mistral AI |\n| LLAMA-MoE | 202312 | github |\n| DeepSeek-MoE | 20241 | MoE |\n| abab6 |20241 | MiniMaxMoE |\n| 2.0 | 20242 |  |\n| Step-2 | 20243 |  |\n| MM1 | 20243 | MoE |\n| Grok-1 | 20243 | X |\n| Qwen1.5-MoE-A2.7B| 20243 |  |\n| DBRX | 20243 | Databricks |\n| Jamba | 20243 | AI21 |\n| Mistral-822B | 20244 | Mistral AI |\n| WizardLM-2-822B | 20244 |  |\n| 3.0 | 20244 | 400BMoE |\n| Arctic | 20244 | Snowflake480BDense-MoE Hybrid |\n\n</center>  \n\nMoEMoE  \n\n{% asset_img xiaomi_moe.jpg MoE %}  \n\nMoE  \n\nMoEMoEMoE  \n\n20244DeepSeek-MoEQwen1.5-MoE\n\n#   \n\nMoE  \n\nMoEGoogle\n\n##   \n\nMoE1991[Adaptive Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)Geoffrey HintonMichael I. JordanMoE1988  \n\n>This idea was first presented by Jacobs and Hinton at the Connectionist Summer School in Pittsburg in 1988.  \n\nMoEMoE  \n\n## RNN  \n\nGoogle20171[Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)MoELSTM137B128kLSTM  \n\n## Transformer  \n\n1. 20206Google[GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)MoEencoder-decodertransformerFFNMoE12.5B600BMoE2048  \n\n2. 20211Google[Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) T5encoder-decoderFFNMoErouting1.6Tswitch transformerSwitch Transformersscaling  \n\n3. 20222Google[ST-MoE: Designing Stable and Transferable Sparse Expert Models](https://arxiv.org/abs/2202.08906)encoder-decoderMoE269B32BST-MoEMoESwitch Transformer  \n\n## GPT  \n\n1. 202112GoogleGLaM[GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://arxiv.org/abs/2112.06905)1.2Tdecoder-onlyencoder-decoderdecoder-onlyGoogle  \n\n2. 20241[DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/abs/2401.06066)2312DeepSeekMoE  \n\n3. 2024DatabricksDBRXQwen1.5-MoE-A2.7BMistral AIMistral-8x22B  \n\n#   \n\nGeoffrey HintonMichael I. Jordan[Adaptive Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)MoE  \n\n1.   \n\n  \n\n  \n\nMoEexpert  \n\nMoEvowel discrimination taskMoEaeiou  \n\n2.   \n\nMoEexpert networkgating networkexpertgating networkexpertexpertstochastictruefalse  \n\n{% asset_img vanilla_moe.png Vanilla MoE %}  \n\n3.   \n\nMoEideaJacobsHinton1988lossensembleexpertexpertexpertresidual  \n\ncase $c$ $d^c$ ground truth $i$ expert $o_{i}^c$$p_{i}^c$ gating network $i$ expert $E^{c}$ \n\n$$E^{c}=\\left|\\left|d^{c}-\\sum_{i}p_{i}^{c}o_{i}^{c}\\right|\\right|^{2}$$\n\nexpert  \n\nexpertexpertexpert  \n\nexpertexpertexpertexpertgating network  \n\n  \n\nHintonJordanlossexpert  \n\ngating networkexpert  \n\n$$E^{c}=\\langle\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2\\rangle=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}$$\n\nexpertexpertexpert  \n\nlosslocalizationcasegating networkexpertgating networkexpert  \n\nlocalizationexpert  \n\nexpertexpertgating networkexpert error+-  \n\nexpert0  \n\n4. \n\nlosslossloss  \n\n$$\\text{loss}E^{c}=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}$$  \n\n$$\\text{loss}E^c=-log\\sum_ip_i^ce^{-\\frac12\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}$$  \n\nlossloss  \n\n$$\\text{loss}\\frac{\\partial E^c}{\\partial\\mathbf{o}_i^c}=-2p_i^c(\\mathbf{d}^c-\\mathbf{o}_i^c)$$  \n\n$$\\text{loss}\\frac{\\partial E^c}{\\partial\\mathbf{o}_i^c}=-\\left[\\frac{p_i^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}}{\\sum_jp_j^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_j^c\\|^2}}\\right](\\mathbf{d}^c-\\mathbf{o}_i^c)$$  \n\nlossloss $i$ expertexpertexpert $i$ casegating networklosscaseexpertlosslossexpertlocalizationexpert  \n\nBTWloss  \n\nMoE  \n\n# LSTM MoE  \n\nGoogle20171\n[OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER](https://arxiv.org/abs/1701.06538)MoELSTM137BLSTM7  \n\n1991  \n\n## \n\nTransformer  \n\nconditional computationconditional computationMoE  \n\n  \n\n- MoEexpertbatch sizebatch size  \nbatch size3216expertexpert2batch sizebatch sizebatch size  \n-   \nNLP  \n-   \n  \n-   \nGPU  \n- GPU  \nGPUbranchingif/elseMoEgating network  \n\n  \n\n## \n\n1.   \n\n  \n\nLSTMMoEembedding  \n\n{% asset_img rnn_moe.png LSTM MoE %}  \n\nexpertfeed-forward neural networknexpertgating networkn  \n\n$$\\begin{aligned}y=\\sum_{i=1}^nG(x)_iE_i(x)\\end{aligned}$$  \n\n$E_{i}(x)$  $i$ expert$G(x)_{i}$ gating network $i$ expert  \n\n $G(x)_{i}$ 0expert  \n\nexperttwo-level hierarchical MoEgating networkgating networkexpertgating networkexpertword2vechierarchical softmax  \n\n2. gating network  \n\ngating network  \n\nsoftmaxgating function  \n\n$$\\begin{aligned}G_\\sigma(x)=Softmax(x\\cdot W_g)\\end{aligned}$$  \n\ntopkksoftmax0expert  \n\nsparsitytopkgating function  \n\nGaussian noisenoise  \n\n$$\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}$$  \n\n$$KeepTopK(v,k)_i=\\begin{cases}v_i&\\text{if }v_i\\text{ is in the top }k\\text{ elements of }v.\\\\-\\infty&\\text{otherwise.}\\end{cases}$$  \n\n$$\\begin{aligned}H(x)_i=(x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\end{aligned}$$  \n\nnoisesoftplusReLU  \n\n{% asset_img softplus.png softplus %}  \n\n  \n\n##   \n\nMoEgating networkexpertexpert  \n\n  \n\nhard constraintexperthard constraintsoft constraint  \n\nexpert  \n\n$$Importance(X)=\\sum_{x\\in X}G(x)$$  \n\n$G(x)$ gating networkexpert  \n\n $L_{importance}$$L_{importance}$   \n\n$$L_{importance}(X)=w_{importance}\\cdot CV(Importance(X))^2$$  \n\n $w_{importance}$ 0.1CVcoefficient of variation  \n\ncoefficient of variation $\\sigma$   $\\mu$   \n\nMoEexpertexpertgating $L_{importance}$   \n\n $L_{importance}$  $L_{importance}$  $L_{importance}$   \n\nexpertexpertgating  \n\n $L_{load}$ expert  \n\nexpertback propagation $L_{load}$ expert  \n\nMoE $H(x)$ KeepTopK  \n\n$$\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}$$  \n\n$$\\begin{aligned}H(x)_i=(x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\end{aligned}$$  \n\n $kth\\_excluding(H(x),k,i)$ $H(x)$  $i$  $k$  $P(x,i)$ noise $i$ noise $kth\\_excluding(H(x),k,i)$   \n\n$$\\begin{aligned}P(x,i)=Pr\\Big((x\\cdot W_g)_i+StandardNormal()\\cdot Softplus((x\\cdot W_{noise})_i)\\\\>kth\\_excluding(H(x),k,i)\\Big)\\end{aligned}$$  \n\nnoise $i$  $i$ $P(x,i)$   \n\n$$\\begin{aligned}P(x,i)&=\\Phi\\Big(\\frac{(x\\cdot W_g)_i-kth\\_excluding(H(x),k,i)}{Softplus((x\\cdot W_{noise})_i)}\\Big)\\end{aligned}$$  \n\n $\\Phi$ CDF  \n\n $i$ expert  \n\n$$\\begin{aligned}Load(X)_i=\\sum_{x\\in X}P(x,i)\\end{aligned}$$  \n\nexpert  \n\n$$L_{load}(X)=w_{load}\\cdot CV(Load(X))^2$$  \n\n$w_{load}$ 0.1  \n\n $L_{importance}(X)$$Load(X)$   \n\nexpert $W_g$   $W_{noise}$ 0  \n\n  \n\n{% asset_img rnn_moe_load_function.png  %}  \n\n  \n\n##   \n\n1.   \n\n  \n\n1batch size  \n\nexpertbatch sizenexpertkbatch sizebexpertbatch sizekb/nexpertbatch size  \n- batchbatchMoEexpertdexpertkbd/nbatch size\n- LSTMbatch size\n\n2  \n\n  \n\nexpertinputoutput[input_size, hidden_size][hidden_size, output_size]GPU1000hidden_sizeexpert1000  \n\n2.  &   \n\nMoEdense4/32/256expertflat MoE256/1024/4096experthierarchical MoEexpert1Mflat4experthierarchical MoEgating2  \n\nppldenseMoEMoE\n\n{% asset_img rnn_moe_perf.png  %}  \n\n3.   \n\n4Bdiminishing returns  \n\n + 100B token32, 256, 10244096, 16384, 65536, 131072expertMoE137B  \n\n  \n\n{% asset_img rnn_moe_137b.png 137 %}  \n\n  \n\n4. Expert Specialization  \n\nMoE\n\ntokenspecialization  \n\n{% asset_img rnn_moe_specilized.png RNN MoE  %}  \n\n# GShard\n\n1. \n\n2018Berttransformer20206GoogleGShard: Scaling Giant Models with Conditional Computation and Automatic ShardingMoEencoder-decodertransformerMoE  \n\nGShardMoE600B  \n\n{% asset_img gshard_moe_family.png GShard MoE family %}  \n\nexpertLSMT MoE -- expert24expertChatGPTBertGPT  \n\nGShardMoE  \n\n2. \n\n  \n\nGoogleencoder-decoder transfomerGShardencoder-decoder transfomer  \n\nGShardencoderdecoderFFNMoENN/2MoE  \n\n{% asset_img gshard_model.png GShard %}  \n\ntop-2 expert  \n\nGShardLSTM MoEgating functionauxiliary loss function  \n\nMoE\n\n$$\\begin{aligned}\n\\mathcal{G}_{s,E}& =\\mathrm{GATE}(x_s)  \\\\\n\\mathrm{FFN}_e(x_s)& =wo_e\\cdot\\text{ReLU}(wi_e\\cdot x_s)  \\\\\ny_{s}& =\\sum_{e=1}^E\\mathcal{G}_{s,e}\\cdot\\mathrm{FFN}_e(x_s) \n\\end{aligned}$$\n\n $x_s$ MoEtoken$w_i$  $w_o$ $\\mathcal{G}_{s}$ gating function\n\nGShardgating function12  \n\nNtokenEexpertNEgating function  \n\ngating function  \n\n1 expert capacity  \n\nexperttokenexperttoken2N/E  \n\nexpert capacityGATE()expert $c_e$ tokentokenexpert  \n\n2 Local group dispatching  \n\ntokenG2N/EG  \n\nbatchbatchbatchgroupall2allgroup  \n\ngroupgradient accumulation  \n\n3 Auxiliary loss  \n\ngatingLSTM MoE  \n\n$$\\ell_{aux}=\\frac1E\\sum_{e=1}^E\\frac{c_e}S\\cdot m_e$$  \n\n$S$ token$E$ $c_e$  $e$ token$m_e$  $e$ expert $S$ token  \n\n $\\frac{c_e}S$  $\\frac{c_e}S$  $m_e$  $m_e$  $e$ expert $S$ tokenloss  \n\nloss  \n\ngating  \n\n{% asset_img gshard_algo_1.png GShard gating  %}  \n\n4 Random routing  \n\ntop-2 experttop-1  \n\ntop-1g2  \n\n3.   \n\n\n\n{% asset_img gshard_perf.png GShard %}  \n\n# Switch Transformer\n\n20224ChatGPTGoogleSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity2021GoogleSwitch Transformer  \n\nSwitch TransformerGShardencoder-decoderT51.6T2048expert  \n\nSwitch Transformer  \n\nSwitch TransformerSwitch TransformerFLOPS/token  \n\nSwitch Transformer  \n\n1TransformerMoESwitch Transformer  \n\n2MoE to denseMoEdenseMoE99%dense  \n\n3  \n- bf16MoE  \n- MoE  \n-   \n\n41TMoE  \n\n5101  \n\n6FLOPS/tokenSwitch Transformer  \n\n##   \n\nSwitch TransformerGShardtransformerFFNMoE  \n\n{% asset_img switch_transformer_structure.png Switch Transformer  %}  \n\nSwitch Transformergating functionSwitch Transformerrouting  \n\nkexpertSwitch Transformergating1expertk=1MoESwitch layer  \n\nroutingrouter  \n\n##   \n\nGShardSwitch Transformerexpert capacityexpertbatchtoken  \n\ntokenexpertoverflowtokenGShard  \n\nSwitch Transformercapacity factor  \n\n$$\\text{expert capacity}=\\left(\\frac{\\text{tokens per batch}}{\\text{number of experts}}\\right)\\times\\text{capacity factor}.$$  \n\ncapacity factorexperttokenoverflow\n\ncapacity factor  \n\n{% asset_img switch_transformer_diff_expert_capacity.png expert capacity %}  \n\nexpert capacity\n\ncapacity factor1overflowcapacity factor  \n\nexpertoverflowMoESwitch Transformer128  \n\ncapacity factoroverflow  \n\n{% asset_img switch_transformer_capacity_effect.png expert capacity %}  \n\ntokenscalingoverflow  \n\n  \n\n $N$ expert $T$ tokenbatch $\\mathcal{B}$ \n\n$$\\begin{aligned}\\text{loss}&=\\alpha\\cdot N\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}$$  \n\n$f_{i}$  $i$ experttoken  \n\n$$\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax }p(x)=i\\}\\end{aligned}$$  \n\n$P_i$ batchtoken$i$ expert  \n\n$$\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}$$  \n\nGShard  \n\n$f$  $P$  $1/N$  \n\n$\\alpha$ 1e-51e-11e-2  \n\n $\\sum_{i=1}^N(f_i\\cdot P_i)=\\sum_{i=1}^N(\\frac1N\\cdot\\frac1N)=\\frac1N$loss $N$expertloss  \n\n## \n\n1. trick  \n\n1bf16  \n\nbf16routing function  \n\nroutingsoftmaxexponentialrounding errorrouting  \n\n2  \n\n $\\mu=0$$\\sigma=\\sqrt{s}/n$sne.g. fan-in  \n\nTransformers=1.010  \n\n{% asset_img switch_transformer_init.png  %}  \n\n3dropout  \n\nSwitch Transformerdropout  \n\n{% asset_img switch_transformer_dropout.png dropout %}  \n\ndropoutdense0.1expertdropout  \n\n2. scaling  \n\nSwitch Transformerscaling  \n\n1Step-Basis  \n\nstepexpert  \n\nstepstep  \n\n{% asset_img switch_transformer_scaling_step.png step scaling %}  \n\n2Time-Basis  \n\nSwitch TransformerstepSwitch Transformerdense  \n\nSwitch TransformerdenseSwitch Transformerdensedense1/7  \n\n{% asset_img switch_transformer_scaling_time.png time scaling %}  \n\n3dense\n\nSwitch TransformerdenseSwitch Transformerdense  \n\nStep-BasisTime-Basis64Switch TransformerT5-LargestepSwitch Transformer  \n\n{% asset_img switch_transformer_scaling_dense.png dense %}  \n\n3. SFT  \n\nGLUESuperGLUEdense  \n\neval  \n\n{% asset_img switch_transformer_sft_result.png sft %}  \n\n4.   \n\nSwitch TransformerBTdense  \n\n  \n- Switch Transformerdense  \n- label25%75%ground truth  \n\ndensedensedenseSwitch Transformer30%  \n\n{% asset_img switch_transformer_distill.png  %}  \n\n99%  \n\n{% asset_img switch_transformer_distill_diff_model.png  %}  \n\nSuperGLUE  \n\n{% asset_img switch_transformer_distill_sft.png sft %}  \n\n# GLaM\n\n1. \n\n202112GoogleGLaM: Efficient Scaling of Language Models with Mixture-of-Experts1.2T64token96.6BMoE  \n\nSwitch TransformerGLaM1.6T token  \n\n  \n\n{% asset_img glam_related_model.png glam %}  \n\nGPT-3175BGPT-3NLPGPT-3  \n\n{% asset_img glam_compare_gpt3.png glamgpt3 %}  \n\n{% asset_img glam_compare_gpt3_2.png glamgpt3 %}  \n\n2. \n\nSwitch TransformerFFNMoESwitch TransformerGLaMexpert  \n\n{% asset_img glam_model.png glam %}  \n\n  \n\n1  \n\nXLNET  \n\n2\n\n> In the non-MoE Transformer feed-forward sub-layers, we replace the first linear projection and the activation function with the Gated Linear Unitwhich computes the component-wise product of two linear transformation of the input, followed by a Gaussian Error Linear Unit.  \n\n3. \n\ntrick  \n\n1Lingvo: a modular and scalable framework for sequence-to-sequence modelingNaNInf  \n\n2BPNaNInfcheckpointNaNInf  \n\nMoE  \n\n{% asset_img glam_family.png glam %}  \n\nGLaMdense  \n\n{% asset_img glam_perf.png glam %}  \n\nGLaM MoEdense  \n\n# ST-MoE  \n\n20222GoogleST-MOE: DESIGNING STABLE AND TRANSFERABLE SPARSE EXPERT MODELSST-MoEMoEMoE  \n\nST-MoE269B32B denseStable Transferable Mixture-of-ExpertsST-MoE-32B  \n\nMoEST-MoESwitch Transformer1MoE  \n\nST-MoE4B269BST-MoE  \n\n{% asset_img st_moe_models.png ST-MoE %}  \n\n##   \n\n  \n\n1.   \n\n  \n\n  \n\n> Some architectural improvements involve more multiplications than additions or do not sum many items at once\n\n1GELU Gated Linear Units (GEGLU)  \n\nGLUcomponent-wiseGELU-Linear FFNtransformerReLU FFN  \n\n$$\\begin{aligned}FFN_{GEGLU}(x,W,V,b,c)=GELU(xW+b)\\odot(xV+c)\\end{aligned}$$  \n\n  \n\n2RMSNorm  \n\nRMSNorm $g$  \n\n$$y_i=\\frac{x_i}{\\sqrt{\\frac1d\\sum_{i=1}^dx_i^2}}\\cdot g_i$$  \n\nST-MoEGEGLURMSNorm  \n\n{% asset_img st_moe_remove_multiplications.png  %}  \n\n  \n\n3dense  \n\nST-MoEexpertdensedense\n\n{% asset_img st_moe_more_dense_layer.png dense %}  \n\n4bias\n\nFFNbias B  \n\n$$\\text{FFN}_{\\text{GEGLU}}+\\text{Add Bias}(x)=[(\\text{GELU}(xW_{11})\\odot xW_{12})+B]W_2$$  \n\n$$\\mathrm{FFN}_{\\mathrm{GEGLU}}+\\mathrm{Mult~Bias}(x)=[(\\mathrm{GELU}(xW_{11})\\odot xW_{12})\\odot B]W_2$$  \n\n  \n\n  \n\n2. noise  \n\nST-MoE  \n\ninput-jitterrouterlogits[1e-2, 1e2]  \n\n{% asset_img st_moe_more_add_noise.png noise %}  \n\nnoise  \n\n  \n\n3.   \n\nactivationgradient  \n\nST-MoE269B  \n\nST-MoErouter z-loss  \n\n$$L_z(x)=\\frac{1}{B}\\sum_{i=1}^B\\left(\\log\\sum_{j=1}^Ne^{x_j^{(i)}}\\right)^2$$  \n\n$B$ token$N$ $x\\in\\mathcal{R}^{B\\times N}$ router  \n\nz-lossrouterlogitsz-loss  \n\n{% asset_img st_moe_z_loss_result.png z-loss %}  \n\nST-MoEz-loss  \n\nz-loss $c_z$   \n\n$$L_{tot}=L_{CE}+c_BL_B+c_zL_Z$$  \n\nST-MoE$c_z=0.001$  \n\n$L_B$  auxiliary load balance lossST-MoEGShard/Switch Transformer  \n\n$$\\begin{aligned}\\text{loss}&=\\alpha\\cdot N\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}$$  \n\n$$\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax }p(x)=i\\}\\end{aligned}$$  \n\n$$\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}$$  \n\n$N$  $\\mathcal{B}$ $T$ tokenbatch$f_{i}$  $i$ experttoken$P_i$ batchtoken$i$ expert  \n\n4. \n\nfloat32bfloat16bfloat16allreducebfloat16float32  \n\nST-MoE-32BallreduceST-MoEallreducefloat32  \n\nbfloat16float32  \n\n{% asset_img st_moe_round_error.png bf16 %}  \n\nz-loss  \n\nMoErouter  \n\nST-MoE1/5token  \n\nsoftmaxMoE  \n\n## \n\ndensescaling lawMoEdense \n \n1expert  \n\n2routing  \n\n3  \n\n4  \n\nMoEscaling lawUnified scaling laws for routed language models  \n\n1. expert  \n\nST-MoE8/16/32<1%>256  \n\n>1<=1  \n\n2. routingcapacity factor  \n\ncapacity factor  \n\n{% asset_img st_moe_capacity_factor.png capacity factor %}  \n\n  \n\n1capacity factor  \n\n2capacity facotr  \n \n3expertcapacity factor  \n\ncapacity factorcapacity factor  \n\ncapacity factor  \n\n{% asset_img st_moe_capacity_factor_speed.png capacity factor %}  \n\n##   \n\n1. ST-MoE  \n\nST-MoE-32BST-MoE-32B  \n\n{% asset_img st_moe_perf.png capacity ST-MoE-32B %}  \n\n2. Expert Specialization  \n\ndecodertokenencoder  \n\n{% asset_img st_moe_encoder_specialization.png encoder %}  \n\nencodertoken  \n\n{% asset_img st_moe_multiling_specialization.png  %}  \n\n# DeepseekMoE\n\n20241DeepseekMoEMoEDeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language ModelsDeepSeekMoE  \n\nDeepSeekMoEMoE2  \n\n1expertexpert  \n\n2expertexpertshared expert  \n\nexpert(specialization)  \n\nDeepSeekMoE2BMoE16BMoEDeepSeekMoE-16B40GB  \n\nDeepSeekMoE-2B2BDeepSeekMoE-16B7B40%  \n\nDeepSeekMoE-16B  \n\n{% asset_img ds_moe_perf.png deepseek moe %}  \n\nDeepSeekMoE-2B16B  \n\nDeepSeekMoE-145BMoEDeepSeek-67B  \n\n##   \n\nMoEmixture of expertmotivationexpert  \n\n1991expert  \n\nMoEknowledge hybridityknowledge redundancy  \n\n1  \n\nexpertexpert  \n\n2  \n\nexpertexpertexpertexpert8expertexpert  \n\n(expert specialization)MoE  \n\nexpertnon-overlap & foucusd knowledge  \n\nDeepSeekMoE2  \n\n1Fine-Grained Expert Segmentation  \n\nexpertexpertexpertspecialization16expert2120expert1/464expert8 $\\binom{64}8=4,426,165,368$    \n\n2Shared Expert Isolation  \n\nexpertcommon knowledgeexpertexpertexpertexpert  \n\nMoEFine-Grained Expert SegmentationShared Expert Isolation  \n\n{% asset_img ds_moe_structure.png deepseek moe  %}  \n\nexpert isolation20221DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale  \n\nMoEexpert $N$expert $K$DeepSeekMoEexpert $1/m$DeepSeekMoE $mN$ expertexpert $mK$ $T$ $L$ $e_i^l$  $i$ expertDeepSeekMoElayernorm  \n\n$$\\mathbf{u}_{1:T}^l=\\text{Self-Att}\\left(\\mathbf{h}_{1:T}^{l-1}\\right)+\\mathbf{h}_{1:T}^{l-1}$$  \n\n$$\\mathbf{h}_t^l=\\sum_{i=1}^{mN}\\left(g_{i,t}\\text{ FFN}_i\\left(\\mathbf{u}_t^l\\right)\\right)+\\mathbf{u}_t^l$$  \n\n$$g_{i,t}=\\begin{cases}s_{i,t},&s_{i,t}\\in\\text{Topk}(\\{s_{j,t}|1\\leqslant j\\leqslant mN\\},mK)\\\\0,&\\text{otherwise,}\\end{cases}$$  \n\n$$s_{i,t}=\\mathrm{Softmax}_i\\left({\\mathbf{u}_t^l}^T\\mathbf{e}_i^l\\right)$$  \n\n## \n\nMoEgating  \n\n1routing collapsegatingexpert  \n\n2  \n\nrouting collapseDeepSeekMoEexpert-level balance loss\n\n$$\\begin{aligned}\n\\mathcal{L}_{\\mathrm{ExpBal}}& =\\alpha_1\\sum_{i=1}^{N'}f_iP_i\n\\end{aligned}$$  \n\n$$\\begin{aligned}\nf_{i}& =\\frac{N^{\\prime}}{K^{\\prime}T}\\sum_{t=1}^T\\mathbb{1}(\\text{Token }t\\text{ selects Expert }i)\n\\end{aligned}$$  \n\n$$\\begin{aligned}\nP_{i}& =\\frac1T\\sum_{t=1}^Ts_{i,t} \n\\end{aligned}$$  \n\n$\\alpha_1$ expert-level balance factor  \n\n $f_i$  $P_i$ Switch Transformer  \n\nSwitch Transformer $f_i$  $i$ experttokenDeepSeekMoE $N'/K'$  $N'=mN-K_s$$K'=mK-K_s$$K_s$ expertDeepSeekMoE $f_i$ Switch Transformer  \n\n$N'/K'$ expertloss  \n\n$P_i$ token $i$ expertSwitch Transformer  \n\n $f_i$ $P_i$   \n\nDeepSeekMoEdevice-level balance loss\n\n$$\\begin{aligned}\n\\mathcal{L}_{\\mathrm{DevBal}}& =\\alpha_2\\sum_{i=1}^Df_i'P_i'\n\\end{aligned}$$\n\n$$\\begin{aligned}\nf_i^{\\prime}& =\\frac1{|\\mathcal{E}_i|}\\sum_{j\\in\\mathcal{E}_i}f_j\n\\end{aligned}$$\n\n$$\\begin{aligned}\nP_{i}^{\\prime}& =\\sum_{j\\in\\mathcal{E}_i}P_j\n\\end{aligned}$$\n\n$\\alpha_2$ device-level balance factor  \n\n$\\mathcal{E}_i$  $i$ \n\ndevice-level balance lossexpert-level balance loss $f_i$  $P_i$ expert  \n\nexpert64expert8tokentokenexpert  \n\nexpert\n\n## \n\n1. \n\n100B tokenDeepSeekMoE-2BBPE8k\n\nDeepSeekMoE-2B0.006multi-head attention0.3B\n\n{% asset_img ds_model_param.png  %}  \n\nrelative expert sizeDeepSeekMoEexpertFFN\n\n  \n\n<center>\n\n|  |  |\n| :----: | :----: |\n| optimizer | AdamW |\n| adam_beta_1 | 0.9 |\n| adam_beta_2 | 0.95 |\n| adam_weight_decay | 0.1 |\n| warmup schedule | linear |\n| warmup step | 2000 |\n| max lr | 1.08e-3 |\n| dropout | 0 |\n| sequence length | 2k |\n| batch size | 2k |\n| total step | 25,000 |\n\n\n</center>  \n\n  \n- expertGPUdevice-level balance loss  \n- expert-level balance factor0.01  \n- 80%0.31690%0.316  \n\n100BDeepSeekMoE-2Bbenchmark4densehash layermoeHash layers for large sparse modelsSwitch TransformerGShard\n\n{% asset_img ds_moe_comparison.png deepseek moe %}  \n\n  \n- Hash LayerSwitch Transformerdense  \n- GSshardHash LayerSwitch Transformer  \n- DeepSeekMoEGShard  \n\nDeepSeekMoEdenseGShardDeepSeekMoE-2B\n\ndenseGShard161.5DeepSeekMoE-2B  \n\n{% asset_img ds_moe_upper_bound_2b.png deepseek moe upper bound %}  \n\nDeepSeekMoEDeepSeekMoE-13B, 1.21.5GShardDeepSeekMoE-13Bmatch  \n\n{% asset_img ds_moe_upper_bound_13b.png deepseek moe upper bound %}  \n\n2. DeepSeekMoE\n\nDeepSeekMoEshared expertfine-grained expertexpert\n\n{% asset_img ds_moe_ablation.png deepseek moe upper bound  %}  \n\n1\n\n2\n\n364expert1/2/4pileloss1.808,1.806,1.8111:32+6\n\n3. expert specialization\n\nDeepSeekMoEexpert specialization\n\n1DeepSeekMoE-2B1.5GShardtop  \n\n\n\n{% asset_img ds_moe_expert_specialization.png  %}  \n\nDeepSeekMoEDeepSeekMoE  \n\n2DeepSeekMoEloss\n\n3GShardDeepSeekMoE  \n\n{% asset_img ds_moe_less_activated_expert.png  %}  \n\n132b2+6GShardDeepSeekMoE\n\n{% asset_img ds_2b_less_expert.png 2B %}  \n\n1. DeepSeekMoE-16B  \n\nDeepSeekMoE-16B2TLLAMA2-7B100k  \n\n{% asset_img ds_model_param.png  %}  \n\nMoE  \n\nMoEloss  \n\nDeepSeekMoE-16B6426gating functiontoken8token16.4B2.8B  \n\ndimension  \n\n  \n- lr = 4.2e-4  \n- 80%90%lr0.316  \n- batch size = 4.5k4kbatch18M token2T10.6w  \n- pipeline parallelism\n\nexpert level balance loss0.001  \n\nDeepSeekMoE-16BDeepSeek-7B  \n\n{% asset_img ds_16b_perf_1.png DeepSeek-7B %}  \n\nDeepSeekMoE-16BLLAMA2-7B  \n\n{% asset_img ds_16b_perf_2.png LLAMA2-7B %}  \n\n5. DeepSeekMoE-145B  \n\n245BtokenDeepSeekMoE-145BDeepSeek-67B  \n\n{% asset_img ds_moe_145b.png 145b %}  \n\n# DBRX\n\n2024327DatabricksDBRX132B36BMoE\n\nDBRXRoPEGLUGQAfine-grained expert16token4MixtralGrok-182DBRX  \n\nDBRX32k12TtokenDBRX3072H100post-trainingred-team3  \n\nDBRXGPT-3.5Gemini 1.0 ProCodeLLaMA-70B  \n\n{% asset_img dbrx_perf.png DBRX %}  \n\nDBRX  \n\n{% asset_img dbrx_infer_efficiency.png  %}  \n\n# Qwen1.5-MoE \n\n2024328Qwen1.5-MoE-A2.7B2.7BQwen1.5-7B  \n\nQwen1.5-MoE-A2.7BDeepSeekMoEDBRXfine-grained expert64token84  \n\nQwen1.5-MoE-A2.7BQwen-1.8B  \n\nQwen1.5-MoE-A2.7B  \n\n{% asset_img qwen1.5_moe_perf.png Qwen1.5-MoE-A2.7B %}  \n\nQwen1.5-MoE-A2.7Bnon-embedding7B  \n\n{% asset_img qwen1.5_moe_params.png Qwen1.5-MoE-A2.7B %}  \n\nQwen1.5-MoE-A2.7BQwen1.5-7B75%  \n\nA100-80GvLLMQwen1.5-7BQwen1.5-MoE-A2.7B  \n\n/token1000token1000TPSthroughput  \n\n{% asset_img qwen1.5_moe_tps.png Qwen1.5-MoE-A2.7B TPS %}  \n\nMoEdenseQwen1.5-MoE-A2.7BQwen1.5-7B1.74  \n\n# Mistral\n\n## Mistral 8x7B\n\n20231211Mistral AIMistral-8x7Btoken82  \n\nMistral-8x7B32kLLAM2-70BGPT-3.5  \n\n{% asset_img mistral_8_7b_perf.png Mistral 8x7B %}  \n\nMistral-8x7BLLAM2-70B6  \n\nLLAM2-13B  \n\n{% asset_img mistral_8_7b_active_perf.png Mistral 8x7B %}  \n\n## Mistral 8x22B\n\n2024417Mistral AIMistral-8x22B141B39BMoE  \n\nMistral-8x22BMistral-8x7B32k64kMistral-8x22Bfunction call  \n\n  \n\n{% asset_img mistral_8_22b_reasoning.png Mistral 8x22B reasoning %}  \n\n{% asset_img mistral_8_22b_multiling.png Mistral 8x22B  %}  \n\n{% asset_img mistral_8_22b_code.png Mistral 8x22B  %}  \n\n#   \n\n- MoEdenseMoE  \n- MoEMoE  \n- denseMoE  \n-   \n- GShardSwitch Transformer  \n- MoEMoE  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n# Reference  \n1Adaptive Mixtures of Local Experts https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf  \n2Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer https://arxiv.org/abs/1701.06538  \n3GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding https://arxiv.org/abs/2006.16668  \n4Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity https://arxiv.org/abs/2101.03961  \n5GLaM: Efficient Scaling of Language Models with Mixture-of-Experts https://arxiv.org/abs/2112.06905  \n6ST-MoE: Designing Stable and Transferable Sparse Expert Models https://arxiv.org/abs/2202.08906  \n7DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models https://arxiv.org/abs/2401.06066  \n8Introducing DBRX: A New State-of-the-Art Open LLM https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm  \n9Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters https://qwenlm.github.io/zh/blog/qwen-moe/  \n","slug":"cs/nlp/2024/03/MoE-","published":1,"updated":"2024-05-10T06:51:36.425Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmu00160p4k621p61y5","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>202434MoEQwen1.5-MoEDBRXJambaMistral</p>\n<p>MoE</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">GPT4</td>\n<td style=\"text-align: center;\">20233</td>\n<td style=\"text-align: center;\">236George\nHotzGPT48220B</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Mistral-87B</td>\n<td style=\"text-align: center;\">202312</td>\n<td style=\"text-align: center;\">Mistral AI</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">LLAMA-MoE</td>\n<td style=\"text-align: center;\">202312</td>\n<td style=\"text-align: center;\">github</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">DeepSeek-MoE</td>\n<td style=\"text-align: center;\">20241</td>\n<td style=\"text-align: center;\">MoE</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">abab6</td>\n<td style=\"text-align: center;\">20241</td>\n<td style=\"text-align: center;\">MiniMaxMoE</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">2.0</td>\n<td style=\"text-align: center;\">20242</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Step-2</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">MM1</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">MoE</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Grok-1</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">X</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Qwen1.5-MoE-A2.7B</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">DBRX</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">Databricks</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Jamba</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">AI21</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Mistral-822B</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\">Mistral AI</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">WizardLM-2-822B</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">3.0</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\">400BMoE</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Arctic</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\">Snowflake480BDense-MoE\nHybrid</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>MoEMoE</p>\n<img src=\"/44e38c1b/xiaomi_moe.jpg\" class title=\"MoE\">\n<p>MoE</p>\n<p>MoEMoEMoE</p>\n<p>20244DeepSeek-MoEQwen1.5-MoE</p>\n<h1 id=\"\"></h1>\n<p>MoE</p>\n<p>MoEGoogle</p>\n<h2 id=\"\"></h2>\n<p>MoE1991<a href=\"https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf\">Adaptive\nMixtures of Local Experts</a>Geoffrey HintonMichael I.\nJordanMoE1988</p>\n<blockquote>\n<p>This idea was first presented by Jacobs and Hinton at the\nConnectionist Summer School in Pittsburg in 1988.</p>\n</blockquote>\n<p>MoEMoE</p>\n<h2 id=\"rnn\">RNN</h2>\n<p>Google20171<a href=\"https://arxiv.org/abs/1701.06538\">Outrageously Large Neural\nNetworks: The Sparsely-Gated Mixture-of-Experts\nLayer</a>MoELSTM137B128kLSTM</p>\n<h2 id=\"transformer\">Transformer</h2>\n<ol type=\"1\">\n<li><p>20206Google<a href=\"https://arxiv.org/abs/2006.16668\">GShard: Scaling Giant Models\nwith Conditional Computation and Automatic\nSharding</a>MoEencoder-decodertransformerFFNMoE12.5B600BMoE2048</p></li>\n<li><p>20211Google<a href=\"https://arxiv.org/abs/2101.03961\">Switch Transformers: Scaling\nto Trillion Parameter Models with Simple and Efficient Sparsity</a>\nT5encoder-decoderFFNMoErouting1.6Tswitch\ntransformerSwitch\nTransformersscaling</p></li>\n<li><p>20222Google<a href=\"https://arxiv.org/abs/2202.08906\">ST-MoE: Designing Stable and\nTransferable Sparse Expert\nModels</a>encoder-decoderMoE269B32BST-MoEMoESwitch\nTransformer</p></li>\n</ol>\n<h2 id=\"gpt\">GPT</h2>\n<ol type=\"1\">\n<li><p>202112GoogleGLaM<a href=\"https://arxiv.org/abs/2112.06905\">GLaM: Efficient Scaling of\nLanguage Models with\nMixture-of-Experts</a>1.2Tdecoder-onlyencoder-decoderdecoder-onlyGoogle</p></li>\n<li><p>20241<a href=\"https://arxiv.org/abs/2401.06066\">DeepSeekMoE: Towards Ultimate\nExpert Specialization in Mixture-of-Experts Language\nModels</a>2312DeepSeekMoE</p></li>\n<li><p>2024DatabricksDBRXQwen1.5-MoE-A2.7BMistral\nAIMistral-8x22B</p></li>\n</ol>\n<h1 id=\"\"></h1>\n<p>Geoffrey HintonMichael I. Jordan<a href=\"https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf\">Adaptive\nMixtures of Local Experts</a>MoE</p>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<p></p>\n<p>MoEexpert</p>\n<p>MoEvowel discrimination\ntaskMoEaeiou</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p>MoEexpert networkgating\nnetworkexpertgating\nnetworkexpertexpertstochastictruefalse</p>\n<img src=\"/44e38c1b/vanilla_moe.png\" class title=\"Vanilla MoE\">\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>MoEideaJacobsHinton1988lossensembleexpertexpertexpertresidual</p>\n<p>case <span class=\"math inline\">\\(c\\)</span>\n<span class=\"math inline\">\\(d^c\\)</span> ground truth <span class=\"math inline\">\\(i\\)</span> expert <span class=\"math inline\">\\(o_{i}^c\\)</span><span class=\"math inline\">\\(p_{i}^c\\)</span> gating network <span class=\"math inline\">\\(i\\)</span>\nexpert <span class=\"math inline\">\\(E^{c}\\)</span> </p>\n<p><span class=\"math display\">\\[E^{c}=\\left|\\left|d^{c}-\\sum_{i}p_{i}^{c}o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p>expert</p>\n<p>expertexpertexpert</p>\n<p>expertexpertexpertexpertgating\nnetwork</p>\n<p></p>\n<p>HintonJordanlossexpert</p>\n<p>gating networkexpert</p>\n<p><span class=\"math display\">\\[E^{c}=\\langle\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2\\rangle=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p>expertexpertexpert</p>\n<p>losslocalizationcasegating\nnetworkexpertgating\nnetworkexpert</p>\n<p>localizationexpert</p>\n<p>expertexpertgating\nnetworkexpert\nerror+-</p>\n<p>expert0</p>\n<ol start=\"4\" type=\"1\">\n<li></li>\n</ol>\n<p>losslossloss</p>\n<p><span class=\"math display\">\\[\\text{loss}E^{c}=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p><span class=\"math display\">\\[\\text{loss}E^c=-log\\sum_ip_i^ce^{-\\frac12\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}\\]</span></p>\n<p>lossloss</p>\n<p><span class=\"math display\">\\[\\text{loss}\\frac{\\partial\nE^c}{\\partial\\mathbf{o}_i^c}=-2p_i^c(\\mathbf{d}^c-\\mathbf{o}_i^c)\\]</span></p>\n<p><span class=\"math display\">\\[\\text{loss}\\frac{\\partial\nE^c}{\\partial\\mathbf{o}_i^c}=-\\left[\\frac{p_i^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}}{\\sum_jp_j^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_j^c\\|^2}}\\right](\\mathbf{d}^c-\\mathbf{o}_i^c)\\]</span></p>\n<p>lossloss <span class=\"math inline\">\\(i\\)</span>\nexpertexpertexpert <span class=\"math inline\">\\(i\\)</span>\ncasegating\nnetworklosscaseexpertlosslossexpertlocalizationexpert</p>\n<p>BTWloss</p>\n<p>MoE</p>\n<h1 id=\"lstm-moe\">LSTM MoE</h1>\n<p>Google20171 <a href=\"https://arxiv.org/abs/1701.06538\">OUTRAGEOUSLY LARGE NEURAL\nNETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS\nLAYER</a>MoELSTM137BLSTM7</p>\n<p>1991</p>\n<h2 id=\"\"></h2>\n<p>Transformer</p>\n<p>conditional\ncomputationconditional\ncomputationMoE</p>\n<p></p>\n<ul>\n<li>MoEexpertbatch sizebatch\nsize<br>\nbatch\nsize3216expertexpert2batch\nsizebatch\nsizebatch\nsize<br>\n</li>\n<li><br>\nNLP<br>\n</li>\n<li><br>\n<br>\n</li>\n<li><br>\nGPU<br>\n</li>\n<li>GPU<br>\nGPUbranchingif/elseMoEgating\nnetwork</li>\n</ul>\n<p></p>\n<h2 id=\"\"></h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<p>LSTMMoEembedding</p>\n<img src=\"/44e38c1b/rnn_moe.png\" class title=\"LSTM MoE\">\n<p>expertfeed-forward neural\nnetworknexpertgating networkn</p>\n<p><span class=\"math display\">\\[\\begin{aligned}y=\\sum_{i=1}^nG(x)_iE_i(x)\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(E_{i}(x)\\)</span>  <span class=\"math inline\">\\(i\\)</span> expert<span class=\"math inline\">\\(G(x)_{i}\\)</span> gating network <span class=\"math inline\">\\(i\\)</span> expert</p>\n<p> <span class=\"math inline\">\\(G(x)_{i}\\)</span>\n0expert</p>\n<p>experttwo-level hierarchical\nMoEgating networkgating\nnetworkexpertgating\nnetworkexpertword2vechierarchical\nsoftmax</p>\n<ol start=\"2\" type=\"1\">\n<li>gating network</li>\n</ol>\n<p>gating network</p>\n<p>softmaxgating\nfunction</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G_\\sigma(x)=Softmax(x\\cdot\nW_g)\\end{aligned}\\]</span></p>\n<p>topkksoftmax0expert</p>\n<p>sparsitytopkgating\nfunction</p>\n<p>Gaussian\nnoisenoise</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[KeepTopK(v,k)_i=\\begin{cases}v_i&amp;\\text{if\n}v_i\\text{ is in the top }k\\text{ elements of\n}v.\\\\-\\infty&amp;\\text{otherwise.}\\end{cases}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}H(x)_i=(x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\end{aligned}\\]</span></p>\n<p>noisesoftplusReLU</p>\n<img src=\"/44e38c1b/softplus.png\" class title=\"softplus\">\n<p></p>\n<h2 id=\"\"></h2>\n<p>MoEgating\nnetworkexpertexpert</p>\n<p></p>\n<p>hard\nconstraintexperthard\nconstraintsoft\nconstraint</p>\n<p>expert</p>\n<p><span class=\"math display\">\\[Importance(X)=\\sum_{x\\in\nX}G(x)\\]</span></p>\n<p><span class=\"math inline\">\\(G(x)\\)</span> gating\nnetworkexpert</p>\n<p> <span class=\"math inline\">\\(L_{importance}\\)</span><span class=\"math inline\">\\(L_{importance}\\)</span> </p>\n<p><span class=\"math display\">\\[L_{importance}(X)=w_{importance}\\cdot\nCV(Importance(X))^2\\]</span></p>\n<p> <span class=\"math inline\">\\(w_{importance}\\)</span>\n0.1CVcoefficient of variation</p>\n<p>coefficient of\nvariation\n<span class=\"math inline\">\\(\\sigma\\)</span>   <span class=\"math inline\">\\(\\mu\\)</span> </p>\n<p>MoEexpertexpertgating\n<span class=\"math inline\">\\(L_{importance}\\)</span> </p>\n<p> <span class=\"math inline\">\\(L_{importance}\\)</span>  <span class=\"math inline\">\\(L_{importance}\\)</span>\n <span class=\"math inline\">\\(L_{importance}\\)</span>\n</p>\n<p>expertexpertgating</p>\n<p> <span class=\"math inline\">\\(L_{load}\\)</span>\nexpert</p>\n<p>expertback\npropagation <span class=\"math inline\">\\(L_{load}\\)</span>\nexpert</p>\n<p>MoE <span class=\"math inline\">\\(H(x)\\)</span> KeepTopK</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}H(x)_i=(x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(kth\\_excluding(H(x),k,i)\\)</span>\n<span class=\"math inline\">\\(H(x)\\)</span>  <span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(P(x,i)\\)</span>\nnoise <span class=\"math inline\">\\(i\\)</span> noise <span class=\"math inline\">\\(kth\\_excluding(H(x),k,i)\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{aligned}P(x,i)=Pr\\Big((x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\\\&gt;kth\\_excluding(H(x),k,i)\\Big)\\end{aligned}\\]</span></p>\n<p>noise <span class=\"math inline\">\\(i\\)</span>\n <span class=\"math inline\">\\(i\\)</span>\n<span class=\"math inline\">\\(P(x,i)\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{aligned}P(x,i)&amp;=\\Phi\\Big(\\frac{(x\\cdot\nW_g)_i-kth\\_excluding(H(x),k,i)}{Softplus((x\\cdot\nW_{noise})_i)}\\Big)\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\Phi\\)</span>\nCDF</p>\n<p> <span class=\"math inline\">\\(i\\)</span>\nexpert</p>\n<p><span class=\"math display\">\\[\\begin{aligned}Load(X)_i=\\sum_{x\\in\nX}P(x,i)\\end{aligned}\\]</span></p>\n<p>expert</p>\n<p><span class=\"math display\">\\[L_{load}(X)=w_{load}\\cdot\nCV(Load(X))^2\\]</span></p>\n<p><span class=\"math inline\">\\(w_{load}\\)</span>\n0.1</p>\n<p> <span class=\"math inline\">\\(L_{importance}(X)\\)</span><span class=\"math inline\">\\(Load(X)\\)</span>\n</p>\n<p>expert\n<span class=\"math inline\">\\(W_g\\)</span>  <span class=\"math inline\">\\(W_{noise}\\)</span>\n0</p>\n<p></p>\n<img src=\"/44e38c1b/rnn_moe_load_function.png\" class title=\"\">\n<p></p>\n<h2 id=\"\"></h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<p>1batch size</p>\n<p>expertbatch\nsizenexpertkbatch\nsizebexpertbatch\nsizekb/nexpertbatch size<br>\n-\nbatchbatchMoEexpertdexpertkbd/nbatch\nsize - LSTMbatch\nsize</p>\n<p>2</p>\n<p></p>\n<p>expertinputoutput[input_size,\nhidden_size][hidden_size,\noutput_size]GPU1000hidden_sizeexpert1000</p>\n<ol start=\"2\" type=\"1\">\n<li> &amp; </li>\n</ol>\n<p>MoEdense4/32/256expertflat\nMoE256/1024/4096experthierarchical\nMoEexpert1Mflat4experthierarchical\nMoEgating2</p>\n<p>ppldenseMoEMoE</p>\n<img src=\"/44e38c1b/rnn_moe_perf.png\" class title=\"\">\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>4Bdiminishing\nreturns</p>\n<p> + 100B\ntoken32, 256, 10244096, 16384, 65536,\n131072expertMoE137B</p>\n<p></p>\n<img src=\"/44e38c1b/rnn_moe_137b.png\" class title=\"137\">\n<p></p>\n<ol start=\"4\" type=\"1\">\n<li>Expert Specialization</li>\n</ol>\n<p>MoE</p>\n<p>tokenspecialization</p>\n<img src=\"/44e38c1b/rnn_moe_specilized.png\" class title=\"RNN MoE \">\n<h1 id=\"gshard\">GShard</h1>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>2018Berttransformer20206GoogleGShard:\nScaling Giant Models with Conditional Computation and Automatic\nShardingMoEencoder-decodertransformerMoE</p>\n<p>GShardMoE600B</p>\n<img src=\"/44e38c1b/gshard_moe_family.png\" class title=\"GShard MoE family\">\n<p>expertLSMT MoE --\nexpert24expertChatGPTBertGPT</p>\n<p>GShardMoE</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<p>Googleencoder-decoder\ntransfomerGShardencoder-decoder\ntransfomer</p>\n<p>GShardencoderdecoderFFNMoENN/2MoE</p>\n<img src=\"/44e38c1b/gshard_model.png\" class title=\"GShard\">\n<p>top-2 expert</p>\n<p>GShardLSTM MoEgating\nfunctionauxiliary loss function</p>\n<p>MoE</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{G}_{s,E}&amp; =\\mathrm{GATE}(x_s)  \\\\\n\\mathrm{FFN}_e(x_s)&amp; =wo_e\\cdot\\text{ReLU}(wi_e\\cdot x_s)  \\\\\ny_{s}&amp; =\\sum_{e=1}^E\\mathcal{G}_{s,e}\\cdot\\mathrm{FFN}_e(x_s)\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(x_s\\)</span> MoEtoken<span class=\"math inline\">\\(w_i\\)</span>  <span class=\"math inline\">\\(w_o\\)</span>\n<span class=\"math inline\">\\(\\mathcal{G}_{s}\\)</span> gating\nfunction</p>\n<p>GShardgating\nfunction12</p>\n<p>NtokenEexpertNEgating\nfunction</p>\n<p>gating function</p>\n<p>1 expert capacity</p>\n<p>experttokenexperttoken2N/E</p>\n<p>expert capacityGATE()expert <span class=\"math inline\">\\(c_e\\)</span>\ntokentokenexpert</p>\n<p>2 Local group dispatching</p>\n<p>tokenG2N/EG</p>\n<p>batchbatchbatchgroupall2allgroup</p>\n<p>groupgradient\naccumulation</p>\n<p>3 Auxiliary loss</p>\n<p>gatingLSTM\nMoE</p>\n<p><span class=\"math display\">\\[\\ell_{aux}=\\frac1E\\sum_{e=1}^E\\frac{c_e}S\\cdot\nm_e\\]</span></p>\n<p><span class=\"math inline\">\\(S\\)</span> token<span class=\"math inline\">\\(E\\)</span> <span class=\"math inline\">\\(c_e\\)</span>  <span class=\"math inline\">\\(e\\)</span> token<span class=\"math inline\">\\(m_e\\)</span>  <span class=\"math inline\">\\(e\\)</span> expert <span class=\"math inline\">\\(S\\)</span> token</p>\n<p> <span class=\"math inline\">\\(\\frac{c_e}S\\)</span>\n <span class=\"math inline\">\\(\\frac{c_e}S\\)</span>  <span class=\"math inline\">\\(m_e\\)</span>  <span class=\"math inline\">\\(m_e\\)</span>  <span class=\"math inline\">\\(e\\)</span> expert <span class=\"math inline\">\\(S\\)</span>\ntokenloss</p>\n<p>loss</p>\n<p>gating</p>\n<img src=\"/44e38c1b/gshard_algo_1.png\" class title=\"GShard gating \">\n<p>4 Random routing</p>\n<p>top-2\nexperttop-1</p>\n<p>top-1g2</p>\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<img src=\"/44e38c1b/gshard_perf.png\" class title=\"GShard\">\n<h1 id=\"switch-transformer\">Switch Transformer</h1>\n<p>20224ChatGPTGoogleSwitch\nTransformers: Scaling to Trillion Parameter Models with Simple and\nEfficient Sparsity2021GoogleSwitch\nTransformer</p>\n<p>Switch\nTransformerGShardencoder-decoderT51.6T2048expert</p>\n<p>Switch\nTransformer</p>\n<p>Switch\nTransformerSwitch\nTransformerFLOPS/token</p>\n<p>Switch Transformer</p>\n<p>1TransformerMoESwitch\nTransformer</p>\n<p>2MoE to\ndenseMoEdenseMoE99%dense</p>\n<p>3<br>\n- bf16MoE<br>\n- MoE<br>\n- </p>\n<p>41TMoE</p>\n<p>5101</p>\n<p>6FLOPS/tokenSwitch\nTransformer</p>\n<h2 id=\"-1\"></h2>\n<p>Switch\nTransformerGShardtransformerFFNMoE</p>\n<img src=\"/44e38c1b/switch_transformer_structure.png\" class title=\"Switch Transformer \">\n<p>Switch Transformergating\nfunctionSwitch Transformerrouting</p>\n<p>kexpertSwitch\nTransformergating1expertk=1MoESwitch\nlayer</p>\n<p>routingrouter</p>\n<h2 id=\"-1\"></h2>\n<p>GShardSwitch Transformerexpert\ncapacityexpertbatchtoken</p>\n<p>tokenexpertoverflowtokenGShard</p>\n<p>Switch Transformercapacity factor</p>\n<p><span class=\"math display\">\\[\\text{expert\ncapacity}=\\left(\\frac{\\text{tokens per batch}}{\\text{number of\nexperts}}\\right)\\times\\text{capacity factor}.\\]</span></p>\n<p>capacity\nfactorexperttokenoverflow</p>\n<p>capacity factor</p>\n<img src=\"/44e38c1b/switch_transformer_diff_expert_capacity.png\" class title=\"expert capacity\">\n<p>expert capacity</p>\n<p>capacity\nfactor1overflowcapacity\nfactor</p>\n<p>expertoverflowMoESwitch\nTransformer128</p>\n<p>capacity\nfactoroverflow</p>\n<img src=\"/44e38c1b/switch_transformer_capacity_effect.png\" class title=\"expert capacity\">\n<p>tokenscalingoverflow</p>\n<p></p>\n<p> <span class=\"math inline\">\\(N\\)</span> expert <span class=\"math inline\">\\(T\\)</span> tokenbatch <span class=\"math inline\">\\(\\mathcal{B}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\text{loss}&amp;=\\alpha\\cdot\nN\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(f_{i}\\)</span>  <span class=\"math inline\">\\(i\\)</span> experttoken</p>\n<p><span class=\"math display\">\\[\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax\n}p(x)=i\\}\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(P_i\\)</span>\nbatchtoken<span class=\"math inline\">\\(i\\)</span>\nexpert</p>\n<p><span class=\"math display\">\\[\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}\\]</span></p>\n<p>GShard</p>\n<p><span class=\"math inline\">\\(f\\)</span> \n<span class=\"math inline\">\\(P\\)</span>  <span class=\"math inline\">\\(1/N\\)</span></p>\n<p><span class=\"math inline\">\\(\\alpha\\)</span>\n1e-51e-11e-2</p>\n<p> <span class=\"math inline\">\\(\\sum_{i=1}^N(f_i\\cdot\nP_i)=\\sum_{i=1}^N(\\frac1N\\cdot\\frac1N)=\\frac1N\\)</span>loss\n<span class=\"math inline\">\\(N\\)</span>expertloss</p>\n<h2 id=\"-1\"></h2>\n<ol type=\"1\">\n<li>trick</li>\n</ol>\n<p>1bf16</p>\n<p>bf16routing\nfunction</p>\n<p>routingsoftmaxexponentialrounding\nerrorrouting</p>\n<p>2</p>\n<p> <span class=\"math inline\">\\(\\mu=0\\)</span><span class=\"math inline\">\\(\\sigma=\\sqrt{s}/n\\)</span>sne.g.\nfan-in</p>\n<p>Transformers=1.010</p>\n<img src=\"/44e38c1b/switch_transformer_init.png\" class title=\"\">\n<p>3dropout</p>\n<p>Switch\nTransformerdropout</p>\n<img src=\"/44e38c1b/switch_transformer_dropout.png\" class title=\"dropout\">\n<p>dropoutdense0.1expertdropout</p>\n<ol start=\"2\" type=\"1\">\n<li>scaling</li>\n</ol>\n<p>Switch Transformerscaling</p>\n<p>1Step-Basis</p>\n<p>stepexpert</p>\n<p>stepstep</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_step.png\" class title=\"step scaling\">\n<p>2Time-Basis</p>\n<p>Switch\nTransformerstepSwitch\nTransformerdense</p>\n<p>Switch\nTransformerdenseSwitch\nTransformerdensedense1/7</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_time.png\" class title=\"time scaling\">\n<p>3dense</p>\n<p>Switch\nTransformerdenseSwitch\nTransformerdense</p>\n<p>Step-BasisTime-Basis64Switch\nTransformerT5-LargestepSwitch\nTransformer</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_dense.png\" class title=\"dense\">\n<ol start=\"3\" type=\"1\">\n<li>SFT</li>\n</ol>\n<p>GLUESuperGLUEdense</p>\n<p>eval</p>\n<img src=\"/44e38c1b/switch_transformer_sft_result.png\" class title=\"sft\">\n<ol start=\"4\" type=\"1\">\n<li></li>\n</ol>\n<p>Switch\nTransformerBTdense</p>\n<p><br>\n- Switch\nTransformerdense<br>\n- label25%75%ground truth</p>\n<p>densedensedenseSwitch\nTransformer30%</p>\n<img src=\"/44e38c1b/switch_transformer_distill.png\" class title=\"\">\n<p>99%</p>\n<img src=\"/44e38c1b/switch_transformer_distill_diff_model.png\" class title=\"\">\n<p>SuperGLUE</p>\n<img src=\"/44e38c1b/switch_transformer_distill_sft.png\" class title=\"sft\">\n<h1 id=\"glam\">GLaM</h1>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>202112GoogleGLaM: Efficient Scaling of Language Models\nwith\nMixture-of-Experts1.2T64token96.6BMoE</p>\n<p>Switch TransformerGLaM1.6T\ntoken</p>\n<p></p>\n<img src=\"/44e38c1b/glam_related_model.png\" class title=\"glam\">\n<p>GPT-3175BGPT-3NLPGPT-3</p>\n<img src=\"/44e38c1b/glam_compare_gpt3.png\" class title=\"glamgpt3\">\n<img src=\"/44e38c1b/glam_compare_gpt3_2.png\" class title=\"glamgpt3\">\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p>Switch\nTransformerFFNMoESwitch\nTransformerGLaMexpert</p>\n<img src=\"/44e38c1b/glam_model.png\" class title=\"glam\">\n<p></p>\n<p>1</p>\n<p>XLNET</p>\n<p>2</p>\n<blockquote>\n<p>In the non-MoE Transformer feed-forward sub-layers, we replace the\nfirst linear projection and the activation function with the Gated\nLinear Unitwhich computes the component-wise product of two linear\ntransformation of the input, followed by a Gaussian Error Linear\nUnit.</p>\n</blockquote>\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>trick</p>\n<p>1Lingvo: a modular and scalable framework for\nsequence-to-sequence\nmodelingNaNInf</p>\n<p>2BPNaNInfcheckpointNaNInf</p>\n<p>MoE</p>\n<img src=\"/44e38c1b/glam_family.png\" class title=\"glam\">\n<p>GLaMdense</p>\n<img src=\"/44e38c1b/glam_perf.png\" class title=\"glam\">\n<p>GLaM MoEdense</p>\n<h1 id=\"st-moe\">ST-MoE</h1>\n<p>20222GoogleST-MOE: DESIGNING STABLE AND TRANSFERABLE\nSPARSE EXPERT\nMODELSST-MoEMoEMoE</p>\n<p>ST-MoE269B32B\ndenseStable Transferable\nMixture-of-ExpertsST-MoE-32B</p>\n<p>MoEST-MoESwitch\nTransformer1MoE</p>\n<p>ST-MoE4B269BST-MoE</p>\n<img src=\"/44e38c1b/st_moe_models.png\" class title=\"ST-MoE\">\n<h2 id=\"\"></h2>\n<p></p>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<p></p>\n<blockquote>\n<p>Some architectural improvements involve more multiplications than\nadditions or do not sum many items at once</p>\n</blockquote>\n<p>1GELU Gated Linear Units (GEGLU)</p>\n<p>GLUcomponent-wiseGELU-Linear\nFFNtransformerReLU FFN</p>\n<p><span class=\"math display\">\\[\\begin{aligned}FFN_{GEGLU}(x,W,V,b,c)=GELU(xW+b)\\odot(xV+c)\\end{aligned}\\]</span></p>\n<p></p>\n<p>2RMSNorm</p>\n<p>RMSNorm <span class=\"math inline\">\\(g\\)</span></p>\n<p><span class=\"math display\">\\[y_i=\\frac{x_i}{\\sqrt{\\frac1d\\sum_{i=1}^dx_i^2}}\\cdot\ng_i\\]</span></p>\n<p>ST-MoEGEGLURMSNorm</p>\n<img src=\"/44e38c1b/st_moe_remove_multiplications.png\" class title=\"\">\n<p></p>\n<p>3dense</p>\n<p>ST-MoEexpertdensedense</p>\n<img src=\"/44e38c1b/st_moe_more_dense_layer.png\" class title=\"dense\">\n<p>4bias</p>\n<p>FFNbias\nB</p>\n<p><span class=\"math display\">\\[\\text{FFN}_{\\text{GEGLU}}+\\text{Add\nBias}(x)=[(\\text{GELU}(xW_{11})\\odot xW_{12})+B]W_2\\]</span></p>\n<p><span class=\"math display\">\\[\\mathrm{FFN}_{\\mathrm{GEGLU}}+\\mathrm{Mult~Bias}(x)=[(\\mathrm{GELU}(xW_{11})\\odot\nxW_{12})\\odot B]W_2\\]</span></p>\n<p></p>\n<p></p>\n<ol start=\"2\" type=\"1\">\n<li>noise</li>\n</ol>\n<p>ST-MoE</p>\n<p>input-jitterrouterlogits[1e-2,\n1e2]</p>\n<img src=\"/44e38c1b/st_moe_more_add_noise.png\" class title=\"noise\">\n<p>noise</p>\n<p></p>\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>activationgradient</p>\n<p>ST-MoE269B</p>\n<p>ST-MoErouter z-loss</p>\n<p><span class=\"math display\">\\[L_z(x)=\\frac{1}{B}\\sum_{i=1}^B\\left(\\log\\sum_{j=1}^Ne^{x_j^{(i)}}\\right)^2\\]</span></p>\n<p><span class=\"math inline\">\\(B\\)</span> token<span class=\"math inline\">\\(N\\)</span> <span class=\"math inline\">\\(x\\in\\mathcal{R}^{B\\times N}\\)</span>\nrouter</p>\n<p>z-lossrouterlogitsz-loss</p>\n<img src=\"/44e38c1b/st_moe_z_loss_result.png\" class title=\"z-loss\">\n<p>ST-MoEz-loss</p>\n<p>z-loss <span class=\"math inline\">\\(c_z\\)</span>\n</p>\n<p><span class=\"math display\">\\[L_{tot}=L_{CE}+c_BL_B+c_zL_Z\\]</span></p>\n<p>ST-MoE<span class=\"math inline\">\\(c_z=0.001\\)</span></p>\n<p><span class=\"math inline\">\\(L_B\\)</span>  auxiliary load balance\nlossST-MoEGShard/Switch\nTransformer</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\text{loss}&amp;=\\alpha\\cdot\nN\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax\n}p(x)=i\\}\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(N\\)</span>  <span class=\"math inline\">\\(\\mathcal{B}\\)</span> <span class=\"math inline\">\\(T\\)</span> tokenbatch<span class=\"math inline\">\\(f_{i}\\)</span>  <span class=\"math inline\">\\(i\\)</span> experttoken<span class=\"math inline\">\\(P_i\\)</span> batchtoken<span class=\"math inline\">\\(i\\)</span> expert</p>\n<ol start=\"4\" type=\"1\">\n<li></li>\n</ol>\n<p>float32bfloat16bfloat16allreducebfloat16float32</p>\n<p>ST-MoE-32BallreduceST-MoEallreducefloat32</p>\n<p>bfloat16float32</p>\n<img src=\"/44e38c1b/st_moe_round_error.png\" class title=\"bf16\">\n<p>z-loss</p>\n<p>MoErouter</p>\n<p>ST-MoE1/5token</p>\n<p>softmaxMoE</p>\n<h2 id=\"-2\"></h2>\n<p>densescaling\nlawMoEdense</p>\n<p>1expert</p>\n<p>2routing</p>\n<p>3</p>\n<p>4</p>\n<p>MoEscaling lawUnified scaling laws for routed\nlanguage models</p>\n<ol type=\"1\">\n<li>expert</li>\n</ol>\n<p>ST-MoE8/16/32&lt;1%&gt;256</p>\n<p>&gt;1&lt;=1</p>\n<ol start=\"2\" type=\"1\">\n<li>routingcapacity factor</li>\n</ol>\n<p>capacity factor</p>\n<img src=\"/44e38c1b/st_moe_capacity_factor.png\" class title=\"capacity factor\">\n<p></p>\n<p>1capacity factor</p>\n<p>2capacity\nfacotr</p>\n<p>3expertcapacity\nfactor</p>\n<p>capacity\nfactorcapacity\nfactor</p>\n<p>capacity factor</p>\n<img src=\"/44e38c1b/st_moe_capacity_factor_speed.png\" class title=\"capacity factor\">\n<h2 id=\"-2\"></h2>\n<ol type=\"1\">\n<li>ST-MoE</li>\n</ol>\n<p>ST-MoE-32BST-MoE-32B</p>\n<img src=\"/44e38c1b/st_moe_perf.png\" class title=\"capacity ST-MoE-32B\">\n<ol start=\"2\" type=\"1\">\n<li>Expert Specialization</li>\n</ol>\n<p>decodertokenencoder</p>\n<img src=\"/44e38c1b/st_moe_encoder_specialization.png\" class title=\"encoder\">\n<p>encodertoken</p>\n<img src=\"/44e38c1b/st_moe_multiling_specialization.png\" class title=\"\">\n<h1 id=\"deepseekmoe\">DeepseekMoE</h1>\n<p>20241DeepseekMoEMoEDeepSeekMoE:\nTowards Ultimate Expert Specialization in Mixture-of-Experts Language\nModelsDeepSeekMoE</p>\n<p>DeepSeekMoEMoE2</p>\n<p>1expertexpert</p>\n<p>2expertexpertshared\nexpert</p>\n<p>expert(specialization)</p>\n<p>DeepSeekMoE2BMoE16BMoEDeepSeekMoE-16B40GB</p>\n<p>DeepSeekMoE-2B2BDeepSeekMoE-16B7B40%</p>\n<p>DeepSeekMoE-16B</p>\n<img src=\"/44e38c1b/ds_moe_perf.png\" class title=\"deepseek moe\">\n<p>DeepSeekMoE-2B16B</p>\n<p>DeepSeekMoE-145BMoEDeepSeek-67B</p>\n<h2 id=\"-3\"></h2>\n<p>MoEmixture of\nexpertmotivationexpert</p>\n<p>1991expert</p>\n<p>MoEknowledge hybridityknowledge\nredundancy</p>\n<p>1</p>\n<p>expertexpert</p>\n<p>2</p>\n<p>expertexpertexpertexpert8expertexpert</p>\n<p>(expert\nspecialization)MoE</p>\n<p>expertnon-overlap &amp; foucusd\nknowledge</p>\n<p>DeepSeekMoE2</p>\n<p>1Fine-Grained Expert Segmentation</p>\n<p>expertexpertexpertspecialization16expert2120expert1/464expert8\n<span class=\"math inline\">\\(\\binom{64}8=4,426,165,368\\)</span>\n</p>\n<p>2Shared Expert Isolation</p>\n<p>expertcommon\nknowledgeexpertexpertexpertexpert</p>\n<p>MoEFine-Grained Expert SegmentationShared\nExpert Isolation</p>\n<img src=\"/44e38c1b/ds_moe_structure.png\" class title=\"deepseek moe \">\n<p>expert isolation20221DeepSpeed-MoE:\nAdvancing Mixture-of-Experts Inference and Training to Power\nNext-Generation AI Scale</p>\n<p>MoEexpert <span class=\"math inline\">\\(N\\)</span>expert <span class=\"math inline\">\\(K\\)</span>DeepSeekMoEexpert\n<span class=\"math inline\">\\(1/m\\)</span>DeepSeekMoE <span class=\"math inline\">\\(mN\\)</span> expertexpert <span class=\"math inline\">\\(mK\\)</span> <span class=\"math inline\">\\(T\\)</span> <span class=\"math inline\">\\(L\\)</span> <span class=\"math inline\">\\(e_i^l\\)</span>  <span class=\"math inline\">\\(i\\)</span>\nexpertDeepSeekMoElayernorm</p>\n<p><span class=\"math display\">\\[\\mathbf{u}_{1:T}^l=\\text{Self-Att}\\left(\\mathbf{h}_{1:T}^{l-1}\\right)+\\mathbf{h}_{1:T}^{l-1}\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{h}_t^l=\\sum_{i=1}^{mN}\\left(g_{i,t}\\text{\nFFN}_i\\left(\\mathbf{u}_t^l\\right)\\right)+\\mathbf{u}_t^l\\]</span></p>\n<p><span class=\"math display\">\\[g_{i,t}=\\begin{cases}s_{i,t},&amp;s_{i,t}\\in\\text{Topk}(\\{s_{j,t}|1\\leqslant\nj\\leqslant mN\\},mK)\\\\0,&amp;\\text{otherwise,}\\end{cases}\\]</span></p>\n<p><span class=\"math display\">\\[s_{i,t}=\\mathrm{Softmax}_i\\left({\\mathbf{u}_t^l}^T\\mathbf{e}_i^l\\right)\\]</span></p>\n<h2 id=\"-2\"></h2>\n<p>MoEgating</p>\n<p>1routing\ncollapsegatingexpert</p>\n<p>2</p>\n<p>routing collapseDeepSeekMoEexpert-level balance\nloss</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}_{\\mathrm{ExpBal}}&amp; =\\alpha_1\\sum_{i=1}^{N&#39;}f_iP_i\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nf_{i}&amp;\n=\\frac{N^{\\prime}}{K^{\\prime}T}\\sum_{t=1}^T\\mathbb{1}(\\text{Token\n}t\\text{ selects Expert }i)\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nP_{i}&amp; =\\frac1T\\sum_{t=1}^Ts_{i,t}\n\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(\\alpha_1\\)</span> expert-level\nbalance factor</p>\n<p> <span class=\"math inline\">\\(f_i\\)</span>  <span class=\"math inline\">\\(P_i\\)</span> Switch\nTransformer</p>\n<p>Switch Transformer <span class=\"math inline\">\\(f_i\\)</span>\n <span class=\"math inline\">\\(i\\)</span>\nexperttokenDeepSeekMoE\n<span class=\"math inline\">\\(N&#39;/K&#39;\\)</span>  <span class=\"math inline\">\\(N&#39;=mN-K_s\\)</span><span class=\"math inline\">\\(K&#39;=mK-K_s\\)</span><span class=\"math inline\">\\(K_s\\)</span>\nexpertDeepSeekMoE\n<span class=\"math inline\">\\(f_i\\)</span> Switch\nTransformer</p>\n<p><span class=\"math inline\">\\(N&#39;/K&#39;\\)</span>\nexpertloss</p>\n<p><span class=\"math inline\">\\(P_i\\)</span> token\n<span class=\"math inline\">\\(i\\)</span> expertSwitch\nTransformer</p>\n<p> <span class=\"math inline\">\\(f_i\\)</span> <span class=\"math inline\">\\(P_i\\)</span> </p>\n<p>DeepSeekMoEdevice-level balance\nloss</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}_{\\mathrm{DevBal}}&amp; =\\alpha_2\\sum_{i=1}^Df_i&#39;P_i&#39;\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nf_i^{\\prime}&amp; =\\frac1{|\\mathcal{E}_i|}\\sum_{j\\in\\mathcal{E}_i}f_j\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nP_{i}^{\\prime}&amp; =\\sum_{j\\in\\mathcal{E}_i}P_j\n\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(\\alpha_2\\)</span> device-level\nbalance factor</p>\n<p><span class=\"math inline\">\\(\\mathcal{E}_i\\)</span>  <span class=\"math inline\">\\(i\\)</span> </p>\n<p>device-level balance lossexpert-level balance loss\n<span class=\"math inline\">\\(f_i\\)</span>  <span class=\"math inline\">\\(P_i\\)</span>\nexpert</p>\n<p>expert64expert8tokentokenexpert</p>\n<p>expert</p>\n<h2 id=\"-3\"></h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>100B\ntokenDeepSeekMoE-2BBPE8k</p>\n<p>DeepSeekMoE-2B0.006multi-head\nattention0.3B</p>\n<img src=\"/44e38c1b/ds_model_param.png\" class title=\"\">\n<p>relative expert\nsizeDeepSeekMoEexpertFFN</p>\n<p></p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">optimizer</td>\n<td style=\"text-align: center;\">AdamW</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">adam_beta_1</td>\n<td style=\"text-align: center;\">0.9</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">adam_beta_2</td>\n<td style=\"text-align: center;\">0.95</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">adam_weight_decay</td>\n<td style=\"text-align: center;\">0.1</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">warmup schedule</td>\n<td style=\"text-align: center;\">linear</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">warmup step</td>\n<td style=\"text-align: center;\">2000</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">max lr</td>\n<td style=\"text-align: center;\">1.08e-3</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">dropout</td>\n<td style=\"text-align: center;\">0</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">sequence length</td>\n<td style=\"text-align: center;\">2k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">batch size</td>\n<td style=\"text-align: center;\">2k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">total step</td>\n<td style=\"text-align: center;\">25,000</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p><br>\n- expertGPUdevice-level balance loss<br>\n- expert-level balance factor0.01<br>\n- 80%0.31690%0.316</p>\n<p>100BDeepSeekMoE-2Bbenchmark4densehash\nlayermoeHash layers for large sparse modelsSwitch\nTransformerGShard</p>\n<img src=\"/44e38c1b/ds_moe_comparison.png\" class title=\"deepseek moe \">\n<p><br>\n- Hash LayerSwitch\nTransformerdense<br>\n- GSshardHash LayerSwitch\nTransformer<br>\n- DeepSeekMoEGShard</p>\n<p>DeepSeekMoEdenseGShardDeepSeekMoE-2B</p>\n<p>denseGShard161.5DeepSeekMoE-2B</p>\n<img src=\"/44e38c1b/ds_moe_upper_bound_2b.png\" class title=\"deepseek moe upper bound\">\n<p>DeepSeekMoEDeepSeekMoE-13B,\n1.21.5GShardDeepSeekMoE-13Bmatch</p>\n<img src=\"/44e38c1b/ds_moe_upper_bound_13b.png\" class title=\"deepseek moe upper bound\">\n<ol start=\"2\" type=\"1\">\n<li>DeepSeekMoE</li>\n</ol>\n<p>DeepSeekMoEshared expertfine-grained\nexpertexpert</p>\n<img src=\"/44e38c1b/ds_moe_ablation.png\" class title=\"deepseek moe upper bound \">\n<p>1</p>\n<p>2</p>\n<p>364expert1/2/4pileloss1.808,1.806,1.8111:32+6</p>\n<ol start=\"3\" type=\"1\">\n<li>expert specialization</li>\n</ol>\n<p>DeepSeekMoEexpert specialization</p>\n<p>1DeepSeekMoE-2B1.5GShardtop</p>\n<p></p>\n<img src=\"/44e38c1b/ds_moe_expert_specialization.png\" class title=\"\">\n<p>DeepSeekMoEDeepSeekMoE</p>\n<p>2DeepSeekMoEloss</p>\n<p>3GShardDeepSeekMoE</p>\n<img src=\"/44e38c1b/ds_moe_less_activated_expert.png\" class title=\"\">\n<p>132b2+6GShardDeepSeekMoE</p>\n<img src=\"/44e38c1b/ds_2b_less_expert.png\" class title=\"2B\">\n<ol type=\"1\">\n<li>DeepSeekMoE-16B</li>\n</ol>\n<p>DeepSeekMoE-16B2TLLAMA2-7B100k</p>\n<img src=\"/44e38c1b/ds_model_param.png\" class title=\"\">\n<p>MoE</p>\n<p>MoEloss</p>\n<p>DeepSeekMoE-16B6426gating\nfunctiontoken8token16.4B2.8B</p>\n<p>dimension</p>\n<p><br>\n- lr = 4.2e-4<br>\n- 80%90%lr0.316<br>\n- batch size = 4.5k4kbatch18M\ntoken2T10.6w<br>\n- pipeline parallelism</p>\n<p>expert level balance\nloss0.001</p>\n<p>DeepSeekMoE-16BDeepSeek-7B</p>\n<img src=\"/44e38c1b/ds_16b_perf_1.png\" class title=\"DeepSeek-7B\">\n<p>DeepSeekMoE-16BLLAMA2-7B</p>\n<img src=\"/44e38c1b/ds_16b_perf_2.png\" class title=\"LLAMA2-7B\">\n<ol start=\"5\" type=\"1\">\n<li>DeepSeekMoE-145B</li>\n</ol>\n<p>245BtokenDeepSeekMoE-145BDeepSeek-67B</p>\n<img src=\"/44e38c1b/ds_moe_145b.png\" class title=\"145b\">\n<h1 id=\"dbrx\">DBRX</h1>\n<p>2024327DatabricksDBRX132B36BMoE</p>\n<p>DBRXRoPEGLUGQAfine-grained\nexpert16token4MixtralGrok-182DBRX</p>\n<p>DBRX32k12TtokenDBRX3072H100post-trainingred-team3</p>\n<p>DBRXGPT-3.5Gemini 1.0\nProCodeLLaMA-70B</p>\n<img src=\"/44e38c1b/dbrx_perf.png\" class title=\"DBRX\">\n<p>DBRX</p>\n<img src=\"/44e38c1b/dbrx_infer_efficiency.png\" class title=\"\">\n<h1 id=\"qwen1.5-moe\">Qwen1.5-MoE</h1>\n<p>2024328Qwen1.5-MoE-A2.7B2.7BQwen1.5-7B</p>\n<p>Qwen1.5-MoE-A2.7BDeepSeekMoEDBRXfine-grained\nexpert64token84</p>\n<p>Qwen1.5-MoE-A2.7BQwen-1.8B</p>\n<p>Qwen1.5-MoE-A2.7B</p>\n<img src=\"/44e38c1b/qwen1.5_moe_perf.png\" class title=\"Qwen1.5-MoE-A2.7B\">\n<p>Qwen1.5-MoE-A2.7Bnon-embedding7B</p>\n<img src=\"/44e38c1b/qwen1.5_moe_params.png\" class title=\"Qwen1.5-MoE-A2.7B\">\n<p>Qwen1.5-MoE-A2.7BQwen1.5-7B75%</p>\n<p>A100-80GvLLMQwen1.5-7BQwen1.5-MoE-A2.7B</p>\n<p>/token1000token1000TPSthroughput</p>\n<img src=\"/44e38c1b/qwen1.5_moe_tps.png\" class title=\"Qwen1.5-MoE-A2.7B TPS\">\n<p>MoEdenseQwen1.5-MoE-A2.7BQwen1.5-7B1.74</p>\n<h1 id=\"mistral\">Mistral</h1>\n<h2 id=\"mistral-8x7b\">Mistral 8x7B</h2>\n<p>20231211Mistral\nAIMistral-8x7Btoken82</p>\n<p>Mistral-8x7B32kLLAM2-70BGPT-3.5</p>\n<img src=\"/44e38c1b/mistral_8_7b_perf.png\" class title=\"Mistral 8x7B\">\n<p>Mistral-8x7BLLAM2-70B6</p>\n<p>LLAM2-13B</p>\n<img src=\"/44e38c1b/mistral_8_7b_active_perf.png\" class title=\"Mistral 8x7B\">\n<h2 id=\"mistral-8x22b\">Mistral 8x22B</h2>\n<p>2024417Mistral\nAIMistral-8x22B141B39BMoE</p>\n<p>Mistral-8x22BMistral-8x7B32k64kMistral-8x22Bfunction\ncall</p>\n<p></p>\n<img src=\"/44e38c1b/mistral_8_22b_reasoning.png\" class title=\"Mistral 8x22B reasoning\">\n<img src=\"/44e38c1b/mistral_8_22b_multiling.png\" class title=\"Mistral 8x22B \">\n<img src=\"/44e38c1b/mistral_8_22b_code.png\" class title=\"Mistral 8x22B \">\n<h1 id=\"\"></h1>\n<ul>\n<li>MoEdenseMoE<br>\n</li>\n<li>MoEMoE<br>\n</li>\n<li>denseMoE<br>\n</li>\n<li><br>\n</li>\n<li>GShardSwitch Transformer<br>\n</li>\n<li>MoEMoE</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Adaptive Mixtures of Local Experts\nhttps://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf<br>\n2Outrageously Large Neural Networks: The Sparsely-Gated\nMixture-of-Experts Layer https://arxiv.org/abs/1701.06538<br>\n3GShard: Scaling Giant Models with Conditional Computation and\nAutomatic Sharding https://arxiv.org/abs/2006.16668<br>\n4Switch Transformers: Scaling to Trillion Parameter Models with\nSimple and Efficient Sparsity https://arxiv.org/abs/2101.03961<br>\n5GLaM: Efficient Scaling of Language Models with Mixture-of-Experts\nhttps://arxiv.org/abs/2112.06905<br>\n6ST-MoE: Designing Stable and Transferable Sparse Expert Models\nhttps://arxiv.org/abs/2202.08906<br>\n7DeepSeekMoE: Towards Ultimate Expert Specialization in\nMixture-of-Experts Language Models\nhttps://arxiv.org/abs/2401.06066<br>\n8Introducing DBRX: A New State-of-the-Art Open LLM\nhttps://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm<br>\n9Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated\nParameters https://qwenlm.github.io/zh/blog/qwen-moe/</p>\n","length":33074,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>202434MoEQwen1.5-MoEDBRXJambaMistral</p>\n<p>MoE</p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">GPT4</td>\n<td style=\"text-align: center;\">20233</td>\n<td style=\"text-align: center;\">236George\nHotzGPT48220B</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Mistral-87B</td>\n<td style=\"text-align: center;\">202312</td>\n<td style=\"text-align: center;\">Mistral AI</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">LLAMA-MoE</td>\n<td style=\"text-align: center;\">202312</td>\n<td style=\"text-align: center;\">github</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">DeepSeek-MoE</td>\n<td style=\"text-align: center;\">20241</td>\n<td style=\"text-align: center;\">MoE</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">abab6</td>\n<td style=\"text-align: center;\">20241</td>\n<td style=\"text-align: center;\">MiniMaxMoE</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">2.0</td>\n<td style=\"text-align: center;\">20242</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Step-2</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">MM1</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">MoE</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Grok-1</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">X</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Qwen1.5-MoE-A2.7B</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">DBRX</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">Databricks</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Jamba</td>\n<td style=\"text-align: center;\">20243</td>\n<td style=\"text-align: center;\">AI21</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">Mistral-822B</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\">Mistral AI</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">WizardLM-2-822B</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">3.0</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\">400BMoE</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">Arctic</td>\n<td style=\"text-align: center;\">20244</td>\n<td style=\"text-align: center;\">Snowflake480BDense-MoE\nHybrid</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p>MoEMoE</p>\n<img src=\"/44e38c1b/xiaomi_moe.jpg\" class title=\"MoE\">\n<p>MoE</p>\n<p>MoEMoEMoE</p>\n<p>20244DeepSeek-MoEQwen1.5-MoE</p>\n<h1 id=\"\"></h1>\n<p>MoE</p>\n<p>MoEGoogle</p>\n<h2 id=\"\"></h2>\n<p>MoE1991<a href=\"https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf\">Adaptive\nMixtures of Local Experts</a>Geoffrey HintonMichael I.\nJordanMoE1988</p>\n<blockquote>\n<p>This idea was first presented by Jacobs and Hinton at the\nConnectionist Summer School in Pittsburg in 1988.</p>\n</blockquote>\n<p>MoEMoE</p>\n<h2 id=\"rnn\">RNN</h2>\n<p>Google20171<a href=\"https://arxiv.org/abs/1701.06538\">Outrageously Large Neural\nNetworks: The Sparsely-Gated Mixture-of-Experts\nLayer</a>MoELSTM137B128kLSTM</p>\n<h2 id=\"transformer\">Transformer</h2>\n<ol type=\"1\">\n<li><p>20206Google<a href=\"https://arxiv.org/abs/2006.16668\">GShard: Scaling Giant Models\nwith Conditional Computation and Automatic\nSharding</a>MoEencoder-decodertransformerFFNMoE12.5B600BMoE2048</p></li>\n<li><p>20211Google<a href=\"https://arxiv.org/abs/2101.03961\">Switch Transformers: Scaling\nto Trillion Parameter Models with Simple and Efficient Sparsity</a>\nT5encoder-decoderFFNMoErouting1.6Tswitch\ntransformerSwitch\nTransformersscaling</p></li>\n<li><p>20222Google<a href=\"https://arxiv.org/abs/2202.08906\">ST-MoE: Designing Stable and\nTransferable Sparse Expert\nModels</a>encoder-decoderMoE269B32BST-MoEMoESwitch\nTransformer</p></li>\n</ol>\n<h2 id=\"gpt\">GPT</h2>\n<ol type=\"1\">\n<li><p>202112GoogleGLaM<a href=\"https://arxiv.org/abs/2112.06905\">GLaM: Efficient Scaling of\nLanguage Models with\nMixture-of-Experts</a>1.2Tdecoder-onlyencoder-decoderdecoder-onlyGoogle</p></li>\n<li><p>20241<a href=\"https://arxiv.org/abs/2401.06066\">DeepSeekMoE: Towards Ultimate\nExpert Specialization in Mixture-of-Experts Language\nModels</a>2312DeepSeekMoE</p></li>\n<li><p>2024DatabricksDBRXQwen1.5-MoE-A2.7BMistral\nAIMistral-8x22B</p></li>\n</ol>\n<h1 id=\"\"></h1>\n<p>Geoffrey HintonMichael I. Jordan<a href=\"https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf\">Adaptive\nMixtures of Local Experts</a>MoE</p>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<p></p>\n<p>MoEexpert</p>\n<p>MoEvowel discrimination\ntaskMoEaeiou</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p>MoEexpert networkgating\nnetworkexpertgating\nnetworkexpertexpertstochastictruefalse</p>\n<img src=\"/44e38c1b/vanilla_moe.png\" class title=\"Vanilla MoE\">\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>MoEideaJacobsHinton1988lossensembleexpertexpertexpertresidual</p>\n<p>case <span class=\"math inline\">\\(c\\)</span>\n<span class=\"math inline\">\\(d^c\\)</span> ground truth <span class=\"math inline\">\\(i\\)</span> expert <span class=\"math inline\">\\(o_{i}^c\\)</span><span class=\"math inline\">\\(p_{i}^c\\)</span> gating network <span class=\"math inline\">\\(i\\)</span>\nexpert <span class=\"math inline\">\\(E^{c}\\)</span> </p>\n<p><span class=\"math display\">\\[E^{c}=\\left|\\left|d^{c}-\\sum_{i}p_{i}^{c}o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p>expert</p>\n<p>expertexpertexpert</p>\n<p>expertexpertexpertexpertgating\nnetwork</p>\n<p></p>\n<p>HintonJordanlossexpert</p>\n<p>gating networkexpert</p>\n<p><span class=\"math display\">\\[E^{c}=\\langle\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2\\rangle=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p>expertexpertexpert</p>\n<p>losslocalizationcasegating\nnetworkexpertgating\nnetworkexpert</p>\n<p>localizationexpert</p>\n<p>expertexpertgating\nnetworkexpert\nerror+-</p>\n<p>expert0</p>\n<ol start=\"4\" type=\"1\">\n<li></li>\n</ol>\n<p>losslossloss</p>\n<p><span class=\"math display\">\\[\\text{loss}E^{c}=\\sum_{i}p_{i}^{c}\\left|\\left|d^{c}-o_{i}^{c}\\right|\\right|^{2}\\]</span></p>\n<p><span class=\"math display\">\\[\\text{loss}E^c=-log\\sum_ip_i^ce^{-\\frac12\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}\\]</span></p>\n<p>lossloss</p>\n<p><span class=\"math display\">\\[\\text{loss}\\frac{\\partial\nE^c}{\\partial\\mathbf{o}_i^c}=-2p_i^c(\\mathbf{d}^c-\\mathbf{o}_i^c)\\]</span></p>\n<p><span class=\"math display\">\\[\\text{loss}\\frac{\\partial\nE^c}{\\partial\\mathbf{o}_i^c}=-\\left[\\frac{p_i^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_i^c\\|^2}}{\\sum_jp_j^ce^{-\\frac{1}{2}\\|\\mathbf{d}^c-\\mathbf{o}_j^c\\|^2}}\\right](\\mathbf{d}^c-\\mathbf{o}_i^c)\\]</span></p>\n<p>lossloss <span class=\"math inline\">\\(i\\)</span>\nexpertexpertexpert <span class=\"math inline\">\\(i\\)</span>\ncasegating\nnetworklosscaseexpertlosslossexpertlocalizationexpert</p>\n<p>BTWloss</p>\n<p>MoE</p>\n<h1 id=\"lstm-moe\">LSTM MoE</h1>\n<p>Google20171 <a href=\"https://arxiv.org/abs/1701.06538\">OUTRAGEOUSLY LARGE NEURAL\nNETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS\nLAYER</a>MoELSTM137BLSTM7</p>\n<p>1991</p>\n<h2 id=\"\"></h2>\n<p>Transformer</p>\n<p>conditional\ncomputationconditional\ncomputationMoE</p>\n<p></p>\n<ul>\n<li>MoEexpertbatch sizebatch\nsize<br>\nbatch\nsize3216expertexpert2batch\nsizebatch\nsizebatch\nsize<br>\n</li>\n<li><br>\nNLP<br>\n</li>\n<li><br>\n<br>\n</li>\n<li><br>\nGPU<br>\n</li>\n<li>GPU<br>\nGPUbranchingif/elseMoEgating\nnetwork</li>\n</ul>\n<p></p>\n<h2 id=\"\"></h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<p>LSTMMoEembedding</p>\n<img src=\"/44e38c1b/rnn_moe.png\" class title=\"LSTM MoE\">\n<p>expertfeed-forward neural\nnetworknexpertgating networkn</p>\n<p><span class=\"math display\">\\[\\begin{aligned}y=\\sum_{i=1}^nG(x)_iE_i(x)\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(E_{i}(x)\\)</span>  <span class=\"math inline\">\\(i\\)</span> expert<span class=\"math inline\">\\(G(x)_{i}\\)</span> gating network <span class=\"math inline\">\\(i\\)</span> expert</p>\n<p> <span class=\"math inline\">\\(G(x)_{i}\\)</span>\n0expert</p>\n<p>experttwo-level hierarchical\nMoEgating networkgating\nnetworkexpertgating\nnetworkexpertword2vechierarchical\nsoftmax</p>\n<ol start=\"2\" type=\"1\">\n<li>gating network</li>\n</ol>\n<p>gating network</p>\n<p>softmaxgating\nfunction</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G_\\sigma(x)=Softmax(x\\cdot\nW_g)\\end{aligned}\\]</span></p>\n<p>topkksoftmax0expert</p>\n<p>sparsitytopkgating\nfunction</p>\n<p>Gaussian\nnoisenoise</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[KeepTopK(v,k)_i=\\begin{cases}v_i&amp;\\text{if\n}v_i\\text{ is in the top }k\\text{ elements of\n}v.\\\\-\\infty&amp;\\text{otherwise.}\\end{cases}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}H(x)_i=(x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\end{aligned}\\]</span></p>\n<p>noisesoftplusReLU</p>\n<img src=\"/44e38c1b/softplus.png\" class title=\"softplus\">\n<p></p>\n<h2 id=\"\"></h2>\n<p>MoEgating\nnetworkexpertexpert</p>\n<p></p>\n<p>hard\nconstraintexperthard\nconstraintsoft\nconstraint</p>\n<p>expert</p>\n<p><span class=\"math display\">\\[Importance(X)=\\sum_{x\\in\nX}G(x)\\]</span></p>\n<p><span class=\"math inline\">\\(G(x)\\)</span> gating\nnetworkexpert</p>\n<p> <span class=\"math inline\">\\(L_{importance}\\)</span><span class=\"math inline\">\\(L_{importance}\\)</span> </p>\n<p><span class=\"math display\">\\[L_{importance}(X)=w_{importance}\\cdot\nCV(Importance(X))^2\\]</span></p>\n<p> <span class=\"math inline\">\\(w_{importance}\\)</span>\n0.1CVcoefficient of variation</p>\n<p>coefficient of\nvariation\n<span class=\"math inline\">\\(\\sigma\\)</span>   <span class=\"math inline\">\\(\\mu\\)</span> </p>\n<p>MoEexpertexpertgating\n<span class=\"math inline\">\\(L_{importance}\\)</span> </p>\n<p> <span class=\"math inline\">\\(L_{importance}\\)</span>  <span class=\"math inline\">\\(L_{importance}\\)</span>\n <span class=\"math inline\">\\(L_{importance}\\)</span>\n</p>\n<p>expertexpertgating</p>\n<p> <span class=\"math inline\">\\(L_{load}\\)</span>\nexpert</p>\n<p>expertback\npropagation <span class=\"math inline\">\\(L_{load}\\)</span>\nexpert</p>\n<p>MoE <span class=\"math inline\">\\(H(x)\\)</span> KeepTopK</p>\n<p><span class=\"math display\">\\[\\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}H(x)_i=(x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(kth\\_excluding(H(x),k,i)\\)</span>\n<span class=\"math inline\">\\(H(x)\\)</span>  <span class=\"math inline\">\\(i\\)</span>  <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(P(x,i)\\)</span>\nnoise <span class=\"math inline\">\\(i\\)</span> noise <span class=\"math inline\">\\(kth\\_excluding(H(x),k,i)\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{aligned}P(x,i)=Pr\\Big((x\\cdot\nW_g)_i+StandardNormal()\\cdot Softplus((x\\cdot\nW_{noise})_i)\\\\&gt;kth\\_excluding(H(x),k,i)\\Big)\\end{aligned}\\]</span></p>\n<p>noise <span class=\"math inline\">\\(i\\)</span>\n <span class=\"math inline\">\\(i\\)</span>\n<span class=\"math inline\">\\(P(x,i)\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\begin{aligned}P(x,i)&amp;=\\Phi\\Big(\\frac{(x\\cdot\nW_g)_i-kth\\_excluding(H(x),k,i)}{Softplus((x\\cdot\nW_{noise})_i)}\\Big)\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\Phi\\)</span>\nCDF</p>\n<p> <span class=\"math inline\">\\(i\\)</span>\nexpert</p>\n<p><span class=\"math display\">\\[\\begin{aligned}Load(X)_i=\\sum_{x\\in\nX}P(x,i)\\end{aligned}\\]</span></p>\n<p>expert</p>\n<p><span class=\"math display\">\\[L_{load}(X)=w_{load}\\cdot\nCV(Load(X))^2\\]</span></p>\n<p><span class=\"math inline\">\\(w_{load}\\)</span>\n0.1</p>\n<p> <span class=\"math inline\">\\(L_{importance}(X)\\)</span><span class=\"math inline\">\\(Load(X)\\)</span>\n</p>\n<p>expert\n<span class=\"math inline\">\\(W_g\\)</span>  <span class=\"math inline\">\\(W_{noise}\\)</span>\n0</p>\n<p></p>\n<img src=\"/44e38c1b/rnn_moe_load_function.png\" class title=\"\">\n<p></p>\n<h2 id=\"\"></h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<p>1batch size</p>\n<p>expertbatch\nsizenexpertkbatch\nsizebexpertbatch\nsizekb/nexpertbatch size<br>\n-\nbatchbatchMoEexpertdexpertkbd/nbatch\nsize - LSTMbatch\nsize</p>\n<p>2</p>\n<p></p>\n<p>expertinputoutput[input_size,\nhidden_size][hidden_size,\noutput_size]GPU1000hidden_sizeexpert1000</p>\n<ol start=\"2\" type=\"1\">\n<li> &amp; </li>\n</ol>\n<p>MoEdense4/32/256expertflat\nMoE256/1024/4096experthierarchical\nMoEexpert1Mflat4experthierarchical\nMoEgating2</p>\n<p>ppldenseMoEMoE</p>\n<img src=\"/44e38c1b/rnn_moe_perf.png\" class title=\"\">\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>4Bdiminishing\nreturns</p>\n<p> + 100B\ntoken32, 256, 10244096, 16384, 65536,\n131072expertMoE137B</p>\n<p></p>\n<img src=\"/44e38c1b/rnn_moe_137b.png\" class title=\"137\">\n<p></p>\n<ol start=\"4\" type=\"1\">\n<li>Expert Specialization</li>\n</ol>\n<p>MoE</p>\n<p>tokenspecialization</p>\n<img src=\"/44e38c1b/rnn_moe_specilized.png\" class title=\"RNN MoE \">\n<h1 id=\"gshard\">GShard</h1>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>2018Berttransformer20206GoogleGShard:\nScaling Giant Models with Conditional Computation and Automatic\nShardingMoEencoder-decodertransformerMoE</p>\n<p>GShardMoE600B</p>\n<img src=\"/44e38c1b/gshard_moe_family.png\" class title=\"GShard MoE family\">\n<p>expertLSMT MoE --\nexpert24expertChatGPTBertGPT</p>\n<p>GShardMoE</p>\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<p>Googleencoder-decoder\ntransfomerGShardencoder-decoder\ntransfomer</p>\n<p>GShardencoderdecoderFFNMoENN/2MoE</p>\n<img src=\"/44e38c1b/gshard_model.png\" class title=\"GShard\">\n<p>top-2 expert</p>\n<p>GShardLSTM MoEgating\nfunctionauxiliary loss function</p>\n<p>MoE</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{G}_{s,E}&amp; =\\mathrm{GATE}(x_s)  \\\\\n\\mathrm{FFN}_e(x_s)&amp; =wo_e\\cdot\\text{ReLU}(wi_e\\cdot x_s)  \\\\\ny_{s}&amp; =\\sum_{e=1}^E\\mathcal{G}_{s,e}\\cdot\\mathrm{FFN}_e(x_s)\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(x_s\\)</span> MoEtoken<span class=\"math inline\">\\(w_i\\)</span>  <span class=\"math inline\">\\(w_o\\)</span>\n<span class=\"math inline\">\\(\\mathcal{G}_{s}\\)</span> gating\nfunction</p>\n<p>GShardgating\nfunction12</p>\n<p>NtokenEexpertNEgating\nfunction</p>\n<p>gating function</p>\n<p>1 expert capacity</p>\n<p>experttokenexperttoken2N/E</p>\n<p>expert capacityGATE()expert <span class=\"math inline\">\\(c_e\\)</span>\ntokentokenexpert</p>\n<p>2 Local group dispatching</p>\n<p>tokenG2N/EG</p>\n<p>batchbatchbatchgroupall2allgroup</p>\n<p>groupgradient\naccumulation</p>\n<p>3 Auxiliary loss</p>\n<p>gatingLSTM\nMoE</p>\n<p><span class=\"math display\">\\[\\ell_{aux}=\\frac1E\\sum_{e=1}^E\\frac{c_e}S\\cdot\nm_e\\]</span></p>\n<p><span class=\"math inline\">\\(S\\)</span> token<span class=\"math inline\">\\(E\\)</span> <span class=\"math inline\">\\(c_e\\)</span>  <span class=\"math inline\">\\(e\\)</span> token<span class=\"math inline\">\\(m_e\\)</span>  <span class=\"math inline\">\\(e\\)</span> expert <span class=\"math inline\">\\(S\\)</span> token</p>\n<p> <span class=\"math inline\">\\(\\frac{c_e}S\\)</span>\n <span class=\"math inline\">\\(\\frac{c_e}S\\)</span>  <span class=\"math inline\">\\(m_e\\)</span>  <span class=\"math inline\">\\(m_e\\)</span>  <span class=\"math inline\">\\(e\\)</span> expert <span class=\"math inline\">\\(S\\)</span>\ntokenloss</p>\n<p>loss</p>\n<p>gating</p>\n<img src=\"/44e38c1b/gshard_algo_1.png\" class title=\"GShard gating \">\n<p>4 Random routing</p>\n<p>top-2\nexperttop-1</p>\n<p>top-1g2</p>\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<img src=\"/44e38c1b/gshard_perf.png\" class title=\"GShard\">\n<h1 id=\"switch-transformer\">Switch Transformer</h1>\n<p>20224ChatGPTGoogleSwitch\nTransformers: Scaling to Trillion Parameter Models with Simple and\nEfficient Sparsity2021GoogleSwitch\nTransformer</p>\n<p>Switch\nTransformerGShardencoder-decoderT51.6T2048expert</p>\n<p>Switch\nTransformer</p>\n<p>Switch\nTransformerSwitch\nTransformerFLOPS/token</p>\n<p>Switch Transformer</p>\n<p>1TransformerMoESwitch\nTransformer</p>\n<p>2MoE to\ndenseMoEdenseMoE99%dense</p>\n<p>3<br>\n- bf16MoE<br>\n- MoE<br>\n- </p>\n<p>41TMoE</p>\n<p>5101</p>\n<p>6FLOPS/tokenSwitch\nTransformer</p>\n<h2 id=\"-1\"></h2>\n<p>Switch\nTransformerGShardtransformerFFNMoE</p>\n<img src=\"/44e38c1b/switch_transformer_structure.png\" class title=\"Switch Transformer \">\n<p>Switch Transformergating\nfunctionSwitch Transformerrouting</p>\n<p>kexpertSwitch\nTransformergating1expertk=1MoESwitch\nlayer</p>\n<p>routingrouter</p>\n<h2 id=\"-1\"></h2>\n<p>GShardSwitch Transformerexpert\ncapacityexpertbatchtoken</p>\n<p>tokenexpertoverflowtokenGShard</p>\n<p>Switch Transformercapacity factor</p>\n<p><span class=\"math display\">\\[\\text{expert\ncapacity}=\\left(\\frac{\\text{tokens per batch}}{\\text{number of\nexperts}}\\right)\\times\\text{capacity factor}.\\]</span></p>\n<p>capacity\nfactorexperttokenoverflow</p>\n<p>capacity factor</p>\n<img src=\"/44e38c1b/switch_transformer_diff_expert_capacity.png\" class title=\"expert capacity\">\n<p>expert capacity</p>\n<p>capacity\nfactor1overflowcapacity\nfactor</p>\n<p>expertoverflowMoESwitch\nTransformer128</p>\n<p>capacity\nfactoroverflow</p>\n<img src=\"/44e38c1b/switch_transformer_capacity_effect.png\" class title=\"expert capacity\">\n<p>tokenscalingoverflow</p>\n<p></p>\n<p> <span class=\"math inline\">\\(N\\)</span> expert <span class=\"math inline\">\\(T\\)</span> tokenbatch <span class=\"math inline\">\\(\\mathcal{B}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\text{loss}&amp;=\\alpha\\cdot\nN\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(f_{i}\\)</span>  <span class=\"math inline\">\\(i\\)</span> experttoken</p>\n<p><span class=\"math display\">\\[\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax\n}p(x)=i\\}\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(P_i\\)</span>\nbatchtoken<span class=\"math inline\">\\(i\\)</span>\nexpert</p>\n<p><span class=\"math display\">\\[\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}\\]</span></p>\n<p>GShard</p>\n<p><span class=\"math inline\">\\(f\\)</span> \n<span class=\"math inline\">\\(P\\)</span>  <span class=\"math inline\">\\(1/N\\)</span></p>\n<p><span class=\"math inline\">\\(\\alpha\\)</span>\n1e-51e-11e-2</p>\n<p> <span class=\"math inline\">\\(\\sum_{i=1}^N(f_i\\cdot\nP_i)=\\sum_{i=1}^N(\\frac1N\\cdot\\frac1N)=\\frac1N\\)</span>loss\n<span class=\"math inline\">\\(N\\)</span>expertloss</p>\n<h2 id=\"-1\"></h2>\n<ol type=\"1\">\n<li>trick</li>\n</ol>\n<p>1bf16</p>\n<p>bf16routing\nfunction</p>\n<p>routingsoftmaxexponentialrounding\nerrorrouting</p>\n<p>2</p>\n<p> <span class=\"math inline\">\\(\\mu=0\\)</span><span class=\"math inline\">\\(\\sigma=\\sqrt{s}/n\\)</span>sne.g.\nfan-in</p>\n<p>Transformers=1.010</p>\n<img src=\"/44e38c1b/switch_transformer_init.png\" class title=\"\">\n<p>3dropout</p>\n<p>Switch\nTransformerdropout</p>\n<img src=\"/44e38c1b/switch_transformer_dropout.png\" class title=\"dropout\">\n<p>dropoutdense0.1expertdropout</p>\n<ol start=\"2\" type=\"1\">\n<li>scaling</li>\n</ol>\n<p>Switch Transformerscaling</p>\n<p>1Step-Basis</p>\n<p>stepexpert</p>\n<p>stepstep</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_step.png\" class title=\"step scaling\">\n<p>2Time-Basis</p>\n<p>Switch\nTransformerstepSwitch\nTransformerdense</p>\n<p>Switch\nTransformerdenseSwitch\nTransformerdensedense1/7</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_time.png\" class title=\"time scaling\">\n<p>3dense</p>\n<p>Switch\nTransformerdenseSwitch\nTransformerdense</p>\n<p>Step-BasisTime-Basis64Switch\nTransformerT5-LargestepSwitch\nTransformer</p>\n<img src=\"/44e38c1b/switch_transformer_scaling_dense.png\" class title=\"dense\">\n<ol start=\"3\" type=\"1\">\n<li>SFT</li>\n</ol>\n<p>GLUESuperGLUEdense</p>\n<p>eval</p>\n<img src=\"/44e38c1b/switch_transformer_sft_result.png\" class title=\"sft\">\n<ol start=\"4\" type=\"1\">\n<li></li>\n</ol>\n<p>Switch\nTransformerBTdense</p>\n<p><br>\n- Switch\nTransformerdense<br>\n- label25%75%ground truth</p>\n<p>densedensedenseSwitch\nTransformer30%</p>\n<img src=\"/44e38c1b/switch_transformer_distill.png\" class title=\"\">\n<p>99%</p>\n<img src=\"/44e38c1b/switch_transformer_distill_diff_model.png\" class title=\"\">\n<p>SuperGLUE</p>\n<img src=\"/44e38c1b/switch_transformer_distill_sft.png\" class title=\"sft\">\n<h1 id=\"glam\">GLaM</h1>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>202112GoogleGLaM: Efficient Scaling of Language Models\nwith\nMixture-of-Experts1.2T64token96.6BMoE</p>\n<p>Switch TransformerGLaM1.6T\ntoken</p>\n<p></p>\n<img src=\"/44e38c1b/glam_related_model.png\" class title=\"glam\">\n<p>GPT-3175BGPT-3NLPGPT-3</p>\n<img src=\"/44e38c1b/glam_compare_gpt3.png\" class title=\"glamgpt3\">\n<img src=\"/44e38c1b/glam_compare_gpt3_2.png\" class title=\"glamgpt3\">\n<ol start=\"2\" type=\"1\">\n<li></li>\n</ol>\n<p>Switch\nTransformerFFNMoESwitch\nTransformerGLaMexpert</p>\n<img src=\"/44e38c1b/glam_model.png\" class title=\"glam\">\n<p></p>\n<p>1</p>\n<p>XLNET</p>\n<p>2</p>\n<blockquote>\n<p>In the non-MoE Transformer feed-forward sub-layers, we replace the\nfirst linear projection and the activation function with the Gated\nLinear Unitwhich computes the component-wise product of two linear\ntransformation of the input, followed by a Gaussian Error Linear\nUnit.</p>\n</blockquote>\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>trick</p>\n<p>1Lingvo: a modular and scalable framework for\nsequence-to-sequence\nmodelingNaNInf</p>\n<p>2BPNaNInfcheckpointNaNInf</p>\n<p>MoE</p>\n<img src=\"/44e38c1b/glam_family.png\" class title=\"glam\">\n<p>GLaMdense</p>\n<img src=\"/44e38c1b/glam_perf.png\" class title=\"glam\">\n<p>GLaM MoEdense</p>\n<h1 id=\"st-moe\">ST-MoE</h1>\n<p>20222GoogleST-MOE: DESIGNING STABLE AND TRANSFERABLE\nSPARSE EXPERT\nMODELSST-MoEMoEMoE</p>\n<p>ST-MoE269B32B\ndenseStable Transferable\nMixture-of-ExpertsST-MoE-32B</p>\n<p>MoEST-MoESwitch\nTransformer1MoE</p>\n<p>ST-MoE4B269BST-MoE</p>\n<img src=\"/44e38c1b/st_moe_models.png\" class title=\"ST-MoE\">\n<h2 id=\"\"></h2>\n<p></p>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p></p>\n<p></p>\n<blockquote>\n<p>Some architectural improvements involve more multiplications than\nadditions or do not sum many items at once</p>\n</blockquote>\n<p>1GELU Gated Linear Units (GEGLU)</p>\n<p>GLUcomponent-wiseGELU-Linear\nFFNtransformerReLU FFN</p>\n<p><span class=\"math display\">\\[\\begin{aligned}FFN_{GEGLU}(x,W,V,b,c)=GELU(xW+b)\\odot(xV+c)\\end{aligned}\\]</span></p>\n<p></p>\n<p>2RMSNorm</p>\n<p>RMSNorm <span class=\"math inline\">\\(g\\)</span></p>\n<p><span class=\"math display\">\\[y_i=\\frac{x_i}{\\sqrt{\\frac1d\\sum_{i=1}^dx_i^2}}\\cdot\ng_i\\]</span></p>\n<p>ST-MoEGEGLURMSNorm</p>\n<img src=\"/44e38c1b/st_moe_remove_multiplications.png\" class title=\"\">\n<p></p>\n<p>3dense</p>\n<p>ST-MoEexpertdensedense</p>\n<img src=\"/44e38c1b/st_moe_more_dense_layer.png\" class title=\"dense\">\n<p>4bias</p>\n<p>FFNbias\nB</p>\n<p><span class=\"math display\">\\[\\text{FFN}_{\\text{GEGLU}}+\\text{Add\nBias}(x)=[(\\text{GELU}(xW_{11})\\odot xW_{12})+B]W_2\\]</span></p>\n<p><span class=\"math display\">\\[\\mathrm{FFN}_{\\mathrm{GEGLU}}+\\mathrm{Mult~Bias}(x)=[(\\mathrm{GELU}(xW_{11})\\odot\nxW_{12})\\odot B]W_2\\]</span></p>\n<p></p>\n<p></p>\n<ol start=\"2\" type=\"1\">\n<li>noise</li>\n</ol>\n<p>ST-MoE</p>\n<p>input-jitterrouterlogits[1e-2,\n1e2]</p>\n<img src=\"/44e38c1b/st_moe_more_add_noise.png\" class title=\"noise\">\n<p>noise</p>\n<p></p>\n<ol start=\"3\" type=\"1\">\n<li></li>\n</ol>\n<p>activationgradient</p>\n<p>ST-MoE269B</p>\n<p>ST-MoErouter z-loss</p>\n<p><span class=\"math display\">\\[L_z(x)=\\frac{1}{B}\\sum_{i=1}^B\\left(\\log\\sum_{j=1}^Ne^{x_j^{(i)}}\\right)^2\\]</span></p>\n<p><span class=\"math inline\">\\(B\\)</span> token<span class=\"math inline\">\\(N\\)</span> <span class=\"math inline\">\\(x\\in\\mathcal{R}^{B\\times N}\\)</span>\nrouter</p>\n<p>z-lossrouterlogitsz-loss</p>\n<img src=\"/44e38c1b/st_moe_z_loss_result.png\" class title=\"z-loss\">\n<p>ST-MoEz-loss</p>\n<p>z-loss <span class=\"math inline\">\\(c_z\\)</span>\n</p>\n<p><span class=\"math display\">\\[L_{tot}=L_{CE}+c_BL_B+c_zL_Z\\]</span></p>\n<p>ST-MoE<span class=\"math inline\">\\(c_z=0.001\\)</span></p>\n<p><span class=\"math inline\">\\(L_B\\)</span>  auxiliary load balance\nlossST-MoEGShard/Switch\nTransformer</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\text{loss}&amp;=\\alpha\\cdot\nN\\cdot\\sum_{i=1}^Nf_i\\cdot P_i\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}f_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbb{1}\\{\\text{argmax\n}p(x)=i\\}\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}P_i=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_i(x).\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(N\\)</span>  <span class=\"math inline\">\\(\\mathcal{B}\\)</span> <span class=\"math inline\">\\(T\\)</span> tokenbatch<span class=\"math inline\">\\(f_{i}\\)</span>  <span class=\"math inline\">\\(i\\)</span> experttoken<span class=\"math inline\">\\(P_i\\)</span> batchtoken<span class=\"math inline\">\\(i\\)</span> expert</p>\n<ol start=\"4\" type=\"1\">\n<li></li>\n</ol>\n<p>float32bfloat16bfloat16allreducebfloat16float32</p>\n<p>ST-MoE-32BallreduceST-MoEallreducefloat32</p>\n<p>bfloat16float32</p>\n<img src=\"/44e38c1b/st_moe_round_error.png\" class title=\"bf16\">\n<p>z-loss</p>\n<p>MoErouter</p>\n<p>ST-MoE1/5token</p>\n<p>softmaxMoE</p>\n<h2 id=\"-2\"></h2>\n<p>densescaling\nlawMoEdense</p>\n<p>1expert</p>\n<p>2routing</p>\n<p>3</p>\n<p>4</p>\n<p>MoEscaling lawUnified scaling laws for routed\nlanguage models</p>\n<ol type=\"1\">\n<li>expert</li>\n</ol>\n<p>ST-MoE8/16/32&lt;1%&gt;256</p>\n<p>&gt;1&lt;=1</p>\n<ol start=\"2\" type=\"1\">\n<li>routingcapacity factor</li>\n</ol>\n<p>capacity factor</p>\n<img src=\"/44e38c1b/st_moe_capacity_factor.png\" class title=\"capacity factor\">\n<p></p>\n<p>1capacity factor</p>\n<p>2capacity\nfacotr</p>\n<p>3expertcapacity\nfactor</p>\n<p>capacity\nfactorcapacity\nfactor</p>\n<p>capacity factor</p>\n<img src=\"/44e38c1b/st_moe_capacity_factor_speed.png\" class title=\"capacity factor\">\n<h2 id=\"-2\"></h2>\n<ol type=\"1\">\n<li>ST-MoE</li>\n</ol>\n<p>ST-MoE-32BST-MoE-32B</p>\n<img src=\"/44e38c1b/st_moe_perf.png\" class title=\"capacity ST-MoE-32B\">\n<ol start=\"2\" type=\"1\">\n<li>Expert Specialization</li>\n</ol>\n<p>decodertokenencoder</p>\n<img src=\"/44e38c1b/st_moe_encoder_specialization.png\" class title=\"encoder\">\n<p>encodertoken</p>\n<img src=\"/44e38c1b/st_moe_multiling_specialization.png\" class title=\"\">\n<h1 id=\"deepseekmoe\">DeepseekMoE</h1>\n<p>20241DeepseekMoEMoEDeepSeekMoE:\nTowards Ultimate Expert Specialization in Mixture-of-Experts Language\nModelsDeepSeekMoE</p>\n<p>DeepSeekMoEMoE2</p>\n<p>1expertexpert</p>\n<p>2expertexpertshared\nexpert</p>\n<p>expert(specialization)</p>\n<p>DeepSeekMoE2BMoE16BMoEDeepSeekMoE-16B40GB</p>\n<p>DeepSeekMoE-2B2BDeepSeekMoE-16B7B40%</p>\n<p>DeepSeekMoE-16B</p>\n<img src=\"/44e38c1b/ds_moe_perf.png\" class title=\"deepseek moe\">\n<p>DeepSeekMoE-2B16B</p>\n<p>DeepSeekMoE-145BMoEDeepSeek-67B</p>\n<h2 id=\"-3\"></h2>\n<p>MoEmixture of\nexpertmotivationexpert</p>\n<p>1991expert</p>\n<p>MoEknowledge hybridityknowledge\nredundancy</p>\n<p>1</p>\n<p>expertexpert</p>\n<p>2</p>\n<p>expertexpertexpertexpert8expertexpert</p>\n<p>(expert\nspecialization)MoE</p>\n<p>expertnon-overlap &amp; foucusd\nknowledge</p>\n<p>DeepSeekMoE2</p>\n<p>1Fine-Grained Expert Segmentation</p>\n<p>expertexpertexpertspecialization16expert2120expert1/464expert8\n<span class=\"math inline\">\\(\\binom{64}8=4,426,165,368\\)</span>\n</p>\n<p>2Shared Expert Isolation</p>\n<p>expertcommon\nknowledgeexpertexpertexpertexpert</p>\n<p>MoEFine-Grained Expert SegmentationShared\nExpert Isolation</p>\n<img src=\"/44e38c1b/ds_moe_structure.png\" class title=\"deepseek moe \">\n<p>expert isolation20221DeepSpeed-MoE:\nAdvancing Mixture-of-Experts Inference and Training to Power\nNext-Generation AI Scale</p>\n<p>MoEexpert <span class=\"math inline\">\\(N\\)</span>expert <span class=\"math inline\">\\(K\\)</span>DeepSeekMoEexpert\n<span class=\"math inline\">\\(1/m\\)</span>DeepSeekMoE <span class=\"math inline\">\\(mN\\)</span> expertexpert <span class=\"math inline\">\\(mK\\)</span> <span class=\"math inline\">\\(T\\)</span> <span class=\"math inline\">\\(L\\)</span> <span class=\"math inline\">\\(e_i^l\\)</span>  <span class=\"math inline\">\\(i\\)</span>\nexpertDeepSeekMoElayernorm</p>\n<p><span class=\"math display\">\\[\\mathbf{u}_{1:T}^l=\\text{Self-Att}\\left(\\mathbf{h}_{1:T}^{l-1}\\right)+\\mathbf{h}_{1:T}^{l-1}\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{h}_t^l=\\sum_{i=1}^{mN}\\left(g_{i,t}\\text{\nFFN}_i\\left(\\mathbf{u}_t^l\\right)\\right)+\\mathbf{u}_t^l\\]</span></p>\n<p><span class=\"math display\">\\[g_{i,t}=\\begin{cases}s_{i,t},&amp;s_{i,t}\\in\\text{Topk}(\\{s_{j,t}|1\\leqslant\nj\\leqslant mN\\},mK)\\\\0,&amp;\\text{otherwise,}\\end{cases}\\]</span></p>\n<p><span class=\"math display\">\\[s_{i,t}=\\mathrm{Softmax}_i\\left({\\mathbf{u}_t^l}^T\\mathbf{e}_i^l\\right)\\]</span></p>\n<h2 id=\"-2\"></h2>\n<p>MoEgating</p>\n<p>1routing\ncollapsegatingexpert</p>\n<p>2</p>\n<p>routing collapseDeepSeekMoEexpert-level balance\nloss</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}_{\\mathrm{ExpBal}}&amp; =\\alpha_1\\sum_{i=1}^{N&#39;}f_iP_i\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nf_{i}&amp;\n=\\frac{N^{\\prime}}{K^{\\prime}T}\\sum_{t=1}^T\\mathbb{1}(\\text{Token\n}t\\text{ selects Expert }i)\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nP_{i}&amp; =\\frac1T\\sum_{t=1}^Ts_{i,t}\n\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(\\alpha_1\\)</span> expert-level\nbalance factor</p>\n<p> <span class=\"math inline\">\\(f_i\\)</span>  <span class=\"math inline\">\\(P_i\\)</span> Switch\nTransformer</p>\n<p>Switch Transformer <span class=\"math inline\">\\(f_i\\)</span>\n <span class=\"math inline\">\\(i\\)</span>\nexperttokenDeepSeekMoE\n<span class=\"math inline\">\\(N&#39;/K&#39;\\)</span>  <span class=\"math inline\">\\(N&#39;=mN-K_s\\)</span><span class=\"math inline\">\\(K&#39;=mK-K_s\\)</span><span class=\"math inline\">\\(K_s\\)</span>\nexpertDeepSeekMoE\n<span class=\"math inline\">\\(f_i\\)</span> Switch\nTransformer</p>\n<p><span class=\"math inline\">\\(N&#39;/K&#39;\\)</span>\nexpertloss</p>\n<p><span class=\"math inline\">\\(P_i\\)</span> token\n<span class=\"math inline\">\\(i\\)</span> expertSwitch\nTransformer</p>\n<p> <span class=\"math inline\">\\(f_i\\)</span> <span class=\"math inline\">\\(P_i\\)</span> </p>\n<p>DeepSeekMoEdevice-level balance\nloss</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}_{\\mathrm{DevBal}}&amp; =\\alpha_2\\sum_{i=1}^Df_i&#39;P_i&#39;\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nf_i^{\\prime}&amp; =\\frac1{|\\mathcal{E}_i|}\\sum_{j\\in\\mathcal{E}_i}f_j\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nP_{i}^{\\prime}&amp; =\\sum_{j\\in\\mathcal{E}_i}P_j\n\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(\\alpha_2\\)</span> device-level\nbalance factor</p>\n<p><span class=\"math inline\">\\(\\mathcal{E}_i\\)</span>  <span class=\"math inline\">\\(i\\)</span> </p>\n<p>device-level balance lossexpert-level balance loss\n<span class=\"math inline\">\\(f_i\\)</span>  <span class=\"math inline\">\\(P_i\\)</span>\nexpert</p>\n<p>expert64expert8tokentokenexpert</p>\n<p>expert</p>\n<h2 id=\"-3\"></h2>\n<ol type=\"1\">\n<li></li>\n</ol>\n<p>100B\ntokenDeepSeekMoE-2BBPE8k</p>\n<p>DeepSeekMoE-2B0.006multi-head\nattention0.3B</p>\n<img src=\"/44e38c1b/ds_model_param.png\" class title=\"\">\n<p>relative expert\nsizeDeepSeekMoEexpertFFN</p>\n<p></p>\n<center>\n<table>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\"></th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">optimizer</td>\n<td style=\"text-align: center;\">AdamW</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">adam_beta_1</td>\n<td style=\"text-align: center;\">0.9</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">adam_beta_2</td>\n<td style=\"text-align: center;\">0.95</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">adam_weight_decay</td>\n<td style=\"text-align: center;\">0.1</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">warmup schedule</td>\n<td style=\"text-align: center;\">linear</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">warmup step</td>\n<td style=\"text-align: center;\">2000</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">max lr</td>\n<td style=\"text-align: center;\">1.08e-3</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">dropout</td>\n<td style=\"text-align: center;\">0</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">sequence length</td>\n<td style=\"text-align: center;\">2k</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: center;\">batch size</td>\n<td style=\"text-align: center;\">2k</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: center;\">total step</td>\n<td style=\"text-align: center;\">25,000</td>\n</tr>\n</tbody>\n</table>\n</center>\n<p><br>\n- expertGPUdevice-level balance loss<br>\n- expert-level balance factor0.01<br>\n- 80%0.31690%0.316</p>\n<p>100BDeepSeekMoE-2Bbenchmark4densehash\nlayermoeHash layers for large sparse modelsSwitch\nTransformerGShard</p>\n<img src=\"/44e38c1b/ds_moe_comparison.png\" class title=\"deepseek moe \">\n<p><br>\n- Hash LayerSwitch\nTransformerdense<br>\n- GSshardHash LayerSwitch\nTransformer<br>\n- DeepSeekMoEGShard</p>\n<p>DeepSeekMoEdenseGShardDeepSeekMoE-2B</p>\n<p>denseGShard161.5DeepSeekMoE-2B</p>\n<img src=\"/44e38c1b/ds_moe_upper_bound_2b.png\" class title=\"deepseek moe upper bound\">\n<p>DeepSeekMoEDeepSeekMoE-13B,\n1.21.5GShardDeepSeekMoE-13Bmatch</p>\n<img src=\"/44e38c1b/ds_moe_upper_bound_13b.png\" class title=\"deepseek moe upper bound\">\n<ol start=\"2\" type=\"1\">\n<li>DeepSeekMoE</li>\n</ol>\n<p>DeepSeekMoEshared expertfine-grained\nexpertexpert</p>\n<img src=\"/44e38c1b/ds_moe_ablation.png\" class title=\"deepseek moe upper bound \">\n<p>1</p>\n<p>2</p>\n<p>364expert1/2/4pileloss1.808,1.806,1.8111:32+6</p>\n<ol start=\"3\" type=\"1\">\n<li>expert specialization</li>\n</ol>\n<p>DeepSeekMoEexpert specialization</p>\n<p>1DeepSeekMoE-2B1.5GShardtop</p>\n<p></p>\n<img src=\"/44e38c1b/ds_moe_expert_specialization.png\" class title=\"\">\n<p>DeepSeekMoEDeepSeekMoE</p>\n<p>2DeepSeekMoEloss</p>\n<p>3GShardDeepSeekMoE</p>\n<img src=\"/44e38c1b/ds_moe_less_activated_expert.png\" class title=\"\">\n<p>132b2+6GShardDeepSeekMoE</p>\n<img src=\"/44e38c1b/ds_2b_less_expert.png\" class title=\"2B\">\n<ol type=\"1\">\n<li>DeepSeekMoE-16B</li>\n</ol>\n<p>DeepSeekMoE-16B2TLLAMA2-7B100k</p>\n<img src=\"/44e38c1b/ds_model_param.png\" class title=\"\">\n<p>MoE</p>\n<p>MoEloss</p>\n<p>DeepSeekMoE-16B6426gating\nfunctiontoken8token16.4B2.8B</p>\n<p>dimension</p>\n<p><br>\n- lr = 4.2e-4<br>\n- 80%90%lr0.316<br>\n- batch size = 4.5k4kbatch18M\ntoken2T10.6w<br>\n- pipeline parallelism</p>\n<p>expert level balance\nloss0.001</p>\n<p>DeepSeekMoE-16BDeepSeek-7B</p>\n<img src=\"/44e38c1b/ds_16b_perf_1.png\" class title=\"DeepSeek-7B\">\n<p>DeepSeekMoE-16BLLAMA2-7B</p>\n<img src=\"/44e38c1b/ds_16b_perf_2.png\" class title=\"LLAMA2-7B\">\n<ol start=\"5\" type=\"1\">\n<li>DeepSeekMoE-145B</li>\n</ol>\n<p>245BtokenDeepSeekMoE-145BDeepSeek-67B</p>\n<img src=\"/44e38c1b/ds_moe_145b.png\" class title=\"145b\">\n<h1 id=\"dbrx\">DBRX</h1>\n<p>2024327DatabricksDBRX132B36BMoE</p>\n<p>DBRXRoPEGLUGQAfine-grained\nexpert16token4MixtralGrok-182DBRX</p>\n<p>DBRX32k12TtokenDBRX3072H100post-trainingred-team3</p>\n<p>DBRXGPT-3.5Gemini 1.0\nProCodeLLaMA-70B</p>\n<img src=\"/44e38c1b/dbrx_perf.png\" class title=\"DBRX\">\n<p>DBRX</p>\n<img src=\"/44e38c1b/dbrx_infer_efficiency.png\" class title=\"\">\n<h1 id=\"qwen1.5-moe\">Qwen1.5-MoE</h1>\n<p>2024328Qwen1.5-MoE-A2.7B2.7BQwen1.5-7B</p>\n<p>Qwen1.5-MoE-A2.7BDeepSeekMoEDBRXfine-grained\nexpert64token84</p>\n<p>Qwen1.5-MoE-A2.7BQwen-1.8B</p>\n<p>Qwen1.5-MoE-A2.7B</p>\n<img src=\"/44e38c1b/qwen1.5_moe_perf.png\" class title=\"Qwen1.5-MoE-A2.7B\">\n<p>Qwen1.5-MoE-A2.7Bnon-embedding7B</p>\n<img src=\"/44e38c1b/qwen1.5_moe_params.png\" class title=\"Qwen1.5-MoE-A2.7B\">\n<p>Qwen1.5-MoE-A2.7BQwen1.5-7B75%</p>\n<p>A100-80GvLLMQwen1.5-7BQwen1.5-MoE-A2.7B</p>\n<p>/token1000token1000TPSthroughput</p>\n<img src=\"/44e38c1b/qwen1.5_moe_tps.png\" class title=\"Qwen1.5-MoE-A2.7B TPS\">\n<p>MoEdenseQwen1.5-MoE-A2.7BQwen1.5-7B1.74</p>\n<h1 id=\"mistral\">Mistral</h1>\n<h2 id=\"mistral-8x7b\">Mistral 8x7B</h2>\n<p>20231211Mistral\nAIMistral-8x7Btoken82</p>\n<p>Mistral-8x7B32kLLAM2-70BGPT-3.5</p>\n<img src=\"/44e38c1b/mistral_8_7b_perf.png\" class title=\"Mistral 8x7B\">\n<p>Mistral-8x7BLLAM2-70B6</p>\n<p>LLAM2-13B</p>\n<img src=\"/44e38c1b/mistral_8_7b_active_perf.png\" class title=\"Mistral 8x7B\">\n<h2 id=\"mistral-8x22b\">Mistral 8x22B</h2>\n<p>2024417Mistral\nAIMistral-8x22B141B39BMoE</p>\n<p>Mistral-8x22BMistral-8x7B32k64kMistral-8x22Bfunction\ncall</p>\n<p></p>\n<img src=\"/44e38c1b/mistral_8_22b_reasoning.png\" class title=\"Mistral 8x22B reasoning\">\n<img src=\"/44e38c1b/mistral_8_22b_multiling.png\" class title=\"Mistral 8x22B \">\n<img src=\"/44e38c1b/mistral_8_22b_code.png\" class title=\"Mistral 8x22B \">\n<h1 id=\"\"></h1>\n<ul>\n<li>MoEdenseMoE<br>\n</li>\n<li>MoEMoE<br>\n</li>\n<li>denseMoE<br>\n</li>\n<li><br>\n</li>\n<li>GShardSwitch Transformer<br>\n</li>\n<li>MoEMoE</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Adaptive Mixtures of Local Experts\nhttps://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf<br>\n2Outrageously Large Neural Networks: The Sparsely-Gated\nMixture-of-Experts Layer https://arxiv.org/abs/1701.06538<br>\n3GShard: Scaling Giant Models with Conditional Computation and\nAutomatic Sharding https://arxiv.org/abs/2006.16668<br>\n4Switch Transformers: Scaling to Trillion Parameter Models with\nSimple and Efficient Sparsity https://arxiv.org/abs/2101.03961<br>\n5GLaM: Efficient Scaling of Language Models with Mixture-of-Experts\nhttps://arxiv.org/abs/2112.06905<br>\n6ST-MoE: Designing Stable and Transferable Sparse Expert Models\nhttps://arxiv.org/abs/2202.08906<br>\n7DeepSeekMoE: Towards Ultimate Expert Specialization in\nMixture-of-Experts Language Models\nhttps://arxiv.org/abs/2401.06066<br>\n8Introducing DBRX: A New State-of-the-Art Open LLM\nhttps://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm<br>\n9Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated\nParameters https://qwenlm.github.io/zh/blog/qwen-moe/</p>\n"},{"title":"Attention:MHA,MQAGQA","abbrlink":"3dc22f96","date":"2024-03-05T10:49:38.000Z","_content":"\n//  \n\nAttentionMHAMulti-Head AttentionMQAMulti-Query AttentionGQAGrouped-Query AttentionKV Cache  \n\nAttentionFlashAttentionSliding Window Attention  \n\nLLM\n\n# AttentionRNNAttention\n\nattention\n\nattention\n\n## RNN\n\n> Memory is attention through time. ~ Alex Graves 2020\n\nTransformerRNNSeq2Seq\n\n{% asset_img seq2seq.png seq2seq %}  \n\n{% asset_img encoder.png encoder %}  \n\n{% asset_img decoder.png decoder %}  \n\n[AI Summer](https://theaisummer.com/attention/)  \n\nRNN cellhidden stateRNN encodercontext $z$ RNN decoder $z$ decodertoken[start]  \n\n $z$   \n\n  \n\nLSMTGRU  \n\n $z$  $z$   \n\n $z$   \n\n  \n\nCNNheatmap  \n\n{% asset_img cnn_heatmap.png heatmap %}  \n\nCNNimplicitly\n\nSeq2Seqimplicitexplicit  \n\nRNN $i$ $h_i$  $h_i$  $h_i$ hidden state  \n\n --   \n\n $i$ decoder $y_{i-1}$ encoder $\\mathbf{h}$ score\n\n$$\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in R^n$$  \n\n\n\n$$e_{ij}=\\text{attentiom}_{\\text{net }(\\mathbf{y}_{i-1},h_j)}$$  \n\n $\\mathbf{y}_{i-1}$  $h_j$  $e_{ij}$fc  \n \n  $e_{ij}$ attention netencoder hidden statesoftmax  \n\n$$\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}$$  \n\ndecoder  \n\n$$z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j$$\n\n{% asset_img seq2seq_attention.png seq2seq attention %}  \n\nattention netdecoderhidden stateencoder hidden state\n\nattentionattention  \n\n{% asset_img attention_calculation.png attention calculation %}  \n\nattention $\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot h$  $s$ decoderhidden state $y$ $h$ encoderhidden state  \n\nscaled dot-product attention  \n\n## Transformerattention\n\nRNN attentiontransformer attentionAttention Is All You NeedRNNtime stepattentionhidden stateattention  \n\n{% asset_img transformer_structure.png transformer structure.png %}  \n\nencoderdecoderencoderattentionself-attentiondecoderself-attentioncross-attention  \n\ntransformerattention $\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V$ $Q=W_{Q}YK=W_{K}XV=W_{V}X$ cross-attention $X$ encoderhidden states$Y$ decoderhidden statesself-attention $X=Y$  \n\nscaled dot-product attentionsoftmax\n\n$$\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V$$  \n\nattention  \n\n\n\n{% asset_img Scaled-dot-product-self-attention.pbm self-attention %}  \n\nquerykeyvalueattention  \n\n+  \n\n-30keyvalue  \n\n30querykey5  \n\ntop5 $[8,4,4,2,2]$  $[5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]$  \n \n1 $[0.4,0.2,0.2,0.1,0.1]$ 30 $0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}$ \n\ntransformer attention $QK^T$ softmax/  \n\nself-attention $QKV$  $X$sequencetokencross-attentiondecodersequence  \n\nself-attention $QKV$  $X$  $QK^T$  $QK^T$ MHA  \n\nattentionMHAMQAGQA\n\n[pytorch forcasting](https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention)\n\n```python\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self, dropout: float = None, scale: bool = True):\n        super(ScaledDotProductAttention, self).__init__()\n        if dropout is not None:\n            self.dropout = nn.Dropout(p=dropout)\n        else:\n            self.dropout = dropout\n        self.softmax = nn.Softmax(dim=2)\n        self.scale = scale\n\ndef forward(self, q, k, v, mask=None):\n        attn = torch.bmm(q, k.permute(0, 2, 1))  # query-key overlap\n\n        if self.scale:\n            dimension = torch.as_tensor(k.size(-1), dtype=attn.dtype, device=attn.device).sqrt()\n            attn = attn / dimension\n\n        if mask is not None:\n            attn = attn.masked_fill(mask, -1e9)\n        attn = self.softmax(attn)\n\n        if self.dropout is not None:\n            attn = self.dropout(attn)\n        output = torch.bmm(attn, v)\n        return output, attn\n```\n\n## scaling\n\nBTW $QK^T$  $\\sqrt{d}$   \n\nsoftmaxsoftmax  \n\n{% asset_img softmax.png softmax %}  \n\n[](https://spaces.ac.cn/archives/8620)attentionscaling $\\sqrt{d}$ softmaxnormalizationattentionscaling  \n\n# MHA\n\nattentionMHAmulti-head attention\n\nMHA2017Attention Is All You Needattentionattention  \n\n$$\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)$$  \n\n$$head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)$$  \n\nhidden size $d$ MHA $QKV$ hidden state $head_{num}$  $d_{head}$  $head_{num}$  $QKV$ attention  $head_{num}$  $d_{head}$ concat  \n\namazing  \n\n{% asset_img multihead_attention.png MHA %}  \n\n  \n\nAttention Is All You Need\n\n>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.  \n\nattention headattention head  \n\nCNN $3\\times3\\times128$ 128 $3\\times3$  $3\\times3$   \n\n$$\\left.\\left[\\begin{matrix}1&0&-1\\\\1&0&-1\\\\1&0&-1\\end{matrix}\\right.\\right]$$  \n\n  \n\n$$\\left.\\left[\\begin{matrix}1&1&1\\\\0&0&0\\\\-1&-1&-1\\end{matrix}\\right.\\right]$$  \n\n  \n\n128 $3\\times3$ 128MHA  \n\nexpect  \n\n[](https://zhuanlan.zhihu.com/p/626820422)12 $QK^T$   \n\nMHAattentionattention\n\n  \n\n[Are Sixteen Heads Really Better than One?](https://arxiv.org/pdf/1905.10650.pdf)MHA  \n\nhidden sizeLLM1216244896  \n\n[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)MHA  \n\n```python\ndef attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n             / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim = -1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\n\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        '''\n        h: head number\n        '''\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d\n        self.d = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n        \n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n        \n        # 1) Do all the linear projections in batch from d_model => h x d \n        query, key, value = \\\n            [l(x).view(nbatches, -1, self.h, self.d).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key, value))]\n        \n        # 2) Apply attention on all the projected vectors in batch. \n        x, self.attn = attention(query, key, value, mask=mask, \n                                 dropout=self.dropout)\n        \n        # 3) \"Concat\" using a view and apply a final linear. \n        x = x.transpose(1, 2).contiguous() \\\n             .view(nbatches, -1, self.h * self.d)\n        return self.linears[-1](x)\n```\n\n[transformers](https://github.com/huggingface/transformers)\n\n# KV Cache\n\nMQAGQAKV Cache  \n\nencoder-decoderAGIdecoder-onlyLLMauto-regressive  \n\n $\\text{input}_{i-1}$  $\\text{token}_{i}$  $\\text{token}_{i}$  $\\text{input}_{i-1}$  $\\text{input}_{i}$  $\\text{input}_{i}$  $\\text{token}_{i+1}$   \n\ntokentoken  \n\n```\nstep0: =[BOS]=\nstep1: =[BOS]=\nstep2: =[BOS]=\nstep3: =[BOS]=\nstep4: =[BOS]=\nstep5: =[BOS]=[EOS]\n```\n\n[BOS][EOS]  \n\nhidden state \n\nstepsteptokenstepstep\n\n\n\nattention  \n\n$$\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n$$\n\ndecodermask attention\n\n34attention\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n$$\n\n45attention\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n$$\n\n5 $o_{0}$  $o_{2}$   \n\n  \n\n  \n\nstep0101step515instruction800stepstep0800step1799...\n\nstep  \n\nKV Cache  \n\n $k$  $v$   \n\n34 $k$  $v$ \n\n\n\n$$\n\\text{cache}_l=\\text{None}\\\\\n$$\n\n<center></center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$  \n\nkv_cache $l$ \n\n5 $l$ <u>****</u> $k$  $v$  $o_{3}$  $o_{0}o_{1}o_{2}$   \n\n $l$  $o_{0}o_{1}o_{2}$ FNN $l+1$  $l+1$  $k$  $v$  $l+1$  $k$  $v$   \n\n $k$  $v$ \n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$\n\n<center></center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n$$  \n\nattentionFFN  \n\ntransformersuse_cache=TrueKV Cache  \n\nGPT2  \n\n```python\nClass GPT2Attention(nn.Module):\n    ...\n    ...\n    def forward(\n        self,\n        hidden_states: Optional[Tuple[torch.FloatTensor]],\n        layer_past: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n        if encoder_hidden_states is not None:\n            if not hasattr(self, \"q_attn\"):\n                raise ValueError(\n                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n                )\n\n            query = self.q_attn(hidden_states)\n            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n            attention_mask = encoder_attention_mask\n        else:\n            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n\n        query = self._split_heads(query, self.num_heads, self.head_dim)\n        key = self._split_heads(key, self.num_heads, self.head_dim)\n        value = self._split_heads(value, self.num_heads, self.head_dim)\n\n        # \n        if layer_past is not None:\n            past_key, past_value = layer_past\n            key = torch.cat((past_key, key), dim=-2)  # key\n            value = torch.cat((past_value, value), dim=-2)  # value\n\n        if use_cache is True:\n            present = (key, value)  # \n        else:\n            present = None\n\n        if self.reorder_and_upcast_attn:\n            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n        else:\n            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n\n        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n        attn_output = self.c_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output)\n\n        outputs = (attn_output, present)\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs  # a, present, (attentions)\n```\n\nKV Cachedecodermask attentiontokentoken  \n\nKV Cache  \n\n $s$  $L$ hidden size $d$   \n\n$$\n2\\times L\\times s\\times d\n$$  \n\n\n\n$$\n2\\times 2\\times L\\times s\\times d\n$$  \n\nLlama2 7B $L=32$  $L=4096$ token524,288 bytes52K $s=1024$ 536,870,912 bytes500M  \n\nbatch size=1batch size1G  \n\nMHA $qkv$ \n\n\n\n{% asset_img gpu_cache.png gpu cache %}  \n\nH10050ML2 CacheL1 CacheLlama2 7B100token  \n\nLLM34B/70B\n\nL2 CacheHBML2 Cache  \n\n{% asset_img sram_dram.png  %}  \n\n  \n\nCache  \n\n  \n\n# MQA\n\nMQA\n\nGoogle2019Fast Transformer Decoding: One Write-Head is All You NeedMQABert  \n\nMQAMHA $W_{Q}W_{K}W_{V}$ nn= $d_{model}$  $d_{head}$ attentionMQA $Q$ MHA $KV$  $d_{head}$ nQuery $KV$ attention  \n\nMHA $KV$ MQA $KV$ MHA  \n\n{% asset_img MQA.webp MQA %}  \n\n $KV$   \n\nLlama2 7B32MQA1024token1/32536,870,912 bytes / 32 = 16,777,216 bytes16M\n\n $KV$   \n\nMQAMHAhidden sizehead num  \n\n{% asset_img mqa_result_1.png MQA results 1 %}  \n\n{% asset_img mqa_result_3.png MQA results 3 %}  \n\n# GQA  \n\nMQAMHAGQAGrouped-Query AttentionMQAMHA  \n\nGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints2023\n\nGQA $Q$ MHA/MQA $KV$  $Q$  $Q$ groupgroup $Q$  $KV$ group $Q$  $KV$   \n\nMHA $KV$ GQAMQA $KV$ GQA  \n\n\n\n{% asset_img GQA.png GQA %}  \n\n  \n\n{% asset_img GQA_result_1.png GQA result %}  \n\n2/3/4GQAMHAMQAMHAMQAGQAaverage poolingMHAMHAGQA  \n\nLlama2GQAtech reportMHAMQAGQA  \n\n{% asset_img llama2_qga.png llama2 GQA result %}  \n\n#   \n\nMHAMQAGQA\n\nGQALLM  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n1The Annotated Transformer \n https://nlp.seas.harvard.edu/2018/04/03/attention.html  \n2Attention Is All You Need https://arxiv.org/pdf/1706.03762.pdf  \n3Fast Transformer Decoding: One Write-Head is All You Need https://arxiv.org/pdf/1911.02150.pdf  \n4https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096  \n5GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf  \n6How Attention works in Deep Learning: understanding the attention mechanism in sequence models https://theaisummer.com/attention/  \n7A simple overview of RNN, LSTM and Attention Mechanism \nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b  \n8https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention  \n9Transformer https://spaces.ac.cn/archives/8620  \n10https://theaisummer.com/self-attention/  https://theaisummer.com/self-attention/  \n11https://zhuanlan.zhihu.com/p/626820422 https://zhuanlan.zhihu.com/p/626820422  \n12Are Sixteen Heads Really Better than One? \nhttps://arxiv.org/pdf/1905.10650.pdf  \n13This post is all you needTransformer \nhttps://zhuanlan.zhihu.com/p/420820453  \n14The Illustrated Transformer https://jalammar.github.io/illustrated-transformer/  \n15Multi-Query Attention is All You Need https://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055  \n\n","source":"_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA.md","raw":"---\ntitle: 'Attention:MHA,MQAGQA'\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - attention\n  - KV Cache\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 3dc22f96\ndate: 2024-03-05 18:49:38\n---\n\n//  \n\nAttentionMHAMulti-Head AttentionMQAMulti-Query AttentionGQAGrouped-Query AttentionKV Cache  \n\nAttentionFlashAttentionSliding Window Attention  \n\nLLM\n\n# AttentionRNNAttention\n\nattention\n\nattention\n\n## RNN\n\n> Memory is attention through time. ~ Alex Graves 2020\n\nTransformerRNNSeq2Seq\n\n{% asset_img seq2seq.png seq2seq %}  \n\n{% asset_img encoder.png encoder %}  \n\n{% asset_img decoder.png decoder %}  \n\n[AI Summer](https://theaisummer.com/attention/)  \n\nRNN cellhidden stateRNN encodercontext $z$ RNN decoder $z$ decodertoken[start]  \n\n $z$   \n\n  \n\nLSMTGRU  \n\n $z$  $z$   \n\n $z$   \n\n  \n\nCNNheatmap  \n\n{% asset_img cnn_heatmap.png heatmap %}  \n\nCNNimplicitly\n\nSeq2Seqimplicitexplicit  \n\nRNN $i$ $h_i$  $h_i$  $h_i$ hidden state  \n\n --   \n\n $i$ decoder $y_{i-1}$ encoder $\\mathbf{h}$ score\n\n$$\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in R^n$$  \n\n\n\n$$e_{ij}=\\text{attentiom}_{\\text{net }(\\mathbf{y}_{i-1},h_j)}$$  \n\n $\\mathbf{y}_{i-1}$  $h_j$  $e_{ij}$fc  \n \n  $e_{ij}$ attention netencoder hidden statesoftmax  \n\n$$\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}$$  \n\ndecoder  \n\n$$z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j$$\n\n{% asset_img seq2seq_attention.png seq2seq attention %}  \n\nattention netdecoderhidden stateencoder hidden state\n\nattentionattention  \n\n{% asset_img attention_calculation.png attention calculation %}  \n\nattention $\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot h$  $s$ decoderhidden state $y$ $h$ encoderhidden state  \n\nscaled dot-product attention  \n\n## Transformerattention\n\nRNN attentiontransformer attentionAttention Is All You NeedRNNtime stepattentionhidden stateattention  \n\n{% asset_img transformer_structure.png transformer structure.png %}  \n\nencoderdecoderencoderattentionself-attentiondecoderself-attentioncross-attention  \n\ntransformerattention $\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V$ $Q=W_{Q}YK=W_{K}XV=W_{V}X$ cross-attention $X$ encoderhidden states$Y$ decoderhidden statesself-attention $X=Y$  \n\nscaled dot-product attentionsoftmax\n\n$$\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V$$  \n\nattention  \n\n\n\n{% asset_img Scaled-dot-product-self-attention.pbm self-attention %}  \n\nquerykeyvalueattention  \n\n+  \n\n-30keyvalue  \n\n30querykey5  \n\ntop5 $[8,4,4,2,2]$  $[5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]$  \n \n1 $[0.4,0.2,0.2,0.1,0.1]$ 30 $0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}$ \n\ntransformer attention $QK^T$ softmax/  \n\nself-attention $QKV$  $X$sequencetokencross-attentiondecodersequence  \n\nself-attention $QKV$  $X$  $QK^T$  $QK^T$ MHA  \n\nattentionMHAMQAGQA\n\n[pytorch forcasting](https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention)\n\n```python\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self, dropout: float = None, scale: bool = True):\n        super(ScaledDotProductAttention, self).__init__()\n        if dropout is not None:\n            self.dropout = nn.Dropout(p=dropout)\n        else:\n            self.dropout = dropout\n        self.softmax = nn.Softmax(dim=2)\n        self.scale = scale\n\ndef forward(self, q, k, v, mask=None):\n        attn = torch.bmm(q, k.permute(0, 2, 1))  # query-key overlap\n\n        if self.scale:\n            dimension = torch.as_tensor(k.size(-1), dtype=attn.dtype, device=attn.device).sqrt()\n            attn = attn / dimension\n\n        if mask is not None:\n            attn = attn.masked_fill(mask, -1e9)\n        attn = self.softmax(attn)\n\n        if self.dropout is not None:\n            attn = self.dropout(attn)\n        output = torch.bmm(attn, v)\n        return output, attn\n```\n\n## scaling\n\nBTW $QK^T$  $\\sqrt{d}$   \n\nsoftmaxsoftmax  \n\n{% asset_img softmax.png softmax %}  \n\n[](https://spaces.ac.cn/archives/8620)attentionscaling $\\sqrt{d}$ softmaxnormalizationattentionscaling  \n\n# MHA\n\nattentionMHAmulti-head attention\n\nMHA2017Attention Is All You Needattentionattention  \n\n$$\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)$$  \n\n$$head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)$$  \n\nhidden size $d$ MHA $QKV$ hidden state $head_{num}$  $d_{head}$  $head_{num}$  $QKV$ attention  $head_{num}$  $d_{head}$ concat  \n\namazing  \n\n{% asset_img multihead_attention.png MHA %}  \n\n  \n\nAttention Is All You Need\n\n>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.  \n\nattention headattention head  \n\nCNN $3\\times3\\times128$ 128 $3\\times3$  $3\\times3$   \n\n$$\\left.\\left[\\begin{matrix}1&0&-1\\\\1&0&-1\\\\1&0&-1\\end{matrix}\\right.\\right]$$  \n\n  \n\n$$\\left.\\left[\\begin{matrix}1&1&1\\\\0&0&0\\\\-1&-1&-1\\end{matrix}\\right.\\right]$$  \n\n  \n\n128 $3\\times3$ 128MHA  \n\nexpect  \n\n[](https://zhuanlan.zhihu.com/p/626820422)12 $QK^T$   \n\nMHAattentionattention\n\n  \n\n[Are Sixteen Heads Really Better than One?](https://arxiv.org/pdf/1905.10650.pdf)MHA  \n\nhidden sizeLLM1216244896  \n\n[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)MHA  \n\n```python\ndef attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n             / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim = -1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\n\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        '''\n        h: head number\n        '''\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d\n        self.d = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n        \n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n        \n        # 1) Do all the linear projections in batch from d_model => h x d \n        query, key, value = \\\n            [l(x).view(nbatches, -1, self.h, self.d).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key, value))]\n        \n        # 2) Apply attention on all the projected vectors in batch. \n        x, self.attn = attention(query, key, value, mask=mask, \n                                 dropout=self.dropout)\n        \n        # 3) \"Concat\" using a view and apply a final linear. \n        x = x.transpose(1, 2).contiguous() \\\n             .view(nbatches, -1, self.h * self.d)\n        return self.linears[-1](x)\n```\n\n[transformers](https://github.com/huggingface/transformers)\n\n# KV Cache\n\nMQAGQAKV Cache  \n\nencoder-decoderAGIdecoder-onlyLLMauto-regressive  \n\n $\\text{input}_{i-1}$  $\\text{token}_{i}$  $\\text{token}_{i}$  $\\text{input}_{i-1}$  $\\text{input}_{i}$  $\\text{input}_{i}$  $\\text{token}_{i+1}$   \n\ntokentoken  \n\n```\nstep0: =[BOS]=\nstep1: =[BOS]=\nstep2: =[BOS]=\nstep3: =[BOS]=\nstep4: =[BOS]=\nstep5: =[BOS]=[EOS]\n```\n\n[BOS][EOS]  \n\nhidden state \n\nstepsteptokenstepstep\n\n\n\nattention  \n\n$$\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n$$\n\ndecodermask attention\n\n34attention\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n$$\n\n45attention\n\n$$\n\\begin{aligned}\no_{0}&=\\alpha_{0,0}v_{0}\\\\\no_{1}&=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n$$\n\n5 $o_{0}$  $o_{2}$   \n\n  \n\n  \n\nstep0101step515instruction800stepstep0800step1799...\n\nstep  \n\nKV Cache  \n\n $k$  $v$   \n\n34 $k$  $v$ \n\n\n\n$$\n\\text{cache}_l=\\text{None}\\\\\n$$\n\n<center></center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$  \n\nkv_cache $l$ \n\n5 $l$ <u>****</u> $k$  $v$  $o_{3}$  $o_{0}o_{1}o_{2}$   \n\n $l$  $o_{0}o_{1}o_{2}$ FNN $l+1$  $l+1$  $k$  $v$  $l+1$  $k$  $v$   \n\n $k$  $v$ \n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n$$\n\n<center></center>\n\n$$\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l}, v_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n$$  \n\nattentionFFN  \n\ntransformersuse_cache=TrueKV Cache  \n\nGPT2  \n\n```python\nClass GPT2Attention(nn.Module):\n    ...\n    ...\n    def forward(\n        self,\n        hidden_states: Optional[Tuple[torch.FloatTensor]],\n        layer_past: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n        if encoder_hidden_states is not None:\n            if not hasattr(self, \"q_attn\"):\n                raise ValueError(\n                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n                )\n\n            query = self.q_attn(hidden_states)\n            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n            attention_mask = encoder_attention_mask\n        else:\n            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n\n        query = self._split_heads(query, self.num_heads, self.head_dim)\n        key = self._split_heads(key, self.num_heads, self.head_dim)\n        value = self._split_heads(value, self.num_heads, self.head_dim)\n\n        # \n        if layer_past is not None:\n            past_key, past_value = layer_past\n            key = torch.cat((past_key, key), dim=-2)  # key\n            value = torch.cat((past_value, value), dim=-2)  # value\n\n        if use_cache is True:\n            present = (key, value)  # \n        else:\n            present = None\n\n        if self.reorder_and_upcast_attn:\n            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n        else:\n            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n\n        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n        attn_output = self.c_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output)\n\n        outputs = (attn_output, present)\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs  # a, present, (attentions)\n```\n\nKV Cachedecodermask attentiontokentoken  \n\nKV Cache  \n\n $s$  $L$ hidden size $d$   \n\n$$\n2\\times L\\times s\\times d\n$$  \n\n\n\n$$\n2\\times 2\\times L\\times s\\times d\n$$  \n\nLlama2 7B $L=32$  $L=4096$ token524,288 bytes52K $s=1024$ 536,870,912 bytes500M  \n\nbatch size=1batch size1G  \n\nMHA $qkv$ \n\n\n\n{% asset_img gpu_cache.png gpu cache %}  \n\nH10050ML2 CacheL1 CacheLlama2 7B100token  \n\nLLM34B/70B\n\nL2 CacheHBML2 Cache  \n\n{% asset_img sram_dram.png  %}  \n\n  \n\nCache  \n\n  \n\n# MQA\n\nMQA\n\nGoogle2019Fast Transformer Decoding: One Write-Head is All You NeedMQABert  \n\nMQAMHA $W_{Q}W_{K}W_{V}$ nn= $d_{model}$  $d_{head}$ attentionMQA $Q$ MHA $KV$  $d_{head}$ nQuery $KV$ attention  \n\nMHA $KV$ MQA $KV$ MHA  \n\n{% asset_img MQA.webp MQA %}  \n\n $KV$   \n\nLlama2 7B32MQA1024token1/32536,870,912 bytes / 32 = 16,777,216 bytes16M\n\n $KV$   \n\nMQAMHAhidden sizehead num  \n\n{% asset_img mqa_result_1.png MQA results 1 %}  \n\n{% asset_img mqa_result_3.png MQA results 3 %}  \n\n# GQA  \n\nMQAMHAGQAGrouped-Query AttentionMQAMHA  \n\nGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints2023\n\nGQA $Q$ MHA/MQA $KV$  $Q$  $Q$ groupgroup $Q$  $KV$ group $Q$  $KV$   \n\nMHA $KV$ GQAMQA $KV$ GQA  \n\n\n\n{% asset_img GQA.png GQA %}  \n\n  \n\n{% asset_img GQA_result_1.png GQA result %}  \n\n2/3/4GQAMHAMQAMHAMQAGQAaverage poolingMHAMHAGQA  \n\nLlama2GQAtech reportMHAMQAGQA  \n\n{% asset_img llama2_qga.png llama2 GQA result %}  \n\n#   \n\nMHAMQAGQA\n\nGQALLM  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n# Reference\n1The Annotated Transformer \n https://nlp.seas.harvard.edu/2018/04/03/attention.html  \n2Attention Is All You Need https://arxiv.org/pdf/1706.03762.pdf  \n3Fast Transformer Decoding: One Write-Head is All You Need https://arxiv.org/pdf/1911.02150.pdf  \n4https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096  \n5GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf  \n6How Attention works in Deep Learning: understanding the attention mechanism in sequence models https://theaisummer.com/attention/  \n7A simple overview of RNN, LSTM and Attention Mechanism \nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b  \n8https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention  \n9Transformer https://spaces.ac.cn/archives/8620  \n10https://theaisummer.com/self-attention/  https://theaisummer.com/self-attention/  \n11https://zhuanlan.zhihu.com/p/626820422 https://zhuanlan.zhihu.com/p/626820422  \n12Are Sixteen Heads Really Better than One? \nhttps://arxiv.org/pdf/1905.10650.pdf  \n13This post is all you needTransformer \nhttps://zhuanlan.zhihu.com/p/420820453  \n14The Illustrated Transformer https://jalammar.github.io/illustrated-transformer/  \n15Multi-Query Attention is All You Need https://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055  \n\n","slug":"cs/nlp/2024/03/Attention-MHA-MQAGQA","published":1,"updated":"2024-04-27T15:23:22.122Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmv00180p4kgx3k0lrt","content":"<p>//</p>\n<p>AttentionMHAMulti-Head\nAttentionMQAMulti-Query AttentionGQAGrouped-Query\nAttentionKV\nCache</p>\n<p>AttentionFlashAttentionSliding\nWindow Attention</p>\n<p>LLM</p>\n<h1 id=\"attentionrnnattention\">AttentionRNNAttention</h1>\n<p>attention</p>\n<p>attention</p>\n<h2 id=\"rnn\">RNN</h2>\n<blockquote>\n<p>Memory is attention through time. ~ Alex Graves 2020</p>\n</blockquote>\n<p>TransformerRNNSeq2Seq</p>\n<img src=\"/3dc22f96/seq2seq.png\" class title=\"seq2seq\">\n<img src=\"/3dc22f96/encoder.png\" class title=\"encoder\">\n<img src=\"/3dc22f96/decoder.png\" class title=\"decoder\">\n<p><a href=\"https://theaisummer.com/attention/\">AI\nSummer</a></p>\n<p>RNN cellhidden stateRNN\nencodercontext <span class=\"math inline\">\\(z\\)</span> RNN decoder <span class=\"math inline\">\\(z\\)</span>\ndecodertoken[start]</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p></p>\n<p>LSMTGRU</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p></p>\n<p>CNNheatmap</p>\n<img src=\"/3dc22f96/cnn_heatmap.png\" class title=\"heatmap\">\n<p>CNNimplicitly</p>\n<p>Seq2Seqimplicitexplicit</p>\n<p>RNN <span class=\"math inline\">\\(i\\)</span> <span class=\"math inline\">\\(h_i\\)</span>  <span class=\"math inline\">\\(h_i\\)</span>\n\n<span class=\"math inline\">\\(h_i\\)</span>\nhidden\nstate</p>\n<p>\n--\n</p>\n<p> <span class=\"math inline\">\\(i\\)</span>\ndecoder <span class=\"math inline\">\\(y_{i-1}\\)</span>\nencoder <span class=\"math inline\">\\(\\mathbf{h}\\)</span>\nscore</p>\n<p><span class=\"math display\">\\[\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in\nR^n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[e_{ij}=\\text{attentiom}_{\\text{net\n}(\\mathbf{y}_{i-1},h_j)}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\mathbf{y}_{i-1}\\)</span>\n <span class=\"math inline\">\\(h_j\\)</span>  <span class=\"math inline\">\\(e_{ij}\\)</span>fc</p>\n<p> <span class=\"math inline\">\\(e_{ij}\\)</span>\nattention\nnetencoder hidden statesoftmax</p>\n<p><span class=\"math display\">\\[\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}\\]</span></p>\n<p>decoder</p>\n<p><span class=\"math display\">\\[z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j\\]</span></p>\n<img src=\"/3dc22f96/seq2seq_attention.png\" class title=\"seq2seq attention\">\n<p>attention netdecoderhidden\nstateencoder hidden\nstate</p>\n<p>attentionattention</p>\n<img src=\"/3dc22f96/attention_calculation.png\" class title=\"attention calculation\">\n<p>attention <span class=\"math inline\">\\(\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot\nh\\)</span>  <span class=\"math inline\">\\(s\\)</span>\ndecoderhidden state <span class=\"math inline\">\\(y\\)</span> <span class=\"math inline\">\\(h\\)</span> encoderhidden state</p>\n<p>scaled dot-product\nattention</p>\n<h2 id=\"transformerattention\">Transformerattention</h2>\n<p>RNN attentiontransformer\nattentionAttention Is All You\nNeedRNNtime\nstepattentionhidden\nstateattention</p>\n<img src=\"/3dc22f96/transformer_structure.png\" class title=\"transformer structure.png\">\n<p>encoderdecoderencoderattentionself-attentiondecoderself-attentioncross-attention</p>\n<p>transformerattention <span class=\"math inline\">\\(\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V\\)</span>\n<span class=\"math inline\">\\(Q=W_{Q}YK=W_{K}XV=W_{V}X\\)</span>\ncross-attention <span class=\"math inline\">\\(X\\)</span>\nencoderhidden states<span class=\"math inline\">\\(Y\\)</span>\ndecoderhidden statesself-attention <span class=\"math inline\">\\(X=Y\\)</span></p>\n<p>scaled dot-product attentionsoftmax</p>\n<p><span class=\"math display\">\\[\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V\\]</span></p>\n<p>attention</p>\n<p></p>\n<img src=\"/3dc22f96/Scaled-dot-product-self-attention.pbm\" class title=\"self-attention\">\n<p>querykeyvalueattention</p>\n<p>+</p>\n<p>-30keyvalue</p>\n<p>30querykey5</p>\n<p>top5 <span class=\"math inline\">\\([8,4,4,2,2]\\)</span>  <span class=\"math inline\">\\([5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]\\)</span>\n</p>\n<p>1 <span class=\"math inline\">\\([0.4,0.2,0.2,0.1,0.1]\\)</span>\n30 <span class=\"math inline\">\\(0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}\\)</span>\n</p>\n<p>transformer attention <span class=\"math inline\">\\(QK^T\\)</span>\nsoftmax/</p>\n<p>self-attention <span class=\"math inline\">\\(QKV\\)</span>\n <span class=\"math inline\">\\(X\\)</span>sequencetokencross-attentiondecodersequence</p>\n<p>self-attention <span class=\"math inline\">\\(QKV\\)</span>\n <span class=\"math inline\">\\(X\\)</span>  <span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(QK^T\\)</span>\nMHA</p>\n<p>attentionMHAMQAGQA</p>\n<p><a href=\"https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention\">pytorch\nforcasting</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ScaledDotProductAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, dropout: <span class=\"built_in\">float</span> = <span class=\"literal\">None</span>, scale: <span class=\"built_in\">bool</span> = <span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.dropout = dropout</span><br><span class=\"line\">        self.softmax = nn.Softmax(dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">        self.scale = scale</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, q, k, v, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        attn = torch.bmm(q, k.permute(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))  <span class=\"comment\"># query-key overlap</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.scale:</span><br><span class=\"line\">            dimension = torch.as_tensor(k.size(-<span class=\"number\">1</span>), dtype=attn.dtype, device=attn.device).sqrt()</span><br><span class=\"line\">            attn = attn / dimension</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = attn.masked_fill(mask, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">        attn = self.softmax(attn)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = self.dropout(attn)</span><br><span class=\"line\">        output = torch.bmm(attn, v)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, attn</span><br></pre></td></tr></table></figure>\n<h2 id=\"scaling\">scaling</h2>\n<p>BTW <span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(\\sqrt{d}\\)</span> </p>\n<p>softmaxsoftmax</p>\n<img src=\"/3dc22f96/softmax.png\" class title=\"softmax\">\n<p><a href=\"https://spaces.ac.cn/archives/8620\"></a>attentionscaling\n<span class=\"math inline\">\\(\\sqrt{d}\\)</span>\nsoftmaxnormalizationattentionscaling</p>\n<h1 id=\"mha\">MHA</h1>\n<p>attentionMHAmulti-head\nattention</p>\n<p>MHA2017Attention Is All You\nNeedattentionattention</p>\n<p><span class=\"math display\">\\[\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)\\]</span></p>\n<p><span class=\"math display\">\\[head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)\\]</span></p>\n<p>hidden size <span class=\"math inline\">\\(d\\)</span>\nMHA <span class=\"math inline\">\\(QKV\\)</span>\nhidden state <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span>  <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(QKV\\)</span>\nattention <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span> concat</p>\n<p>amazing</p>\n<img src=\"/3dc22f96/multihead_attention.png\" class title=\"MHA\">\n<p></p>\n<p>Attention Is All You Need</p>\n<blockquote>\n<p>Multi-head attention allows the model to jointly attend to\ninformation from different representation subspaces at different\npositions.</p>\n</blockquote>\n<p>attention\nheadattention\nhead</p>\n<p>CNN <span class=\"math inline\">\\(3\\times3\\times128\\)</span> 128 <span class=\"math inline\">\\(3\\times3\\)</span>\n <span class=\"math inline\">\\(3\\times3\\)</span> </p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;1&amp;1\\\\0&amp;0&amp;0\\\\-1&amp;-1&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p></p>\n<p>128 <span class=\"math inline\">\\(3\\times3\\)</span>\n128MHA</p>\n<p>expect</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/626820422\"></a>12\n<span class=\"math inline\">\\(QK^T\\)</span> </p>\n<p>MHAattentionattention</p>\n<p></p>\n<p><a href=\"https://arxiv.org/pdf/1905.10650.pdf\">Are Sixteen Heads Really\nBetter than One?</a>MHA</p>\n<p>hidden\nsizeLLM1216244896</p>\n<p><a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\">The\nAnnotated Transformer</a>MHA</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">attention</span>(<span class=\"params\">query, key, value, mask=<span class=\"literal\">None</span>, dropout=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class=\"line\">    d_k = query.size(-<span class=\"number\">1</span>)</span><br><span class=\"line\">    scores = torch.matmul(query, key.transpose(-<span class=\"number\">2</span>, -<span class=\"number\">1</span>)) \\</span><br><span class=\"line\">             / math.sqrt(d_k)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        scores = scores.masked_fill(mask == <span class=\"number\">0</span>, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">    p_attn = F.softmax(scores, dim = -<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        p_attn = dropout(p_attn)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultiHeadedAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, h, d_model, dropout=<span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        h: head number</span></span><br><span class=\"line\"><span class=\"string\">        &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> d_model % h == <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"comment\"># We assume d_v always equals d</span></span><br><span class=\"line\">        self.d = d_model // h</span><br><span class=\"line\">        self.h = h</span><br><span class=\"line\">        self.linears = clones(nn.Linear(d_model, d_model), <span class=\"number\">4</span>)</span><br><span class=\"line\">        self.attn = <span class=\"literal\">None</span></span><br><span class=\"line\">        self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, query, key, value, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"comment\"># Same mask applied to all h heads.</span></span><br><span class=\"line\">            mask = mask.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">        nbatches = query.size(<span class=\"number\">0</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 1) Do all the linear projections in batch from d_model =&gt; h x d </span></span><br><span class=\"line\">        query, key, value = \\</span><br><span class=\"line\">            [l(x).view(nbatches, -<span class=\"number\">1</span>, self.h, self.d).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">             <span class=\"keyword\">for</span> l, x <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(self.linears, (query, key, value))]</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class=\"line\">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class=\"line\">                                 dropout=self.dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class=\"line\">        x = x.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous() \\</span><br><span class=\"line\">             .view(nbatches, -<span class=\"number\">1</span>, self.h * self.d)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.linears[-<span class=\"number\">1</span>](x)</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://github.com/huggingface/transformers\">transformers</a></p>\n<h1 id=\"kv-cache\">KV Cache</h1>\n<p>MQAGQAKV\nCache</p>\n<p>encoder-decoderAGIdecoder-onlyLLMauto-regressive</p>\n<p> <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span> \n<span class=\"math inline\">\\(\\text{input}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{input}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i+1}\\)</span>\n</p>\n<p>tokentoken</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">step0: =[BOS]=</span><br><span class=\"line\">step1: =[BOS]=</span><br><span class=\"line\">step2: =[BOS]=</span><br><span class=\"line\">step3: =[BOS]=</span><br><span class=\"line\">step4: =[BOS]=</span><br><span class=\"line\">step5: =[BOS]=[EOS]</span><br></pre></td></tr></table></figure>\n<p>[BOS][EOS]</p>\n<p>hidden\nstate</p>\n<p>stepsteptokenstepstep</p>\n<p></p>\n<p>attention</p>\n<p><span class=\"math display\">\\[\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n\\]</span></p>\n<p>decodermask\nattention</p>\n<p>34attention</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>45attention</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&amp;=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>5 <span class=\"math inline\">\\(o_{0}\\)</span>  <span class=\"math inline\">\\(o_{2}\\)</span> </p>\n<p></p>\n<p></p>\n<p>step0101step515instruction800stepstep0800step1799...</p>\n<p>step</p>\n<p>KV\nCache</p>\n<p> <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p>34\n<span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=\\text{None}\\\\\n\\]</span></p>\n<center>\n\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<p>kv_cache <span class=\"math inline\">\\(l\\)</span>\n</p>\n<p>5 <span class=\"math inline\">\\(l\\)</span>\n<u><strong></strong></u> <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span>  <span class=\"math inline\">\\(o_{3}\\)</span>  <span class=\"math inline\">\\(o_{0}o_{1}o_{2}\\)</span> </p>\n<p> <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(o_{0}o_{1}o_{2}\\)</span>\nFNN <span class=\"math inline\">\\(l+1\\)</span>\n <span class=\"math inline\">\\(l+1\\)</span>\n <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span>  <span class=\"math inline\">\\(l+1\\)</span>  <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p> <span class=\"math inline\">\\(k\\)</span> \n<span class=\"math inline\">\\(v\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<center>\n\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n\\]</span></p>\n<p>attentionFFN</p>\n<p>transformersuse_cache=TrueKV Cache</p>\n<p>GPT2</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Class GPT2Attention(nn.Module):</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">        self,</span></span><br><span class=\"line\"><span class=\"params\">        hidden_states: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.FloatTensor]],</span></span><br><span class=\"line\"><span class=\"params\">        layer_past: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.Tensor]] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        head_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_hidden_states: <span class=\"type\">Optional</span>[torch.Tensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        use_cache: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">        output_attentions: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">    </span>) -&gt; <span class=\"type\">Tuple</span>[<span class=\"type\">Union</span>[torch.Tensor, <span class=\"type\">Tuple</span>[torch.Tensor]], ...]:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> encoder_hidden_states <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">hasattr</span>(self, <span class=\"string\">&quot;q_attn&quot;</span>):</span><br><span class=\"line\">                <span class=\"keyword\">raise</span> ValueError(</span><br><span class=\"line\">                    <span class=\"string\">&quot;If class is used as cross attention, the weights `q_attn` have to be defined. &quot;</span></span><br><span class=\"line\">                    <span class=\"string\">&quot;Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.&quot;</span></span><br><span class=\"line\">                )</span><br><span class=\"line\"></span><br><span class=\"line\">            query = self.q_attn(hidden_states)</span><br><span class=\"line\">            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">            attention_mask = encoder_attention_mask</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        query = self._split_heads(query, self.num_heads, self.head_dim)</span><br><span class=\"line\">        key = self._split_heads(key, self.num_heads, self.head_dim)</span><br><span class=\"line\">        value = self._split_heads(value, self.num_heads, self.head_dim)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># </span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> layer_past <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            past_key, past_value = layer_past</span><br><span class=\"line\">            key = torch.cat((past_key, key), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># key</span></span><br><span class=\"line\">            value = torch.cat((past_value, value), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># value</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> use_cache <span class=\"keyword\">is</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">            present = (key, value)  <span class=\"comment\"># </span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            present = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.reorder_and_upcast_attn:</span><br><span class=\"line\">            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)</span><br><span class=\"line\">        attn_output = self.c_proj(attn_output)</span><br><span class=\"line\">        attn_output = self.resid_dropout(attn_output)</span><br><span class=\"line\"></span><br><span class=\"line\">        outputs = (attn_output, present)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> output_attentions:</span><br><span class=\"line\">            outputs += (attn_weights,)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> outputs  <span class=\"comment\"># a, present, (attentions)</span></span><br></pre></td></tr></table></figure>\n<p>KV\nCachedecodermask\nattentiontokentoken</p>\n<p>KV Cache</p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n <span class=\"math inline\">\\(L\\)</span> hidden size <span class=\"math inline\">\\(d\\)</span> </p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d\n\\]</span></p>\n<p>Llama2 7B <span class=\"math inline\">\\(L=32\\)</span> \n<span class=\"math inline\">\\(L=4096\\)</span>\ntoken524,288 bytes52K <span class=\"math inline\">\\(s=1024\\)</span> 536,870,912\nbytes500M</p>\n<p>batch size=1batch\nsize1G</p>\n<p>MHA <span class=\"math inline\">\\(qkv\\)</span>\n</p>\n<p></p>\n<img src=\"/3dc22f96/gpu_cache.png\" class title=\"gpu cache\">\n<p>H10050ML2 CacheL1\nCacheLlama2\n7B100token</p>\n<p>LLM34B/70B</p>\n<p>L2 CacheHBML2\nCache</p>\n<img src=\"/3dc22f96/sram_dram.png\" class title=\"\">\n<p></p>\n<p>Cache</p>\n<p></p>\n<h1 id=\"mqa\">MQA</h1>\n<p>MQA</p>\n<p>Google2019Fast Transformer Decoding: One Write-Head is All\nYou\nNeedMQABert</p>\n<p>MQAMHA <span class=\"math inline\">\\(W_{Q}W_{K}W_{V}\\)</span>\nnn= <span class=\"math inline\">\\(d_{model}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span>\nattentionMQA <span class=\"math inline\">\\(Q\\)</span> MHA <span class=\"math inline\">\\(KV\\)</span> \n<span class=\"math inline\">\\(d_{head}\\)</span>\nnQuery <span class=\"math inline\">\\(KV\\)</span>\nattention</p>\n<p>MHA <span class=\"math inline\">\\(KV\\)</span>\nMQA <span class=\"math inline\">\\(KV\\)</span>\nMHA</p>\n<img src=\"/3dc22f96/MQA.webp\" class title=\"MQA\">\n<p> <span class=\"math inline\">\\(KV\\)</span>\n</p>\n<p>Llama2\n7B32MQA1024token1/32536,870,912\nbytes / 32 = 16,777,216 bytes16M</p>\n<p> <span class=\"math inline\">\\(KV\\)</span>\n</p>\n<p>MQAMHAhidden\nsizehead num</p>\n<img src=\"/3dc22f96/mqa_result_1.png\" class title=\"MQA results 1\">\n<img src=\"/3dc22f96/mqa_result_3.png\" class title=\"MQA results 3\">\n<h1 id=\"gqa\">GQA</h1>\n<p>MQAMHAGQAGrouped-Query\nAttentionMQAMHA</p>\n<p>GQA: Training Generalized Multi-Query Transformer Models\nfrom Multi-Head Checkpoints2023</p>\n<p>GQA <span class=\"math inline\">\\(Q\\)</span>\nMHA/MQA <span class=\"math inline\">\\(KV\\)</span>\n <span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(Q\\)</span> groupgroup\n<span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(KV\\)</span> group <span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(KV\\)</span> </p>\n<p>MHA <span class=\"math inline\">\\(KV\\)</span>\nGQAMQA <span class=\"math inline\">\\(KV\\)</span> GQA</p>\n<p></p>\n<img src=\"/3dc22f96/GQA.png\" class title=\"GQA\">\n<p></p>\n<img src=\"/3dc22f96/GQA_result_1.png\" class title=\"GQA result\">\n<p>2/3/4GQAMHAMQAMHAMQAGQAaverage\npoolingMHAMHAGQA</p>\n<p>Llama2GQAtech\nreportMHAMQAGQA</p>\n<img src=\"/3dc22f96/llama2_qga.png\" class title=\"llama2 GQA result\">\n<h1 id=\"\"></h1>\n<p>MHAMQAGQA</p>\n<p>GQALLM</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1The Annotated Transformer\nhttps://nlp.seas.harvard.edu/2018/04/03/attention.html<br>\n2Attention Is All You Need\nhttps://arxiv.org/pdf/1706.03762.pdf<br>\n3Fast Transformer Decoding: One Write-Head is All You Need\nhttps://arxiv.org/pdf/1911.02150.pdf<br>\n4https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096<br>\n5GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf<br>\n6How Attention works in Deep Learning: understanding the attention\nmechanism in sequence models https://theaisummer.com/attention/<br>\n7A simple overview of RNN, LSTM and Attention Mechanism\nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b<br>\n8https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention<br>\n9Transformer\nhttps://spaces.ac.cn/archives/8620<br>\n10https://theaisummer.com/self-attention/\nhttps://theaisummer.com/self-attention/<br>\n11https://zhuanlan.zhihu.com/p/626820422\nhttps://zhuanlan.zhihu.com/p/626820422<br>\n12Are Sixteen Heads Really Better than One?\nhttps://arxiv.org/pdf/1905.10650.pdf<br>\n13This post is all you needTransformer\nhttps://zhuanlan.zhihu.com/p/420820453<br>\n14The Illustrated Transformer\nhttps://jalammar.github.io/illustrated-transformer/<br>\n15Multi-Query Attention is All You Need\nhttps://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055</p>\n","length":16733,"excerpt":"","more":"<p>//</p>\n<p>AttentionMHAMulti-Head\nAttentionMQAMulti-Query AttentionGQAGrouped-Query\nAttentionKV\nCache</p>\n<p>AttentionFlashAttentionSliding\nWindow Attention</p>\n<p>LLM</p>\n<h1 id=\"attentionrnnattention\">AttentionRNNAttention</h1>\n<p>attention</p>\n<p>attention</p>\n<h2 id=\"rnn\">RNN</h2>\n<blockquote>\n<p>Memory is attention through time. ~ Alex Graves 2020</p>\n</blockquote>\n<p>TransformerRNNSeq2Seq</p>\n<img src=\"/3dc22f96/seq2seq.png\" class title=\"seq2seq\">\n<img src=\"/3dc22f96/encoder.png\" class title=\"encoder\">\n<img src=\"/3dc22f96/decoder.png\" class title=\"decoder\">\n<p><a href=\"https://theaisummer.com/attention/\">AI\nSummer</a></p>\n<p>RNN cellhidden stateRNN\nencodercontext <span class=\"math inline\">\\(z\\)</span> RNN decoder <span class=\"math inline\">\\(z\\)</span>\ndecodertoken[start]</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p></p>\n<p>LSMTGRU</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(z\\)</span>\n</p>\n<p></p>\n<p>CNNheatmap</p>\n<img src=\"/3dc22f96/cnn_heatmap.png\" class title=\"heatmap\">\n<p>CNNimplicitly</p>\n<p>Seq2Seqimplicitexplicit</p>\n<p>RNN <span class=\"math inline\">\\(i\\)</span> <span class=\"math inline\">\\(h_i\\)</span>  <span class=\"math inline\">\\(h_i\\)</span>\n\n<span class=\"math inline\">\\(h_i\\)</span>\nhidden\nstate</p>\n<p>\n--\n</p>\n<p> <span class=\"math inline\">\\(i\\)</span>\ndecoder <span class=\"math inline\">\\(y_{i-1}\\)</span>\nencoder <span class=\"math inline\">\\(\\mathbf{h}\\)</span>\nscore</p>\n<p><span class=\"math display\">\\[\\mathbf{e}_i=\\text{attention}_{\\mathrm{net}}\\left(y_{i-1},\\mathbf{h}\\right)\\in\nR^n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[e_{ij}=\\text{attentiom}_{\\text{net\n}(\\mathbf{y}_{i-1},h_j)}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\mathbf{y}_{i-1}\\)</span>\n <span class=\"math inline\">\\(h_j\\)</span>  <span class=\"math inline\">\\(e_{ij}\\)</span>fc</p>\n<p> <span class=\"math inline\">\\(e_{ij}\\)</span>\nattention\nnetencoder hidden statesoftmax</p>\n<p><span class=\"math display\">\\[\\alpha_{ij}=\\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k=1}^{T_x}\\exp\\left(e_{ik}\\right)}\\]</span></p>\n<p>decoder</p>\n<p><span class=\"math display\">\\[z_i=\\sum_{j=1}^T\\alpha_{ij}\\mathbf{h}_j\\]</span></p>\n<img src=\"/3dc22f96/seq2seq_attention.png\" class title=\"seq2seq attention\">\n<p>attention netdecoderhidden\nstateencoder hidden\nstate</p>\n<p>attentionattention</p>\n<img src=\"/3dc22f96/attention_calculation.png\" class title=\"attention calculation\">\n<p>attention <span class=\"math inline\">\\(\\mathrm{Attention}(s, h)=\\mathrm{Score}(s,h)\\cdot\nh\\)</span>  <span class=\"math inline\">\\(s\\)</span>\ndecoderhidden state <span class=\"math inline\">\\(y\\)</span> <span class=\"math inline\">\\(h\\)</span> encoderhidden state</p>\n<p>scaled dot-product\nattention</p>\n<h2 id=\"transformerattention\">Transformerattention</h2>\n<p>RNN attentiontransformer\nattentionAttention Is All You\nNeedRNNtime\nstepattentionhidden\nstateattention</p>\n<img src=\"/3dc22f96/transformer_structure.png\" class title=\"transformer structure.png\">\n<p>encoderdecoderencoderattentionself-attentiondecoderself-attentioncross-attention</p>\n<p>transformerattention <span class=\"math inline\">\\(\\mathrm{Attention}(Q,K,V)=\\mathrm{Score}(Q,K)V\\)</span>\n<span class=\"math inline\">\\(Q=W_{Q}YK=W_{K}XV=W_{V}X\\)</span>\ncross-attention <span class=\"math inline\">\\(X\\)</span>\nencoderhidden states<span class=\"math inline\">\\(Y\\)</span>\ndecoderhidden statesself-attention <span class=\"math inline\">\\(X=Y\\)</span></p>\n<p>scaled dot-product attentionsoftmax</p>\n<p><span class=\"math display\">\\[\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d}})V\\]</span></p>\n<p>attention</p>\n<p></p>\n<img src=\"/3dc22f96/Scaled-dot-product-self-attention.pbm\" class title=\"self-attention\">\n<p>querykeyvalueattention</p>\n<p>+</p>\n<p>-30keyvalue</p>\n<p>30querykey5</p>\n<p>top5 <span class=\"math inline\">\\([8,4,4,2,2]\\)</span>  <span class=\"math inline\">\\([5\\text{w},2\\text{w},8\\text{w},3\\text{w},6\\text{w}]\\)</span>\n</p>\n<p>1 <span class=\"math inline\">\\([0.4,0.2,0.2,0.1,0.1]\\)</span>\n30 <span class=\"math inline\">\\(0.4\\times5+0.2\\times2+0.2\\times8+0.1\\times3+0.1\\times6=4.9\\text{w}\\)</span>\n</p>\n<p>transformer attention <span class=\"math inline\">\\(QK^T\\)</span>\nsoftmax/</p>\n<p>self-attention <span class=\"math inline\">\\(QKV\\)</span>\n <span class=\"math inline\">\\(X\\)</span>sequencetokencross-attentiondecodersequence</p>\n<p>self-attention <span class=\"math inline\">\\(QKV\\)</span>\n <span class=\"math inline\">\\(X\\)</span>  <span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(QK^T\\)</span>\nMHA</p>\n<p>attentionMHAMQAGQA</p>\n<p><a href=\"https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention\">pytorch\nforcasting</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ScaledDotProductAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, dropout: <span class=\"built_in\">float</span> = <span class=\"literal\">None</span>, scale: <span class=\"built_in\">bool</span> = <span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.dropout = dropout</span><br><span class=\"line\">        self.softmax = nn.Softmax(dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">        self.scale = scale</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, q, k, v, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        attn = torch.bmm(q, k.permute(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>))  <span class=\"comment\"># query-key overlap</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.scale:</span><br><span class=\"line\">            dimension = torch.as_tensor(k.size(-<span class=\"number\">1</span>), dtype=attn.dtype, device=attn.device).sqrt()</span><br><span class=\"line\">            attn = attn / dimension</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = attn.masked_fill(mask, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">        attn = self.softmax(attn)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            attn = self.dropout(attn)</span><br><span class=\"line\">        output = torch.bmm(attn, v)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, attn</span><br></pre></td></tr></table></figure>\n<h2 id=\"scaling\">scaling</h2>\n<p>BTW <span class=\"math inline\">\\(QK^T\\)</span>\n <span class=\"math inline\">\\(\\sqrt{d}\\)</span> </p>\n<p>softmaxsoftmax</p>\n<img src=\"/3dc22f96/softmax.png\" class title=\"softmax\">\n<p><a href=\"https://spaces.ac.cn/archives/8620\"></a>attentionscaling\n<span class=\"math inline\">\\(\\sqrt{d}\\)</span>\nsoftmaxnormalizationattentionscaling</p>\n<h1 id=\"mha\">MHA</h1>\n<p>attentionMHAmulti-head\nattention</p>\n<p>MHA2017Attention Is All You\nNeedattentionattention</p>\n<p><span class=\"math display\">\\[\\mathrm{MultiHeadAttention}(Q,K,V)=\\mathrm{Concat}(head_1,\\ldots,head_h)\\]</span></p>\n<p><span class=\"math display\">\\[head_i=\\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)\\]</span></p>\n<p>hidden size <span class=\"math inline\">\\(d\\)</span>\nMHA <span class=\"math inline\">\\(QKV\\)</span>\nhidden state <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span>  <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(QKV\\)</span>\nattention <span class=\"math inline\">\\(head_{num}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span> concat</p>\n<p>amazing</p>\n<img src=\"/3dc22f96/multihead_attention.png\" class title=\"MHA\">\n<p></p>\n<p>Attention Is All You Need</p>\n<blockquote>\n<p>Multi-head attention allows the model to jointly attend to\ninformation from different representation subspaces at different\npositions.</p>\n</blockquote>\n<p>attention\nheadattention\nhead</p>\n<p>CNN <span class=\"math inline\">\\(3\\times3\\times128\\)</span> 128 <span class=\"math inline\">\\(3\\times3\\)</span>\n <span class=\"math inline\">\\(3\\times3\\)</span> </p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\\\1&amp;0&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\left.\\left[\\begin{matrix}1&amp;1&amp;1\\\\0&amp;0&amp;0\\\\-1&amp;-1&amp;-1\\end{matrix}\\right.\\right]\\]</span></p>\n<p></p>\n<p>128 <span class=\"math inline\">\\(3\\times3\\)</span>\n128MHA</p>\n<p>expect</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/626820422\"></a>12\n<span class=\"math inline\">\\(QK^T\\)</span> </p>\n<p>MHAattentionattention</p>\n<p></p>\n<p><a href=\"https://arxiv.org/pdf/1905.10650.pdf\">Are Sixteen Heads Really\nBetter than One?</a>MHA</p>\n<p>hidden\nsizeLLM1216244896</p>\n<p><a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\">The\nAnnotated Transformer</a>MHA</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">attention</span>(<span class=\"params\">query, key, value, mask=<span class=\"literal\">None</span>, dropout=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class=\"line\">    d_k = query.size(-<span class=\"number\">1</span>)</span><br><span class=\"line\">    scores = torch.matmul(query, key.transpose(-<span class=\"number\">2</span>, -<span class=\"number\">1</span>)) \\</span><br><span class=\"line\">             / math.sqrt(d_k)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        scores = scores.masked_fill(mask == <span class=\"number\">0</span>, -<span class=\"number\">1e9</span>)</span><br><span class=\"line\">    p_attn = F.softmax(scores, dim = -<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> dropout <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        p_attn = dropout(p_attn)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultiHeadedAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, h, d_model, dropout=<span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        h: head number</span></span><br><span class=\"line\"><span class=\"string\">        &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> d_model % h == <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"comment\"># We assume d_v always equals d</span></span><br><span class=\"line\">        self.d = d_model // h</span><br><span class=\"line\">        self.h = h</span><br><span class=\"line\">        self.linears = clones(nn.Linear(d_model, d_model), <span class=\"number\">4</span>)</span><br><span class=\"line\">        self.attn = <span class=\"literal\">None</span></span><br><span class=\"line\">        self.dropout = nn.Dropout(p=dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, query, key, value, mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"comment\"># Same mask applied to all h heads.</span></span><br><span class=\"line\">            mask = mask.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">        nbatches = query.size(<span class=\"number\">0</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 1) Do all the linear projections in batch from d_model =&gt; h x d </span></span><br><span class=\"line\">        query, key, value = \\</span><br><span class=\"line\">            [l(x).view(nbatches, -<span class=\"number\">1</span>, self.h, self.d).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">             <span class=\"keyword\">for</span> l, x <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(self.linears, (query, key, value))]</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class=\"line\">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class=\"line\">                                 dropout=self.dropout)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class=\"line\">        x = x.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous() \\</span><br><span class=\"line\">             .view(nbatches, -<span class=\"number\">1</span>, self.h * self.d)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.linears[-<span class=\"number\">1</span>](x)</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://github.com/huggingface/transformers\">transformers</a></p>\n<h1 id=\"kv-cache\">KV Cache</h1>\n<p>MQAGQAKV\nCache</p>\n<p>encoder-decoderAGIdecoder-onlyLLMauto-regressive</p>\n<p> <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{input}_{i-1}\\)</span> \n<span class=\"math inline\">\\(\\text{input}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{input}_{i}\\)</span>  <span class=\"math inline\">\\(\\text{token}_{i+1}\\)</span>\n</p>\n<p>tokentoken</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">step0: =[BOS]=</span><br><span class=\"line\">step1: =[BOS]=</span><br><span class=\"line\">step2: =[BOS]=</span><br><span class=\"line\">step3: =[BOS]=</span><br><span class=\"line\">step4: =[BOS]=</span><br><span class=\"line\">step5: =[BOS]=[EOS]</span><br></pre></td></tr></table></figure>\n<p>[BOS][EOS]</p>\n<p>hidden\nstate</p>\n<p>stepsteptokenstepstep</p>\n<p></p>\n<p>attention</p>\n<p><span class=\"math display\">\\[\n\\alpha_{i,j}=\\text{softmax}(q_{i}k_{j}^\\top)\\\\\no_{i}=\\sum_{j=0}^{i}{\\alpha_{i,j}v_{j}}\n\\]</span></p>\n<p>decodermask\nattention</p>\n<p>34attention</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>45attention</p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\no_{0}&amp;=\\alpha_{0,0}v_{0}\\\\\no_{1}&amp;=\\alpha_{1,0}v_{0}+\\alpha_{1,1}v_{1}\\\\\no_{2}&amp;=\\alpha_{2,0}v_{0}+\\alpha_{2,1}v_{1}+\\alpha_{2,2}v_{2}\\\\\no_{3}&amp;=\\alpha_{3,0}v_{0}+\\alpha_{3,1}v_{1}+\\alpha_{3,2}v_{2}+\\alpha_{3,3}v_{3}\\\\\n\\end{aligned}\n\\]</span></p>\n<p>5 <span class=\"math inline\">\\(o_{0}\\)</span>  <span class=\"math inline\">\\(o_{2}\\)</span> </p>\n<p></p>\n<p></p>\n<p>step0101step515instruction800stepstep0800step1799...</p>\n<p>step</p>\n<p>KV\nCache</p>\n<p> <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p>34\n<span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=\\text{None}\\\\\n\\]</span></p>\n<center>\n\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<p>kv_cache <span class=\"math inline\">\\(l\\)</span>\n</p>\n<p>5 <span class=\"math inline\">\\(l\\)</span>\n<u><strong></strong></u> <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span>  <span class=\"math inline\">\\(o_{3}\\)</span>  <span class=\"math inline\">\\(o_{0}o_{1}o_{2}\\)</span> </p>\n<p> <span class=\"math inline\">\\(l\\)</span>  <span class=\"math inline\">\\(o_{0}o_{1}o_{2}\\)</span>\nFNN <span class=\"math inline\">\\(l+1\\)</span>\n <span class=\"math inline\">\\(l+1\\)</span>\n <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span>  <span class=\"math inline\">\\(l+1\\)</span>  <span class=\"math inline\">\\(k\\)</span>  <span class=\"math inline\">\\(v\\)</span> </p>\n<p> <span class=\"math inline\">\\(k\\)</span> \n<span class=\"math inline\">\\(v\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]\n\\]</span></p>\n<center>\n\n</center>\n<p><span class=\"math display\">\\[\n\\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},\nv_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]\n\\]</span></p>\n<p>attentionFFN</p>\n<p>transformersuse_cache=TrueKV Cache</p>\n<p>GPT2</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Class GPT2Attention(nn.Module):</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">        self,</span></span><br><span class=\"line\"><span class=\"params\">        hidden_states: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.FloatTensor]],</span></span><br><span class=\"line\"><span class=\"params\">        layer_past: <span class=\"type\">Optional</span>[<span class=\"type\">Tuple</span>[torch.Tensor]] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        head_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_hidden_states: <span class=\"type\">Optional</span>[torch.Tensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        encoder_attention_mask: <span class=\"type\">Optional</span>[torch.FloatTensor] = <span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">        use_cache: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">        output_attentions: <span class=\"type\">Optional</span>[<span class=\"built_in\">bool</span>] = <span class=\"literal\">False</span>,</span></span><br><span class=\"line\"><span class=\"params\">    </span>) -&gt; <span class=\"type\">Tuple</span>[<span class=\"type\">Union</span>[torch.Tensor, <span class=\"type\">Tuple</span>[torch.Tensor]], ...]:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> encoder_hidden_states <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">hasattr</span>(self, <span class=\"string\">&quot;q_attn&quot;</span>):</span><br><span class=\"line\">                <span class=\"keyword\">raise</span> ValueError(</span><br><span class=\"line\">                    <span class=\"string\">&quot;If class is used as cross attention, the weights `q_attn` have to be defined. &quot;</span></span><br><span class=\"line\">                    <span class=\"string\">&quot;Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.&quot;</span></span><br><span class=\"line\">                )</span><br><span class=\"line\"></span><br><span class=\"line\">            query = self.q_attn(hidden_states)</span><br><span class=\"line\">            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">            attention_mask = encoder_attention_mask</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        query = self._split_heads(query, self.num_heads, self.head_dim)</span><br><span class=\"line\">        key = self._split_heads(key, self.num_heads, self.head_dim)</span><br><span class=\"line\">        value = self._split_heads(value, self.num_heads, self.head_dim)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># </span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> layer_past <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            past_key, past_value = layer_past</span><br><span class=\"line\">            key = torch.cat((past_key, key), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># key</span></span><br><span class=\"line\">            value = torch.cat((past_value, value), dim=-<span class=\"number\">2</span>)  <span class=\"comment\"># value</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> use_cache <span class=\"keyword\">is</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">            present = (key, value)  <span class=\"comment\"># </span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            present = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.reorder_and_upcast_attn:</span><br><span class=\"line\">            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)</span><br><span class=\"line\">        attn_output = self.c_proj(attn_output)</span><br><span class=\"line\">        attn_output = self.resid_dropout(attn_output)</span><br><span class=\"line\"></span><br><span class=\"line\">        outputs = (attn_output, present)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> output_attentions:</span><br><span class=\"line\">            outputs += (attn_weights,)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> outputs  <span class=\"comment\"># a, present, (attentions)</span></span><br></pre></td></tr></table></figure>\n<p>KV\nCachedecodermask\nattentiontokentoken</p>\n<p>KV Cache</p>\n<p> <span class=\"math inline\">\\(s\\)</span>\n <span class=\"math inline\">\\(L\\)</span> hidden size <span class=\"math inline\">\\(d\\)</span> </p>\n<p><span class=\"math display\">\\[\n2\\times L\\times s\\times d\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n2\\times 2\\times L\\times s\\times d\n\\]</span></p>\n<p>Llama2 7B <span class=\"math inline\">\\(L=32\\)</span> \n<span class=\"math inline\">\\(L=4096\\)</span>\ntoken524,288 bytes52K <span class=\"math inline\">\\(s=1024\\)</span> 536,870,912\nbytes500M</p>\n<p>batch size=1batch\nsize1G</p>\n<p>MHA <span class=\"math inline\">\\(qkv\\)</span>\n</p>\n<p></p>\n<img src=\"/3dc22f96/gpu_cache.png\" class title=\"gpu cache\">\n<p>H10050ML2 CacheL1\nCacheLlama2\n7B100token</p>\n<p>LLM34B/70B</p>\n<p>L2 CacheHBML2\nCache</p>\n<img src=\"/3dc22f96/sram_dram.png\" class title=\"\">\n<p></p>\n<p>Cache</p>\n<p></p>\n<h1 id=\"mqa\">MQA</h1>\n<p>MQA</p>\n<p>Google2019Fast Transformer Decoding: One Write-Head is All\nYou\nNeedMQABert</p>\n<p>MQAMHA <span class=\"math inline\">\\(W_{Q}W_{K}W_{V}\\)</span>\nnn= <span class=\"math inline\">\\(d_{model}\\)</span>  <span class=\"math inline\">\\(d_{head}\\)</span>\nattentionMQA <span class=\"math inline\">\\(Q\\)</span> MHA <span class=\"math inline\">\\(KV\\)</span> \n<span class=\"math inline\">\\(d_{head}\\)</span>\nnQuery <span class=\"math inline\">\\(KV\\)</span>\nattention</p>\n<p>MHA <span class=\"math inline\">\\(KV\\)</span>\nMQA <span class=\"math inline\">\\(KV\\)</span>\nMHA</p>\n<img src=\"/3dc22f96/MQA.webp\" class title=\"MQA\">\n<p> <span class=\"math inline\">\\(KV\\)</span>\n</p>\n<p>Llama2\n7B32MQA1024token1/32536,870,912\nbytes / 32 = 16,777,216 bytes16M</p>\n<p> <span class=\"math inline\">\\(KV\\)</span>\n</p>\n<p>MQAMHAhidden\nsizehead num</p>\n<img src=\"/3dc22f96/mqa_result_1.png\" class title=\"MQA results 1\">\n<img src=\"/3dc22f96/mqa_result_3.png\" class title=\"MQA results 3\">\n<h1 id=\"gqa\">GQA</h1>\n<p>MQAMHAGQAGrouped-Query\nAttentionMQAMHA</p>\n<p>GQA: Training Generalized Multi-Query Transformer Models\nfrom Multi-Head Checkpoints2023</p>\n<p>GQA <span class=\"math inline\">\\(Q\\)</span>\nMHA/MQA <span class=\"math inline\">\\(KV\\)</span>\n <span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(Q\\)</span> groupgroup\n<span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(KV\\)</span> group <span class=\"math inline\">\\(Q\\)</span>  <span class=\"math inline\">\\(KV\\)</span> </p>\n<p>MHA <span class=\"math inline\">\\(KV\\)</span>\nGQAMQA <span class=\"math inline\">\\(KV\\)</span> GQA</p>\n<p></p>\n<img src=\"/3dc22f96/GQA.png\" class title=\"GQA\">\n<p></p>\n<img src=\"/3dc22f96/GQA_result_1.png\" class title=\"GQA result\">\n<p>2/3/4GQAMHAMQAMHAMQAGQAaverage\npoolingMHAMHAGQA</p>\n<p>Llama2GQAtech\nreportMHAMQAGQA</p>\n<img src=\"/3dc22f96/llama2_qga.png\" class title=\"llama2 GQA result\">\n<h1 id=\"\"></h1>\n<p>MHAMQAGQA</p>\n<p>GQALLM</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1The Annotated Transformer\nhttps://nlp.seas.harvard.edu/2018/04/03/attention.html<br>\n2Attention Is All You Need\nhttps://arxiv.org/pdf/1706.03762.pdf<br>\n3Fast Transformer Decoding: One Write-Head is All You Need\nhttps://arxiv.org/pdf/1911.02150.pdf<br>\n4https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096<br>\n5GQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf<br>\n6How Attention works in Deep Learning: understanding the attention\nmechanism in sequence models https://theaisummer.com/attention/<br>\n7A simple overview of RNN, LSTM and Attention Mechanism\nhttps://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b<br>\n8https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention<br>\n9Transformer\nhttps://spaces.ac.cn/archives/8620<br>\n10https://theaisummer.com/self-attention/\nhttps://theaisummer.com/self-attention/<br>\n11https://zhuanlan.zhihu.com/p/626820422\nhttps://zhuanlan.zhihu.com/p/626820422<br>\n12Are Sixteen Heads Really Better than One?\nhttps://arxiv.org/pdf/1905.10650.pdf<br>\n13This post is all you needTransformer\nhttps://zhuanlan.zhihu.com/p/420820453<br>\n14The Illustrated Transformer\nhttps://jalammar.github.io/illustrated-transformer/<br>\n15Multi-Query Attention is All You Need\nhttps://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055</p>\n"},{"title":"-DPO","abbrlink":"473f2b43","date":"2024-05-26T14:01:48.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\nSFT  \n\nChatGPTSFT+RLHF PPOPPOPPOrewardPPOPPO  \n\nDPODirect Preference OptimizationPPODPOrewardPPO  \n\nDPO  \n\n#   \n\n  \n\n80%  \n\n  \n\n  \n\nresponseactionAI  \n\nSFTRLHF/RLAIFRLHF  \n\nDPORLHF  \n\n# RLHF\n\nRLHF  \n\n1. SFT Phase  \n\n $\\pi^{\\mathrm{SFT}}$  \n\n2. Reward Modelling Phase  \n\nprompt $x$ $(y_1,y_2)\\sim\\pi^\\text{SFT}(y|x)$ $y_1,y_2$(preference) $y_w\\succ y_l\\mid x$wlwinlose  \n\nlatent reward model $r^*(y,x)$ $(x,y)$  $r^*(y,x)$ RLHFreward model  \n\n $r^*(y,x)$preferenceBradley-Terry modelranked answersPlackett-Luce ranking models  \n\nBradley-Terry model $p^{*}$   \n\n$$\\begin{aligned}p^*(y_1\\succ y_2\\mid x)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}$$  \n\nrewardsoftmax  \n\n $p^{*}$  $\\mathcal{D}=\\left\\{x^{(i)},y_w^{(i)},y_l^{(i)}\\right\\}_{i=1}^N$  $\\pi^{\\mathrm{SFT}}$ reward $r_\\phi(x,y)$maximum likelihood $r^*(y,x)$negative log-likelihood loss  \n\n$$\\mathcal{L}_R(r_\\phi,\\mathcal{D})=-\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\begin{bmatrix}\\log\\sigma(r_\\phi(x,y_w)-r_\\phi(x,y_l))\\end{bmatrix}$$  \n\nreward functionreward $x$ $\\mathbb{E}_{x,y\\thicksim\\mathcal{D}}\\left[r_\\phi(x,y)\\right]=0$  \n\n3. RL Fine-Tuning Phase  \n\nreward  \n\n$$\\max_{\\pi_\\theta}\\mathbb{E}_{x\\sim\\mathcal{D},y\\sim\\pi_\\theta(y|x)}\\begin{bmatrix}r_\\phi(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi_\\theta(y\\mid x)\\mid\\mid\\pi_{\\mathrm{ref}}(y\\mid x)\\end{bmatrix}$$  \n\nrewardRLHFactor modelreward  \n\nKL $\\pi^{\\mathrm{SFT}}$rewardrewardmode-collapse$\\beta$   \n\nRL  \n\nRLreward fucntion  \n\n$$r(x,y)=r_\\phi(x,y)-\\beta(\\log\\pi_\\theta(y\\mid x)-\\log\\pi_\\text{ref}(y\\mid x))$$  \n\nPPO  \n\n# Direct Preference Optimization  \n\nDPOpolicy optimizationreward  \n\n{% asset_img intro.png DPO %}  \n\n## DPO  \n\nDPORLreward function $r(x,y)$reference model $\\pi_{\\mathrm{ref}}$  \n\n$$\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi(y|x)||\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}$$  \n\nKL  \n\n$$\\beta\\mathbb{D}_{\\mathrm{KL}}(\\pi\\|\\pi_{\\mathrm{ref}})=\\beta\\sum_y\\pi(y|x)\\log\\frac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}$$  \n\n  \n\n$$\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathbf{KL}}\\begin{bmatrix}\\pi(y|x)&\\mid\\mid\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}$$  \n\n$$\\begin{aligned}&=\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}$$  \n\n  \n\n$$\\begin{aligned}\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}-\\frac{1}{\\beta}r(x,y)\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}$$  \n\n  \n\n$$Z(x)=\\sum_y\\pi_\\text{ref}(y|x)\\exp\\left(\\frac1\\beta r(x,y)\\right)$$  \n\n  \n\n$$\\begin{aligned}\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\n  \n\n$$\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}-\\log Z(x)\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log Z(x)\\right]\\end{aligned}$$  \n\n $Z(x)$  $y$   \n\n$$\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log Z(x)\\right]\\end{aligned}$$  \n\n$$=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{E}_{y\\thicksim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}\\right]-\\log Z(x)\\right]$$  \n\n$$=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{D}_{\\text{KL}}(\\pi(y|x)\\mid\\mid\\pi^*(y|x))-\\log Z(x)\\right]$$  \n\n$Z(x)$  $\\pi$ KLKL0  \n\n$$\\begin{aligned}\\pi(y|x)=\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\n $Z(x)$   \n\n  \n\n$$\\begin{aligned}\\pi_r(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\n$$\\begin{aligned}\n\\log Z(x)+\\log \\pi_r(y|x)=\\log \\pi_{\\text{ref}}(y|x) +\\frac{1}{\\beta}r(x,y)\n\\end{aligned}$$  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\nBradley-Terry modelBradley-Terry model  \n\n$$\\begin{aligned}p^*(y_1\\succ y_2\\mid x)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}$$  \n\n  \n\n$$\\begin{aligned}\np^*(y_1\\succ y_2\\mid x)&=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\\\\n&=\\frac1{1+\\frac{\\exp(r^*(x,y_2))}{\\exp(r^*(x,y_1))}}\\\\\n&=\\frac1{1+\\exp(r^*(x,y_2)-r^*(x,y_1))}\n\\end{aligned}$$  \n\n $r$   \n\n$$p^*(y_1\\succ y_2\\mid x)=\\frac{1}{1+\\exp\\left(\\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{\\text{ref}}(y_2|x)}-\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{\\text{ref}}(y_1|x)}\\right)}$$  \n\noptimal policyrewardMLE  \n\n$$\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid x)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid x)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]$$  \n\nDPO loss  \n\n{% asset_img dpo_loss_code.png DPO %}  \n\n## DPO  \n\nDPOlossDPO  \n\n  \n\n$$u=\\beta\\log\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}-\\beta\\log\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\mathrm{ref}}(y_{l}|x)}$$  \n\n  \n\n$$L_{DPO}(\\pi_{\\theta};\\pi_{\\mathrm{ref}})=-\\min_{\\pi_{0}}E_{(x,y_{u},y_{t})\\sim D}[\\log\\sigma(u)]$$  \n\nsigmoid  \n\n$$\\frac\\partial{\\partial u}\\log\\sigma(u)=\\frac1{\\sigma(u)}\\cdot\\sigma(u)(1-\\sigma(u))=1-\\sigma(u)$$  \n\nsigmoid  \n\n$$1-\\sigma(u)=\\sigma(-u)$$  \n\n $u$   \n\n$$\\frac{\\partial u}{\\partial\\theta}=\\beta\\left(\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\mathrm{ref}}(y_w|x)}-\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}\\right)$$  \n\n $\\pi_{\\mathrm{ref}}$  $\\theta$  \n\n$$\\begin{aligned}\n\\frac\\partial{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_\\mathrm{ref}(y_w|x)}=&\\frac{1}{\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}}\\cdot\\frac{\\partial}{\\partial\\theta}\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}\\\\\n=&\\frac{1}{\\pi_{\\theta}(y_{w}|x)}\\cdot\\frac{\\partial}{\\partial\\theta}\\pi_{\\theta}(y_{w}|x)\\\\\n=&\\begin{aligned}\\nabla_\\theta\\log\\pi(y_w\\mid x)\\end{aligned}\n\\end{aligned}$$  \n\n  \n\n$$\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}=\\nabla_\\theta\\log\\pi(y_l\\mid x)$$  \n\nDPO  \n\n$$\\begin{aligned}\n&\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\beta\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}-\\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid x)\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}$$  \n\n  \n\n$$\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}$$  \n\nDPO  \n\n$$\\begin{aligned}\n&\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&=-\\beta\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\sigma\\left(\\hat{r}_\\theta(x,y_l)-\\hat{r}_\\theta(x,y_w)\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid x)\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}$$  \n\n  \n\n{% asset_img gradient.png DPO %}  \n\n$\\hat{r}_\\theta(x,y)$  $\\pi_{\\theta}$  $\\pi_{\\mathrm{ref}}$ reward  \n\n## DPO  \n\nDPO  \n- prompt $x$ $y_1,y_2\\sim\\pi_{\\text{ref}}(\\cdot\\mid x)$ $\\mathcal{D}=\\{x^{(i)},y_w^{(i)},y_l)^{(i)}\\}_{i=1}^N$  \n-  $\\mathcal{L}_{\\mathrm{DPO}}$ $\\pi_{\\mathrm{ref}}$$\\mathcal{D}$  $\\beta$  $\\pi\\theta $  \n\n  \n\n $\\pi^{\\mathrm{SFT}}$  $\\pi_{\\mathrm{ref}}=\\pi^{\\mathrm{SFT}}$ $(x,y_w)$  $\\pi_{\\mathrm{ref}}$   \n\n$$\\pi_{\\text{ref}}=\\arg\\max_\\pi\\mathbb{E}_{x,y_w\\thicksim\\mathcal{D}}\\left[\\log\\pi(y_w\\mid x)\\right]$$  \n\n $\\pi_{\\mathrm{ref}}$  reference distribution distribution shift  \n\n## Your Language Model Is Secretly a Reward Model  \n\nDPOlossreward  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\n $Z(x)$   \n\n\"Plackett-Luce/Bradley-Terryreward functionpreference distribution\"  \n\n> Under the Plackett-Luce preference framework, and in particular the BradleyTerry framework, two reward functions from the same equivalence class induce the same preference distribution  \n\nreward function $r(x,y)$  $r^{\\prime}(x,y)$   \n\n$$r'(x,y)=r(x,y)+f(x)$$  \n\nreward function(equivalence class)  \n\nprompt $x$  answer $y_1,\\ldots,y_K$ranking $\\tau$Plackett-Luce frameworkBradleyTerry  \n\n$$\\begin{aligned}\np_{r'}(\\tau|y_1,\\ldots,y_K,x)& =\\prod_{k=1}^K\\frac{\\exp(r'(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r'(x,y_{\\tau(j)}))}  \\\\\n&=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)})+f(x))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)})+f(x))} \\\\\n&=\\prod_{k=1}^K\\frac{\\exp(f(x))\\exp(r(x,y_{\\tau(k)}))}{\\exp(f(x))\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))} \\\\\n&=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))} \\\\\n&=p_r(\\tau|y_1,\\ldots,y_K,x)\n\\end{aligned}$$  \n\n $\\beta\\log Z(x)$ reward functionpreference distribution  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\n$$\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}$$  \n\nreward functionRLoptimal policy  \n\nDPOlossoptimal policy  \n\n$$\\begin{aligned}\\pi(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\nreward functionoptimal policy$r'(x,y)=r(x,y)+f(x)$$\\pi_r$  $\\pi_{r'}$ optimal policy  \n\n$$\\begin{aligned}\n\\pi_{r^{\\prime}}(y|x)& \\begin{aligned}&=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r'(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r'(x,y)\\right)\\end{aligned}  \\\\\n&=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right) \\\\\n&\\begin{aligned}=\\frac{1}{\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\end{aligned} \\\\\n&\\begin{aligned}&=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned} \\\\\n&=\\pi_r(y|x)\n\\end{aligned}$$  \n\nPlackett-LuceBradley-Terryreward $\\pi(y\\mid x)$  reference model $\\pi_{ref}(y\\mid x)$   \n\n$$r(x,y)=\\beta\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}$$  \n\nreward model  \n\n##   \n\n  \n- $\\beta=0.1$TL;DR summarization0.5  \n- batch size = 64  \n- RMSprop optimizer  \n- learning rate = 1e-6  \n- linearly warmup 0 to 1e-6 over 150 steps  \n\nPPOSFTDPO  \n\nDPO  \n\n{% asset_img result_1.png 1 %}  \n\n{% asset_img result_2.png 2 %}  \n\n{% asset_img result_3.png 3 %}  \n\n{% asset_img result_4.png 4 %}  \n\n#   \n\n- DPORLHF PPOreward  \n- DPOPPO  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n1Direct Preference Optimization: Your Language Model is Secretly a Reward Model https://arxiv.org/abs/2305.18290v2  ","source":"_posts/cs/nlp/2024/05/-DPO.md","raw":"---\ntitle: -DPO\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \n  - SFT\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 473f2b43\ndate: 2024-05-26 22:01:48\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\nSFT  \n\nChatGPTSFT+RLHF PPOPPOPPOrewardPPOPPO  \n\nDPODirect Preference OptimizationPPODPOrewardPPO  \n\nDPO  \n\n#   \n\n  \n\n80%  \n\n  \n\n  \n\nresponseactionAI  \n\nSFTRLHF/RLAIFRLHF  \n\nDPORLHF  \n\n# RLHF\n\nRLHF  \n\n1. SFT Phase  \n\n $\\pi^{\\mathrm{SFT}}$  \n\n2. Reward Modelling Phase  \n\nprompt $x$ $(y_1,y_2)\\sim\\pi^\\text{SFT}(y|x)$ $y_1,y_2$(preference) $y_w\\succ y_l\\mid x$wlwinlose  \n\nlatent reward model $r^*(y,x)$ $(x,y)$  $r^*(y,x)$ RLHFreward model  \n\n $r^*(y,x)$preferenceBradley-Terry modelranked answersPlackett-Luce ranking models  \n\nBradley-Terry model $p^{*}$   \n\n$$\\begin{aligned}p^*(y_1\\succ y_2\\mid x)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}$$  \n\nrewardsoftmax  \n\n $p^{*}$  $\\mathcal{D}=\\left\\{x^{(i)},y_w^{(i)},y_l^{(i)}\\right\\}_{i=1}^N$  $\\pi^{\\mathrm{SFT}}$ reward $r_\\phi(x,y)$maximum likelihood $r^*(y,x)$negative log-likelihood loss  \n\n$$\\mathcal{L}_R(r_\\phi,\\mathcal{D})=-\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\begin{bmatrix}\\log\\sigma(r_\\phi(x,y_w)-r_\\phi(x,y_l))\\end{bmatrix}$$  \n\nreward functionreward $x$ $\\mathbb{E}_{x,y\\thicksim\\mathcal{D}}\\left[r_\\phi(x,y)\\right]=0$  \n\n3. RL Fine-Tuning Phase  \n\nreward  \n\n$$\\max_{\\pi_\\theta}\\mathbb{E}_{x\\sim\\mathcal{D},y\\sim\\pi_\\theta(y|x)}\\begin{bmatrix}r_\\phi(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi_\\theta(y\\mid x)\\mid\\mid\\pi_{\\mathrm{ref}}(y\\mid x)\\end{bmatrix}$$  \n\nrewardRLHFactor modelreward  \n\nKL $\\pi^{\\mathrm{SFT}}$rewardrewardmode-collapse$\\beta$   \n\nRL  \n\nRLreward fucntion  \n\n$$r(x,y)=r_\\phi(x,y)-\\beta(\\log\\pi_\\theta(y\\mid x)-\\log\\pi_\\text{ref}(y\\mid x))$$  \n\nPPO  \n\n# Direct Preference Optimization  \n\nDPOpolicy optimizationreward  \n\n{% asset_img intro.png DPO %}  \n\n## DPO  \n\nDPORLreward function $r(x,y)$reference model $\\pi_{\\mathrm{ref}}$  \n\n$$\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi(y|x)||\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}$$  \n\nKL  \n\n$$\\beta\\mathbb{D}_{\\mathrm{KL}}(\\pi\\|\\pi_{\\mathrm{ref}})=\\beta\\sum_y\\pi(y|x)\\log\\frac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}$$  \n\n  \n\n$$\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathbf{KL}}\\begin{bmatrix}\\pi(y|x)&\\mid\\mid\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}$$  \n\n$$\\begin{aligned}&=\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}$$  \n\n  \n\n$$\\begin{aligned}\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}-\\frac{1}{\\beta}r(x,y)\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}$$  \n\n  \n\n$$Z(x)=\\sum_y\\pi_\\text{ref}(y|x)\\exp\\left(\\frac1\\beta r(x,y)\\right)$$  \n\n  \n\n$$\\begin{aligned}\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\n  \n\n$$\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}-\\log Z(x)\\right]\\end{aligned}$$  \n\n$$\\begin{aligned}&=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log Z(x)\\right]\\end{aligned}$$  \n\n $Z(x)$  $y$   \n\n$$\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log Z(x)\\right]\\end{aligned}$$  \n\n$$=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{E}_{y\\thicksim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}\\right]-\\log Z(x)\\right]$$  \n\n$$=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{D}_{\\text{KL}}(\\pi(y|x)\\mid\\mid\\pi^*(y|x))-\\log Z(x)\\right]$$  \n\n$Z(x)$  $\\pi$ KLKL0  \n\n$$\\begin{aligned}\\pi(y|x)=\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\n $Z(x)$   \n\n  \n\n$$\\begin{aligned}\\pi_r(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\n$$\\begin{aligned}\n\\log Z(x)+\\log \\pi_r(y|x)=\\log \\pi_{\\text{ref}}(y|x) +\\frac{1}{\\beta}r(x,y)\n\\end{aligned}$$  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\nBradley-Terry modelBradley-Terry model  \n\n$$\\begin{aligned}p^*(y_1\\succ y_2\\mid x)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}$$  \n\n  \n\n$$\\begin{aligned}\np^*(y_1\\succ y_2\\mid x)&=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\\\\n&=\\frac1{1+\\frac{\\exp(r^*(x,y_2))}{\\exp(r^*(x,y_1))}}\\\\\n&=\\frac1{1+\\exp(r^*(x,y_2)-r^*(x,y_1))}\n\\end{aligned}$$  \n\n $r$   \n\n$$p^*(y_1\\succ y_2\\mid x)=\\frac{1}{1+\\exp\\left(\\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{\\text{ref}}(y_2|x)}-\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{\\text{ref}}(y_1|x)}\\right)}$$  \n\noptimal policyrewardMLE  \n\n$$\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid x)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid x)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]$$  \n\nDPO loss  \n\n{% asset_img dpo_loss_code.png DPO %}  \n\n## DPO  \n\nDPOlossDPO  \n\n  \n\n$$u=\\beta\\log\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}-\\beta\\log\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\mathrm{ref}}(y_{l}|x)}$$  \n\n  \n\n$$L_{DPO}(\\pi_{\\theta};\\pi_{\\mathrm{ref}})=-\\min_{\\pi_{0}}E_{(x,y_{u},y_{t})\\sim D}[\\log\\sigma(u)]$$  \n\nsigmoid  \n\n$$\\frac\\partial{\\partial u}\\log\\sigma(u)=\\frac1{\\sigma(u)}\\cdot\\sigma(u)(1-\\sigma(u))=1-\\sigma(u)$$  \n\nsigmoid  \n\n$$1-\\sigma(u)=\\sigma(-u)$$  \n\n $u$   \n\n$$\\frac{\\partial u}{\\partial\\theta}=\\beta\\left(\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\mathrm{ref}}(y_w|x)}-\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}\\right)$$  \n\n $\\pi_{\\mathrm{ref}}$  $\\theta$  \n\n$$\\begin{aligned}\n\\frac\\partial{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_\\mathrm{ref}(y_w|x)}=&\\frac{1}{\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}}\\cdot\\frac{\\partial}{\\partial\\theta}\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}\\\\\n=&\\frac{1}{\\pi_{\\theta}(y_{w}|x)}\\cdot\\frac{\\partial}{\\partial\\theta}\\pi_{\\theta}(y_{w}|x)\\\\\n=&\\begin{aligned}\\nabla_\\theta\\log\\pi(y_w\\mid x)\\end{aligned}\n\\end{aligned}$$  \n\n  \n\n$$\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}=\\nabla_\\theta\\log\\pi(y_l\\mid x)$$  \n\nDPO  \n\n$$\\begin{aligned}\n&\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\beta\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}-\\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid x)\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}$$  \n\n  \n\n$$\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}$$  \n\nDPO  \n\n$$\\begin{aligned}\n&\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&=-\\beta\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\sigma\\left(\\hat{r}_\\theta(x,y_l)-\\hat{r}_\\theta(x,y_w)\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid x)\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}$$  \n\n  \n\n{% asset_img gradient.png DPO %}  \n\n$\\hat{r}_\\theta(x,y)$  $\\pi_{\\theta}$  $\\pi_{\\mathrm{ref}}$ reward  \n\n## DPO  \n\nDPO  \n- prompt $x$ $y_1,y_2\\sim\\pi_{\\text{ref}}(\\cdot\\mid x)$ $\\mathcal{D}=\\{x^{(i)},y_w^{(i)},y_l)^{(i)}\\}_{i=1}^N$  \n-  $\\mathcal{L}_{\\mathrm{DPO}}$ $\\pi_{\\mathrm{ref}}$$\\mathcal{D}$  $\\beta$  $\\pi\\theta $  \n\n  \n\n $\\pi^{\\mathrm{SFT}}$  $\\pi_{\\mathrm{ref}}=\\pi^{\\mathrm{SFT}}$ $(x,y_w)$  $\\pi_{\\mathrm{ref}}$   \n\n$$\\pi_{\\text{ref}}=\\arg\\max_\\pi\\mathbb{E}_{x,y_w\\thicksim\\mathcal{D}}\\left[\\log\\pi(y_w\\mid x)\\right]$$  \n\n $\\pi_{\\mathrm{ref}}$  reference distribution distribution shift  \n\n## Your Language Model Is Secretly a Reward Model  \n\nDPOlossreward  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\n $Z(x)$   \n\n\"Plackett-Luce/Bradley-Terryreward functionpreference distribution\"  \n\n> Under the Plackett-Luce preference framework, and in particular the BradleyTerry framework, two reward functions from the same equivalence class induce the same preference distribution  \n\nreward function $r(x,y)$  $r^{\\prime}(x,y)$   \n\n$$r'(x,y)=r(x,y)+f(x)$$  \n\nreward function(equivalence class)  \n\nprompt $x$  answer $y_1,\\ldots,y_K$ranking $\\tau$Plackett-Luce frameworkBradleyTerry  \n\n$$\\begin{aligned}\np_{r'}(\\tau|y_1,\\ldots,y_K,x)& =\\prod_{k=1}^K\\frac{\\exp(r'(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r'(x,y_{\\tau(j)}))}  \\\\\n&=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)})+f(x))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)})+f(x))} \\\\\n&=\\prod_{k=1}^K\\frac{\\exp(f(x))\\exp(r(x,y_{\\tau(k)}))}{\\exp(f(x))\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))} \\\\\n&=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))} \\\\\n&=p_r(\\tau|y_1,\\ldots,y_K,x)\n\\end{aligned}$$  \n\n $\\beta\\log Z(x)$ reward functionpreference distribution  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\n$$\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}$$  \n\nreward functionRLoptimal policy  \n\nDPOlossoptimal policy  \n\n$$\\begin{aligned}\\pi(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}$$  \n\nreward functionoptimal policy$r'(x,y)=r(x,y)+f(x)$$\\pi_r$  $\\pi_{r'}$ optimal policy  \n\n$$\\begin{aligned}\n\\pi_{r^{\\prime}}(y|x)& \\begin{aligned}&=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r'(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r'(x,y)\\right)\\end{aligned}  \\\\\n&=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right) \\\\\n&\\begin{aligned}=\\frac{1}{\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\end{aligned} \\\\\n&\\begin{aligned}&=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned} \\\\\n&=\\pi_r(y|x)\n\\end{aligned}$$  \n\nPlackett-LuceBradley-Terryreward $\\pi(y\\mid x)$  reference model $\\pi_{ref}(y\\mid x)$   \n\n$$r(x,y)=\\beta\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}$$  \n\nreward model  \n\n##   \n\n  \n- $\\beta=0.1$TL;DR summarization0.5  \n- batch size = 64  \n- RMSprop optimizer  \n- learning rate = 1e-6  \n- linearly warmup 0 to 1e-6 over 150 steps  \n\nPPOSFTDPO  \n\nDPO  \n\n{% asset_img result_1.png 1 %}  \n\n{% asset_img result_2.png 2 %}  \n\n{% asset_img result_3.png 3 %}  \n\n{% asset_img result_4.png 4 %}  \n\n#   \n\n- DPORLHF PPOreward  \n- DPOPPO  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n1Direct Preference Optimization: Your Language Model is Secretly a Reward Model https://arxiv.org/abs/2305.18290v2  ","slug":"cs/nlp/2024/05/-DPO","published":1,"updated":"2024-05-29T12:33:13.225Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmv001a0p4kfweq0629","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>SFT</p>\n<p>ChatGPTSFT+RLHF\nPPOPPOPPOrewardPPOPPO</p>\n<p>DPODirect Preference\nOptimizationPPODPOrewardPPO</p>\n<p>DPO</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>80%</p>\n<p></p>\n<p></p>\n<p>responseactionAI</p>\n<p>SFTRLHF/RLAIFRLHF</p>\n<p>DPORLHF</p>\n<h1 id=\"rlhf\">RLHF</h1>\n<p>RLHF</p>\n<ol type=\"1\">\n<li>SFT Phase</li>\n</ol>\n<p> <span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span></p>\n<ol start=\"2\" type=\"1\">\n<li>Reward Modelling Phase</li>\n</ol>\n<p>prompt <span class=\"math inline\">\\(x\\)</span>\n<span class=\"math inline\">\\((y_1,y_2)\\sim\\pi^\\text{SFT}(y|x)\\)</span>\n<span class=\"math inline\">\\(y_1,y_2\\)</span>(preference)\n<span class=\"math inline\">\\(y_w\\succ y_l\\mid\nx\\)</span>wlwinlose</p>\n<p>latent reward model\n<span class=\"math inline\">\\(r^*(y,x)\\)</span> <span class=\"math inline\">\\((x,y)\\)</span>  <span class=\"math inline\">\\(r^*(y,x)\\)</span> RLHFreward\nmodel</p>\n<p> <span class=\"math inline\">\\(r^*(y,x)\\)</span>preferenceBradley-Terry\nmodelranked\nanswersPlackett-Luce ranking models</p>\n<p>Bradley-Terry model <span class=\"math inline\">\\(p^{*}\\)</span> </p>\n<p><span class=\"math display\">\\[\\begin{aligned}p^*(y_1\\succ y_2\\mid\nx)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}\\]</span></p>\n<p>rewardsoftmax</p>\n<p> <span class=\"math inline\">\\(p^{*}\\)</span>\n <span class=\"math inline\">\\(\\mathcal{D}=\\left\\{x^{(i)},y_w^{(i)},y_l^{(i)}\\right\\}_{i=1}^N\\)</span>\n <span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span> reward\n<span class=\"math inline\">\\(r_\\phi(x,y)\\)</span>maximum\nlikelihood <span class=\"math inline\">\\(r^*(y,x)\\)</span>negative\nlog-likelihood loss</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_R(r_\\phi,\\mathcal{D})=-\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\begin{bmatrix}\\log\\sigma(r_\\phi(x,y_w)-r_\\phi(x,y_l))\\end{bmatrix}\\]</span></p>\n<p>reward\nfunctionreward <span class=\"math inline\">\\(x\\)</span> <span class=\"math inline\">\\(\\mathbb{E}_{x,y\\thicksim\\mathcal{D}}\\left[r_\\phi(x,y)\\right]=0\\)</span></p>\n<ol start=\"3\" type=\"1\">\n<li>RL Fine-Tuning Phase</li>\n</ol>\n<p>reward</p>\n<p><span class=\"math display\">\\[\\max_{\\pi_\\theta}\\mathbb{E}_{x\\sim\\mathcal{D},y\\sim\\pi_\\theta(y|x)}\\begin{bmatrix}r_\\phi(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi_\\theta(y\\mid\nx)\\mid\\mid\\pi_{\\mathrm{ref}}(y\\mid x)\\end{bmatrix}\\]</span></p>\n<p>rewardRLHFactor\nmodelreward</p>\n<p>KL\n<span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span>rewardrewardmode-collapse<span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p>RL</p>\n<p>RLreward fucntion</p>\n<p><span class=\"math display\">\\[r(x,y)=r_\\phi(x,y)-\\beta(\\log\\pi_\\theta(y\\mid\nx)-\\log\\pi_\\text{ref}(y\\mid x))\\]</span></p>\n<p>PPO</p>\n<h1 id=\"direct-preference-optimization\">Direct Preference\nOptimization</h1>\n<p>DPOpolicy\noptimizationreward</p>\n<img src=\"/473f2b43/intro.png\" class title=\"DPO\">\n<h2 id=\"dpo\">DPO</h2>\n<p>DPORLreward function <span class=\"math inline\">\\(r(x,y)\\)</span>reference model <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span></p>\n<p><span class=\"math display\">\\[\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi(y|x)||\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}\\]</span></p>\n<p>KL</p>\n<p><span class=\"math display\">\\[\\beta\\mathbb{D}_{\\mathrm{KL}}(\\pi\\|\\pi_{\\mathrm{ref}})=\\beta\\sum_y\\pi(y|x)\\log\\frac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathbf{KL}}\\begin{bmatrix}\\pi(y|x)&amp;\\mid\\mid\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}-\\frac{1}{\\beta}r(x,y)\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[Z(x)=\\sum_y\\pi_\\text{ref}(y|x)\\exp\\left(\\frac1\\beta\nr(x,y)\\right)\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}-\\log\nZ(x)\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log\nZ(x)\\right]\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(Z(x)\\)</span>  <span class=\"math inline\">\\(y\\)</span> </p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log\nZ(x)\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{E}_{y\\thicksim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}\\right]-\\log\nZ(x)\\right]\\]</span></p>\n<p><span class=\"math display\">\\[=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{D}_{\\text{KL}}(\\pi(y|x)\\mid\\mid\\pi^*(y|x))-\\log\nZ(x)\\right]\\]</span></p>\n<p><span class=\"math inline\">\\(Z(x)\\)</span>  <span class=\"math inline\">\\(\\pi\\)</span>\nKLKL0</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi(y|x)=\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(Z(x)\\)</span>\n</p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi_r(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\log Z(x)+\\log \\pi_r(y|x)=\\log \\pi_{\\text{ref}}(y|x)\n+\\frac{1}{\\beta}r(x,y)\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p>Bradley-Terry modelBradley-Terry\nmodel</p>\n<p><span class=\"math display\">\\[\\begin{aligned}p^*(y_1\\succ y_2\\mid\nx)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\np^*(y_1\\succ y_2\\mid\nx)&amp;=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\\\\n&amp;=\\frac1{1+\\frac{\\exp(r^*(x,y_2))}{\\exp(r^*(x,y_1))}}\\\\\n&amp;=\\frac1{1+\\exp(r^*(x,y_2)-r^*(x,y_1))}\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(r\\)</span>\n</p>\n<p><span class=\"math display\">\\[p^*(y_1\\succ y_2\\mid\nx)=\\frac{1}{1+\\exp\\left(\\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{\\text{ref}}(y_2|x)}-\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{\\text{ref}}(y_1|x)}\\right)}\\]</span></p>\n<p>optimal\npolicyrewardMLE</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid\nx)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid\nx)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]\\]</span></p>\n<p>DPO loss</p>\n<img src=\"/473f2b43/dpo_loss_code.png\" class title=\"DPO\">\n<h2 id=\"dpo\">DPO</h2>\n<p>DPOlossDPO</p>\n<p></p>\n<p><span class=\"math display\">\\[u=\\beta\\log\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}-\\beta\\log\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\mathrm{ref}}(y_{l}|x)}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[L_{DPO}(\\pi_{\\theta};\\pi_{\\mathrm{ref}})=-\\min_{\\pi_{0}}E_{(x,y_{u},y_{t})\\sim\nD}[\\log\\sigma(u)]\\]</span></p>\n<p>sigmoid</p>\n<p><span class=\"math display\">\\[\\frac\\partial{\\partial\nu}\\log\\sigma(u)=\\frac1{\\sigma(u)}\\cdot\\sigma(u)(1-\\sigma(u))=1-\\sigma(u)\\]</span></p>\n<p>sigmoid</p>\n<p><span class=\"math display\">\\[1-\\sigma(u)=\\sigma(-u)\\]</span></p>\n<p> <span class=\"math inline\">\\(u\\)</span> </p>\n<p><span class=\"math display\">\\[\\frac{\\partial\nu}{\\partial\\theta}=\\beta\\left(\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\mathrm{ref}}(y_w|x)}-\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}\\right)\\]</span></p>\n<p> <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span>  <span class=\"math inline\">\\(\\theta\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\frac\\partial{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_\\mathrm{ref}(y_w|x)}=&amp;\\frac{1}{\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}}\\cdot\\frac{\\partial}{\\partial\\theta}\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}\\\\\n=&amp;\\frac{1}{\\pi_{\\theta}(y_{w}|x)}\\cdot\\frac{\\partial}{\\partial\\theta}\\pi_{\\theta}(y_{w}|x)\\\\\n=&amp;\\begin{aligned}\\nabla_\\theta\\log\\pi(y_w\\mid x)\\end{aligned}\n\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}=\\nabla_\\theta\\log\\pi(y_l\\mid\nx)\\]</span></p>\n<p>DPO</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&amp;=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\beta\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}-\\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid\nx)\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}\\]</span></p>\n<p>DPO</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&amp;=-\\beta\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\sigma\\left(\\hat{r}_\\theta(x,y_l)-\\hat{r}_\\theta(x,y_w)\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid\nx)\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}\\]</span></p>\n<p></p>\n<img src=\"/473f2b43/gradient.png\" class title=\"DPO\">\n<p><span class=\"math inline\">\\(\\hat{r}_\\theta(x,y)\\)</span>  <span class=\"math inline\">\\(\\pi_{\\theta}\\)</span>  <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span>\nreward</p>\n<h2 id=\"dpo\">DPO</h2>\n<p>DPO<br>\n- prompt <span class=\"math inline\">\\(x\\)</span> <span class=\"math inline\">\\(y_1,y_2\\sim\\pi_{\\text{ref}}(\\cdot\\mid\nx)\\)</span> <span class=\"math inline\">\\(\\mathcal{D}=\\{x^{(i)},y_w^{(i)},y_l)^{(i)}\\}_{i=1}^N\\)</span><br>\n-  <span class=\"math inline\">\\(\\mathcal{L}_{\\mathrm{DPO}}\\)</span>\n<span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span><span class=\"math inline\">\\(\\mathcal{D}\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span>  $$</p>\n<p></p>\n<p> <span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span> \n<span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}=\\pi^{\\mathrm{SFT}}\\)</span>\n<span class=\"math inline\">\\((x,y_w)\\)</span>  <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span> </p>\n<p><span class=\"math display\">\\[\\pi_{\\text{ref}}=\\arg\\max_\\pi\\mathbb{E}_{x,y_w\\thicksim\\mathcal{D}}\\left[\\log\\pi(y_w\\mid\nx)\\right]\\]</span></p>\n<p> <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span>  reference\ndistribution distribution shift</p>\n<h2 id=\"your-language-model-is-secretly-a-reward-model\">Your Language\nModel Is Secretly a Reward Model</h2>\n<p>DPOlossreward</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(Z(x)\\)</span>\n</p>\n<p>\"Plackett-Luce/Bradley-Terryreward\nfunctionpreference distribution\"</p>\n<blockquote>\n<p>Under the Plackett-Luce preference framework, and in particular the\nBradleyTerry framework, two reward functions from the same equivalence\nclass induce the same preference distribution</p>\n</blockquote>\n<p>reward function <span class=\"math inline\">\\(r(x,y)\\)</span>\n <span class=\"math inline\">\\(r^{\\prime}(x,y)\\)</span> </p>\n<p><span class=\"math display\">\\[r&#39;(x,y)=r(x,y)+f(x)\\]</span></p>\n<p>reward function(equivalence class)</p>\n<p>prompt <span class=\"math inline\">\\(x\\)</span>  answer <span class=\"math inline\">\\(y_1,\\ldots,y_K\\)</span>ranking <span class=\"math inline\">\\(\\tau\\)</span>Plackett-Luce\nframeworkBradleyTerry</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\np_{r&#39;}(\\tau|y_1,\\ldots,y_K,x)&amp;\n=\\prod_{k=1}^K\\frac{\\exp(r&#39;(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r&#39;(x,y_{\\tau(j)}))}  \\\\\n&amp;=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)})+f(x))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)})+f(x))}\n\\\\\n&amp;=\\prod_{k=1}^K\\frac{\\exp(f(x))\\exp(r(x,y_{\\tau(k)}))}{\\exp(f(x))\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))}\n\\\\\n&amp;=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))}\n\\\\\n&amp;=p_r(\\tau|y_1,\\ldots,y_K,x)\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\beta\\log\nZ(x)\\)</span> reward\nfunctionpreference distribution</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}\\]</span></p>\n<p>reward\nfunctionRLoptimal policy</p>\n<p>DPOlossoptimal policy</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p>reward functionoptimal\npolicy<span class=\"math inline\">\\(r&#39;(x,y)=r(x,y)+f(x)\\)</span><span class=\"math inline\">\\(\\pi_r\\)</span>  <span class=\"math inline\">\\(\\pi_{r&#39;}\\)</span> optimal\npolicy</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\pi_{r^{\\prime}}(y|x)&amp;\n\\begin{aligned}&amp;=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r&#39;(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r&#39;(x,y)\\right)\\end{aligned}  \\\\\n&amp;=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right)\n\\\\\n&amp;\\begin{aligned}=\\frac{1}{\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\end{aligned}\n\\\\\n&amp;\\begin{aligned}&amp;=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\n\\\\\n&amp;=\\pi_r(y|x)\n\\end{aligned}\\]</span></p>\n<p>Plackett-LuceBradley-Terryreward\n<span class=\"math inline\">\\(\\pi(y\\mid x)\\)</span>  reference\nmodel <span class=\"math inline\">\\(\\pi_{ref}(y\\mid x)\\)</span>\n</p>\n<p><span class=\"math display\">\\[r(x,y)=\\beta\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}\\]</span></p>\n<p>reward model</p>\n<h2 id=\"\"></h2>\n<p><br>\n- <span class=\"math inline\">\\(\\beta=0.1\\)</span>TL;DR\nsummarization0.5<br>\n- batch size = 64<br>\n- RMSprop optimizer<br>\n- learning rate = 1e-6<br>\n- linearly warmup 0 to 1e-6 over 150 steps</p>\n<p>PPOSFTDPO</p>\n<p>DPO</p>\n<img src=\"/473f2b43/result_1.png\" class title=\"1\">\n<img src=\"/473f2b43/result_2.png\" class title=\"2\">\n<img src=\"/473f2b43/result_3.png\" class title=\"3\">\n<img src=\"/473f2b43/result_4.png\" class title=\"4\">\n<h1 id=\"\"></h1>\n<ul>\n<li>DPORLHF\nPPOreward<br>\n</li>\n<li>DPOPPO</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Direct Preference Optimization: Your Language Model is Secretly\na Reward Model https://arxiv.org/abs/2305.18290v2</p>\n","length":13170,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>SFT</p>\n<p>ChatGPTSFT+RLHF\nPPOPPOPPOrewardPPOPPO</p>\n<p>DPODirect Preference\nOptimizationPPODPOrewardPPO</p>\n<p>DPO</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>80%</p>\n<p></p>\n<p></p>\n<p>responseactionAI</p>\n<p>SFTRLHF/RLAIFRLHF</p>\n<p>DPORLHF</p>\n<h1 id=\"rlhf\">RLHF</h1>\n<p>RLHF</p>\n<ol type=\"1\">\n<li>SFT Phase</li>\n</ol>\n<p> <span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span></p>\n<ol start=\"2\" type=\"1\">\n<li>Reward Modelling Phase</li>\n</ol>\n<p>prompt <span class=\"math inline\">\\(x\\)</span>\n<span class=\"math inline\">\\((y_1,y_2)\\sim\\pi^\\text{SFT}(y|x)\\)</span>\n<span class=\"math inline\">\\(y_1,y_2\\)</span>(preference)\n<span class=\"math inline\">\\(y_w\\succ y_l\\mid\nx\\)</span>wlwinlose</p>\n<p>latent reward model\n<span class=\"math inline\">\\(r^*(y,x)\\)</span> <span class=\"math inline\">\\((x,y)\\)</span>  <span class=\"math inline\">\\(r^*(y,x)\\)</span> RLHFreward\nmodel</p>\n<p> <span class=\"math inline\">\\(r^*(y,x)\\)</span>preferenceBradley-Terry\nmodelranked\nanswersPlackett-Luce ranking models</p>\n<p>Bradley-Terry model <span class=\"math inline\">\\(p^{*}\\)</span> </p>\n<p><span class=\"math display\">\\[\\begin{aligned}p^*(y_1\\succ y_2\\mid\nx)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}\\]</span></p>\n<p>rewardsoftmax</p>\n<p> <span class=\"math inline\">\\(p^{*}\\)</span>\n <span class=\"math inline\">\\(\\mathcal{D}=\\left\\{x^{(i)},y_w^{(i)},y_l^{(i)}\\right\\}_{i=1}^N\\)</span>\n <span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span> reward\n<span class=\"math inline\">\\(r_\\phi(x,y)\\)</span>maximum\nlikelihood <span class=\"math inline\">\\(r^*(y,x)\\)</span>negative\nlog-likelihood loss</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_R(r_\\phi,\\mathcal{D})=-\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\begin{bmatrix}\\log\\sigma(r_\\phi(x,y_w)-r_\\phi(x,y_l))\\end{bmatrix}\\]</span></p>\n<p>reward\nfunctionreward <span class=\"math inline\">\\(x\\)</span> <span class=\"math inline\">\\(\\mathbb{E}_{x,y\\thicksim\\mathcal{D}}\\left[r_\\phi(x,y)\\right]=0\\)</span></p>\n<ol start=\"3\" type=\"1\">\n<li>RL Fine-Tuning Phase</li>\n</ol>\n<p>reward</p>\n<p><span class=\"math display\">\\[\\max_{\\pi_\\theta}\\mathbb{E}_{x\\sim\\mathcal{D},y\\sim\\pi_\\theta(y|x)}\\begin{bmatrix}r_\\phi(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi_\\theta(y\\mid\nx)\\mid\\mid\\pi_{\\mathrm{ref}}(y\\mid x)\\end{bmatrix}\\]</span></p>\n<p>rewardRLHFactor\nmodelreward</p>\n<p>KL\n<span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span>rewardrewardmode-collapse<span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p>RL</p>\n<p>RLreward fucntion</p>\n<p><span class=\"math display\">\\[r(x,y)=r_\\phi(x,y)-\\beta(\\log\\pi_\\theta(y\\mid\nx)-\\log\\pi_\\text{ref}(y\\mid x))\\]</span></p>\n<p>PPO</p>\n<h1 id=\"direct-preference-optimization\">Direct Preference\nOptimization</h1>\n<p>DPOpolicy\noptimizationreward</p>\n<img src=\"/473f2b43/intro.png\" class title=\"DPO\">\n<h2 id=\"dpo\">DPO</h2>\n<p>DPORLreward function <span class=\"math inline\">\\(r(x,y)\\)</span>reference model <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span></p>\n<p><span class=\"math display\">\\[\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathrm{KL}}\\begin{bmatrix}\\pi(y|x)||\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}\\]</span></p>\n<p>KL</p>\n<p><span class=\"math display\">\\[\\beta\\mathbb{D}_{\\mathrm{KL}}(\\pi\\|\\pi_{\\mathrm{ref}})=\\beta\\sum_y\\pi(y|x)\\log\\frac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\max_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D},y\\thicksim\\pi}\\begin{bmatrix}r(x,y)\\end{bmatrix}-\\beta\\mathbb{D}_{\\mathbf{KL}}\\begin{bmatrix}\\pi(y|x)&amp;\\mid\\mid\\pi_{\\mathrm{ref}}(y|x)\\end{bmatrix}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\max_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}-\\frac{1}{\\beta}r(x,y)\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[Z(x)=\\sum_y\\pi_\\text{ref}(y|x)\\exp\\left(\\frac1\\beta\nr(x,y)\\right)\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp{\\left(\\frac{1}{\\beta}r(x,y)\\right)}}\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}-\\log\nZ(x)\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;=\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log\nZ(x)\\right]\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(Z(x)\\)</span>  <span class=\"math inline\">\\(y\\)</span> </p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\min_\\pi\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}-\\log\nZ(x)\\right]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{E}_{y\\thicksim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}\\right]-\\log\nZ(x)\\right]\\]</span></p>\n<p><span class=\"math display\">\\[=\\min_\\pi\\mathbb{E}_{x\\thicksim\\mathcal{D}}\\left[\\mathbb{D}_{\\text{KL}}(\\pi(y|x)\\mid\\mid\\pi^*(y|x))-\\log\nZ(x)\\right]\\]</span></p>\n<p><span class=\"math inline\">\\(Z(x)\\)</span>  <span class=\"math inline\">\\(\\pi\\)</span>\nKLKL0</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi(y|x)=\\pi^*(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(Z(x)\\)</span>\n</p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi_r(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\log Z(x)+\\log \\pi_r(y|x)=\\log \\pi_{\\text{ref}}(y|x)\n+\\frac{1}{\\beta}r(x,y)\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p>Bradley-Terry modelBradley-Terry\nmodel</p>\n<p><span class=\"math display\">\\[\\begin{aligned}p^*(y_1\\succ y_2\\mid\nx)=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\np^*(y_1\\succ y_2\\mid\nx)&amp;=\\frac{\\exp\\left(r^*(x,y_1)\\right)}{\\exp\\left(r^*(x,y_1)\\right)+\\exp\\left(r^*(x,y_2)\\right)}\\\\\n&amp;=\\frac1{1+\\frac{\\exp(r^*(x,y_2))}{\\exp(r^*(x,y_1))}}\\\\\n&amp;=\\frac1{1+\\exp(r^*(x,y_2)-r^*(x,y_1))}\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(r\\)</span>\n</p>\n<p><span class=\"math display\">\\[p^*(y_1\\succ y_2\\mid\nx)=\\frac{1}{1+\\exp\\left(\\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{\\text{ref}}(y_2|x)}-\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{\\text{ref}}(y_1|x)}\\right)}\\]</span></p>\n<p>optimal\npolicyrewardMLE</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid\nx)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid\nx)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]\\]</span></p>\n<p>DPO loss</p>\n<img src=\"/473f2b43/dpo_loss_code.png\" class title=\"DPO\">\n<h2 id=\"dpo\">DPO</h2>\n<p>DPOlossDPO</p>\n<p></p>\n<p><span class=\"math display\">\\[u=\\beta\\log\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}-\\beta\\log\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\mathrm{ref}}(y_{l}|x)}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[L_{DPO}(\\pi_{\\theta};\\pi_{\\mathrm{ref}})=-\\min_{\\pi_{0}}E_{(x,y_{u},y_{t})\\sim\nD}[\\log\\sigma(u)]\\]</span></p>\n<p>sigmoid</p>\n<p><span class=\"math display\">\\[\\frac\\partial{\\partial\nu}\\log\\sigma(u)=\\frac1{\\sigma(u)}\\cdot\\sigma(u)(1-\\sigma(u))=1-\\sigma(u)\\]</span></p>\n<p>sigmoid</p>\n<p><span class=\"math display\">\\[1-\\sigma(u)=\\sigma(-u)\\]</span></p>\n<p> <span class=\"math inline\">\\(u\\)</span> </p>\n<p><span class=\"math display\">\\[\\frac{\\partial\nu}{\\partial\\theta}=\\beta\\left(\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\mathrm{ref}}(y_w|x)}-\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}\\right)\\]</span></p>\n<p> <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span>  <span class=\"math inline\">\\(\\theta\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\frac\\partial{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_\\mathrm{ref}(y_w|x)}=&amp;\\frac{1}{\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}}\\cdot\\frac{\\partial}{\\partial\\theta}\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}\\\\\n=&amp;\\frac{1}{\\pi_{\\theta}(y_{w}|x)}\\cdot\\frac{\\partial}{\\partial\\theta}\\pi_{\\theta}(y_{w}|x)\\\\\n=&amp;\\begin{aligned}\\nabla_\\theta\\log\\pi(y_w\\mid x)\\end{aligned}\n\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\frac{\\partial}{\\partial\\theta}\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\mathrm{ref}}(y_l|x)}=\\nabla_\\theta\\log\\pi(y_l\\mid\nx)\\]</span></p>\n<p>DPO</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&amp;=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\beta\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}-\\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid\nx)\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}\\]</span></p>\n<p>DPO</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n&amp;\\nabla_\\theta\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})\\\\&amp;=-\\beta\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\sigma\\left(\\hat{r}_\\theta(x,y_l)-\\hat{r}_\\theta(x,y_w)\\right)\\left[\\nabla_\\theta\\log\\pi(y_w\\mid\nx)\\nabla_\\theta\\log\\pi(y_l\\mid x)\\right]\\right]\n\\end{aligned}\\]</span></p>\n<p></p>\n<img src=\"/473f2b43/gradient.png\" class title=\"DPO\">\n<p><span class=\"math inline\">\\(\\hat{r}_\\theta(x,y)\\)</span>  <span class=\"math inline\">\\(\\pi_{\\theta}\\)</span>  <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span>\nreward</p>\n<h2 id=\"dpo\">DPO</h2>\n<p>DPO<br>\n- prompt <span class=\"math inline\">\\(x\\)</span> <span class=\"math inline\">\\(y_1,y_2\\sim\\pi_{\\text{ref}}(\\cdot\\mid\nx)\\)</span> <span class=\"math inline\">\\(\\mathcal{D}=\\{x^{(i)},y_w^{(i)},y_l)^{(i)}\\}_{i=1}^N\\)</span><br>\n-  <span class=\"math inline\">\\(\\mathcal{L}_{\\mathrm{DPO}}\\)</span>\n<span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span><span class=\"math inline\">\\(\\mathcal{D}\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span>  $$</p>\n<p></p>\n<p> <span class=\"math inline\">\\(\\pi^{\\mathrm{SFT}}\\)</span> \n<span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}=\\pi^{\\mathrm{SFT}}\\)</span>\n<span class=\"math inline\">\\((x,y_w)\\)</span>  <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span> </p>\n<p><span class=\"math display\">\\[\\pi_{\\text{ref}}=\\arg\\max_\\pi\\mathbb{E}_{x,y_w\\thicksim\\mathcal{D}}\\left[\\log\\pi(y_w\\mid\nx)\\right]\\]</span></p>\n<p> <span class=\"math inline\">\\(\\pi_{\\mathrm{ref}}\\)</span>  reference\ndistribution distribution shift</p>\n<h2 id=\"your-language-model-is-secretly-a-reward-model\">Your Language\nModel Is Secretly a Reward Model</h2>\n<p>DPOlossreward</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(Z(x)\\)</span>\n</p>\n<p>\"Plackett-Luce/Bradley-Terryreward\nfunctionpreference distribution\"</p>\n<blockquote>\n<p>Under the Plackett-Luce preference framework, and in particular the\nBradleyTerry framework, two reward functions from the same equivalence\nclass induce the same preference distribution</p>\n</blockquote>\n<p>reward function <span class=\"math inline\">\\(r(x,y)\\)</span>\n <span class=\"math inline\">\\(r^{\\prime}(x,y)\\)</span> </p>\n<p><span class=\"math display\">\\[r&#39;(x,y)=r(x,y)+f(x)\\]</span></p>\n<p>reward function(equivalence class)</p>\n<p>prompt <span class=\"math inline\">\\(x\\)</span>  answer <span class=\"math inline\">\\(y_1,\\ldots,y_K\\)</span>ranking <span class=\"math inline\">\\(\\tau\\)</span>Plackett-Luce\nframeworkBradleyTerry</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\np_{r&#39;}(\\tau|y_1,\\ldots,y_K,x)&amp;\n=\\prod_{k=1}^K\\frac{\\exp(r&#39;(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r&#39;(x,y_{\\tau(j)}))}  \\\\\n&amp;=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)})+f(x))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)})+f(x))}\n\\\\\n&amp;=\\prod_{k=1}^K\\frac{\\exp(f(x))\\exp(r(x,y_{\\tau(k)}))}{\\exp(f(x))\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))}\n\\\\\n&amp;=\\prod_{k=1}^K\\frac{\\exp(r(x,y_{\\tau(k)}))}{\\sum_{j=k}^K\\exp(r(x,y_{\\tau(j)}))}\n\\\\\n&amp;=p_r(\\tau|y_1,\\ldots,y_K,x)\n\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\beta\\log\nZ(x)\\)</span> reward\nfunctionpreference distribution</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_r(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\hat{r}_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}\\]</span></p>\n<p>reward\nfunctionRLoptimal policy</p>\n<p>DPOlossoptimal policy</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\\pi(y|x)=\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\\]</span></p>\n<p>reward functionoptimal\npolicy<span class=\"math inline\">\\(r&#39;(x,y)=r(x,y)+f(x)\\)</span><span class=\"math inline\">\\(\\pi_r\\)</span>  <span class=\"math inline\">\\(\\pi_{r&#39;}\\)</span> optimal\npolicy</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\pi_{r^{\\prime}}(y|x)&amp;\n\\begin{aligned}&amp;=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r&#39;(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r&#39;(x,y)\\right)\\end{aligned}  \\\\\n&amp;=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x,y)+f(x))\\right)\n\\\\\n&amp;\\begin{aligned}=\\frac{1}{\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\end{aligned}\n\\\\\n&amp;\\begin{aligned}&amp;=\\frac{1}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)}\\pi_{\\text{ref}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)\\end{aligned}\n\\\\\n&amp;=\\pi_r(y|x)\n\\end{aligned}\\]</span></p>\n<p>Plackett-LuceBradley-Terryreward\n<span class=\"math inline\">\\(\\pi(y\\mid x)\\)</span>  reference\nmodel <span class=\"math inline\">\\(\\pi_{ref}(y\\mid x)\\)</span>\n</p>\n<p><span class=\"math display\">\\[r(x,y)=\\beta\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}\\]</span></p>\n<p>reward model</p>\n<h2 id=\"\"></h2>\n<p><br>\n- <span class=\"math inline\">\\(\\beta=0.1\\)</span>TL;DR\nsummarization0.5<br>\n- batch size = 64<br>\n- RMSprop optimizer<br>\n- learning rate = 1e-6<br>\n- linearly warmup 0 to 1e-6 over 150 steps</p>\n<p>PPOSFTDPO</p>\n<p>DPO</p>\n<img src=\"/473f2b43/result_1.png\" class title=\"1\">\n<img src=\"/473f2b43/result_2.png\" class title=\"2\">\n<img src=\"/473f2b43/result_3.png\" class title=\"3\">\n<img src=\"/473f2b43/result_4.png\" class title=\"4\">\n<h1 id=\"\"></h1>\n<ul>\n<li>DPORLHF\nPPOreward<br>\n</li>\n<li>DPOPPO</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Direct Preference Optimization: Your Language Model is Secretly\na Reward Model https://arxiv.org/abs/2305.18290v2</p>\n"},{"title":"-ODPO","abbrlink":"da871ebe","date":"2024-05-30T07:23:05.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nDPO[-DPO](http://www.linsight.cn/473f2b43.html)  \n\nDPORLHFODPODPO with an offsetDPODPO  \n\n#   \n\n  \n\nmaximize the response log-likelihoodgap  \n\nmisalignmentmaximum likelihood  \n\n> Training with the maximum likelihood objective makes the model assign nonzero probability mass to all responses in SFT dataset, even those of lower quality.  \n\nRLHFRLrewardreward  \n\nrewardmodelingpointwise rewardpairwise preference  \n\npointwise rewardrewardpositivereward1negativereward0toxicity/classifier  \n\npairwise preference  \n\nRLHFDPO  \n\n# BradleyTerry model  \n\nDPO  \n\n$$\\begin{aligned}\n\\mathcal{L}^{\\mathrm{DPO}}(\\boldsymbol{\\theta})& =-\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\text{HF}}}\\left[\\log\\sigma\\Big(\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}-\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}\\Big)\\right]  \\\\\n&=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\thicksim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)\\right)\\right]\n\\end{aligned}$$  \n\n  \n\n$$\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y})=\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}|\\boldsymbol{x})}{\\pi_{\\mathrm{SFT}}(\\boldsymbol{y}|\\boldsymbol{x})}$$  \n\nestimated reward  \n\nDPOBradleyTerry modelBradleyTerry modelresponseresponse  \n\nresponse  \n\nODPOresponseoffset  \n\n# DPO with an Offset  \n\n $\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)$ Gumbel noise  \n\n$$\\tilde{r}_w\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),1)$$  \n\n$$\\tilde{r}_l\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l),1)$$  \n\n  \n\n$$p\\big(\\tilde{r}_w-\\tilde{r}_l>\\Delta_r\\big)=\\sigma(\\Delta_{\\hat{r}_\\theta}-\\Delta_r)$$  \n\nODPO  \n\n$$\\mathcal{L}^{\\mathrm{ODPO}}(\\boldsymbol{\\theta})=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma{\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)-\\Delta_r\\right)}\\right]$$  \n\npreferred responseestimated rewarddispreferred responseestimated rewardoffset  \n\noffset=0ODPODPO  \n\nODPOsoftmax margin loss/marginal losslossmarginpenalization  \n\nODPOoffsetresponseactual rewardincreasing scaling function  \n\n$$\\Delta_r=\\alpha\\mathbf{f}\\big(\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_l)\\big)$$  \n\n $\\alpha$   \n\n{% asset_img odpo_intro.png intro %}  \n\n#   \n\n  \n\n## sentiment control  \n\nsentiment controlpositiveresponse  \n\nGPT2-LargeIMDB datasetfinetuneSFTsentiment classifierrewardresponse  \n\n$$r_{negative}(\\boldsymbol{x},\\boldsymbol{y}) = 1-p(\\text{negative}\\mid\\cdot)$$  \n\n$$r_{positive}(\\boldsymbol{x},\\boldsymbol{y}) = 1+p(\\text{positive}\\mid\\cdot)$$  \n\nrewardpromptrewardresponse  \n\nDPOODPOoffset  \n\n$$\\Delta_r=\\log\\left(r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\right)$$  \n\n $\\alpha$ 1  \n\nrandom seedSFT2  \n\n $\\beta$ 14 $\\{0.1,0.2,\\ldots,1\\}\\cup\\{1,2,3,4,5\\}$   \n\n250007500,10000DPOODPO2314=84  \n\nsentimentSFTKL divergence  \n\n{% asset_img sentiment_control.png sentiment control %}  \n\nsentimentSFTODPODPO  \n\n## toxicity control  \n\ntoxicity controlsentiment controlresponse  \n\nGPT-neo-2.7b$\\beta$  $\\{0.05,0.1,0.2,0.3,0.4,0.5\\}$REALTOXICITYPROMPTS100000.3prompt  \n\n  \n\n{% asset_img toxicity_control.png toxicity control %}  \n\n8000 & 9000ODPO  \n\n## summarization  \n\nREDDIT TL;DRGPTJ-6B  \n\nDPOODPO100prompttemperatureGPT-4  \n\n{% asset_img summarization.png summarization %}  \n\nDPOODPOSFTtemperatureDPOODPOhuman-written  \n\n## scaling function  \n\noffsetrewardlog  \n\n$$\\Delta_r=\\log r(\\boldsymbol{y}_w)-\\log r(\\boldsymbol{y}_l)$$  \n\n$$\\begin{array}{rcl}\\Delta_r=r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\end{array}$$  \n\n5000sentiment control$\\beta \\in \\{0.1,0.2,\\ldots,0.9\\}\\cup\\{1,2,3,4,5\\}$  \n\n  \n\n{% asset_img scaling_function.png scaling function %}  \n\nlog scalingODPOKL divergence0.40.8rewardlog scalingKL divergencereward  \n\n##   \n\n7500sentiment control$\\beta=0.5$$\\alpha\\in\\{0.0,0.1,0.2,0.3,0.5,0.8,1.\\}$  \n\n{% asset_img alpha.png alpha %}  \n\n $\\alpha$ SFTreward  \n\n#   \n\nODPODPOoffset  \n\nODPOLLAMA  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n1Direct Preference Optimization with an Offset https://arxiv.org/pdf/2402.10571  ","source":"_posts/cs/nlp/2024/05/-ODPO.md","raw":"---\ntitle: -ODPO\nabbrlink: da871ebe\ndate: 2024-05-30 15:23:05\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \n  - SFT\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nDPO[-DPO](http://www.linsight.cn/473f2b43.html)  \n\nDPORLHFODPODPO with an offsetDPODPO  \n\n#   \n\n  \n\nmaximize the response log-likelihoodgap  \n\nmisalignmentmaximum likelihood  \n\n> Training with the maximum likelihood objective makes the model assign nonzero probability mass to all responses in SFT dataset, even those of lower quality.  \n\nRLHFRLrewardreward  \n\nrewardmodelingpointwise rewardpairwise preference  \n\npointwise rewardrewardpositivereward1negativereward0toxicity/classifier  \n\npairwise preference  \n\nRLHFDPO  \n\n# BradleyTerry model  \n\nDPO  \n\n$$\\begin{aligned}\n\\mathcal{L}^{\\mathrm{DPO}}(\\boldsymbol{\\theta})& =-\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\text{HF}}}\\left[\\log\\sigma\\Big(\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}-\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}\\Big)\\right]  \\\\\n&=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\thicksim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)\\right)\\right]\n\\end{aligned}$$  \n\n  \n\n$$\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y})=\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}|\\boldsymbol{x})}{\\pi_{\\mathrm{SFT}}(\\boldsymbol{y}|\\boldsymbol{x})}$$  \n\nestimated reward  \n\nDPOBradleyTerry modelBradleyTerry modelresponseresponse  \n\nresponse  \n\nODPOresponseoffset  \n\n# DPO with an Offset  \n\n $\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)$ Gumbel noise  \n\n$$\\tilde{r}_w\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),1)$$  \n\n$$\\tilde{r}_l\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l),1)$$  \n\n  \n\n$$p\\big(\\tilde{r}_w-\\tilde{r}_l>\\Delta_r\\big)=\\sigma(\\Delta_{\\hat{r}_\\theta}-\\Delta_r)$$  \n\nODPO  \n\n$$\\mathcal{L}^{\\mathrm{ODPO}}(\\boldsymbol{\\theta})=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma{\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)-\\Delta_r\\right)}\\right]$$  \n\npreferred responseestimated rewarddispreferred responseestimated rewardoffset  \n\noffset=0ODPODPO  \n\nODPOsoftmax margin loss/marginal losslossmarginpenalization  \n\nODPOoffsetresponseactual rewardincreasing scaling function  \n\n$$\\Delta_r=\\alpha\\mathbf{f}\\big(\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_l)\\big)$$  \n\n $\\alpha$   \n\n{% asset_img odpo_intro.png intro %}  \n\n#   \n\n  \n\n## sentiment control  \n\nsentiment controlpositiveresponse  \n\nGPT2-LargeIMDB datasetfinetuneSFTsentiment classifierrewardresponse  \n\n$$r_{negative}(\\boldsymbol{x},\\boldsymbol{y}) = 1-p(\\text{negative}\\mid\\cdot)$$  \n\n$$r_{positive}(\\boldsymbol{x},\\boldsymbol{y}) = 1+p(\\text{positive}\\mid\\cdot)$$  \n\nrewardpromptrewardresponse  \n\nDPOODPOoffset  \n\n$$\\Delta_r=\\log\\left(r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\right)$$  \n\n $\\alpha$ 1  \n\nrandom seedSFT2  \n\n $\\beta$ 14 $\\{0.1,0.2,\\ldots,1\\}\\cup\\{1,2,3,4,5\\}$   \n\n250007500,10000DPOODPO2314=84  \n\nsentimentSFTKL divergence  \n\n{% asset_img sentiment_control.png sentiment control %}  \n\nsentimentSFTODPODPO  \n\n## toxicity control  \n\ntoxicity controlsentiment controlresponse  \n\nGPT-neo-2.7b$\\beta$  $\\{0.05,0.1,0.2,0.3,0.4,0.5\\}$REALTOXICITYPROMPTS100000.3prompt  \n\n  \n\n{% asset_img toxicity_control.png toxicity control %}  \n\n8000 & 9000ODPO  \n\n## summarization  \n\nREDDIT TL;DRGPTJ-6B  \n\nDPOODPO100prompttemperatureGPT-4  \n\n{% asset_img summarization.png summarization %}  \n\nDPOODPOSFTtemperatureDPOODPOhuman-written  \n\n## scaling function  \n\noffsetrewardlog  \n\n$$\\Delta_r=\\log r(\\boldsymbol{y}_w)-\\log r(\\boldsymbol{y}_l)$$  \n\n$$\\begin{array}{rcl}\\Delta_r=r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\end{array}$$  \n\n5000sentiment control$\\beta \\in \\{0.1,0.2,\\ldots,0.9\\}\\cup\\{1,2,3,4,5\\}$  \n\n  \n\n{% asset_img scaling_function.png scaling function %}  \n\nlog scalingODPOKL divergence0.40.8rewardlog scalingKL divergencereward  \n\n##   \n\n7500sentiment control$\\beta=0.5$$\\alpha\\in\\{0.0,0.1,0.2,0.3,0.5,0.8,1.\\}$  \n\n{% asset_img alpha.png alpha %}  \n\n $\\alpha$ SFTreward  \n\n#   \n\nODPODPOoffset  \n\nODPOLLAMA  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n1Direct Preference Optimization with an Offset https://arxiv.org/pdf/2402.10571  ","slug":"cs/nlp/2024/05/-ODPO","published":1,"updated":"2024-05-31T12:26:56.345Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmv001c0p4ketw1b4vf","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>DPO<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a></p>\n<p>DPORLHFODPODPO\nwith an\noffsetDPODPO</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>maximize the response\nlog-likelihoodgap</p>\n<p>misalignmentmaximum\nlikelihood</p>\n<blockquote>\n<p>Training with the maximum likelihood objective makes the model assign\nnonzero probability mass to all responses in SFT dataset, even those of\nlower quality.</p>\n</blockquote>\n<p>RLHFRLrewardreward</p>\n<p>rewardmodelingpointwise rewardpairwise preference</p>\n<p>pointwise\nrewardrewardpositivereward1negativereward0toxicity/classifier</p>\n<p>pairwise\npreference</p>\n<p>RLHFDPO</p>\n<h1 id=\"bradleyterry-model\">BradleyTerry model</h1>\n<p>DPO</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}^{\\mathrm{DPO}}(\\boldsymbol{\\theta})&amp;\n=-\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\text{HF}}}\\left[\\log\\sigma\\Big(\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}-\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}\\Big)\\right]  \\\\\n&amp;=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\thicksim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)\\right)\\right]\n\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y})=\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}|\\boldsymbol{x})}{\\pi_{\\mathrm{SFT}}(\\boldsymbol{y}|\\boldsymbol{x})}\\]</span></p>\n<p>estimated reward</p>\n<p>DPOBradleyTerry\nmodelBradleyTerry\nmodelresponseresponse</p>\n<p>response</p>\n<p>ODPOresponseoffset</p>\n<h1 id=\"dpo-with-an-offset\">DPO with an Offset</h1>\n<p> <span class=\"math inline\">\\(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)\\)</span>\nGumbel noise</p>\n<p><span class=\"math display\">\\[\\tilde{r}_w\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),1)\\]</span></p>\n<p><span class=\"math display\">\\[\\tilde{r}_l\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l),1)\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[p\\big(\\tilde{r}_w-\\tilde{r}_l&gt;\\Delta_r\\big)=\\sigma(\\Delta_{\\hat{r}_\\theta}-\\Delta_r)\\]</span></p>\n<p>ODPO</p>\n<p><span class=\"math display\">\\[\\mathcal{L}^{\\mathrm{ODPO}}(\\boldsymbol{\\theta})=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma{\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)-\\Delta_r\\right)}\\right]\\]</span></p>\n<p>preferred responseestimated rewarddispreferred\nresponseestimated rewardoffset</p>\n<p>offset=0ODPODPO</p>\n<p>ODPOsoftmax margin loss/marginal\nlosslossmarginpenalization</p>\n<p>ODPOoffsetresponseactual rewardincreasing scaling\nfunction</p>\n<p><span class=\"math display\">\\[\\Delta_r=\\alpha\\mathbf{f}\\big(\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_l)\\big)\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<img src=\"/da871ebe/odpo_intro.png\" class title=\"intro\">\n<h1 id=\"\"></h1>\n<p></p>\n<h2 id=\"sentiment-control\">sentiment control</h2>\n<p>sentiment controlpositiveresponse</p>\n<p>GPT2-LargeIMDB\ndatasetfinetuneSFTsentiment\nclassifierrewardresponse</p>\n<p><span class=\"math display\">\\[r_{negative}(\\boldsymbol{x},\\boldsymbol{y}) =\n1-p(\\text{negative}\\mid\\cdot)\\]</span></p>\n<p><span class=\"math display\">\\[r_{positive}(\\boldsymbol{x},\\boldsymbol{y}) =\n1+p(\\text{positive}\\mid\\cdot)\\]</span></p>\n<p>rewardpromptrewardresponse</p>\n<p>DPOODPOoffset</p>\n<p><span class=\"math display\">\\[\\Delta_r=\\log\\left(r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\right)\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha\\)</span> 1</p>\n<p>random\nseedSFT2</p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span> 14\n<span class=\"math inline\">\\(\\{0.1,0.2,\\ldots,1\\}\\cup\\{1,2,3,4,5\\}\\)</span>\n</p>\n<p>250007500,10000DPOODPO2314=84</p>\n<p>sentimentSFTKL\ndivergence</p>\n<img src=\"/da871ebe/sentiment_control.png\" class title=\"sentiment control\">\n<p>sentimentSFTODPODPO</p>\n<h2 id=\"toxicity-control\">toxicity control</h2>\n<p>toxicity controlsentiment\ncontrolresponse</p>\n<p>GPT-neo-2.7b<span class=\"math inline\">\\(\\beta\\)</span>\n <span class=\"math inline\">\\(\\{0.05,0.1,0.2,0.3,0.4,0.5\\}\\)</span>REALTOXICITYPROMPTS100000.3prompt</p>\n<p></p>\n<img src=\"/da871ebe/toxicity_control.png\" class title=\"toxicity control\">\n<p>8000 &amp; 9000ODPO</p>\n<h2 id=\"summarization\">summarization</h2>\n<p>REDDIT TL;DRGPTJ-6B</p>\n<p>DPOODPO100prompttemperatureGPT-4</p>\n<img src=\"/da871ebe/summarization.png\" class title=\"summarization\">\n<p>DPOODPOSFTtemperatureDPOODPOhuman-written</p>\n<h2 id=\"scaling-function\">scaling function</h2>\n<p>offsetrewardlog</p>\n<p><span class=\"math display\">\\[\\Delta_r=\\log r(\\boldsymbol{y}_w)-\\log\nr(\\boldsymbol{y}_l)\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{array}{rcl}\\Delta_r=r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\end{array}\\]</span></p>\n<p>5000sentiment control<span class=\"math inline\">\\(\\beta\n\\in \\{0.1,0.2,\\ldots,0.9\\}\\cup\\{1,2,3,4,5\\}\\)</span></p>\n<p></p>\n<img src=\"/da871ebe/scaling_function.png\" class title=\"scaling function\">\n<p>log scalingODPOKL\ndivergence0.40.8rewardlog\nscalingKL divergencereward</p>\n<h2 id=\"\"></h2>\n<p>7500sentiment control<span class=\"math inline\">\\(\\beta=0.5\\)</span><span class=\"math inline\">\\(\\alpha\\in\\{0.0,0.1,0.2,0.3,0.5,0.8,1.\\}\\)</span></p>\n<img src=\"/da871ebe/alpha.png\" class title=\"alpha\">\n<p> <span class=\"math inline\">\\(\\alpha\\)</span>\nSFTreward</p>\n<h1 id=\"\"></h1>\n<p>ODPODPOoffset</p>\n<p>ODPOLLAMA</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Direct Preference Optimization with an Offset\nhttps://arxiv.org/pdf/2402.10571</p>\n","length":5653,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>DPO<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a></p>\n<p>DPORLHFODPODPO\nwith an\noffsetDPODPO</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>maximize the response\nlog-likelihoodgap</p>\n<p>misalignmentmaximum\nlikelihood</p>\n<blockquote>\n<p>Training with the maximum likelihood objective makes the model assign\nnonzero probability mass to all responses in SFT dataset, even those of\nlower quality.</p>\n</blockquote>\n<p>RLHFRLrewardreward</p>\n<p>rewardmodelingpointwise rewardpairwise preference</p>\n<p>pointwise\nrewardrewardpositivereward1negativereward0toxicity/classifier</p>\n<p>pairwise\npreference</p>\n<p>RLHFDPO</p>\n<h1 id=\"bradleyterry-model\">BradleyTerry model</h1>\n<p>DPO</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}^{\\mathrm{DPO}}(\\boldsymbol{\\theta})&amp;\n=-\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\text{HF}}}\\left[\\log\\sigma\\Big(\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_w\\mid\\boldsymbol{x})}-\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}{\\pi_{\\text{SFT}}(\\boldsymbol{y}_l\\mid\\boldsymbol{x})}\\Big)\\right]  \\\\\n&amp;=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\thicksim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)\\right)\\right]\n\\end{aligned}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y})=\\beta\\log\\frac{\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}|\\boldsymbol{x})}{\\pi_{\\mathrm{SFT}}(\\boldsymbol{y}|\\boldsymbol{x})}\\]</span></p>\n<p>estimated reward</p>\n<p>DPOBradleyTerry\nmodelBradleyTerry\nmodelresponseresponse</p>\n<p>response</p>\n<p>ODPOresponseoffset</p>\n<h1 id=\"dpo-with-an-offset\">DPO with an Offset</h1>\n<p> <span class=\"math inline\">\\(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)\\)</span>\nGumbel noise</p>\n<p><span class=\"math display\">\\[\\tilde{r}_w\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w),1)\\]</span></p>\n<p><span class=\"math display\">\\[\\tilde{r}_l\\sim\\operatorname{Gumbel}(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l),1)\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[p\\big(\\tilde{r}_w-\\tilde{r}_l&gt;\\Delta_r\\big)=\\sigma(\\Delta_{\\hat{r}_\\theta}-\\Delta_r)\\]</span></p>\n<p>ODPO</p>\n<p><span class=\"math display\">\\[\\mathcal{L}^{\\mathrm{ODPO}}(\\boldsymbol{\\theta})=-\\underset{(\\boldsymbol{x},\\boldsymbol{y}_w,\\boldsymbol{y}_l)\\sim\\mathcal{D}_{\\mathrm{HF}}}{\\operatorname*{\\mathbb{E}}}\\left[\\log\\sigma{\\left(\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\hat{r}_{\\boldsymbol{\\theta}}(\\boldsymbol{x},\\boldsymbol{y}_l)-\\Delta_r\\right)}\\right]\\]</span></p>\n<p>preferred responseestimated rewarddispreferred\nresponseestimated rewardoffset</p>\n<p>offset=0ODPODPO</p>\n<p>ODPOsoftmax margin loss/marginal\nlosslossmarginpenalization</p>\n<p>ODPOoffsetresponseactual rewardincreasing scaling\nfunction</p>\n<p><span class=\"math display\">\\[\\Delta_r=\\alpha\\mathbf{f}\\big(\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_w)-\\mathrm{score}(\\boldsymbol{x},\\boldsymbol{y}_l)\\big)\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<img src=\"/da871ebe/odpo_intro.png\" class title=\"intro\">\n<h1 id=\"\"></h1>\n<p></p>\n<h2 id=\"sentiment-control\">sentiment control</h2>\n<p>sentiment controlpositiveresponse</p>\n<p>GPT2-LargeIMDB\ndatasetfinetuneSFTsentiment\nclassifierrewardresponse</p>\n<p><span class=\"math display\">\\[r_{negative}(\\boldsymbol{x},\\boldsymbol{y}) =\n1-p(\\text{negative}\\mid\\cdot)\\]</span></p>\n<p><span class=\"math display\">\\[r_{positive}(\\boldsymbol{x},\\boldsymbol{y}) =\n1+p(\\text{positive}\\mid\\cdot)\\]</span></p>\n<p>rewardpromptrewardresponse</p>\n<p>DPOODPOoffset</p>\n<p><span class=\"math display\">\\[\\Delta_r=\\log\\left(r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\right)\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha\\)</span> 1</p>\n<p>random\nseedSFT2</p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span> 14\n<span class=\"math inline\">\\(\\{0.1,0.2,\\ldots,1\\}\\cup\\{1,2,3,4,5\\}\\)</span>\n</p>\n<p>250007500,10000DPOODPO2314=84</p>\n<p>sentimentSFTKL\ndivergence</p>\n<img src=\"/da871ebe/sentiment_control.png\" class title=\"sentiment control\">\n<p>sentimentSFTODPODPO</p>\n<h2 id=\"toxicity-control\">toxicity control</h2>\n<p>toxicity controlsentiment\ncontrolresponse</p>\n<p>GPT-neo-2.7b<span class=\"math inline\">\\(\\beta\\)</span>\n <span class=\"math inline\">\\(\\{0.05,0.1,0.2,0.3,0.4,0.5\\}\\)</span>REALTOXICITYPROMPTS100000.3prompt</p>\n<p></p>\n<img src=\"/da871ebe/toxicity_control.png\" class title=\"toxicity control\">\n<p>8000 &amp; 9000ODPO</p>\n<h2 id=\"summarization\">summarization</h2>\n<p>REDDIT TL;DRGPTJ-6B</p>\n<p>DPOODPO100prompttemperatureGPT-4</p>\n<img src=\"/da871ebe/summarization.png\" class title=\"summarization\">\n<p>DPOODPOSFTtemperatureDPOODPOhuman-written</p>\n<h2 id=\"scaling-function\">scaling function</h2>\n<p>offsetrewardlog</p>\n<p><span class=\"math display\">\\[\\Delta_r=\\log r(\\boldsymbol{y}_w)-\\log\nr(\\boldsymbol{y}_l)\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{array}{rcl}\\Delta_r=r(\\boldsymbol{y}_w)-r(\\boldsymbol{y}_l)\\end{array}\\]</span></p>\n<p>5000sentiment control<span class=\"math inline\">\\(\\beta\n\\in \\{0.1,0.2,\\ldots,0.9\\}\\cup\\{1,2,3,4,5\\}\\)</span></p>\n<p></p>\n<img src=\"/da871ebe/scaling_function.png\" class title=\"scaling function\">\n<p>log scalingODPOKL\ndivergence0.40.8rewardlog\nscalingKL divergencereward</p>\n<h2 id=\"\"></h2>\n<p>7500sentiment control<span class=\"math inline\">\\(\\beta=0.5\\)</span><span class=\"math inline\">\\(\\alpha\\in\\{0.0,0.1,0.2,0.3,0.5,0.8,1.\\}\\)</span></p>\n<img src=\"/da871ebe/alpha.png\" class title=\"alpha\">\n<p> <span class=\"math inline\">\\(\\alpha\\)</span>\nSFTreward</p>\n<h1 id=\"\"></h1>\n<p>ODPODPOoffset</p>\n<p>ODPOLLAMA</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Direct Preference Optimization with an Offset\nhttps://arxiv.org/pdf/2402.10571</p>\n"},{"title":"-","abbrlink":"f5c015c","date":"2024-05-13T08:47:13.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n  \n\nspeculative decoding  \n\n#   \n\n202211GoogleFast Inference from Transformers via Speculative DecodingDeepMind2023Accelerating Large Language Model Decoding with Speculative SamplingideaGoogleDeepMind  \n\nspeculative decoding  \n- HintonDistilling the Knowledge in a Neural NetworkKnowledge Distillation: A SurveytransformerTinyBERT: Distilling BERT for Natural Language UnderstandingDistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter  \n- Quantized Neural Networks: Training Neural Networks with Low Precision Weights and ActivationsLLM.int8(): 8-bit Matrix Multiplication for Transformers at ScaleZeroquant: Efficient and affordable post-training quantization for large-scale transformersint8int4  \n- Sparse is Enough in Scaling TransformersKVMQAFast Transformer Decoding: One Write-Head is All You NeedGQAGQA: Training Generalized Multi-Query Transformer Models from Multi-Head CheckpointsDeepSeek-V2MLAPrimer: Searching for Efficient Transformers for Language Modeling  \n\n  \n\nstepstep  \n- Dynamic Neural Networks: A Survey  \n- Adaptive Attention Span in Transformers  \n- Consistent Accelerated Inference via Confident Adaptive Transformers  \n- Why should we add early exits to neural networks?  \n- Controlling Computation versus Quality for Neural Sequence Models  \n- The Right Tool for the Job: Matching Model and Instance Complexities  \n- Depth-Adaptive Transformer  \n-   \n\nMoE  \n\nTraining compute-optimal large language modelsscaling law  \n\n  \n\nBlockwise Parallel Decoding for Deep Autoregressive ModelsLossless Acceleration for Seq2seq Generation with Aggressive Decoding  \n\nspeculative decoding2x-3x  \n\n# speculative decoding  \n\ntokentokentokentokentoken  \n\n```\n      \n      \n      EOS\n```\n\ntoken  \n\n```\nstep1\nstep2\nstep3\nstep4EOS\n```\n\n   EOStokendraft token  \n\n```\n      \n      \n      EOS\n```\n\ntoken4  \n\n   EOSdraft tokentoken  \n\n```\n      \n      \n      EOS\n```\n\ntoken  \n\n  token > 0  \n\nspeculative decodingapproximation modeldraft modeltarget model  \n\n  \n\n{% asset_img fi_example.png  %}  \n\napproximation modeltarget modeltokentokentoken  \n\ntarget938token  \n\n  \n\n$M_p$ target model $M_q$ approximation modelprefix  \n\n $M_q$  $\\gamma$ draft token $M_p$  $\\gamma$ draft tokentoken $M_p$ tokentoken  \n\nGoogle  \n\n{% asset_img fi_sd_algo.png  %}  \n\nDeepMind  \n\ntoken $n$ draft token $M_p$ token $n+1$ tokenapproximation model $\\gamma$ draft token $\\gamma+1$ token1target  \n\n  \n- speculative samplingtarget modelapproximation modeltokendraft token  \n-  $\\gamma$   \n- approximation modelapproximation model  \n\nDeepMindGoogleDeepMindGoogle  \n\n{% asset_img acce_alog.png DeepMind %}  \n\n $(.)_+$  $(f(x))_+=\\frac{\\max(0,f(x))}{\\sum_x\\max(0,f(x))}$   \n\n# speculative sampling  \n\ntarget model  \n\ntransformerargmaxtop-klogits  \n\n $p(x)$ target model $M_p$  $q(x)$ approximation model $M_q$   \n\n $x\\sim q(x)$ $q(x)\\leq p(x)$ $x$ $1-\\frac{p(x)}{q(x)}$  $x$ $p'(x)=norm(max(0,p(x)-q(x)))$  $x$   \n\n $norm(max(0,p(x)-q(x)))=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}$  \n\n $q(x)$  $p(x)$target model  \n\napproximation model $\\tilde{x}\\sim q$ $X$  $\\mathbb{P}(X=x)=p(x)$  \n\n $X=x$ $\\tilde{x}=x$  $\\tilde{x}$  $\\tilde{x}$  $\\tilde{x}=x$   \n\n$$\\mathbb{P}(X=x)\\\\=\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\textit{ accepted}|\\tilde{x}=x)\\\\+\\mathbb{P}(\\tilde{x}\\textit{ rejected})\\mathbb{P}(X=x|\\tilde{x}\\textit{ rejected})$$  \n\n  \n\n$$\n\\begin{aligned}\n&\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\text{ }d|\\tilde{x}=x)\\\\=&q(x)\\min\\left(1,\\frac{p(x)}{q(x)}\\right)\\\\=&\\min\\left(q(x),p(x)\\right)\n\\end{aligned}\n$$  \n\n  \n\n$$\\begin{gathered}\n\\mathbb{P}(\\tilde{x}\\textit{ rejected})=1-\\mathbb{P}(\\tilde{x}\\textit{ accepted}) \\\\\n=1-\\sum_{x^{\\prime}}\\mathbb{P}(X=x^{\\prime},\\tilde{x}\\text{ }d) \\\\\n=1-\\sum_{x'}\\min(q(x'),p(x')) \\\\\n=\\sum_{x'}\\max(0,p(x')-q(x')) \\\\\n\\end{gathered}$$  \n\n1ba+b1a $\\sum_{x'}\\max(0,p(x')-q(x'))$  \n\n{% asset_img formula.png  %}  \n\n  \n\n$$\\mathbb{P}(X=x|\\tilde{x}\\text{ rejected})=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}$$  \n\n  \n\n$$\\mathbb{P}(\\tilde{x}\\text{ rejected})\\mathbb{P}(X=x|\\tilde{x}\\text{ rejected})=\\max(0,p(x)-q(x))$$  \n\n  \n\n$$\\mathbb{P}(X=x)\\\\=\\min(q(x),p(x))+\\max(0,p(x)-q(x))\\\\=p(x)$$  \n\ntarget model  \n\n# approximation model  \n\napproximation model $x\\sim q(x)$ target model $\\beta$acceptance rate  \n\n $E(\\beta)$ approximation modeltarget model  \n\n$E(\\beta)$ tokentoken  \n\n $\\alpha=E(\\beta)$ $\\beta$ i.i.d.tokencapped geometric variable  \n\n$$E(\\#\\textit{ generated tokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$$  \n\n $\\gamma$   \n\n{% asset_img fi_expected_token_num.png  %}  \n\n $\\alpha$   \n\n $M_p$  $M_q$ divergence $D_{LK}$  \n\n$$\\begin{aligned}D_{LK}(p,q)=\\sum_x|p(x)-M(x)|=\\sum_x|q(x)-M(x)|\\end{aligned}$$  \n\n $M(x)=\\frac{p(x)+q(x)}2$  \n\n  \n\n$$\n\\begin{aligned}\n&\\sum_x|p(x)-M(x)|\\\\=&\\sum_x\\frac{|p-q|}{2}\\\\=&1-\\sum_x\\frac{p+q-|p-q|}2\\\\=&1-\\sum_x\\min(p(x),q(x))\n\\end{aligned}\n$$  \n\n  \n\n$$D_{LK}(p,q)=1-\\sum_x\\min(p(x),q(x))$$  \n\n$D_{LK}(p,q)$ $M_p$  $M_q$  $D_{LK}(p,q)=0$ $p=q$ $D_{LK}(p,q)=1$ $p$  $q$   \n\n $\\beta$   \n\n$$\n\\begin{aligned}\n\\beta=&E_{x\\sim q(x)}\\begin{cases}1&q(x)\\leq p(x)\\\\\\frac{p(x)}{q(x)}&q(x)>p(x)\\end{cases}\\\\\n=&E_{x\\thicksim q(x)}\\min(1,\\frac{p(x)}{q(x)})\\\\\n=&\\sum_x\\min(p(x),q(x))\\\\\n=&1-D_{LK}(p,q)\n\\end{aligned}\n$$  \n\n  \n\n$$\\alpha=E(\\beta)=1-E(D_{LK}(p,q))=E(\\min(p,q))$$\n\napproximation modeltarget model $\\alpha$   \n\n{% asset_img fi_alpha.png alpha %}  \n\n#   \n\ncost coefficient $c$ $M_q$   $M_p$   \n\n $\\alpha$ $c$  $c$ 0.05  \n\n $M_p$  $T$ $Tc\\gamma+T$  \n\ntoken $E(\\#\\textit{ generated tokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$ token $\\frac{(c\\gamma+1)(1-\\alpha)}{1-\\alpha^{\\gamma+1}}T$improvement factor  \n\n$$\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma c+1)}$$  \n\n $\\alpha>c$ $\\gamma$improvement factor $\\frac{1+\\alpha}{1+c}$$\\gamma=1$  \n\n#   \n\n$M_p$  $\\gamma+1$ tokentokentoken  \n\n $\\hat{c}$  $M_q$  $M_p$ tokenarithmetic operations$\\hat{T}$  $M_p$ tokenarithmetic operations  \n\n $\\hat{T}\\hat{c}\\gamma+\\hat{T}(\\gamma+1)$token $\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$ token $\\hat{T}\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}$  \n\n$\\alpha$ $\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}$   \n\nKV cache\n\n# $\\gamma$   \n\n $\\alpha$  $c$ $\\gamma$ walltime improvement factor $\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma c+1)}$  \n\n $\\alpha$  $c$  $\\gamma$   \n\n{% asset_img fi_choose_gamma.png gamma %}  \n\ntradeoff $\\gamma$   \n\n{% asset_img fi_speed_and_op_table.png  %}  \n\n{% asset_img fi_speed_and_op.png  %}  \n\n{% asset_img fi_walltime.png walltime %}  \n\n$\\beta$  $\\beta$  $\\gamma$  \n\n# approximation model  \n\napproximation modelapproximation modeltarget model  \n\nn-gramapproximation model $\\alpha$   \n\ncopy tokenapproximation model $\\alpha$   \n\napproximation modelBlockwise parallel decoding for deep autoregressive models  \n\n#   \n\nT5approximation modelT5-XXL3+    \n\n{% asset_img fi_t5_result.png T5 %}  \n\n $\\alpha$   \n\n{% asset_img fi_alpha.png alpha %}  \n\ntarget modelapproximation model0.50.9 $\\alpha$ Targmax $\\alpha$   \n\n#   \n\n- 2~3  \n- n-gram  \n- target modelapproximation model  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n# Reference  \n\n1Fast Inference from Transformers via Speculative Decoding https://arxiv.org/abs/2211.17192  \n2Accelerating Large Language Model Decoding with Speculative Sampling https://arxiv.org/abs/2302.01318  \n","source":"_posts/cs/nlp/2024/05/-.md","raw":"---\ntitle: -\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: f5c015c\ndate: 2024-05-13 16:47:13\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n  \n\nspeculative decoding  \n\n#   \n\n202211GoogleFast Inference from Transformers via Speculative DecodingDeepMind2023Accelerating Large Language Model Decoding with Speculative SamplingideaGoogleDeepMind  \n\nspeculative decoding  \n- HintonDistilling the Knowledge in a Neural NetworkKnowledge Distillation: A SurveytransformerTinyBERT: Distilling BERT for Natural Language UnderstandingDistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter  \n- Quantized Neural Networks: Training Neural Networks with Low Precision Weights and ActivationsLLM.int8(): 8-bit Matrix Multiplication for Transformers at ScaleZeroquant: Efficient and affordable post-training quantization for large-scale transformersint8int4  \n- Sparse is Enough in Scaling TransformersKVMQAFast Transformer Decoding: One Write-Head is All You NeedGQAGQA: Training Generalized Multi-Query Transformer Models from Multi-Head CheckpointsDeepSeek-V2MLAPrimer: Searching for Efficient Transformers for Language Modeling  \n\n  \n\nstepstep  \n- Dynamic Neural Networks: A Survey  \n- Adaptive Attention Span in Transformers  \n- Consistent Accelerated Inference via Confident Adaptive Transformers  \n- Why should we add early exits to neural networks?  \n- Controlling Computation versus Quality for Neural Sequence Models  \n- The Right Tool for the Job: Matching Model and Instance Complexities  \n- Depth-Adaptive Transformer  \n-   \n\nMoE  \n\nTraining compute-optimal large language modelsscaling law  \n\n  \n\nBlockwise Parallel Decoding for Deep Autoregressive ModelsLossless Acceleration for Seq2seq Generation with Aggressive Decoding  \n\nspeculative decoding2x-3x  \n\n# speculative decoding  \n\ntokentokentokentokentoken  \n\n```\n      \n      \n      EOS\n```\n\ntoken  \n\n```\nstep1\nstep2\nstep3\nstep4EOS\n```\n\n   EOStokendraft token  \n\n```\n      \n      \n      EOS\n```\n\ntoken4  \n\n   EOSdraft tokentoken  \n\n```\n      \n      \n      EOS\n```\n\ntoken  \n\n  token > 0  \n\nspeculative decodingapproximation modeldraft modeltarget model  \n\n  \n\n{% asset_img fi_example.png  %}  \n\napproximation modeltarget modeltokentokentoken  \n\ntarget938token  \n\n  \n\n$M_p$ target model $M_q$ approximation modelprefix  \n\n $M_q$  $\\gamma$ draft token $M_p$  $\\gamma$ draft tokentoken $M_p$ tokentoken  \n\nGoogle  \n\n{% asset_img fi_sd_algo.png  %}  \n\nDeepMind  \n\ntoken $n$ draft token $M_p$ token $n+1$ tokenapproximation model $\\gamma$ draft token $\\gamma+1$ token1target  \n\n  \n- speculative samplingtarget modelapproximation modeltokendraft token  \n-  $\\gamma$   \n- approximation modelapproximation model  \n\nDeepMindGoogleDeepMindGoogle  \n\n{% asset_img acce_alog.png DeepMind %}  \n\n $(.)_+$  $(f(x))_+=\\frac{\\max(0,f(x))}{\\sum_x\\max(0,f(x))}$   \n\n# speculative sampling  \n\ntarget model  \n\ntransformerargmaxtop-klogits  \n\n $p(x)$ target model $M_p$  $q(x)$ approximation model $M_q$   \n\n $x\\sim q(x)$ $q(x)\\leq p(x)$ $x$ $1-\\frac{p(x)}{q(x)}$  $x$ $p'(x)=norm(max(0,p(x)-q(x)))$  $x$   \n\n $norm(max(0,p(x)-q(x)))=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}$  \n\n $q(x)$  $p(x)$target model  \n\napproximation model $\\tilde{x}\\sim q$ $X$  $\\mathbb{P}(X=x)=p(x)$  \n\n $X=x$ $\\tilde{x}=x$  $\\tilde{x}$  $\\tilde{x}$  $\\tilde{x}=x$   \n\n$$\\mathbb{P}(X=x)\\\\=\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\textit{ accepted}|\\tilde{x}=x)\\\\+\\mathbb{P}(\\tilde{x}\\textit{ rejected})\\mathbb{P}(X=x|\\tilde{x}\\textit{ rejected})$$  \n\n  \n\n$$\n\\begin{aligned}\n&\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\text{ }d|\\tilde{x}=x)\\\\=&q(x)\\min\\left(1,\\frac{p(x)}{q(x)}\\right)\\\\=&\\min\\left(q(x),p(x)\\right)\n\\end{aligned}\n$$  \n\n  \n\n$$\\begin{gathered}\n\\mathbb{P}(\\tilde{x}\\textit{ rejected})=1-\\mathbb{P}(\\tilde{x}\\textit{ accepted}) \\\\\n=1-\\sum_{x^{\\prime}}\\mathbb{P}(X=x^{\\prime},\\tilde{x}\\text{ }d) \\\\\n=1-\\sum_{x'}\\min(q(x'),p(x')) \\\\\n=\\sum_{x'}\\max(0,p(x')-q(x')) \\\\\n\\end{gathered}$$  \n\n1ba+b1a $\\sum_{x'}\\max(0,p(x')-q(x'))$  \n\n{% asset_img formula.png  %}  \n\n  \n\n$$\\mathbb{P}(X=x|\\tilde{x}\\text{ rejected})=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}$$  \n\n  \n\n$$\\mathbb{P}(\\tilde{x}\\text{ rejected})\\mathbb{P}(X=x|\\tilde{x}\\text{ rejected})=\\max(0,p(x)-q(x))$$  \n\n  \n\n$$\\mathbb{P}(X=x)\\\\=\\min(q(x),p(x))+\\max(0,p(x)-q(x))\\\\=p(x)$$  \n\ntarget model  \n\n# approximation model  \n\napproximation model $x\\sim q(x)$ target model $\\beta$acceptance rate  \n\n $E(\\beta)$ approximation modeltarget model  \n\n$E(\\beta)$ tokentoken  \n\n $\\alpha=E(\\beta)$ $\\beta$ i.i.d.tokencapped geometric variable  \n\n$$E(\\#\\textit{ generated tokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$$  \n\n $\\gamma$   \n\n{% asset_img fi_expected_token_num.png  %}  \n\n $\\alpha$   \n\n $M_p$  $M_q$ divergence $D_{LK}$  \n\n$$\\begin{aligned}D_{LK}(p,q)=\\sum_x|p(x)-M(x)|=\\sum_x|q(x)-M(x)|\\end{aligned}$$  \n\n $M(x)=\\frac{p(x)+q(x)}2$  \n\n  \n\n$$\n\\begin{aligned}\n&\\sum_x|p(x)-M(x)|\\\\=&\\sum_x\\frac{|p-q|}{2}\\\\=&1-\\sum_x\\frac{p+q-|p-q|}2\\\\=&1-\\sum_x\\min(p(x),q(x))\n\\end{aligned}\n$$  \n\n  \n\n$$D_{LK}(p,q)=1-\\sum_x\\min(p(x),q(x))$$  \n\n$D_{LK}(p,q)$ $M_p$  $M_q$  $D_{LK}(p,q)=0$ $p=q$ $D_{LK}(p,q)=1$ $p$  $q$   \n\n $\\beta$   \n\n$$\n\\begin{aligned}\n\\beta=&E_{x\\sim q(x)}\\begin{cases}1&q(x)\\leq p(x)\\\\\\frac{p(x)}{q(x)}&q(x)>p(x)\\end{cases}\\\\\n=&E_{x\\thicksim q(x)}\\min(1,\\frac{p(x)}{q(x)})\\\\\n=&\\sum_x\\min(p(x),q(x))\\\\\n=&1-D_{LK}(p,q)\n\\end{aligned}\n$$  \n\n  \n\n$$\\alpha=E(\\beta)=1-E(D_{LK}(p,q))=E(\\min(p,q))$$\n\napproximation modeltarget model $\\alpha$   \n\n{% asset_img fi_alpha.png alpha %}  \n\n#   \n\ncost coefficient $c$ $M_q$   $M_p$   \n\n $\\alpha$ $c$  $c$ 0.05  \n\n $M_p$  $T$ $Tc\\gamma+T$  \n\ntoken $E(\\#\\textit{ generated tokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$ token $\\frac{(c\\gamma+1)(1-\\alpha)}{1-\\alpha^{\\gamma+1}}T$improvement factor  \n\n$$\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma c+1)}$$  \n\n $\\alpha>c$ $\\gamma$improvement factor $\\frac{1+\\alpha}{1+c}$$\\gamma=1$  \n\n#   \n\n$M_p$  $\\gamma+1$ tokentokentoken  \n\n $\\hat{c}$  $M_q$  $M_p$ tokenarithmetic operations$\\hat{T}$  $M_p$ tokenarithmetic operations  \n\n $\\hat{T}\\hat{c}\\gamma+\\hat{T}(\\gamma+1)$token $\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}$ token $\\hat{T}\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}$  \n\n$\\alpha$ $\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}$   \n\nKV cache\n\n# $\\gamma$   \n\n $\\alpha$  $c$ $\\gamma$ walltime improvement factor $\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma c+1)}$  \n\n $\\alpha$  $c$  $\\gamma$   \n\n{% asset_img fi_choose_gamma.png gamma %}  \n\ntradeoff $\\gamma$   \n\n{% asset_img fi_speed_and_op_table.png  %}  \n\n{% asset_img fi_speed_and_op.png  %}  \n\n{% asset_img fi_walltime.png walltime %}  \n\n$\\beta$  $\\beta$  $\\gamma$  \n\n# approximation model  \n\napproximation modelapproximation modeltarget model  \n\nn-gramapproximation model $\\alpha$   \n\ncopy tokenapproximation model $\\alpha$   \n\napproximation modelBlockwise parallel decoding for deep autoregressive models  \n\n#   \n\nT5approximation modelT5-XXL3+    \n\n{% asset_img fi_t5_result.png T5 %}  \n\n $\\alpha$   \n\n{% asset_img fi_alpha.png alpha %}  \n\ntarget modelapproximation model0.50.9 $\\alpha$ Targmax $\\alpha$   \n\n#   \n\n- 2~3  \n- n-gram  \n- target modelapproximation model  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n# Reference  \n\n1Fast Inference from Transformers via Speculative Decoding https://arxiv.org/abs/2211.17192  \n2Accelerating Large Language Model Decoding with Speculative Sampling https://arxiv.org/abs/2302.01318  \n","slug":"cs/nlp/2024/05/-","published":1,"updated":"2024-05-25T03:38:10.437Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmw001e0p4k5v58h187","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p></p>\n<p>speculative\ndecoding</p>\n<h1 id=\"\"></h1>\n<p>202211GoogleFast Inference from Transformers via\nSpeculative\nDecodingDeepMind2023Accelerating\nLarge Language Model Decoding with Speculative\nSamplingideaGoogleDeepMind</p>\n<p>speculative\ndecoding<br>\n- HintonDistilling the Knowledge in a Neural\nNetworkKnowledge\nDistillation: A\nSurveytransformerTinyBERT:\nDistilling BERT for Natural Language UnderstandingDistilBERT, a\ndistilled version of BERT: smaller, faster, cheaper and\nlighter<br>\n- Quantized Neural Networks: Training Neural Networks with\nLow Precision Weights and ActivationsLLM.int8(): 8-bit Matrix\nMultiplication for Transformers at ScaleZeroquant: Efficient and\naffordable post-training quantization for large-scale\ntransformersint8int4<br>\n- Sparse is Enough in Scaling\nTransformersKVMQAFast Transformer Decoding: One\nWrite-Head is All You NeedGQAGQA: Training Generalized\nMulti-Query Transformer Models from Multi-Head\nCheckpointsDeepSeek-V2MLAPrimer:\nSearching for Efficient Transformers for Language Modeling</p>\n<p></p>\n<p>stepstep<br>\n- Dynamic Neural Networks: A Survey<br>\n- Adaptive Attention Span in Transformers<br>\n- Consistent Accelerated Inference via Confident Adaptive\nTransformers<br>\n- Why should we add early exits to neural networks?<br>\n- Controlling Computation versus Quality for Neural Sequence\nModels<br>\n- The Right Tool for the Job: Matching Model and Instance\nComplexities<br>\n- Depth-Adaptive Transformer<br>\n- </p>\n<p>MoE</p>\n<p>Training compute-optimal large language modelsscaling\nlaw</p>\n<p></p>\n<p>Blockwise Parallel Decoding\nfor Deep Autoregressive ModelsLossless Acceleration for Seq2seq\nGeneration with Aggressive Decoding</p>\n<p>speculative\ndecoding2x-3x</p>\n<h1 id=\"speculative-decoding\">speculative decoding</h1>\n<p>tokentokentokentokentoken</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">      </span><br><span class=\"line\">      EOS</span><br></pre></td></tr></table></figure>\n<p>token</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">step1</span><br><span class=\"line\">step2</span><br><span class=\"line\">step3</span><br><span class=\"line\">step4EOS</span><br></pre></td></tr></table></figure>\n<p>  \nEOStokendraft\ntoken</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">      </span><br><span class=\"line\">      EOS</span><br></pre></td></tr></table></figure>\n<p>token4</p>\n<p>\n  EOSdraft\ntokentoken</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">      </span><br><span class=\"line\">      EOS</span><br></pre></td></tr></table></figure>\n<p>token</p>\n<p>\n\ntoken\n&gt; 0</p>\n<p>speculative\ndecodingapproximation\nmodeldraft modeltarget model</p>\n<p></p>\n<img src=\"/f5c015c/fi_example.png\" class title=\"\">\n<p>approximation modeltarget\nmodeltokentokentoken</p>\n<p>target938token</p>\n<p></p>\n<p><span class=\"math inline\">\\(M_p\\)</span> target model <span class=\"math inline\">\\(M_q\\)</span> approximation\nmodelprefix</p>\n<p> <span class=\"math inline\">\\(M_q\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span> draft token <span class=\"math inline\">\\(M_p\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span> draft\ntokentoken\n<span class=\"math inline\">\\(M_p\\)</span>\ntokentoken</p>\n<p>Google</p>\n<img src=\"/f5c015c/fi_sd_algo.png\" class title=\"\">\n<p>DeepMind</p>\n<p>token <span class=\"math inline\">\\(n\\)</span> draft token <span class=\"math inline\">\\(M_p\\)</span>\ntoken <span class=\"math inline\">\\(n+1\\)</span> tokenapproximation\nmodel <span class=\"math inline\">\\(\\gamma\\)</span> draft\ntoken <span class=\"math inline\">\\(\\gamma+1\\)</span>\ntoken1target</p>\n<p><br>\n- speculative samplingtarget modelapproximation\nmodeltokendraft\ntoken<br>\n-  <span class=\"math inline\">\\(\\gamma\\)</span> <br>\n- approximation modelapproximation\nmodel</p>\n<p>DeepMindGoogleDeepMindGoogle</p>\n<img src=\"/f5c015c/acce_alog.png\" class title=\"DeepMind\">\n<p> <span class=\"math inline\">\\((.)_+\\)</span>  <span class=\"math inline\">\\((f(x))_+=\\frac{\\max(0,f(x))}{\\sum_x\\max(0,f(x))}\\)</span>\n</p>\n<h1 id=\"speculative-sampling\">speculative sampling</h1>\n<p>target\nmodel</p>\n<p>transformerargmaxtop-klogits</p>\n<p> <span class=\"math inline\">\\(p(x)\\)</span> target model <span class=\"math inline\">\\(M_p\\)</span>  <span class=\"math inline\">\\(q(x)\\)</span> approximation model <span class=\"math inline\">\\(M_q\\)</span> </p>\n<p> <span class=\"math inline\">\\(x\\sim\nq(x)\\)</span> <span class=\"math inline\">\\(q(x)\\leq\np(x)\\)</span> <span class=\"math inline\">\\(x\\)</span>\n<span class=\"math inline\">\\(1-\\frac{p(x)}{q(x)}\\)</span> \n<span class=\"math inline\">\\(x\\)</span> <span class=\"math inline\">\\(p&#39;(x)=norm(max(0,p(x)-q(x)))\\)</span>\n <span class=\"math inline\">\\(x\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(norm(max(0,p(x)-q(x)))=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(q(x)\\)</span> \n<span class=\"math inline\">\\(p(x)\\)</span>target\nmodel</p>\n<p>approximation model <span class=\"math inline\">\\(\\tilde{x}\\sim q\\)</span> <span class=\"math inline\">\\(X\\)</span>  <span class=\"math inline\">\\(\\mathbb{P}(X=x)=p(x)\\)</span></p>\n<p> <span class=\"math inline\">\\(X=x\\)</span> <span class=\"math inline\">\\(\\tilde{x}=x\\)</span>  <span class=\"math inline\">\\(\\tilde{x}\\)</span>  <span class=\"math inline\">\\(\\tilde{x}\\)</span>  <span class=\"math inline\">\\(\\tilde{x}=x\\)</span> </p>\n<p><span class=\"math display\">\\[\\mathbb{P}(X=x)\\\\=\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\textit{\naccepted}|\\tilde{x}=x)\\\\+\\mathbb{P}(\\tilde{x}\\textit{\nrejected})\\mathbb{P}(X=x|\\tilde{x}\\textit{ rejected})\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\text{\n}d|\\tilde{x}=x)\\\\=&amp;q(x)\\min\\left(1,\\frac{p(x)}{q(x)}\\right)\\\\=&amp;\\min\\left(q(x),p(x)\\right)\n\\end{aligned}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{gathered}\n\\mathbb{P}(\\tilde{x}\\textit{ rejected})=1-\\mathbb{P}(\\tilde{x}\\textit{\naccepted}) \\\\\n=1-\\sum_{x^{\\prime}}\\mathbb{P}(X=x^{\\prime},\\tilde{x}\\text{ }d)\n\\\\\n=1-\\sum_{x&#39;}\\min(q(x&#39;),p(x&#39;)) \\\\\n=\\sum_{x&#39;}\\max(0,p(x&#39;)-q(x&#39;)) \\\\\n\\end{gathered}\\]</span></p>\n<p>1ba+b1a\n<span class=\"math inline\">\\(\\sum_{x&#39;}\\max(0,p(x&#39;)-q(x&#39;))\\)</span></p>\n<img src=\"/f5c015c/formula.png\" class title=\"\">\n<p></p>\n<p><span class=\"math display\">\\[\\mathbb{P}(X=x|\\tilde{x}\\text{\nrejected})=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\mathbb{P}(\\tilde{x}\\text{\nrejected})\\mathbb{P}(X=x|\\tilde{x}\\text{\nrejected})=\\max(0,p(x)-q(x))\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\mathbb{P}(X=x)\\\\=\\min(q(x),p(x))+\\max(0,p(x)-q(x))\\\\=p(x)\\]</span></p>\n<p>target\nmodel</p>\n<h1 id=\"approximation-model\">approximation model</h1>\n<p>approximation model <span class=\"math inline\">\\(x\\sim\nq(x)\\)</span> target model <span class=\"math inline\">\\(\\beta\\)</span>acceptance\nrate</p>\n<p> <span class=\"math inline\">\\(E(\\beta)\\)</span>\napproximation modeltarget model</p>\n<p><span class=\"math inline\">\\(E(\\beta)\\)</span>\ntokentoken</p>\n<p> <span class=\"math inline\">\\(\\alpha=E(\\beta)\\)</span>\n<span class=\"math inline\">\\(\\beta\\)</span>\ni.i.d.tokencapped\ngeometric variable</p>\n<p><span class=\"math display\">\\[E(\\#\\textit{ generated\ntokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\gamma\\)</span> </p>\n<img src=\"/f5c015c/fi_expected_token_num.png\" class title=\"\">\n<p> <span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<p> <span class=\"math inline\">\\(M_p\\)</span>  <span class=\"math inline\">\\(M_q\\)</span> divergence <span class=\"math inline\">\\(D_{LK}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}D_{LK}(p,q)=\\sum_x|p(x)-M(x)|=\\sum_x|q(x)-M(x)|\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(M(x)=\\frac{p(x)+q(x)}2\\)</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\sum_x|p(x)-M(x)|\\\\=&amp;\\sum_x\\frac{|p-q|}{2}\\\\=&amp;1-\\sum_x\\frac{p+q-|p-q|}2\\\\=&amp;1-\\sum_x\\min(p(x),q(x))\n\\end{aligned}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[D_{LK}(p,q)=1-\\sum_x\\min(p(x),q(x))\\]</span></p>\n<p><span class=\"math inline\">\\(D_{LK}(p,q)\\)</span> <span class=\"math inline\">\\(M_p\\)</span>  <span class=\"math inline\">\\(M_q\\)</span>  <span class=\"math inline\">\\(D_{LK}(p,q)=0\\)</span> <span class=\"math inline\">\\(p=q\\)</span> <span class=\"math inline\">\\(D_{LK}(p,q)=1\\)</span> <span class=\"math inline\">\\(p\\)</span>  <span class=\"math inline\">\\(q\\)</span> </p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n\\beta=&amp;E_{x\\sim q(x)}\\begin{cases}1&amp;q(x)\\leq\np(x)\\\\\\frac{p(x)}{q(x)}&amp;q(x)&gt;p(x)\\end{cases}\\\\\n=&amp;E_{x\\thicksim q(x)}\\min(1,\\frac{p(x)}{q(x)})\\\\\n=&amp;\\sum_x\\min(p(x),q(x))\\\\\n=&amp;1-D_{LK}(p,q)\n\\end{aligned}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\alpha=E(\\beta)=1-E(D_{LK}(p,q))=E(\\min(p,q))\\]</span></p>\n<p>approximation modeltarget model <span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<img src=\"/f5c015c/fi_alpha.png\" class title=\"alpha\">\n<h1 id=\"\"></h1>\n<p>cost coefficient <span class=\"math inline\">\\(c\\)</span>\n<span class=\"math inline\">\\(M_q\\)</span>   <span class=\"math inline\">\\(M_p\\)</span> </p>\n<p> <span class=\"math inline\">\\(\\alpha\\)</span>\n<span class=\"math inline\">\\(c\\)</span>\n <span class=\"math inline\">\\(c\\)</span> 0.05</p>\n<p> <span class=\"math inline\">\\(M_p\\)</span> \n<span class=\"math inline\">\\(T\\)</span> <span class=\"math inline\">\\(Tc\\gamma+T\\)</span></p>\n<p>token <span class=\"math inline\">\\(E(\\#\\textit{ generated\ntokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}\\)</span>\ntoken <span class=\"math inline\">\\(\\frac{(c\\gamma+1)(1-\\alpha)}{1-\\alpha^{\\gamma+1}}T\\)</span>improvement\nfactor</p>\n<p><span class=\"math display\">\\[\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma\nc+1)}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha&gt;c\\)</span>\n<span class=\"math inline\">\\(\\gamma\\)</span>improvement\nfactor <span class=\"math inline\">\\(\\frac{1+\\alpha}{1+c}\\)</span><span class=\"math inline\">\\(\\gamma=1\\)</span></p>\n<h1 id=\"\"></h1>\n<p><span class=\"math inline\">\\(M_p\\)</span>  <span class=\"math inline\">\\(\\gamma+1\\)</span>\ntokentokentoken</p>\n<p> <span class=\"math inline\">\\(\\hat{c}\\)</span>  <span class=\"math inline\">\\(M_q\\)</span>  <span class=\"math inline\">\\(M_p\\)</span> tokenarithmetic\noperations<span class=\"math inline\">\\(\\hat{T}\\)</span>  <span class=\"math inline\">\\(M_p\\)</span> tokenarithmetic\noperations</p>\n<p> <span class=\"math inline\">\\(\\hat{T}\\hat{c}\\gamma+\\hat{T}(\\gamma+1)\\)</span>token\n<span class=\"math inline\">\\(\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}\\)</span>\ntoken <span class=\"math inline\">\\(\\hat{T}\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}\\)</span></p>\n<p><span class=\"math inline\">\\(\\alpha\\)</span> <span class=\"math inline\">\\(\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}\\)</span>\n</p>\n<p>KV cache</p>\n<h1 id=\"gamma-\"><span class=\"math inline\">\\(\\gamma\\)</span>\n</h1>\n<p> <span class=\"math inline\">\\(\\alpha\\)</span>  <span class=\"math inline\">\\(c\\)</span> <span class=\"math inline\">\\(\\gamma\\)</span> walltime improvement\nfactor <span class=\"math inline\">\\(\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma\nc+1)}\\)</span></p>\n<p> <span class=\"math inline\">\\(\\alpha\\)</span>  <span class=\"math inline\">\\(c\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span> </p>\n<img src=\"/f5c015c/fi_choose_gamma.png\" class title=\"gamma\">\n<p>tradeoff <span class=\"math inline\">\\(\\gamma\\)</span>\n</p>\n<img src=\"/f5c015c/fi_speed_and_op_table.png\" class title=\"\">\n<img src=\"/f5c015c/fi_speed_and_op.png\" class title=\"\">\n<img src=\"/f5c015c/fi_walltime.png\" class title=\"walltime\">\n<p><span class=\"math inline\">\\(\\beta\\)</span>\n <span class=\"math inline\">\\(\\beta\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span></p>\n<h1 id=\"approximation-model\">approximation model</h1>\n<p>approximation\nmodelapproximation modeltarget\nmodel</p>\n<p>n-gramapproximation\nmodel <span class=\"math inline\">\\(\\alpha\\)</span>\n</p>\n<p>copy\ntokenapproximation model <span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<p>approximation modelBlockwise parallel decoding for\ndeep autoregressive models</p>\n<h1 id=\"\"></h1>\n<p>T5approximation\nmodelT5-XXL3+</p>\n<img src=\"/f5c015c/fi_t5_result.png\" class title=\"T5\">\n<p> <span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<img src=\"/f5c015c/fi_alpha.png\" class title=\"alpha\">\n<p>target modelapproximation\nmodel0.50.9 <span class=\"math inline\">\\(\\alpha\\)</span>\nTargmax\n<span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<h1 id=\"\"></h1>\n<ul>\n<li>2~3<br>\n</li>\n<li>n-gram<br>\n</li>\n<li>target modelapproximation\nmodel</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Fast Inference from Transformers via Speculative Decoding\nhttps://arxiv.org/abs/2211.17192<br>\n2Accelerating Large Language Model Decoding with Speculative\nSampling https://arxiv.org/abs/2302.01318</p>\n","length":10078,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p></p>\n<p>speculative\ndecoding</p>\n<h1 id=\"\"></h1>\n<p>202211GoogleFast Inference from Transformers via\nSpeculative\nDecodingDeepMind2023Accelerating\nLarge Language Model Decoding with Speculative\nSamplingideaGoogleDeepMind</p>\n<p>speculative\ndecoding<br>\n- HintonDistilling the Knowledge in a Neural\nNetworkKnowledge\nDistillation: A\nSurveytransformerTinyBERT:\nDistilling BERT for Natural Language UnderstandingDistilBERT, a\ndistilled version of BERT: smaller, faster, cheaper and\nlighter<br>\n- Quantized Neural Networks: Training Neural Networks with\nLow Precision Weights and ActivationsLLM.int8(): 8-bit Matrix\nMultiplication for Transformers at ScaleZeroquant: Efficient and\naffordable post-training quantization for large-scale\ntransformersint8int4<br>\n- Sparse is Enough in Scaling\nTransformersKVMQAFast Transformer Decoding: One\nWrite-Head is All You NeedGQAGQA: Training Generalized\nMulti-Query Transformer Models from Multi-Head\nCheckpointsDeepSeek-V2MLAPrimer:\nSearching for Efficient Transformers for Language Modeling</p>\n<p></p>\n<p>stepstep<br>\n- Dynamic Neural Networks: A Survey<br>\n- Adaptive Attention Span in Transformers<br>\n- Consistent Accelerated Inference via Confident Adaptive\nTransformers<br>\n- Why should we add early exits to neural networks?<br>\n- Controlling Computation versus Quality for Neural Sequence\nModels<br>\n- The Right Tool for the Job: Matching Model and Instance\nComplexities<br>\n- Depth-Adaptive Transformer<br>\n- </p>\n<p>MoE</p>\n<p>Training compute-optimal large language modelsscaling\nlaw</p>\n<p></p>\n<p>Blockwise Parallel Decoding\nfor Deep Autoregressive ModelsLossless Acceleration for Seq2seq\nGeneration with Aggressive Decoding</p>\n<p>speculative\ndecoding2x-3x</p>\n<h1 id=\"speculative-decoding\">speculative decoding</h1>\n<p>tokentokentokentokentoken</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">      </span><br><span class=\"line\">      EOS</span><br></pre></td></tr></table></figure>\n<p>token</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">step1</span><br><span class=\"line\">step2</span><br><span class=\"line\">step3</span><br><span class=\"line\">step4EOS</span><br></pre></td></tr></table></figure>\n<p>  \nEOStokendraft\ntoken</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">      </span><br><span class=\"line\">      EOS</span><br></pre></td></tr></table></figure>\n<p>token4</p>\n<p>\n  EOSdraft\ntokentoken</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">      </span><br><span class=\"line\">      </span><br><span class=\"line\">      EOS</span><br></pre></td></tr></table></figure>\n<p>token</p>\n<p>\n\ntoken\n&gt; 0</p>\n<p>speculative\ndecodingapproximation\nmodeldraft modeltarget model</p>\n<p></p>\n<img src=\"/f5c015c/fi_example.png\" class title=\"\">\n<p>approximation modeltarget\nmodeltokentokentoken</p>\n<p>target938token</p>\n<p></p>\n<p><span class=\"math inline\">\\(M_p\\)</span> target model <span class=\"math inline\">\\(M_q\\)</span> approximation\nmodelprefix</p>\n<p> <span class=\"math inline\">\\(M_q\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span> draft token <span class=\"math inline\">\\(M_p\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span> draft\ntokentoken\n<span class=\"math inline\">\\(M_p\\)</span>\ntokentoken</p>\n<p>Google</p>\n<img src=\"/f5c015c/fi_sd_algo.png\" class title=\"\">\n<p>DeepMind</p>\n<p>token <span class=\"math inline\">\\(n\\)</span> draft token <span class=\"math inline\">\\(M_p\\)</span>\ntoken <span class=\"math inline\">\\(n+1\\)</span> tokenapproximation\nmodel <span class=\"math inline\">\\(\\gamma\\)</span> draft\ntoken <span class=\"math inline\">\\(\\gamma+1\\)</span>\ntoken1target</p>\n<p><br>\n- speculative samplingtarget modelapproximation\nmodeltokendraft\ntoken<br>\n-  <span class=\"math inline\">\\(\\gamma\\)</span> <br>\n- approximation modelapproximation\nmodel</p>\n<p>DeepMindGoogleDeepMindGoogle</p>\n<img src=\"/f5c015c/acce_alog.png\" class title=\"DeepMind\">\n<p> <span class=\"math inline\">\\((.)_+\\)</span>  <span class=\"math inline\">\\((f(x))_+=\\frac{\\max(0,f(x))}{\\sum_x\\max(0,f(x))}\\)</span>\n</p>\n<h1 id=\"speculative-sampling\">speculative sampling</h1>\n<p>target\nmodel</p>\n<p>transformerargmaxtop-klogits</p>\n<p> <span class=\"math inline\">\\(p(x)\\)</span> target model <span class=\"math inline\">\\(M_p\\)</span>  <span class=\"math inline\">\\(q(x)\\)</span> approximation model <span class=\"math inline\">\\(M_q\\)</span> </p>\n<p> <span class=\"math inline\">\\(x\\sim\nq(x)\\)</span> <span class=\"math inline\">\\(q(x)\\leq\np(x)\\)</span> <span class=\"math inline\">\\(x\\)</span>\n<span class=\"math inline\">\\(1-\\frac{p(x)}{q(x)}\\)</span> \n<span class=\"math inline\">\\(x\\)</span> <span class=\"math inline\">\\(p&#39;(x)=norm(max(0,p(x)-q(x)))\\)</span>\n <span class=\"math inline\">\\(x\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(norm(max(0,p(x)-q(x)))=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(q(x)\\)</span> \n<span class=\"math inline\">\\(p(x)\\)</span>target\nmodel</p>\n<p>approximation model <span class=\"math inline\">\\(\\tilde{x}\\sim q\\)</span> <span class=\"math inline\">\\(X\\)</span>  <span class=\"math inline\">\\(\\mathbb{P}(X=x)=p(x)\\)</span></p>\n<p> <span class=\"math inline\">\\(X=x\\)</span> <span class=\"math inline\">\\(\\tilde{x}=x\\)</span>  <span class=\"math inline\">\\(\\tilde{x}\\)</span>  <span class=\"math inline\">\\(\\tilde{x}\\)</span>  <span class=\"math inline\">\\(\\tilde{x}=x\\)</span> </p>\n<p><span class=\"math display\">\\[\\mathbb{P}(X=x)\\\\=\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\textit{\naccepted}|\\tilde{x}=x)\\\\+\\mathbb{P}(\\tilde{x}\\textit{\nrejected})\\mathbb{P}(X=x|\\tilde{x}\\textit{ rejected})\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\mathbb{P}(\\tilde{x}=x)\\mathbb{P}(\\tilde{x}\\text{\n}d|\\tilde{x}=x)\\\\=&amp;q(x)\\min\\left(1,\\frac{p(x)}{q(x)}\\right)\\\\=&amp;\\min\\left(q(x),p(x)\\right)\n\\end{aligned}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{gathered}\n\\mathbb{P}(\\tilde{x}\\textit{ rejected})=1-\\mathbb{P}(\\tilde{x}\\textit{\naccepted}) \\\\\n=1-\\sum_{x^{\\prime}}\\mathbb{P}(X=x^{\\prime},\\tilde{x}\\text{ }d)\n\\\\\n=1-\\sum_{x&#39;}\\min(q(x&#39;),p(x&#39;)) \\\\\n=\\sum_{x&#39;}\\max(0,p(x&#39;)-q(x&#39;)) \\\\\n\\end{gathered}\\]</span></p>\n<p>1ba+b1a\n<span class=\"math inline\">\\(\\sum_{x&#39;}\\max(0,p(x&#39;)-q(x&#39;))\\)</span></p>\n<img src=\"/f5c015c/formula.png\" class title=\"\">\n<p></p>\n<p><span class=\"math display\">\\[\\mathbb{P}(X=x|\\tilde{x}\\text{\nrejected})=\\frac{\\max(0,p(x)-q(x))}{\\sum_x\\max(0,p(x)-q(x))}\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\mathbb{P}(\\tilde{x}\\text{\nrejected})\\mathbb{P}(X=x|\\tilde{x}\\text{\nrejected})=\\max(0,p(x)-q(x))\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\mathbb{P}(X=x)\\\\=\\min(q(x),p(x))+\\max(0,p(x)-q(x))\\\\=p(x)\\]</span></p>\n<p>target\nmodel</p>\n<h1 id=\"approximation-model\">approximation model</h1>\n<p>approximation model <span class=\"math inline\">\\(x\\sim\nq(x)\\)</span> target model <span class=\"math inline\">\\(\\beta\\)</span>acceptance\nrate</p>\n<p> <span class=\"math inline\">\\(E(\\beta)\\)</span>\napproximation modeltarget model</p>\n<p><span class=\"math inline\">\\(E(\\beta)\\)</span>\ntokentoken</p>\n<p> <span class=\"math inline\">\\(\\alpha=E(\\beta)\\)</span>\n<span class=\"math inline\">\\(\\beta\\)</span>\ni.i.d.tokencapped\ngeometric variable</p>\n<p><span class=\"math display\">\\[E(\\#\\textit{ generated\ntokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\gamma\\)</span> </p>\n<img src=\"/f5c015c/fi_expected_token_num.png\" class title=\"\">\n<p> <span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<p> <span class=\"math inline\">\\(M_p\\)</span>  <span class=\"math inline\">\\(M_q\\)</span> divergence <span class=\"math inline\">\\(D_{LK}\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}D_{LK}(p,q)=\\sum_x|p(x)-M(x)|=\\sum_x|q(x)-M(x)|\\end{aligned}\\]</span></p>\n<p> <span class=\"math inline\">\\(M(x)=\\frac{p(x)+q(x)}2\\)</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n&amp;\\sum_x|p(x)-M(x)|\\\\=&amp;\\sum_x\\frac{|p-q|}{2}\\\\=&amp;1-\\sum_x\\frac{p+q-|p-q|}2\\\\=&amp;1-\\sum_x\\min(p(x),q(x))\n\\end{aligned}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[D_{LK}(p,q)=1-\\sum_x\\min(p(x),q(x))\\]</span></p>\n<p><span class=\"math inline\">\\(D_{LK}(p,q)\\)</span> <span class=\"math inline\">\\(M_p\\)</span>  <span class=\"math inline\">\\(M_q\\)</span>  <span class=\"math inline\">\\(D_{LK}(p,q)=0\\)</span> <span class=\"math inline\">\\(p=q\\)</span> <span class=\"math inline\">\\(D_{LK}(p,q)=1\\)</span> <span class=\"math inline\">\\(p\\)</span>  <span class=\"math inline\">\\(q\\)</span> </p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span> </p>\n<p><span class=\"math display\">\\[\n\\begin{aligned}\n\\beta=&amp;E_{x\\sim q(x)}\\begin{cases}1&amp;q(x)\\leq\np(x)\\\\\\frac{p(x)}{q(x)}&amp;q(x)&gt;p(x)\\end{cases}\\\\\n=&amp;E_{x\\thicksim q(x)}\\min(1,\\frac{p(x)}{q(x)})\\\\\n=&amp;\\sum_x\\min(p(x),q(x))\\\\\n=&amp;1-D_{LK}(p,q)\n\\end{aligned}\n\\]</span></p>\n<p></p>\n<p><span class=\"math display\">\\[\\alpha=E(\\beta)=1-E(D_{LK}(p,q))=E(\\min(p,q))\\]</span></p>\n<p>approximation modeltarget model <span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<img src=\"/f5c015c/fi_alpha.png\" class title=\"alpha\">\n<h1 id=\"\"></h1>\n<p>cost coefficient <span class=\"math inline\">\\(c\\)</span>\n<span class=\"math inline\">\\(M_q\\)</span>   <span class=\"math inline\">\\(M_p\\)</span> </p>\n<p> <span class=\"math inline\">\\(\\alpha\\)</span>\n<span class=\"math inline\">\\(c\\)</span>\n <span class=\"math inline\">\\(c\\)</span> 0.05</p>\n<p> <span class=\"math inline\">\\(M_p\\)</span> \n<span class=\"math inline\">\\(T\\)</span> <span class=\"math inline\">\\(Tc\\gamma+T\\)</span></p>\n<p>token <span class=\"math inline\">\\(E(\\#\\textit{ generated\ntokens})=\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}\\)</span>\ntoken <span class=\"math inline\">\\(\\frac{(c\\gamma+1)(1-\\alpha)}{1-\\alpha^{\\gamma+1}}T\\)</span>improvement\nfactor</p>\n<p><span class=\"math display\">\\[\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma\nc+1)}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha&gt;c\\)</span>\n<span class=\"math inline\">\\(\\gamma\\)</span>improvement\nfactor <span class=\"math inline\">\\(\\frac{1+\\alpha}{1+c}\\)</span><span class=\"math inline\">\\(\\gamma=1\\)</span></p>\n<h1 id=\"\"></h1>\n<p><span class=\"math inline\">\\(M_p\\)</span>  <span class=\"math inline\">\\(\\gamma+1\\)</span>\ntokentokentoken</p>\n<p> <span class=\"math inline\">\\(\\hat{c}\\)</span>  <span class=\"math inline\">\\(M_q\\)</span>  <span class=\"math inline\">\\(M_p\\)</span> tokenarithmetic\noperations<span class=\"math inline\">\\(\\hat{T}\\)</span>  <span class=\"math inline\">\\(M_p\\)</span> tokenarithmetic\noperations</p>\n<p> <span class=\"math inline\">\\(\\hat{T}\\hat{c}\\gamma+\\hat{T}(\\gamma+1)\\)</span>token\n<span class=\"math inline\">\\(\\frac{1-\\alpha^{\\gamma+1}}{1-\\alpha}\\)</span>\ntoken <span class=\"math inline\">\\(\\hat{T}\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}\\)</span></p>\n<p><span class=\"math inline\">\\(\\alpha\\)</span> <span class=\"math inline\">\\(\\frac{(1-\\alpha)(\\gamma\\hat{c}+\\gamma+1)}{1-\\alpha^{\\gamma+1}}\\)</span>\n</p>\n<p>KV cache</p>\n<h1 id=\"gamma-\"><span class=\"math inline\">\\(\\gamma\\)</span>\n</h1>\n<p> <span class=\"math inline\">\\(\\alpha\\)</span>  <span class=\"math inline\">\\(c\\)</span> <span class=\"math inline\">\\(\\gamma\\)</span> walltime improvement\nfactor <span class=\"math inline\">\\(\\frac{1-\\alpha^{\\gamma+1}}{(1-\\alpha)(\\gamma\nc+1)}\\)</span></p>\n<p> <span class=\"math inline\">\\(\\alpha\\)</span>  <span class=\"math inline\">\\(c\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span> </p>\n<img src=\"/f5c015c/fi_choose_gamma.png\" class title=\"gamma\">\n<p>tradeoff <span class=\"math inline\">\\(\\gamma\\)</span>\n</p>\n<img src=\"/f5c015c/fi_speed_and_op_table.png\" class title=\"\">\n<img src=\"/f5c015c/fi_speed_and_op.png\" class title=\"\">\n<img src=\"/f5c015c/fi_walltime.png\" class title=\"walltime\">\n<p><span class=\"math inline\">\\(\\beta\\)</span>\n <span class=\"math inline\">\\(\\beta\\)</span>  <span class=\"math inline\">\\(\\gamma\\)</span></p>\n<h1 id=\"approximation-model\">approximation model</h1>\n<p>approximation\nmodelapproximation modeltarget\nmodel</p>\n<p>n-gramapproximation\nmodel <span class=\"math inline\">\\(\\alpha\\)</span>\n</p>\n<p>copy\ntokenapproximation model <span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<p>approximation modelBlockwise parallel decoding for\ndeep autoregressive models</p>\n<h1 id=\"\"></h1>\n<p>T5approximation\nmodelT5-XXL3+</p>\n<img src=\"/f5c015c/fi_t5_result.png\" class title=\"T5\">\n<p> <span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<img src=\"/f5c015c/fi_alpha.png\" class title=\"alpha\">\n<p>target modelapproximation\nmodel0.50.9 <span class=\"math inline\">\\(\\alpha\\)</span>\nTargmax\n<span class=\"math inline\">\\(\\alpha\\)</span> </p>\n<h1 id=\"\"></h1>\n<ul>\n<li>2~3<br>\n</li>\n<li>n-gram<br>\n</li>\n<li>target modelapproximation\nmodel</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Fast Inference from Transformers via Speculative Decoding\nhttps://arxiv.org/abs/2211.17192<br>\n2Accelerating Large Language Model Decoding with Speculative\nSampling https://arxiv.org/abs/2302.01318</p>\n"},{"title":"(5)","abbrlink":"336f2f3e","date":"2024-05-04T07:47:14.000Z","_content":"\n![](/images/cover.png)  \n\n\n\n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~  \n\n~\n\n***  \n\n# 1.bf16fp16  \n\n16bit  \n\nfp161510fp16bf16bf16  \n\nbf16187fp16bf16fp16  \n\nbf16fp16  \n\n{% asset_img bfloat16.jpeg bf16 %}  \n\n\n# 2.NTK-aware interpolation  \n\n1.NTK  \n\n2.NTKRoPEbase  \n\n# 3.LLMNTK-by-parts  \n\nNTK-by-partsNTKNTK-awareRoPENTK-by-parts<=1/32  \n\nNTK-by-parts  \n\n{% asset_img ntk_by_parts.png NTK-by-parts %}  \n\n# 4.LLMYaRN  \n\nPI/NTK/NTK-by-partstokensoftmax  \n\nRoPEtoken  \n\nsoftmax t>1RoPEt  \n\nYaRNNTK-by-partsattention score  \n\n{% asset_img yarn.png YaRN %}  \n\n# 5.Group-Query Attentionhidden size=DQhdD=dhkvnsbatch size=bLkv cache  \n\nkv cacheKV  \n\nGQAnKVQD/hKVsD/h2LnsD/hbatch2bLnsD/h\n4bLnsD/h  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  ","source":"_posts/cs/nlp/2024/05/-5.md","raw":"---\ntitle: (5)\nabbrlink: 336f2f3e\ndate: 2024-05-04 15:47:14\ntags:\n  - NLP\n  - LLM\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n![](/images/cover.png)  \n\n\n\n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~  \n\n~\n\n***  \n\n# 1.bf16fp16  \n\n16bit  \n\nfp161510fp16bf16bf16  \n\nbf16187fp16bf16fp16  \n\nbf16fp16  \n\n{% asset_img bfloat16.jpeg bf16 %}  \n\n\n# 2.NTK-aware interpolation  \n\n1.NTK  \n\n2.NTKRoPEbase  \n\n# 3.LLMNTK-by-parts  \n\nNTK-by-partsNTKNTK-awareRoPENTK-by-parts<=1/32  \n\nNTK-by-parts  \n\n{% asset_img ntk_by_parts.png NTK-by-parts %}  \n\n# 4.LLMYaRN  \n\nPI/NTK/NTK-by-partstokensoftmax  \n\nRoPEtoken  \n\nsoftmax t>1RoPEt  \n\nYaRNNTK-by-partsattention score  \n\n{% asset_img yarn.png YaRN %}  \n\n# 5.Group-Query Attentionhidden size=DQhdD=dhkvnsbatch size=bLkv cache  \n\nkv cacheKV  \n\nGQAnKVQD/hKVsD/h2LnsD/hbatch2bLnsD/h\n4bLnsD/h  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  ","slug":"cs/nlp/2024/05/-5","published":1,"updated":"2024-05-10T06:50:19.009Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmw001g0p4k0k4vhutb","content":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM~</p>\n<p>~</p>\n<hr>\n<h1 id=\"bf16fp16\">1.bf16fp16</h1>\n<p>16bit</p>\n<p>fp161510fp16bf16bf16</p>\n<p>bf16187fp16bf16fp16</p>\n<p>bf16fp16</p>\n<img src=\"/336f2f3e/bfloat16.jpeg\" class title=\"bf16\">\n<h1 id=\"ntk-aware-interpolation\">2.NTK-aware\ninterpolation</h1>\n<p>1.NTK</p>\n<p>2.NTKRoPEbase</p>\n<h1 id=\"llmntk-by-parts\">3.LLMNTK-by-parts</h1>\n<p>NTK-by-partsNTKNTK-awareRoPENTK-by-parts&lt;=1/32</p>\n<p>NTK-by-parts</p>\n<img src=\"/336f2f3e/ntk_by_parts.png\" class title=\"NTK-by-parts\">\n<h1 id=\"llmyarn\">4.LLMYaRN</h1>\n<p>PI/NTK/NTK-by-partstokensoftmax</p>\n<p>RoPEtoken</p>\n<p>softmax\nt&gt;1RoPEt</p>\n<p>YaRNNTK-by-partsattention score</p>\n<img src=\"/336f2f3e/yarn.png\" class title=\"YaRN\">\n<h1 id=\"group-query-attentionhidden-sizedqhdddhkvnsbatch-sizeblkv-cache\">5.Group-Query\nAttentionhidden\nsize=DQhdD=dhkvnsbatch\nsize=bLkv cache</h1>\n<p>kv cacheKV</p>\n<p>GQAnKVQD/hKVsD/h2LnsD/hbatch2bLnsD/h\n4bLnsD/h</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n","length":1925,"excerpt":"","more":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM~</p>\n<p>~</p>\n<hr>\n<h1 id=\"bf16fp16\">1.bf16fp16</h1>\n<p>16bit</p>\n<p>fp161510fp16bf16bf16</p>\n<p>bf16187fp16bf16fp16</p>\n<p>bf16fp16</p>\n<img src=\"/336f2f3e/bfloat16.jpeg\" class title=\"bf16\">\n<h1 id=\"ntk-aware-interpolation\">2.NTK-aware\ninterpolation</h1>\n<p>1.NTK</p>\n<p>2.NTKRoPEbase</p>\n<h1 id=\"llmntk-by-parts\">3.LLMNTK-by-parts</h1>\n<p>NTK-by-partsNTKNTK-awareRoPENTK-by-parts&lt;=1/32</p>\n<p>NTK-by-parts</p>\n<img src=\"/336f2f3e/ntk_by_parts.png\" class title=\"NTK-by-parts\">\n<h1 id=\"llmyarn\">4.LLMYaRN</h1>\n<p>PI/NTK/NTK-by-partstokensoftmax</p>\n<p>RoPEtoken</p>\n<p>softmax\nt&gt;1RoPEt</p>\n<p>YaRNNTK-by-partsattention score</p>\n<img src=\"/336f2f3e/yarn.png\" class title=\"YaRN\">\n<h1 id=\"group-query-attentionhidden-sizedqhdddhkvnsbatch-sizeblkv-cache\">5.Group-Query\nAttentionhidden\nsize=DQhdD=dhkvnsbatch\nsize=bLkv cache</h1>\n<p>kv cacheKV</p>\n<p>GQAnKVQD/hKVsD/h2LnsD/hbatch2bLnsD/h\n4bLnsD/h</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n"},{"title":"-","abbrlink":"45ee1a6d","date":"2024-05-06T08:22:38.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n2024Q2RAGAgent  \n\n()128k+  \n\n  \n\n# StreamingLLM  \n\nEfficient Streaming Language Models with Attention Sinks  \n\n20239  \n\n4M  \n\n  \n\nMITCMUNVIDIAMETAStreamingLLM -- LLAMA2, MPT, FalconPythia4MStreamingLLMPPL  \n\nStreamingLLMPPL  \n\n##   \n\n  \n- KVKVGPU H100A10040G/80GKV  \n- RoPERoPE  \n\n  \n- Longformerwindow attentionKVwindow attentiontoken  \n- sliding window with recomputationhttps://github.com/mit-han-lab/streaming-llm/issues/51KVrecomputation  \n- NTKYaRNNSFW()https://kaiokendev.github.io/til#extending-context-to-8k  \n- Efficiently scaling transformer inferenceSmoothQuant: Accurate and efficient post-training quantization for large language modelsDynamic context pruning for efficient and interpretable autoregressive transformersSpatten: Efficient sparse attention architecture with cascade token and head pruningH2o: Heavyhitter oracle for efficient generative inference of large language models  \n- FlashAttention  \n- Big BirdLinformer  \n\nStreamingLLMattention sinkwindow attentionattention sinkPPLsliding window with recomputationStreamingLLM22+  \n\nStreamingLLMPPLStreamingLLMPPL  \n\n{% asset_img streamingllm_model_ppl.png PPL %}  \n\n## attention sink  \n\nwindow attentionLLMtoken  \n\n{% asset_img stremingllm_attention_sink.png attention sink %}  \n\nattention scoretokentoken  \n\ntokenattention sinktoken  \n\nsoftmaxsoftmaxtoken1tokentokentoken1  \n\ntokentokentokentokenattention sink  \n\n4token\\ntokenattention sinktoken  \n\nquantSmoothQuant: Accurate and efficient post-training quantization for large language modelsQuantizable transformers: Removing outliers by helping attention heads do nothing  \n\n##   \n\nStreamingLLMStreamingLLMattention sink  \n\nStreamingLLMwindow attentionattention sink tokenKVtokenKVtokenKV cachetokenStreamingLLMattention  \n\n{% asset_img streamingllm_compare.png StreamingLLM %}  \n\ntoken  \n\n{% asset_img stremingllm_init_token_num.png attention sink number %}  \n\n4tokenPPLtoken  \n\nwindow attentionStreamingLLMKV cache  \n- 4tokenattention sink  \n- KV cache  \n\nStreamingLLMKV  \n\n{% asset_img stremingllm_kv_cache.png cache %}  \n\nStreamingLLMdistancetokencachedistanceRoPEtokenKVtokenLM-Infinite  \n\nattention sinktokenattention sink tokentoken  \n\ntokensoftmaxsoftmax-off-by-oneattentionsoftmax  \n\nsoftmax-off-by-one  \n\n$$\\text{SoftMax}_1(x)_i=\\frac{e^{x_i}}{1+\\sum_{j=1}^Ne^{x_j}}$$  \n\nsoftmax-off-by-one1attention score1KV0tokentoken1tokenattention sink  \n\ntokenattention sinksoftmax-off-by-one3160M  \n\n{% asset_img stremingllm_exp.png  %}  \n\nsoftmax-off-by-onezero sinktokenattention sink  \n\nLLAMA2, MPT, FalconPythiaStreamingLLM4MMPTALIBIRoPE\n\n{% asset_img stremingllm_perf_4m.png  %}  \n\n4MStreamingLLMPPL  \n\nStreamingLLMwindow attentioncacheStreamingLLMPPLStreamingLLM  \n\n# LM-Infinite  \n\nLM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models  \n\n20238  \n\n2k4k200MPPL  \n\n  \n\nLM-InfiniteLLMPasskey RetrievalQasper2.77.5  \n\n##   \n\nLM-InfiniteTransformer LLM3  \n\n- 1challenges in handling unseen distances among tokens  \n\ntokentokenattention logits  \n\nLLAMA2ArXiv8kattention logits  \n\n{% asset_img lm_infinite_attention_logits_explode.png logits %}  \n\nLLAMA24k4kattention logits  \n\ntokenlogits  \n\n- 2attending to unseen numbers of tokens  \n\ntokenlogits(attention entropy)  \n\ntokentoken  \n\n8k  \n\n{% asset_img lm_infinite_attention_entropy.png  %}  \n\nattention context size  \n\nwindow attentiontokentokenhandle12XPosLongformer  \n\nwindow attention  \n\n- 3starting tokens occupy a distinct feature space  \n\ntoken  \n\nThe impact of positional encoding on length generalization in transformers1tokentokentoken  \n\nStreamingLLMattention sinktokenwindow attentiontoken  \n\nLLAMA2hidden state outputPCA2  \n\n{% asset_img lm_infinite_starting_tokens.png token %}  \n\ntokentokentokentokentokentoken  \n\ntoken  \n\n##   \n\nLM-InfiniteLLMzero-shotLM-Infinite-shaped attention maskDistance ceiling  \n\n- -shaped attention mask  \n\n-shaped attention maskLongformerLongNetBig Bird  \n\n-shaped attention maskwindow attentiontokentokentoken1$n_{\\mathrm{starting}}$ token2$L_{\\mathrm{pretrain}}$ token$n_{\\mathrm{starting}}$ $L_{\\mathrm{pretrain}}$ tokentoken  \n\n $n_{\\mathrm{starting}}$  $n_{\\mathrm{starting}}\\in[5,100]$   \n\n{% asset_img lm_infinite_starting_tokens_num.png token %}  \n\n  \n\n-shaped attention mask23  \n\n- Distance ceiling  \n\nLM-Infinite $L_{\\mathrm{pretrain}}$token  \n\nattention logit $w(\\mathbf{q},\\mathbf{k},d)$ $d$ tokenDistance ceilingattention logit  \n\n$$\\text{attention logits}=w(\\mathbf{q},\\mathbf{k},d')$$  \n\n$$d'=\\min(d,L_\\text{pretrain})$$  \n\nDistance ceiling1  \n\n- Optionally attending to top-k tokens in the middle  \n\n-shaped attention maskDistance ceilingtokentokentokenattentiontoken  \n\ntoken $k$ attention logitstokenattention  $k$ token $d=\\frac12L_\\text{pre-train}$  \n\n $k$ Passkey Retrievalvalidation set  \n\n{% asset_img lm_infinite_middle_k.png k %}  \n\n $k=5$>5  \n\nmiddle tokenmiddle token2  \n\nLM-Infinite  \n\n{% asset_img lm_infinite_design.png  %}  \n\n##   \n\nLLaMA-7BLLaMA2-7BMPT-7BGPT-J-6BLM-InfiniteMPT-7BAlibiRoPE  \n\n- Language Modeling  \n\nArXivOpenWebText2  \n\nLM-Infinite0-12kPPL  \n\n{% asset_img lm_infinite_ppl_figure.png PPL %}  \n\nLLAMA210KNaN32KOOM  \n\nPPLLM-InfinitePPL\n\nLM-InfiniteLM-Infinite + Llama2ArXiv200M tokenPPL200MPPL  \n\n{% asset_img lm_infinite_ppl_200m.png PPL 200M %}  \n\n- Passkey Retrieval and Qapser  \n\nPasskey RetrievalQapsertop-5middle tokenattention  \n\nPasskey Retrieval0LM-Infinite  \n\n{% asset_img lm_infinite_downstream.png  %}  \n\n- Ablation study  \n\nLM-Infinite-shaped attention maskDistance ceiling  \n\n{% asset_img lm_infinite_ablation.png  %}  \n\n-shaped attention maskdistance ceilingPPL  \n\n# Transformer-XL  \n\nInfini-TransformerTransformer-XL  \n\nTransformer-XL20196CMUGoogle Brain  \n\ntransformersegmentsegment  \n\n{% asset_img xl_vanilla_sw.png vanilla transformer %}  \n\n  \n\nTransformer-XLattentionsegmentLsegment $[\\mathbf{s}_{\\tau}=x_{\\tau,1},\\cdots,x_{\\tau,L}]$  $\\mathbf{s}_{\\tau+1}=[x_{\\tau+1,1},\\cdots,x_{\\tau+1,L}]$  \n\n$$\\begin{aligned}&\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}=\\left[\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\right]\\\\&\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n=\\mathbf{h}_{\\tau+1}^{n-1}\\mathbf{W}_q^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_k^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_v^\\top\\\\&\\mathbf{h}_{\\tau+1}^n=\\text{Transformer-Layer}\\left(\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n\\right)\\end{aligned}$$  \n\nSGstop gradient$\\begin{bmatrix}\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\end{bmatrix}$ hqkv $n$  $n-1$   \n\nsegmentattentionTransformer-XLsegmentKVsegmentsegmentsegment  \n\n{% asset_img xl_attention.png Transformer-XL %}  \n\n# Infini-Transformer  \n\nLeave No Context Behind:Efficient Infinite Context Transformers with Infini-attention  \n\n20244  \n\n500k/1M  \n\n/  \n\nGoogleInfini-attention  \n\n## \n\n  \n\ntransformer  \n\nStreamingLLM  \n\nMetalearned neural memory/Enhancing the transformer with explicit relational encoding for math problem solvingcompressive memorycontextsimplicitytradeoff  \n\n## Infini-attention  \n\nInfini-TransformerInfini-attentionTransformer blockmask local attentionlong term linear attention  \n\n{% asset_img infini_attention_structure.png infini-attention %}  \n\nInfini-attentionQKVQInfini-attentionconcat  \n\nTransformerLLM  \n\nInfini-attentionTransformer-XL  \n\n{% asset_img infini_attention_process.png infini-attention %}  \n\nTransformer-XLInfini-TransformersegmentTransformer-XLsegment  \n\nInfini-Transformersegment  \n\ncompressive memorysimplicitycomputational efficiencyLearning associative inference using fast weight memorymemoryassociative matrix  \n\n  \n\n-   \n\nsegment $N$ $Q\\in\\mathbf{R}^{N\\times d_{key}}$ memory $M_{s-1}\\in\\mathbf{R}^{d_{key}\\times d_{value}}$   \n\n$$A_{\\text{mm}}=\\frac{\\sigma(Q)M_{s-1}}{\\sigma(Q)z_{s-1}}$$  \n\n$\\sigma$element-wise ELU + 1  \n\n$z_{s-1}\\in\\mathbf{R}^{d_{key}}$normalization termnormalization termTransformers are rnns: Fast autoregressive transformers with linear attentionK  \n\n-   \n\nnormalization termmemory  \n\n$$M_s\\leftarrow M_{s-1}+\\sigma(K)^TV$$  \n\n$$z_s\\leftarrow z_{s-1}+\\sum_{t=1}^N\\sigma(K_t)$$  \n\nmemoryMetalearned neural memoryLearning associative inference using fast weight memorydelta rule  \n\n$$M_s\\leftarrow M_{s-1}+\\sigma(K)^T(V-\\frac{\\sigma(K)M_{s-1}}{\\sigma(K)z_{s-1}})$$  \n\n- local attention  \n\nsegment $A_{mem}$ local attention state $A_{dot}$   \n\n$$A=sigmoid(\\beta)\\odot A_{mem}+(1-sigmoid(\\beta))\\odot A_{dot}$$  \n\n $\\beta$   \n\n $\\beta$ 0segment101  \n\n{% asset_img infini_attention_gating.png gating %}  \n\nsegment-level memory  \n\n{% asset_img infini_attention_compare.png  %}  \n\n## \n\nsegment $N$ 20483276816segment  \n\n- \n\nInfini-TransformerTransformer-XL/Memorzing Transformer/RMTPG-19Arxivlanguage modeling  \n\n{% asset_img infini_attention_language_modeling.png  %}  \n\nInfini-TransformerMemorizing Transformers1%  \n\n100kInfini-TransformerPPL  \n\n- \n\n1B  \n- batch size = 64\n- step = 30k\n-  > 4k\n- segment length = 2k\n\n1Mpasskey retrievalzero-shotfine-tune  \n\n{% asset_img infini_attention_passkey.png passkey %}  \n\nInfini-Transformerlinear + delta1Mpasskey retrieval  \n\n8B8k30k500kBookSum  \n\n{% asset_img infini_attention_booksum.png booksum %}  \n\nInfini-Transformerlinear + delta  \n\n#   \n\n1. StreamingLM-Infiniteattention sinktokenPPLLM-Infinitedistance ceilingtokenMPPLtoken  \n2. Infini-Transformer  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n# Reference  \n1Efficient Streaming Language Models with Attention Sinks https://arxiv.org/abs/2309.17453  \n2Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://arxiv.org/abs/1901.02860  \n3Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention https://arxiv.org/abs/2404.07143  \n4LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models https://arxiv.org/abs/2308.16137  ","source":"_posts/cs/nlp/2024/05/-.md","raw":"---\ntitle: -\nabbrlink: 45ee1a6d\ndate: 2024-05-06 16:22:38\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \n  - \n  - \n  - attention\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n2024Q2RAGAgent  \n\n()128k+  \n\n  \n\n# StreamingLLM  \n\nEfficient Streaming Language Models with Attention Sinks  \n\n20239  \n\n4M  \n\n  \n\nMITCMUNVIDIAMETAStreamingLLM -- LLAMA2, MPT, FalconPythia4MStreamingLLMPPL  \n\nStreamingLLMPPL  \n\n##   \n\n  \n- KVKVGPU H100A10040G/80GKV  \n- RoPERoPE  \n\n  \n- Longformerwindow attentionKVwindow attentiontoken  \n- sliding window with recomputationhttps://github.com/mit-han-lab/streaming-llm/issues/51KVrecomputation  \n- NTKYaRNNSFW()https://kaiokendev.github.io/til#extending-context-to-8k  \n- Efficiently scaling transformer inferenceSmoothQuant: Accurate and efficient post-training quantization for large language modelsDynamic context pruning for efficient and interpretable autoregressive transformersSpatten: Efficient sparse attention architecture with cascade token and head pruningH2o: Heavyhitter oracle for efficient generative inference of large language models  \n- FlashAttention  \n- Big BirdLinformer  \n\nStreamingLLMattention sinkwindow attentionattention sinkPPLsliding window with recomputationStreamingLLM22+  \n\nStreamingLLMPPLStreamingLLMPPL  \n\n{% asset_img streamingllm_model_ppl.png PPL %}  \n\n## attention sink  \n\nwindow attentionLLMtoken  \n\n{% asset_img stremingllm_attention_sink.png attention sink %}  \n\nattention scoretokentoken  \n\ntokenattention sinktoken  \n\nsoftmaxsoftmaxtoken1tokentokentoken1  \n\ntokentokentokentokenattention sink  \n\n4token\\ntokenattention sinktoken  \n\nquantSmoothQuant: Accurate and efficient post-training quantization for large language modelsQuantizable transformers: Removing outliers by helping attention heads do nothing  \n\n##   \n\nStreamingLLMStreamingLLMattention sink  \n\nStreamingLLMwindow attentionattention sink tokenKVtokenKVtokenKV cachetokenStreamingLLMattention  \n\n{% asset_img streamingllm_compare.png StreamingLLM %}  \n\ntoken  \n\n{% asset_img stremingllm_init_token_num.png attention sink number %}  \n\n4tokenPPLtoken  \n\nwindow attentionStreamingLLMKV cache  \n- 4tokenattention sink  \n- KV cache  \n\nStreamingLLMKV  \n\n{% asset_img stremingllm_kv_cache.png cache %}  \n\nStreamingLLMdistancetokencachedistanceRoPEtokenKVtokenLM-Infinite  \n\nattention sinktokenattention sink tokentoken  \n\ntokensoftmaxsoftmax-off-by-oneattentionsoftmax  \n\nsoftmax-off-by-one  \n\n$$\\text{SoftMax}_1(x)_i=\\frac{e^{x_i}}{1+\\sum_{j=1}^Ne^{x_j}}$$  \n\nsoftmax-off-by-one1attention score1KV0tokentoken1tokenattention sink  \n\ntokenattention sinksoftmax-off-by-one3160M  \n\n{% asset_img stremingllm_exp.png  %}  \n\nsoftmax-off-by-onezero sinktokenattention sink  \n\nLLAMA2, MPT, FalconPythiaStreamingLLM4MMPTALIBIRoPE\n\n{% asset_img stremingllm_perf_4m.png  %}  \n\n4MStreamingLLMPPL  \n\nStreamingLLMwindow attentioncacheStreamingLLMPPLStreamingLLM  \n\n# LM-Infinite  \n\nLM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models  \n\n20238  \n\n2k4k200MPPL  \n\n  \n\nLM-InfiniteLLMPasskey RetrievalQasper2.77.5  \n\n##   \n\nLM-InfiniteTransformer LLM3  \n\n- 1challenges in handling unseen distances among tokens  \n\ntokentokenattention logits  \n\nLLAMA2ArXiv8kattention logits  \n\n{% asset_img lm_infinite_attention_logits_explode.png logits %}  \n\nLLAMA24k4kattention logits  \n\ntokenlogits  \n\n- 2attending to unseen numbers of tokens  \n\ntokenlogits(attention entropy)  \n\ntokentoken  \n\n8k  \n\n{% asset_img lm_infinite_attention_entropy.png  %}  \n\nattention context size  \n\nwindow attentiontokentokenhandle12XPosLongformer  \n\nwindow attention  \n\n- 3starting tokens occupy a distinct feature space  \n\ntoken  \n\nThe impact of positional encoding on length generalization in transformers1tokentokentoken  \n\nStreamingLLMattention sinktokenwindow attentiontoken  \n\nLLAMA2hidden state outputPCA2  \n\n{% asset_img lm_infinite_starting_tokens.png token %}  \n\ntokentokentokentokentokentoken  \n\ntoken  \n\n##   \n\nLM-InfiniteLLMzero-shotLM-Infinite-shaped attention maskDistance ceiling  \n\n- -shaped attention mask  \n\n-shaped attention maskLongformerLongNetBig Bird  \n\n-shaped attention maskwindow attentiontokentokentoken1$n_{\\mathrm{starting}}$ token2$L_{\\mathrm{pretrain}}$ token$n_{\\mathrm{starting}}$ $L_{\\mathrm{pretrain}}$ tokentoken  \n\n $n_{\\mathrm{starting}}$  $n_{\\mathrm{starting}}\\in[5,100]$   \n\n{% asset_img lm_infinite_starting_tokens_num.png token %}  \n\n  \n\n-shaped attention mask23  \n\n- Distance ceiling  \n\nLM-Infinite $L_{\\mathrm{pretrain}}$token  \n\nattention logit $w(\\mathbf{q},\\mathbf{k},d)$ $d$ tokenDistance ceilingattention logit  \n\n$$\\text{attention logits}=w(\\mathbf{q},\\mathbf{k},d')$$  \n\n$$d'=\\min(d,L_\\text{pretrain})$$  \n\nDistance ceiling1  \n\n- Optionally attending to top-k tokens in the middle  \n\n-shaped attention maskDistance ceilingtokentokentokenattentiontoken  \n\ntoken $k$ attention logitstokenattention  $k$ token $d=\\frac12L_\\text{pre-train}$  \n\n $k$ Passkey Retrievalvalidation set  \n\n{% asset_img lm_infinite_middle_k.png k %}  \n\n $k=5$>5  \n\nmiddle tokenmiddle token2  \n\nLM-Infinite  \n\n{% asset_img lm_infinite_design.png  %}  \n\n##   \n\nLLaMA-7BLLaMA2-7BMPT-7BGPT-J-6BLM-InfiniteMPT-7BAlibiRoPE  \n\n- Language Modeling  \n\nArXivOpenWebText2  \n\nLM-Infinite0-12kPPL  \n\n{% asset_img lm_infinite_ppl_figure.png PPL %}  \n\nLLAMA210KNaN32KOOM  \n\nPPLLM-InfinitePPL\n\nLM-InfiniteLM-Infinite + Llama2ArXiv200M tokenPPL200MPPL  \n\n{% asset_img lm_infinite_ppl_200m.png PPL 200M %}  \n\n- Passkey Retrieval and Qapser  \n\nPasskey RetrievalQapsertop-5middle tokenattention  \n\nPasskey Retrieval0LM-Infinite  \n\n{% asset_img lm_infinite_downstream.png  %}  \n\n- Ablation study  \n\nLM-Infinite-shaped attention maskDistance ceiling  \n\n{% asset_img lm_infinite_ablation.png  %}  \n\n-shaped attention maskdistance ceilingPPL  \n\n# Transformer-XL  \n\nInfini-TransformerTransformer-XL  \n\nTransformer-XL20196CMUGoogle Brain  \n\ntransformersegmentsegment  \n\n{% asset_img xl_vanilla_sw.png vanilla transformer %}  \n\n  \n\nTransformer-XLattentionsegmentLsegment $[\\mathbf{s}_{\\tau}=x_{\\tau,1},\\cdots,x_{\\tau,L}]$  $\\mathbf{s}_{\\tau+1}=[x_{\\tau+1,1},\\cdots,x_{\\tau+1,L}]$  \n\n$$\\begin{aligned}&\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}=\\left[\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\right]\\\\&\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n=\\mathbf{h}_{\\tau+1}^{n-1}\\mathbf{W}_q^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_k^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_v^\\top\\\\&\\mathbf{h}_{\\tau+1}^n=\\text{Transformer-Layer}\\left(\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n\\right)\\end{aligned}$$  \n\nSGstop gradient$\\begin{bmatrix}\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\end{bmatrix}$ hqkv $n$  $n-1$   \n\nsegmentattentionTransformer-XLsegmentKVsegmentsegmentsegment  \n\n{% asset_img xl_attention.png Transformer-XL %}  \n\n# Infini-Transformer  \n\nLeave No Context Behind:Efficient Infinite Context Transformers with Infini-attention  \n\n20244  \n\n500k/1M  \n\n/  \n\nGoogleInfini-attention  \n\n## \n\n  \n\ntransformer  \n\nStreamingLLM  \n\nMetalearned neural memory/Enhancing the transformer with explicit relational encoding for math problem solvingcompressive memorycontextsimplicitytradeoff  \n\n## Infini-attention  \n\nInfini-TransformerInfini-attentionTransformer blockmask local attentionlong term linear attention  \n\n{% asset_img infini_attention_structure.png infini-attention %}  \n\nInfini-attentionQKVQInfini-attentionconcat  \n\nTransformerLLM  \n\nInfini-attentionTransformer-XL  \n\n{% asset_img infini_attention_process.png infini-attention %}  \n\nTransformer-XLInfini-TransformersegmentTransformer-XLsegment  \n\nInfini-Transformersegment  \n\ncompressive memorysimplicitycomputational efficiencyLearning associative inference using fast weight memorymemoryassociative matrix  \n\n  \n\n-   \n\nsegment $N$ $Q\\in\\mathbf{R}^{N\\times d_{key}}$ memory $M_{s-1}\\in\\mathbf{R}^{d_{key}\\times d_{value}}$   \n\n$$A_{\\text{mm}}=\\frac{\\sigma(Q)M_{s-1}}{\\sigma(Q)z_{s-1}}$$  \n\n$\\sigma$element-wise ELU + 1  \n\n$z_{s-1}\\in\\mathbf{R}^{d_{key}}$normalization termnormalization termTransformers are rnns: Fast autoregressive transformers with linear attentionK  \n\n-   \n\nnormalization termmemory  \n\n$$M_s\\leftarrow M_{s-1}+\\sigma(K)^TV$$  \n\n$$z_s\\leftarrow z_{s-1}+\\sum_{t=1}^N\\sigma(K_t)$$  \n\nmemoryMetalearned neural memoryLearning associative inference using fast weight memorydelta rule  \n\n$$M_s\\leftarrow M_{s-1}+\\sigma(K)^T(V-\\frac{\\sigma(K)M_{s-1}}{\\sigma(K)z_{s-1}})$$  \n\n- local attention  \n\nsegment $A_{mem}$ local attention state $A_{dot}$   \n\n$$A=sigmoid(\\beta)\\odot A_{mem}+(1-sigmoid(\\beta))\\odot A_{dot}$$  \n\n $\\beta$   \n\n $\\beta$ 0segment101  \n\n{% asset_img infini_attention_gating.png gating %}  \n\nsegment-level memory  \n\n{% asset_img infini_attention_compare.png  %}  \n\n## \n\nsegment $N$ 20483276816segment  \n\n- \n\nInfini-TransformerTransformer-XL/Memorzing Transformer/RMTPG-19Arxivlanguage modeling  \n\n{% asset_img infini_attention_language_modeling.png  %}  \n\nInfini-TransformerMemorizing Transformers1%  \n\n100kInfini-TransformerPPL  \n\n- \n\n1B  \n- batch size = 64\n- step = 30k\n-  > 4k\n- segment length = 2k\n\n1Mpasskey retrievalzero-shotfine-tune  \n\n{% asset_img infini_attention_passkey.png passkey %}  \n\nInfini-Transformerlinear + delta1Mpasskey retrieval  \n\n8B8k30k500kBookSum  \n\n{% asset_img infini_attention_booksum.png booksum %}  \n\nInfini-Transformerlinear + delta  \n\n#   \n\n1. StreamingLM-Infiniteattention sinktokenPPLLM-Infinitedistance ceilingtokenMPPLtoken  \n2. Infini-Transformer  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n# Reference  \n1Efficient Streaming Language Models with Attention Sinks https://arxiv.org/abs/2309.17453  \n2Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context https://arxiv.org/abs/1901.02860  \n3Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention https://arxiv.org/abs/2404.07143  \n4LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models https://arxiv.org/abs/2308.16137  ","slug":"cs/nlp/2024/05/-","published":1,"updated":"2024-05-13T09:06:18.036Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmw001i0p4k2olbb79a","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>2024Q2RAGAgent</p>\n<p>()128k+</p>\n<p></p>\n<h1 id=\"streamingllm\">StreamingLLM</h1>\n<p>Efficient Streaming Language Models with Attention Sinks</p>\n<p>20239</p>\n<p>4M</p>\n<p></p>\n<p>MITCMUNVIDIAMETAStreamingLLM\n-- LLAMA2, MPT,\nFalconPythia4MStreamingLLMPPL</p>\n<p>StreamingLLMPPL</p>\n<h2 id=\"\"></h2>\n<p><br>\n-\nKVKVGPU\nH100A10040G/80GKV<br>\n-\nRoPERoPE</p>\n<p><br>\n- Longformerwindow\nattentionKVwindow\nattentiontoken<br>\n- sliding window with\nrecomputationhttps://github.com/mit-han-lab/streaming-llm/issues/51KVrecomputation<br>\n-\nNTKYaRNNSFW()https://kaiokendev.github.io/til#extending-context-to-8k<br>\n- Efficiently scaling transformer\ninferenceSmoothQuant: Accurate and efficient post-training\nquantization for large language modelsDynamic context pruning for\nefficient and interpretable autoregressive transformersSpatten:\nEfficient sparse attention architecture with cascade token and head\npruningH2o: Heavyhitter oracle for efficient generative inference\nof large language models<br>\n-\nFlashAttention<br>\n- Big BirdLinformer</p>\n<p>StreamingLLMattention\nsinkwindow\nattentionattention\nsinkPPLsliding\nwindow with\nrecomputationStreamingLLM22+</p>\n<p>StreamingLLMPPLStreamingLLMPPL</p>\n<img src=\"/45ee1a6d/streamingllm_model_ppl.png\" class title=\"PPL\">\n<h2 id=\"attention-sink\">attention sink</h2>\n<p>window\nattentionLLMtoken</p>\n<img src=\"/45ee1a6d/stremingllm_attention_sink.png\" class title=\"attention sink\">\n<p>attention\nscoretokentoken</p>\n<p>tokenattention\nsinktoken</p>\n<p>softmaxsoftmaxtoken1tokentokentoken1</p>\n<p>tokentokentokentokenattention\nsink</p>\n<p>4tokentokenattention\nsinktoken</p>\n<p>quantSmoothQuant: Accurate and\nefficient post-training quantization for large language\nmodelsQuantizable transformers: Removing outliers by helping\nattention heads do nothing</p>\n<h2 id=\"\"></h2>\n<p>StreamingLLMStreamingLLMattention\nsink</p>\n<p>StreamingLLMwindow\nattentionattention sink\ntokenKVtokenKVtokenKV\ncachetokenStreamingLLMattention</p>\n<img src=\"/45ee1a6d/streamingllm_compare.png\" class title=\"StreamingLLM\">\n<p>token</p>\n<img src=\"/45ee1a6d/stremingllm_init_token_num.png\" class title=\"attention sink number\">\n<p>4tokenPPLtoken</p>\n<p>window attentionStreamingLLMKV\ncache<br>\n- 4tokenattention sink<br>\n- KV cache</p>\n<p>StreamingLLMKV</p>\n<img src=\"/45ee1a6d/stremingllm_kv_cache.png\" class title=\"cache\">\n<p>StreamingLLMdistancetokencachedistanceRoPEtokenKVtokenLM-Infinite</p>\n<p>attention\nsinktokenattention\nsink tokentoken</p>\n<p>tokensoftmaxsoftmax-off-by-oneattentionsoftmax</p>\n<p>softmax-off-by-one</p>\n<p><span class=\"math display\">\\[\\text{SoftMax}_1(x)_i=\\frac{e^{x_i}}{1+\\sum_{j=1}^Ne^{x_j}}\\]</span></p>\n<p>softmax-off-by-one1attention\nscore1KV0tokentoken1tokenattention\nsink</p>\n<p>tokenattention\nsinksoftmax-off-by-one3160M</p>\n<img src=\"/45ee1a6d/stremingllm_exp.png\" class title=\"\">\n<p>softmax-off-by-onezero\nsinktokenattention sink</p>\n<p>LLAMA2, MPT,\nFalconPythiaStreamingLLM4MMPTALIBIRoPE</p>\n<img src=\"/45ee1a6d/stremingllm_perf_4m.png\" class title=\"\">\n<p>4MStreamingLLMPPL</p>\n<p>StreamingLLMwindow\nattentioncacheStreamingLLMPPLStreamingLLM</p>\n<h1 id=\"lm-infinite\">LM-Infinite</h1>\n<p>LM-Infinite: Zero-Shot Extreme Length Generalization for Large\nLanguage Models</p>\n<p>20238</p>\n<p>2k4k200MPPL</p>\n<p></p>\n<p>LM-InfiniteLLMPasskey\nRetrievalQasper2.77.5</p>\n<h2 id=\"\"></h2>\n<p>LM-InfiniteTransformer\nLLM3</p>\n<ul>\n<li>1challenges in handling unseen distances among tokens</li>\n</ul>\n<p>tokentokenattention\nlogits</p>\n<p>LLAMA2ArXiv8kattention\nlogits</p>\n<img src=\"/45ee1a6d/lm_infinite_attention_logits_explode.png\" class title=\"logits\">\n<p>LLAMA24k4kattention\nlogits</p>\n<p>tokenlogits</p>\n<ul>\n<li>2attending to unseen numbers of tokens</li>\n</ul>\n<p>tokenlogits(attention\nentropy)</p>\n<p>tokentoken</p>\n<p>8k</p>\n<img src=\"/45ee1a6d/lm_infinite_attention_entropy.png\" class title=\"\">\n<p>attention context\nsize</p>\n<p>window\nattentiontokentokenhandle12XPosLongformer</p>\n<p>window attention</p>\n<ul>\n<li>3starting tokens occupy a distinct feature space</li>\n</ul>\n<p>token</p>\n<p>The impact of positional encoding on length\ngeneralization in\ntransformers1tokentokentoken</p>\n<p>StreamingLLMattention\nsinktokenwindow\nattentiontoken</p>\n<p>LLAMA2hidden state\noutputPCA2</p>\n<img src=\"/45ee1a6d/lm_infinite_starting_tokens.png\" class title=\"token\">\n<p>tokentokentokentokentokentoken</p>\n<p>token</p>\n<h2 id=\"\"></h2>\n<p>LM-InfiniteLLMzero-shotLM-Infinite-shaped\nattention maskDistance ceiling</p>\n<ul>\n<li>-shaped attention mask</li>\n</ul>\n<p>-shaped attention maskLongformerLongNetBig\nBird</p>\n<p>-shaped attention maskwindow\nattentiontokentokentoken1<span class=\"math inline\">\\(n_{\\mathrm{starting}}\\)</span>\ntoken2<span class=\"math inline\">\\(L_{\\mathrm{pretrain}}\\)</span>\ntoken<span class=\"math inline\">\\(n_{\\mathrm{starting}}\\)</span> <span class=\"math inline\">\\(L_{\\mathrm{pretrain}}\\)</span>\ntokentoken</p>\n<p> <span class=\"math inline\">\\(n_{\\mathrm{starting}}\\)</span>\n <span class=\"math inline\">\\(n_{\\mathrm{starting}}\\in[5,100]\\)</span>\n</p>\n<img src=\"/45ee1a6d/lm_infinite_starting_tokens_num.png\" class title=\"token\">\n<p></p>\n<p>-shaped attention mask23</p>\n<ul>\n<li>Distance ceiling</li>\n</ul>\n<p>LM-Infinite <span class=\"math inline\">\\(L_{\\mathrm{pretrain}}\\)</span>token</p>\n<p>attention logit <span class=\"math inline\">\\(w(\\mathbf{q},\\mathbf{k},d)\\)</span> <span class=\"math inline\">\\(d\\)</span> tokenDistance\nceilingattention logit</p>\n<p><span class=\"math display\">\\[\\text{attention\nlogits}=w(\\mathbf{q},\\mathbf{k},d&#39;)\\]</span></p>\n<p><span class=\"math display\">\\[d&#39;=\\min(d,L_\\text{pretrain})\\]</span></p>\n<p>Distance ceiling1</p>\n<ul>\n<li>Optionally attending to top-k tokens in the middle</li>\n</ul>\n<p>-shaped attention maskDistance\nceilingtokentokentokenattentiontoken</p>\n<p>token <span class=\"math inline\">\\(k\\)</span> attention\nlogitstokenattention  <span class=\"math inline\">\\(k\\)</span> token <span class=\"math inline\">\\(d=\\frac12L_\\text{pre-train}\\)</span></p>\n<p> <span class=\"math inline\">\\(k\\)</span> Passkey\nRetrievalvalidation set</p>\n<img src=\"/45ee1a6d/lm_infinite_middle_k.png\" class title=\"k\">\n<p> <span class=\"math inline\">\\(k=5\\)</span>&gt;5</p>\n<p>middle\ntokenmiddle\ntoken2</p>\n<p>LM-Infinite</p>\n<img src=\"/45ee1a6d/lm_infinite_design.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p>LLaMA-7BLLaMA2-7BMPT-7BGPT-J-6BLM-InfiniteMPT-7BAlibiRoPE</p>\n<ul>\n<li>Language Modeling</li>\n</ul>\n<p>ArXivOpenWebText2</p>\n<p>LM-Infinite0-12kPPL</p>\n<img src=\"/45ee1a6d/lm_infinite_ppl_figure.png\" class title=\"PPL\">\n<p>LLAMA210KNaN32KOOM</p>\n<p>PPLLM-InfinitePPL</p>\n<p>LM-InfiniteLM-Infinite +\nLlama2ArXiv200M\ntokenPPL200MPPL</p>\n<img src=\"/45ee1a6d/lm_infinite_ppl_200m.png\" class title=\"PPL 200M\">\n<ul>\n<li>Passkey Retrieval and Qapser</li>\n</ul>\n<p>Passkey\nRetrievalQapsertop-5middle\ntokenattention</p>\n<p>Passkey\nRetrieval0LM-Infinite</p>\n<img src=\"/45ee1a6d/lm_infinite_downstream.png\" class title=\"\">\n<ul>\n<li>Ablation study</li>\n</ul>\n<p>LM-Infinite-shaped attention maskDistance\nceiling</p>\n<img src=\"/45ee1a6d/lm_infinite_ablation.png\" class title=\"\">\n<p>-shaped attention maskdistance\nceilingPPL</p>\n<h1 id=\"transformer-xl\">Transformer-XL</h1>\n<p>Infini-TransformerTransformer-XL</p>\n<p>Transformer-XL20196CMUGoogle\nBrain</p>\n<p>transformersegmentsegment</p>\n<img src=\"/45ee1a6d/xl_vanilla_sw.png\" class title=\"vanilla transformer\">\n<p></p>\n<p>Transformer-XLattentionsegmentLsegment\n<span class=\"math inline\">\\([\\mathbf{s}_{\\tau}=x_{\\tau,1},\\cdots,x_{\\tau,L}]\\)</span>\n <span class=\"math inline\">\\(\\mathbf{s}_{\\tau+1}=[x_{\\tau+1,1},\\cdots,x_{\\tau+1,L}]\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}=\\left[\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\right]\\\\&amp;\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n=\\mathbf{h}_{\\tau+1}^{n-1}\\mathbf{W}_q^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_k^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_v^\\top\\\\&amp;\\mathbf{h}_{\\tau+1}^n=\\text{Transformer-Layer}\\left(\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n\\right)\\end{aligned}\\]</span></p>\n<p>SGstop\ngradient<span class=\"math inline\">\\(\\begin{bmatrix}\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\end{bmatrix}\\)</span>\nhqkv <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(n-1\\)</span> </p>\n<p>segmentattentionTransformer-XLsegmentKVsegmentsegmentsegment</p>\n<img src=\"/45ee1a6d/xl_attention.png\" class title=\"Transformer-XL\">\n<h1 id=\"infini-transformer\">Infini-Transformer</h1>\n<p>Leave No Context Behind:Efficient Infinite Context Transformers\nwith Infini-attention</p>\n<p>20244</p>\n<p>500k/1M</p>\n<p>/</p>\n<p>GoogleInfini-attention</p>\n<h2 id=\"-1\"></h2>\n<p></p>\n<p>transformer</p>\n<p>StreamingLLM</p>\n<p>Metalearned neural\nmemory/Enhancing the transformer with explicit relational encoding\nfor math problem solvingcompressive\nmemorycontextsimplicitytradeoff</p>\n<h2 id=\"infini-attention\">Infini-attention</h2>\n<p>Infini-TransformerInfini-attentionTransformer\nblockmask local attentionlong term\nlinear attention</p>\n<img src=\"/45ee1a6d/infini_attention_structure.png\" class title=\"infini-attention\">\n<p>Infini-attentionQKVQInfini-attentionconcat</p>\n<p>TransformerLLM</p>\n<p>Infini-attentionTransformer-XL</p>\n<img src=\"/45ee1a6d/infini_attention_process.png\" class title=\"infini-attention\">\n<p>Transformer-XLInfini-TransformersegmentTransformer-XLsegment</p>\n<p>Infini-Transformersegment</p>\n<p>compressive memorysimplicitycomputational\nefficiencyLearning associative inference using fast\nweight memorymemoryassociative matrix</p>\n<p></p>\n<ul>\n<li></li>\n</ul>\n<p>segment <span class=\"math inline\">\\(N\\)</span> <span class=\"math inline\">\\(Q\\in\\mathbf{R}^{N\\times d_{key}}\\)</span> memory\n<span class=\"math inline\">\\(M_{s-1}\\in\\mathbf{R}^{d_{key}\\times\nd_{value}}\\)</span> </p>\n<p><span class=\"math display\">\\[A_{\\text{mm}}=\\frac{\\sigma(Q)M_{s-1}}{\\sigma(Q)z_{s-1}}\\]</span></p>\n<p><span class=\"math inline\">\\(\\sigma\\)</span>element-wise\nELU + 1</p>\n<p><span class=\"math inline\">\\(z_{s-1}\\in\\mathbf{R}^{d_{key}}\\)</span>normalization\ntermnormalization termTransformers are rnns: Fast\nautoregressive transformers with linear\nattentionK</p>\n<ul>\n<li></li>\n</ul>\n<p>normalization\ntermmemory</p>\n<p><span class=\"math display\">\\[M_s\\leftarrow\nM_{s-1}+\\sigma(K)^TV\\]</span></p>\n<p><span class=\"math display\">\\[z_s\\leftarrow\nz_{s-1}+\\sum_{t=1}^N\\sigma(K_t)\\]</span></p>\n<p>memoryMetalearned neural memoryLearning\nassociative inference using fast weight memorydelta\nrule</p>\n<p><span class=\"math display\">\\[M_s\\leftarrow\nM_{s-1}+\\sigma(K)^T(V-\\frac{\\sigma(K)M_{s-1}}{\\sigma(K)z_{s-1}})\\]</span></p>\n<ul>\n<li>local attention</li>\n</ul>\n<p>segment <span class=\"math inline\">\\(A_{mem}\\)</span> local attention state <span class=\"math inline\">\\(A_{dot}\\)</span> </p>\n<p><span class=\"math display\">\\[A=sigmoid(\\beta)\\odot\nA_{mem}+(1-sigmoid(\\beta))\\odot A_{dot}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span>\n0segment101</p>\n<img src=\"/45ee1a6d/infini_attention_gating.png\" class title=\"gating\">\n<p>segment-level\nmemory</p>\n<img src=\"/45ee1a6d/infini_attention_compare.png\" class title=\"\">\n<h2 id=\"-1\"></h2>\n<p>segment <span class=\"math inline\">\\(N\\)</span>\n20483276816segment</p>\n<ul>\n<li></li>\n</ul>\n<p>Infini-TransformerTransformer-XL/Memorzing\nTransformer/RMTPG-19Arxivlanguage modeling</p>\n<img src=\"/45ee1a6d/infini_attention_language_modeling.png\" class title=\"\">\n<p>Infini-TransformerMemorizing\nTransformers1%</p>\n<p>100kInfini-TransformerPPL</p>\n<ul>\n<li></li>\n</ul>\n<p>1B<br>\n- batch size = 64 - step = 30k -  &gt; 4k - segment length =\n2k</p>\n<p>1Mpasskey\nretrievalzero-shotfine-tune</p>\n<img src=\"/45ee1a6d/infini_attention_passkey.png\" class title=\"passkey\">\n<p>Infini-Transformerlinear + delta1Mpasskey\nretrieval</p>\n<p>8B8k30k500kBookSum</p>\n<img src=\"/45ee1a6d/infini_attention_booksum.png\" class title=\"booksum\">\n<p>Infini-Transformerlinear + delta</p>\n<h1 id=\"\"></h1>\n<ol type=\"1\">\n<li>StreamingLM-Infiniteattention\nsinktokenPPLLM-Infinitedistance\nceilingtokenMPPLtoken<br>\n</li>\n<li>Infini-Transformer</li>\n</ol>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Efficient Streaming Language Models with Attention Sinks\nhttps://arxiv.org/abs/2309.17453<br>\n2Transformer-XL: Attentive Language Models Beyond a Fixed-Length\nContext https://arxiv.org/abs/1901.02860<br>\n3Leave No Context Behind: Efficient Infinite Context Transformers\nwith Infini-attention https://arxiv.org/abs/2404.07143<br>\n4LM-Infinite: Zero-Shot Extreme Length Generalization for Large\nLanguage Models https://arxiv.org/abs/2308.16137</p>\n","length":14036,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>2024Q2RAGAgent</p>\n<p>()128k+</p>\n<p></p>\n<h1 id=\"streamingllm\">StreamingLLM</h1>\n<p>Efficient Streaming Language Models with Attention Sinks</p>\n<p>20239</p>\n<p>4M</p>\n<p></p>\n<p>MITCMUNVIDIAMETAStreamingLLM\n-- LLAMA2, MPT,\nFalconPythia4MStreamingLLMPPL</p>\n<p>StreamingLLMPPL</p>\n<h2 id=\"\"></h2>\n<p><br>\n-\nKVKVGPU\nH100A10040G/80GKV<br>\n-\nRoPERoPE</p>\n<p><br>\n- Longformerwindow\nattentionKVwindow\nattentiontoken<br>\n- sliding window with\nrecomputationhttps://github.com/mit-han-lab/streaming-llm/issues/51KVrecomputation<br>\n-\nNTKYaRNNSFW()https://kaiokendev.github.io/til#extending-context-to-8k<br>\n- Efficiently scaling transformer\ninferenceSmoothQuant: Accurate and efficient post-training\nquantization for large language modelsDynamic context pruning for\nefficient and interpretable autoregressive transformersSpatten:\nEfficient sparse attention architecture with cascade token and head\npruningH2o: Heavyhitter oracle for efficient generative inference\nof large language models<br>\n-\nFlashAttention<br>\n- Big BirdLinformer</p>\n<p>StreamingLLMattention\nsinkwindow\nattentionattention\nsinkPPLsliding\nwindow with\nrecomputationStreamingLLM22+</p>\n<p>StreamingLLMPPLStreamingLLMPPL</p>\n<img src=\"/45ee1a6d/streamingllm_model_ppl.png\" class title=\"PPL\">\n<h2 id=\"attention-sink\">attention sink</h2>\n<p>window\nattentionLLMtoken</p>\n<img src=\"/45ee1a6d/stremingllm_attention_sink.png\" class title=\"attention sink\">\n<p>attention\nscoretokentoken</p>\n<p>tokenattention\nsinktoken</p>\n<p>softmaxsoftmaxtoken1tokentokentoken1</p>\n<p>tokentokentokentokenattention\nsink</p>\n<p>4tokentokenattention\nsinktoken</p>\n<p>quantSmoothQuant: Accurate and\nefficient post-training quantization for large language\nmodelsQuantizable transformers: Removing outliers by helping\nattention heads do nothing</p>\n<h2 id=\"\"></h2>\n<p>StreamingLLMStreamingLLMattention\nsink</p>\n<p>StreamingLLMwindow\nattentionattention sink\ntokenKVtokenKVtokenKV\ncachetokenStreamingLLMattention</p>\n<img src=\"/45ee1a6d/streamingllm_compare.png\" class title=\"StreamingLLM\">\n<p>token</p>\n<img src=\"/45ee1a6d/stremingllm_init_token_num.png\" class title=\"attention sink number\">\n<p>4tokenPPLtoken</p>\n<p>window attentionStreamingLLMKV\ncache<br>\n- 4tokenattention sink<br>\n- KV cache</p>\n<p>StreamingLLMKV</p>\n<img src=\"/45ee1a6d/stremingllm_kv_cache.png\" class title=\"cache\">\n<p>StreamingLLMdistancetokencachedistanceRoPEtokenKVtokenLM-Infinite</p>\n<p>attention\nsinktokenattention\nsink tokentoken</p>\n<p>tokensoftmaxsoftmax-off-by-oneattentionsoftmax</p>\n<p>softmax-off-by-one</p>\n<p><span class=\"math display\">\\[\\text{SoftMax}_1(x)_i=\\frac{e^{x_i}}{1+\\sum_{j=1}^Ne^{x_j}}\\]</span></p>\n<p>softmax-off-by-one1attention\nscore1KV0tokentoken1tokenattention\nsink</p>\n<p>tokenattention\nsinksoftmax-off-by-one3160M</p>\n<img src=\"/45ee1a6d/stremingllm_exp.png\" class title=\"\">\n<p>softmax-off-by-onezero\nsinktokenattention sink</p>\n<p>LLAMA2, MPT,\nFalconPythiaStreamingLLM4MMPTALIBIRoPE</p>\n<img src=\"/45ee1a6d/stremingllm_perf_4m.png\" class title=\"\">\n<p>4MStreamingLLMPPL</p>\n<p>StreamingLLMwindow\nattentioncacheStreamingLLMPPLStreamingLLM</p>\n<h1 id=\"lm-infinite\">LM-Infinite</h1>\n<p>LM-Infinite: Zero-Shot Extreme Length Generalization for Large\nLanguage Models</p>\n<p>20238</p>\n<p>2k4k200MPPL</p>\n<p></p>\n<p>LM-InfiniteLLMPasskey\nRetrievalQasper2.77.5</p>\n<h2 id=\"\"></h2>\n<p>LM-InfiniteTransformer\nLLM3</p>\n<ul>\n<li>1challenges in handling unseen distances among tokens</li>\n</ul>\n<p>tokentokenattention\nlogits</p>\n<p>LLAMA2ArXiv8kattention\nlogits</p>\n<img src=\"/45ee1a6d/lm_infinite_attention_logits_explode.png\" class title=\"logits\">\n<p>LLAMA24k4kattention\nlogits</p>\n<p>tokenlogits</p>\n<ul>\n<li>2attending to unseen numbers of tokens</li>\n</ul>\n<p>tokenlogits(attention\nentropy)</p>\n<p>tokentoken</p>\n<p>8k</p>\n<img src=\"/45ee1a6d/lm_infinite_attention_entropy.png\" class title=\"\">\n<p>attention context\nsize</p>\n<p>window\nattentiontokentokenhandle12XPosLongformer</p>\n<p>window attention</p>\n<ul>\n<li>3starting tokens occupy a distinct feature space</li>\n</ul>\n<p>token</p>\n<p>The impact of positional encoding on length\ngeneralization in\ntransformers1tokentokentoken</p>\n<p>StreamingLLMattention\nsinktokenwindow\nattentiontoken</p>\n<p>LLAMA2hidden state\noutputPCA2</p>\n<img src=\"/45ee1a6d/lm_infinite_starting_tokens.png\" class title=\"token\">\n<p>tokentokentokentokentokentoken</p>\n<p>token</p>\n<h2 id=\"\"></h2>\n<p>LM-InfiniteLLMzero-shotLM-Infinite-shaped\nattention maskDistance ceiling</p>\n<ul>\n<li>-shaped attention mask</li>\n</ul>\n<p>-shaped attention maskLongformerLongNetBig\nBird</p>\n<p>-shaped attention maskwindow\nattentiontokentokentoken1<span class=\"math inline\">\\(n_{\\mathrm{starting}}\\)</span>\ntoken2<span class=\"math inline\">\\(L_{\\mathrm{pretrain}}\\)</span>\ntoken<span class=\"math inline\">\\(n_{\\mathrm{starting}}\\)</span> <span class=\"math inline\">\\(L_{\\mathrm{pretrain}}\\)</span>\ntokentoken</p>\n<p> <span class=\"math inline\">\\(n_{\\mathrm{starting}}\\)</span>\n <span class=\"math inline\">\\(n_{\\mathrm{starting}}\\in[5,100]\\)</span>\n</p>\n<img src=\"/45ee1a6d/lm_infinite_starting_tokens_num.png\" class title=\"token\">\n<p></p>\n<p>-shaped attention mask23</p>\n<ul>\n<li>Distance ceiling</li>\n</ul>\n<p>LM-Infinite <span class=\"math inline\">\\(L_{\\mathrm{pretrain}}\\)</span>token</p>\n<p>attention logit <span class=\"math inline\">\\(w(\\mathbf{q},\\mathbf{k},d)\\)</span> <span class=\"math inline\">\\(d\\)</span> tokenDistance\nceilingattention logit</p>\n<p><span class=\"math display\">\\[\\text{attention\nlogits}=w(\\mathbf{q},\\mathbf{k},d&#39;)\\]</span></p>\n<p><span class=\"math display\">\\[d&#39;=\\min(d,L_\\text{pretrain})\\]</span></p>\n<p>Distance ceiling1</p>\n<ul>\n<li>Optionally attending to top-k tokens in the middle</li>\n</ul>\n<p>-shaped attention maskDistance\nceilingtokentokentokenattentiontoken</p>\n<p>token <span class=\"math inline\">\\(k\\)</span> attention\nlogitstokenattention  <span class=\"math inline\">\\(k\\)</span> token <span class=\"math inline\">\\(d=\\frac12L_\\text{pre-train}\\)</span></p>\n<p> <span class=\"math inline\">\\(k\\)</span> Passkey\nRetrievalvalidation set</p>\n<img src=\"/45ee1a6d/lm_infinite_middle_k.png\" class title=\"k\">\n<p> <span class=\"math inline\">\\(k=5\\)</span>&gt;5</p>\n<p>middle\ntokenmiddle\ntoken2</p>\n<p>LM-Infinite</p>\n<img src=\"/45ee1a6d/lm_infinite_design.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p>LLaMA-7BLLaMA2-7BMPT-7BGPT-J-6BLM-InfiniteMPT-7BAlibiRoPE</p>\n<ul>\n<li>Language Modeling</li>\n</ul>\n<p>ArXivOpenWebText2</p>\n<p>LM-Infinite0-12kPPL</p>\n<img src=\"/45ee1a6d/lm_infinite_ppl_figure.png\" class title=\"PPL\">\n<p>LLAMA210KNaN32KOOM</p>\n<p>PPLLM-InfinitePPL</p>\n<p>LM-InfiniteLM-Infinite +\nLlama2ArXiv200M\ntokenPPL200MPPL</p>\n<img src=\"/45ee1a6d/lm_infinite_ppl_200m.png\" class title=\"PPL 200M\">\n<ul>\n<li>Passkey Retrieval and Qapser</li>\n</ul>\n<p>Passkey\nRetrievalQapsertop-5middle\ntokenattention</p>\n<p>Passkey\nRetrieval0LM-Infinite</p>\n<img src=\"/45ee1a6d/lm_infinite_downstream.png\" class title=\"\">\n<ul>\n<li>Ablation study</li>\n</ul>\n<p>LM-Infinite-shaped attention maskDistance\nceiling</p>\n<img src=\"/45ee1a6d/lm_infinite_ablation.png\" class title=\"\">\n<p>-shaped attention maskdistance\nceilingPPL</p>\n<h1 id=\"transformer-xl\">Transformer-XL</h1>\n<p>Infini-TransformerTransformer-XL</p>\n<p>Transformer-XL20196CMUGoogle\nBrain</p>\n<p>transformersegmentsegment</p>\n<img src=\"/45ee1a6d/xl_vanilla_sw.png\" class title=\"vanilla transformer\">\n<p></p>\n<p>Transformer-XLattentionsegmentLsegment\n<span class=\"math inline\">\\([\\mathbf{s}_{\\tau}=x_{\\tau,1},\\cdots,x_{\\tau,L}]\\)</span>\n <span class=\"math inline\">\\(\\mathbf{s}_{\\tau+1}=[x_{\\tau+1,1},\\cdots,x_{\\tau+1,L}]\\)</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}&amp;\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}=\\left[\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\right]\\\\&amp;\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n=\\mathbf{h}_{\\tau+1}^{n-1}\\mathbf{W}_q^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_k^\\top,\\widetilde{\\mathbf{h}}_{\\tau+1}^{n-1}\\mathbf{W}_v^\\top\\\\&amp;\\mathbf{h}_{\\tau+1}^n=\\text{Transformer-Layer}\\left(\\mathbf{q}_{\\tau+1}^n,\\mathbf{k}_{\\tau+1}^n,\\mathbf{v}_{\\tau+1}^n\\right)\\end{aligned}\\]</span></p>\n<p>SGstop\ngradient<span class=\"math inline\">\\(\\begin{bmatrix}\\mathrm{SG}(\\mathbf{h}_\\tau^{n-1})\\circ\\mathbf{h}_{\\tau+1}^{n-1}\\end{bmatrix}\\)</span>\nhqkv <span class=\"math inline\">\\(n\\)</span>  <span class=\"math inline\">\\(n-1\\)</span> </p>\n<p>segmentattentionTransformer-XLsegmentKVsegmentsegmentsegment</p>\n<img src=\"/45ee1a6d/xl_attention.png\" class title=\"Transformer-XL\">\n<h1 id=\"infini-transformer\">Infini-Transformer</h1>\n<p>Leave No Context Behind:Efficient Infinite Context Transformers\nwith Infini-attention</p>\n<p>20244</p>\n<p>500k/1M</p>\n<p>/</p>\n<p>GoogleInfini-attention</p>\n<h2 id=\"-1\"></h2>\n<p></p>\n<p>transformer</p>\n<p>StreamingLLM</p>\n<p>Metalearned neural\nmemory/Enhancing the transformer with explicit relational encoding\nfor math problem solvingcompressive\nmemorycontextsimplicitytradeoff</p>\n<h2 id=\"infini-attention\">Infini-attention</h2>\n<p>Infini-TransformerInfini-attentionTransformer\nblockmask local attentionlong term\nlinear attention</p>\n<img src=\"/45ee1a6d/infini_attention_structure.png\" class title=\"infini-attention\">\n<p>Infini-attentionQKVQInfini-attentionconcat</p>\n<p>TransformerLLM</p>\n<p>Infini-attentionTransformer-XL</p>\n<img src=\"/45ee1a6d/infini_attention_process.png\" class title=\"infini-attention\">\n<p>Transformer-XLInfini-TransformersegmentTransformer-XLsegment</p>\n<p>Infini-Transformersegment</p>\n<p>compressive memorysimplicitycomputational\nefficiencyLearning associative inference using fast\nweight memorymemoryassociative matrix</p>\n<p></p>\n<ul>\n<li></li>\n</ul>\n<p>segment <span class=\"math inline\">\\(N\\)</span> <span class=\"math inline\">\\(Q\\in\\mathbf{R}^{N\\times d_{key}}\\)</span> memory\n<span class=\"math inline\">\\(M_{s-1}\\in\\mathbf{R}^{d_{key}\\times\nd_{value}}\\)</span> </p>\n<p><span class=\"math display\">\\[A_{\\text{mm}}=\\frac{\\sigma(Q)M_{s-1}}{\\sigma(Q)z_{s-1}}\\]</span></p>\n<p><span class=\"math inline\">\\(\\sigma\\)</span>element-wise\nELU + 1</p>\n<p><span class=\"math inline\">\\(z_{s-1}\\in\\mathbf{R}^{d_{key}}\\)</span>normalization\ntermnormalization termTransformers are rnns: Fast\nautoregressive transformers with linear\nattentionK</p>\n<ul>\n<li></li>\n</ul>\n<p>normalization\ntermmemory</p>\n<p><span class=\"math display\">\\[M_s\\leftarrow\nM_{s-1}+\\sigma(K)^TV\\]</span></p>\n<p><span class=\"math display\">\\[z_s\\leftarrow\nz_{s-1}+\\sum_{t=1}^N\\sigma(K_t)\\]</span></p>\n<p>memoryMetalearned neural memoryLearning\nassociative inference using fast weight memorydelta\nrule</p>\n<p><span class=\"math display\">\\[M_s\\leftarrow\nM_{s-1}+\\sigma(K)^T(V-\\frac{\\sigma(K)M_{s-1}}{\\sigma(K)z_{s-1}})\\]</span></p>\n<ul>\n<li>local attention</li>\n</ul>\n<p>segment <span class=\"math inline\">\\(A_{mem}\\)</span> local attention state <span class=\"math inline\">\\(A_{dot}\\)</span> </p>\n<p><span class=\"math display\">\\[A=sigmoid(\\beta)\\odot\nA_{mem}+(1-sigmoid(\\beta))\\odot A_{dot}\\]</span></p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span>\n</p>\n<p> <span class=\"math inline\">\\(\\beta\\)</span>\n0segment101</p>\n<img src=\"/45ee1a6d/infini_attention_gating.png\" class title=\"gating\">\n<p>segment-level\nmemory</p>\n<img src=\"/45ee1a6d/infini_attention_compare.png\" class title=\"\">\n<h2 id=\"-1\"></h2>\n<p>segment <span class=\"math inline\">\\(N\\)</span>\n20483276816segment</p>\n<ul>\n<li></li>\n</ul>\n<p>Infini-TransformerTransformer-XL/Memorzing\nTransformer/RMTPG-19Arxivlanguage modeling</p>\n<img src=\"/45ee1a6d/infini_attention_language_modeling.png\" class title=\"\">\n<p>Infini-TransformerMemorizing\nTransformers1%</p>\n<p>100kInfini-TransformerPPL</p>\n<ul>\n<li></li>\n</ul>\n<p>1B<br>\n- batch size = 64 - step = 30k -  &gt; 4k - segment length =\n2k</p>\n<p>1Mpasskey\nretrievalzero-shotfine-tune</p>\n<img src=\"/45ee1a6d/infini_attention_passkey.png\" class title=\"passkey\">\n<p>Infini-Transformerlinear + delta1Mpasskey\nretrieval</p>\n<p>8B8k30k500kBookSum</p>\n<img src=\"/45ee1a6d/infini_attention_booksum.png\" class title=\"booksum\">\n<p>Infini-Transformerlinear + delta</p>\n<h1 id=\"\"></h1>\n<ol type=\"1\">\n<li>StreamingLM-Infiniteattention\nsinktokenPPLLM-Infinitedistance\nceilingtokenMPPLtoken<br>\n</li>\n<li>Infini-Transformer</li>\n</ol>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Efficient Streaming Language Models with Attention Sinks\nhttps://arxiv.org/abs/2309.17453<br>\n2Transformer-XL: Attentive Language Models Beyond a Fixed-Length\nContext https://arxiv.org/abs/1901.02860<br>\n3Leave No Context Behind: Efficient Infinite Context Transformers\nwith Infini-attention https://arxiv.org/abs/2404.07143<br>\n4LM-Infinite: Zero-Shot Extreme Length Generalization for Large\nLanguage Models https://arxiv.org/abs/2308.16137</p>\n"},{"title":"-simPO","abbrlink":"280fa97a","date":"2024-05-31T14:09:23.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nDPOODPO[-DPO](http://www.linsight.cn/473f2b43.html)[-ODPO](http://www.linsight.cn/da871ebe.html)  \n\nsimPODPOsimPOreference modelsimPO  \n\n{% asset_img intro.png simPO %}  \n\n# DPO  \n\nDPODPOreward functionclosed-form expression  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\nBradley-Terry model  \n\n$$\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid x)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid x)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]$$  \n\nDPORLHFDPO  \n- reference model  \n- reward  \n\nresponsetokenlog likelihood  \n\n$$\\begin{aligned}p_\\theta(y\\mid x)=\\frac{1}{|y|}\\log\\pi_\\theta(y\\mid x)=\\frac{1}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid x,y_{<i})\\end{aligned}$$  \n\ngreedy decodingbeam searchtop-k samplingresponse/  \n\nDPOrewardreferenc modelDPO $r(x,y_w)>r(x,y_l)$  $p_\\theta(y_w\\mid x)>p_\\theta(y_l\\mid x)$  \n\nDPO $r(x,y_w)>r(x,y_l)$  $p_\\theta(y_w\\mid x)>p_\\theta(y_l\\mid x)$ 50%  \n\n{% asset_img contingency_table.png contingency table %}  \n\n  \n\nsimPO  \n\n{% asset_img simpo_contingency.png simPO contingency table %}  \n\n# simPO  \n\n##   \n\nreward  \n\n$$\\begin{aligned}r^*(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}\\end{aligned}$$  \n\nZ\n\n  \n\n$$\\begin{aligned}r_{\\text{SimPO}}(x,y)=\\frac{\\beta}{|y|}\\log\\pi_\\theta(y\\mid x)=\\frac{\\beta}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid x,y_{<i})\\end{aligned}$$  \n\n  \n\nrewardsimPOIPOODPOreward marginwinning responselosing responserewardreward margin  \n\n$$p(y_w\\succ y_l\\mid x)=\\sigma\\left(r(x,y_w)-r(x,y_l)-\\gamma\\right)$$  \n\nmarginmargin  \n\nsimPO  \n\n$$\\mathcal{L}_{\\text{SimPO}}(\\pi_\\theta)=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\frac{\\beta}{|y_w|}\\log\\pi_\\theta(y_w|x)-\\frac{\\beta}{|y_l|}\\log\\pi_\\theta(y_l|x)-\\gamma\\right)\\right]$$  \n\n## simPO  \n\nDPOsimPO  \n\n{% asset_img gradient.png  %}  \n\nDPOsimPO  \n- simPOreference modelpolicy modeldispreferred responserewardcase  \n- simPOlength-normalizedDisentangling length from quality in direct preference optimizationDPOtoken  \n\n#   \n\n##   \n\nLlama3-8BMistral-7Bbaseinstruct  \n\nbaseUltraChat-200kSFT UltraFeedbackpreference optimization  \n\ninstructIterative DPO alignmentSFTpreferenceUltraFeedbackprompttemperature=0.8SFT5responsePairRMLLM-Blender: Ensembling large language models with pairwise ranking and generative fusion5responsepreferred responsedispreferred response  \n\nLlama3-Base, Llama3-Instruct, Mistral-BaseMistral-Instruct  \n\npreference optimization  \n\n{% asset_img hyperparameters.png  %}  \n\n{% asset_img simpo_hyperparameters.png  %}  \n\nbatch size  \n\n  \n\n{% asset_img benchmark.png benchmark %}  \n\n##   \n\n  \n\n{% asset_img main_results.png  %}  \n\nLClength-controlledwin rate  \n\n  \n- MT-BenchFrom live data to high-quality benchmarks: The Arena-Hard pipeline  \n- instructbase  \n- AlpacaEval 2Arena-HardsimPOraw win ratelength-controlled win rate  \n\n##   \n\nsimPOlength normalizationmargin  \n\n{% asset_img ablation.png  %}  \n\nlength normalizationmargin  \n\n  \n\nsimPO  \n\nsimPOwin ratesimPO  \n\nwr  \n\n{% asset_img ln.png  %}  \n\nrewardrewardymargin  \n\nreward  \n\n{% asset_img ln_effect.png  %}  \n\nsimPOpositive reward marginwinning responsenegative reward difference  \n\nbcrewardresponse length  \n\nreward margin  \n\nreward accuracypolicy modelwinning responserewardlosing responsemarginreward accuracy  \n\n{% asset_img reward_accuracy.png reward accuracy %}  \n\nreward marginreward differencewinning responsewinning response  \n\n{% asset_img margin_dist.png  %}  \n\nmargin  \n\n## DPOsimPO  \n\n1. DPOrewardreference modellength biasDPOrewardsimPO  \n\n{% asset_img dpo_correlation.png correlation %}  \n\n2. simPODPOreward accuracysimPOreward  \n\n{% asset_img reward_accuracy_compare.png reward accuracy %}  \n\n#   \n\nsimPOpolicy modelsimPOreference model  \n\nLLAMAMistral  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n1SimPO: Simple Preference Optimization with a Reference-Free Reward https://arxiv.org/abs/2405.14734  \n","source":"_posts/cs/nlp/2024/05/-simPO.md","raw":"---\ntitle: -simPO\nabbrlink: 280fa97a\ndate: 2024-05-31 22:09:23\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \n  - SFT\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nDPOODPO[-DPO](http://www.linsight.cn/473f2b43.html)[-ODPO](http://www.linsight.cn/da871ebe.html)  \n\nsimPODPOsimPOreference modelsimPO  \n\n{% asset_img intro.png simPO %}  \n\n# DPO  \n\nDPODPOreward functionclosed-form expression  \n\n$$\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}$$  \n\nBradley-Terry model  \n\n$$\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid x)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid x)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]$$  \n\nDPORLHFDPO  \n- reference model  \n- reward  \n\nresponsetokenlog likelihood  \n\n$$\\begin{aligned}p_\\theta(y\\mid x)=\\frac{1}{|y|}\\log\\pi_\\theta(y\\mid x)=\\frac{1}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid x,y_{<i})\\end{aligned}$$  \n\ngreedy decodingbeam searchtop-k samplingresponse/  \n\nDPOrewardreferenc modelDPO $r(x,y_w)>r(x,y_l)$  $p_\\theta(y_w\\mid x)>p_\\theta(y_l\\mid x)$  \n\nDPO $r(x,y_w)>r(x,y_l)$  $p_\\theta(y_w\\mid x)>p_\\theta(y_l\\mid x)$ 50%  \n\n{% asset_img contingency_table.png contingency table %}  \n\n  \n\nsimPO  \n\n{% asset_img simpo_contingency.png simPO contingency table %}  \n\n# simPO  \n\n##   \n\nreward  \n\n$$\\begin{aligned}r^*(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}\\end{aligned}$$  \n\nZ\n\n  \n\n$$\\begin{aligned}r_{\\text{SimPO}}(x,y)=\\frac{\\beta}{|y|}\\log\\pi_\\theta(y\\mid x)=\\frac{\\beta}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid x,y_{<i})\\end{aligned}$$  \n\n  \n\nrewardsimPOIPOODPOreward marginwinning responselosing responserewardreward margin  \n\n$$p(y_w\\succ y_l\\mid x)=\\sigma\\left(r(x,y_w)-r(x,y_l)-\\gamma\\right)$$  \n\nmarginmargin  \n\nsimPO  \n\n$$\\mathcal{L}_{\\text{SimPO}}(\\pi_\\theta)=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\frac{\\beta}{|y_w|}\\log\\pi_\\theta(y_w|x)-\\frac{\\beta}{|y_l|}\\log\\pi_\\theta(y_l|x)-\\gamma\\right)\\right]$$  \n\n## simPO  \n\nDPOsimPO  \n\n{% asset_img gradient.png  %}  \n\nDPOsimPO  \n- simPOreference modelpolicy modeldispreferred responserewardcase  \n- simPOlength-normalizedDisentangling length from quality in direct preference optimizationDPOtoken  \n\n#   \n\n##   \n\nLlama3-8BMistral-7Bbaseinstruct  \n\nbaseUltraChat-200kSFT UltraFeedbackpreference optimization  \n\ninstructIterative DPO alignmentSFTpreferenceUltraFeedbackprompttemperature=0.8SFT5responsePairRMLLM-Blender: Ensembling large language models with pairwise ranking and generative fusion5responsepreferred responsedispreferred response  \n\nLlama3-Base, Llama3-Instruct, Mistral-BaseMistral-Instruct  \n\npreference optimization  \n\n{% asset_img hyperparameters.png  %}  \n\n{% asset_img simpo_hyperparameters.png  %}  \n\nbatch size  \n\n  \n\n{% asset_img benchmark.png benchmark %}  \n\n##   \n\n  \n\n{% asset_img main_results.png  %}  \n\nLClength-controlledwin rate  \n\n  \n- MT-BenchFrom live data to high-quality benchmarks: The Arena-Hard pipeline  \n- instructbase  \n- AlpacaEval 2Arena-HardsimPOraw win ratelength-controlled win rate  \n\n##   \n\nsimPOlength normalizationmargin  \n\n{% asset_img ablation.png  %}  \n\nlength normalizationmargin  \n\n  \n\nsimPO  \n\nsimPOwin ratesimPO  \n\nwr  \n\n{% asset_img ln.png  %}  \n\nrewardrewardymargin  \n\nreward  \n\n{% asset_img ln_effect.png  %}  \n\nsimPOpositive reward marginwinning responsenegative reward difference  \n\nbcrewardresponse length  \n\nreward margin  \n\nreward accuracypolicy modelwinning responserewardlosing responsemarginreward accuracy  \n\n{% asset_img reward_accuracy.png reward accuracy %}  \n\nreward marginreward differencewinning responsewinning response  \n\n{% asset_img margin_dist.png  %}  \n\nmargin  \n\n## DPOsimPO  \n\n1. DPOrewardreference modellength biasDPOrewardsimPO  \n\n{% asset_img dpo_correlation.png correlation %}  \n\n2. simPODPOreward accuracysimPOreward  \n\n{% asset_img reward_accuracy_compare.png reward accuracy %}  \n\n#   \n\nsimPOpolicy modelsimPOreference model  \n\nLLAMAMistral  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n\n***  \n\n# Reference  \n\n1SimPO: Simple Preference Optimization with a Reference-Free Reward https://arxiv.org/abs/2405.14734  \n","slug":"cs/nlp/2024/05/-simPO","published":1,"updated":"2024-06-02T04:02:19.947Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmx001l0p4k0q3edf3p","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>DPOODPO<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a></p>\n<p>simPODPOsimPOreference\nmodelsimPO</p>\n<img src=\"/280fa97a/intro.png\" class title=\"simPO\">\n<h1 id=\"dpo\">DPO</h1>\n<p>DPODPOreward functionclosed-form expression</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p>Bradley-Terry model</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid\nx)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid\nx)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]\\]</span></p>\n<p>DPORLHFDPO<br>\n- reference model<br>\n-\nreward</p>\n<p>responsetokenlog\nlikelihood</p>\n<p><span class=\"math display\">\\[\\begin{aligned}p_\\theta(y\\mid\nx)=\\frac{1}{|y|}\\log\\pi_\\theta(y\\mid\nx)=\\frac{1}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid\nx,y_{&lt;i})\\end{aligned}\\]</span></p>\n<p>greedy\ndecodingbeam searchtop-k\nsamplingresponse/</p>\n<p>DPOrewardreferenc\nmodelDPO <span class=\"math inline\">\\(r(x,y_w)&gt;r(x,y_l)\\)</span>\n <span class=\"math inline\">\\(p_\\theta(y_w\\mid\nx)&gt;p_\\theta(y_l\\mid x)\\)</span></p>\n<p>DPO <span class=\"math inline\">\\(r(x,y_w)&gt;r(x,y_l)\\)</span>  <span class=\"math inline\">\\(p_\\theta(y_w\\mid x)&gt;p_\\theta(y_l\\mid\nx)\\)</span> 50%</p>\n<img src=\"/280fa97a/contingency_table.png\" class title=\"contingency table\">\n<p></p>\n<p>simPO</p>\n<img src=\"/280fa97a/simpo_contingency.png\" class title=\"simPO contingency table\">\n<h1 id=\"simpo\">simPO</h1>\n<h2 id=\"\"></h2>\n<p>reward</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r^*(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}\\end{aligned}\\]</span></p>\n<p>Z</p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}r_{\\text{SimPO}}(x,y)=\\frac{\\beta}{|y|}\\log\\pi_\\theta(y\\mid\nx)=\\frac{\\beta}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid\nx,y_{&lt;i})\\end{aligned}\\]</span></p>\n<p></p>\n<p>rewardsimPOIPOODPOreward\nmarginwinning responselosing\nresponserewardreward margin</p>\n<p><span class=\"math display\">\\[p(y_w\\succ y_l\\mid\nx)=\\sigma\\left(r(x,y_w)-r(x,y_l)-\\gamma\\right)\\]</span></p>\n<p>marginmargin</p>\n<p>simPO</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{SimPO}}(\\pi_\\theta)=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\frac{\\beta}{|y_w|}\\log\\pi_\\theta(y_w|x)-\\frac{\\beta}{|y_l|}\\log\\pi_\\theta(y_l|x)-\\gamma\\right)\\right]\\]</span></p>\n<h2 id=\"simpo\">simPO</h2>\n<p>DPOsimPO</p>\n<img src=\"/280fa97a/gradient.png\" class title=\"\">\n<p>DPOsimPO<br>\n- simPOreference modelpolicy\nmodeldispreferred\nresponserewardcase<br>\n- simPOlength-normalizedDisentangling length from\nquality in direct preference\noptimizationDPOtoken</p>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>Llama3-8BMistral-7Bbaseinstruct</p>\n<p>baseUltraChat-200kSFT\nUltraFeedbackpreference optimization</p>\n<p>instructIterative DPO\nalignmentSFTpreferenceUltraFeedbackprompttemperature=0.8SFT5responsePairRMLLM-Blender:\nEnsembling large language models with pairwise ranking and generative\nfusion5responsepreferred\nresponsedispreferred response</p>\n<p>Llama3-Base, Llama3-Instruct,\nMistral-BaseMistral-Instruct</p>\n<p>preference\noptimization</p>\n<img src=\"/280fa97a/hyperparameters.png\" class title=\"\">\n<img src=\"/280fa97a/simpo_hyperparameters.png\" class title=\"\">\n<p>batch size</p>\n<p></p>\n<img src=\"/280fa97a/benchmark.png\" class title=\"benchmark\">\n<h2 id=\"\"></h2>\n<p></p>\n<img src=\"/280fa97a/main_results.png\" class title=\"\">\n<p>LClength-controlledwin rate</p>\n<p><br>\n-\nMT-BenchFrom\nlive data to high-quality benchmarks: The Arena-Hard\npipeline<br>\n-\ninstructbase<br>\n- AlpacaEval 2Arena-HardsimPOraw win ratelength-controlled\nwin rate</p>\n<h2 id=\"\"></h2>\n<p>simPOlength\nnormalizationmargin</p>\n<img src=\"/280fa97a/ablation.png\" class title=\"\">\n<p>length normalizationmargin</p>\n<p></p>\n<p>simPO</p>\n<p>simPOwin\nratesimPO</p>\n<p>wr</p>\n<img src=\"/280fa97a/ln.png\" class title=\"\">\n<p>rewardrewardymargin</p>\n<p>reward</p>\n<img src=\"/280fa97a/ln_effect.png\" class title=\"\">\n<p>simPOpositive\nreward marginwinning\nresponsenegative reward\ndifference</p>\n<p>bcrewardresponse\nlength</p>\n<p>reward margin</p>\n<p>reward accuracypolicy modelwinning\nresponserewardlosing\nresponsemarginreward\naccuracy</p>\n<img src=\"/280fa97a/reward_accuracy.png\" class title=\"reward accuracy\">\n<p>reward marginreward differencewinning\nresponsewinning\nresponse</p>\n<img src=\"/280fa97a/margin_dist.png\" class title=\"\">\n<p>margin</p>\n<h2 id=\"dposimpo\">DPOsimPO</h2>\n<ol type=\"1\">\n<li>DPOrewardreference\nmodellength\nbiasDPOrewardsimPO</li>\n</ol>\n<img src=\"/280fa97a/dpo_correlation.png\" class title=\"correlation\">\n<ol start=\"2\" type=\"1\">\n<li>simPODPOreward\naccuracysimPOreward</li>\n</ol>\n<img src=\"/280fa97a/reward_accuracy_compare.png\" class title=\"reward accuracy\">\n<h1 id=\"\"></h1>\n<p>simPOpolicy\nmodelsimPOreference\nmodel</p>\n<p>LLAMAMistral</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1SimPO: Simple Preference Optimization with a Reference-Free\nReward https://arxiv.org/abs/2405.14734</p>\n","length":5196,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>DPOODPO<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a></p>\n<p>simPODPOsimPOreference\nmodelsimPO</p>\n<img src=\"/280fa97a/intro.png\" class title=\"simPO\">\n<h1 id=\"dpo\">DPO</h1>\n<p>DPODPOreward functionclosed-form expression</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}+\\beta\\log Z(x)\\end{aligned}\\]</span></p>\n<p>Bradley-Terry model</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w\\mid\nx)}{\\pi_{\\text{ref}}(y_w\\mid x)}-\\beta\\log\\frac{\\pi_\\theta(y_l\\mid\nx)}{\\pi_{\\text{ref}}(y_l\\mid x)}\\right)\\right]\\]</span></p>\n<p>DPORLHFDPO<br>\n- reference model<br>\n-\nreward</p>\n<p>responsetokenlog\nlikelihood</p>\n<p><span class=\"math display\">\\[\\begin{aligned}p_\\theta(y\\mid\nx)=\\frac{1}{|y|}\\log\\pi_\\theta(y\\mid\nx)=\\frac{1}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid\nx,y_{&lt;i})\\end{aligned}\\]</span></p>\n<p>greedy\ndecodingbeam searchtop-k\nsamplingresponse/</p>\n<p>DPOrewardreferenc\nmodelDPO <span class=\"math inline\">\\(r(x,y_w)&gt;r(x,y_l)\\)</span>\n <span class=\"math inline\">\\(p_\\theta(y_w\\mid\nx)&gt;p_\\theta(y_l\\mid x)\\)</span></p>\n<p>DPO <span class=\"math inline\">\\(r(x,y_w)&gt;r(x,y_l)\\)</span>  <span class=\"math inline\">\\(p_\\theta(y_w\\mid x)&gt;p_\\theta(y_l\\mid\nx)\\)</span> 50%</p>\n<img src=\"/280fa97a/contingency_table.png\" class title=\"contingency table\">\n<p></p>\n<p>simPO</p>\n<img src=\"/280fa97a/simpo_contingency.png\" class title=\"simPO contingency table\">\n<h1 id=\"simpo\">simPO</h1>\n<h2 id=\"\"></h2>\n<p>reward</p>\n<p><span class=\"math display\">\\[\\begin{aligned}r^*(x,y)=\\beta\\log\\frac{\\pi_\\theta(y\\mid\nx)}{\\pi_\\text{ref}(y\\mid x)}\\end{aligned}\\]</span></p>\n<p>Z</p>\n<p></p>\n<p><span class=\"math display\">\\[\\begin{aligned}r_{\\text{SimPO}}(x,y)=\\frac{\\beta}{|y|}\\log\\pi_\\theta(y\\mid\nx)=\\frac{\\beta}{|y|}\\sum_{i=1}^{|y|}\\log\\pi_\\theta(y_i\\mid\nx,y_{&lt;i})\\end{aligned}\\]</span></p>\n<p></p>\n<p>rewardsimPOIPOODPOreward\nmarginwinning responselosing\nresponserewardreward margin</p>\n<p><span class=\"math display\">\\[p(y_w\\succ y_l\\mid\nx)=\\sigma\\left(r(x,y_w)-r(x,y_l)-\\gamma\\right)\\]</span></p>\n<p>marginmargin</p>\n<p>simPO</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\text{SimPO}}(\\pi_\\theta)=-\\mathbb{E}_{(x,y_w,y_l)\\thicksim\\mathcal{D}}\\left[\\log\\sigma\\left(\\frac{\\beta}{|y_w|}\\log\\pi_\\theta(y_w|x)-\\frac{\\beta}{|y_l|}\\log\\pi_\\theta(y_l|x)-\\gamma\\right)\\right]\\]</span></p>\n<h2 id=\"simpo\">simPO</h2>\n<p>DPOsimPO</p>\n<img src=\"/280fa97a/gradient.png\" class title=\"\">\n<p>DPOsimPO<br>\n- simPOreference modelpolicy\nmodeldispreferred\nresponserewardcase<br>\n- simPOlength-normalizedDisentangling length from\nquality in direct preference\noptimizationDPOtoken</p>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>Llama3-8BMistral-7Bbaseinstruct</p>\n<p>baseUltraChat-200kSFT\nUltraFeedbackpreference optimization</p>\n<p>instructIterative DPO\nalignmentSFTpreferenceUltraFeedbackprompttemperature=0.8SFT5responsePairRMLLM-Blender:\nEnsembling large language models with pairwise ranking and generative\nfusion5responsepreferred\nresponsedispreferred response</p>\n<p>Llama3-Base, Llama3-Instruct,\nMistral-BaseMistral-Instruct</p>\n<p>preference\noptimization</p>\n<img src=\"/280fa97a/hyperparameters.png\" class title=\"\">\n<img src=\"/280fa97a/simpo_hyperparameters.png\" class title=\"\">\n<p>batch size</p>\n<p></p>\n<img src=\"/280fa97a/benchmark.png\" class title=\"benchmark\">\n<h2 id=\"\"></h2>\n<p></p>\n<img src=\"/280fa97a/main_results.png\" class title=\"\">\n<p>LClength-controlledwin rate</p>\n<p><br>\n-\nMT-BenchFrom\nlive data to high-quality benchmarks: The Arena-Hard\npipeline<br>\n-\ninstructbase<br>\n- AlpacaEval 2Arena-HardsimPOraw win ratelength-controlled\nwin rate</p>\n<h2 id=\"\"></h2>\n<p>simPOlength\nnormalizationmargin</p>\n<img src=\"/280fa97a/ablation.png\" class title=\"\">\n<p>length normalizationmargin</p>\n<p></p>\n<p>simPO</p>\n<p>simPOwin\nratesimPO</p>\n<p>wr</p>\n<img src=\"/280fa97a/ln.png\" class title=\"\">\n<p>rewardrewardymargin</p>\n<p>reward</p>\n<img src=\"/280fa97a/ln_effect.png\" class title=\"\">\n<p>simPOpositive\nreward marginwinning\nresponsenegative reward\ndifference</p>\n<p>bcrewardresponse\nlength</p>\n<p>reward margin</p>\n<p>reward accuracypolicy modelwinning\nresponserewardlosing\nresponsemarginreward\naccuracy</p>\n<img src=\"/280fa97a/reward_accuracy.png\" class title=\"reward accuracy\">\n<p>reward marginreward differencewinning\nresponsewinning\nresponse</p>\n<img src=\"/280fa97a/margin_dist.png\" class title=\"\">\n<p>margin</p>\n<h2 id=\"dposimpo\">DPOsimPO</h2>\n<ol type=\"1\">\n<li>DPOrewardreference\nmodellength\nbiasDPOrewardsimPO</li>\n</ol>\n<img src=\"/280fa97a/dpo_correlation.png\" class title=\"correlation\">\n<ol start=\"2\" type=\"1\">\n<li>simPODPOreward\naccuracysimPOreward</li>\n</ol>\n<img src=\"/280fa97a/reward_accuracy_compare.png\" class title=\"reward accuracy\">\n<h1 id=\"\"></h1>\n<p>simPOpolicy\nmodelsimPOreference\nmodel</p>\n<p>LLAMAMistral</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1SimPO: Simple Preference Optimization with a Reference-Free\nReward https://arxiv.org/abs/2405.14734</p>\n"},{"title":"(6)","abbrlink":"7c04944d","date":"2024-05-14T11:21:09.000Z","_content":"\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~  \n\n~\n\n***  \n\n# 1.Xavier  \n\n2010Understanding the difficulty of training deep feedforward neural networksXavier  \n\n1  \n\n2  \n\n3magnitude  \n\n4  \n\nXavierfan-infan-out  \n\n11/fan-in  \n\n21/fan-out  \n\n32/fan-in + fan-out1/fan-in  \n\n# 2.RLHFRewardCritic  \n\nCriticPPOCriticadvantage functionactorCriticactor  \n\nRewardRLHFRewardActorActor\nRLHFActorCriticRewardActorobjective1Reward/Critic2Actor-CriticActorCriticCritic3  \n\nCriticRewardActor  \n\n# 3.Kaiming  \n\nKaimingXavierXavierReLUXavier\nKaimingReLUReLU00Xaviersqrt(1/N)sqrt(2/N)ReLU  \n\n# 4.Bert[CLS]  \n\nself-attentiontokentokenBert[CLS]token[CLS][CLS][CLS]token\n\n\n# 5.pytorchregister_buffer  \n\nregister_buffernn.Moduleregister_bufferregister_bufferstate_dictbatchnormregister_bufferregister_bufferstate_dict\n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  ","source":"_posts/cs/nlp/2024/05/-6.md","raw":"---\ntitle: (6)\ntags:\n  - NLP\n  - LLM\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 7c04944d\ndate: 2024-05-14 19:21:09\n---\n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~  \n\n~\n\n***  \n\n# 1.Xavier  \n\n2010Understanding the difficulty of training deep feedforward neural networksXavier  \n\n1  \n\n2  \n\n3magnitude  \n\n4  \n\nXavierfan-infan-out  \n\n11/fan-in  \n\n21/fan-out  \n\n32/fan-in + fan-out1/fan-in  \n\n# 2.RLHFRewardCritic  \n\nCriticPPOCriticadvantage functionactorCriticactor  \n\nRewardRLHFRewardActorActor\nRLHFActorCriticRewardActorobjective1Reward/Critic2Actor-CriticActorCriticCritic3  \n\nCriticRewardActor  \n\n# 3.Kaiming  \n\nKaimingXavierXavierReLUXavier\nKaimingReLUReLU00Xaviersqrt(1/N)sqrt(2/N)ReLU  \n\n# 4.Bert[CLS]  \n\nself-attentiontokentokenBert[CLS]token[CLS][CLS][CLS]token\n\n\n# 5.pytorchregister_buffer  \n\nregister_buffernn.Moduleregister_bufferregister_bufferstate_dictbatchnormregister_bufferregister_bufferstate_dict\n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  ","slug":"cs/nlp/2024/05/-6","published":1,"updated":"2024-05-14T11:26:49.973Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmx001o0p4kcld16xa6","content":"<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM~</p>\n<p>~</p>\n<hr>\n<h1 id=\"xavier\">1.Xavier</h1>\n<p>2010Understanding the difficulty of training deep feedforward\nneural\nnetworksXavier</p>\n<p>1</p>\n<p>2</p>\n<p>3magnitude</p>\n<p>4</p>\n<p>Xavierfan-infan-out</p>\n<p>11/fan-in</p>\n<p>21/fan-out</p>\n<p>32/fan-in +\nfan-out1/fan-in</p>\n<h1 id=\"rlhfrewardcritic\">2.RLHFRewardCritic</h1>\n<p>CriticPPOCriticadvantage\nfunctionactorCriticactor</p>\n<p>RewardRLHFRewardActorActor\nRLHFActorCriticRewardActorobjective1Reward/Critic2Actor-CriticActorCriticCritic3</p>\n<p>CriticRewardActor</p>\n<h1 id=\"kaiming\">3.Kaiming</h1>\n<p>KaimingXavierXavierReLUXavier\nKaimingReLUReLU00Xaviersqrt(1/N)sqrt(2/N)ReLU</p>\n<h1 id=\"bertcls\">4.Bert[CLS]</h1>\n<p>self-attentiontokentokenBert[CLS]token[CLS][CLS][CLS]token</p>\n<h1 id=\"pytorchregister_buffer\">5.pytorchregister_buffer</h1>\n<p>register_buffernn.Moduleregister_bufferregister_bufferstate_dictbatchnormregister_bufferregister_bufferstate_dict</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n","length":2799,"excerpt":"","more":"<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM~</p>\n<p>~</p>\n<hr>\n<h1 id=\"xavier\">1.Xavier</h1>\n<p>2010Understanding the difficulty of training deep feedforward\nneural\nnetworksXavier</p>\n<p>1</p>\n<p>2</p>\n<p>3magnitude</p>\n<p>4</p>\n<p>Xavierfan-infan-out</p>\n<p>11/fan-in</p>\n<p>21/fan-out</p>\n<p>32/fan-in +\nfan-out1/fan-in</p>\n<h1 id=\"rlhfrewardcritic\">2.RLHFRewardCritic</h1>\n<p>CriticPPOCriticadvantage\nfunctionactorCriticactor</p>\n<p>RewardRLHFRewardActorActor\nRLHFActorCriticRewardActorobjective1Reward/Critic2Actor-CriticActorCriticCritic3</p>\n<p>CriticRewardActor</p>\n<h1 id=\"kaiming\">3.Kaiming</h1>\n<p>KaimingXavierXavierReLUXavier\nKaimingReLUReLU00Xaviersqrt(1/N)sqrt(2/N)ReLU</p>\n<h1 id=\"bertcls\">4.Bert[CLS]</h1>\n<p>self-attentiontokentokenBert[CLS]token[CLS][CLS][CLS]token</p>\n<h1 id=\"pytorchregister_buffer\">5.pytorchregister_buffer</h1>\n<p>register_buffernn.Moduleregister_bufferregister_bufferstate_dictbatchnormregister_bufferregister_bufferstate_dict</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n"},{"title":"","abbrlink":"cc852861","date":"2024-05-04T11:05:48.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n2024Q2RAGAgent  \n\n()CoT/ToT  \n\n  \n\n# 128k  \n\nData Engineering for Scaling Language Models to 128K Context  \n\n20242  \n\n  \n\n128k\n\n##   \n\nPPL  \n\nvalidation datasetPPL Needle in a Haystack  \n\nPPL  \n\n{% asset_img eng_ppl.png PPL %}  \n\nLongLoRAMistralYaRN>100kGPT-4  \n\n{% asset_img eng_needle_comp.png  %}  \n\n##   \n\n<=4k128ke.g. <5B token  \n\n32k400B tokenEffective long-context scaling of foundation modelsXverse  \n\n  \n\nLLAMALLAMASlimPajama  \n\ngithub  \n\n  \n- Cut at 4K4k4kLLAMA  \n- Cut at 128K128kLongLoRA  \n- Per-source Upsampling  \n- Global Upsampling  \n- Upsample Arxiv/ Book/ Github  \n\n  \n\nSlimPajama  \n\n{% asset_img eng_data_dist.png  %}  \n\nPer-source Upsampling  \n\n##   \n\n80kLLAMA2-7B64kLLAMA2-13B  \n\nFlashAttentionAttentionGPUCPUGPUGPUconstantlinear80k4k3400  \n\njiaqian  \n\n{% asset_img add_money.jpg  %}  \n\nPer-source Upsampling  \n\n{% asset_img eng_data.png  %}  \n\n  \n\n{% asset_img eng_config.png  %}  \n\n  \n- lr = 2e-5  \n- RoPE base1,0000500,000  \n- batch size = 4M token  \n\n##   \n\ntoken  \n\n100M300M500M1B5B10B tokencheckpointPPL  \n\n{% asset_img eng_tokens.png  %}  \n\n500M token5B token10B token  \n\n##   \n\nLLAMA2-7B5B token  \n\nLLAMA24k0-4k4k-128k  \n\n  \n\n{% asset_img eng_sample.png  %}  \n\n  \n- 0-4kPer-source Upsampling  \n- BookGithub  \n- 4k-128kPer-source Upsampling  \n\nlength upsamplingPer-source Upsampling  \n\n80kLLAMA2-7BPer-source UpsamplingPPLPer-source Upsampling  \n\n{% asset_img eng_ppl.png PPL %}  \n\n## \n\n  \n- PPL  \n-   \n-   \n\n# Paraphrasing  \n\nTraining With \"Paraphrasing the Original Text\" Improves Long-Context Performance  \n\n202312  \n\n  \n\n50k  \n\n{% asset_img paraphrasing_intro.png paraphrasing %}  \n\n##   \n\n  \n\nNTKYaRN  \n\nlost in the middlemiddleretrieval  \n\n##   \n\n  \n- TogetherLLaMA-2-7B-32Khttps://huggingface.co/datasets/togethercomputer/Long-Data-CollectionsTogetherMultipassage-QA-from-Natural-QuestionsBookSum  \n- LongAlpacaLongLoRA: Efficient Fine-tuning of Long-Context Large Language Models  \n- Ziya-ReaderNever Lost in the Middle:Improving Large Language Models via Attention Strengthening Question Answering  \n\n  \n-   \n-   \n-   \n\npromptCoT  \n\npromptClaude-2.1promptHere is the most relevant sentence in the context27%98%https://www.anthropic.com/news/claude-2-1-prompting  \n\n  \n- LongLLMLinguaLongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression  \n- Attention SortingAttention Sorting Combats Recency Bias In Long Context Language Modelsdecode  \n\n##   \n\nretrieval relevancetokenn-gram $x$  $R(x)$   \n\n$$R(x)=\\frac{n^\\prime}n\\log\\frac N{N^\\prime+1}$$  \n\nTF-IDF$n^\\prime$  $x$ gold-chunk $n$ gold-chunktoken$N$ chunk$N^\\prime$ xchunk  \n\ntoken $x$  $R(x)$  $S$   \n\n$$\\mathcal{R}(S)=\\frac{1}{|S_a|}\\sum_{x\\in\\mathcal{S}_a}R(x)$$  \n\n $S_a$  $S$   \n\n $\\mathcal{R}(S)$ $\\mathcal{R}(S)$     \n\ngold-chunkparaphrasing the original text  \n\nparaphrasing  \n\n{% asset_img paraphrasing_example.png paraphrasing %}  \n\ntokentokenparaphrasing  \n\nGPT-4paraphrasing  \n\n{% asset_img paraphrasing_dataset.png  %}  \n\n10,8258,4548k32k  \n\n{% asset_img paraphrasing_dataset_dist.png  %}  \n\nMulti-passage-QA-from-NQ  \n\n{% asset_img paraphrasing_quality.png  %}  \n\nLongBench  \n\n{% asset_img paraphrasing_perf.png  %}  \n\nlost in the middle  \n\n{% asset_img paraphrasing_lost.png lost in the middle %}  \n\n# PoSE  \n\nPoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training  \n\n20239  \n\n  \n\n128k\n\n##   \n\nRoPE  \n\nNTKYaRN8k32k128k...  \n\n**Po**sitional **S**kip-wis**E**PoSE2k128k128k  \n\nRandPosRandomized positional encodings boost length generalization of transformersRandPostokenPoSEtoken  \n\n##   \n\nPoSE  \n- index128k1-128k  \n-   \n\n $L_c$ $N$ chunk $c_0,c_1,\\ldots,c_{N-1}$ $l_0,l_1,\\ldots,l_{N-1}$chunk $i$token  \n\n$$\\mathrm{Pos}(c_i)=\\{st_i,st_i+1,\\ldots,st_i+l_i-1\\},\\quad st_i=\\sum_{j=0}^{i-1}l_j$$  \n\nchunkuniform distribution $\\mathcal{U}(S)$ skipping bias $u_i$biaschunktoken  \n\n$$\\mathrm{PoSE}(c_i)=\\{u_i+st_i,u_i+st_i+1,\\ldots,u_i+st_i+l_i-1\\}$$\n\nchunkoverlap $u_i\\geq u_{i-1}$  \n\nskipping biassamplechunkskipping bias  \n\nindexchunkindex  \n\nchunktoken  \n\ntoken$v_i\\sim\\mathcal{U}(\\{v_{i-1},\\ldots,L_x-L_c\\})$ $c_i$ token  \n\n$$c_i=\\boldsymbol{x}[v_i+st_i:v_i+st_i+l_i]$$  \n\n $v_i=u_i$$v_i=0$  $v_i$   \n\n$N$ 2 $u_0$  $v_0$ 0  \n\nPoSE  \n\n{% asset_img pose_method.png PoSE %}  \n\nLLAMA-7B2k1,000batch size64lr=2e-5warmup step=10  \n\nPoSEPPLFull-length  \n\n{% asset_img pose_ppl.png PPL %}  \n\npasskey retrieval  \n\n{% asset_img pose_passkey.png passkey %}  \n\nPoSEPoSE1M  \n\n#   \n\n1. FlashAttention128k5Btoken  \n2.   \n3. \n4. M/PoSE/  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n# Reference  \n1Data Engineering for Scaling Language Models to 128K Context https://arxiv.org/abs/2402.10171  \n2Training With \"Paraphrasing the Original Text\" Improves Long-Context Performance https://arxiv.org/abs/2312.11193  \n3PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training https://arxiv.org/abs/2309.10400  \n","source":"_posts/cs/nlp/2024/05/.md","raw":"---\ntitle: \nabbrlink: cc852861\ndate: 2024-05-04 19:05:48\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \n  - \n  - attention\ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n2024Q2RAGAgent  \n\n()CoT/ToT  \n\n  \n\n# 128k  \n\nData Engineering for Scaling Language Models to 128K Context  \n\n20242  \n\n  \n\n128k\n\n##   \n\nPPL  \n\nvalidation datasetPPL Needle in a Haystack  \n\nPPL  \n\n{% asset_img eng_ppl.png PPL %}  \n\nLongLoRAMistralYaRN>100kGPT-4  \n\n{% asset_img eng_needle_comp.png  %}  \n\n##   \n\n<=4k128ke.g. <5B token  \n\n32k400B tokenEffective long-context scaling of foundation modelsXverse  \n\n  \n\nLLAMALLAMASlimPajama  \n\ngithub  \n\n  \n- Cut at 4K4k4kLLAMA  \n- Cut at 128K128kLongLoRA  \n- Per-source Upsampling  \n- Global Upsampling  \n- Upsample Arxiv/ Book/ Github  \n\n  \n\nSlimPajama  \n\n{% asset_img eng_data_dist.png  %}  \n\nPer-source Upsampling  \n\n##   \n\n80kLLAMA2-7B64kLLAMA2-13B  \n\nFlashAttentionAttentionGPUCPUGPUGPUconstantlinear80k4k3400  \n\njiaqian  \n\n{% asset_img add_money.jpg  %}  \n\nPer-source Upsampling  \n\n{% asset_img eng_data.png  %}  \n\n  \n\n{% asset_img eng_config.png  %}  \n\n  \n- lr = 2e-5  \n- RoPE base1,0000500,000  \n- batch size = 4M token  \n\n##   \n\ntoken  \n\n100M300M500M1B5B10B tokencheckpointPPL  \n\n{% asset_img eng_tokens.png  %}  \n\n500M token5B token10B token  \n\n##   \n\nLLAMA2-7B5B token  \n\nLLAMA24k0-4k4k-128k  \n\n  \n\n{% asset_img eng_sample.png  %}  \n\n  \n- 0-4kPer-source Upsampling  \n- BookGithub  \n- 4k-128kPer-source Upsampling  \n\nlength upsamplingPer-source Upsampling  \n\n80kLLAMA2-7BPer-source UpsamplingPPLPer-source Upsampling  \n\n{% asset_img eng_ppl.png PPL %}  \n\n## \n\n  \n- PPL  \n-   \n-   \n\n# Paraphrasing  \n\nTraining With \"Paraphrasing the Original Text\" Improves Long-Context Performance  \n\n202312  \n\n  \n\n50k  \n\n{% asset_img paraphrasing_intro.png paraphrasing %}  \n\n##   \n\n  \n\nNTKYaRN  \n\nlost in the middlemiddleretrieval  \n\n##   \n\n  \n- TogetherLLaMA-2-7B-32Khttps://huggingface.co/datasets/togethercomputer/Long-Data-CollectionsTogetherMultipassage-QA-from-Natural-QuestionsBookSum  \n- LongAlpacaLongLoRA: Efficient Fine-tuning of Long-Context Large Language Models  \n- Ziya-ReaderNever Lost in the Middle:Improving Large Language Models via Attention Strengthening Question Answering  \n\n  \n-   \n-   \n-   \n\npromptCoT  \n\npromptClaude-2.1promptHere is the most relevant sentence in the context27%98%https://www.anthropic.com/news/claude-2-1-prompting  \n\n  \n- LongLLMLinguaLongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression  \n- Attention SortingAttention Sorting Combats Recency Bias In Long Context Language Modelsdecode  \n\n##   \n\nretrieval relevancetokenn-gram $x$  $R(x)$   \n\n$$R(x)=\\frac{n^\\prime}n\\log\\frac N{N^\\prime+1}$$  \n\nTF-IDF$n^\\prime$  $x$ gold-chunk $n$ gold-chunktoken$N$ chunk$N^\\prime$ xchunk  \n\ntoken $x$  $R(x)$  $S$   \n\n$$\\mathcal{R}(S)=\\frac{1}{|S_a|}\\sum_{x\\in\\mathcal{S}_a}R(x)$$  \n\n $S_a$  $S$   \n\n $\\mathcal{R}(S)$ $\\mathcal{R}(S)$     \n\ngold-chunkparaphrasing the original text  \n\nparaphrasing  \n\n{% asset_img paraphrasing_example.png paraphrasing %}  \n\ntokentokenparaphrasing  \n\nGPT-4paraphrasing  \n\n{% asset_img paraphrasing_dataset.png  %}  \n\n10,8258,4548k32k  \n\n{% asset_img paraphrasing_dataset_dist.png  %}  \n\nMulti-passage-QA-from-NQ  \n\n{% asset_img paraphrasing_quality.png  %}  \n\nLongBench  \n\n{% asset_img paraphrasing_perf.png  %}  \n\nlost in the middle  \n\n{% asset_img paraphrasing_lost.png lost in the middle %}  \n\n# PoSE  \n\nPoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training  \n\n20239  \n\n  \n\n128k\n\n##   \n\nRoPE  \n\nNTKYaRN8k32k128k...  \n\n**Po**sitional **S**kip-wis**E**PoSE2k128k128k  \n\nRandPosRandomized positional encodings boost length generalization of transformersRandPostokenPoSEtoken  \n\n##   \n\nPoSE  \n- index128k1-128k  \n-   \n\n $L_c$ $N$ chunk $c_0,c_1,\\ldots,c_{N-1}$ $l_0,l_1,\\ldots,l_{N-1}$chunk $i$token  \n\n$$\\mathrm{Pos}(c_i)=\\{st_i,st_i+1,\\ldots,st_i+l_i-1\\},\\quad st_i=\\sum_{j=0}^{i-1}l_j$$  \n\nchunkuniform distribution $\\mathcal{U}(S)$ skipping bias $u_i$biaschunktoken  \n\n$$\\mathrm{PoSE}(c_i)=\\{u_i+st_i,u_i+st_i+1,\\ldots,u_i+st_i+l_i-1\\}$$\n\nchunkoverlap $u_i\\geq u_{i-1}$  \n\nskipping biassamplechunkskipping bias  \n\nindexchunkindex  \n\nchunktoken  \n\ntoken$v_i\\sim\\mathcal{U}(\\{v_{i-1},\\ldots,L_x-L_c\\})$ $c_i$ token  \n\n$$c_i=\\boldsymbol{x}[v_i+st_i:v_i+st_i+l_i]$$  \n\n $v_i=u_i$$v_i=0$  $v_i$   \n\n$N$ 2 $u_0$  $v_0$ 0  \n\nPoSE  \n\n{% asset_img pose_method.png PoSE %}  \n\nLLAMA-7B2k1,000batch size64lr=2e-5warmup step=10  \n\nPoSEPPLFull-length  \n\n{% asset_img pose_ppl.png PPL %}  \n\npasskey retrieval  \n\n{% asset_img pose_passkey.png passkey %}  \n\nPoSEPoSE1M  \n\n#   \n\n1. FlashAttention128k5Btoken  \n2.   \n3. \n4. M/PoSE/  \n\n***\n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***  \n\n# Reference  \n1Data Engineering for Scaling Language Models to 128K Context https://arxiv.org/abs/2402.10171  \n2Training With \"Paraphrasing the Original Text\" Improves Long-Context Performance https://arxiv.org/abs/2312.11193  \n3PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training https://arxiv.org/abs/2309.10400  \n","slug":"cs/nlp/2024/05/","published":1,"updated":"2024-05-10T06:50:20.731Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmx001r0p4kfkhz57lw","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>2024Q2RAGAgent</p>\n<p>()CoT/ToT</p>\n<p></p>\n<h1 id=\"128k\">128k</h1>\n<p>Data Engineering for Scaling Language Models to 128K\nContext</p>\n<p>20242</p>\n<p></p>\n<p>128k</p>\n<h2 id=\"\"></h2>\n<p>PPL</p>\n<p>validation\ndatasetPPL\nNeedle in a\nHaystack</p>\n<p>PPL</p>\n<img src=\"/cc852861/eng_ppl.png\" class title=\"PPL\">\n<p>LongLoRAMistralYaRN&gt;100kGPT-4</p>\n<img src=\"/cc852861/eng_needle_comp.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p>&lt;=4k128ke.g.\n&lt;5B token</p>\n<p>32k400B\ntokenEffective long-context scaling of foundation\nmodelsXverse</p>\n<p></p>\n<p>LLAMALLAMASlimPajama</p>\n<p>github</p>\n<p><br>\n- Cut at\n4K4k4kLLAMA<br>\n- Cut at\n128K128kLongLoRA<br>\n- Per-source\nUpsampling<br>\n- Global Upsampling<br>\n- Upsample Arxiv/ Book/\nGithub</p>\n<p></p>\n<p>SlimPajama</p>\n<img src=\"/cc852861/eng_data_dist.png\" class title=\"\">\n<p>Per-source\nUpsampling</p>\n<h2 id=\"\"></h2>\n<p>80kLLAMA2-7B64kLLAMA2-13B</p>\n<p>FlashAttentionAttentionGPUCPUGPUGPUconstantlinear80k4k3400</p>\n<p>jiaqian</p>\n<img src=\"/cc852861/add_money.jpg\" class title=\"\">\n<p>Per-source Upsampling</p>\n<img src=\"/cc852861/eng_data.png\" class title=\"\">\n<p></p>\n<img src=\"/cc852861/eng_config.png\" class title=\"\">\n<p><br>\n- lr = 2e-5<br>\n- RoPE base1,0000500,000<br>\n- batch size = 4M token</p>\n<h2 id=\"\"></h2>\n<p>token</p>\n<p>100M300M500M1B5B10B\ntokencheckpointPPL</p>\n<img src=\"/cc852861/eng_tokens.png\" class title=\"\">\n<p>500M\ntoken5B\ntoken10B\ntoken</p>\n<h2 id=\"\"></h2>\n<p>LLAMA2-7B5B\ntoken</p>\n<p>LLAMA24k0-4k4k-128k</p>\n<p></p>\n<img src=\"/cc852861/eng_sample.png\" class title=\"\">\n<p><br>\n- 0-4kPer-source\nUpsampling<br>\n-\nBookGithub<br>\n- 4k-128kPer-source\nUpsampling</p>\n<p>length upsamplingPer-source\nUpsampling</p>\n<p>80kLLAMA2-7BPer-source\nUpsamplingPPLPer-source\nUpsampling</p>\n<img src=\"/cc852861/eng_ppl.png\" class title=\"PPL\">\n<h2 id=\"\"></h2>\n<p><br>\n-\nPPL<br>\n- <br>\n- </p>\n<h1 id=\"paraphrasing\">Paraphrasing</h1>\n<p>Training With \"Paraphrasing the Original Text\" Improves\nLong-Context Performance</p>\n<p>202312</p>\n<p></p>\n<p>50k</p>\n<img src=\"/cc852861/paraphrasing_intro.png\" class title=\"paraphrasing\">\n<h2 id=\"\"></h2>\n<p></p>\n<p>NTKYaRN</p>\n<p>lost in the\nmiddlemiddleretrieval</p>\n<h2 id=\"\"></h2>\n<p><br>\n-\nTogetherLLaMA-2-7B-32Khttps://huggingface.co/datasets/togethercomputer/Long-Data-CollectionsTogetherMultipassage-QA-from-Natural-QuestionsBookSum<br>\n- LongAlpacaLongLoRA: Efficient Fine-tuning of Long-Context Large\nLanguage Models<br>\n- Ziya-ReaderNever Lost in the Middle:Improving Large Language\nModels via Attention Strengthening Question Answering</p>\n<p><br>\n- <br>\n- <br>\n-\n</p>\n<p>promptCoT</p>\n<p>promptClaude-2.1promptHere\nis the most relevant sentence in the\ncontext27%98%https://www.anthropic.com/news/claude-2-1-prompting</p>\n<p><br>\n- LongLLMLinguaLongLLMLingua: Accelerating and Enhancing LLMs in\nLong Context Scenarios via Prompt\nCompression<br>\n- Attention SortingAttention Sorting Combats Recency Bias In Long\nContext Language\nModelsdecode</p>\n<h2 id=\"\"></h2>\n<p>retrieval\nrelevancetokenn-gram <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(R(x)\\)</span> </p>\n<p><span class=\"math display\">\\[R(x)=\\frac{n^\\prime}n\\log\\frac\nN{N^\\prime+1}\\]</span></p>\n<p>TF-IDF<span class=\"math inline\">\\(n^\\prime\\)</span>  <span class=\"math inline\">\\(x\\)</span> gold-chunk <span class=\"math inline\">\\(n\\)</span> gold-chunktoken<span class=\"math inline\">\\(N\\)</span> chunk<span class=\"math inline\">\\(N^\\prime\\)</span> xchunk</p>\n<p>token <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(R(x)\\)</span>  <span class=\"math inline\">\\(S\\)</span> </p>\n<p><span class=\"math display\">\\[\\mathcal{R}(S)=\\frac{1}{|S_a|}\\sum_{x\\in\\mathcal{S}_a}R(x)\\]</span></p>\n<p> <span class=\"math inline\">\\(S_a\\)</span>  <span class=\"math inline\">\\(S\\)</span> </p>\n<p> <span class=\"math inline\">\\(\\mathcal{R}(S)\\)</span>\n<span class=\"math inline\">\\(\\mathcal{R}(S)\\)</span>\n</p>\n<p>gold-chunkparaphrasing\nthe original text</p>\n<p>paraphrasing</p>\n<img src=\"/cc852861/paraphrasing_example.png\" class title=\"paraphrasing\">\n<p>tokentokenparaphrasing</p>\n<p>GPT-4paraphrasing</p>\n<img src=\"/cc852861/paraphrasing_dataset.png\" class title=\"\">\n<p>10,8258,4548k32k</p>\n<img src=\"/cc852861/paraphrasing_dataset_dist.png\" class title=\"\">\n<p>Multi-passage-QA-from-NQ</p>\n<img src=\"/cc852861/paraphrasing_quality.png\" class title=\"\">\n<p>LongBench</p>\n<img src=\"/cc852861/paraphrasing_perf.png\" class title=\"\">\n<p>lost in the\nmiddle</p>\n<img src=\"/cc852861/paraphrasing_lost.png\" class title=\"lost in the middle\">\n<h1 id=\"pose\">PoSE</h1>\n<p>PoSE: Efficient Context Window Extension of LLMs via Positional\nSkip-wise Training</p>\n<p>20239</p>\n<p></p>\n<p>128k</p>\n<h2 id=\"\"></h2>\n<p>RoPE</p>\n<p>NTKYaRN8k32k128k...</p>\n<p><strong>Po</strong>sitional\n<strong>S</strong>kip-wis<strong>E</strong>PoSE2k128k128k</p>\n<p>RandPosRandomized\npositional encodings boost length generalization of\ntransformersRandPostokenPoSEtoken</p>\n<h2 id=\"\"></h2>\n<p>PoSE<br>\n-\nindex128k1-128k<br>\n-\n</p>\n<p> <span class=\"math inline\">\\(L_c\\)</span> <span class=\"math inline\">\\(N\\)</span> chunk <span class=\"math inline\">\\(c_0,c_1,\\ldots,c_{N-1}\\)</span> <span class=\"math inline\">\\(l_0,l_1,\\ldots,l_{N-1}\\)</span>chunk <span class=\"math inline\">\\(i\\)</span>token</p>\n<p><span class=\"math display\">\\[\\mathrm{Pos}(c_i)=\\{st_i,st_i+1,\\ldots,st_i+l_i-1\\},\\quad\nst_i=\\sum_{j=0}^{i-1}l_j\\]</span></p>\n<p>chunkuniform distribution <span class=\"math inline\">\\(\\mathcal{U}(S)\\)</span> skipping\nbias <span class=\"math inline\">\\(u_i\\)</span>biaschunktoken</p>\n<p><span class=\"math display\">\\[\\mathrm{PoSE}(c_i)=\\{u_i+st_i,u_i+st_i+1,\\ldots,u_i+st_i+l_i-1\\}\\]</span></p>\n<p>chunkoverlap\n<span class=\"math inline\">\\(u_i\\geq u_{i-1}\\)</span></p>\n<p>skipping\nbiassamplechunkskipping\nbias</p>\n<p>indexchunkindex</p>\n<p>chunktoken</p>\n<p>token<span class=\"math inline\">\\(v_i\\sim\\mathcal{U}(\\{v_{i-1},\\ldots,L_x-L_c\\})\\)</span>\n<span class=\"math inline\">\\(c_i\\)</span> token</p>\n<p><span class=\"math display\">\\[c_i=\\boldsymbol{x}[v_i+st_i:v_i+st_i+l_i]\\]</span></p>\n<p> <span class=\"math inline\">\\(v_i=u_i\\)</span><span class=\"math inline\">\\(v_i=0\\)</span>\n <span class=\"math inline\">\\(v_i\\)</span> </p>\n<p><span class=\"math inline\">\\(N\\)</span>\n2 <span class=\"math inline\">\\(u_0\\)</span>  <span class=\"math inline\">\\(v_0\\)</span> 0</p>\n<p>PoSE</p>\n<img src=\"/cc852861/pose_method.png\" class title=\"PoSE\">\n<p>LLAMA-7B2k1,000batch\nsize64lr=2e-5warmup step=10</p>\n<p>PoSEPPLFull-length</p>\n<img src=\"/cc852861/pose_ppl.png\" class title=\"PPL\">\n<p>passkey retrieval</p>\n<img src=\"/cc852861/pose_passkey.png\" class title=\"passkey\">\n<p>PoSEPoSE1M</p>\n<h1 id=\"\"></h1>\n<ol type=\"1\">\n<li>FlashAttention128k5Btoken<br>\n</li>\n<li><br>\n</li>\n<li></li>\n<li>M/PoSE/</li>\n</ol>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Data Engineering for Scaling Language Models to 128K Context\nhttps://arxiv.org/abs/2402.10171<br>\n2Training With \"Paraphrasing the Original Text\" Improves\nLong-Context Performance https://arxiv.org/abs/2312.11193<br>\n3PoSE: Efficient Context Window Extension of LLMs via Positional\nSkip-wise Training https://arxiv.org/abs/2309.10400</p>\n","length":8097,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>2024Q2RAGAgent</p>\n<p>()CoT/ToT</p>\n<p></p>\n<h1 id=\"128k\">128k</h1>\n<p>Data Engineering for Scaling Language Models to 128K\nContext</p>\n<p>20242</p>\n<p></p>\n<p>128k</p>\n<h2 id=\"\"></h2>\n<p>PPL</p>\n<p>validation\ndatasetPPL\nNeedle in a\nHaystack</p>\n<p>PPL</p>\n<img src=\"/cc852861/eng_ppl.png\" class title=\"PPL\">\n<p>LongLoRAMistralYaRN&gt;100kGPT-4</p>\n<img src=\"/cc852861/eng_needle_comp.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p>&lt;=4k128ke.g.\n&lt;5B token</p>\n<p>32k400B\ntokenEffective long-context scaling of foundation\nmodelsXverse</p>\n<p></p>\n<p>LLAMALLAMASlimPajama</p>\n<p>github</p>\n<p><br>\n- Cut at\n4K4k4kLLAMA<br>\n- Cut at\n128K128kLongLoRA<br>\n- Per-source\nUpsampling<br>\n- Global Upsampling<br>\n- Upsample Arxiv/ Book/\nGithub</p>\n<p></p>\n<p>SlimPajama</p>\n<img src=\"/cc852861/eng_data_dist.png\" class title=\"\">\n<p>Per-source\nUpsampling</p>\n<h2 id=\"\"></h2>\n<p>80kLLAMA2-7B64kLLAMA2-13B</p>\n<p>FlashAttentionAttentionGPUCPUGPUGPUconstantlinear80k4k3400</p>\n<p>jiaqian</p>\n<img src=\"/cc852861/add_money.jpg\" class title=\"\">\n<p>Per-source Upsampling</p>\n<img src=\"/cc852861/eng_data.png\" class title=\"\">\n<p></p>\n<img src=\"/cc852861/eng_config.png\" class title=\"\">\n<p><br>\n- lr = 2e-5<br>\n- RoPE base1,0000500,000<br>\n- batch size = 4M token</p>\n<h2 id=\"\"></h2>\n<p>token</p>\n<p>100M300M500M1B5B10B\ntokencheckpointPPL</p>\n<img src=\"/cc852861/eng_tokens.png\" class title=\"\">\n<p>500M\ntoken5B\ntoken10B\ntoken</p>\n<h2 id=\"\"></h2>\n<p>LLAMA2-7B5B\ntoken</p>\n<p>LLAMA24k0-4k4k-128k</p>\n<p></p>\n<img src=\"/cc852861/eng_sample.png\" class title=\"\">\n<p><br>\n- 0-4kPer-source\nUpsampling<br>\n-\nBookGithub<br>\n- 4k-128kPer-source\nUpsampling</p>\n<p>length upsamplingPer-source\nUpsampling</p>\n<p>80kLLAMA2-7BPer-source\nUpsamplingPPLPer-source\nUpsampling</p>\n<img src=\"/cc852861/eng_ppl.png\" class title=\"PPL\">\n<h2 id=\"\"></h2>\n<p><br>\n-\nPPL<br>\n- <br>\n- </p>\n<h1 id=\"paraphrasing\">Paraphrasing</h1>\n<p>Training With \"Paraphrasing the Original Text\" Improves\nLong-Context Performance</p>\n<p>202312</p>\n<p></p>\n<p>50k</p>\n<img src=\"/cc852861/paraphrasing_intro.png\" class title=\"paraphrasing\">\n<h2 id=\"\"></h2>\n<p></p>\n<p>NTKYaRN</p>\n<p>lost in the\nmiddlemiddleretrieval</p>\n<h2 id=\"\"></h2>\n<p><br>\n-\nTogetherLLaMA-2-7B-32Khttps://huggingface.co/datasets/togethercomputer/Long-Data-CollectionsTogetherMultipassage-QA-from-Natural-QuestionsBookSum<br>\n- LongAlpacaLongLoRA: Efficient Fine-tuning of Long-Context Large\nLanguage Models<br>\n- Ziya-ReaderNever Lost in the Middle:Improving Large Language\nModels via Attention Strengthening Question Answering</p>\n<p><br>\n- <br>\n- <br>\n-\n</p>\n<p>promptCoT</p>\n<p>promptClaude-2.1promptHere\nis the most relevant sentence in the\ncontext27%98%https://www.anthropic.com/news/claude-2-1-prompting</p>\n<p><br>\n- LongLLMLinguaLongLLMLingua: Accelerating and Enhancing LLMs in\nLong Context Scenarios via Prompt\nCompression<br>\n- Attention SortingAttention Sorting Combats Recency Bias In Long\nContext Language\nModelsdecode</p>\n<h2 id=\"\"></h2>\n<p>retrieval\nrelevancetokenn-gram <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(R(x)\\)</span> </p>\n<p><span class=\"math display\">\\[R(x)=\\frac{n^\\prime}n\\log\\frac\nN{N^\\prime+1}\\]</span></p>\n<p>TF-IDF<span class=\"math inline\">\\(n^\\prime\\)</span>  <span class=\"math inline\">\\(x\\)</span> gold-chunk <span class=\"math inline\">\\(n\\)</span> gold-chunktoken<span class=\"math inline\">\\(N\\)</span> chunk<span class=\"math inline\">\\(N^\\prime\\)</span> xchunk</p>\n<p>token <span class=\"math inline\">\\(x\\)</span>  <span class=\"math inline\">\\(R(x)\\)</span>  <span class=\"math inline\">\\(S\\)</span> </p>\n<p><span class=\"math display\">\\[\\mathcal{R}(S)=\\frac{1}{|S_a|}\\sum_{x\\in\\mathcal{S}_a}R(x)\\]</span></p>\n<p> <span class=\"math inline\">\\(S_a\\)</span>  <span class=\"math inline\">\\(S\\)</span> </p>\n<p> <span class=\"math inline\">\\(\\mathcal{R}(S)\\)</span>\n<span class=\"math inline\">\\(\\mathcal{R}(S)\\)</span>\n</p>\n<p>gold-chunkparaphrasing\nthe original text</p>\n<p>paraphrasing</p>\n<img src=\"/cc852861/paraphrasing_example.png\" class title=\"paraphrasing\">\n<p>tokentokenparaphrasing</p>\n<p>GPT-4paraphrasing</p>\n<img src=\"/cc852861/paraphrasing_dataset.png\" class title=\"\">\n<p>10,8258,4548k32k</p>\n<img src=\"/cc852861/paraphrasing_dataset_dist.png\" class title=\"\">\n<p>Multi-passage-QA-from-NQ</p>\n<img src=\"/cc852861/paraphrasing_quality.png\" class title=\"\">\n<p>LongBench</p>\n<img src=\"/cc852861/paraphrasing_perf.png\" class title=\"\">\n<p>lost in the\nmiddle</p>\n<img src=\"/cc852861/paraphrasing_lost.png\" class title=\"lost in the middle\">\n<h1 id=\"pose\">PoSE</h1>\n<p>PoSE: Efficient Context Window Extension of LLMs via Positional\nSkip-wise Training</p>\n<p>20239</p>\n<p></p>\n<p>128k</p>\n<h2 id=\"\"></h2>\n<p>RoPE</p>\n<p>NTKYaRN8k32k128k...</p>\n<p><strong>Po</strong>sitional\n<strong>S</strong>kip-wis<strong>E</strong>PoSE2k128k128k</p>\n<p>RandPosRandomized\npositional encodings boost length generalization of\ntransformersRandPostokenPoSEtoken</p>\n<h2 id=\"\"></h2>\n<p>PoSE<br>\n-\nindex128k1-128k<br>\n-\n</p>\n<p> <span class=\"math inline\">\\(L_c\\)</span> <span class=\"math inline\">\\(N\\)</span> chunk <span class=\"math inline\">\\(c_0,c_1,\\ldots,c_{N-1}\\)</span> <span class=\"math inline\">\\(l_0,l_1,\\ldots,l_{N-1}\\)</span>chunk <span class=\"math inline\">\\(i\\)</span>token</p>\n<p><span class=\"math display\">\\[\\mathrm{Pos}(c_i)=\\{st_i,st_i+1,\\ldots,st_i+l_i-1\\},\\quad\nst_i=\\sum_{j=0}^{i-1}l_j\\]</span></p>\n<p>chunkuniform distribution <span class=\"math inline\">\\(\\mathcal{U}(S)\\)</span> skipping\nbias <span class=\"math inline\">\\(u_i\\)</span>biaschunktoken</p>\n<p><span class=\"math display\">\\[\\mathrm{PoSE}(c_i)=\\{u_i+st_i,u_i+st_i+1,\\ldots,u_i+st_i+l_i-1\\}\\]</span></p>\n<p>chunkoverlap\n<span class=\"math inline\">\\(u_i\\geq u_{i-1}\\)</span></p>\n<p>skipping\nbiassamplechunkskipping\nbias</p>\n<p>indexchunkindex</p>\n<p>chunktoken</p>\n<p>token<span class=\"math inline\">\\(v_i\\sim\\mathcal{U}(\\{v_{i-1},\\ldots,L_x-L_c\\})\\)</span>\n<span class=\"math inline\">\\(c_i\\)</span> token</p>\n<p><span class=\"math display\">\\[c_i=\\boldsymbol{x}[v_i+st_i:v_i+st_i+l_i]\\]</span></p>\n<p> <span class=\"math inline\">\\(v_i=u_i\\)</span><span class=\"math inline\">\\(v_i=0\\)</span>\n <span class=\"math inline\">\\(v_i\\)</span> </p>\n<p><span class=\"math inline\">\\(N\\)</span>\n2 <span class=\"math inline\">\\(u_0\\)</span>  <span class=\"math inline\">\\(v_0\\)</span> 0</p>\n<p>PoSE</p>\n<img src=\"/cc852861/pose_method.png\" class title=\"PoSE\">\n<p>LLAMA-7B2k1,000batch\nsize64lr=2e-5warmup step=10</p>\n<p>PoSEPPLFull-length</p>\n<img src=\"/cc852861/pose_ppl.png\" class title=\"PPL\">\n<p>passkey retrieval</p>\n<img src=\"/cc852861/pose_passkey.png\" class title=\"passkey\">\n<p>PoSEPoSE1M</p>\n<h1 id=\"\"></h1>\n<ol type=\"1\">\n<li>FlashAttention128k5Btoken<br>\n</li>\n<li><br>\n</li>\n<li></li>\n<li>M/PoSE/</li>\n</ol>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Data Engineering for Scaling Language Models to 128K Context\nhttps://arxiv.org/abs/2402.10171<br>\n2Training With \"Paraphrasing the Original Text\" Improves\nLong-Context Performance https://arxiv.org/abs/2312.11193<br>\n3PoSE: Efficient Context Window Extension of LLMs via Positional\nSkip-wise Training https://arxiv.org/abs/2309.10400</p>\n"},{"title":"normalization-","abbrlink":"b70b4a2d","date":"2024-04-06T04:24:25.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)normalizationnorm  \n\nnorm  \n\n[https://github.com/Saicat/normalization_exp](https://github.com/Saicat/normalization_exp)\n\n# \n\nnormalizationCNN  \n\n```python\nimport torch\nfrom torch import nn\n\n# epsilon\neps = 1e-8\n\n# \nbatch_size = 3\nfeature_num = 4\ntorch.manual_seed(0)  # \ninputs = torch.randn(batch_size, feature_num)\nprint(':\\n', inputs)\n```\n\n34batch size=34  \n\n```python\n:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\n```\n\n## batchnorm  \n\npytorchBatchNorm1d  \n\n```python\n# torchbatchnorm\ntorch_bn = nn.BatchNorm1d(num_features=feature_num, affine=True)  # batchnorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(1)  # \ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(feature_num))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(feature_num))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_bn(inputs)\nprint('torch bn:\\n', torch_normed)\n```\n\nbatchnorm/layernorm+BatchNorm\"affine\"\"affine\"False  \n\npytorchnorm1.00  \n\n```python\nweight:\n Parameter containing:\ntensor([0.6614, 0.2669, 0.0617, 0.6213], requires_grad=True)\nbias:\n Parameter containing:\ntensor([-0.4519, -0.1661, -1.5228,  0.3817], requires_grad=True) \n```\n\nweightbias4  \n\nbatchnorm  \n\n```python\ntorch bn:\n tensor([[ 0.4756,  0.0513, -1.6033,  0.4715],\n        [-1.0197, -0.5421, -1.4535,  1.0937],\n        [-0.8117, -0.0077, -1.5115, -0.4202]],\n       grad_fn=<NativeBatchNormBackward0>)\n```\n\nbatchnorm  \n\n```python\n# bn\n\n# \nmean = torch.mean(inputs, dim=0, keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=0, keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias\nprint('bn:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(isclose)\n```\n\ndim=0batchsamplefeature    \n\n```python\n:\n tensor([[-0.0876, -0.6985, -0.7907,  0.5295]])\n:\n tensor([[1.1612, 0.4971, 1.0630, 0.2692]]) \n```\n\nmeanstdkeepdimTrueFalsebroadcast  \n\nstdunbiasedFalsetorchbatchnorm  \n\nbatchnorm  \n\n```python\nbn:\n tensor([[ 0.4756,  0.0514, -1.6033,  0.4715],\n        [-1.0197, -0.5421, -1.4535,  1.0937],\n        [-0.8117, -0.0077, -1.5115, -0.4202]], grad_fn=<AddBackward0>)\n```\n\ntorch.isclosebatchnormbatchnorm  \n\n```python\ntensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\nequal1e-5~1e-4eps  \n\n## layernorm  \n\nlayernorm34\n\ntorch\n\n```python\n# torchlayernorm\ntorch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=True)  # layernorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(2)  # \ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(feature_num))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(feature_num))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n```\n\nlayernorm  \n\n```python\nweight:\n Parameter containing:\ntensor([ 0.3923, -0.2236, -0.3195, -1.2050], requires_grad=True)\nbias:\n Parameter containing:\ntensor([ 1.0445, -0.6332,  0.5731,  0.5409], requires_grad=True) \n```\n  \n\nlayernorm  \n\n```python\ntorch ln:\n tensor([[ 1.5120, -0.6001,  1.0604, -0.0392],\n        [ 0.7249, -0.3772,  0.3331, -0.9155],\n        [ 0.6645, -0.6209,  0.7693, -1.4324]],\n       grad_fn=<NativeLayerNormBackward0>)\n```\n\n  \n\ndim=1  \n\n```python\n# ln\n\n# \nmean = torch.mean(inputs, dim=1, keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=1, keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\nprint('ln:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(isclose)\n```\n\n  \n\n```python\n:\n tensor([[-0.0907],\n        [-0.3104],\n        [-0.3843]])\n:\n tensor([[1.3691],\n        [0.9502],\n        [0.3458]]) \n```\n\n  \n\n```python\nln:\n tensor([[ 1.5120, -0.6001,  1.0604, -0.0392],\n        [ 0.7249, -0.3772,  0.3331, -0.9155],\n        [ 0.6645, -0.6209,  0.7693, -1.4325]], grad_fn=<AddBackward0>)\n:\n tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\n##   \n\nbatchnormlayernorm  \n\n{% asset_img bn_and_ln.png bnln %}  \n\nbatchnormdim=0batchlayernormdim=1  \n\nbatchnormlayernorm  \n\n# CV\n\nCV  \n\nCV[N,C,H,W]Nbatch sizeCchannelHWfeature mapCV  \n\n```python\n# [N,C,H,W]\nbatch_size = 2\nchannel = 2\nheight = 2\nwidth = 3\ntorch.manual_seed(3)  # \ninputs = torch.randn(batch_size, channel, height, width)\nprint(':\\n', inputs)\n```\n\n  \n\n```python\n:\n tensor([[[[-0.0766,  0.3599, -0.7820],\n          [ 0.0715,  0.6648, -0.2868]],\n\n         [[ 1.6206, -1.5967,  0.4046],\n          [ 0.6113,  0.7604, -0.0336]]],\n\n\n        [[[-0.3448,  0.4937, -0.0776],\n          [-1.8054,  0.4851,  0.2052]],\n\n         [[ 0.3384,  1.3528,  0.3736],\n          [ 0.0134,  0.7737, -0.1092]]]])\n```\n\n## batchnorm  \n\nBatchNorm2dchannel  \n\n```python\n# torchbatchnorm\ntorch_bn = nn.BatchNorm2d(num_features=channel, affine=True)  # batchnorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(4)  # \ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(channel))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(channel))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_bn(inputs)\nprint('torch bn:\\n', torch_normed)\n```\n\nchannel  \n\n```python\nweight:\n Parameter containing:\ntensor([-1.6053,  0.2325], requires_grad=True)\nbias:\n Parameter containing:\ntensor([2.2399, 0.8473], requires_grad=True) \n```\n\ntorchbatchnorm2d  \n\n```python\ntorch bn:\n tensor([[[[2.2043, 1.1275, 3.9442],\n          [1.8388, 0.3753, 2.7226]],\n\n         [[1.2185, 0.2591, 0.8559],\n          [0.9175, 0.9620, 0.7252]]],\n\n\n        [[[2.8658, 0.7975, 2.2066],\n          [6.4684, 0.8186, 1.5090]],\n\n         [[0.8362, 1.1387, 0.8467],\n          [0.7392, 0.9660, 0.7027]]]], grad_fn=<NativeBatchNormBackward0>)\n```\n\nbatchnorm2d  \n\n```python\n# bn\n\nmanual_normed = []\n# channel\nfor c in range(channel):\n    # \n    mean = torch.mean(inputs[:, c, :, :])\n    std = torch.std(inputs[:, c, :, :], unbiased=False)\n    normed = (inputs[:, c, :, :] - mean) / (std + eps) * torch_bn.weight[c] + torch_bn.bias[c]\n    normed = normed.unsqueeze(1)\n    manual_normed.append(normed)\nmanual_normed = torch.cat(manual_normed, 1)\nprint('bn:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\nCVchannel\"batch\"  \n\nNHW  \n\n{% asset_img cv_batchnorm.png CVbatchnorm %}  \n\n  \n\n```python\nbn:\n tensor([[[[2.2043, 1.1275, 3.9442],\n          [1.8388, 0.3752, 2.7226]],\n\n         [[1.2185, 0.2591, 0.8559],\n          [0.9175, 0.9620, 0.7252]]],\n\n\n        [[[2.8658, 0.7975, 2.2066],\n          [6.4685, 0.8186, 1.5089]],\n\n         [[0.8362, 1.1387, 0.8467],\n          [0.7392, 0.9660, 0.7027]]]], grad_fn=<CatBackward0>)\n:\n tensor([[[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]],\n\n\n        [[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]]])\n```\n\n## layernorm  \n\n[torchlayernorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)layernorm  \n\n```python\n# torchlayernorm\ntorch_ln = nn.LayerNorm(\n    normalized_shape=[channel, height, width], \n    elementwise_affine=True\n)  # layernorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(5)  # \ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(channel, height, width))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(channel, height, width))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n```\n\n\n\n{% asset_img cv_layernorm.jpeg CVlayernorm %}  \n\n[channel, height, width]  \n\n```python\nweight:\n Parameter containing:\ntensor([[[-0.4868, -0.6038, -0.5581],\n         [ 0.6675, -0.1974,  1.9428]],\n\n        [[-1.4017, -0.7626,  0.6312],\n         [-0.8991, -0.5578,  0.6907]]], requires_grad=True)\nbias:\n Parameter containing:\ntensor([[[ 0.2225, -0.6662,  0.6846],\n         [ 0.5740, -0.5829,  0.7679]],\n\n        [[ 0.0571, -1.1894, -0.5659],\n         [-0.8327,  0.9014,  0.2116]]], requires_grad=True) \n```\n\nchannel  \n\nlayernorm  \n\n```python\ntorch ln:\n tensor([[[[ 0.3594, -0.8338,  1.3456],\n          [ 0.5128, -0.7147, -0.3012]],\n\n         [[-2.5939,  0.5089, -0.3546],\n          [-1.3715,  0.4607,  0.0553]]],\n\n\n        [[[ 0.5477, -0.9583,  0.8526],\n          [-1.2112, -0.6760,  0.9378]],\n\n         [[-0.3219, -2.4580, -0.3647],\n          [-0.6744,  0.4171, -0.0264]]]], grad_fn=<NativeLayerNormBackward0>)\n```\n\nlayernorm  \n\n```python\n# ln\n\nmanual_normed = []\n# channel\nfor b in range(batch_size):\n    # \n    mean = torch.mean(inputs[b, :, :, :])\n    std = torch.std(inputs[b, :, :, :], unbiased=False)\n    normed = (inputs[b, :, :, :] - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\n    normed = normed.unsqueeze(0)\n    manual_normed.append(normed)\nmanual_normed = torch.cat(manual_normed, 0)\nprint('ln:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\nchannel  \n\n  \n\n```python\nln:\n tensor([[[[ 0.3594, -0.8338,  1.3456],\n          [ 0.5128, -0.7147, -0.3012]],\n\n         [[-2.5939,  0.5090, -0.3546],\n          [-1.3715,  0.4607,  0.0553]]],\n\n\n        [[[ 0.5477, -0.9583,  0.8527],\n          [-1.2112, -0.6760,  0.9378]],\n\n         [[-0.3219, -2.4581, -0.3647],\n          [-0.6744,  0.4171, -0.0264]]]], grad_fn=<CatBackward0>)\n:\n tensor([[[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]],\n\n\n        [[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]]])\n```\n\n# NLP  \n\nNLP  \n\nNbatch sizeSsequence lengthHhidden size  \n\n```python\n# [N,S,H]\nbatch_size = 2\nseq_len = 3\nhidden_size = 4\ntorch.manual_seed(6)  # \ninputs = torch.randn(batch_size, seq_len, hidden_size)\nprint(':\\n', inputs)\n```\n\n## batchnorm  \n\n  \n\n```python\n# torchbatchnorm\ntorch_bn = nn.BatchNorm1d(num_features=hidden_size, affine=True)  # batchnorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(7)  # \ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(hidden_size))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(hidden_size))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# # \ntorch_normed = torch_bn(inputs.transpose(1, 2)).transpose(1, 2)\nprint('torch bn:\\n', torch_normed)\n```\n\ntransposetranspose  \n\n  \n\n```python\nweight:\n Parameter containing:\ntensor([-0.1468,  0.7861,  0.9468, -1.1143], requires_grad=True)\nbias:\n Parameter containing:\ntensor([ 1.6908, -0.8948, -0.3556,  1.2324], requires_grad=True) \n\ntorch bn:\n tensor([[[ 1.8740, -0.7037, -1.8222,  2.3385],\n         [ 1.7413, -1.8119,  0.3641,  0.0200],\n         [ 1.4615, -0.2676,  0.1081,  1.3450]],\n\n        [[ 1.7084, -1.9653,  1.0169,  0.5785],\n         [ 1.8213, -0.8614, -0.8056,  2.9892],\n         [ 1.5383,  0.2409, -0.9949,  0.1231]]], grad_fn=<TransposeBackward0>)\n```\n\nbatchnorm  \n\n  \n\n```python\n# bn\n\n# \nmean = torch.mean(inputs, dim=(0, 1) , keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=(0, 1), keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias\nprint('bn:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\ndim=(0,1)[N, S, H][NS, H]batchnorm  \n\n  \n\n```python\n:\n tensor([[[-0.2151,  0.5444, -0.2633, -0.5424]]])\n:\n tensor([[[0.7984, 0.3537, 0.7799, 0.7986]]]) \n\nbn:\n tensor([[[ 1.8740, -0.7037, -1.8222,  2.3385],\n         [ 1.7413, -1.8119,  0.3641,  0.0200],\n         [ 1.4615, -0.2676,  0.1081,  1.3450]],\n\n        [[ 1.7084, -1.9653,  1.0169,  0.5785],\n         [ 1.8213, -0.8614, -0.8056,  2.9892],\n         [ 1.5383,  0.2409, -0.9950,  0.1231]]], grad_fn=<AddBackward0>)\n:\n tensor([[[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]],\n\n        [[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]]])\n```\n\n## layernorm  \n\nNLPlayernormhuggingfacebertlayernorm  \n\n```python\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states: torch.normTensor, input_tensor: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n```\n\n  \n\n```python\n# torchlayernorm\ntorch_ln = nn.LayerNorm(normalized_shape=hidden_size, elementwise_affine=True)  # layernorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(8)  # \ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(hidden_size))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(hidden_size))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n```\n\nhidden size  \n\n```python\nweight:\n Parameter containing:\ntensor([ 0.2713, -1.2729,  0.5027,  0.4181], requires_grad=True)\nbias:\n Parameter containing:\ntensor([-0.6394, -0.6608, -0.1433, -0.1043], requires_grad=True) \n\ntorch ln:\n tensor([[[-0.7547, -2.8528, -0.5092, -0.3423],\n         [-1.0957, -0.8780,  0.2388,  0.2097],\n         [-0.3502, -1.6158, -0.3133, -0.7224]],\n\n        [[-0.9134, -0.4490,  0.6868, -0.3029],\n         [-0.7116, -2.5589, -0.1039, -0.6493],\n         [-0.5076, -2.1031, -0.9346, -0.1230]]],\n       grad_fn=<NativeLayerNormBackward0>)\n```\n\n  \n\n```python\n# ln\n\n# \nmean = torch.mean(inputs, dim=2, keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=2, keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\nprint('ln:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\n  \n\n```python\n:\n tensor([[[-0.8469],\n         [ 0.0745],\n         [ 0.3386]],\n\n        [[ 0.1364],\n         [-0.7003],\n         [ 0.2831]]])\n:\n tensor([[[0.8578],\n         [0.3354],\n         [0.6505]],\n\n        [[0.4426],\n         [0.8448],\n         [0.6816]]]) \n```\n\nsampletoken  \n\n  \n\n```python\nln:\n tensor([[[-0.7547, -2.8528, -0.5092, -0.3423],\n         [-1.0957, -0.8780,  0.2388,  0.2097],\n         [-0.3502, -1.6158, -0.3133, -0.7224]],\n\n        [[-0.9134, -0.4490,  0.6868, -0.3029],\n         [-0.7116, -2.5590, -0.1039, -0.6493],\n         [-0.5076, -2.1031, -0.9347, -0.1230]]], grad_fn=<AddBackward0>)\n:\n tensor([[[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]],\n\n        [[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]]])\n```\n\n#   \n\n\n\nbatchnorm  \n\n```python\n# \nbatch_size = 3\nfeature_num = 4\ntorch.manual_seed(0)  # \ninputs = torch.randn(batch_size, feature_num)\nprint(':\\n', inputs)\n\n# \nmean = torch.mean(inputs, dim=0, keepdim=True)\n# print(':\\n', mean)\nstd = torch.std(inputs, dim=0, keepdim=True, unbiased=False)\n# print(':\\n', std, '\\n')\n\n# torchbatchnorm\ntorch_bn = nn.BatchNorm1d(num_features=feature_num, affine=True)\n\n# \ntorch_bn.weight = nn.Parameter(std)\ntorch_bn.bias =  nn.Parameter(mean)\n# print('weight:\\n', torch_bn.weight)\n# print('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_bn(inputs)\nprint('torch bn:\\n', torch_normed)\n\nisclose = torch.isclose(torch_normed, inputs, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\n  \n\n```python\n:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\ntorch bn:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1821]],\n       grad_fn=<NativeBatchNormBackward0>)\n:\n tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\nbatchnorm  \n\nlayernorm  \n\n```python\nprint(':\\n', inputs)\n\n# \nmean = torch.mean(inputs, dim=1, keepdim=True)\n# print(':\\n', mean)\nstd = torch.std(inputs, dim=1, keepdim=True, unbiased=False)\n# print(':\\n', std, '\\n')\n\n# torchlayernorm\ntorch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=True)  # layernorm\n\n# \ntorch_bn.weight = nn.Parameter(std)\ntorch_bn.bias =  nn.Parameter(mean)\n# print('weight:\\n', torch_bn.weight)\n# print('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n\nisclose = torch.isclose(torch_normed, inputs, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\n\n\n```python\n:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\ntorch ln:\n tensor([[ 1.1918, -0.1481, -1.5251,  0.4814],\n        [-0.8146, -1.1451,  0.7512,  1.2086],\n        [-0.9685, -0.0551, -0.6140,  1.6376]],\n       grad_fn=<NativeLayerNormBackward0>)\n:\n tensor([[False, False, False, False],\n        [False, False, False, False],\n        [False, False, False, False]])\n```\n\nlayernormlayernorm  \n\nCVNLP\n\nbatchnormlayernorm  \n\n# \n\nbatchnormlayernorm  \n\nbatchnormlayernorm  \n\nbatchnorm\"\"\"\"batchbatchbatchfeature map  \n\nlayernorm  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***\n\n# Reference  \n1LAYERNORM https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html  \n2BATCHNORM1D https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html  \n3BATCHNORM2D https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html  \n","source":"_posts/cs/nlp/2024/04/normalization-.md","raw":"---\ntitle: normalization-\nabbrlink: b70b4a2d\ndate: 2024-04-06 12:24:25\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - layernorm\n  - normalization\n  - batchnorm\ncategories:\n  - CS\n  - NLP\n  - LLM\n\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***\n\n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)normalizationnorm  \n\nnorm  \n\n[https://github.com/Saicat/normalization_exp](https://github.com/Saicat/normalization_exp)\n\n# \n\nnormalizationCNN  \n\n```python\nimport torch\nfrom torch import nn\n\n# epsilon\neps = 1e-8\n\n# \nbatch_size = 3\nfeature_num = 4\ntorch.manual_seed(0)  # \ninputs = torch.randn(batch_size, feature_num)\nprint(':\\n', inputs)\n```\n\n34batch size=34  \n\n```python\n:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\n```\n\n## batchnorm  \n\npytorchBatchNorm1d  \n\n```python\n# torchbatchnorm\ntorch_bn = nn.BatchNorm1d(num_features=feature_num, affine=True)  # batchnorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(1)  # \ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(feature_num))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(feature_num))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_bn(inputs)\nprint('torch bn:\\n', torch_normed)\n```\n\nbatchnorm/layernorm+BatchNorm\"affine\"\"affine\"False  \n\npytorchnorm1.00  \n\n```python\nweight:\n Parameter containing:\ntensor([0.6614, 0.2669, 0.0617, 0.6213], requires_grad=True)\nbias:\n Parameter containing:\ntensor([-0.4519, -0.1661, -1.5228,  0.3817], requires_grad=True) \n```\n\nweightbias4  \n\nbatchnorm  \n\n```python\ntorch bn:\n tensor([[ 0.4756,  0.0513, -1.6033,  0.4715],\n        [-1.0197, -0.5421, -1.4535,  1.0937],\n        [-0.8117, -0.0077, -1.5115, -0.4202]],\n       grad_fn=<NativeBatchNormBackward0>)\n```\n\nbatchnorm  \n\n```python\n# bn\n\n# \nmean = torch.mean(inputs, dim=0, keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=0, keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias\nprint('bn:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(isclose)\n```\n\ndim=0batchsamplefeature    \n\n```python\n:\n tensor([[-0.0876, -0.6985, -0.7907,  0.5295]])\n:\n tensor([[1.1612, 0.4971, 1.0630, 0.2692]]) \n```\n\nmeanstdkeepdimTrueFalsebroadcast  \n\nstdunbiasedFalsetorchbatchnorm  \n\nbatchnorm  \n\n```python\nbn:\n tensor([[ 0.4756,  0.0514, -1.6033,  0.4715],\n        [-1.0197, -0.5421, -1.4535,  1.0937],\n        [-0.8117, -0.0077, -1.5115, -0.4202]], grad_fn=<AddBackward0>)\n```\n\ntorch.isclosebatchnormbatchnorm  \n\n```python\ntensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\nequal1e-5~1e-4eps  \n\n## layernorm  \n\nlayernorm34\n\ntorch\n\n```python\n# torchlayernorm\ntorch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=True)  # layernorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(2)  # \ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(feature_num))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(feature_num))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n```\n\nlayernorm  \n\n```python\nweight:\n Parameter containing:\ntensor([ 0.3923, -0.2236, -0.3195, -1.2050], requires_grad=True)\nbias:\n Parameter containing:\ntensor([ 1.0445, -0.6332,  0.5731,  0.5409], requires_grad=True) \n```\n  \n\nlayernorm  \n\n```python\ntorch ln:\n tensor([[ 1.5120, -0.6001,  1.0604, -0.0392],\n        [ 0.7249, -0.3772,  0.3331, -0.9155],\n        [ 0.6645, -0.6209,  0.7693, -1.4324]],\n       grad_fn=<NativeLayerNormBackward0>)\n```\n\n  \n\ndim=1  \n\n```python\n# ln\n\n# \nmean = torch.mean(inputs, dim=1, keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=1, keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\nprint('ln:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(isclose)\n```\n\n  \n\n```python\n:\n tensor([[-0.0907],\n        [-0.3104],\n        [-0.3843]])\n:\n tensor([[1.3691],\n        [0.9502],\n        [0.3458]]) \n```\n\n  \n\n```python\nln:\n tensor([[ 1.5120, -0.6001,  1.0604, -0.0392],\n        [ 0.7249, -0.3772,  0.3331, -0.9155],\n        [ 0.6645, -0.6209,  0.7693, -1.4325]], grad_fn=<AddBackward0>)\n:\n tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\n##   \n\nbatchnormlayernorm  \n\n{% asset_img bn_and_ln.png bnln %}  \n\nbatchnormdim=0batchlayernormdim=1  \n\nbatchnormlayernorm  \n\n# CV\n\nCV  \n\nCV[N,C,H,W]Nbatch sizeCchannelHWfeature mapCV  \n\n```python\n# [N,C,H,W]\nbatch_size = 2\nchannel = 2\nheight = 2\nwidth = 3\ntorch.manual_seed(3)  # \ninputs = torch.randn(batch_size, channel, height, width)\nprint(':\\n', inputs)\n```\n\n  \n\n```python\n:\n tensor([[[[-0.0766,  0.3599, -0.7820],\n          [ 0.0715,  0.6648, -0.2868]],\n\n         [[ 1.6206, -1.5967,  0.4046],\n          [ 0.6113,  0.7604, -0.0336]]],\n\n\n        [[[-0.3448,  0.4937, -0.0776],\n          [-1.8054,  0.4851,  0.2052]],\n\n         [[ 0.3384,  1.3528,  0.3736],\n          [ 0.0134,  0.7737, -0.1092]]]])\n```\n\n## batchnorm  \n\nBatchNorm2dchannel  \n\n```python\n# torchbatchnorm\ntorch_bn = nn.BatchNorm2d(num_features=channel, affine=True)  # batchnorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(4)  # \ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(channel))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(channel))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_bn(inputs)\nprint('torch bn:\\n', torch_normed)\n```\n\nchannel  \n\n```python\nweight:\n Parameter containing:\ntensor([-1.6053,  0.2325], requires_grad=True)\nbias:\n Parameter containing:\ntensor([2.2399, 0.8473], requires_grad=True) \n```\n\ntorchbatchnorm2d  \n\n```python\ntorch bn:\n tensor([[[[2.2043, 1.1275, 3.9442],\n          [1.8388, 0.3753, 2.7226]],\n\n         [[1.2185, 0.2591, 0.8559],\n          [0.9175, 0.9620, 0.7252]]],\n\n\n        [[[2.8658, 0.7975, 2.2066],\n          [6.4684, 0.8186, 1.5090]],\n\n         [[0.8362, 1.1387, 0.8467],\n          [0.7392, 0.9660, 0.7027]]]], grad_fn=<NativeBatchNormBackward0>)\n```\n\nbatchnorm2d  \n\n```python\n# bn\n\nmanual_normed = []\n# channel\nfor c in range(channel):\n    # \n    mean = torch.mean(inputs[:, c, :, :])\n    std = torch.std(inputs[:, c, :, :], unbiased=False)\n    normed = (inputs[:, c, :, :] - mean) / (std + eps) * torch_bn.weight[c] + torch_bn.bias[c]\n    normed = normed.unsqueeze(1)\n    manual_normed.append(normed)\nmanual_normed = torch.cat(manual_normed, 1)\nprint('bn:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\nCVchannel\"batch\"  \n\nNHW  \n\n{% asset_img cv_batchnorm.png CVbatchnorm %}  \n\n  \n\n```python\nbn:\n tensor([[[[2.2043, 1.1275, 3.9442],\n          [1.8388, 0.3752, 2.7226]],\n\n         [[1.2185, 0.2591, 0.8559],\n          [0.9175, 0.9620, 0.7252]]],\n\n\n        [[[2.8658, 0.7975, 2.2066],\n          [6.4685, 0.8186, 1.5089]],\n\n         [[0.8362, 1.1387, 0.8467],\n          [0.7392, 0.9660, 0.7027]]]], grad_fn=<CatBackward0>)\n:\n tensor([[[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]],\n\n\n        [[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]]])\n```\n\n## layernorm  \n\n[torchlayernorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)layernorm  \n\n```python\n# torchlayernorm\ntorch_ln = nn.LayerNorm(\n    normalized_shape=[channel, height, width], \n    elementwise_affine=True\n)  # layernorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(5)  # \ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(channel, height, width))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(channel, height, width))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n```\n\n\n\n{% asset_img cv_layernorm.jpeg CVlayernorm %}  \n\n[channel, height, width]  \n\n```python\nweight:\n Parameter containing:\ntensor([[[-0.4868, -0.6038, -0.5581],\n         [ 0.6675, -0.1974,  1.9428]],\n\n        [[-1.4017, -0.7626,  0.6312],\n         [-0.8991, -0.5578,  0.6907]]], requires_grad=True)\nbias:\n Parameter containing:\ntensor([[[ 0.2225, -0.6662,  0.6846],\n         [ 0.5740, -0.5829,  0.7679]],\n\n        [[ 0.0571, -1.1894, -0.5659],\n         [-0.8327,  0.9014,  0.2116]]], requires_grad=True) \n```\n\nchannel  \n\nlayernorm  \n\n```python\ntorch ln:\n tensor([[[[ 0.3594, -0.8338,  1.3456],\n          [ 0.5128, -0.7147, -0.3012]],\n\n         [[-2.5939,  0.5089, -0.3546],\n          [-1.3715,  0.4607,  0.0553]]],\n\n\n        [[[ 0.5477, -0.9583,  0.8526],\n          [-1.2112, -0.6760,  0.9378]],\n\n         [[-0.3219, -2.4580, -0.3647],\n          [-0.6744,  0.4171, -0.0264]]]], grad_fn=<NativeLayerNormBackward0>)\n```\n\nlayernorm  \n\n```python\n# ln\n\nmanual_normed = []\n# channel\nfor b in range(batch_size):\n    # \n    mean = torch.mean(inputs[b, :, :, :])\n    std = torch.std(inputs[b, :, :, :], unbiased=False)\n    normed = (inputs[b, :, :, :] - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\n    normed = normed.unsqueeze(0)\n    manual_normed.append(normed)\nmanual_normed = torch.cat(manual_normed, 0)\nprint('ln:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\nchannel  \n\n  \n\n```python\nln:\n tensor([[[[ 0.3594, -0.8338,  1.3456],\n          [ 0.5128, -0.7147, -0.3012]],\n\n         [[-2.5939,  0.5090, -0.3546],\n          [-1.3715,  0.4607,  0.0553]]],\n\n\n        [[[ 0.5477, -0.9583,  0.8527],\n          [-1.2112, -0.6760,  0.9378]],\n\n         [[-0.3219, -2.4581, -0.3647],\n          [-0.6744,  0.4171, -0.0264]]]], grad_fn=<CatBackward0>)\n:\n tensor([[[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]],\n\n\n        [[[True, True, True],\n          [True, True, True]],\n\n         [[True, True, True],\n          [True, True, True]]]])\n```\n\n# NLP  \n\nNLP  \n\nNbatch sizeSsequence lengthHhidden size  \n\n```python\n# [N,S,H]\nbatch_size = 2\nseq_len = 3\nhidden_size = 4\ntorch.manual_seed(6)  # \ninputs = torch.randn(batch_size, seq_len, hidden_size)\nprint(':\\n', inputs)\n```\n\n## batchnorm  \n\n  \n\n```python\n# torchbatchnorm\ntorch_bn = nn.BatchNorm1d(num_features=hidden_size, affine=True)  # batchnorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(7)  # \ntorch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(hidden_size))\ntorch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(hidden_size))\nprint('weight:\\n', torch_bn.weight)\nprint('bias:\\n', torch_bn.bias, '\\n')\n\n# # \ntorch_normed = torch_bn(inputs.transpose(1, 2)).transpose(1, 2)\nprint('torch bn:\\n', torch_normed)\n```\n\ntransposetranspose  \n\n  \n\n```python\nweight:\n Parameter containing:\ntensor([-0.1468,  0.7861,  0.9468, -1.1143], requires_grad=True)\nbias:\n Parameter containing:\ntensor([ 1.6908, -0.8948, -0.3556,  1.2324], requires_grad=True) \n\ntorch bn:\n tensor([[[ 1.8740, -0.7037, -1.8222,  2.3385],\n         [ 1.7413, -1.8119,  0.3641,  0.0200],\n         [ 1.4615, -0.2676,  0.1081,  1.3450]],\n\n        [[ 1.7084, -1.9653,  1.0169,  0.5785],\n         [ 1.8213, -0.8614, -0.8056,  2.9892],\n         [ 1.5383,  0.2409, -0.9949,  0.1231]]], grad_fn=<TransposeBackward0>)\n```\n\nbatchnorm  \n\n  \n\n```python\n# bn\n\n# \nmean = torch.mean(inputs, dim=(0, 1) , keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=(0, 1), keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias\nprint('bn:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\ndim=(0,1)[N, S, H][NS, H]batchnorm  \n\n  \n\n```python\n:\n tensor([[[-0.2151,  0.5444, -0.2633, -0.5424]]])\n:\n tensor([[[0.7984, 0.3537, 0.7799, 0.7986]]]) \n\nbn:\n tensor([[[ 1.8740, -0.7037, -1.8222,  2.3385],\n         [ 1.7413, -1.8119,  0.3641,  0.0200],\n         [ 1.4615, -0.2676,  0.1081,  1.3450]],\n\n        [[ 1.7084, -1.9653,  1.0169,  0.5785],\n         [ 1.8213, -0.8614, -0.8056,  2.9892],\n         [ 1.5383,  0.2409, -0.9950,  0.1231]]], grad_fn=<AddBackward0>)\n:\n tensor([[[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]],\n\n        [[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]]])\n```\n\n## layernorm  \n\nNLPlayernormhuggingfacebertlayernorm  \n\n```python\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states: torch.normTensor, input_tensor: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n```\n\n  \n\n```python\n# torchlayernorm\ntorch_ln = nn.LayerNorm(normalized_shape=hidden_size, elementwise_affine=True)  # layernorm\n\n# weigh=1bias=0\n# \ntorch.manual_seed(8)  # \ntorch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(hidden_size))\ntorch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(hidden_size))\nprint('weight:\\n', torch_ln.weight)\nprint('bias:\\n', torch_ln.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n```\n\nhidden size  \n\n```python\nweight:\n Parameter containing:\ntensor([ 0.2713, -1.2729,  0.5027,  0.4181], requires_grad=True)\nbias:\n Parameter containing:\ntensor([-0.6394, -0.6608, -0.1433, -0.1043], requires_grad=True) \n\ntorch ln:\n tensor([[[-0.7547, -2.8528, -0.5092, -0.3423],\n         [-1.0957, -0.8780,  0.2388,  0.2097],\n         [-0.3502, -1.6158, -0.3133, -0.7224]],\n\n        [[-0.9134, -0.4490,  0.6868, -0.3029],\n         [-0.7116, -2.5589, -0.1039, -0.6493],\n         [-0.5076, -2.1031, -0.9346, -0.1230]]],\n       grad_fn=<NativeLayerNormBackward0>)\n```\n\n  \n\n```python\n# ln\n\n# \nmean = torch.mean(inputs, dim=2, keepdim=True)\nprint(':\\n', mean)\nstd = torch.std(inputs, dim=2, keepdim=True, unbiased=False)\nprint(':\\n', std, '\\n')\n\nmanual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias\nprint('ln:\\n', manual_normed)\n\n# torch<1e-4\nisclose = torch.isclose(torch_normed, manual_normed, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\n  \n\n```python\n:\n tensor([[[-0.8469],\n         [ 0.0745],\n         [ 0.3386]],\n\n        [[ 0.1364],\n         [-0.7003],\n         [ 0.2831]]])\n:\n tensor([[[0.8578],\n         [0.3354],\n         [0.6505]],\n\n        [[0.4426],\n         [0.8448],\n         [0.6816]]]) \n```\n\nsampletoken  \n\n  \n\n```python\nln:\n tensor([[[-0.7547, -2.8528, -0.5092, -0.3423],\n         [-1.0957, -0.8780,  0.2388,  0.2097],\n         [-0.3502, -1.6158, -0.3133, -0.7224]],\n\n        [[-0.9134, -0.4490,  0.6868, -0.3029],\n         [-0.7116, -2.5590, -0.1039, -0.6493],\n         [-0.5076, -2.1031, -0.9347, -0.1230]]], grad_fn=<AddBackward0>)\n:\n tensor([[[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]],\n\n        [[True, True, True, True],\n         [True, True, True, True],\n         [True, True, True, True]]])\n```\n\n#   \n\n\n\nbatchnorm  \n\n```python\n# \nbatch_size = 3\nfeature_num = 4\ntorch.manual_seed(0)  # \ninputs = torch.randn(batch_size, feature_num)\nprint(':\\n', inputs)\n\n# \nmean = torch.mean(inputs, dim=0, keepdim=True)\n# print(':\\n', mean)\nstd = torch.std(inputs, dim=0, keepdim=True, unbiased=False)\n# print(':\\n', std, '\\n')\n\n# torchbatchnorm\ntorch_bn = nn.BatchNorm1d(num_features=feature_num, affine=True)\n\n# \ntorch_bn.weight = nn.Parameter(std)\ntorch_bn.bias =  nn.Parameter(mean)\n# print('weight:\\n', torch_bn.weight)\n# print('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_bn(inputs)\nprint('torch bn:\\n', torch_normed)\n\nisclose = torch.isclose(torch_normed, inputs, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\n  \n\n```python\n:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\ntorch bn:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1821]],\n       grad_fn=<NativeBatchNormBackward0>)\n:\n tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n```\n\nbatchnorm  \n\nlayernorm  \n\n```python\nprint(':\\n', inputs)\n\n# \nmean = torch.mean(inputs, dim=1, keepdim=True)\n# print(':\\n', mean)\nstd = torch.std(inputs, dim=1, keepdim=True, unbiased=False)\n# print(':\\n', std, '\\n')\n\n# torchlayernorm\ntorch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=True)  # layernorm\n\n# \ntorch_bn.weight = nn.Parameter(std)\ntorch_bn.bias =  nn.Parameter(mean)\n# print('weight:\\n', torch_bn.weight)\n# print('bias:\\n', torch_bn.bias, '\\n')\n\n# \ntorch_normed = torch_ln(inputs)\nprint('torch ln:\\n', torch_normed)\n\nisclose = torch.isclose(torch_normed, inputs, rtol=1e-4, atol=1e-4)\nprint(':\\n', isclose)\n```\n\n\n\n```python\n:\n tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n        [-1.0845, -1.3986,  0.4033,  0.8380],\n        [-0.7193, -0.4033, -0.5966,  0.1820]])\ntorch ln:\n tensor([[ 1.1918, -0.1481, -1.5251,  0.4814],\n        [-0.8146, -1.1451,  0.7512,  1.2086],\n        [-0.9685, -0.0551, -0.6140,  1.6376]],\n       grad_fn=<NativeLayerNormBackward0>)\n:\n tensor([[False, False, False, False],\n        [False, False, False, False],\n        [False, False, False, False]])\n```\n\nlayernormlayernorm  \n\nCVNLP\n\nbatchnormlayernorm  \n\n# \n\nbatchnormlayernorm  \n\nbatchnormlayernorm  \n\nbatchnorm\"\"\"\"batchbatchbatchfeature map  \n\nlayernorm  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n\n***\n\n# Reference  \n1LAYERNORM https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html  \n2BATCHNORM1D https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html  \n3BATCHNORM2D https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html  \n","slug":"cs/nlp/2024/04/normalization-","published":1,"updated":"2024-05-10T06:51:09.591Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmy001u0p4kbhyla7a5","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a>normalizationnorm</p>\n<p>norm</p>\n<p><a href=\"https://github.com/Saicat/normalization_exp\">https://github.com/Saicat/normalization_exp</a></p>\n<h1 id=\"\"></h1>\n<p>normalizationCNN</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># epsilon</span></span><br><span class=\"line\">eps = <span class=\"number\">1e-8</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">batch_size = <span class=\"number\">3</span></span><br><span class=\"line\">feature_num = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">0</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, feature_num)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<p>34batch\nsize=34</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm\">batchnorm</h2>\n<p>pytorchBatchNorm1d</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=feature_num, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># batchnorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">1</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(feature_num))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(feature_num))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>batchnorm/layernorm+BatchNorm\"affine\"\"affine\"False</p>\n<p>pytorchnorm1.00</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([<span class=\"number\">0.6614</span>, <span class=\"number\">0.2669</span>, <span class=\"number\">0.0617</span>, <span class=\"number\">0.6213</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.4519</span>, -<span class=\"number\">0.1661</span>, -<span class=\"number\">1.5228</span>,  <span class=\"number\">0.3817</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>weightbias4</p>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">0.4756</span>,  <span class=\"number\">0.0513</span>, -<span class=\"number\">1.6033</span>,  <span class=\"number\">0.4715</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0197</span>, -<span class=\"number\">0.5421</span>, -<span class=\"number\">1.4535</span>,  <span class=\"number\">1.0937</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8117</span>, -<span class=\"number\">0.0077</span>, -<span class=\"number\">1.5115</span>, -<span class=\"number\">0.4202</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># bn</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bn:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(isclose)</span><br></pre></td></tr></table></figure>\n<p>dim=0batchsamplefeature</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[-<span class=\"number\">0.0876</span>, -<span class=\"number\">0.6985</span>, -<span class=\"number\">0.7907</span>,  <span class=\"number\">0.5295</span>]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"number\">1.1612</span>, <span class=\"number\">0.4971</span>, <span class=\"number\">1.0630</span>, <span class=\"number\">0.2692</span>]]) </span><br></pre></td></tr></table></figure>\n<p>meanstdkeepdimTrueFalsebroadcast</p>\n<p>stdunbiasedFalsetorchbatchnorm</p>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bn:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">0.4756</span>,  <span class=\"number\">0.0514</span>, -<span class=\"number\">1.6033</span>,  <span class=\"number\">0.4715</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0197</span>, -<span class=\"number\">0.5421</span>, -<span class=\"number\">1.4535</span>,  <span class=\"number\">1.0937</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8117</span>, -<span class=\"number\">0.0077</span>, -<span class=\"number\">1.5115</span>, -<span class=\"number\">0.4202</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>torch.isclosebatchnormbatchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<p>equal1e-5~1e-4eps</p>\n<h2 id=\"layernorm\">layernorm</h2>\n<p>layernorm34</p>\n<p>torch</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">2</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(feature_num))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(feature_num))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">0.3923</span>, -<span class=\"number\">0.2236</span>, -<span class=\"number\">0.3195</span>, -<span class=\"number\">1.2050</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">1.0445</span>, -<span class=\"number\">0.6332</span>,  <span class=\"number\">0.5731</span>,  <span class=\"number\">0.5409</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p></p>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5120</span>, -<span class=\"number\">0.6001</span>,  <span class=\"number\">1.0604</span>, -<span class=\"number\">0.0392</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.7249</span>, -<span class=\"number\">0.3772</span>,  <span class=\"number\">0.3331</span>, -<span class=\"number\">0.9155</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.6645</span>, -<span class=\"number\">0.6209</span>,  <span class=\"number\">0.7693</span>, -<span class=\"number\">1.4324</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p></p>\n<p>dim=1</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ln</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;ln:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[-<span class=\"number\">0.0907</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.3104</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.3843</span>]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"number\">1.3691</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.9502</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.3458</span>]]) </span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5120</span>, -<span class=\"number\">0.6001</span>,  <span class=\"number\">1.0604</span>, -<span class=\"number\">0.0392</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.7249</span>, -<span class=\"number\">0.3772</span>,  <span class=\"number\">0.3331</span>, -<span class=\"number\">0.9155</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.6645</span>, -<span class=\"number\">0.6209</span>,  <span class=\"number\">0.7693</span>, -<span class=\"number\">1.4325</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"\"></h2>\n<p>batchnormlayernorm</p>\n<img src=\"/b70b4a2d/bn_and_ln.png\" class title=\"bnln\">\n<p>batchnormdim=0batchlayernormdim=1</p>\n<p>batchnormlayernorm</p>\n<h1 id=\"cv\">CV</h1>\n<p>CV</p>\n<p>CV[N,C,H,W]Nbatch\nsizeCchannelHWfeature\nmapCV</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># [N,C,H,W]</span></span><br><span class=\"line\">batch_size = <span class=\"number\">2</span></span><br><span class=\"line\">channel = <span class=\"number\">2</span></span><br><span class=\"line\">height = <span class=\"number\">2</span></span><br><span class=\"line\">width = <span class=\"number\">3</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">3</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, channel, height, width)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[[-<span class=\"number\">0.0766</span>,  <span class=\"number\">0.3599</span>, -<span class=\"number\">0.7820</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.0715</span>,  <span class=\"number\">0.6648</span>, -<span class=\"number\">0.2868</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[ <span class=\"number\">1.6206</span>, -<span class=\"number\">1.5967</span>,  <span class=\"number\">0.4046</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.6113</span>,  <span class=\"number\">0.7604</span>, -<span class=\"number\">0.0336</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[-<span class=\"number\">0.3448</span>,  <span class=\"number\">0.4937</span>, -<span class=\"number\">0.0776</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.8054</span>,  <span class=\"number\">0.4851</span>,  <span class=\"number\">0.2052</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[ <span class=\"number\">0.3384</span>,  <span class=\"number\">1.3528</span>,  <span class=\"number\">0.3736</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.0134</span>,  <span class=\"number\">0.7737</span>, -<span class=\"number\">0.1092</span>]]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm-1\">batchnorm</h2>\n<p>BatchNorm2dchannel</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm2d(num_features=channel, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># batchnorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">4</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(channel))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(channel))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>channel</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">1.6053</span>,  <span class=\"number\">0.2325</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([<span class=\"number\">2.2399</span>, <span class=\"number\">0.8473</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>torchbatchnorm2d</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[[[<span class=\"number\">2.2043</span>, <span class=\"number\">1.1275</span>, <span class=\"number\">3.9442</span>],</span><br><span class=\"line\">          [<span class=\"number\">1.8388</span>, <span class=\"number\">0.3753</span>, <span class=\"number\">2.7226</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">1.2185</span>, <span class=\"number\">0.2591</span>, <span class=\"number\">0.8559</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.9175</span>, <span class=\"number\">0.9620</span>, <span class=\"number\">0.7252</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"number\">2.8658</span>, <span class=\"number\">0.7975</span>, <span class=\"number\">2.2066</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.4684</span>, <span class=\"number\">0.8186</span>, <span class=\"number\">1.5090</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">0.8362</span>, <span class=\"number\">1.1387</span>, <span class=\"number\">0.8467</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.7392</span>, <span class=\"number\">0.9660</span>, <span class=\"number\">0.7027</span>]]]], grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>batchnorm2d</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># bn</span></span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = []</span><br><span class=\"line\"><span class=\"comment\"># channel</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(channel):</span><br><span class=\"line\">    <span class=\"comment\"># </span></span><br><span class=\"line\">    mean = torch.mean(inputs[:, c, :, :])</span><br><span class=\"line\">    std = torch.std(inputs[:, c, :, :], unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    normed = (inputs[:, c, :, :] - mean) / (std + eps) * torch_bn.weight[c] + torch_bn.bias[c]</span><br><span class=\"line\">    normed = normed.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">    manual_normed.append(normed)</span><br><span class=\"line\">manual_normed = torch.cat(manual_normed, <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bn:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>CVchannel\"batch\"</p>\n<p>NHW</p>\n<img src=\"/b70b4a2d/cv_batchnorm.png\" class title=\"CVbatchnorm\">\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bn:</span><br><span class=\"line\"> tensor([[[[<span class=\"number\">2.2043</span>, <span class=\"number\">1.1275</span>, <span class=\"number\">3.9442</span>],</span><br><span class=\"line\">          [<span class=\"number\">1.8388</span>, <span class=\"number\">0.3752</span>, <span class=\"number\">2.7226</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">1.2185</span>, <span class=\"number\">0.2591</span>, <span class=\"number\">0.8559</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.9175</span>, <span class=\"number\">0.9620</span>, <span class=\"number\">0.7252</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"number\">2.8658</span>, <span class=\"number\">0.7975</span>, <span class=\"number\">2.2066</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.4685</span>, <span class=\"number\">0.8186</span>, <span class=\"number\">1.5089</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">0.8362</span>, <span class=\"number\">1.1387</span>, <span class=\"number\">0.8467</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.7392</span>, <span class=\"number\">0.9660</span>, <span class=\"number\">0.7027</span>]]]], grad_fn=&lt;CatBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"layernorm-1\">layernorm</h2>\n<p><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\">torchlayernorm</a>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(</span><br><span class=\"line\">    normalized_shape=[channel, height, width], </span><br><span class=\"line\">    elementwise_affine=<span class=\"literal\">True</span></span><br><span class=\"line\">)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">5</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(channel, height, width))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(channel, height, width))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p></p>\n<img src=\"/b70b4a2d/cv_layernorm.jpeg\" class title=\"CVlayernorm\">\n<p>[channel, height, width]</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([[[-<span class=\"number\">0.4868</span>, -<span class=\"number\">0.6038</span>, -<span class=\"number\">0.5581</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.6675</span>, -<span class=\"number\">0.1974</span>,  <span class=\"number\">1.9428</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">1.4017</span>, -<span class=\"number\">0.7626</span>,  <span class=\"number\">0.6312</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.8991</span>, -<span class=\"number\">0.5578</span>,  <span class=\"number\">0.6907</span>]]], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([[[ <span class=\"number\">0.2225</span>, -<span class=\"number\">0.6662</span>,  <span class=\"number\">0.6846</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.5740</span>, -<span class=\"number\">0.5829</span>,  <span class=\"number\">0.7679</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">0.0571</span>, -<span class=\"number\">1.1894</span>, -<span class=\"number\">0.5659</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.8327</span>,  <span class=\"number\">0.9014</span>,  <span class=\"number\">0.2116</span>]]], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>channel</p>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[[[ <span class=\"number\">0.3594</span>, -<span class=\"number\">0.8338</span>,  <span class=\"number\">1.3456</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.5128</span>, -<span class=\"number\">0.7147</span>, -<span class=\"number\">0.3012</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">2.5939</span>,  <span class=\"number\">0.5089</span>, -<span class=\"number\">0.3546</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.3715</span>,  <span class=\"number\">0.4607</span>,  <span class=\"number\">0.0553</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[ <span class=\"number\">0.5477</span>, -<span class=\"number\">0.9583</span>,  <span class=\"number\">0.8526</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.2112</span>, -<span class=\"number\">0.6760</span>,  <span class=\"number\">0.9378</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">0.3219</span>, -<span class=\"number\">2.4580</span>, -<span class=\"number\">0.3647</span>],</span><br><span class=\"line\">          [-<span class=\"number\">0.6744</span>,  <span class=\"number\">0.4171</span>, -<span class=\"number\">0.0264</span>]]]], grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ln</span></span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = []</span><br><span class=\"line\"><span class=\"comment\"># channel</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> b <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(batch_size):</span><br><span class=\"line\">    <span class=\"comment\"># </span></span><br><span class=\"line\">    mean = torch.mean(inputs[b, :, :, :])</span><br><span class=\"line\">    std = torch.std(inputs[b, :, :, :], unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    normed = (inputs[b, :, :, :] - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\">    normed = normed.unsqueeze(<span class=\"number\">0</span>)</span><br><span class=\"line\">    manual_normed.append(normed)</span><br><span class=\"line\">manual_normed = torch.cat(manual_normed, <span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;ln:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>channel</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln:</span><br><span class=\"line\"> tensor([[[[ <span class=\"number\">0.3594</span>, -<span class=\"number\">0.8338</span>,  <span class=\"number\">1.3456</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.5128</span>, -<span class=\"number\">0.7147</span>, -<span class=\"number\">0.3012</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">2.5939</span>,  <span class=\"number\">0.5090</span>, -<span class=\"number\">0.3546</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.3715</span>,  <span class=\"number\">0.4607</span>,  <span class=\"number\">0.0553</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[ <span class=\"number\">0.5477</span>, -<span class=\"number\">0.9583</span>,  <span class=\"number\">0.8527</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.2112</span>, -<span class=\"number\">0.6760</span>,  <span class=\"number\">0.9378</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">0.3219</span>, -<span class=\"number\">2.4581</span>, -<span class=\"number\">0.3647</span>],</span><br><span class=\"line\">          [-<span class=\"number\">0.6744</span>,  <span class=\"number\">0.4171</span>, -<span class=\"number\">0.0264</span>]]]], grad_fn=&lt;CatBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]]])</span><br></pre></td></tr></table></figure>\n<h1 id=\"nlp\">NLP</h1>\n<p>NLP</p>\n<p>Nbatch sizeSsequence lengthHhidden size</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># [N,S,H]</span></span><br><span class=\"line\">batch_size = <span class=\"number\">2</span></span><br><span class=\"line\">seq_len = <span class=\"number\">3</span></span><br><span class=\"line\">hidden_size = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">6</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, seq_len, hidden_size)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm-2\">batchnorm</h2>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=hidden_size, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># batchnorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">7</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(hidden_size))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(hidden_size))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># # </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>transposetranspose</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.1468</span>,  <span class=\"number\">0.7861</span>,  <span class=\"number\">0.9468</span>, -<span class=\"number\">1.1143</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">1.6908</span>, -<span class=\"number\">0.8948</span>, -<span class=\"number\">0.3556</span>,  <span class=\"number\">1.2324</span>], requires_grad=<span class=\"literal\">True</span>) </span><br><span class=\"line\"></span><br><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[[ <span class=\"number\">1.8740</span>, -<span class=\"number\">0.7037</span>, -<span class=\"number\">1.8222</span>,  <span class=\"number\">2.3385</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.7413</span>, -<span class=\"number\">1.8119</span>,  <span class=\"number\">0.3641</span>,  <span class=\"number\">0.0200</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.4615</span>, -<span class=\"number\">0.2676</span>,  <span class=\"number\">0.1081</span>,  <span class=\"number\">1.3450</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">1.7084</span>, -<span class=\"number\">1.9653</span>,  <span class=\"number\">1.0169</span>,  <span class=\"number\">0.5785</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.8213</span>, -<span class=\"number\">0.8614</span>, -<span class=\"number\">0.8056</span>,  <span class=\"number\">2.9892</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.5383</span>,  <span class=\"number\">0.2409</span>, -<span class=\"number\">0.9949</span>,  <span class=\"number\">0.1231</span>]]], grad_fn=&lt;TransposeBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>batchnorm</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># bn</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=(<span class=\"number\">0</span>, <span class=\"number\">1</span>) , keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=(<span class=\"number\">0</span>, <span class=\"number\">1</span>), keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bn:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>dim=(0,1)[N,\nS, H][NS,\nH]batchnorm</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.2151</span>,  <span class=\"number\">0.5444</span>, -<span class=\"number\">0.2633</span>, -<span class=\"number\">0.5424</span>]]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"number\">0.7984</span>, <span class=\"number\">0.3537</span>, <span class=\"number\">0.7799</span>, <span class=\"number\">0.7986</span>]]]) </span><br><span class=\"line\"></span><br><span class=\"line\">bn:</span><br><span class=\"line\"> tensor([[[ <span class=\"number\">1.8740</span>, -<span class=\"number\">0.7037</span>, -<span class=\"number\">1.8222</span>,  <span class=\"number\">2.3385</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.7413</span>, -<span class=\"number\">1.8119</span>,  <span class=\"number\">0.3641</span>,  <span class=\"number\">0.0200</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.4615</span>, -<span class=\"number\">0.2676</span>,  <span class=\"number\">0.1081</span>,  <span class=\"number\">1.3450</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">1.7084</span>, -<span class=\"number\">1.9653</span>,  <span class=\"number\">1.0169</span>,  <span class=\"number\">0.5785</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.8213</span>, -<span class=\"number\">0.8614</span>, -<span class=\"number\">0.8056</span>,  <span class=\"number\">2.9892</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.5383</span>,  <span class=\"number\">0.2409</span>, -<span class=\"number\">0.9950</span>,  <span class=\"number\">0.1231</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"layernorm-2\">layernorm</h2>\n<p>NLPlayernormhuggingfacebertlayernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BertSelfOutput</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, config</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class=\"line\">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class=\"line\">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, hidden_states: torch.normTensor, input_tensor: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class=\"line\">        hidden_states = self.dense(hidden_states)</span><br><span class=\"line\">        hidden_states = self.dropout(hidden_states)</span><br><span class=\"line\">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> hidden_states</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=hidden_size, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">8</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(hidden_size))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(hidden_size))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>hidden size</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">0.2713</span>, -<span class=\"number\">1.2729</span>,  <span class=\"number\">0.5027</span>,  <span class=\"number\">0.4181</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.6394</span>, -<span class=\"number\">0.6608</span>, -<span class=\"number\">0.1433</span>, -<span class=\"number\">0.1043</span>], requires_grad=<span class=\"literal\">True</span>) </span><br><span class=\"line\"></span><br><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.7547</span>, -<span class=\"number\">2.8528</span>, -<span class=\"number\">0.5092</span>, -<span class=\"number\">0.3423</span>],</span><br><span class=\"line\">         [-<span class=\"number\">1.0957</span>, -<span class=\"number\">0.8780</span>,  <span class=\"number\">0.2388</span>,  <span class=\"number\">0.2097</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.3502</span>, -<span class=\"number\">1.6158</span>, -<span class=\"number\">0.3133</span>, -<span class=\"number\">0.7224</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">0.9134</span>, -<span class=\"number\">0.4490</span>,  <span class=\"number\">0.6868</span>, -<span class=\"number\">0.3029</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7116</span>, -<span class=\"number\">2.5589</span>, -<span class=\"number\">0.1039</span>, -<span class=\"number\">0.6493</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.5076</span>, -<span class=\"number\">2.1031</span>, -<span class=\"number\">0.9346</span>, -<span class=\"number\">0.1230</span>]]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ln</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">2</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">2</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;ln:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.8469</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.0745</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.3386</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">0.1364</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7003</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.2831</span>]]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"number\">0.8578</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.3354</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.6505</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"number\">0.4426</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.8448</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.6816</span>]]]) </span><br></pre></td></tr></table></figure>\n<p>sampletoken</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.7547</span>, -<span class=\"number\">2.8528</span>, -<span class=\"number\">0.5092</span>, -<span class=\"number\">0.3423</span>],</span><br><span class=\"line\">         [-<span class=\"number\">1.0957</span>, -<span class=\"number\">0.8780</span>,  <span class=\"number\">0.2388</span>,  <span class=\"number\">0.2097</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.3502</span>, -<span class=\"number\">1.6158</span>, -<span class=\"number\">0.3133</span>, -<span class=\"number\">0.7224</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">0.9134</span>, -<span class=\"number\">0.4490</span>,  <span class=\"number\">0.6868</span>, -<span class=\"number\">0.3029</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7116</span>, -<span class=\"number\">2.5590</span>, -<span class=\"number\">0.1039</span>, -<span class=\"number\">0.6493</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.5076</span>, -<span class=\"number\">2.1031</span>, -<span class=\"number\">0.9347</span>, -<span class=\"number\">0.1230</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]])</span><br></pre></td></tr></table></figure>\n<h1 id=\"\"></h1>\n<p></p>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">batch_size = <span class=\"number\">3</span></span><br><span class=\"line\">feature_num = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">0</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, feature_num)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, mean)</span></span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, std, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=feature_num, affine=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(std)</span><br><span class=\"line\">torch_bn.bias =  nn.Parameter(mean)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;weight:\\n&#x27;, torch_bn.weight)</span></span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;bias:\\n&#x27;, torch_bn.bias, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br><span class=\"line\"></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, inputs, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1821</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<p>batchnorm</p>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, mean)</span></span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, std, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(std)</span><br><span class=\"line\">torch_bn.bias =  nn.Parameter(mean)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;weight:\\n&#x27;, torch_bn.weight)</span></span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;bias:\\n&#x27;, torch_bn.bias, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br><span class=\"line\"></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, inputs, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.1918</span>, -<span class=\"number\">0.1481</span>, -<span class=\"number\">1.5251</span>,  <span class=\"number\">0.4814</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8146</span>, -<span class=\"number\">1.1451</span>,  <span class=\"number\">0.7512</span>,  <span class=\"number\">1.2086</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.9685</span>, -<span class=\"number\">0.0551</span>, -<span class=\"number\">0.6140</span>,  <span class=\"number\">1.6376</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>],</span><br><span class=\"line\">        [<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>],</span><br><span class=\"line\">        [<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>]])</span><br></pre></td></tr></table></figure>\n<p>layernormlayernorm</p>\n<p>CVNLP</p>\n<p>batchnormlayernorm</p>\n<h1 id=\"\"></h1>\n<p>batchnormlayernorm</p>\n<p>batchnormlayernorm</p>\n<p>batchnorm\"\"\"\"batchbatchbatchfeature\nmap</p>\n<p>layernorm</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1LAYERNORM\nhttps://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html<br>\n2BATCHNORM1D\nhttps://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html<br>\n3BATCHNORM2D\nhttps://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html</p>\n","length":18670,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p><a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a>normalizationnorm</p>\n<p>norm</p>\n<p><a href=\"https://github.com/Saicat/normalization_exp\">https://github.com/Saicat/normalization_exp</a></p>\n<h1 id=\"\"></h1>\n<p>normalizationCNN</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># epsilon</span></span><br><span class=\"line\">eps = <span class=\"number\">1e-8</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">batch_size = <span class=\"number\">3</span></span><br><span class=\"line\">feature_num = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">0</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, feature_num)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<p>34batch\nsize=34</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm\">batchnorm</h2>\n<p>pytorchBatchNorm1d</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=feature_num, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># batchnorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">1</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(feature_num))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(feature_num))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>batchnorm/layernorm+BatchNorm\"affine\"\"affine\"False</p>\n<p>pytorchnorm1.00</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([<span class=\"number\">0.6614</span>, <span class=\"number\">0.2669</span>, <span class=\"number\">0.0617</span>, <span class=\"number\">0.6213</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.4519</span>, -<span class=\"number\">0.1661</span>, -<span class=\"number\">1.5228</span>,  <span class=\"number\">0.3817</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>weightbias4</p>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">0.4756</span>,  <span class=\"number\">0.0513</span>, -<span class=\"number\">1.6033</span>,  <span class=\"number\">0.4715</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0197</span>, -<span class=\"number\">0.5421</span>, -<span class=\"number\">1.4535</span>,  <span class=\"number\">1.0937</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8117</span>, -<span class=\"number\">0.0077</span>, -<span class=\"number\">1.5115</span>, -<span class=\"number\">0.4202</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># bn</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bn:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(isclose)</span><br></pre></td></tr></table></figure>\n<p>dim=0batchsamplefeature</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[-<span class=\"number\">0.0876</span>, -<span class=\"number\">0.6985</span>, -<span class=\"number\">0.7907</span>,  <span class=\"number\">0.5295</span>]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"number\">1.1612</span>, <span class=\"number\">0.4971</span>, <span class=\"number\">1.0630</span>, <span class=\"number\">0.2692</span>]]) </span><br></pre></td></tr></table></figure>\n<p>meanstdkeepdimTrueFalsebroadcast</p>\n<p>stdunbiasedFalsetorchbatchnorm</p>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bn:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">0.4756</span>,  <span class=\"number\">0.0514</span>, -<span class=\"number\">1.6033</span>,  <span class=\"number\">0.4715</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0197</span>, -<span class=\"number\">0.5421</span>, -<span class=\"number\">1.4535</span>,  <span class=\"number\">1.0937</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8117</span>, -<span class=\"number\">0.0077</span>, -<span class=\"number\">1.5115</span>, -<span class=\"number\">0.4202</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>torch.isclosebatchnormbatchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<p>equal1e-5~1e-4eps</p>\n<h2 id=\"layernorm\">layernorm</h2>\n<p>layernorm34</p>\n<p>torch</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">2</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(feature_num))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(feature_num))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">0.3923</span>, -<span class=\"number\">0.2236</span>, -<span class=\"number\">0.3195</span>, -<span class=\"number\">1.2050</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">1.0445</span>, -<span class=\"number\">0.6332</span>,  <span class=\"number\">0.5731</span>,  <span class=\"number\">0.5409</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p></p>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5120</span>, -<span class=\"number\">0.6001</span>,  <span class=\"number\">1.0604</span>, -<span class=\"number\">0.0392</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.7249</span>, -<span class=\"number\">0.3772</span>,  <span class=\"number\">0.3331</span>, -<span class=\"number\">0.9155</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.6645</span>, -<span class=\"number\">0.6209</span>,  <span class=\"number\">0.7693</span>, -<span class=\"number\">1.4324</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p></p>\n<p>dim=1</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ln</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;ln:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[-<span class=\"number\">0.0907</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.3104</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.3843</span>]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"number\">1.3691</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.9502</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.3458</span>]]) </span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5120</span>, -<span class=\"number\">0.6001</span>,  <span class=\"number\">1.0604</span>, -<span class=\"number\">0.0392</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.7249</span>, -<span class=\"number\">0.3772</span>,  <span class=\"number\">0.3331</span>, -<span class=\"number\">0.9155</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.6645</span>, -<span class=\"number\">0.6209</span>,  <span class=\"number\">0.7693</span>, -<span class=\"number\">1.4325</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"\"></h2>\n<p>batchnormlayernorm</p>\n<img src=\"/b70b4a2d/bn_and_ln.png\" class title=\"bnln\">\n<p>batchnormdim=0batchlayernormdim=1</p>\n<p>batchnormlayernorm</p>\n<h1 id=\"cv\">CV</h1>\n<p>CV</p>\n<p>CV[N,C,H,W]Nbatch\nsizeCchannelHWfeature\nmapCV</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># [N,C,H,W]</span></span><br><span class=\"line\">batch_size = <span class=\"number\">2</span></span><br><span class=\"line\">channel = <span class=\"number\">2</span></span><br><span class=\"line\">height = <span class=\"number\">2</span></span><br><span class=\"line\">width = <span class=\"number\">3</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">3</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, channel, height, width)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[[-<span class=\"number\">0.0766</span>,  <span class=\"number\">0.3599</span>, -<span class=\"number\">0.7820</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.0715</span>,  <span class=\"number\">0.6648</span>, -<span class=\"number\">0.2868</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[ <span class=\"number\">1.6206</span>, -<span class=\"number\">1.5967</span>,  <span class=\"number\">0.4046</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.6113</span>,  <span class=\"number\">0.7604</span>, -<span class=\"number\">0.0336</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[-<span class=\"number\">0.3448</span>,  <span class=\"number\">0.4937</span>, -<span class=\"number\">0.0776</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.8054</span>,  <span class=\"number\">0.4851</span>,  <span class=\"number\">0.2052</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[ <span class=\"number\">0.3384</span>,  <span class=\"number\">1.3528</span>,  <span class=\"number\">0.3736</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.0134</span>,  <span class=\"number\">0.7737</span>, -<span class=\"number\">0.1092</span>]]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm-1\">batchnorm</h2>\n<p>BatchNorm2dchannel</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm2d(num_features=channel, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># batchnorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">4</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(channel))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(channel))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>channel</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">1.6053</span>,  <span class=\"number\">0.2325</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([<span class=\"number\">2.2399</span>, <span class=\"number\">0.8473</span>], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>torchbatchnorm2d</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[[[<span class=\"number\">2.2043</span>, <span class=\"number\">1.1275</span>, <span class=\"number\">3.9442</span>],</span><br><span class=\"line\">          [<span class=\"number\">1.8388</span>, <span class=\"number\">0.3753</span>, <span class=\"number\">2.7226</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">1.2185</span>, <span class=\"number\">0.2591</span>, <span class=\"number\">0.8559</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.9175</span>, <span class=\"number\">0.9620</span>, <span class=\"number\">0.7252</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"number\">2.8658</span>, <span class=\"number\">0.7975</span>, <span class=\"number\">2.2066</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.4684</span>, <span class=\"number\">0.8186</span>, <span class=\"number\">1.5090</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">0.8362</span>, <span class=\"number\">1.1387</span>, <span class=\"number\">0.8467</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.7392</span>, <span class=\"number\">0.9660</span>, <span class=\"number\">0.7027</span>]]]], grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>batchnorm2d</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># bn</span></span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = []</span><br><span class=\"line\"><span class=\"comment\"># channel</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(channel):</span><br><span class=\"line\">    <span class=\"comment\"># </span></span><br><span class=\"line\">    mean = torch.mean(inputs[:, c, :, :])</span><br><span class=\"line\">    std = torch.std(inputs[:, c, :, :], unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    normed = (inputs[:, c, :, :] - mean) / (std + eps) * torch_bn.weight[c] + torch_bn.bias[c]</span><br><span class=\"line\">    normed = normed.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">    manual_normed.append(normed)</span><br><span class=\"line\">manual_normed = torch.cat(manual_normed, <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bn:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>CVchannel\"batch\"</p>\n<p>NHW</p>\n<img src=\"/b70b4a2d/cv_batchnorm.png\" class title=\"CVbatchnorm\">\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bn:</span><br><span class=\"line\"> tensor([[[[<span class=\"number\">2.2043</span>, <span class=\"number\">1.1275</span>, <span class=\"number\">3.9442</span>],</span><br><span class=\"line\">          [<span class=\"number\">1.8388</span>, <span class=\"number\">0.3752</span>, <span class=\"number\">2.7226</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">1.2185</span>, <span class=\"number\">0.2591</span>, <span class=\"number\">0.8559</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.9175</span>, <span class=\"number\">0.9620</span>, <span class=\"number\">0.7252</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"number\">2.8658</span>, <span class=\"number\">0.7975</span>, <span class=\"number\">2.2066</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.4685</span>, <span class=\"number\">0.8186</span>, <span class=\"number\">1.5089</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"number\">0.8362</span>, <span class=\"number\">1.1387</span>, <span class=\"number\">0.8467</span>],</span><br><span class=\"line\">          [<span class=\"number\">0.7392</span>, <span class=\"number\">0.9660</span>, <span class=\"number\">0.7027</span>]]]], grad_fn=&lt;CatBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"layernorm-1\">layernorm</h2>\n<p><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\">torchlayernorm</a>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(</span><br><span class=\"line\">    normalized_shape=[channel, height, width], </span><br><span class=\"line\">    elementwise_affine=<span class=\"literal\">True</span></span><br><span class=\"line\">)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">5</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(channel, height, width))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(channel, height, width))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p></p>\n<img src=\"/b70b4a2d/cv_layernorm.jpeg\" class title=\"CVlayernorm\">\n<p>[channel, height, width]</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([[[-<span class=\"number\">0.4868</span>, -<span class=\"number\">0.6038</span>, -<span class=\"number\">0.5581</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.6675</span>, -<span class=\"number\">0.1974</span>,  <span class=\"number\">1.9428</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">1.4017</span>, -<span class=\"number\">0.7626</span>,  <span class=\"number\">0.6312</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.8991</span>, -<span class=\"number\">0.5578</span>,  <span class=\"number\">0.6907</span>]]], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([[[ <span class=\"number\">0.2225</span>, -<span class=\"number\">0.6662</span>,  <span class=\"number\">0.6846</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.5740</span>, -<span class=\"number\">0.5829</span>,  <span class=\"number\">0.7679</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">0.0571</span>, -<span class=\"number\">1.1894</span>, -<span class=\"number\">0.5659</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.8327</span>,  <span class=\"number\">0.9014</span>,  <span class=\"number\">0.2116</span>]]], requires_grad=<span class=\"literal\">True</span>) </span><br></pre></td></tr></table></figure>\n<p>channel</p>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[[[ <span class=\"number\">0.3594</span>, -<span class=\"number\">0.8338</span>,  <span class=\"number\">1.3456</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.5128</span>, -<span class=\"number\">0.7147</span>, -<span class=\"number\">0.3012</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">2.5939</span>,  <span class=\"number\">0.5089</span>, -<span class=\"number\">0.3546</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.3715</span>,  <span class=\"number\">0.4607</span>,  <span class=\"number\">0.0553</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[ <span class=\"number\">0.5477</span>, -<span class=\"number\">0.9583</span>,  <span class=\"number\">0.8526</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.2112</span>, -<span class=\"number\">0.6760</span>,  <span class=\"number\">0.9378</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">0.3219</span>, -<span class=\"number\">2.4580</span>, -<span class=\"number\">0.3647</span>],</span><br><span class=\"line\">          [-<span class=\"number\">0.6744</span>,  <span class=\"number\">0.4171</span>, -<span class=\"number\">0.0264</span>]]]], grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ln</span></span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = []</span><br><span class=\"line\"><span class=\"comment\"># channel</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> b <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(batch_size):</span><br><span class=\"line\">    <span class=\"comment\"># </span></span><br><span class=\"line\">    mean = torch.mean(inputs[b, :, :, :])</span><br><span class=\"line\">    std = torch.std(inputs[b, :, :, :], unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    normed = (inputs[b, :, :, :] - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\">    normed = normed.unsqueeze(<span class=\"number\">0</span>)</span><br><span class=\"line\">    manual_normed.append(normed)</span><br><span class=\"line\">manual_normed = torch.cat(manual_normed, <span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;ln:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>channel</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln:</span><br><span class=\"line\"> tensor([[[[ <span class=\"number\">0.3594</span>, -<span class=\"number\">0.8338</span>,  <span class=\"number\">1.3456</span>],</span><br><span class=\"line\">          [ <span class=\"number\">0.5128</span>, -<span class=\"number\">0.7147</span>, -<span class=\"number\">0.3012</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">2.5939</span>,  <span class=\"number\">0.5090</span>, -<span class=\"number\">0.3546</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.3715</span>,  <span class=\"number\">0.4607</span>,  <span class=\"number\">0.0553</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[ <span class=\"number\">0.5477</span>, -<span class=\"number\">0.9583</span>,  <span class=\"number\">0.8527</span>],</span><br><span class=\"line\">          [-<span class=\"number\">1.2112</span>, -<span class=\"number\">0.6760</span>,  <span class=\"number\">0.9378</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[-<span class=\"number\">0.3219</span>, -<span class=\"number\">2.4581</span>, -<span class=\"number\">0.3647</span>],</span><br><span class=\"line\">          [-<span class=\"number\">0.6744</span>,  <span class=\"number\">0.4171</span>, -<span class=\"number\">0.0264</span>]]]], grad_fn=&lt;CatBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]],</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        [[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">          [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]]])</span><br></pre></td></tr></table></figure>\n<h1 id=\"nlp\">NLP</h1>\n<p>NLP</p>\n<p>Nbatch sizeSsequence lengthHhidden size</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># [N,S,H]</span></span><br><span class=\"line\">batch_size = <span class=\"number\">2</span></span><br><span class=\"line\">seq_len = <span class=\"number\">3</span></span><br><span class=\"line\">hidden_size = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">6</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, seq_len, hidden_size)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>\n<h2 id=\"batchnorm-2\">batchnorm</h2>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=hidden_size, affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># batchnorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">7</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(hidden_size))</span><br><span class=\"line\">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(hidden_size))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_bn.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_bn.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># # </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>transposetranspose</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.1468</span>,  <span class=\"number\">0.7861</span>,  <span class=\"number\">0.9468</span>, -<span class=\"number\">1.1143</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">1.6908</span>, -<span class=\"number\">0.8948</span>, -<span class=\"number\">0.3556</span>,  <span class=\"number\">1.2324</span>], requires_grad=<span class=\"literal\">True</span>) </span><br><span class=\"line\"></span><br><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[[ <span class=\"number\">1.8740</span>, -<span class=\"number\">0.7037</span>, -<span class=\"number\">1.8222</span>,  <span class=\"number\">2.3385</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.7413</span>, -<span class=\"number\">1.8119</span>,  <span class=\"number\">0.3641</span>,  <span class=\"number\">0.0200</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.4615</span>, -<span class=\"number\">0.2676</span>,  <span class=\"number\">0.1081</span>,  <span class=\"number\">1.3450</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">1.7084</span>, -<span class=\"number\">1.9653</span>,  <span class=\"number\">1.0169</span>,  <span class=\"number\">0.5785</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.8213</span>, -<span class=\"number\">0.8614</span>, -<span class=\"number\">0.8056</span>,  <span class=\"number\">2.9892</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.5383</span>,  <span class=\"number\">0.2409</span>, -<span class=\"number\">0.9949</span>,  <span class=\"number\">0.1231</span>]]], grad_fn=&lt;TransposeBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>batchnorm</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># bn</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=(<span class=\"number\">0</span>, <span class=\"number\">1</span>) , keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=(<span class=\"number\">0</span>, <span class=\"number\">1</span>), keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bn:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p>dim=(0,1)[N,\nS, H][NS,\nH]batchnorm</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.2151</span>,  <span class=\"number\">0.5444</span>, -<span class=\"number\">0.2633</span>, -<span class=\"number\">0.5424</span>]]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"number\">0.7984</span>, <span class=\"number\">0.3537</span>, <span class=\"number\">0.7799</span>, <span class=\"number\">0.7986</span>]]]) </span><br><span class=\"line\"></span><br><span class=\"line\">bn:</span><br><span class=\"line\"> tensor([[[ <span class=\"number\">1.8740</span>, -<span class=\"number\">0.7037</span>, -<span class=\"number\">1.8222</span>,  <span class=\"number\">2.3385</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.7413</span>, -<span class=\"number\">1.8119</span>,  <span class=\"number\">0.3641</span>,  <span class=\"number\">0.0200</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.4615</span>, -<span class=\"number\">0.2676</span>,  <span class=\"number\">0.1081</span>,  <span class=\"number\">1.3450</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">1.7084</span>, -<span class=\"number\">1.9653</span>,  <span class=\"number\">1.0169</span>,  <span class=\"number\">0.5785</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.8213</span>, -<span class=\"number\">0.8614</span>, -<span class=\"number\">0.8056</span>,  <span class=\"number\">2.9892</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.5383</span>,  <span class=\"number\">0.2409</span>, -<span class=\"number\">0.9950</span>,  <span class=\"number\">0.1231</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]])</span><br></pre></td></tr></table></figure>\n<h2 id=\"layernorm-2\">layernorm</h2>\n<p>NLPlayernormhuggingfacebertlayernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BertSelfOutput</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, config</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class=\"line\">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class=\"line\">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, hidden_states: torch.normTensor, input_tensor: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class=\"line\">        hidden_states = self.dense(hidden_states)</span><br><span class=\"line\">        hidden_states = self.dropout(hidden_states)</span><br><span class=\"line\">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> hidden_states</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=hidden_size, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weigh=1bias=0</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">8</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(hidden_size))</span><br><span class=\"line\">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(hidden_size))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;weight:\\n&#x27;</span>, torch_ln.weight)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bias:\\n&#x27;</span>, torch_ln.bias, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>\n<p>hidden size</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([ <span class=\"number\">0.2713</span>, -<span class=\"number\">1.2729</span>,  <span class=\"number\">0.5027</span>,  <span class=\"number\">0.4181</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">bias:</span><br><span class=\"line\"> Parameter containing:</span><br><span class=\"line\">tensor([-<span class=\"number\">0.6394</span>, -<span class=\"number\">0.6608</span>, -<span class=\"number\">0.1433</span>, -<span class=\"number\">0.1043</span>], requires_grad=<span class=\"literal\">True</span>) </span><br><span class=\"line\"></span><br><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.7547</span>, -<span class=\"number\">2.8528</span>, -<span class=\"number\">0.5092</span>, -<span class=\"number\">0.3423</span>],</span><br><span class=\"line\">         [-<span class=\"number\">1.0957</span>, -<span class=\"number\">0.8780</span>,  <span class=\"number\">0.2388</span>,  <span class=\"number\">0.2097</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.3502</span>, -<span class=\"number\">1.6158</span>, -<span class=\"number\">0.3133</span>, -<span class=\"number\">0.7224</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">0.9134</span>, -<span class=\"number\">0.4490</span>,  <span class=\"number\">0.6868</span>, -<span class=\"number\">0.3029</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7116</span>, -<span class=\"number\">2.5589</span>, -<span class=\"number\">0.1039</span>, -<span class=\"number\">0.6493</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.5076</span>, -<span class=\"number\">2.1031</span>, -<span class=\"number\">0.9346</span>, -<span class=\"number\">0.1230</span>]]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ln</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">2</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, mean)</span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">2</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, std, <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">manual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;ln:\\n&#x27;</span>, manual_normed)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch&lt;1e-4</span></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.8469</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.0745</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.3386</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ <span class=\"number\">0.1364</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7003</span>],</span><br><span class=\"line\">         [ <span class=\"number\">0.2831</span>]]])</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"number\">0.8578</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.3354</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.6505</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"number\">0.4426</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.8448</span>],</span><br><span class=\"line\">         [<span class=\"number\">0.6816</span>]]]) </span><br></pre></td></tr></table></figure>\n<p>sampletoken</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln:</span><br><span class=\"line\"> tensor([[[-<span class=\"number\">0.7547</span>, -<span class=\"number\">2.8528</span>, -<span class=\"number\">0.5092</span>, -<span class=\"number\">0.3423</span>],</span><br><span class=\"line\">         [-<span class=\"number\">1.0957</span>, -<span class=\"number\">0.8780</span>,  <span class=\"number\">0.2388</span>,  <span class=\"number\">0.2097</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.3502</span>, -<span class=\"number\">1.6158</span>, -<span class=\"number\">0.3133</span>, -<span class=\"number\">0.7224</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[-<span class=\"number\">0.9134</span>, -<span class=\"number\">0.4490</span>,  <span class=\"number\">0.6868</span>, -<span class=\"number\">0.3029</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.7116</span>, -<span class=\"number\">2.5590</span>, -<span class=\"number\">0.1039</span>, -<span class=\"number\">0.6493</span>],</span><br><span class=\"line\">         [-<span class=\"number\">0.5076</span>, -<span class=\"number\">2.1031</span>, -<span class=\"number\">0.9347</span>, -<span class=\"number\">0.1230</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">         [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]]])</span><br></pre></td></tr></table></figure>\n<h1 id=\"\"></h1>\n<p></p>\n<p>batchnorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">batch_size = <span class=\"number\">3</span></span><br><span class=\"line\">feature_num = <span class=\"number\">4</span></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">0</span>)  <span class=\"comment\"># </span></span><br><span class=\"line\">inputs = torch.randn(batch_size, feature_num)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, mean)</span></span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, std, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torchbatchnorm</span></span><br><span class=\"line\">torch_bn = nn.BatchNorm1d(num_features=feature_num, affine=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(std)</span><br><span class=\"line\">torch_bn.bias =  nn.Parameter(mean)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;weight:\\n&#x27;, torch_bn.weight)</span></span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;bias:\\n&#x27;, torch_bn.bias, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_bn(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch bn:\\n&#x27;</span>, torch_normed)</span><br><span class=\"line\"></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, inputs, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br><span class=\"line\">torch bn:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1821</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>],</span><br><span class=\"line\">        [<span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>, <span class=\"literal\">True</span>]])</span><br></pre></td></tr></table></figure>\n<p>batchnorm</p>\n<p>layernorm</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, inputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">mean = torch.mean(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, mean)</span></span><br><span class=\"line\">std = torch.std(inputs, dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>, unbiased=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;:\\n&#x27;, std, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torchlayernorm</span></span><br><span class=\"line\">torch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=<span class=\"literal\">True</span>)  <span class=\"comment\"># layernorm</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_bn.weight = nn.Parameter(std)</span><br><span class=\"line\">torch_bn.bias =  nn.Parameter(mean)</span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;weight:\\n&#x27;, torch_bn.weight)</span></span><br><span class=\"line\"><span class=\"comment\"># print(&#x27;bias:\\n&#x27;, torch_bn.bias, &#x27;\\n&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">torch_normed = torch_ln(inputs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;torch ln:\\n&#x27;</span>, torch_normed)</span><br><span class=\"line\"></span><br><span class=\"line\">isclose = torch.isclose(torch_normed, inputs, rtol=<span class=\"number\">1e-4</span>, atol=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;:\\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.5410</span>, -<span class=\"number\">0.2934</span>, -<span class=\"number\">2.1788</span>,  <span class=\"number\">0.5684</span>],</span><br><span class=\"line\">        [-<span class=\"number\">1.0845</span>, -<span class=\"number\">1.3986</span>,  <span class=\"number\">0.4033</span>,  <span class=\"number\">0.8380</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.7193</span>, -<span class=\"number\">0.4033</span>, -<span class=\"number\">0.5966</span>,  <span class=\"number\">0.1820</span>]])</span><br><span class=\"line\">torch ln:</span><br><span class=\"line\"> tensor([[ <span class=\"number\">1.1918</span>, -<span class=\"number\">0.1481</span>, -<span class=\"number\">1.5251</span>,  <span class=\"number\">0.4814</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.8146</span>, -<span class=\"number\">1.1451</span>,  <span class=\"number\">0.7512</span>,  <span class=\"number\">1.2086</span>],</span><br><span class=\"line\">        [-<span class=\"number\">0.9685</span>, -<span class=\"number\">0.0551</span>, -<span class=\"number\">0.6140</span>,  <span class=\"number\">1.6376</span>]],</span><br><span class=\"line\">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br><span class=\"line\">:</span><br><span class=\"line\"> tensor([[<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>],</span><br><span class=\"line\">        [<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>],</span><br><span class=\"line\">        [<span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>, <span class=\"literal\">False</span>]])</span><br></pre></td></tr></table></figure>\n<p>layernormlayernorm</p>\n<p>CVNLP</p>\n<p>batchnormlayernorm</p>\n<h1 id=\"\"></h1>\n<p>batchnormlayernorm</p>\n<p>batchnormlayernorm</p>\n<p>batchnorm\"\"\"\"batchbatchbatchfeature\nmap</p>\n<p>layernorm</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1LAYERNORM\nhttps://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html<br>\n2BATCHNORM1D\nhttps://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html<br>\n3BATCHNORM2D\nhttps://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html</p>\n"},{"title":"(3)","abbrlink":"1736008","date":"2024-04-05T06:08:31.000Z","_content":"\n![](/images/cover.png)  \n\n\n\n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~~\n\n***  \n\n# 1.RoPE  \n\nRoPERoPE  \n\nRoPE2k2kAlibiNTKYaRN  \n\n# 2.batchnormmomentum  \n\nbatchnormbatchnormalization  \n\nmoving_mean = momentum  moving_mean + (1.0  momentum)  mean  \n\nmoving_var = momentum  moving_var + (1.0  momentum)  var  \n\nmomentumbatch sizemini batchmomentum  \n\n# 3.  \n\n  \n\n  \n\n  \n\n  \n\n# 4.kv cache  \n\nGPT  \n\ntoken  \n\ntokenkv  \n\nkvL2kv  \n\n# 5.ReLU  \n\n1max(0, x)21/  \n\n1020  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  ","source":"_posts/cs/nlp/2024/04/-3.md","raw":"---\ntitle: (3)\ntags:\n  - NLP\n  - LLM\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: '1736008'\ndate: 2024-04-05 14:08:31\n---\n\n![](/images/cover.png)  \n\n\n\n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~~\n\n***  \n\n# 1.RoPE  \n\nRoPERoPE  \n\nRoPE2k2kAlibiNTKYaRN  \n\n# 2.batchnormmomentum  \n\nbatchnormbatchnormalization  \n\nmoving_mean = momentum  moving_mean + (1.0  momentum)  mean  \n\nmoving_var = momentum  moving_var + (1.0  momentum)  var  \n\nmomentumbatch sizemini batchmomentum  \n\n# 3.  \n\n  \n\n  \n\n  \n\n  \n\n# 4.kv cache  \n\nGPT  \n\ntoken  \n\ntokenkv  \n\nkvL2kv  \n\n# 5.ReLU  \n\n1max(0, x)21/  \n\n1020  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  ","slug":"cs/nlp/2024/04/-3","published":1,"updated":"2024-05-10T06:51:01.150Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmy001x0p4ka1x236kq","content":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM<sub></sub></p>\n<hr>\n<h1 id=\"rope\">1.RoPE</h1>\n<p>RoPERoPE</p>\n<p>RoPE2k2kAlibiNTKYaRN</p>\n<h1 id=\"batchnormmomentum\">2.batchnormmomentum</h1>\n<p>batchnormbatchnormalization</p>\n<p>moving_mean = momentum  moving_mean + (1.0  momentum)  mean</p>\n<p>moving_var = momentum  moving_var + (1.0  momentum)  var</p>\n<p>momentumbatch\nsizemini batchmomentum</p>\n<h1 id=\"\">3.</h1>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<h1 id=\"kv-cache\">4.kv cache</h1>\n<p>GPT</p>\n<p>token</p>\n<p>tokenkv</p>\n<p>kvL2kv</p>\n<h1 id=\"relu\">5.ReLU</h1>\n<p>1max(0,\nx)21/</p>\n<p>1020</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n","length":1653,"excerpt":"","more":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM<sub></sub></p>\n<hr>\n<h1 id=\"rope\">1.RoPE</h1>\n<p>RoPERoPE</p>\n<p>RoPE2k2kAlibiNTKYaRN</p>\n<h1 id=\"batchnormmomentum\">2.batchnormmomentum</h1>\n<p>batchnormbatchnormalization</p>\n<p>moving_mean = momentum  moving_mean + (1.0  momentum)  mean</p>\n<p>moving_var = momentum  moving_var + (1.0  momentum)  var</p>\n<p>momentumbatch\nsizemini batchmomentum</p>\n<h1 id=\"\">3.</h1>\n<p></p>\n<p></p>\n<p></p>\n<p></p>\n<h1 id=\"kv-cache\">4.kv cache</h1>\n<p>GPT</p>\n<p>token</p>\n<p>tokenkv</p>\n<p>kvL2kv</p>\n<h1 id=\"relu\">5.ReLU</h1>\n<p>1max(0,\nx)21/</p>\n<p>1020</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n"},{"title":"(4)","abbrlink":"1736008","date":"2024-04-20T08:56:45.000Z","_content":"\n![](/images/cover.png)  \n\n\n\n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~~\n\n***  \n\n# 1.Transformerlayernormbatchnorm  \n\nNLPpaddingpaddingnormalizationbatchnormbatchtokentokenPowerNorm: Rethinking Batch Normalization in TransformersNLPbatchnormlayernorm  \n\n# 2.transformerencdoerdecoder  \n\ndecoderself-attentionencoderattentionKVdecoderQcross-attention  \n\n{% asset_img transformer.png transformer %}  \n\n# 3.PyTorchTensorview()reshape()  \n\n1.view()reshape()tensorview()reshape  \n\n2.view()tensor  \n\n3.reshape()tensortensorview()  \n\n4.is_contiguous()contiguous()  \n\n# 4.RLHFPPO  \n\nPPO4  \n\n1.ActorSFTactionCriticloss  \n\n2.ReferenceSFTRLHFReferenceActorActorReferenceKL penaltyActor  \n\n3.RewardSFTRLHF  \n\n4.CriticRewardActortoken  \n\n# 5.GPTLVhidden sizeHbatch sizeBSAdamN  \n\noptimizer  \n\n1.VH12H^2+13H=VH+L(12H^2+13H)2  \n\n2.2  \n\n3.optimizer(+)*2+(++2)*4=2016  \n\n4.softmaxdropout34BSH+5BNS^22  \n\n\nGPT3175BH=12288L=96N=96350GB=1S=102490GS=81923420G  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  ","source":"_posts/cs/nlp/2024/04/-4.md","raw":"---\ntitle: (4)\nabbrlink: '1736008'\ndate: 2024-04-20 16:56:45\ntags:\n  - NLP\n  - LLM\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n![](/images/cover.png)  \n\n\n\n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n\n***  \n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~~\n\n***  \n\n# 1.Transformerlayernormbatchnorm  \n\nNLPpaddingpaddingnormalizationbatchnormbatchtokentokenPowerNorm: Rethinking Batch Normalization in TransformersNLPbatchnormlayernorm  \n\n# 2.transformerencdoerdecoder  \n\ndecoderself-attentionencoderattentionKVdecoderQcross-attention  \n\n{% asset_img transformer.png transformer %}  \n\n# 3.PyTorchTensorview()reshape()  \n\n1.view()reshape()tensorview()reshape  \n\n2.view()tensor  \n\n3.reshape()tensortensorview()  \n\n4.is_contiguous()contiguous()  \n\n# 4.RLHFPPO  \n\nPPO4  \n\n1.ActorSFTactionCriticloss  \n\n2.ReferenceSFTRLHFReferenceActorActorReferenceKL penaltyActor  \n\n3.RewardSFTRLHF  \n\n4.CriticRewardActortoken  \n\n# 5.GPTLVhidden sizeHbatch sizeBSAdamN  \n\noptimizer  \n\n1.VH12H^2+13H=VH+L(12H^2+13H)2  \n\n2.2  \n\n3.optimizer(+)*2+(++2)*4=2016  \n\n4.softmaxdropout34BSH+5BNS^22  \n\n\nGPT3175BH=12288L=96N=96350GB=1S=102490GS=81923420G  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n\n\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  ","slug":"cs/nlp/2024/04/-4","published":1,"updated":"2024-05-10T06:50:46.826Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmy001z0p4kavkaedcm","content":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM<sub></sub></p>\n<hr>\n<h1 id=\"transformerlayernormbatchnorm\">1.Transformerlayernormbatchnorm</h1>\n<p>NLPpaddingpaddingnormalizationbatchnormbatchtokentokenPowerNorm:\nRethinking Batch Normalization in\nTransformersNLPbatchnormlayernorm</p>\n<h1 id=\"transformerencdoerdecoder\">2.transformerencdoerdecoder</h1>\n<p>decoderself-attentionencoderattentionKVdecoderQcross-attention</p>\n<img src=\"/1736008/transformer.png\" class title=\"transformer\">\n<h1 id=\"pytorchtensorviewreshape\">3.PyTorchTensorview()reshape()</h1>\n<p>1.view()reshape()tensorview()reshape</p>\n<p>2.view()tensor</p>\n<p>3.reshape()tensortensorview()</p>\n<p>4.is_contiguous()contiguous()</p>\n<h1 id=\"rlhfppo\">4.RLHFPPO</h1>\n<p>PPO4</p>\n<p>1.ActorSFTactionCriticloss</p>\n<p>2.ReferenceSFTRLHFReferenceActorActorReferenceKL\npenaltyActor</p>\n<p>3.RewardSFTRLHF</p>\n<p>4.CriticRewardActortoken</p>\n<h1 id=\"gptlvhidden-sizehbatch-sizebsadamn\">5.GPTLVhidden\nsizeHbatch\nsizeBSAdamN</h1>\n<p>optimizer</p>\n<p>1.VH12H<sup>2+13H=VH+L(12H</sup>2+13H)2</p>\n<p>2.2</p>\n<p>3.optimizer(+)<em>2+(++2)</em>4=2016</p>\n<p>4.softmaxdropout34BSH+5BNS^22</p>\n<p>\nGPT3175BH=12288L=96N=96350GB=1S=102490GS=81923420G</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a></p>\n","length":2305,"excerpt":"","more":"<p><img src=\"/images/cover.png\"></p>\n<p></p>\n<p><a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a></p>\n<hr>\n<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM<sub></sub></p>\n<hr>\n<h1 id=\"transformerlayernormbatchnorm\">1.Transformerlayernormbatchnorm</h1>\n<p>NLPpaddingpaddingnormalizationbatchnormbatchtokentokenPowerNorm:\nRethinking Batch Normalization in\nTransformersNLPbatchnormlayernorm</p>\n<h1 id=\"transformerencdoerdecoder\">2.transformerencdoerdecoder</h1>\n<p>decoderself-attentionencoderattentionKVdecoderQcross-attention</p>\n<img src=\"/1736008/transformer.png\" class title=\"transformer\">\n<h1 id=\"pytorchtensorviewreshape\">3.PyTorchTensorview()reshape()</h1>\n<p>1.view()reshape()tensorview()reshape</p>\n<p>2.view()tensor</p>\n<p>3.reshape()tensortensorview()</p>\n<p>4.is_contiguous()contiguous()</p>\n<h1 id=\"rlhfppo\">4.RLHFPPO</h1>\n<p>PPO4</p>\n<p>1.ActorSFTactionCriticloss</p>\n<p>2.ReferenceSFTRLHFReferenceActorActorReferenceKL\npenaltyActor</p>\n<p>3.RewardSFTRLHF</p>\n<p>4.CriticRewardActortoken</p>\n<h1 id=\"gptlvhidden-sizehbatch-sizebsadamn\">5.GPTLVhidden\nsizeHbatch\nsizeBSAdamN</h1>\n<p>optimizer</p>\n<p>1.VH12H<sup>2+13H=VH+L(12H</sup>2+13H)2</p>\n<p>2.2</p>\n<p>3.optimizer(+)<em>2+(++2)</em>4=2016</p>\n<p>4.softmaxdropout34BSH+5BNS^22</p>\n<p>\nGPT3175BH=12288L=96N=96350GB=1S=102490GS=81923420G</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<p><a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a></p>\n"},{"title":"DeepSeek-V2MLA","abbrlink":"83c49df0","date":"2024-07-12T12:54:22.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nDeepSeek-V2  \n\nDeepSeek-V2MLA  \n\nMLADeepSeek-V2  \n\nDeepSeek-V2236BDeepSeek-V2-Lite15.7B  \n\n#   \n\nDeepSeek-V2  \n- 236B21B  \n- 128k  \n- DeepSeek-67BDeepSeek-V242.5%93.3%KV cachethroughput5.76  \n\nDeepSeek-V2MMLU  \n\n{% asset_img intro.png DeepSeek-V2 %}  \n\nDeepSeek-V270B dense  \n\nDeepSeek-V2\n\n{% asset_img model.png  %}  \n\nV1V2MoEfine-grained expertshared expertDeepSeekMoE[MoE](http://www.linsight.cn/44e38c1b.html)V2Multi-Head Latent AttentionMLA  \n\n## MLA  \n\nMLADeepSeek-V2KV cache  \n\nKV cacheMHA/GQA/MQA[Attention:MHA,MQAGQA](https://zhuanlan.zhihu.com/p/686149289)  \n\n1MHA  \n\nMHA $n_h$ $d_h$ $\\mathbf{h}_{t}\\in\\mathbb{R}^{d}$ ttoken  \n\nMHA $W^{Q},W^{K},W^{V}\\in\\mathbb{R}^{d_{h}n_{h}\\times d}$ $\\mathbf{q}_t,\\mathbf{k}_t,\\mathbf{v}_t\\in\\mathbb{R}^{d_hn_h}$  \n\n$$\\mathbf{q}_t=W^Q\\mathbf{h}_t$$  \n\n$$\\mathbf{k}_t=W^K\\mathbf{h}_t$$  \n\n$$\\mathbf{v}_t=W^V\\mathbf{h}_t$$  \n\n $\\mathbf{q}_t,\\mathbf{k}_t,\\mathbf{v}_t$  $n_h$   \n\n$$[\\mathbf{q}_{t,1};\\mathbf{q}_{t,2};...;\\mathbf{q}_{t,n_{h}}]=\\mathbf{q}_{t}$$  \n\n$$[\\mathbf{k}_{t,1};\\mathbf{k}_{t,2};...;\\mathbf{k}_{t,n_{h}}]=\\mathbf{k}_{t}$$  \n\n$$[\\mathbf{v}_{t,1};\\mathbf{v}_{t,2};...;\\mathbf{v}_{t,n_{h}}]=\\mathbf{v}_{t}$$  \n\n$$\\mathbf{o}_{t,i}=\\sum_{j=1}^t\\mathrm{Softmax}_j(\\frac{\\mathbf{q}_{t,i}^T\\mathbf{k}_{j,i}}{\\sqrt{d_h}})\\mathbf{v}_{j,i}$$  \n\n$$\\mathbf{u}_t=W^O[\\mathbf{o}_{t,1};\\mathbf{o}_{t,2};...;\\mathbf{o}_{t,n_h}]$$  \n\n $\\mathbf{q}_{t,i},\\mathbf{k}_{t,i},\\mathbf{v}_{t,i}\\in\\mathbb{R}^{d_{h}}$$W^O\\in\\mathbb{R}^{d\\times d_hn_h}$  \n\nKVtoken $2{n}_{h}{d}_{h}$   \n\nGQA/MQAKVKV  \n\n{% asset_img GQA.png GQA %}  \n\nMQA1GQAtoken $2{d}_{h}$MHA1~2KVGQA/MQA  \n\nDeepSeek-V21.33T tokenMHAGQAMQA7B4benchmark  \n\n{% asset_img GQA_compare_MHA.png MHA/GQA/MQA %}  \n\nMHAMQAGQA  \n\n2MLA  \n\nMLAKVlow-rank joint compressionKV cacheKV  \n\n{% asset_img MLA.png MLA %}  \n\nMLA  \n\nMHAKV $h_t$ MLAKVdown-projection matrixup-projection matrices  \n\n$$\\mathbf{c}_t^{KV}=W^{DKV}\\mathbf{h}_t$$  \n\n$$\\mathbf{k}_t^C=W^{UK}\\mathbf{c}_t^{KV}$$  \n\n$$\\mathbf{v}_t^C=W^{UV}\\mathbf{c}_t^{KV}$$  \n\n$\\mathfrak{c}_t^{KV}\\in\\mathbb{R}^{d_c}$ KVcompressed latent vector\n\nMHA $W^{K},W^{V}$   \n\n$$\\mathbf{k}_t=W^K\\mathbf{h}_t\\rightarrow\\mathbf{k}_tW^{UK}W^{DKV}\\mathbf{h}_t$$  \n\n$$\\mathbf{v}_t=W^V\\mathbf{h}_t\\rightarrow\\mathbf{k}_tW^{UV}W^{DKV}\\mathbf{h}_t$$  \n\n$d_c$ KV $d_c\\ll d_hn_h$  \n\nattentionqkv  \n\n$$\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{Q^TK}{\\sqrt{d}})V$$  \n\n $Q^TK=H^T(W^Q)^TW^{UK}C$  $W^{UK}$  $W^{Q}$ K $W^Q$ shapeC $W^{UV}$  $W^{O}$  \n\nDeepSeek-V2Qlow-rank compressionKV  \n\n$$\\mathbf{c}_t^Q=W^{DQ}\\mathbf{h}_t,\\\\\\mathbf{q}_t^C=W^{UQ}\\mathbf{c}_t^Q,$$  \n\nQactivationactivation[MHAMQAGQAMLA](https://kexue.fm/archives/10091)Q  \n\n3RoPE  \n\nMLAoverheadup-projection matrices  \n\nDeepSeek-V2RoPERoPEQK[LLM:RoPE](https://zhuanlan.zhihu.com/p/684072868)  \n\nMLAKRoPElatent vectorKVMLA  \n\nDeepSeek-V2decoupled RoPEmulti-head queries $\\mathbf{q}_{t,i}^R\\in\\mathbb{R}^{d_h^R}$ shared key $\\mathbf{k}_t^R\\in\\mathbb{R}^{d_h^R}$ RoPE$d_h^R$ decoupled queries  \n\nqkRoPEattention  \n\nMLA  \n\n{% asset_img MLA_formula.png MLA %}  \n\n  \n\nMLA2.5GQA  \n\n{% asset_img MLA_cache.png MLA %}  \n\nDeepSeek-V2MLAMHA16B1.33T token250B420B token  \n\n{% asset_img MLA_perf.png MLA %}  \n\n4benchmarkMLAMHAKV cacheMLA  \n\n##   \n\nMoE  \n\n1Device-Limited Routing  \n\ntokentarget expertdevicefine-grained expertdevice  \n\nDeepSeek-V2target expertdeviceMM3  \n\n2Expert-Level Balance Loss  \n\nDeepSeekMoE V1  \n\n$$\\begin{aligned}\n\\mathcal{L}_{\\mathrm{ExpBal}}& =\\alpha_1\\sum_{i=1}^{N_r}f_iP_i\n\\end{aligned}$$  \n\n$$\\begin{aligned}\nf_{i}& =\\frac{N_{r}}{K_{r}T}\\sum_{t=1}^T\\mathbb{1}(\\text{Token }t\\text{ selects Expert }i)\n\\end{aligned}$$  \n\n$$\\begin{aligned}\nP_{i}& =\\frac1T\\sum_{t=1}^Ts_{i,t} \n\\end{aligned}$$  \n\n$\\alpha_1$ expert-level balance factorTtoken  \n\n3Device-Level Balance Loss  \n\nD$\\{\\mathcal{E}_1,\\mathcal{E}_2,...,\\mathcal{E}_D\\}$  \n\n$$\\mathcal{L}_\\mathrm{DevBal}=\\alpha_2\\sum_{i=1}^Df_i^{\\prime}P_i^{\\prime}$$  \n\n$$f_i'=\\frac1{|\\mathcal{E}_i|}\\sum_{j\\in\\mathcal{E}_i}f_j$$  \n\n$$P_i'=\\sum_{j\\in\\mathcal{E}_i}P_j$$  \n\n$\\alpha_2$ device-level balance factor  \n\n4Communication Balance Loss  \n\ntokentargetdevicedevicetoken  \n\ncommunication balance loss  \n\n$$\\mathcal{L}_{\\mathrm{CommBal}}=\\alpha_3\\sum_{i=1}^Df_i^{\\prime\\prime}P_i^{\\prime\\prime}$$  \n\n$$f_i^{\\prime\\prime}=\\frac D{MT}\\sum_{t=1}^T1(\\text{Token t is sent to Device i})$$  \n\n$$P_i''=\\sum_{j\\in\\mathcal{E}_i}P_j$$  \n\n$\\alpha_3$ communication balance factor  \n\n5Token-Dropping Strategy  \n\nlossdevice-level token-dropping strategydevicecapacitybatchdevicetokendevicetokendrop  \n\nsequence10%sequencedroptoken  \n\ndevice  \n\n#   \n\nDeepSeek-V2DeepSeek 67BtokenizerBBPE100k  \n\n8.1T12%  \n\n##   \n\n1  \n\n- layer num = 60  \n- hidden size = 5120  \n- initialization standard deviation = 0.006  \n- attention head = 128attention head size = 128  \n- KV $d_c=512$  \n- Q $d_c'=1536$  \n- decoupled queries and key per head dimension = 64  \n- 2 + 6/160  \n-  = 1536  \n- 236B21B  \n\n2  \n\n- AdamWbeta_1 = 0.9beta_2 = 0.95weight_decay = 0.1  \n- lr schedulerwarmup-and-step-decaywarmup = 2k steplr = 2.4E-460%90%lr0.316  \n- gradient clipping norm = 1.0  \n- batch size scheduling strategy225Bbatch size23049216  \n- maximum sequence length = 4k  \n- $\\alpha_1=0.003$$\\alpha_2=0.05$$\\alpha_3=0.02$  \n\n##   \n\n $k_t^R$ YaRN4k128kYaRN  \n- s = 40  \n-  = 1  \n-  = 32  \n- target maximum context length = 160k  \n\nYaRNlength scaling factor $\\sqrt{t}=0.0707\\ln s+1$  \n\n32kbatch size = 5761000  \n\n{% asset_img needle.png  %}  \n\n##   \n\nDeepSeek-V2base  \n\n{% asset_img pt_eval.png  %}  \n\nDeepSeek-V270Bdense  \n\n##   \n\nSFT1.5M1.2Mhelpfulness0.3Msafety  \n\n  \n- epoch = 2  \n- lr = 5e-6  \n  \nSFTDeepSeek-V2GRPO  \n\n  \n\n{% asset_img align_eval.png  %}  \n\n# DeepSeek-V2-Lite\n\nDeepSeek-V2-Lite  \n\n  \n- layer num = 27  \n- hidden size = 2048  \n- initialization standard deviation = 0.006  \n- attention head = 16attention head size = 128  \n- KV $d_c=512$  \n- Q  \n- decoupled queries and key per head dimension = 64  \n- 2 + 6/64  \n- MoE  \n-  = 1408  \n- 15.7B2.4B  \n\n  \n- AdamWbeta_1 = 0.9beta_2 = 0.95weight_decay = 0.1  \n- lr schedulerwarmup-and-step-decaywarmup = 2k steplr = 4.2E-460%90%lr0.316  \n- gradient clipping norm = 1.0  \n- constant batch size = 4608  \n- maximum sequence length = 4k  \n- $\\alpha_1=0.003$loss  \n-  = 5.7T  \n\n{% asset_img lite_eval_1.png  %}  \n\n{% asset_img lite_eval_2.png  %}  \n\n#   \n\n- MLADeepSeek-V2  \n- MoEpopularMoE  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model https://arxiv.org/abs/2405.04434  \n2MHAMQAGQAMLA https://kexue.fm/archives/10091  \n","source":"_posts/cs/nlp/2024/07/DeepSeek-V2MLA.md","raw":"---\ntitle: DeepSeek-V2MLA\nabbrlink: 83c49df0\ndate: 2024-07-12 20:54:22\ntags:\n  - NLP \n  - LLM \n  - transformer \n  - \n  - DeepSeek\n  - MLA\n  - GQA\n  - MoE \ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nDeepSeek-V2  \n\nDeepSeek-V2MLA  \n\nMLADeepSeek-V2  \n\nDeepSeek-V2236BDeepSeek-V2-Lite15.7B  \n\n#   \n\nDeepSeek-V2  \n- 236B21B  \n- 128k  \n- DeepSeek-67BDeepSeek-V242.5%93.3%KV cachethroughput5.76  \n\nDeepSeek-V2MMLU  \n\n{% asset_img intro.png DeepSeek-V2 %}  \n\nDeepSeek-V270B dense  \n\nDeepSeek-V2\n\n{% asset_img model.png  %}  \n\nV1V2MoEfine-grained expertshared expertDeepSeekMoE[MoE](http://www.linsight.cn/44e38c1b.html)V2Multi-Head Latent AttentionMLA  \n\n## MLA  \n\nMLADeepSeek-V2KV cache  \n\nKV cacheMHA/GQA/MQA[Attention:MHA,MQAGQA](https://zhuanlan.zhihu.com/p/686149289)  \n\n1MHA  \n\nMHA $n_h$ $d_h$ $\\mathbf{h}_{t}\\in\\mathbb{R}^{d}$ ttoken  \n\nMHA $W^{Q},W^{K},W^{V}\\in\\mathbb{R}^{d_{h}n_{h}\\times d}$ $\\mathbf{q}_t,\\mathbf{k}_t,\\mathbf{v}_t\\in\\mathbb{R}^{d_hn_h}$  \n\n$$\\mathbf{q}_t=W^Q\\mathbf{h}_t$$  \n\n$$\\mathbf{k}_t=W^K\\mathbf{h}_t$$  \n\n$$\\mathbf{v}_t=W^V\\mathbf{h}_t$$  \n\n $\\mathbf{q}_t,\\mathbf{k}_t,\\mathbf{v}_t$  $n_h$   \n\n$$[\\mathbf{q}_{t,1};\\mathbf{q}_{t,2};...;\\mathbf{q}_{t,n_{h}}]=\\mathbf{q}_{t}$$  \n\n$$[\\mathbf{k}_{t,1};\\mathbf{k}_{t,2};...;\\mathbf{k}_{t,n_{h}}]=\\mathbf{k}_{t}$$  \n\n$$[\\mathbf{v}_{t,1};\\mathbf{v}_{t,2};...;\\mathbf{v}_{t,n_{h}}]=\\mathbf{v}_{t}$$  \n\n$$\\mathbf{o}_{t,i}=\\sum_{j=1}^t\\mathrm{Softmax}_j(\\frac{\\mathbf{q}_{t,i}^T\\mathbf{k}_{j,i}}{\\sqrt{d_h}})\\mathbf{v}_{j,i}$$  \n\n$$\\mathbf{u}_t=W^O[\\mathbf{o}_{t,1};\\mathbf{o}_{t,2};...;\\mathbf{o}_{t,n_h}]$$  \n\n $\\mathbf{q}_{t,i},\\mathbf{k}_{t,i},\\mathbf{v}_{t,i}\\in\\mathbb{R}^{d_{h}}$$W^O\\in\\mathbb{R}^{d\\times d_hn_h}$  \n\nKVtoken $2{n}_{h}{d}_{h}$   \n\nGQA/MQAKVKV  \n\n{% asset_img GQA.png GQA %}  \n\nMQA1GQAtoken $2{d}_{h}$MHA1~2KVGQA/MQA  \n\nDeepSeek-V21.33T tokenMHAGQAMQA7B4benchmark  \n\n{% asset_img GQA_compare_MHA.png MHA/GQA/MQA %}  \n\nMHAMQAGQA  \n\n2MLA  \n\nMLAKVlow-rank joint compressionKV cacheKV  \n\n{% asset_img MLA.png MLA %}  \n\nMLA  \n\nMHAKV $h_t$ MLAKVdown-projection matrixup-projection matrices  \n\n$$\\mathbf{c}_t^{KV}=W^{DKV}\\mathbf{h}_t$$  \n\n$$\\mathbf{k}_t^C=W^{UK}\\mathbf{c}_t^{KV}$$  \n\n$$\\mathbf{v}_t^C=W^{UV}\\mathbf{c}_t^{KV}$$  \n\n$\\mathfrak{c}_t^{KV}\\in\\mathbb{R}^{d_c}$ KVcompressed latent vector\n\nMHA $W^{K},W^{V}$   \n\n$$\\mathbf{k}_t=W^K\\mathbf{h}_t\\rightarrow\\mathbf{k}_tW^{UK}W^{DKV}\\mathbf{h}_t$$  \n\n$$\\mathbf{v}_t=W^V\\mathbf{h}_t\\rightarrow\\mathbf{k}_tW^{UV}W^{DKV}\\mathbf{h}_t$$  \n\n$d_c$ KV $d_c\\ll d_hn_h$  \n\nattentionqkv  \n\n$$\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{Q^TK}{\\sqrt{d}})V$$  \n\n $Q^TK=H^T(W^Q)^TW^{UK}C$  $W^{UK}$  $W^{Q}$ K $W^Q$ shapeC $W^{UV}$  $W^{O}$  \n\nDeepSeek-V2Qlow-rank compressionKV  \n\n$$\\mathbf{c}_t^Q=W^{DQ}\\mathbf{h}_t,\\\\\\mathbf{q}_t^C=W^{UQ}\\mathbf{c}_t^Q,$$  \n\nQactivationactivation[MHAMQAGQAMLA](https://kexue.fm/archives/10091)Q  \n\n3RoPE  \n\nMLAoverheadup-projection matrices  \n\nDeepSeek-V2RoPERoPEQK[LLM:RoPE](https://zhuanlan.zhihu.com/p/684072868)  \n\nMLAKRoPElatent vectorKVMLA  \n\nDeepSeek-V2decoupled RoPEmulti-head queries $\\mathbf{q}_{t,i}^R\\in\\mathbb{R}^{d_h^R}$ shared key $\\mathbf{k}_t^R\\in\\mathbb{R}^{d_h^R}$ RoPE$d_h^R$ decoupled queries  \n\nqkRoPEattention  \n\nMLA  \n\n{% asset_img MLA_formula.png MLA %}  \n\n  \n\nMLA2.5GQA  \n\n{% asset_img MLA_cache.png MLA %}  \n\nDeepSeek-V2MLAMHA16B1.33T token250B420B token  \n\n{% asset_img MLA_perf.png MLA %}  \n\n4benchmarkMLAMHAKV cacheMLA  \n\n##   \n\nMoE  \n\n1Device-Limited Routing  \n\ntokentarget expertdevicefine-grained expertdevice  \n\nDeepSeek-V2target expertdeviceMM3  \n\n2Expert-Level Balance Loss  \n\nDeepSeekMoE V1  \n\n$$\\begin{aligned}\n\\mathcal{L}_{\\mathrm{ExpBal}}& =\\alpha_1\\sum_{i=1}^{N_r}f_iP_i\n\\end{aligned}$$  \n\n$$\\begin{aligned}\nf_{i}& =\\frac{N_{r}}{K_{r}T}\\sum_{t=1}^T\\mathbb{1}(\\text{Token }t\\text{ selects Expert }i)\n\\end{aligned}$$  \n\n$$\\begin{aligned}\nP_{i}& =\\frac1T\\sum_{t=1}^Ts_{i,t} \n\\end{aligned}$$  \n\n$\\alpha_1$ expert-level balance factorTtoken  \n\n3Device-Level Balance Loss  \n\nD$\\{\\mathcal{E}_1,\\mathcal{E}_2,...,\\mathcal{E}_D\\}$  \n\n$$\\mathcal{L}_\\mathrm{DevBal}=\\alpha_2\\sum_{i=1}^Df_i^{\\prime}P_i^{\\prime}$$  \n\n$$f_i'=\\frac1{|\\mathcal{E}_i|}\\sum_{j\\in\\mathcal{E}_i}f_j$$  \n\n$$P_i'=\\sum_{j\\in\\mathcal{E}_i}P_j$$  \n\n$\\alpha_2$ device-level balance factor  \n\n4Communication Balance Loss  \n\ntokentargetdevicedevicetoken  \n\ncommunication balance loss  \n\n$$\\mathcal{L}_{\\mathrm{CommBal}}=\\alpha_3\\sum_{i=1}^Df_i^{\\prime\\prime}P_i^{\\prime\\prime}$$  \n\n$$f_i^{\\prime\\prime}=\\frac D{MT}\\sum_{t=1}^T1(\\text{Token t is sent to Device i})$$  \n\n$$P_i''=\\sum_{j\\in\\mathcal{E}_i}P_j$$  \n\n$\\alpha_3$ communication balance factor  \n\n5Token-Dropping Strategy  \n\nlossdevice-level token-dropping strategydevicecapacitybatchdevicetokendevicetokendrop  \n\nsequence10%sequencedroptoken  \n\ndevice  \n\n#   \n\nDeepSeek-V2DeepSeek 67BtokenizerBBPE100k  \n\n8.1T12%  \n\n##   \n\n1  \n\n- layer num = 60  \n- hidden size = 5120  \n- initialization standard deviation = 0.006  \n- attention head = 128attention head size = 128  \n- KV $d_c=512$  \n- Q $d_c'=1536$  \n- decoupled queries and key per head dimension = 64  \n- 2 + 6/160  \n-  = 1536  \n- 236B21B  \n\n2  \n\n- AdamWbeta_1 = 0.9beta_2 = 0.95weight_decay = 0.1  \n- lr schedulerwarmup-and-step-decaywarmup = 2k steplr = 2.4E-460%90%lr0.316  \n- gradient clipping norm = 1.0  \n- batch size scheduling strategy225Bbatch size23049216  \n- maximum sequence length = 4k  \n- $\\alpha_1=0.003$$\\alpha_2=0.05$$\\alpha_3=0.02$  \n\n##   \n\n $k_t^R$ YaRN4k128kYaRN  \n- s = 40  \n-  = 1  \n-  = 32  \n- target maximum context length = 160k  \n\nYaRNlength scaling factor $\\sqrt{t}=0.0707\\ln s+1$  \n\n32kbatch size = 5761000  \n\n{% asset_img needle.png  %}  \n\n##   \n\nDeepSeek-V2base  \n\n{% asset_img pt_eval.png  %}  \n\nDeepSeek-V270Bdense  \n\n##   \n\nSFT1.5M1.2Mhelpfulness0.3Msafety  \n\n  \n- epoch = 2  \n- lr = 5e-6  \n  \nSFTDeepSeek-V2GRPO  \n\n  \n\n{% asset_img align_eval.png  %}  \n\n# DeepSeek-V2-Lite\n\nDeepSeek-V2-Lite  \n\n  \n- layer num = 27  \n- hidden size = 2048  \n- initialization standard deviation = 0.006  \n- attention head = 16attention head size = 128  \n- KV $d_c=512$  \n- Q  \n- decoupled queries and key per head dimension = 64  \n- 2 + 6/64  \n- MoE  \n-  = 1408  \n- 15.7B2.4B  \n\n  \n- AdamWbeta_1 = 0.9beta_2 = 0.95weight_decay = 0.1  \n- lr schedulerwarmup-and-step-decaywarmup = 2k steplr = 4.2E-460%90%lr0.316  \n- gradient clipping norm = 1.0  \n- constant batch size = 4608  \n- maximum sequence length = 4k  \n- $\\alpha_1=0.003$loss  \n-  = 5.7T  \n\n{% asset_img lite_eval_1.png  %}  \n\n{% asset_img lite_eval_2.png  %}  \n\n#   \n\n- MLADeepSeek-V2  \n- MoEpopularMoE  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model https://arxiv.org/abs/2405.04434  \n2MHAMQAGQAMLA https://kexue.fm/archives/10091  \n","slug":"cs/nlp/2024/07/DeepSeek-V2MLA","published":1,"updated":"2024-07-13T05:44:27.012Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmz00220p4k0zzh79l1","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>DeepSeek-V2</p>\n<p>DeepSeek-V2MLA</p>\n<p>MLADeepSeek-V2</p>\n<p>DeepSeek-V2236BDeepSeek-V2-Lite15.7B</p>\n<h1 id=\"\"></h1>\n<p>DeepSeek-V2<br>\n- 236B21B<br>\n- 128k<br>\n- DeepSeek-67BDeepSeek-V242.5%93.3%KV\ncachethroughput5.76</p>\n<p>DeepSeek-V2MMLU</p>\n<img src=\"/83c49df0/intro.png\" class title=\"DeepSeek-V2\">\n<p>DeepSeek-V270B\ndense</p>\n<p>DeepSeek-V2</p>\n<img src=\"/83c49df0/model.png\" class title=\"\">\n<p>V1V2MoEfine-grained expertshared\nexpertDeepSeekMoE<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a>V2Multi-Head\nLatent AttentionMLA</p>\n<h2 id=\"mla\">MLA</h2>\n<p>MLADeepSeek-V2KV cache</p>\n<p>KV cacheMHA/GQA/MQA<a href=\"https://zhuanlan.zhihu.com/p/686149289\">Attention:MHA,MQAGQA</a></p>\n<p>1MHA</p>\n<p>MHA <span class=\"math inline\">\\(n_h\\)</span>\n<span class=\"math inline\">\\(d_h\\)</span>\n<span class=\"math inline\">\\(\\mathbf{h}_{t}\\in\\mathbb{R}^{d}\\)</span>\nttoken</p>\n<p>MHA <span class=\"math inline\">\\(W^{Q},W^{K},W^{V}\\in\\mathbb{R}^{d_{h}n_{h}\\times\nd}\\)</span> <span class=\"math inline\">\\(\\mathbf{q}_t,\\mathbf{k}_t,\\mathbf{v}_t\\in\\mathbb{R}^{d_hn_h}\\)</span></p>\n<p><span class=\"math display\">\\[\\mathbf{q}_t=W^Q\\mathbf{h}_t\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{k}_t=W^K\\mathbf{h}_t\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{v}_t=W^V\\mathbf{h}_t\\]</span></p>\n<p> <span class=\"math inline\">\\(\\mathbf{q}_t,\\mathbf{k}_t,\\mathbf{v}_t\\)</span>\n <span class=\"math inline\">\\(n_h\\)</span>\n</p>\n<p><span class=\"math display\">\\[[\\mathbf{q}_{t,1};\\mathbf{q}_{t,2};...;\\mathbf{q}_{t,n_{h}}]=\\mathbf{q}_{t}\\]</span></p>\n<p><span class=\"math display\">\\[[\\mathbf{k}_{t,1};\\mathbf{k}_{t,2};...;\\mathbf{k}_{t,n_{h}}]=\\mathbf{k}_{t}\\]</span></p>\n<p><span class=\"math display\">\\[[\\mathbf{v}_{t,1};\\mathbf{v}_{t,2};...;\\mathbf{v}_{t,n_{h}}]=\\mathbf{v}_{t}\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{o}_{t,i}=\\sum_{j=1}^t\\mathrm{Softmax}_j(\\frac{\\mathbf{q}_{t,i}^T\\mathbf{k}_{j,i}}{\\sqrt{d_h}})\\mathbf{v}_{j,i}\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{u}_t=W^O[\\mathbf{o}_{t,1};\\mathbf{o}_{t,2};...;\\mathbf{o}_{t,n_h}]\\]</span></p>\n<p> <span class=\"math inline\">\\(\\mathbf{q}_{t,i},\\mathbf{k}_{t,i},\\mathbf{v}_{t,i}\\in\\mathbb{R}^{d_{h}}\\)</span><span class=\"math inline\">\\(W^O\\in\\mathbb{R}^{d\\times d_hn_h}\\)</span></p>\n<p>KVtoken\n<span class=\"math inline\">\\(2{n}_{h}{d}_{h}\\)</span> </p>\n<p>GQA/MQAKVKV</p>\n<img src=\"/83c49df0/GQA.png\" class title=\"GQA\">\n<p>MQA1GQAtoken\n<span class=\"math inline\">\\(2{d}_{h}\\)</span>MHA1~2KVGQA/MQA</p>\n<p>DeepSeek-V21.33T\ntokenMHAGQAMQA7B4benchmark</p>\n<img src=\"/83c49df0/GQA_compare_MHA.png\" class title=\"MHA&#x2F;GQA&#x2F;MQA\">\n<p>MHAMQAGQA</p>\n<p>2MLA</p>\n<p>MLAKVlow-rank joint compressionKV\ncacheKV</p>\n<img src=\"/83c49df0/MLA.png\" class title=\"MLA\">\n<p>MLA</p>\n<p>MHAKV <span class=\"math inline\">\\(h_t\\)</span>\nMLAKVdown-projection\nmatrixup-projection matrices</p>\n<p><span class=\"math display\">\\[\\mathbf{c}_t^{KV}=W^{DKV}\\mathbf{h}_t\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{k}_t^C=W^{UK}\\mathbf{c}_t^{KV}\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{v}_t^C=W^{UV}\\mathbf{c}_t^{KV}\\]</span></p>\n<p><span class=\"math inline\">\\(\\mathfrak{c}_t^{KV}\\in\\mathbb{R}^{d_c}\\)</span>\nKVcompressed latent vector</p>\n<p>MHA <span class=\"math inline\">\\(W^{K},W^{V}\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\mathbf{k}_t=W^K\\mathbf{h}_t\\rightarrow\\mathbf{k}_tW^{UK}W^{DKV}\\mathbf{h}_t\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{v}_t=W^V\\mathbf{h}_t\\rightarrow\\mathbf{k}_tW^{UV}W^{DKV}\\mathbf{h}_t\\]</span></p>\n<p><span class=\"math inline\">\\(d_c\\)</span> KV <span class=\"math inline\">\\(d_c\\ll\nd_hn_h\\)</span></p>\n<p>attentionqkv</p>\n<p><span class=\"math display\">\\[\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{Q^TK}{\\sqrt{d}})V\\]</span></p>\n<p> <span class=\"math inline\">\\(Q^TK=H^T(W^Q)^TW^{UK}C\\)</span>\n <span class=\"math inline\">\\(W^{UK}\\)</span>  <span class=\"math inline\">\\(W^{Q}\\)</span>\nK <span class=\"math inline\">\\(W^Q\\)</span> shapeC <span class=\"math inline\">\\(W^{UV}\\)</span>  <span class=\"math inline\">\\(W^{O}\\)</span></p>\n<p>DeepSeek-V2Qlow-rank\ncompressionKV</p>\n<p><span class=\"math display\">\\[\\mathbf{c}_t^Q=W^{DQ}\\mathbf{h}_t,\\\\\\mathbf{q}_t^C=W^{UQ}\\mathbf{c}_t^Q,\\]</span></p>\n<p>Qactivationactivation<a href=\"https://kexue.fm/archives/10091\">MHAMQAGQAMLA</a>Q</p>\n<p>3RoPE</p>\n<p>MLAoverheadup-projection\nmatrices</p>\n<p>DeepSeek-V2RoPERoPEQK<a href=\"https://zhuanlan.zhihu.com/p/684072868\">LLM:RoPE</a></p>\n<p>MLAKRoPElatent\nvectorKVMLA</p>\n<p>DeepSeek-V2decoupled\nRoPEmulti-head queries <span class=\"math inline\">\\(\\mathbf{q}_{t,i}^R\\in\\mathbb{R}^{d_h^R}\\)</span>\nshared key <span class=\"math inline\">\\(\\mathbf{k}_t^R\\in\\mathbb{R}^{d_h^R}\\)</span>\nRoPE<span class=\"math inline\">\\(d_h^R\\)</span>\ndecoupled queries</p>\n<p>qkRoPEattention</p>\n<p>MLA</p>\n<img src=\"/83c49df0/MLA_formula.png\" class title=\"MLA\">\n<p></p>\n<p>MLA2.5GQA</p>\n<img src=\"/83c49df0/MLA_cache.png\" class title=\"MLA\">\n<p>DeepSeek-V2MLAMHA16B1.33T\ntoken250B420B token</p>\n<img src=\"/83c49df0/MLA_perf.png\" class title=\"MLA\">\n<p>4benchmarkMLAMHAKV\ncacheMLA</p>\n<h2 id=\"\"></h2>\n<p>MoE</p>\n<p>1Device-Limited Routing</p>\n<p>tokentarget\nexpertdevicefine-grained\nexpertdevice</p>\n<p>DeepSeek-V2target\nexpertdeviceMM3</p>\n<p>2Expert-Level Balance Loss</p>\n<p>DeepSeekMoE V1</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}_{\\mathrm{ExpBal}}&amp; =\\alpha_1\\sum_{i=1}^{N_r}f_iP_i\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nf_{i}&amp; =\\frac{N_{r}}{K_{r}T}\\sum_{t=1}^T\\mathbb{1}(\\text{Token\n}t\\text{ selects Expert }i)\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nP_{i}&amp; =\\frac1T\\sum_{t=1}^Ts_{i,t}\n\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(\\alpha_1\\)</span> expert-level balance\nfactorTtoken</p>\n<p>3Device-Level Balance Loss</p>\n<p>D<span class=\"math inline\">\\(\\{\\mathcal{E}_1,\\mathcal{E}_2,...,\\mathcal{E}_D\\}\\)</span></p>\n<p><span class=\"math display\">\\[\\mathcal{L}_\\mathrm{DevBal}=\\alpha_2\\sum_{i=1}^Df_i^{\\prime}P_i^{\\prime}\\]</span></p>\n<p><span class=\"math display\">\\[f_i&#39;=\\frac1{|\\mathcal{E}_i|}\\sum_{j\\in\\mathcal{E}_i}f_j\\]</span></p>\n<p><span class=\"math display\">\\[P_i&#39;=\\sum_{j\\in\\mathcal{E}_i}P_j\\]</span></p>\n<p><span class=\"math inline\">\\(\\alpha_2\\)</span> device-level balance\nfactor</p>\n<p>4Communication Balance Loss</p>\n<p>tokentargetdevicedevicetoken</p>\n<p>communication balance loss</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\mathrm{CommBal}}=\\alpha_3\\sum_{i=1}^Df_i^{\\prime\\prime}P_i^{\\prime\\prime}\\]</span></p>\n<p><span class=\"math display\">\\[f_i^{\\prime\\prime}=\\frac\nD{MT}\\sum_{t=1}^T1(\\text{Token t is sent to Device i})\\]</span></p>\n<p><span class=\"math display\">\\[P_i&#39;&#39;=\\sum_{j\\in\\mathcal{E}_i}P_j\\]</span></p>\n<p><span class=\"math inline\">\\(\\alpha_3\\)</span> communication balance\nfactor</p>\n<p>5Token-Dropping Strategy</p>\n<p>lossdevice-level\ntoken-dropping\nstrategydevicecapacitybatchdevicetokendevicetokendrop</p>\n<p>sequence10%sequencedroptoken</p>\n<p>device</p>\n<h1 id=\"\"></h1>\n<p>DeepSeek-V2DeepSeek\n67BtokenizerBBPE100k</p>\n<p>8.1T12%</p>\n<h2 id=\"\"></h2>\n<p>1</p>\n<ul>\n<li>layer num = 60<br>\n</li>\n<li>hidden size = 5120<br>\n</li>\n<li>initialization standard deviation = 0.006<br>\n</li>\n<li>attention head = 128attention head size = 128<br>\n</li>\n<li>KV <span class=\"math inline\">\\(d_c=512\\)</span><br>\n</li>\n<li>Q <span class=\"math inline\">\\(d_c&#39;=1536\\)</span><br>\n</li>\n<li>decoupled queries and key per head dimension = 64<br>\n</li>\n<li>2 + 6/160<br>\n</li>\n<li> = 1536<br>\n</li>\n<li>236B21B</li>\n</ul>\n<p>2</p>\n<ul>\n<li>AdamWbeta_1 = 0.9beta_2 = 0.95weight_decay = 0.1<br>\n</li>\n<li>lr schedulerwarmup-and-step-decaywarmup = 2k steplr =\n2.4E-460%90%lr0.316<br>\n</li>\n<li>gradient clipping norm = 1.0<br>\n</li>\n<li>batch size scheduling strategy225Bbatch\nsize23049216<br>\n</li>\n<li>maximum sequence length = 4k<br>\n</li>\n<li><span class=\"math inline\">\\(\\alpha_1=0.003\\)</span><span class=\"math inline\">\\(\\alpha_2=0.05\\)</span><span class=\"math inline\">\\(\\alpha_3=0.02\\)</span></li>\n</ul>\n<h2 id=\"\"></h2>\n<p> <span class=\"math inline\">\\(k_t^R\\)</span>\nYaRN4k128kYaRN<br>\n- s = 40<br>\n-  = 1<br>\n-  = 32<br>\n- target maximum context length = 160k</p>\n<p>YaRNlength scaling\nfactor <span class=\"math inline\">\\(\\sqrt{t}=0.0707\\ln\ns+1\\)</span></p>\n<p>32kbatch size =\n5761000</p>\n<img src=\"/83c49df0/needle.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p>DeepSeek-V2base</p>\n<img src=\"/83c49df0/pt_eval.png\" class title=\"\">\n<p>DeepSeek-V270Bdense</p>\n<h2 id=\"\"></h2>\n<p>SFT1.5M1.2Mhelpfulness0.3Msafety</p>\n<p><br>\n- epoch = 2<br>\n- lr = 5e-6</p>\n<p>SFTDeepSeek-V2GRPO</p>\n<p></p>\n<img src=\"/83c49df0/align_eval.png\" class title=\"\">\n<h1 id=\"deepseek-v2-lite\">DeepSeek-V2-Lite</h1>\n<p>DeepSeek-V2-Lite</p>\n<p><br>\n- layer num = 27<br>\n- hidden size = 2048<br>\n- initialization standard deviation = 0.006<br>\n- attention head = 16attention head size = 128<br>\n- KV <span class=\"math inline\">\\(d_c=512\\)</span><br>\n- Q<br>\n- decoupled queries and key per head dimension = 64<br>\n- 2 + 6/64<br>\n- MoE<br>\n-  = 1408<br>\n- 15.7B2.4B</p>\n<p><br>\n- AdamWbeta_1 = 0.9beta_2 = 0.95weight_decay = 0.1<br>\n- lr schedulerwarmup-and-step-decaywarmup = 2k steplr =\n4.2E-460%90%lr0.316<br>\n- gradient clipping norm = 1.0<br>\n- constant batch size = 4608<br>\n- maximum sequence length = 4k<br>\n- <span class=\"math inline\">\\(\\alpha_1=0.003\\)</span>loss<br>\n-  = 5.7T</p>\n<img src=\"/83c49df0/lite_eval_1.png\" class title=\"\">\n<img src=\"/83c49df0/lite_eval_2.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<ul>\n<li>MLADeepSeek-V2<br>\n</li>\n<li>MoEpopularMoE</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1DeepSeek-V2: A Strong, Economical, and Efficient\nMixture-of-Experts Language Model https://arxiv.org/abs/2405.04434<br>\n2MHAMQAGQAMLA\nhttps://kexue.fm/archives/10091</p>\n","length":8003,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>DeepSeek-V2</p>\n<p>DeepSeek-V2MLA</p>\n<p>MLADeepSeek-V2</p>\n<p>DeepSeek-V2236BDeepSeek-V2-Lite15.7B</p>\n<h1 id=\"\"></h1>\n<p>DeepSeek-V2<br>\n- 236B21B<br>\n- 128k<br>\n- DeepSeek-67BDeepSeek-V242.5%93.3%KV\ncachethroughput5.76</p>\n<p>DeepSeek-V2MMLU</p>\n<img src=\"/83c49df0/intro.png\" class title=\"DeepSeek-V2\">\n<p>DeepSeek-V270B\ndense</p>\n<p>DeepSeek-V2</p>\n<img src=\"/83c49df0/model.png\" class title=\"\">\n<p>V1V2MoEfine-grained expertshared\nexpertDeepSeekMoE<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a>V2Multi-Head\nLatent AttentionMLA</p>\n<h2 id=\"mla\">MLA</h2>\n<p>MLADeepSeek-V2KV cache</p>\n<p>KV cacheMHA/GQA/MQA<a href=\"https://zhuanlan.zhihu.com/p/686149289\">Attention:MHA,MQAGQA</a></p>\n<p>1MHA</p>\n<p>MHA <span class=\"math inline\">\\(n_h\\)</span>\n<span class=\"math inline\">\\(d_h\\)</span>\n<span class=\"math inline\">\\(\\mathbf{h}_{t}\\in\\mathbb{R}^{d}\\)</span>\nttoken</p>\n<p>MHA <span class=\"math inline\">\\(W^{Q},W^{K},W^{V}\\in\\mathbb{R}^{d_{h}n_{h}\\times\nd}\\)</span> <span class=\"math inline\">\\(\\mathbf{q}_t,\\mathbf{k}_t,\\mathbf{v}_t\\in\\mathbb{R}^{d_hn_h}\\)</span></p>\n<p><span class=\"math display\">\\[\\mathbf{q}_t=W^Q\\mathbf{h}_t\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{k}_t=W^K\\mathbf{h}_t\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{v}_t=W^V\\mathbf{h}_t\\]</span></p>\n<p> <span class=\"math inline\">\\(\\mathbf{q}_t,\\mathbf{k}_t,\\mathbf{v}_t\\)</span>\n <span class=\"math inline\">\\(n_h\\)</span>\n</p>\n<p><span class=\"math display\">\\[[\\mathbf{q}_{t,1};\\mathbf{q}_{t,2};...;\\mathbf{q}_{t,n_{h}}]=\\mathbf{q}_{t}\\]</span></p>\n<p><span class=\"math display\">\\[[\\mathbf{k}_{t,1};\\mathbf{k}_{t,2};...;\\mathbf{k}_{t,n_{h}}]=\\mathbf{k}_{t}\\]</span></p>\n<p><span class=\"math display\">\\[[\\mathbf{v}_{t,1};\\mathbf{v}_{t,2};...;\\mathbf{v}_{t,n_{h}}]=\\mathbf{v}_{t}\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{o}_{t,i}=\\sum_{j=1}^t\\mathrm{Softmax}_j(\\frac{\\mathbf{q}_{t,i}^T\\mathbf{k}_{j,i}}{\\sqrt{d_h}})\\mathbf{v}_{j,i}\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{u}_t=W^O[\\mathbf{o}_{t,1};\\mathbf{o}_{t,2};...;\\mathbf{o}_{t,n_h}]\\]</span></p>\n<p> <span class=\"math inline\">\\(\\mathbf{q}_{t,i},\\mathbf{k}_{t,i},\\mathbf{v}_{t,i}\\in\\mathbb{R}^{d_{h}}\\)</span><span class=\"math inline\">\\(W^O\\in\\mathbb{R}^{d\\times d_hn_h}\\)</span></p>\n<p>KVtoken\n<span class=\"math inline\">\\(2{n}_{h}{d}_{h}\\)</span> </p>\n<p>GQA/MQAKVKV</p>\n<img src=\"/83c49df0/GQA.png\" class title=\"GQA\">\n<p>MQA1GQAtoken\n<span class=\"math inline\">\\(2{d}_{h}\\)</span>MHA1~2KVGQA/MQA</p>\n<p>DeepSeek-V21.33T\ntokenMHAGQAMQA7B4benchmark</p>\n<img src=\"/83c49df0/GQA_compare_MHA.png\" class title=\"MHA&#x2F;GQA&#x2F;MQA\">\n<p>MHAMQAGQA</p>\n<p>2MLA</p>\n<p>MLAKVlow-rank joint compressionKV\ncacheKV</p>\n<img src=\"/83c49df0/MLA.png\" class title=\"MLA\">\n<p>MLA</p>\n<p>MHAKV <span class=\"math inline\">\\(h_t\\)</span>\nMLAKVdown-projection\nmatrixup-projection matrices</p>\n<p><span class=\"math display\">\\[\\mathbf{c}_t^{KV}=W^{DKV}\\mathbf{h}_t\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{k}_t^C=W^{UK}\\mathbf{c}_t^{KV}\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{v}_t^C=W^{UV}\\mathbf{c}_t^{KV}\\]</span></p>\n<p><span class=\"math inline\">\\(\\mathfrak{c}_t^{KV}\\in\\mathbb{R}^{d_c}\\)</span>\nKVcompressed latent vector</p>\n<p>MHA <span class=\"math inline\">\\(W^{K},W^{V}\\)</span>\n</p>\n<p><span class=\"math display\">\\[\\mathbf{k}_t=W^K\\mathbf{h}_t\\rightarrow\\mathbf{k}_tW^{UK}W^{DKV}\\mathbf{h}_t\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{v}_t=W^V\\mathbf{h}_t\\rightarrow\\mathbf{k}_tW^{UV}W^{DKV}\\mathbf{h}_t\\]</span></p>\n<p><span class=\"math inline\">\\(d_c\\)</span> KV <span class=\"math inline\">\\(d_c\\ll\nd_hn_h\\)</span></p>\n<p>attentionqkv</p>\n<p><span class=\"math display\">\\[\\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{Q^TK}{\\sqrt{d}})V\\]</span></p>\n<p> <span class=\"math inline\">\\(Q^TK=H^T(W^Q)^TW^{UK}C\\)</span>\n <span class=\"math inline\">\\(W^{UK}\\)</span>  <span class=\"math inline\">\\(W^{Q}\\)</span>\nK <span class=\"math inline\">\\(W^Q\\)</span> shapeC <span class=\"math inline\">\\(W^{UV}\\)</span>  <span class=\"math inline\">\\(W^{O}\\)</span></p>\n<p>DeepSeek-V2Qlow-rank\ncompressionKV</p>\n<p><span class=\"math display\">\\[\\mathbf{c}_t^Q=W^{DQ}\\mathbf{h}_t,\\\\\\mathbf{q}_t^C=W^{UQ}\\mathbf{c}_t^Q,\\]</span></p>\n<p>Qactivationactivation<a href=\"https://kexue.fm/archives/10091\">MHAMQAGQAMLA</a>Q</p>\n<p>3RoPE</p>\n<p>MLAoverheadup-projection\nmatrices</p>\n<p>DeepSeek-V2RoPERoPEQK<a href=\"https://zhuanlan.zhihu.com/p/684072868\">LLM:RoPE</a></p>\n<p>MLAKRoPElatent\nvectorKVMLA</p>\n<p>DeepSeek-V2decoupled\nRoPEmulti-head queries <span class=\"math inline\">\\(\\mathbf{q}_{t,i}^R\\in\\mathbb{R}^{d_h^R}\\)</span>\nshared key <span class=\"math inline\">\\(\\mathbf{k}_t^R\\in\\mathbb{R}^{d_h^R}\\)</span>\nRoPE<span class=\"math inline\">\\(d_h^R\\)</span>\ndecoupled queries</p>\n<p>qkRoPEattention</p>\n<p>MLA</p>\n<img src=\"/83c49df0/MLA_formula.png\" class title=\"MLA\">\n<p></p>\n<p>MLA2.5GQA</p>\n<img src=\"/83c49df0/MLA_cache.png\" class title=\"MLA\">\n<p>DeepSeek-V2MLAMHA16B1.33T\ntoken250B420B token</p>\n<img src=\"/83c49df0/MLA_perf.png\" class title=\"MLA\">\n<p>4benchmarkMLAMHAKV\ncacheMLA</p>\n<h2 id=\"\"></h2>\n<p>MoE</p>\n<p>1Device-Limited Routing</p>\n<p>tokentarget\nexpertdevicefine-grained\nexpertdevice</p>\n<p>DeepSeek-V2target\nexpertdeviceMM3</p>\n<p>2Expert-Level Balance Loss</p>\n<p>DeepSeekMoE V1</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\n\\mathcal{L}_{\\mathrm{ExpBal}}&amp; =\\alpha_1\\sum_{i=1}^{N_r}f_iP_i\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nf_{i}&amp; =\\frac{N_{r}}{K_{r}T}\\sum_{t=1}^T\\mathbb{1}(\\text{Token\n}t\\text{ selects Expert }i)\n\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nP_{i}&amp; =\\frac1T\\sum_{t=1}^Ts_{i,t}\n\\end{aligned}\\]</span></p>\n<p><span class=\"math inline\">\\(\\alpha_1\\)</span> expert-level balance\nfactorTtoken</p>\n<p>3Device-Level Balance Loss</p>\n<p>D<span class=\"math inline\">\\(\\{\\mathcal{E}_1,\\mathcal{E}_2,...,\\mathcal{E}_D\\}\\)</span></p>\n<p><span class=\"math display\">\\[\\mathcal{L}_\\mathrm{DevBal}=\\alpha_2\\sum_{i=1}^Df_i^{\\prime}P_i^{\\prime}\\]</span></p>\n<p><span class=\"math display\">\\[f_i&#39;=\\frac1{|\\mathcal{E}_i|}\\sum_{j\\in\\mathcal{E}_i}f_j\\]</span></p>\n<p><span class=\"math display\">\\[P_i&#39;=\\sum_{j\\in\\mathcal{E}_i}P_j\\]</span></p>\n<p><span class=\"math inline\">\\(\\alpha_2\\)</span> device-level balance\nfactor</p>\n<p>4Communication Balance Loss</p>\n<p>tokentargetdevicedevicetoken</p>\n<p>communication balance loss</p>\n<p><span class=\"math display\">\\[\\mathcal{L}_{\\mathrm{CommBal}}=\\alpha_3\\sum_{i=1}^Df_i^{\\prime\\prime}P_i^{\\prime\\prime}\\]</span></p>\n<p><span class=\"math display\">\\[f_i^{\\prime\\prime}=\\frac\nD{MT}\\sum_{t=1}^T1(\\text{Token t is sent to Device i})\\]</span></p>\n<p><span class=\"math display\">\\[P_i&#39;&#39;=\\sum_{j\\in\\mathcal{E}_i}P_j\\]</span></p>\n<p><span class=\"math inline\">\\(\\alpha_3\\)</span> communication balance\nfactor</p>\n<p>5Token-Dropping Strategy</p>\n<p>lossdevice-level\ntoken-dropping\nstrategydevicecapacitybatchdevicetokendevicetokendrop</p>\n<p>sequence10%sequencedroptoken</p>\n<p>device</p>\n<h1 id=\"\"></h1>\n<p>DeepSeek-V2DeepSeek\n67BtokenizerBBPE100k</p>\n<p>8.1T12%</p>\n<h2 id=\"\"></h2>\n<p>1</p>\n<ul>\n<li>layer num = 60<br>\n</li>\n<li>hidden size = 5120<br>\n</li>\n<li>initialization standard deviation = 0.006<br>\n</li>\n<li>attention head = 128attention head size = 128<br>\n</li>\n<li>KV <span class=\"math inline\">\\(d_c=512\\)</span><br>\n</li>\n<li>Q <span class=\"math inline\">\\(d_c&#39;=1536\\)</span><br>\n</li>\n<li>decoupled queries and key per head dimension = 64<br>\n</li>\n<li>2 + 6/160<br>\n</li>\n<li> = 1536<br>\n</li>\n<li>236B21B</li>\n</ul>\n<p>2</p>\n<ul>\n<li>AdamWbeta_1 = 0.9beta_2 = 0.95weight_decay = 0.1<br>\n</li>\n<li>lr schedulerwarmup-and-step-decaywarmup = 2k steplr =\n2.4E-460%90%lr0.316<br>\n</li>\n<li>gradient clipping norm = 1.0<br>\n</li>\n<li>batch size scheduling strategy225Bbatch\nsize23049216<br>\n</li>\n<li>maximum sequence length = 4k<br>\n</li>\n<li><span class=\"math inline\">\\(\\alpha_1=0.003\\)</span><span class=\"math inline\">\\(\\alpha_2=0.05\\)</span><span class=\"math inline\">\\(\\alpha_3=0.02\\)</span></li>\n</ul>\n<h2 id=\"\"></h2>\n<p> <span class=\"math inline\">\\(k_t^R\\)</span>\nYaRN4k128kYaRN<br>\n- s = 40<br>\n-  = 1<br>\n-  = 32<br>\n- target maximum context length = 160k</p>\n<p>YaRNlength scaling\nfactor <span class=\"math inline\">\\(\\sqrt{t}=0.0707\\ln\ns+1\\)</span></p>\n<p>32kbatch size =\n5761000</p>\n<img src=\"/83c49df0/needle.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p>DeepSeek-V2base</p>\n<img src=\"/83c49df0/pt_eval.png\" class title=\"\">\n<p>DeepSeek-V270Bdense</p>\n<h2 id=\"\"></h2>\n<p>SFT1.5M1.2Mhelpfulness0.3Msafety</p>\n<p><br>\n- epoch = 2<br>\n- lr = 5e-6</p>\n<p>SFTDeepSeek-V2GRPO</p>\n<p></p>\n<img src=\"/83c49df0/align_eval.png\" class title=\"\">\n<h1 id=\"deepseek-v2-lite\">DeepSeek-V2-Lite</h1>\n<p>DeepSeek-V2-Lite</p>\n<p><br>\n- layer num = 27<br>\n- hidden size = 2048<br>\n- initialization standard deviation = 0.006<br>\n- attention head = 16attention head size = 128<br>\n- KV <span class=\"math inline\">\\(d_c=512\\)</span><br>\n- Q<br>\n- decoupled queries and key per head dimension = 64<br>\n- 2 + 6/64<br>\n- MoE<br>\n-  = 1408<br>\n- 15.7B2.4B</p>\n<p><br>\n- AdamWbeta_1 = 0.9beta_2 = 0.95weight_decay = 0.1<br>\n- lr schedulerwarmup-and-step-decaywarmup = 2k steplr =\n4.2E-460%90%lr0.316<br>\n- gradient clipping norm = 1.0<br>\n- constant batch size = 4608<br>\n- maximum sequence length = 4k<br>\n- <span class=\"math inline\">\\(\\alpha_1=0.003\\)</span>loss<br>\n-  = 5.7T</p>\n<img src=\"/83c49df0/lite_eval_1.png\" class title=\"\">\n<img src=\"/83c49df0/lite_eval_2.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<ul>\n<li>MLADeepSeek-V2<br>\n</li>\n<li>MoEpopularMoE</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1DeepSeek-V2: A Strong, Economical, and Efficient\nMixture-of-Experts Language Model https://arxiv.org/abs/2405.04434<br>\n2MHAMQAGQAMLA\nhttps://kexue.fm/archives/10091</p>\n"},{"title":"Gemma2","abbrlink":"cf3f1f81","date":"2024-07-01T08:30:28.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nGoogleGemma22B9B27B  \n\n9B27Bhuggingfacebasefine-tuned2B  \n\n{% asset_img intro.png Gemma2 %}  \n\n  \n\n#   \n\n3  \n\n{% asset_img model.png  %}  \n\nGemma1  \n- decocer-only  \n- RoPE  \n- context length = 8192  \n- GeGLU  \n\n  \n\n## sliding window attention  \n\nGemma2sliding window attentionsliding window4096  \n\nsliding window[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n\nsliding window  \n\nMistralsliding window attentionsliding window attention  \n\n## logit soft-capping  \n\nGemini 1.5Gemma2logit soft-capping  \n\nsoft-cappingtruncationlogits  \n\nlogits  \n\n```python  \nlogits = soft_cap  tanh(logits / soft_cap)  \n```  \n\nlogits(-soft_cap, +soft_cap)  \n\nsoft-cappingfinal layerattention layer9B27Bfinal layerattention layersoft_cap30.050.0  \n\nFlash Attention / SDPAsoft-cappingeager attentionSDPA  \n\nsoft-cappingsoft-capping  \n\n##   \n\nGemma2  \n\n1post-norm  pre-norm  RMSNorm  \n\n2group num = 2GQA  \n\n#   \n\n##   \n\n2B2B token9B8T token27B13Tdata mixtureGemma1  \n\nGemma2tokenizerGemma1GeminiBPE256k  \n\n## knowledge distillation  \n\nGemma2 27B2B9Bnext token prediction  \n\n$$\\min_{P_S}\\sum_x-P_T(x\\mid x_c)\\log P_S(x\\mid x_c)$$  \n\nteacher modeltokenvocabularysubset  \n\nSFTsynthetic datapromptteacherresponsedistillationZephyrOpenHermes  \n\ntrain-inference mismatchstudent model  \n\nmismatchGemma2On-policy distillation of language models: Learning from self-generated mistakeson-policy distillation  \n\nstudentpromptresponseteacherstudentresponseKL divergencetrain-inference mismatch  \n\nSFTRLHF  \n\npost-trainingtoken  \n\n{% asset_img format.png formatting %}  \n\n{% asset_img example.png formatting %}  \n\nGemma2Warp: On the benefits of weight averaged rewarded policiesmodel merging  \n\nBertExponential Moving Averagecheckpoint  \n\n#   \n\nGemma2  \n\n1distillation versus from scratch  \n\n  \n\n{% asset_img ablation_1.png  %}  \n\n2impact of distillation w.r.t. model size  \n\n7Bteacher modelstudent  \n\n{% asset_img ablation_2.png  %}  \n\n3GQA versus MHA  \n\n9BGQAMHAGQA  \n\n{% asset_img ablation_3.png  %}  \n\n4wide versus deep  \n\n9B9B  \n\n{% asset_img ablation_4.png  %}  \n\n5changing sliding window size  \n\nsliding windowppl  \n\n{% asset_img ablation_5.png  %}  \n\n6impact of formatting  \n\nMistralGemma2  \n\n{% asset_img ablation_6.png  %}  \n\n#   \n\nbenchmark  \n\n{% asset_img eval1.png eval %}  \n\n{% asset_img eval2.png eval %}  \n\n#   \n\nGemma2soft-cappint/  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1Gemma 2: Improving Open Language Models\nat a Practical Size https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf  \n2https://huggingface.co/blog/gemma2  \n3:sliding window attention http://www.linsight.cn/c61d17e3.html  \n","source":"_posts/cs/nlp/2024/07/Gemma2.md","raw":"---\ntitle: Gemma2\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - Gemma2\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: cf3f1f81\ndate: 2024-07-01 16:30:28\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nGoogleGemma22B9B27B  \n\n9B27Bhuggingfacebasefine-tuned2B  \n\n{% asset_img intro.png Gemma2 %}  \n\n  \n\n#   \n\n3  \n\n{% asset_img model.png  %}  \n\nGemma1  \n- decocer-only  \n- RoPE  \n- context length = 8192  \n- GeGLU  \n\n  \n\n## sliding window attention  \n\nGemma2sliding window attentionsliding window4096  \n\nsliding window[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n\nsliding window  \n\nMistralsliding window attentionsliding window attention  \n\n## logit soft-capping  \n\nGemini 1.5Gemma2logit soft-capping  \n\nsoft-cappingtruncationlogits  \n\nlogits  \n\n```python  \nlogits = soft_cap  tanh(logits / soft_cap)  \n```  \n\nlogits(-soft_cap, +soft_cap)  \n\nsoft-cappingfinal layerattention layer9B27Bfinal layerattention layersoft_cap30.050.0  \n\nFlash Attention / SDPAsoft-cappingeager attentionSDPA  \n\nsoft-cappingsoft-capping  \n\n##   \n\nGemma2  \n\n1post-norm  pre-norm  RMSNorm  \n\n2group num = 2GQA  \n\n#   \n\n##   \n\n2B2B token9B8T token27B13Tdata mixtureGemma1  \n\nGemma2tokenizerGemma1GeminiBPE256k  \n\n## knowledge distillation  \n\nGemma2 27B2B9Bnext token prediction  \n\n$$\\min_{P_S}\\sum_x-P_T(x\\mid x_c)\\log P_S(x\\mid x_c)$$  \n\nteacher modeltokenvocabularysubset  \n\nSFTsynthetic datapromptteacherresponsedistillationZephyrOpenHermes  \n\ntrain-inference mismatchstudent model  \n\nmismatchGemma2On-policy distillation of language models: Learning from self-generated mistakeson-policy distillation  \n\nstudentpromptresponseteacherstudentresponseKL divergencetrain-inference mismatch  \n\nSFTRLHF  \n\npost-trainingtoken  \n\n{% asset_img format.png formatting %}  \n\n{% asset_img example.png formatting %}  \n\nGemma2Warp: On the benefits of weight averaged rewarded policiesmodel merging  \n\nBertExponential Moving Averagecheckpoint  \n\n#   \n\nGemma2  \n\n1distillation versus from scratch  \n\n  \n\n{% asset_img ablation_1.png  %}  \n\n2impact of distillation w.r.t. model size  \n\n7Bteacher modelstudent  \n\n{% asset_img ablation_2.png  %}  \n\n3GQA versus MHA  \n\n9BGQAMHAGQA  \n\n{% asset_img ablation_3.png  %}  \n\n4wide versus deep  \n\n9B9B  \n\n{% asset_img ablation_4.png  %}  \n\n5changing sliding window size  \n\nsliding windowppl  \n\n{% asset_img ablation_5.png  %}  \n\n6impact of formatting  \n\nMistralGemma2  \n\n{% asset_img ablation_6.png  %}  \n\n#   \n\nbenchmark  \n\n{% asset_img eval1.png eval %}  \n\n{% asset_img eval2.png eval %}  \n\n#   \n\nGemma2soft-cappint/  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1Gemma 2: Improving Open Language Models\nat a Practical Size https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf  \n2https://huggingface.co/blog/gemma2  \n3:sliding window attention http://www.linsight.cn/c61d17e3.html  \n","slug":"cs/nlp/2024/07/Gemma2","published":1,"updated":"2024-07-02T12:25:08.361Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmz00250p4k2ui96aqa","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>GoogleGemma22B9B27B</p>\n<p>9B27Bhuggingfacebasefine-tuned2B</p>\n<img src=\"/cf3f1f81/intro.png\" class title=\"Gemma2\">\n<p></p>\n<h1 id=\"\"></h1>\n<p>3</p>\n<img src=\"/cf3f1f81/model.png\" class title=\"\">\n<p>Gemma1<br>\n- decocer-only<br>\n- RoPE<br>\n- context length = 8192<br>\n- GeGLU</p>\n<p></p>\n<h2 id=\"sliding-window-attention\">sliding window attention</h2>\n<p>Gemma2sliding window attentionsliding\nwindow4096</p>\n<p>sliding window<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p>sliding\nwindow</p>\n<p>Mistralsliding window\nattentionsliding window\nattention</p>\n<h2 id=\"logit-soft-capping\">logit soft-capping</h2>\n<p>Gemini 1.5Gemma2logit soft-capping</p>\n<p>soft-cappingtruncationlogits</p>\n<p>logits</p>\n<div class=\"sourceCode\" id=\"cb1\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span id=\"cb1-1\"><a href=\"#cb1-1\" aria-hidden=\"true\" tabindex=\"-1\"></a>logits <span class=\"op\">=</span> soft_cap  tanh(logits <span class=\"op\">/</span> soft_cap)  </span></code></pre></div>\n<p>logits(-soft_cap,\n+soft_cap)</p>\n<p>soft-cappingfinal layerattention\nlayer9B27Bfinal layerattention\nlayersoft_cap30.050.0</p>\n<p>Flash Attention /\nSDPAsoft-cappingeager\nattentionSDPA</p>\n<p>soft-cappingsoft-capping</p>\n<h2 id=\"\"></h2>\n<p>Gemma2</p>\n<p>1post-norm  pre-norm  RMSNorm</p>\n<p>2group num = 2GQA</p>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>2B2B token9B8T\ntoken27B13Tdata\nmixtureGemma1</p>\n<p>Gemma2tokenizerGemma1GeminiBPE256k</p>\n<h2 id=\"knowledge-distillation\">knowledge distillation</h2>\n<p>Gemma2 27B2B9Bnext token\nprediction</p>\n<p><span class=\"math display\">\\[\\min_{P_S}\\sum_x-P_T(x\\mid x_c)\\log\nP_S(x\\mid x_c)\\]</span></p>\n<p>teacher\nmodeltokenvocabularysubset</p>\n<p>SFTsynthetic\ndatapromptteacherresponsedistillationZephyrOpenHermes</p>\n<p>train-inference\nmismatchstudent model</p>\n<p>mismatchGemma2On-policy distillation of\nlanguage models: Learning from self-generated mistakeson-policy\ndistillation</p>\n<p>studentpromptresponseteacherstudentresponseKL\ndivergencetrain-inference mismatch</p>\n<p>SFTRLHF</p>\n<p>post-trainingtoken</p>\n<img src=\"/cf3f1f81/format.png\" class title=\"formatting\">\n<img src=\"/cf3f1f81/example.png\" class title=\"formatting\">\n<p>Gemma2Warp: On the benefits of weight averaged\nrewarded policiesmodel merging</p>\n<p>BertExponential Moving\nAveragecheckpoint</p>\n<h1 id=\"\"></h1>\n<p>Gemma2</p>\n<p>1distillation versus from scratch</p>\n<p></p>\n<img src=\"/cf3f1f81/ablation_1.png\" class title=\"\">\n<p>2impact of distillation w.r.t. model size</p>\n<p>7Bteacher\nmodelstudent</p>\n<img src=\"/cf3f1f81/ablation_2.png\" class title=\"\">\n<p>3GQA versus MHA</p>\n<p>9BGQAMHAGQA</p>\n<img src=\"/cf3f1f81/ablation_3.png\" class title=\"\">\n<p>4wide versus deep</p>\n<p>9B9B</p>\n<img src=\"/cf3f1f81/ablation_4.png\" class title=\"\">\n<p>5changing sliding window size</p>\n<p>sliding windowppl</p>\n<img src=\"/cf3f1f81/ablation_5.png\" class title=\"\">\n<p>6impact of formatting</p>\n<p>MistralGemma2</p>\n<img src=\"/cf3f1f81/ablation_6.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<p>benchmark</p>\n<img src=\"/cf3f1f81/eval1.png\" class title=\"eval\">\n<img src=\"/cf3f1f81/eval2.png\" class title=\"eval\">\n<h1 id=\"\"></h1>\n<p>Gemma2soft-cappint/</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Gemma 2: Improving Open Language Models at a Practical Size\nhttps://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf<br>\n2https://huggingface.co/blog/gemma2<br>\n3:sliding window attention\nhttp://www.linsight.cn/c61d17e3.html</p>\n","length":3377,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>GoogleGemma22B9B27B</p>\n<p>9B27Bhuggingfacebasefine-tuned2B</p>\n<img src=\"/cf3f1f81/intro.png\" class title=\"Gemma2\">\n<p></p>\n<h1 id=\"\"></h1>\n<p>3</p>\n<img src=\"/cf3f1f81/model.png\" class title=\"\">\n<p>Gemma1<br>\n- decocer-only<br>\n- RoPE<br>\n- context length = 8192<br>\n- GeGLU</p>\n<p></p>\n<h2 id=\"sliding-window-attention\">sliding window attention</h2>\n<p>Gemma2sliding window attentionsliding\nwindow4096</p>\n<p>sliding window<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a></p>\n<p>sliding\nwindow</p>\n<p>Mistralsliding window\nattentionsliding window\nattention</p>\n<h2 id=\"logit-soft-capping\">logit soft-capping</h2>\n<p>Gemini 1.5Gemma2logit soft-capping</p>\n<p>soft-cappingtruncationlogits</p>\n<p>logits</p>\n<div class=\"sourceCode\" id=\"cb1\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span id=\"cb1-1\"><a href=\"#cb1-1\" aria-hidden=\"true\" tabindex=\"-1\"></a>logits <span class=\"op\">=</span> soft_cap  tanh(logits <span class=\"op\">/</span> soft_cap)  </span></code></pre></div>\n<p>logits(-soft_cap,\n+soft_cap)</p>\n<p>soft-cappingfinal layerattention\nlayer9B27Bfinal layerattention\nlayersoft_cap30.050.0</p>\n<p>Flash Attention /\nSDPAsoft-cappingeager\nattentionSDPA</p>\n<p>soft-cappingsoft-capping</p>\n<h2 id=\"\"></h2>\n<p>Gemma2</p>\n<p>1post-norm  pre-norm  RMSNorm</p>\n<p>2group num = 2GQA</p>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>2B2B token9B8T\ntoken27B13Tdata\nmixtureGemma1</p>\n<p>Gemma2tokenizerGemma1GeminiBPE256k</p>\n<h2 id=\"knowledge-distillation\">knowledge distillation</h2>\n<p>Gemma2 27B2B9Bnext token\nprediction</p>\n<p><span class=\"math display\">\\[\\min_{P_S}\\sum_x-P_T(x\\mid x_c)\\log\nP_S(x\\mid x_c)\\]</span></p>\n<p>teacher\nmodeltokenvocabularysubset</p>\n<p>SFTsynthetic\ndatapromptteacherresponsedistillationZephyrOpenHermes</p>\n<p>train-inference\nmismatchstudent model</p>\n<p>mismatchGemma2On-policy distillation of\nlanguage models: Learning from self-generated mistakeson-policy\ndistillation</p>\n<p>studentpromptresponseteacherstudentresponseKL\ndivergencetrain-inference mismatch</p>\n<p>SFTRLHF</p>\n<p>post-trainingtoken</p>\n<img src=\"/cf3f1f81/format.png\" class title=\"formatting\">\n<img src=\"/cf3f1f81/example.png\" class title=\"formatting\">\n<p>Gemma2Warp: On the benefits of weight averaged\nrewarded policiesmodel merging</p>\n<p>BertExponential Moving\nAveragecheckpoint</p>\n<h1 id=\"\"></h1>\n<p>Gemma2</p>\n<p>1distillation versus from scratch</p>\n<p></p>\n<img src=\"/cf3f1f81/ablation_1.png\" class title=\"\">\n<p>2impact of distillation w.r.t. model size</p>\n<p>7Bteacher\nmodelstudent</p>\n<img src=\"/cf3f1f81/ablation_2.png\" class title=\"\">\n<p>3GQA versus MHA</p>\n<p>9BGQAMHAGQA</p>\n<img src=\"/cf3f1f81/ablation_3.png\" class title=\"\">\n<p>4wide versus deep</p>\n<p>9B9B</p>\n<img src=\"/cf3f1f81/ablation_4.png\" class title=\"\">\n<p>5changing sliding window size</p>\n<p>sliding windowppl</p>\n<img src=\"/cf3f1f81/ablation_5.png\" class title=\"\">\n<p>6impact of formatting</p>\n<p>MistralGemma2</p>\n<img src=\"/cf3f1f81/ablation_6.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<p>benchmark</p>\n<img src=\"/cf3f1f81/eval1.png\" class title=\"eval\">\n<img src=\"/cf3f1f81/eval2.png\" class title=\"eval\">\n<h1 id=\"\"></h1>\n<p>Gemma2soft-cappint/</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Gemma 2: Improving Open Language Models at a Practical Size\nhttps://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf<br>\n2https://huggingface.co/blog/gemma2<br>\n3:sliding window attention\nhttp://www.linsight.cn/c61d17e3.html</p>\n"},{"title":"Llama3.1--","abbrlink":"7d7294cb","date":"2024-07-25T14:15:40.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nLlama-3.1-405BGPT-4-0125Claude-3.5-SonnetGPT-4-OMNI8B70B  \n\n{% asset_img eval.png  %}  \n\nMeta100Llama-3  \n\nMistral Large V2Llama-3  \n\n# Llama-3  \n\nLlama-38B/70BLlama-3.18B/70/405B  \n\n{% asset_img model.png  %}  \n\nLlama-3.1-405B405B  \n\n## 405B  \n\n405B15.6T token128kLlama-2915.6T vs 1.8TLlama-2-70B50+  \n\n405B  15.6TMetascaling lawMetascaling lawcompute-optimal token  \n\n405BdenseMoEMetaMoEdenseMoETransformerpost-trainingsupervised finetuningSFTrejection samplingRSand direct preference optimizationDPOMeta  \n\n405B16k H100tensor parallelismpipeline parallelismcontext parallelismdata parallelism  \n\n##   \n\nLlama-3  \n- GQAKV cache  \n- document mask  \n- RoPEbase frequency500,000Effective long-context scaling of foundation models32,768  \n- 128k100ktiktoken28ktoken3.17-->3.94  \n\n  \n\n{% asset_img llama3.png  %}  \n\n## Scaling Laws  \n\nLLMscaling lawscaling law  \n- scaling lawnext-token predictionlossloss  \n- scaling lawcompute budgetscaling law  \n\nMetatwo-stagedownstream benchmark performace+  \n- compute-optimal modeldownstream tasknegative log-likelihoodFLOPs  \n- negative log-likelihoodtask accuracyscaling law modelsLlama-2FLOPs  \n\npre-training data mix  \n\n40M16BFLOPscompute  \n\n{% asset_img scaling_law_exp.png  %}  \n\nlrcompute budget250k4Mbatch size  \n\ncompute budget Coptimal number of training token $N^{\\star}(C)$   \n\n$$N^\\star(C)=AC^\\alpha $$  \n\n $(\\alpha,A)=(0.53,0.29)$ $3.8\\times10^{25}$ FLOPscompute budget402B16.55T token    \n\ncompute budgetIsoFLOPsrobust  \n\ncompute budgetbenchmarkNormalized NLL per CharFLOPsNormalized NLL per CharaccuracysigmoidFLOPsbenchmarkaccuracyARC Challenge  \n\n{% asset_img scaling_law.png  %}  \n\n405B  \n\n# Pre-Training  \n\n##   \n\nLlama-32023  \n\n1  \n\n  \n\n1personally identifiable informationPIIand safety filtering  \n\n  \n\n2text extraction and cleaning  \n\nMetaHTML parser  \n\n  \n\nmarkdownmarkdown marker  \n\n3  \n\n- URL-level  \n- Document-levelMinHash  \n- Line-levelccNet30Mbucket6cookie warnings  \n\n4Heuristic filtering  \n\n  \n- Scaling language models: Methods, analysis & insights from training gophern-gram coverage ratiologgingerror messages  \n- Exploring the limits of transfer learning with a unified text-to-text transformerdirty word counting  \n- tokenKLoutlier token  \n\n5Model-based quality filtering  \n\nLlama-2fasttextDistilRobertaLlama-2  \n\n6Code and reasoning data  \n\nDeepSeek-Coder-V2STEMHTMLprompt  \n\n7Multilingual data  \n\nPII  \n- fasttext176  \n- document-levelline-level  \n-   \n\n  \n\n2Data Mix  \n\nknowledge classificationscaling law experiments  \n\n- Knowledge classificationartsentertainment  \n- Scaling laws for data mixdata mixscaling lawdata mix  \n- Data mix summary50%general knowledge25%17%8%  \n\n3Annealing Data  \n\nlearning ratebenchmarkDatacomp-lm: In search of the next generation of training sets for language modelsupsampled  \n\nGSM8kMATH8B405B405Bin-context learning  \n\nannealingannealingDoes your data spark joy?performance gains from domain upsampling at the end of training  \n\n##   \n\n405B3  \n- initial pre-training  \n- long-context pre-training  \n- annealing  \n\n1initial pre-training  \n\n  \n- cosine learning rate schedule  \n- peark lr = 8e-5  \n- batch size schedule4kbatch size4M252M252B token8kbatch size8M token2.87Tdoublebatch size16Mbatch size schedule  \n\n  \n\n2long-context pre-training  \n\nLlama-3128k8k16k128k8k6128k  \n\n  \n-   \n- 100%  \n\n800B  \n\n3annealing  \n\n40M40Bstep128M tokenlr0annealingmodel checkpoint  \n\n#   \n\n  \n- annealing  \n- curriculum learning  \n- scaling lawFLOPs  \n- scaling lawdata mixall you need is money  \n- checkpoint averagemodel soup  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n- MoE  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[denseMoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  \n[MoE--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  \n-   \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[--](https://www.linsight.cn/210dbccd.html)  \n-   \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n-   \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n-   \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n- Transformer  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n-   \n[(1)](http://www.linsight.cn/3345028a.html)\n[(2)](http://www.linsight.cn/ad0bba9d.html)\n[(3)](http://www.linsight.cn/1736008.html)\n[(4)](http://www.linsight.cn/1736008.html)\n[(5)](http://www.linsight.cn/336f2f3e.html)\n[(6)](http://www.linsight.cn/7c04944d.html)\n[(7)](https://www.linsight.cn/dd614e12.html)\n[(8)](https://www.linsight.cn/e287b9c3.html)  \n\n# Reference  \n\n1The Llama 3 Herd of Models https://ai.meta.com/research/publications/the-llama-3-herd-of-models/  \n2https://ai.meta.com/blog/meta-llama-3-1/  \n","source":"_posts/cs/nlp/2024/07/Llama-3-1-.md","raw":"---\ntitle: Llama3.1--\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - Meta\n  - Llama\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 7d7294cb\ndate: 2024-07-25 22:15:40\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nLlama-3.1-405BGPT-4-0125Claude-3.5-SonnetGPT-4-OMNI8B70B  \n\n{% asset_img eval.png  %}  \n\nMeta100Llama-3  \n\nMistral Large V2Llama-3  \n\n# Llama-3  \n\nLlama-38B/70BLlama-3.18B/70/405B  \n\n{% asset_img model.png  %}  \n\nLlama-3.1-405B405B  \n\n## 405B  \n\n405B15.6T token128kLlama-2915.6T vs 1.8TLlama-2-70B50+  \n\n405B  15.6TMetascaling lawMetascaling lawcompute-optimal token  \n\n405BdenseMoEMetaMoEdenseMoETransformerpost-trainingsupervised finetuningSFTrejection samplingRSand direct preference optimizationDPOMeta  \n\n405B16k H100tensor parallelismpipeline parallelismcontext parallelismdata parallelism  \n\n##   \n\nLlama-3  \n- GQAKV cache  \n- document mask  \n- RoPEbase frequency500,000Effective long-context scaling of foundation models32,768  \n- 128k100ktiktoken28ktoken3.17-->3.94  \n\n  \n\n{% asset_img llama3.png  %}  \n\n## Scaling Laws  \n\nLLMscaling lawscaling law  \n- scaling lawnext-token predictionlossloss  \n- scaling lawcompute budgetscaling law  \n\nMetatwo-stagedownstream benchmark performace+  \n- compute-optimal modeldownstream tasknegative log-likelihoodFLOPs  \n- negative log-likelihoodtask accuracyscaling law modelsLlama-2FLOPs  \n\npre-training data mix  \n\n40M16BFLOPscompute  \n\n{% asset_img scaling_law_exp.png  %}  \n\nlrcompute budget250k4Mbatch size  \n\ncompute budget Coptimal number of training token $N^{\\star}(C)$   \n\n$$N^\\star(C)=AC^\\alpha $$  \n\n $(\\alpha,A)=(0.53,0.29)$ $3.8\\times10^{25}$ FLOPscompute budget402B16.55T token    \n\ncompute budgetIsoFLOPsrobust  \n\ncompute budgetbenchmarkNormalized NLL per CharFLOPsNormalized NLL per CharaccuracysigmoidFLOPsbenchmarkaccuracyARC Challenge  \n\n{% asset_img scaling_law.png  %}  \n\n405B  \n\n# Pre-Training  \n\n##   \n\nLlama-32023  \n\n1  \n\n  \n\n1personally identifiable informationPIIand safety filtering  \n\n  \n\n2text extraction and cleaning  \n\nMetaHTML parser  \n\n  \n\nmarkdownmarkdown marker  \n\n3  \n\n- URL-level  \n- Document-levelMinHash  \n- Line-levelccNet30Mbucket6cookie warnings  \n\n4Heuristic filtering  \n\n  \n- Scaling language models: Methods, analysis & insights from training gophern-gram coverage ratiologgingerror messages  \n- Exploring the limits of transfer learning with a unified text-to-text transformerdirty word counting  \n- tokenKLoutlier token  \n\n5Model-based quality filtering  \n\nLlama-2fasttextDistilRobertaLlama-2  \n\n6Code and reasoning data  \n\nDeepSeek-Coder-V2STEMHTMLprompt  \n\n7Multilingual data  \n\nPII  \n- fasttext176  \n- document-levelline-level  \n-   \n\n  \n\n2Data Mix  \n\nknowledge classificationscaling law experiments  \n\n- Knowledge classificationartsentertainment  \n- Scaling laws for data mixdata mixscaling lawdata mix  \n- Data mix summary50%general knowledge25%17%8%  \n\n3Annealing Data  \n\nlearning ratebenchmarkDatacomp-lm: In search of the next generation of training sets for language modelsupsampled  \n\nGSM8kMATH8B405B405Bin-context learning  \n\nannealingannealingDoes your data spark joy?performance gains from domain upsampling at the end of training  \n\n##   \n\n405B3  \n- initial pre-training  \n- long-context pre-training  \n- annealing  \n\n1initial pre-training  \n\n  \n- cosine learning rate schedule  \n- peark lr = 8e-5  \n- batch size schedule4kbatch size4M252M252B token8kbatch size8M token2.87Tdoublebatch size16Mbatch size schedule  \n\n  \n\n2long-context pre-training  \n\nLlama-3128k8k16k128k8k6128k  \n\n  \n-   \n- 100%  \n\n800B  \n\n3annealing  \n\n40M40Bstep128M tokenlr0annealingmodel checkpoint  \n\n#   \n\n  \n- annealing  \n- curriculum learning  \n- scaling lawFLOPs  \n- scaling lawdata mixall you need is money  \n- checkpoint averagemodel soup  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n- MoE  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[denseMoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  \n[MoE--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  \n-   \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[--](https://www.linsight.cn/210dbccd.html)  \n-   \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n-   \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n-   \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n- Transformer  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n-   \n[(1)](http://www.linsight.cn/3345028a.html)\n[(2)](http://www.linsight.cn/ad0bba9d.html)\n[(3)](http://www.linsight.cn/1736008.html)\n[(4)](http://www.linsight.cn/1736008.html)\n[(5)](http://www.linsight.cn/336f2f3e.html)\n[(6)](http://www.linsight.cn/7c04944d.html)\n[(7)](https://www.linsight.cn/dd614e12.html)\n[(8)](https://www.linsight.cn/e287b9c3.html)  \n\n# Reference  \n\n1The Llama 3 Herd of Models https://ai.meta.com/research/publications/the-llama-3-herd-of-models/  \n2https://ai.meta.com/blog/meta-llama-3-1/  \n","slug":"cs/nlp/2024/07/Llama-3-1-","published":1,"updated":"2024-07-25T14:42:23.512Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsmz00280p4k2nfvggv5","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>Llama-3.1-405BGPT-4-0125Claude-3.5-SonnetGPT-4-OMNI8B70B</p>\n<img src=\"/7d7294cb/eval.png\" class title=\"\">\n<p>Meta100Llama-3</p>\n<p>Mistral Large\nV2Llama-3</p>\n<h1 id=\"llama-3\">Llama-3</h1>\n<p>Llama-38B/70BLlama-3.18B/70/405B</p>\n<img src=\"/7d7294cb/model.png\" class title=\"\">\n<p>Llama-3.1-405B405B</p>\n<h2 id=\"405b\">405B</h2>\n<p>405B15.6T\ntoken128kLlama-2915.6T\nvs 1.8TLlama-2-70B50+</p>\n<p>405B  15.6TMetascaling\nlawMetascaling\nlawcompute-optimal\ntoken</p>\n<p>405BdenseMoEMetaMoEdenseMoETransformerpost-trainingsupervised\nfinetuningSFTrejection samplingRSand direct preference\noptimizationDPOMeta</p>\n<p>405B16k H100tensor\nparallelismpipeline parallelismcontext parallelismdata\nparallelism</p>\n<h2 id=\"\"></h2>\n<p>Llama-3<br>\n- GQAKV cache<br>\n- document\nmask<br>\n- RoPEbase frequency500,000Effective long-context\nscaling of foundation\nmodels32,768<br>\n-\n128k100ktiktoken28ktoken3.17--&gt;3.94</p>\n<p></p>\n<img src=\"/7d7294cb/llama3.png\" class title=\"\">\n<h2 id=\"scaling-laws\">Scaling Laws</h2>\n<p>LLMscaling\nlawscaling\nlaw<br>\n- scaling lawnext-token\npredictionlossloss<br>\n- scaling lawcompute\nbudgetscaling\nlaw</p>\n<p>Metatwo-stagedownstream benchmark\nperformace+<br>\n- compute-optimal modeldownstream tasknegative\nlog-likelihoodFLOPs<br>\n- negative log-likelihoodtask\naccuracyscaling law\nmodelsLlama-2FLOPs</p>\n<p>pre-training data mix</p>\n<p>40M16BFLOPscompute</p>\n<img src=\"/7d7294cb/scaling_law_exp.png\" class title=\"\">\n<p>lrcompute\nbudget250k4Mbatch size</p>\n<p>compute budget Coptimal number of\ntraining token <span class=\"math inline\">\\(N^{\\star}(C)\\)</span>\n</p>\n<p><span class=\"math display\">\\[N^\\star(C)=AC^\\alpha \\]</span></p>\n<p> <span class=\"math inline\">\\((\\alpha,A)=(0.53,0.29)\\)</span>\n<span class=\"math inline\">\\(3.8\\times10^{25}\\)</span> FLOPscompute\nbudget402B16.55T token</p>\n<p>compute\nbudgetIsoFLOPsrobust</p>\n<p>compute\nbudgetbenchmarkNormalized NLL per\nCharFLOPsNormalized NLL per\nCharaccuracysigmoidFLOPsbenchmarkaccuracyARC\nChallenge</p>\n<img src=\"/7d7294cb/scaling_law.png\" class title=\"\">\n<p>405B</p>\n<h1 id=\"pre-training\">Pre-Training</h1>\n<h2 id=\"\"></h2>\n<p>Llama-32023</p>\n<p>1</p>\n<p></p>\n<p>1personally identifiable informationPIIand safety\nfiltering</p>\n<p></p>\n<p>2text extraction and cleaning</p>\n<p>MetaHTML\nparser</p>\n<p></p>\n<p>markdownmarkdown\nmarker</p>\n<p>3</p>\n<ul>\n<li>URL-level<br>\n</li>\n<li>Document-levelMinHash<br>\n</li>\n<li>Line-levelccNet30Mbucket6cookie\nwarnings</li>\n</ul>\n<p>4Heuristic filtering</p>\n<p><br>\n- Scaling language models: Methods, analysis &amp; insights from\ntraining gophern-gram coverage\nratiologgingerror\nmessages<br>\n- Exploring the limits of transfer learning with a unified\ntext-to-text transformerdirty word counting<br>\n- tokenKLoutlier token</p>\n<p>5Model-based quality filtering</p>\n<p>Llama-2fasttextDistilRobertaLlama-2</p>\n<p>6Code and reasoning data</p>\n<p>DeepSeek-Coder-V2STEMHTMLprompt</p>\n<p>7Multilingual data</p>\n<p>PII<br>\n- fasttext176<br>\n- document-levelline-level<br>\n- </p>\n<p></p>\n<p>2Data Mix</p>\n<p>knowledge\nclassificationscaling law experiments</p>\n<ul>\n<li>Knowledge\nclassificationartsentertainment<br>\n</li>\n<li>Scaling laws for data mixdata\nmixscaling lawdata mix<br>\n</li>\n<li>Data mix summary50%general\nknowledge25%17%8%</li>\n</ul>\n<p>3Annealing Data</p>\n<p>learning\nratebenchmarkDatacomp-lm:\nIn search of the next generation of training sets for language\nmodelsupsampled</p>\n<p>GSM8kMATH8B405B405Bin-context\nlearning</p>\n<p>annealingannealingDoes\nyour data spark joy?performance gains from domain upsampling at the end\nof training</p>\n<h2 id=\"\"></h2>\n<p>405B3<br>\n- initial pre-training<br>\n- long-context pre-training<br>\n- annealing</p>\n<p>1initial pre-training</p>\n<p><br>\n- cosine learning rate schedule<br>\n- peark lr = 8e-5<br>\n- batch size schedule4kbatch\nsize4M252M252B\ntoken8kbatch size8M\ntoken2.87Tdoublebatch\nsize16Mbatch size schedule</p>\n<p></p>\n<p>2long-context pre-training</p>\n<p>Llama-3128k8k16k128k8k6128k</p>\n<p><br>\n- <br>\n- 100%</p>\n<p>800B</p>\n<p>3annealing</p>\n<p>40M40Bstep128M\ntokenlr0annealingmodel\ncheckpoint</p>\n<h1 id=\"\"></h1>\n<p><br>\n- annealing<br>\n- curriculum learning<br>\n- scaling\nlawFLOPs<br>\n- scaling lawdata mixall\nyou need is money<br>\n- checkpoint averagemodel soup</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n- MoE<br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/a0824e29.html\">denseMoE -- sparse\nupcycling</a><br>\n<a href=\"https://www.linsight.cn/2c8bbc7.html\">MoE--expert choice\nrouting</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"https://www.linsight.cn/210dbccd.html\">--</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n- Transformer<br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a> <a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a> <a href=\"http://www.linsight.cn/1736008.html\">(3)</a> <a href=\"http://www.linsight.cn/1736008.html\">(4)</a> <a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a> <a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a> <a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a> <a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1The Llama 3 Herd of Models\nhttps://ai.meta.com/research/publications/the-llama-3-herd-of-models/<br>\n2https://ai.meta.com/blog/meta-llama-3-1/</p>\n","length":6400,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>Llama-3.1-405BGPT-4-0125Claude-3.5-SonnetGPT-4-OMNI8B70B</p>\n<img src=\"/7d7294cb/eval.png\" class title=\"\">\n<p>Meta100Llama-3</p>\n<p>Mistral Large\nV2Llama-3</p>\n<h1 id=\"llama-3\">Llama-3</h1>\n<p>Llama-38B/70BLlama-3.18B/70/405B</p>\n<img src=\"/7d7294cb/model.png\" class title=\"\">\n<p>Llama-3.1-405B405B</p>\n<h2 id=\"405b\">405B</h2>\n<p>405B15.6T\ntoken128kLlama-2915.6T\nvs 1.8TLlama-2-70B50+</p>\n<p>405B  15.6TMetascaling\nlawMetascaling\nlawcompute-optimal\ntoken</p>\n<p>405BdenseMoEMetaMoEdenseMoETransformerpost-trainingsupervised\nfinetuningSFTrejection samplingRSand direct preference\noptimizationDPOMeta</p>\n<p>405B16k H100tensor\nparallelismpipeline parallelismcontext parallelismdata\nparallelism</p>\n<h2 id=\"\"></h2>\n<p>Llama-3<br>\n- GQAKV cache<br>\n- document\nmask<br>\n- RoPEbase frequency500,000Effective long-context\nscaling of foundation\nmodels32,768<br>\n-\n128k100ktiktoken28ktoken3.17--&gt;3.94</p>\n<p></p>\n<img src=\"/7d7294cb/llama3.png\" class title=\"\">\n<h2 id=\"scaling-laws\">Scaling Laws</h2>\n<p>LLMscaling\nlawscaling\nlaw<br>\n- scaling lawnext-token\npredictionlossloss<br>\n- scaling lawcompute\nbudgetscaling\nlaw</p>\n<p>Metatwo-stagedownstream benchmark\nperformace+<br>\n- compute-optimal modeldownstream tasknegative\nlog-likelihoodFLOPs<br>\n- negative log-likelihoodtask\naccuracyscaling law\nmodelsLlama-2FLOPs</p>\n<p>pre-training data mix</p>\n<p>40M16BFLOPscompute</p>\n<img src=\"/7d7294cb/scaling_law_exp.png\" class title=\"\">\n<p>lrcompute\nbudget250k4Mbatch size</p>\n<p>compute budget Coptimal number of\ntraining token <span class=\"math inline\">\\(N^{\\star}(C)\\)</span>\n</p>\n<p><span class=\"math display\">\\[N^\\star(C)=AC^\\alpha \\]</span></p>\n<p> <span class=\"math inline\">\\((\\alpha,A)=(0.53,0.29)\\)</span>\n<span class=\"math inline\">\\(3.8\\times10^{25}\\)</span> FLOPscompute\nbudget402B16.55T token</p>\n<p>compute\nbudgetIsoFLOPsrobust</p>\n<p>compute\nbudgetbenchmarkNormalized NLL per\nCharFLOPsNormalized NLL per\nCharaccuracysigmoidFLOPsbenchmarkaccuracyARC\nChallenge</p>\n<img src=\"/7d7294cb/scaling_law.png\" class title=\"\">\n<p>405B</p>\n<h1 id=\"pre-training\">Pre-Training</h1>\n<h2 id=\"\"></h2>\n<p>Llama-32023</p>\n<p>1</p>\n<p></p>\n<p>1personally identifiable informationPIIand safety\nfiltering</p>\n<p></p>\n<p>2text extraction and cleaning</p>\n<p>MetaHTML\nparser</p>\n<p></p>\n<p>markdownmarkdown\nmarker</p>\n<p>3</p>\n<ul>\n<li>URL-level<br>\n</li>\n<li>Document-levelMinHash<br>\n</li>\n<li>Line-levelccNet30Mbucket6cookie\nwarnings</li>\n</ul>\n<p>4Heuristic filtering</p>\n<p><br>\n- Scaling language models: Methods, analysis &amp; insights from\ntraining gophern-gram coverage\nratiologgingerror\nmessages<br>\n- Exploring the limits of transfer learning with a unified\ntext-to-text transformerdirty word counting<br>\n- tokenKLoutlier token</p>\n<p>5Model-based quality filtering</p>\n<p>Llama-2fasttextDistilRobertaLlama-2</p>\n<p>6Code and reasoning data</p>\n<p>DeepSeek-Coder-V2STEMHTMLprompt</p>\n<p>7Multilingual data</p>\n<p>PII<br>\n- fasttext176<br>\n- document-levelline-level<br>\n- </p>\n<p></p>\n<p>2Data Mix</p>\n<p>knowledge\nclassificationscaling law experiments</p>\n<ul>\n<li>Knowledge\nclassificationartsentertainment<br>\n</li>\n<li>Scaling laws for data mixdata\nmixscaling lawdata mix<br>\n</li>\n<li>Data mix summary50%general\nknowledge25%17%8%</li>\n</ul>\n<p>3Annealing Data</p>\n<p>learning\nratebenchmarkDatacomp-lm:\nIn search of the next generation of training sets for language\nmodelsupsampled</p>\n<p>GSM8kMATH8B405B405Bin-context\nlearning</p>\n<p>annealingannealingDoes\nyour data spark joy?performance gains from domain upsampling at the end\nof training</p>\n<h2 id=\"\"></h2>\n<p>405B3<br>\n- initial pre-training<br>\n- long-context pre-training<br>\n- annealing</p>\n<p>1initial pre-training</p>\n<p><br>\n- cosine learning rate schedule<br>\n- peark lr = 8e-5<br>\n- batch size schedule4kbatch\nsize4M252M252B\ntoken8kbatch size8M\ntoken2.87Tdoublebatch\nsize16Mbatch size schedule</p>\n<p></p>\n<p>2long-context pre-training</p>\n<p>Llama-3128k8k16k128k8k6128k</p>\n<p><br>\n- <br>\n- 100%</p>\n<p>800B</p>\n<p>3annealing</p>\n<p>40M40Bstep128M\ntokenlr0annealingmodel\ncheckpoint</p>\n<h1 id=\"\"></h1>\n<p><br>\n- annealing<br>\n- curriculum learning<br>\n- scaling\nlawFLOPs<br>\n- scaling lawdata mixall\nyou need is money<br>\n- checkpoint averagemodel soup</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n- MoE<br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/a0824e29.html\">denseMoE -- sparse\nupcycling</a><br>\n<a href=\"https://www.linsight.cn/2c8bbc7.html\">MoE--expert choice\nrouting</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"https://www.linsight.cn/210dbccd.html\">--</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n- Transformer<br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a> <a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a> <a href=\"http://www.linsight.cn/1736008.html\">(3)</a> <a href=\"http://www.linsight.cn/1736008.html\">(4)</a> <a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a> <a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a> <a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a> <a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1The Llama 3 Herd of Models\nhttps://ai.meta.com/research/publications/the-llama-3-herd-of-models/<br>\n2https://ai.meta.com/blog/meta-llama-3-1/</p>\n"},{"title":"Llama3.1--post-training","abbrlink":"93328a2a","date":"2024-07-26T13:10:04.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\n[Llama3.1--](https://www.linsight.cn/7d7294cb.html)Llama-3.1post-training  \n\nLlama-3pre-trainingpost-trainingSFTDPO  \n\nLlama-3post-trainingroundpost-training6SFTDPO  \n\n# Modeling  \n\npost-training  \n\n{% asset_img post_training.png post-training %}  \n\n## Chat Dialog Format  \n\nLlama-3tool useMetamulti-message chat protocol  \n\n## Reward Modeling  \n\nreward modelRMpost-training  \n\nLlama-2RMmargin termchosenrejected responsemargin term  \n\nLlama-2preference dataRM  \n\nchosenrejected response -- edited responsechosenresponseranking sample3responseedited > chosen > rejected  \n\npromptresponseprompt + resp_1 + resp_2 + resp_3responsepromptprompt + resp_1, prompt + resp_2, prompt + resp_3accuracydocument mask  \n\n## SFT  \n\nRMrejection samplinghuman annotation promptSFT  \n\nSFTlr=1e-58.5k~9kpost-training  \n\n## DPO  \n\nDPOpost-trainingpolicy model  \n\nDPOMetaon-policyPPODPOinstruction followingpost-trainingDPO  \n\nDPOlr=1e-5beta=0.1  \n\n  \n\n1Masking out formatting tokens in DPO loss  \n\ntokenheadertermination tokenlosstokenlosschosen repsponserejected responsetokenlikelihood  \n\n2Regularization with NLL loss  \n\nDPOlossMetaNLLIterative reasoning preference optimizationPPOnext token prediction lossSFTchosen responselog probabilitySmaug: Fixing failure modes of preference optimisation with dpo-positive  \n\n## Model Averaging  \n\nAveraging weights leads to wider optima and better generalizationModel soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference timeBranch-train-merge: Embarrassingly parallel training of expert language modelsRMSFTDPO  \n\n#   \n\n##   \n\nLlama-2  \n\n  \n\nuser promptresponsechosenrejected response4  \n- significantly better\n- better  \n- slightly better  \n- marginally better  \n\nchosen responseresponse  \n\n  \n\n{% asset_img preference_data.png preference data %}  \n\nLlama-2Llama-3promptresponseLlama-3  \n\npost-trainingprompt  \n\npost-trainingRMDPO  \n\nRMDPOsignificantly better  better  \n\n## SFT Data  \n\nSFT  \n- promptresponse  \n- capacities  \n-   \n\n1RS  \n\nRSprompt/chatK10~30RMConstitutional AI: harmlessness from AI feedback  \n\npost-trainingRSsystem promptprompt  \n\n2  \n\nhelpful  \n\n{% asset_img sft_data.png sft data %}  \n\n##  &   \n\n  \n\n1  \n\npost-trainingemojipatternoverly-apologeticIm sorry  \n\n2Data pruning  \n\nmodel-based  \n- Llama-3-8B &   \n- RMRMLlama-3 checkpointpromptpromptRMLlama-3  \n- InstagLlama  \n- Robertaquality score  difficulty scoreSemdedup: Data-efficient learning at web-scale through semantic deduplicationWhat makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning  \n\n# Capabilities  \n\nMeta  \n\n## Code  \n\nPython, Java, Javascript, C/C++, Typescript, Rust, PHP, HTML/CSS, SQL, bash/shell  \n\ncode expertSFTsystem promptquality filter  \n\n### Expert training  \n\n1T>85%CodeLlamacode expert  \n\nsteprepo-levelcode expert  \n\npost-training  \n\ncode expert  \n- post-training  \n- code promptrejection sampling  \n\n###   \n\n  \n\nLlama 3code expertSFT  \n\n###   \n\n2.7MSFT  \n\n1  \n\nLlama-38B70B405B405B  \n\nMetaexecution feedback  \n\n1M  \n\n1  \n\nMagicoder: Empowering code generation with oss-instruct  \n\n2Solution  \n\nLlama-3  \n\npromptgeneral rule  \n\n3  \n\nsolution  \n\nparserlinter  \n\n  \n\n4 &   \n\n  \n\nprompt  \n\n20%  \n\n5 &   \n\nroundround  \n\n2programming language translation  \n\nMetaLlama-3syntax parsing, compilation, executionBreaking language barriers in multilingual mathematical reasoning: Insights and observations  \n\n3backtranslation  \n\ndocumentationdebuggingexplanation+  \n\nbacktranslation  \n- Generate  \n- Backtranslate  \n- Filter/  \n\nbacktranslation1.2Mdocumentationdebuggingexplanation  \n\n###   \n\n1system prompt  \n\nsystem promptcomment  \n\n{% asset_img code_sample.png  %}  \n\n2Filtering training data with execution and model-as-judge signals  \n\nrejection samplingstraightforward  \n\nmodel-as-judgeLlama-3  \n\nresponseresponseLlama-3  \n\n##   \n\nLlama-38German, French, Italian, Portuguese, Hindi, Spanish, Thai  \n\n### Expert training  \n\n90%data mixcode expertpost-trainingexpert model  \n\n###   \n\nSFT  \n- 2.4%  \n- 44.2%NLP task  \n- 18.8%rejection sampling  \n- 34.6%translated reasoning data  \n\n1  \n\nnative speaker  \n\n2NLP task  \n\n- NLP  \n- alignmentParallel global voices: a collection of multilingual corpora with citizen media storiesWikimediaparallel text  \n- LID based filteringBlaser2.0 Seamlessm4tmassively multilingual & multimodal machine translation  \n\n3  \n\nRS  \n- Generationpost-training0.2~1.00.6  \n- SelectionRMpromptresponse  \n\n4  \n\nsynthetic quantitative reasoning data  \n\n  \n\n## Math and Reasoning  \n\nreasoning  \n\nreasoning  \n- prompt  \n- CoTreasoningCoT  \n- CoT  \n-   \n-   \n\nMeta  \n\n1prompt  \n\npromptcontext  \n\npromptMetacognitive capabilities of llms: An exploration in mathematical problem solvingprompt  \n\n2Augmenting training data with step-wise reasoning traces \n \nLlama-3promptstep-by-step  \n\npromptCommon 7b language models already possess strong math capabilities  \n\nLlama-3  \n\n3Filtering incorrect reasoning trace  \n\noutcome RMstepwise RMLets verify step by stepMath-shepherd:Verify and reinforce llms step-by-step without human annotations  \n\npromptMonte Carlo Tree Search (MCTS)Monte carlo tree search boosts reasoning via iterative preference learning  \n\n4Interleaving code and text reasoning  \n\npython codeTora: A tool-integrated reasoning agent for mathematical problem solving  \n\n5Learning from feedback and mistakes  \n\nLearning from mistakes makes llm better reasonerGenerating sequences by learning to self-correctSelf-refine: Iterative refinement with self-feedback  \n\n## Long Context  \n\n8k128k  \n\npost-training  \n\n1SFT  \n\nSFTSFTSFT  \n\n128kSFT  \n\nLlama-3reasoning  \n\n1Question answering  \n\n8kQAQA  \n\n2Summarization  \n\n8k8k  \n\nMetaQA  \n\n3Long context code reasoning  \n\nPython  \n\n  \n\n16K, 32K, 64K128K  \n\nSFT0.1%  \n\n2DPO  \n\nDPODPODPO  \n\n## Tool Use  \n\nLlama-3core tools  \n- Brave Search  \n- Python interpreter  \n- Mathematical computational engineWolfram Alpha API  \n\nqueryLlama-3plan  \n\ncore toolLlama-3zero-shotquery  \n\n1Implementation  \n\nMetacore toolsPython  \n\nzero-shot toolPython  \n\njsonWeb API  \n\nPythonLlama-3system promptcore toolsystem prompt  \n\n2Data collection  \n\nToolFormerLlama-3  \n\nmessage  \n\nrejection sampling  \n\nfinetune  \n\n3Tool datasets  \n\n  \n\n1Single-step tool use  \n\nfew-shot promptcore toolsquery  \n\nsystem prompt, user prompt, tool call, tool output, final answer  \n\n30%  \n\n2Multi-step tool use  \n\nLlama-32core toolpromptfew shot promptLlama-3ReAct  \n\n{% asset_img multi_step_tool.png  %}  \n\n3File uploads  \n\n.txt, .docx, .pdf, .pptx, .xlsx, .csv, .tsv, .py, .json, .jsonl, .html, .xml  \n\n  \n\n{% asset_img file_upload.png  %}  \n\nMetacase  \n\nqueryqueryresponsesystem promptavailable  \n\n4Zero-shot tool use data  \n\nLlama-3zero-shot  \n\nquery  \n\n1Single, nested, and parallel function calling  \n\n  \n\nToolverifier: Generalization to new tools via self-verificationStackThe stack: 3 tb of permissively licensed source codeLlama-3query  \n\n2Multi-turn function calling  \n\nApi-bank: A comprehensive benchmark for tool-augmented llms  \n\npromptLlama-3agentdomains, APIs, user queries, API calls,  responses  \n\n## Factuality  \n\nHallucination  \n\nMetapost-training know what it knows Does fine-tuning llms on new knowledge encourage hallucinations?Linguistic calibration through metacognition: aligning dialogue agent responses with expected correctness  \n\n --   \n\nMetaLlama-3in-contextknowledge probing  \n\n\n\n-   \n- Llama-3  \n- Llama-3  \n- context  \n- informativeness  \n- Llama-3responserefusal  \n\nMetaknowledge probing  \n\n  \n\n## Steerability  \n\n  \n\nLlama-3  \n\nMetasystem promptLlama-3response  \n\nannotatorLlama-3system promptannotatorsystem prompt  \n\nsystem prompt  \n\n{% asset_img steerability.png steerability %}  \n\n#   \n\n- Llama-3  \n- post-training  \n-   \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n- MoE  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[denseMoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  \n[MoE--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  \n-   \n[Llama3.1--](https://www.linsight.cn/7d7294cb.html)  \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[--](https://www.linsight.cn/210dbccd.html)  \n-   \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n-   \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n-   \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n- Transformer  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n-   \n[(1)](http://www.linsight.cn/3345028a.html)\n[(2)](http://www.linsight.cn/ad0bba9d.html)\n[(3)](http://www.linsight.cn/1736008.html)\n[(4)](http://www.linsight.cn/1736008.html)\n[(5)](http://www.linsight.cn/336f2f3e.html)\n[(6)](http://www.linsight.cn/7c04944d.html)\n[(7)](https://www.linsight.cn/dd614e12.html)\n[(8)](https://www.linsight.cn/e287b9c3.html)  \n\n# Reference  \n\n1The Llama 3 Herd of Models https://ai.meta.com/research/publications/the-llama-3-herd-of-models/   \n","source":"_posts/cs/nlp/2024/07/Llama3-1-post-training.md","raw":"---\ntitle: Llama3.1--post-training\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - Meta\n  - Llama\n  - post-training\n  - SFT\n  - DPO\n  - RM\n  - RS\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 93328a2a\ndate: 2024-07-26 21:10:04\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\n[Llama3.1--](https://www.linsight.cn/7d7294cb.html)Llama-3.1post-training  \n\nLlama-3pre-trainingpost-trainingSFTDPO  \n\nLlama-3post-trainingroundpost-training6SFTDPO  \n\n# Modeling  \n\npost-training  \n\n{% asset_img post_training.png post-training %}  \n\n## Chat Dialog Format  \n\nLlama-3tool useMetamulti-message chat protocol  \n\n## Reward Modeling  \n\nreward modelRMpost-training  \n\nLlama-2RMmargin termchosenrejected responsemargin term  \n\nLlama-2preference dataRM  \n\nchosenrejected response -- edited responsechosenresponseranking sample3responseedited > chosen > rejected  \n\npromptresponseprompt + resp_1 + resp_2 + resp_3responsepromptprompt + resp_1, prompt + resp_2, prompt + resp_3accuracydocument mask  \n\n## SFT  \n\nRMrejection samplinghuman annotation promptSFT  \n\nSFTlr=1e-58.5k~9kpost-training  \n\n## DPO  \n\nDPOpost-trainingpolicy model  \n\nDPOMetaon-policyPPODPOinstruction followingpost-trainingDPO  \n\nDPOlr=1e-5beta=0.1  \n\n  \n\n1Masking out formatting tokens in DPO loss  \n\ntokenheadertermination tokenlosstokenlosschosen repsponserejected responsetokenlikelihood  \n\n2Regularization with NLL loss  \n\nDPOlossMetaNLLIterative reasoning preference optimizationPPOnext token prediction lossSFTchosen responselog probabilitySmaug: Fixing failure modes of preference optimisation with dpo-positive  \n\n## Model Averaging  \n\nAveraging weights leads to wider optima and better generalizationModel soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference timeBranch-train-merge: Embarrassingly parallel training of expert language modelsRMSFTDPO  \n\n#   \n\n##   \n\nLlama-2  \n\n  \n\nuser promptresponsechosenrejected response4  \n- significantly better\n- better  \n- slightly better  \n- marginally better  \n\nchosen responseresponse  \n\n  \n\n{% asset_img preference_data.png preference data %}  \n\nLlama-2Llama-3promptresponseLlama-3  \n\npost-trainingprompt  \n\npost-trainingRMDPO  \n\nRMDPOsignificantly better  better  \n\n## SFT Data  \n\nSFT  \n- promptresponse  \n- capacities  \n-   \n\n1RS  \n\nRSprompt/chatK10~30RMConstitutional AI: harmlessness from AI feedback  \n\npost-trainingRSsystem promptprompt  \n\n2  \n\nhelpful  \n\n{% asset_img sft_data.png sft data %}  \n\n##  &   \n\n  \n\n1  \n\npost-trainingemojipatternoverly-apologeticIm sorry  \n\n2Data pruning  \n\nmodel-based  \n- Llama-3-8B &   \n- RMRMLlama-3 checkpointpromptpromptRMLlama-3  \n- InstagLlama  \n- Robertaquality score  difficulty scoreSemdedup: Data-efficient learning at web-scale through semantic deduplicationWhat makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning  \n\n# Capabilities  \n\nMeta  \n\n## Code  \n\nPython, Java, Javascript, C/C++, Typescript, Rust, PHP, HTML/CSS, SQL, bash/shell  \n\ncode expertSFTsystem promptquality filter  \n\n### Expert training  \n\n1T>85%CodeLlamacode expert  \n\nsteprepo-levelcode expert  \n\npost-training  \n\ncode expert  \n- post-training  \n- code promptrejection sampling  \n\n###   \n\n  \n\nLlama 3code expertSFT  \n\n###   \n\n2.7MSFT  \n\n1  \n\nLlama-38B70B405B405B  \n\nMetaexecution feedback  \n\n1M  \n\n1  \n\nMagicoder: Empowering code generation with oss-instruct  \n\n2Solution  \n\nLlama-3  \n\npromptgeneral rule  \n\n3  \n\nsolution  \n\nparserlinter  \n\n  \n\n4 &   \n\n  \n\nprompt  \n\n20%  \n\n5 &   \n\nroundround  \n\n2programming language translation  \n\nMetaLlama-3syntax parsing, compilation, executionBreaking language barriers in multilingual mathematical reasoning: Insights and observations  \n\n3backtranslation  \n\ndocumentationdebuggingexplanation+  \n\nbacktranslation  \n- Generate  \n- Backtranslate  \n- Filter/  \n\nbacktranslation1.2Mdocumentationdebuggingexplanation  \n\n###   \n\n1system prompt  \n\nsystem promptcomment  \n\n{% asset_img code_sample.png  %}  \n\n2Filtering training data with execution and model-as-judge signals  \n\nrejection samplingstraightforward  \n\nmodel-as-judgeLlama-3  \n\nresponseresponseLlama-3  \n\n##   \n\nLlama-38German, French, Italian, Portuguese, Hindi, Spanish, Thai  \n\n### Expert training  \n\n90%data mixcode expertpost-trainingexpert model  \n\n###   \n\nSFT  \n- 2.4%  \n- 44.2%NLP task  \n- 18.8%rejection sampling  \n- 34.6%translated reasoning data  \n\n1  \n\nnative speaker  \n\n2NLP task  \n\n- NLP  \n- alignmentParallel global voices: a collection of multilingual corpora with citizen media storiesWikimediaparallel text  \n- LID based filteringBlaser2.0 Seamlessm4tmassively multilingual & multimodal machine translation  \n\n3  \n\nRS  \n- Generationpost-training0.2~1.00.6  \n- SelectionRMpromptresponse  \n\n4  \n\nsynthetic quantitative reasoning data  \n\n  \n\n## Math and Reasoning  \n\nreasoning  \n\nreasoning  \n- prompt  \n- CoTreasoningCoT  \n- CoT  \n-   \n-   \n\nMeta  \n\n1prompt  \n\npromptcontext  \n\npromptMetacognitive capabilities of llms: An exploration in mathematical problem solvingprompt  \n\n2Augmenting training data with step-wise reasoning traces \n \nLlama-3promptstep-by-step  \n\npromptCommon 7b language models already possess strong math capabilities  \n\nLlama-3  \n\n3Filtering incorrect reasoning trace  \n\noutcome RMstepwise RMLets verify step by stepMath-shepherd:Verify and reinforce llms step-by-step without human annotations  \n\npromptMonte Carlo Tree Search (MCTS)Monte carlo tree search boosts reasoning via iterative preference learning  \n\n4Interleaving code and text reasoning  \n\npython codeTora: A tool-integrated reasoning agent for mathematical problem solving  \n\n5Learning from feedback and mistakes  \n\nLearning from mistakes makes llm better reasonerGenerating sequences by learning to self-correctSelf-refine: Iterative refinement with self-feedback  \n\n## Long Context  \n\n8k128k  \n\npost-training  \n\n1SFT  \n\nSFTSFTSFT  \n\n128kSFT  \n\nLlama-3reasoning  \n\n1Question answering  \n\n8kQAQA  \n\n2Summarization  \n\n8k8k  \n\nMetaQA  \n\n3Long context code reasoning  \n\nPython  \n\n  \n\n16K, 32K, 64K128K  \n\nSFT0.1%  \n\n2DPO  \n\nDPODPODPO  \n\n## Tool Use  \n\nLlama-3core tools  \n- Brave Search  \n- Python interpreter  \n- Mathematical computational engineWolfram Alpha API  \n\nqueryLlama-3plan  \n\ncore toolLlama-3zero-shotquery  \n\n1Implementation  \n\nMetacore toolsPython  \n\nzero-shot toolPython  \n\njsonWeb API  \n\nPythonLlama-3system promptcore toolsystem prompt  \n\n2Data collection  \n\nToolFormerLlama-3  \n\nmessage  \n\nrejection sampling  \n\nfinetune  \n\n3Tool datasets  \n\n  \n\n1Single-step tool use  \n\nfew-shot promptcore toolsquery  \n\nsystem prompt, user prompt, tool call, tool output, final answer  \n\n30%  \n\n2Multi-step tool use  \n\nLlama-32core toolpromptfew shot promptLlama-3ReAct  \n\n{% asset_img multi_step_tool.png  %}  \n\n3File uploads  \n\n.txt, .docx, .pdf, .pptx, .xlsx, .csv, .tsv, .py, .json, .jsonl, .html, .xml  \n\n  \n\n{% asset_img file_upload.png  %}  \n\nMetacase  \n\nqueryqueryresponsesystem promptavailable  \n\n4Zero-shot tool use data  \n\nLlama-3zero-shot  \n\nquery  \n\n1Single, nested, and parallel function calling  \n\n  \n\nToolverifier: Generalization to new tools via self-verificationStackThe stack: 3 tb of permissively licensed source codeLlama-3query  \n\n2Multi-turn function calling  \n\nApi-bank: A comprehensive benchmark for tool-augmented llms  \n\npromptLlama-3agentdomains, APIs, user queries, API calls,  responses  \n\n## Factuality  \n\nHallucination  \n\nMetapost-training know what it knows Does fine-tuning llms on new knowledge encourage hallucinations?Linguistic calibration through metacognition: aligning dialogue agent responses with expected correctness  \n\n --   \n\nMetaLlama-3in-contextknowledge probing  \n\n\n\n-   \n- Llama-3  \n- Llama-3  \n- context  \n- informativeness  \n- Llama-3responserefusal  \n\nMetaknowledge probing  \n\n  \n\n## Steerability  \n\n  \n\nLlama-3  \n\nMetasystem promptLlama-3response  \n\nannotatorLlama-3system promptannotatorsystem prompt  \n\nsystem prompt  \n\n{% asset_img steerability.png steerability %}  \n\n#   \n\n- Llama-3  \n- post-training  \n-   \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n- MoE  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[denseMoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  \n[MoE--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  \n-   \n[Llama3.1--](https://www.linsight.cn/7d7294cb.html)  \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[--](https://www.linsight.cn/210dbccd.html)  \n-   \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n-   \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n-   \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n- Transformer  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n-   \n[(1)](http://www.linsight.cn/3345028a.html)\n[(2)](http://www.linsight.cn/ad0bba9d.html)\n[(3)](http://www.linsight.cn/1736008.html)\n[(4)](http://www.linsight.cn/1736008.html)\n[(5)](http://www.linsight.cn/336f2f3e.html)\n[(6)](http://www.linsight.cn/7c04944d.html)\n[(7)](https://www.linsight.cn/dd614e12.html)\n[(8)](https://www.linsight.cn/e287b9c3.html)  \n\n# Reference  \n\n1The Llama 3 Herd of Models https://ai.meta.com/research/publications/the-llama-3-herd-of-models/   \n","slug":"cs/nlp/2024/07/Llama3-1-post-training","published":1,"updated":"2024-07-27T12:36:52.204Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsn0002a0p4kh8jtd4dw","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p><a href=\"https://www.linsight.cn/7d7294cb.html\">Llama3.1--</a>Llama-3.1post-training</p>\n<p>Llama-3pre-trainingpost-trainingSFTDPO</p>\n<p>Llama-3post-trainingroundpost-training6SFTDPO</p>\n<h1 id=\"modeling\">Modeling</h1>\n<p>post-training</p>\n<img src=\"/93328a2a/post_training.png\" class title=\"post-training\">\n<h2 id=\"chat-dialog-format\">Chat Dialog Format</h2>\n<p>Llama-3tool\nuseMetamulti-message\nchat protocol</p>\n<h2 id=\"reward-modeling\">Reward Modeling</h2>\n<p>reward modelRMpost-training</p>\n<p>Llama-2RMmargin\ntermchosenrejected\nresponsemargin\nterm</p>\n<p>Llama-2preference\ndataRM</p>\n<p>chosenrejected response --\nedited\nresponsechosenresponseranking\nsample3responseedited &gt; chosen &gt; rejected</p>\n<p>promptresponseprompt\n+ resp_1 + resp_2 +\nresp_3responsepromptprompt +\nresp_1, prompt + resp_2, prompt +\nresp_3accuracydocument\nmask</p>\n<h2 id=\"sft\">SFT</h2>\n<p>RMrejection samplinghuman annotation\npromptSFT</p>\n<p>SFTlr=1e-58.5k~9kpost-training</p>\n<h2 id=\"dpo\">DPO</h2>\n<p>DPOpost-trainingpolicy\nmodel</p>\n<p>DPOMetaon-policyPPODPOinstruction\nfollowingpost-trainingDPO</p>\n<p>DPOlr=1e-5beta=0.1</p>\n<p></p>\n<p>1Masking out formatting tokens in DPO loss</p>\n<p>tokenheadertermination\ntokenlosstokenlosschosen\nrepsponserejected\nresponsetokenlikelihood</p>\n<p>2Regularization with NLL loss</p>\n<p>DPOlossMetaNLLIterative reasoning\npreference optimizationPPOnext token\nprediction\nlossSFTchosen\nresponselog probabilitySmaug: Fixing failure modes of\npreference optimisation with dpo-positive</p>\n<h2 id=\"model-averaging\">Model Averaging</h2>\n<p>Averaging weights leads to wider optima and better\ngeneralizationModel soups: averaging weights of multiple fine-tuned\nmodels improves accuracy without increasing inference\ntimeBranch-train-merge: Embarrassingly parallel training of expert\nlanguage\nmodelsRMSFTDPO</p>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>Llama-2</p>\n<p></p>\n<p>user\npromptresponsechosenrejected\nresponse4<br>\n- significantly better - better<br>\n- slightly better<br>\n- marginally better</p>\n<p>chosen\nresponseresponse</p>\n<p></p>\n<img src=\"/93328a2a/preference_data.png\" class title=\"preference data\">\n<p>Llama-2Llama-3promptresponseLlama-3</p>\n<p>post-trainingprompt</p>\n<p>post-trainingRMDPO</p>\n<p>RMDPOsignificantly better \nbetter</p>\n<h2 id=\"sft-data\">SFT Data</h2>\n<p>SFT<br>\n- promptresponse<br>\n- capacities<br>\n- </p>\n<p>1RS</p>\n<p>RSprompt/chatK10~30RMConstitutional\nAI: harmlessness from AI feedback</p>\n<p>post-trainingRSsystem\npromptprompt</p>\n<p>2</p>\n<p>helpful</p>\n<img src=\"/93328a2a/sft_data.png\" class title=\"sft data\">\n<h2 id=\"-\"> &amp; </h2>\n<p></p>\n<p>1</p>\n<p>post-trainingemojipatternoverly-apologeticIm\nsorry</p>\n<p>2Data pruning</p>\n<p>model-based<br>\n- Llama-3-8B &amp; <br>\n-\nRMRMLlama-3\ncheckpointpromptpromptRMLlama-3<br>\n- InstagLlama<br>\n- Robertaquality score \ndifficulty\nscoreSemdedup:\nData-efficient learning at web-scale through semantic\ndeduplicationWhat makes good data for alignment? a comprehensive\nstudy of automatic data selection in instruction tuning</p>\n<h1 id=\"capabilities\">Capabilities</h1>\n<p>Meta</p>\n<h2 id=\"code\">Code</h2>\n<p>Python, Java, Javascript, C/C++,\nTypescript, Rust, PHP, HTML/CSS, SQL, bash/shell</p>\n<p>code\nexpertSFTsystem promptquality\nfilter</p>\n<h3 id=\"expert-training\">Expert training</h3>\n<p>1T&gt;85%CodeLlamacode\nexpert</p>\n<p>steprepo-levelcode\nexpert</p>\n<p>post-training</p>\n<p>code expert<br>\n- post-training<br>\n- code promptrejection sampling</p>\n<h3 id=\"\"></h3>\n<p></p>\n<p>Llama\n3code expertSFT</p>\n<h3 id=\"\"></h3>\n<p>2.7MSFT</p>\n<p>1</p>\n<p>Llama-38B70B405B405B</p>\n<p>Metaexecution\nfeedback</p>\n<p>1M</p>\n<p>1</p>\n<p>Magicoder:\nEmpowering code generation with oss-instruct</p>\n<p>2Solution</p>\n<p>Llama-3</p>\n<p>promptgeneral\nrule</p>\n<p>3</p>\n<p>solution</p>\n<p>parserlinter</p>\n<p></p>\n<p>4 &amp; </p>\n<p></p>\n<p>prompt</p>\n<p>20%</p>\n<p>5 &amp; </p>\n<p>roundround</p>\n<p>2programming language translation</p>\n<p>MetaLlama-3syntax\nparsing, compilation, executionBreaking\nlanguage barriers in multilingual mathematical reasoning: Insights and\nobservations</p>\n<p>3backtranslation</p>\n<p>documentationdebuggingexplanation+</p>\n<p>backtranslation<br>\n- Generate<br>\n- Backtranslate<br>\n-\nFilter/</p>\n<p>backtranslation1.2Mdocumentationdebuggingexplanation</p>\n<h3 id=\"\"></h3>\n<p>1system prompt</p>\n<p>system\npromptcomment</p>\n<img src=\"/93328a2a/code_sample.png\" class title=\"\">\n<p>2Filtering training data with execution and model-as-judge\nsignals</p>\n<p>rejection\nsamplingstraightforward</p>\n<p>model-as-judgeLlama-3</p>\n<p>responseresponseLlama-3</p>\n<h2 id=\"\"></h2>\n<p>Llama-38German, French, Italian, Portuguese, Hindi,\nSpanish, Thai</p>\n<h3 id=\"expert-training-1\">Expert training</h3>\n<p>90%data\nmixcode\nexpertpost-trainingexpert\nmodel</p>\n<h3 id=\"\"></h3>\n<p>SFT<br>\n- 2.4%<br>\n- 44.2%NLP task<br>\n- 18.8%rejection sampling<br>\n- 34.6%translated reasoning data</p>\n<p>1</p>\n<p>native\nspeaker</p>\n<p>2NLP task</p>\n<ul>\n<li>NLP<br>\n</li>\n<li>alignmentParallel global voices: a\ncollection of multilingual corpora with citizen media\nstoriesWikimediaparallel text<br>\n</li>\n<li>LID based filteringBlaser2.0 Seamlessm4tmassively\nmultilingual &amp; multimodal machine\ntranslation</li>\n</ul>\n<p>3</p>\n<p>RS<br>\n-\nGenerationpost-training0.2~1.00.6<br>\n-\nSelectionRMpromptresponse</p>\n<p>4</p>\n<p>synthetic\nquantitative reasoning data</p>\n<p></p>\n<h2 id=\"math-and-reasoning\">Math and Reasoning</h2>\n<p>reasoning</p>\n<p>reasoning<br>\n- prompt<br>\n-\nCoTreasoningCoT<br>\n- CoT<br>\n- <br>\n-\n</p>\n<p>Meta</p>\n<p>1prompt</p>\n<p>promptcontext</p>\n<p>promptMetacognitive\ncapabilities of llms: An exploration in mathematical problem\nsolvingprompt</p>\n<p>2Augmenting training data with step-wise reasoning traces</p>\n<p>Llama-3promptstep-by-step</p>\n<p>promptCommon\n7b language models already possess strong math capabilities</p>\n<p>Llama-3</p>\n<p>3Filtering incorrect reasoning trace</p>\n<p>outcome RMstepwise RMLets\nverify step by stepMath-shepherd:Verify and reinforce llms\nstep-by-step without human annotations</p>\n<p>promptMonte Carlo Tree Search (MCTS)Monte\ncarlo tree search boosts reasoning via iterative preference\nlearning</p>\n<p>4Interleaving code and text reasoning</p>\n<p>python\ncodeTora: A tool-integrated\nreasoning agent for mathematical problem solving</p>\n<p>5Learning from feedback and mistakes</p>\n<p>Learning\nfrom mistakes makes llm better reasonerGenerating sequences by\nlearning to self-correctSelf-refine: Iterative refinement with\nself-feedback</p>\n<h2 id=\"long-context\">Long Context</h2>\n<p>8k128k</p>\n<p>post-training</p>\n<p>1SFT</p>\n<p>SFTSFTSFT</p>\n<p>128kSFT</p>\n<p>Llama-3reasoning</p>\n<p>1Question answering</p>\n<p>8kQAQA</p>\n<p>2Summarization</p>\n<p>8k8k</p>\n<p>MetaQA</p>\n<p>3Long context code reasoning</p>\n<p>Python</p>\n<p></p>\n<p>16K, 32K,\n64K128K</p>\n<p>SFT0.1%</p>\n<p>2DPO</p>\n<p>DPODPODPO</p>\n<h2 id=\"tool-use\">Tool Use</h2>\n<p>Llama-3core\ntools<br>\n- Brave Search<br>\n- Python interpreter<br>\n- Mathematical computational engineWolfram Alpha API</p>\n<p>queryLlama-3plan</p>\n<p>core\ntoolLlama-3zero-shotquery</p>\n<p>1Implementation</p>\n<p>Metacore toolsPython</p>\n<p>zero-shot\ntoolPython</p>\n<p>jsonWeb API</p>\n<p>PythonLlama-3system\npromptcore toolsystem\nprompt</p>\n<p>2Data collection</p>\n<p>ToolFormerLlama-3</p>\n<p>message</p>\n<p>rejection\nsampling</p>\n<p>finetune</p>\n<p>3Tool datasets</p>\n<p></p>\n<p>1Single-step tool use</p>\n<p>few-shot promptcore\ntoolsquery</p>\n<p>system prompt, user prompt, tool call, tool output, final\nanswer</p>\n<p>30%</p>\n<p>2Multi-step tool use</p>\n<p>Llama-32core\ntoolpromptfew shot\npromptLlama-3ReAct</p>\n<img src=\"/93328a2a/multi_step_tool.png\" class title=\"\">\n<p>3File uploads</p>\n<p>.txt, .docx, .pdf, .pptx, .xlsx, .csv, .tsv, .py,\n.json, .jsonl, .html, .xml</p>\n<p></p>\n<img src=\"/93328a2a/file_upload.png\" class title=\"\">\n<p>Metacase</p>\n<p>queryqueryresponsesystem\npromptavailable</p>\n<p>4Zero-shot tool use data</p>\n<p>Llama-3zero-shot</p>\n<p>query</p>\n<p>1Single, nested, and parallel function calling</p>\n<p></p>\n<p>Toolverifier: Generalization to\nnew tools via self-verificationStackThe stack: 3\ntb of permissively licensed source\ncodeLlama-3query</p>\n<p>2Multi-turn function calling</p>\n<p>Api-bank: A comprehensive benchmark for tool-augmented\nllms</p>\n<p>promptLlama-3agentdomains,\nAPIs, user queries, API calls,  responses</p>\n<h2 id=\"factuality\">Factuality</h2>\n<p>Hallucination</p>\n<p>Metapost-training know what it knows\nDoes fine-tuning llms on new knowledge encourage\nhallucinations?Linguistic calibration through metacognition:\naligning dialogue agent responses with expected correctness</p>\n<p> --\n</p>\n<p>MetaLlama-3in-contextknowledge\nprobing</p>\n<p></p>\n<ul>\n<li><br>\n</li>\n<li>Llama-3<br>\n</li>\n<li>Llama-3<br>\n</li>\n<li>context<br>\n</li>\n<li>informativeness<br>\n</li>\n<li>Llama-3responserefusal</li>\n</ul>\n<p>Metaknowledge\nprobing</p>\n<p></p>\n<h2 id=\"steerability\">Steerability</h2>\n<p></p>\n<p>Llama-3</p>\n<p>Metasystem\npromptLlama-3response</p>\n<p>annotatorLlama-3system\npromptannotatorsystem\nprompt</p>\n<p>system prompt</p>\n<img src=\"/93328a2a/steerability.png\" class title=\"steerability\">\n<h1 id=\"\"></h1>\n<ul>\n<li>Llama-3<br>\n</li>\n<li>post-training<br>\n</li>\n<li></li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n- MoE<br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/a0824e29.html\">denseMoE -- sparse\nupcycling</a><br>\n<a href=\"https://www.linsight.cn/2c8bbc7.html\">MoE--expert choice\nrouting</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/7d7294cb.html\">Llama3.1--</a><br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"https://www.linsight.cn/210dbccd.html\">--</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n- Transformer<br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a> <a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a> <a href=\"http://www.linsight.cn/1736008.html\">(3)</a> <a href=\"http://www.linsight.cn/1736008.html\">(4)</a> <a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a> <a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a> <a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a> <a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1The Llama 3 Herd of Models\nhttps://ai.meta.com/research/publications/the-llama-3-herd-of-models/</p>\n","length":13122,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p><a href=\"https://www.linsight.cn/7d7294cb.html\">Llama3.1--</a>Llama-3.1post-training</p>\n<p>Llama-3pre-trainingpost-trainingSFTDPO</p>\n<p>Llama-3post-trainingroundpost-training6SFTDPO</p>\n<h1 id=\"modeling\">Modeling</h1>\n<p>post-training</p>\n<img src=\"/93328a2a/post_training.png\" class title=\"post-training\">\n<h2 id=\"chat-dialog-format\">Chat Dialog Format</h2>\n<p>Llama-3tool\nuseMetamulti-message\nchat protocol</p>\n<h2 id=\"reward-modeling\">Reward Modeling</h2>\n<p>reward modelRMpost-training</p>\n<p>Llama-2RMmargin\ntermchosenrejected\nresponsemargin\nterm</p>\n<p>Llama-2preference\ndataRM</p>\n<p>chosenrejected response --\nedited\nresponsechosenresponseranking\nsample3responseedited &gt; chosen &gt; rejected</p>\n<p>promptresponseprompt\n+ resp_1 + resp_2 +\nresp_3responsepromptprompt +\nresp_1, prompt + resp_2, prompt +\nresp_3accuracydocument\nmask</p>\n<h2 id=\"sft\">SFT</h2>\n<p>RMrejection samplinghuman annotation\npromptSFT</p>\n<p>SFTlr=1e-58.5k~9kpost-training</p>\n<h2 id=\"dpo\">DPO</h2>\n<p>DPOpost-trainingpolicy\nmodel</p>\n<p>DPOMetaon-policyPPODPOinstruction\nfollowingpost-trainingDPO</p>\n<p>DPOlr=1e-5beta=0.1</p>\n<p></p>\n<p>1Masking out formatting tokens in DPO loss</p>\n<p>tokenheadertermination\ntokenlosstokenlosschosen\nrepsponserejected\nresponsetokenlikelihood</p>\n<p>2Regularization with NLL loss</p>\n<p>DPOlossMetaNLLIterative reasoning\npreference optimizationPPOnext token\nprediction\nlossSFTchosen\nresponselog probabilitySmaug: Fixing failure modes of\npreference optimisation with dpo-positive</p>\n<h2 id=\"model-averaging\">Model Averaging</h2>\n<p>Averaging weights leads to wider optima and better\ngeneralizationModel soups: averaging weights of multiple fine-tuned\nmodels improves accuracy without increasing inference\ntimeBranch-train-merge: Embarrassingly parallel training of expert\nlanguage\nmodelsRMSFTDPO</p>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>Llama-2</p>\n<p></p>\n<p>user\npromptresponsechosenrejected\nresponse4<br>\n- significantly better - better<br>\n- slightly better<br>\n- marginally better</p>\n<p>chosen\nresponseresponse</p>\n<p></p>\n<img src=\"/93328a2a/preference_data.png\" class title=\"preference data\">\n<p>Llama-2Llama-3promptresponseLlama-3</p>\n<p>post-trainingprompt</p>\n<p>post-trainingRMDPO</p>\n<p>RMDPOsignificantly better \nbetter</p>\n<h2 id=\"sft-data\">SFT Data</h2>\n<p>SFT<br>\n- promptresponse<br>\n- capacities<br>\n- </p>\n<p>1RS</p>\n<p>RSprompt/chatK10~30RMConstitutional\nAI: harmlessness from AI feedback</p>\n<p>post-trainingRSsystem\npromptprompt</p>\n<p>2</p>\n<p>helpful</p>\n<img src=\"/93328a2a/sft_data.png\" class title=\"sft data\">\n<h2 id=\"-\"> &amp; </h2>\n<p></p>\n<p>1</p>\n<p>post-trainingemojipatternoverly-apologeticIm\nsorry</p>\n<p>2Data pruning</p>\n<p>model-based<br>\n- Llama-3-8B &amp; <br>\n-\nRMRMLlama-3\ncheckpointpromptpromptRMLlama-3<br>\n- InstagLlama<br>\n- Robertaquality score \ndifficulty\nscoreSemdedup:\nData-efficient learning at web-scale through semantic\ndeduplicationWhat makes good data for alignment? a comprehensive\nstudy of automatic data selection in instruction tuning</p>\n<h1 id=\"capabilities\">Capabilities</h1>\n<p>Meta</p>\n<h2 id=\"code\">Code</h2>\n<p>Python, Java, Javascript, C/C++,\nTypescript, Rust, PHP, HTML/CSS, SQL, bash/shell</p>\n<p>code\nexpertSFTsystem promptquality\nfilter</p>\n<h3 id=\"expert-training\">Expert training</h3>\n<p>1T&gt;85%CodeLlamacode\nexpert</p>\n<p>steprepo-levelcode\nexpert</p>\n<p>post-training</p>\n<p>code expert<br>\n- post-training<br>\n- code promptrejection sampling</p>\n<h3 id=\"\"></h3>\n<p></p>\n<p>Llama\n3code expertSFT</p>\n<h3 id=\"\"></h3>\n<p>2.7MSFT</p>\n<p>1</p>\n<p>Llama-38B70B405B405B</p>\n<p>Metaexecution\nfeedback</p>\n<p>1M</p>\n<p>1</p>\n<p>Magicoder:\nEmpowering code generation with oss-instruct</p>\n<p>2Solution</p>\n<p>Llama-3</p>\n<p>promptgeneral\nrule</p>\n<p>3</p>\n<p>solution</p>\n<p>parserlinter</p>\n<p></p>\n<p>4 &amp; </p>\n<p></p>\n<p>prompt</p>\n<p>20%</p>\n<p>5 &amp; </p>\n<p>roundround</p>\n<p>2programming language translation</p>\n<p>MetaLlama-3syntax\nparsing, compilation, executionBreaking\nlanguage barriers in multilingual mathematical reasoning: Insights and\nobservations</p>\n<p>3backtranslation</p>\n<p>documentationdebuggingexplanation+</p>\n<p>backtranslation<br>\n- Generate<br>\n- Backtranslate<br>\n-\nFilter/</p>\n<p>backtranslation1.2Mdocumentationdebuggingexplanation</p>\n<h3 id=\"\"></h3>\n<p>1system prompt</p>\n<p>system\npromptcomment</p>\n<img src=\"/93328a2a/code_sample.png\" class title=\"\">\n<p>2Filtering training data with execution and model-as-judge\nsignals</p>\n<p>rejection\nsamplingstraightforward</p>\n<p>model-as-judgeLlama-3</p>\n<p>responseresponseLlama-3</p>\n<h2 id=\"\"></h2>\n<p>Llama-38German, French, Italian, Portuguese, Hindi,\nSpanish, Thai</p>\n<h3 id=\"expert-training-1\">Expert training</h3>\n<p>90%data\nmixcode\nexpertpost-trainingexpert\nmodel</p>\n<h3 id=\"\"></h3>\n<p>SFT<br>\n- 2.4%<br>\n- 44.2%NLP task<br>\n- 18.8%rejection sampling<br>\n- 34.6%translated reasoning data</p>\n<p>1</p>\n<p>native\nspeaker</p>\n<p>2NLP task</p>\n<ul>\n<li>NLP<br>\n</li>\n<li>alignmentParallel global voices: a\ncollection of multilingual corpora with citizen media\nstoriesWikimediaparallel text<br>\n</li>\n<li>LID based filteringBlaser2.0 Seamlessm4tmassively\nmultilingual &amp; multimodal machine\ntranslation</li>\n</ul>\n<p>3</p>\n<p>RS<br>\n-\nGenerationpost-training0.2~1.00.6<br>\n-\nSelectionRMpromptresponse</p>\n<p>4</p>\n<p>synthetic\nquantitative reasoning data</p>\n<p></p>\n<h2 id=\"math-and-reasoning\">Math and Reasoning</h2>\n<p>reasoning</p>\n<p>reasoning<br>\n- prompt<br>\n-\nCoTreasoningCoT<br>\n- CoT<br>\n- <br>\n-\n</p>\n<p>Meta</p>\n<p>1prompt</p>\n<p>promptcontext</p>\n<p>promptMetacognitive\ncapabilities of llms: An exploration in mathematical problem\nsolvingprompt</p>\n<p>2Augmenting training data with step-wise reasoning traces</p>\n<p>Llama-3promptstep-by-step</p>\n<p>promptCommon\n7b language models already possess strong math capabilities</p>\n<p>Llama-3</p>\n<p>3Filtering incorrect reasoning trace</p>\n<p>outcome RMstepwise RMLets\nverify step by stepMath-shepherd:Verify and reinforce llms\nstep-by-step without human annotations</p>\n<p>promptMonte Carlo Tree Search (MCTS)Monte\ncarlo tree search boosts reasoning via iterative preference\nlearning</p>\n<p>4Interleaving code and text reasoning</p>\n<p>python\ncodeTora: A tool-integrated\nreasoning agent for mathematical problem solving</p>\n<p>5Learning from feedback and mistakes</p>\n<p>Learning\nfrom mistakes makes llm better reasonerGenerating sequences by\nlearning to self-correctSelf-refine: Iterative refinement with\nself-feedback</p>\n<h2 id=\"long-context\">Long Context</h2>\n<p>8k128k</p>\n<p>post-training</p>\n<p>1SFT</p>\n<p>SFTSFTSFT</p>\n<p>128kSFT</p>\n<p>Llama-3reasoning</p>\n<p>1Question answering</p>\n<p>8kQAQA</p>\n<p>2Summarization</p>\n<p>8k8k</p>\n<p>MetaQA</p>\n<p>3Long context code reasoning</p>\n<p>Python</p>\n<p></p>\n<p>16K, 32K,\n64K128K</p>\n<p>SFT0.1%</p>\n<p>2DPO</p>\n<p>DPODPODPO</p>\n<h2 id=\"tool-use\">Tool Use</h2>\n<p>Llama-3core\ntools<br>\n- Brave Search<br>\n- Python interpreter<br>\n- Mathematical computational engineWolfram Alpha API</p>\n<p>queryLlama-3plan</p>\n<p>core\ntoolLlama-3zero-shotquery</p>\n<p>1Implementation</p>\n<p>Metacore toolsPython</p>\n<p>zero-shot\ntoolPython</p>\n<p>jsonWeb API</p>\n<p>PythonLlama-3system\npromptcore toolsystem\nprompt</p>\n<p>2Data collection</p>\n<p>ToolFormerLlama-3</p>\n<p>message</p>\n<p>rejection\nsampling</p>\n<p>finetune</p>\n<p>3Tool datasets</p>\n<p></p>\n<p>1Single-step tool use</p>\n<p>few-shot promptcore\ntoolsquery</p>\n<p>system prompt, user prompt, tool call, tool output, final\nanswer</p>\n<p>30%</p>\n<p>2Multi-step tool use</p>\n<p>Llama-32core\ntoolpromptfew shot\npromptLlama-3ReAct</p>\n<img src=\"/93328a2a/multi_step_tool.png\" class title=\"\">\n<p>3File uploads</p>\n<p>.txt, .docx, .pdf, .pptx, .xlsx, .csv, .tsv, .py,\n.json, .jsonl, .html, .xml</p>\n<p></p>\n<img src=\"/93328a2a/file_upload.png\" class title=\"\">\n<p>Metacase</p>\n<p>queryqueryresponsesystem\npromptavailable</p>\n<p>4Zero-shot tool use data</p>\n<p>Llama-3zero-shot</p>\n<p>query</p>\n<p>1Single, nested, and parallel function calling</p>\n<p></p>\n<p>Toolverifier: Generalization to\nnew tools via self-verificationStackThe stack: 3\ntb of permissively licensed source\ncodeLlama-3query</p>\n<p>2Multi-turn function calling</p>\n<p>Api-bank: A comprehensive benchmark for tool-augmented\nllms</p>\n<p>promptLlama-3agentdomains,\nAPIs, user queries, API calls,  responses</p>\n<h2 id=\"factuality\">Factuality</h2>\n<p>Hallucination</p>\n<p>Metapost-training know what it knows\nDoes fine-tuning llms on new knowledge encourage\nhallucinations?Linguistic calibration through metacognition:\naligning dialogue agent responses with expected correctness</p>\n<p> --\n</p>\n<p>MetaLlama-3in-contextknowledge\nprobing</p>\n<p></p>\n<ul>\n<li><br>\n</li>\n<li>Llama-3<br>\n</li>\n<li>Llama-3<br>\n</li>\n<li>context<br>\n</li>\n<li>informativeness<br>\n</li>\n<li>Llama-3responserefusal</li>\n</ul>\n<p>Metaknowledge\nprobing</p>\n<p></p>\n<h2 id=\"steerability\">Steerability</h2>\n<p></p>\n<p>Llama-3</p>\n<p>Metasystem\npromptLlama-3response</p>\n<p>annotatorLlama-3system\npromptannotatorsystem\nprompt</p>\n<p>system prompt</p>\n<img src=\"/93328a2a/steerability.png\" class title=\"steerability\">\n<h1 id=\"\"></h1>\n<ul>\n<li>Llama-3<br>\n</li>\n<li>post-training<br>\n</li>\n<li></li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n- MoE<br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/a0824e29.html\">denseMoE -- sparse\nupcycling</a><br>\n<a href=\"https://www.linsight.cn/2c8bbc7.html\">MoE--expert choice\nrouting</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/7d7294cb.html\">Llama3.1--</a><br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"https://www.linsight.cn/210dbccd.html\">--</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n- Transformer<br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a> <a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a> <a href=\"http://www.linsight.cn/1736008.html\">(3)</a> <a href=\"http://www.linsight.cn/1736008.html\">(4)</a> <a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a> <a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a> <a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a> <a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1The Llama 3 Herd of Models\nhttps://ai.meta.com/research/publications/the-llama-3-herd-of-models/</p>\n"},{"title":"MoE--expert choice routing","abbrlink":"2c8bbc7","date":"2024-07-21T07:44:12.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nMoEgating networkexpert network  \n\ngating  \n\nMoEtoken choice routingtop-k routingtokenktoken  \n\ntokentokenexpert choice routingEC  \n\nrouting  \n\n{% asset_img intro.png routing %}  \n\ntoken choice routingtokenexpert choice  \n\n# token choice routing  \n\nMoEtoken choice routing  \n\n1Load Imbalance  \n\n2017Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer  \n\ntoken choicetokenkktoken  \n\ntoken  \n\nMoE  \n\nGshardSwitch TransformerST-MoEQwen2-MoEDeepSeek-MoElevel  \n\n2Under Specialization  \n\nMoEgatingtoken  \n\n3Same Compute for Every Token  \n\ntoken choice routingtokenexperttokentokenk  \n\ntokenk  \n\n# expert choice routing  \n\n##   \n\nexpert choice routingexperttokenbatchktoken  \n\nntokenek  \n\n$$k=\\frac{n\\times c}e$$  \n\nccapacity factortokenexperttoken choice routing  \n\n $X\\in\\mathbb{R}^{n\\times d}$ dhidden sizeexpert choice routing3IGP  \n\n$$S=\\mathrm{Softmax}(X\\cdot W_g),\\quad S\\in\\mathbb{R}^{n\\times e}\\\\G,I=\\mathrm{TopK}(S^\\top,k),P=\\mathrm{Onehot}(I)$$  \n\n$W_g\\in\\mathbb{R}^{d\\times e}$ expert embeddingStoken  \n\nIindex matrix$I[i,j]$ iexpertjtoken  \n\n$G\\in\\mathbb{R}^{e\\times k}$ gating matrixexperttoken  \n\nPpermutation matrixIone-hottoken  \n\n$$X_{in}=P\\cdot X$$  \n\n$X_{\\mathrm{in}}\\in\\mathbb{R}^{e\\times k\\times d}$ $X_\\text{in}[i]\\in\\mathbb{R}^{k\\times d}$ i  \n\n ${X}_e[i]$   \n\n$$X_e[i]=\\mathrm{GeLU}(X_{in}[i]\\cdot W_1[i])\\cdot W_2[i]^\\top $$  \n\nMoE $X_{\\mathrm{out}}\\in\\mathbb{R}^{n\\times d}$ PG  \n\n$$X_\\mathrm{out}[l,d]=\\sum_{i,j}P[i,j,l] G[i,j] X_e[i,j,d]$$  \n\n## constraint  \n\nexpert choice routingexpertexperttokentokentoken  \n\ntokenexpert  \n\n $A\\in\\mathbb{R}^{e\\times n}$  $A[i,j]$ ijtoken  \n\nA $TopK(A,k)$ I  \n\n$$\\max_A\\left\\langle S^\\top,A\\right\\rangle+\\lambda H(A)$$  \n\n$$\\begin{aligned}H(A)=\\sum_{ij}-A[i,j]\\log A[i,j]\\end{aligned}$$  \n\n$$\\mathrm{s.t.}\\quad\\forall i:\\sum_{j^{\\prime}}A[i,j^{\\prime}]=k; \\forall j:\\sum_{i^{\\prime}}A[i^{\\prime},j]\\leq b; \\forall i,j: 0\\leq A[i,j]\\leq1$$  \n\nbtokenH(A)sum of element-wise entropyH(A)  \n\n> Adding a small entropy term gives a near-integer solution while enabling a fast iterative solver we can run on TPUs.  \n\n  = 0.001  \n\n#   \n\nMoE  \n\n{% asset_img intro.png routing %}  \n\n## \n\n1Training Efficiency  \n\nstepGShard top-2 gatingEC-CF2 > 2x  \n\n{% asset_img efficiency.png efficiency %}  \n\nEC-CF2stepGShard top-2 gating20%  \n\n2Scaling the Number of Experts  \n\nexpert choice routingtop-2 routing  \n\n{% asset_img expert_num.png expert num %}  \n\n3Capped Expert Choice  \n\ntoken  \n\n{% asset_img capped.png Capped Expert Choice %}  \n\n23token  \n\n4Variable Experts per Token  \n\ntoken  \n\n{% asset_img dist.png Variable Experts per Token %}  \n\ntoken3%token  \n\ntokenEC  \n\n##   \n\n1Capacity Factor  \n\nCF  \n\n{% asset_img cf.png cf %}  \n\nCFCF=0.5token0.5switch transformertop-1  \n\n2Comparison with Dense Models on Pre-training  \n\nECppldense  \n\n{% asset_img dense.png Comparison with Dense Models on Pre-training %}  \n\n#   \n\n- ECtoken choiceECtoken  \n- cache  \n\n***  \n\n~  \n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n- MoE  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[denseMoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  \n-   \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n-   \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n-   \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n-   \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n- Transformer  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n-   \n[(1)](http://www.linsight.cn/3345028a.html)\n[(2)](http://www.linsight.cn/ad0bba9d.html)\n[(3)](http://www.linsight.cn/1736008.html)\n[(4)](http://www.linsight.cn/1736008.html)\n[(5)](http://www.linsight.cn/336f2f3e.html)\n[(6)](http://www.linsight.cn/7c04944d.html)\n[(7)](https://www.linsight.cn/dd614e12.html)\n[(8)](https://www.linsight.cn/e287b9c3.html)  \n\n# Reference  \n\n1Mixture-of-Experts with Expert Choice Routing https://arxiv.org/abs/2202.09368  \n","source":"_posts/cs/nlp/2024/07/MoE-expert-choice-routing.md","raw":"---\ntitle: MoE--expert choice routing\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - MoE\n  - routing\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 2c8bbc7\ndate: 2024-07-21 15:44:12\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nMoEgating networkexpert network  \n\ngating  \n\nMoEtoken choice routingtop-k routingtokenktoken  \n\ntokentokenexpert choice routingEC  \n\nrouting  \n\n{% asset_img intro.png routing %}  \n\ntoken choice routingtokenexpert choice  \n\n# token choice routing  \n\nMoEtoken choice routing  \n\n1Load Imbalance  \n\n2017Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer  \n\ntoken choicetokenkktoken  \n\ntoken  \n\nMoE  \n\nGshardSwitch TransformerST-MoEQwen2-MoEDeepSeek-MoElevel  \n\n2Under Specialization  \n\nMoEgatingtoken  \n\n3Same Compute for Every Token  \n\ntoken choice routingtokenexperttokentokenk  \n\ntokenk  \n\n# expert choice routing  \n\n##   \n\nexpert choice routingexperttokenbatchktoken  \n\nntokenek  \n\n$$k=\\frac{n\\times c}e$$  \n\nccapacity factortokenexperttoken choice routing  \n\n $X\\in\\mathbb{R}^{n\\times d}$ dhidden sizeexpert choice routing3IGP  \n\n$$S=\\mathrm{Softmax}(X\\cdot W_g),\\quad S\\in\\mathbb{R}^{n\\times e}\\\\G,I=\\mathrm{TopK}(S^\\top,k),P=\\mathrm{Onehot}(I)$$  \n\n$W_g\\in\\mathbb{R}^{d\\times e}$ expert embeddingStoken  \n\nIindex matrix$I[i,j]$ iexpertjtoken  \n\n$G\\in\\mathbb{R}^{e\\times k}$ gating matrixexperttoken  \n\nPpermutation matrixIone-hottoken  \n\n$$X_{in}=P\\cdot X$$  \n\n$X_{\\mathrm{in}}\\in\\mathbb{R}^{e\\times k\\times d}$ $X_\\text{in}[i]\\in\\mathbb{R}^{k\\times d}$ i  \n\n ${X}_e[i]$   \n\n$$X_e[i]=\\mathrm{GeLU}(X_{in}[i]\\cdot W_1[i])\\cdot W_2[i]^\\top $$  \n\nMoE $X_{\\mathrm{out}}\\in\\mathbb{R}^{n\\times d}$ PG  \n\n$$X_\\mathrm{out}[l,d]=\\sum_{i,j}P[i,j,l] G[i,j] X_e[i,j,d]$$  \n\n## constraint  \n\nexpert choice routingexpertexperttokentokentoken  \n\ntokenexpert  \n\n $A\\in\\mathbb{R}^{e\\times n}$  $A[i,j]$ ijtoken  \n\nA $TopK(A,k)$ I  \n\n$$\\max_A\\left\\langle S^\\top,A\\right\\rangle+\\lambda H(A)$$  \n\n$$\\begin{aligned}H(A)=\\sum_{ij}-A[i,j]\\log A[i,j]\\end{aligned}$$  \n\n$$\\mathrm{s.t.}\\quad\\forall i:\\sum_{j^{\\prime}}A[i,j^{\\prime}]=k; \\forall j:\\sum_{i^{\\prime}}A[i^{\\prime},j]\\leq b; \\forall i,j: 0\\leq A[i,j]\\leq1$$  \n\nbtokenH(A)sum of element-wise entropyH(A)  \n\n> Adding a small entropy term gives a near-integer solution while enabling a fast iterative solver we can run on TPUs.  \n\n  = 0.001  \n\n#   \n\nMoE  \n\n{% asset_img intro.png routing %}  \n\n## \n\n1Training Efficiency  \n\nstepGShard top-2 gatingEC-CF2 > 2x  \n\n{% asset_img efficiency.png efficiency %}  \n\nEC-CF2stepGShard top-2 gating20%  \n\n2Scaling the Number of Experts  \n\nexpert choice routingtop-2 routing  \n\n{% asset_img expert_num.png expert num %}  \n\n3Capped Expert Choice  \n\ntoken  \n\n{% asset_img capped.png Capped Expert Choice %}  \n\n23token  \n\n4Variable Experts per Token  \n\ntoken  \n\n{% asset_img dist.png Variable Experts per Token %}  \n\ntoken3%token  \n\ntokenEC  \n\n##   \n\n1Capacity Factor  \n\nCF  \n\n{% asset_img cf.png cf %}  \n\nCFCF=0.5token0.5switch transformertop-1  \n\n2Comparison with Dense Models on Pre-training  \n\nECppldense  \n\n{% asset_img dense.png Comparison with Dense Models on Pre-training %}  \n\n#   \n\n- ECtoken choiceECtoken  \n- cache  \n\n***  \n\n~  \n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n- MoE  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[denseMoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  \n-   \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n-   \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n-   \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n-   \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n- Transformer  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n-   \n[(1)](http://www.linsight.cn/3345028a.html)\n[(2)](http://www.linsight.cn/ad0bba9d.html)\n[(3)](http://www.linsight.cn/1736008.html)\n[(4)](http://www.linsight.cn/1736008.html)\n[(5)](http://www.linsight.cn/336f2f3e.html)\n[(6)](http://www.linsight.cn/7c04944d.html)\n[(7)](https://www.linsight.cn/dd614e12.html)\n[(8)](https://www.linsight.cn/e287b9c3.html)  \n\n# Reference  \n\n1Mixture-of-Experts with Expert Choice Routing https://arxiv.org/abs/2202.09368  \n","slug":"cs/nlp/2024/07/MoE-expert-choice-routing","published":1,"updated":"2024-07-22T14:46:05.168Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsn0002e0p4k3m414qtc","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>MoEgating networkexpert network</p>\n<p>gating</p>\n<p>MoEtoken choice routingtop-k\nroutingtokenktoken</p>\n<p>tokentokenexpert\nchoice routingEC</p>\n<p>routing</p>\n<img src=\"/2c8bbc7/intro.png\" class title=\"routing\">\n<p>token choice\nroutingtokenexpert\nchoice</p>\n<h1 id=\"token-choice-routing\">token choice routing</h1>\n<p>MoEtoken choice\nrouting</p>\n<p>1Load Imbalance</p>\n<p>2017Outrageously Large Neural\nNetworks: The Sparsely-Gated Mixture-of-Experts\nLayer</p>\n<p>token\nchoicetokenkktoken</p>\n<p>token</p>\n<p>MoE</p>\n<p>GshardSwitch\nTransformerST-MoEQwen2-MoEDeepSeek-MoElevel</p>\n<p>2Under Specialization</p>\n<p>MoEgatingtoken</p>\n<p>3Same Compute for Every Token</p>\n<p>token choice\nroutingtokenexperttokentokenk</p>\n<p>tokenk</p>\n<h1 id=\"expert-choice-routing\">expert choice routing</h1>\n<h2 id=\"\"></h2>\n<p>expert choice\nroutingexperttokenbatchktoken</p>\n<p>ntokenek</p>\n<p><span class=\"math display\">\\[k=\\frac{n\\times c}e\\]</span></p>\n<p>ccapacity\nfactortokenexperttoken choice\nrouting</p>\n<p> <span class=\"math inline\">\\(X\\in\\mathbb{R}^{n\\times\nd}\\)</span> dhidden sizeexpert choice\nrouting3IGP</p>\n<p><span class=\"math display\">\\[S=\\mathrm{Softmax}(X\\cdot W_g),\\quad\nS\\in\\mathbb{R}^{n\\times\ne}\\\\G,I=\\mathrm{TopK}(S^\\top,k),P=\\mathrm{Onehot}(I)\\]</span></p>\n<p><span class=\"math inline\">\\(W_g\\in\\mathbb{R}^{d\\times e}\\)</span>\nexpert embeddingStoken</p>\n<p>Iindex matrix<span class=\"math inline\">\\(I[i,j]\\)</span>\niexpertjtoken</p>\n<p><span class=\"math inline\">\\(G\\in\\mathbb{R}^{e\\times k}\\)</span>\ngating matrixexperttoken</p>\n<p>Ppermutation matrixIone-hottoken</p>\n<p><span class=\"math display\">\\[X_{in}=P\\cdot X\\]</span></p>\n<p><span class=\"math inline\">\\(X_{\\mathrm{in}}\\in\\mathbb{R}^{e\\times\nk\\times d}\\)</span> <span class=\"math inline\">\\(X_\\text{in}[i]\\in\\mathbb{R}^{k\\times d}\\)</span>\ni</p>\n<p> <span class=\"math inline\">\\({X}_e[i]\\)</span>\n</p>\n<p><span class=\"math display\">\\[X_e[i]=\\mathrm{GeLU}(X_{in}[i]\\cdot\nW_1[i])\\cdot W_2[i]^\\top \\]</span></p>\n<p>MoE <span class=\"math inline\">\\(X_{\\mathrm{out}}\\in\\mathbb{R}^{n\\times d}\\)</span>\nPG</p>\n<p><span class=\"math display\">\\[X_\\mathrm{out}[l,d]=\\sum_{i,j}P[i,j,l]\nG[i,j] X_e[i,j,d]\\]</span></p>\n<h2 id=\"constraint\">constraint</h2>\n<p>expert choice\nroutingexpertexperttokentokentoken</p>\n<p>tokenexpert</p>\n<p> <span class=\"math inline\">\\(A\\in\\mathbb{R}^{e\\times n}\\)</span>\n <span class=\"math inline\">\\(A[i,j]\\)</span>\nijtoken</p>\n<p>A <span class=\"math inline\">\\(TopK(A,k)\\)</span> I</p>\n<p><span class=\"math display\">\\[\\max_A\\left\\langle\nS^\\top,A\\right\\rangle+\\lambda H(A)\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}H(A)=\\sum_{ij}-A[i,j]\\log\nA[i,j]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\mathrm{s.t.}\\quad\\forall\ni:\\sum_{j^{\\prime}}A[i,j^{\\prime}]=k; \\forall\nj:\\sum_{i^{\\prime}}A[i^{\\prime},j]\\leq b; \\forall i,j: 0\\leq\nA[i,j]\\leq1\\]</span></p>\n<p>btokenH(A)sum of element-wise\nentropyH(A)</p>\n<blockquote>\n<p>Adding a small entropy term gives a near-integer solution while\nenabling a fast iterative solver we can run on TPUs.</p>\n</blockquote>\n<p>  = 0.001</p>\n<h1 id=\"\"></h1>\n<p>MoE</p>\n<img src=\"/2c8bbc7/intro.png\" class title=\"routing\">\n<h2 id=\"\"></h2>\n<p>1Training Efficiency</p>\n<p>stepGShard top-2 gatingEC-CF2 &gt;\n2x</p>\n<img src=\"/2c8bbc7/efficiency.png\" class title=\"efficiency\">\n<p>EC-CF2stepGShard top-2\ngating20%</p>\n<p>2Scaling the Number of Experts</p>\n<p>expert choice routingtop-2\nrouting</p>\n<img src=\"/2c8bbc7/expert_num.png\" class title=\"expert num\">\n<p>3Capped Expert Choice</p>\n<p>token</p>\n<img src=\"/2c8bbc7/capped.png\" class title=\"Capped Expert Choice\">\n<p>23token</p>\n<p>4Variable Experts per Token</p>\n<p>token</p>\n<img src=\"/2c8bbc7/dist.png\" class title=\"Variable Experts per Token\">\n<p>token3%token</p>\n<p>tokenEC</p>\n<h2 id=\"\"></h2>\n<p>1Capacity Factor</p>\n<p>CF</p>\n<img src=\"/2c8bbc7/cf.png\" class title=\"cf\">\n<p>CFCF=0.5token0.5switch\ntransformertop-1</p>\n<p>2Comparison with Dense Models on Pre-training</p>\n<p>ECppldense</p>\n<img src=\"/2c8bbc7/dense.png\" class title=\"Comparison with Dense Models on Pre-training\">\n<h1 id=\"\"></h1>\n<ul>\n<li>ECtoken\nchoiceECtoken<br>\n</li>\n<li>cache</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n- MoE<br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/a0824e29.html\">denseMoE -- sparse\nupcycling</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n- Transformer<br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a> <a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a> <a href=\"http://www.linsight.cn/1736008.html\">(3)</a> <a href=\"http://www.linsight.cn/1736008.html\">(4)</a> <a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a> <a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a> <a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a> <a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Mixture-of-Experts with Expert Choice Routing\nhttps://arxiv.org/abs/2202.09368</p>\n","length":4386,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>MoEgating networkexpert network</p>\n<p>gating</p>\n<p>MoEtoken choice routingtop-k\nroutingtokenktoken</p>\n<p>tokentokenexpert\nchoice routingEC</p>\n<p>routing</p>\n<img src=\"/2c8bbc7/intro.png\" class title=\"routing\">\n<p>token choice\nroutingtokenexpert\nchoice</p>\n<h1 id=\"token-choice-routing\">token choice routing</h1>\n<p>MoEtoken choice\nrouting</p>\n<p>1Load Imbalance</p>\n<p>2017Outrageously Large Neural\nNetworks: The Sparsely-Gated Mixture-of-Experts\nLayer</p>\n<p>token\nchoicetokenkktoken</p>\n<p>token</p>\n<p>MoE</p>\n<p>GshardSwitch\nTransformerST-MoEQwen2-MoEDeepSeek-MoElevel</p>\n<p>2Under Specialization</p>\n<p>MoEgatingtoken</p>\n<p>3Same Compute for Every Token</p>\n<p>token choice\nroutingtokenexperttokentokenk</p>\n<p>tokenk</p>\n<h1 id=\"expert-choice-routing\">expert choice routing</h1>\n<h2 id=\"\"></h2>\n<p>expert choice\nroutingexperttokenbatchktoken</p>\n<p>ntokenek</p>\n<p><span class=\"math display\">\\[k=\\frac{n\\times c}e\\]</span></p>\n<p>ccapacity\nfactortokenexperttoken choice\nrouting</p>\n<p> <span class=\"math inline\">\\(X\\in\\mathbb{R}^{n\\times\nd}\\)</span> dhidden sizeexpert choice\nrouting3IGP</p>\n<p><span class=\"math display\">\\[S=\\mathrm{Softmax}(X\\cdot W_g),\\quad\nS\\in\\mathbb{R}^{n\\times\ne}\\\\G,I=\\mathrm{TopK}(S^\\top,k),P=\\mathrm{Onehot}(I)\\]</span></p>\n<p><span class=\"math inline\">\\(W_g\\in\\mathbb{R}^{d\\times e}\\)</span>\nexpert embeddingStoken</p>\n<p>Iindex matrix<span class=\"math inline\">\\(I[i,j]\\)</span>\niexpertjtoken</p>\n<p><span class=\"math inline\">\\(G\\in\\mathbb{R}^{e\\times k}\\)</span>\ngating matrixexperttoken</p>\n<p>Ppermutation matrixIone-hottoken</p>\n<p><span class=\"math display\">\\[X_{in}=P\\cdot X\\]</span></p>\n<p><span class=\"math inline\">\\(X_{\\mathrm{in}}\\in\\mathbb{R}^{e\\times\nk\\times d}\\)</span> <span class=\"math inline\">\\(X_\\text{in}[i]\\in\\mathbb{R}^{k\\times d}\\)</span>\ni</p>\n<p> <span class=\"math inline\">\\({X}_e[i]\\)</span>\n</p>\n<p><span class=\"math display\">\\[X_e[i]=\\mathrm{GeLU}(X_{in}[i]\\cdot\nW_1[i])\\cdot W_2[i]^\\top \\]</span></p>\n<p>MoE <span class=\"math inline\">\\(X_{\\mathrm{out}}\\in\\mathbb{R}^{n\\times d}\\)</span>\nPG</p>\n<p><span class=\"math display\">\\[X_\\mathrm{out}[l,d]=\\sum_{i,j}P[i,j,l]\nG[i,j] X_e[i,j,d]\\]</span></p>\n<h2 id=\"constraint\">constraint</h2>\n<p>expert choice\nroutingexpertexperttokentokentoken</p>\n<p>tokenexpert</p>\n<p> <span class=\"math inline\">\\(A\\in\\mathbb{R}^{e\\times n}\\)</span>\n <span class=\"math inline\">\\(A[i,j]\\)</span>\nijtoken</p>\n<p>A <span class=\"math inline\">\\(TopK(A,k)\\)</span> I</p>\n<p><span class=\"math display\">\\[\\max_A\\left\\langle\nS^\\top,A\\right\\rangle+\\lambda H(A)\\]</span></p>\n<p><span class=\"math display\">\\[\\begin{aligned}H(A)=\\sum_{ij}-A[i,j]\\log\nA[i,j]\\end{aligned}\\]</span></p>\n<p><span class=\"math display\">\\[\\mathrm{s.t.}\\quad\\forall\ni:\\sum_{j^{\\prime}}A[i,j^{\\prime}]=k; \\forall\nj:\\sum_{i^{\\prime}}A[i^{\\prime},j]\\leq b; \\forall i,j: 0\\leq\nA[i,j]\\leq1\\]</span></p>\n<p>btokenH(A)sum of element-wise\nentropyH(A)</p>\n<blockquote>\n<p>Adding a small entropy term gives a near-integer solution while\nenabling a fast iterative solver we can run on TPUs.</p>\n</blockquote>\n<p>  = 0.001</p>\n<h1 id=\"\"></h1>\n<p>MoE</p>\n<img src=\"/2c8bbc7/intro.png\" class title=\"routing\">\n<h2 id=\"\"></h2>\n<p>1Training Efficiency</p>\n<p>stepGShard top-2 gatingEC-CF2 &gt;\n2x</p>\n<img src=\"/2c8bbc7/efficiency.png\" class title=\"efficiency\">\n<p>EC-CF2stepGShard top-2\ngating20%</p>\n<p>2Scaling the Number of Experts</p>\n<p>expert choice routingtop-2\nrouting</p>\n<img src=\"/2c8bbc7/expert_num.png\" class title=\"expert num\">\n<p>3Capped Expert Choice</p>\n<p>token</p>\n<img src=\"/2c8bbc7/capped.png\" class title=\"Capped Expert Choice\">\n<p>23token</p>\n<p>4Variable Experts per Token</p>\n<p>token</p>\n<img src=\"/2c8bbc7/dist.png\" class title=\"Variable Experts per Token\">\n<p>token3%token</p>\n<p>tokenEC</p>\n<h2 id=\"\"></h2>\n<p>1Capacity Factor</p>\n<p>CF</p>\n<img src=\"/2c8bbc7/cf.png\" class title=\"cf\">\n<p>CFCF=0.5token0.5switch\ntransformertop-1</p>\n<p>2Comparison with Dense Models on Pre-training</p>\n<p>ECppldense</p>\n<img src=\"/2c8bbc7/dense.png\" class title=\"Comparison with Dense Models on Pre-training\">\n<h1 id=\"\"></h1>\n<ul>\n<li>ECtoken\nchoiceECtoken<br>\n</li>\n<li>cache</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n- MoE<br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/a0824e29.html\">denseMoE -- sparse\nupcycling</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n- Transformer<br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a> <a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a> <a href=\"http://www.linsight.cn/1736008.html\">(3)</a> <a href=\"http://www.linsight.cn/1736008.html\">(4)</a> <a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a> <a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a> <a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a> <a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Mixture-of-Experts with Expert Choice Routing\nhttps://arxiv.org/abs/2202.09368</p>\n"},{"title":"Qwen2","abbrlink":"a8f8b641","date":"2024-07-17T14:01:21.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nQwen24dense1MoE0.5B57B  \n\n#   \n\nQwen25token  \n\n{% asset_img model.png Qwen2 %}  \n\n## tokenizer  \n\nQwen2Qwen1tokenizer  \n\n151,643token3control tokenvocab size151936  \n\n## dense model  \n\n- Qwen2GQAMHA  \n- SwiGLURoPERMSNorm + pre-norm  \n- BiasRoPE + Bias = QKVbiasRoPE  \n- Training-free long-context scaling of large language modelsDual Chunk AttentionDCAYaRNattention weightsrescale  \n\n## MoE model  \n\nQwen2-57B-A14Bfine-grained expertshared expert  \n\nQwen2-57B-A14BQwen2-7BSparse upcycling: Training mixture-ofexperts from dense checkpoints  \n\nh_E, nMoEFFNh_FFNFFN n  h_E / h_FFN   \n\nFFN copyintermeidateshuffle  \n\n50%50%\n\n#   \n\n##   \n\nQwen2  \n- Quality EnhancementQwen  \n- Data ExpansionQwen1.5Qwen230  \n- Distribution Improvement  \n\nQwen27T0.5Bdense7TQwen2-57B-A14B4.5T0.5B12T7T12T  \n\n##   \n\n409632,768  \n\nRoPEbase10,0001,000,000YaRNDual Chunk AttentionQwen2131,072  \n\n# POST-TRAINING  \n\nQwen2SFTRLHFcodingmathematicslogical reasoninginstruction following  multilingual comprehension   \n\n##   \n\nQwen2scalable alignment with minimal human annotationTowards scalable automated alignment of LLMs: A survey  \n\ncollaborative data annotation  automated data synthesis  \n\n1collaborative data annotation  \n\n- InsTag#InsTag: Instruction tagging for analyzing supervised fine-tuning of large language modelstagger  \n- tagtagHow abilities in large language models are affected by supervised fine-tuning data composition  \n- self-evolutionTree-Instruct: A preliminary study of the intrinsic relationship between complexity and alignmentQwen  \n- Qwenresponseresponseresponsedemonstrationpreference  \n\n2automated data synthesis  \n\n- Scaling relationship on learning mathematical reasoning with large language modelssolutionLLMresponseresponseresponse  \n- LLMsolutionsolutionSelf-play with execution feedback: Improving instruction-following capabilities of large language modelsLLMPythonresponse  \n- LLMLLMresponseLarge language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment  \n- Constitutional FeedbackConstitutional AI: Harmlessness from AI feedbackresponseresponse  \n\n## SFT  \n\n- >500,000  \n- 2epoch  \n- lr = 7e-6decay7e-7  \n- weight decay = 0.1  \n- gradient clip = 1.0  \n- seq length = 32,768  \n\n## RLHF  \n\nDPOOnline merging optimizers for boosting rewards and mitigating tax in alignmentOnline Merging Optimizeralignment tax  \n\n#   \n\n## base  \n\nbase  \n\n10.5B1.5B  \n\n{% asset_img eval_base_small.png  %}  \n\n27B  \n\n{% asset_img eval_base_7B.png  %}  \n\n332B57B-A14B  \n\n{% asset_img eval_base_large.png  %}  \n\n## INSTRUCTION-TUNED  \n\nit  \n\n10.5B1.5B  \n\n{% asset_img eval_chat_small.png  %}  \n\n27B  \n\n{% asset_img eval_chat_7B.png  %}  \n\n332B57B-A14B  \n\n{% asset_img eval_chat_large.png  %}  \n\n##   \n\nQwen23  \n\n1the Needle in a Haystack   \n\n{% asset_img eval_needle.png  %}  \n\n2NeedleBenchOpenCompass  \n\n  \n\n3LV-Eval  \n\n{% asset_img eval_long.png  %}  \n\n#   \n\n- Qwen2 MoEdenseupcycling  \n- 10T token12T0.5B  \n-   \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1QWEN2 TECHNICAL REPORT https://arxiv.org/abs/2407.10671  \n","source":"_posts/cs/nlp/2024/07/Qwen2.md","raw":"---\ntitle: Qwen2\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - Qwen\n  - MoE\n  - \n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: a8f8b641\ndate: 2024-07-17 22:01:21\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nQwen24dense1MoE0.5B57B  \n\n#   \n\nQwen25token  \n\n{% asset_img model.png Qwen2 %}  \n\n## tokenizer  \n\nQwen2Qwen1tokenizer  \n\n151,643token3control tokenvocab size151936  \n\n## dense model  \n\n- Qwen2GQAMHA  \n- SwiGLURoPERMSNorm + pre-norm  \n- BiasRoPE + Bias = QKVbiasRoPE  \n- Training-free long-context scaling of large language modelsDual Chunk AttentionDCAYaRNattention weightsrescale  \n\n## MoE model  \n\nQwen2-57B-A14Bfine-grained expertshared expert  \n\nQwen2-57B-A14BQwen2-7BSparse upcycling: Training mixture-ofexperts from dense checkpoints  \n\nh_E, nMoEFFNh_FFNFFN n  h_E / h_FFN   \n\nFFN copyintermeidateshuffle  \n\n50%50%\n\n#   \n\n##   \n\nQwen2  \n- Quality EnhancementQwen  \n- Data ExpansionQwen1.5Qwen230  \n- Distribution Improvement  \n\nQwen27T0.5Bdense7TQwen2-57B-A14B4.5T0.5B12T7T12T  \n\n##   \n\n409632,768  \n\nRoPEbase10,0001,000,000YaRNDual Chunk AttentionQwen2131,072  \n\n# POST-TRAINING  \n\nQwen2SFTRLHFcodingmathematicslogical reasoninginstruction following  multilingual comprehension   \n\n##   \n\nQwen2scalable alignment with minimal human annotationTowards scalable automated alignment of LLMs: A survey  \n\ncollaborative data annotation  automated data synthesis  \n\n1collaborative data annotation  \n\n- InsTag#InsTag: Instruction tagging for analyzing supervised fine-tuning of large language modelstagger  \n- tagtagHow abilities in large language models are affected by supervised fine-tuning data composition  \n- self-evolutionTree-Instruct: A preliminary study of the intrinsic relationship between complexity and alignmentQwen  \n- Qwenresponseresponseresponsedemonstrationpreference  \n\n2automated data synthesis  \n\n- Scaling relationship on learning mathematical reasoning with large language modelssolutionLLMresponseresponseresponse  \n- LLMsolutionsolutionSelf-play with execution feedback: Improving instruction-following capabilities of large language modelsLLMPythonresponse  \n- LLMLLMresponseLarge language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment  \n- Constitutional FeedbackConstitutional AI: Harmlessness from AI feedbackresponseresponse  \n\n## SFT  \n\n- >500,000  \n- 2epoch  \n- lr = 7e-6decay7e-7  \n- weight decay = 0.1  \n- gradient clip = 1.0  \n- seq length = 32,768  \n\n## RLHF  \n\nDPOOnline merging optimizers for boosting rewards and mitigating tax in alignmentOnline Merging Optimizeralignment tax  \n\n#   \n\n## base  \n\nbase  \n\n10.5B1.5B  \n\n{% asset_img eval_base_small.png  %}  \n\n27B  \n\n{% asset_img eval_base_7B.png  %}  \n\n332B57B-A14B  \n\n{% asset_img eval_base_large.png  %}  \n\n## INSTRUCTION-TUNED  \n\nit  \n\n10.5B1.5B  \n\n{% asset_img eval_chat_small.png  %}  \n\n27B  \n\n{% asset_img eval_chat_7B.png  %}  \n\n332B57B-A14B  \n\n{% asset_img eval_chat_large.png  %}  \n\n##   \n\nQwen23  \n\n1the Needle in a Haystack   \n\n{% asset_img eval_needle.png  %}  \n\n2NeedleBenchOpenCompass  \n\n  \n\n3LV-Eval  \n\n{% asset_img eval_long.png  %}  \n\n#   \n\n- Qwen2 MoEdenseupcycling  \n- 10T token12T0.5B  \n-   \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1QWEN2 TECHNICAL REPORT https://arxiv.org/abs/2407.10671  \n","slug":"cs/nlp/2024/07/Qwen2","published":1,"updated":"2024-07-17T14:27:48.841Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsn0002g0p4kcz8k0zgz","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>Qwen24dense1MoE0.5B57B</p>\n<h1 id=\"\"></h1>\n<p>Qwen25token</p>\n<img src=\"/a8f8b641/model.png\" class title=\"Qwen2\">\n<h2 id=\"tokenizer\">tokenizer</h2>\n<p>Qwen2Qwen1tokenizer</p>\n<p>151,643token3control\ntokenvocab\nsize151936</p>\n<h2 id=\"dense-model\">dense model</h2>\n<ul>\n<li>Qwen2GQAMHA<br>\n</li>\n<li>SwiGLURoPERMSNorm + pre-norm<br>\n</li>\n<li>BiasRoPE + Bias =\nQKVbiasRoPE<br>\n</li>\n<li>Training-free long-context scaling of large language\nmodelsDual Chunk AttentionDCAYaRNattention\nweightsrescale</li>\n</ul>\n<h2 id=\"moe-model\">MoE model</h2>\n<p>Qwen2-57B-A14Bfine-grained expertshared\nexpert</p>\n<p>Qwen2-57B-A14BQwen2-7BSparse upcycling: Training\nmixture-ofexperts from dense\ncheckpoints</p>\n<p>h_E,\nnMoEFFNh_FFNFFN\nn  h_E / h_FFN </p>\n<p>FFN\ncopyintermeidateshuffle</p>\n<p>50%50%</p>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>Qwen2<br>\n- Quality\nEnhancementQwen<br>\n- Data\nExpansionQwen1.5Qwen230<br>\n- Distribution\nImprovement</p>\n<p>Qwen27T0.5Bdense7TQwen2-57B-A14B4.5T0.5B12T7T12T</p>\n<h2 id=\"\"></h2>\n<p>409632,768</p>\n<p>RoPEbase10,0001,000,000YaRNDual\nChunk\nAttentionQwen2131,072</p>\n<h1 id=\"post-training\">POST-TRAINING</h1>\n<p>Qwen2SFTRLHFcodingmathematicslogical\nreasoninginstruction following  multilingual comprehension\n</p>\n<h2 id=\"\"></h2>\n<p>Qwen2scalable alignment with minimal\nhuman annotationTowards scalable automated alignment of LLMs: A\nsurvey</p>\n<p>collaborative data annotation \nautomated data synthesis</p>\n<p>1collaborative data annotation</p>\n<ul>\n<li>InsTag#InsTag: Instruction tagging for analyzing\nsupervised fine-tuning of large language\nmodelstagger<br>\n</li>\n<li>tagtagHow\nabilities in large language models are affected by supervised\nfine-tuning data composition<br>\n</li>\n<li>self-evolutionTree-Instruct:\nA preliminary study of the intrinsic relationship between complexity and\nalignmentQwen<br>\n</li>\n<li>Qwenresponseresponseresponsedemonstrationpreference</li>\n</ul>\n<p>2automated data synthesis</p>\n<ul>\n<li>Scaling\nrelationship on learning mathematical reasoning with large language\nmodelssolutionLLMresponseresponseresponse<br>\n</li>\n<li>LLMsolutionsolutionSelf-play\nwith execution feedback: Improving instruction-following capabilities of\nlarge language\nmodelsLLMPythonresponse<br>\n</li>\n<li>LLMLLMresponseLarge\nlanguage models are superpositions of all characters: Attaining\narbitrary role-play via\nself-alignment<br>\n</li>\n<li>Constitutional FeedbackConstitutional AI: Harmlessness from\nAI\nfeedbackresponseresponse</li>\n</ul>\n<h2 id=\"sft\">SFT</h2>\n<ul>\n<li>&gt;500,000<br>\n</li>\n<li>2epoch<br>\n</li>\n<li>lr = 7e-6decay7e-7<br>\n</li>\n<li>weight decay = 0.1<br>\n</li>\n<li>gradient clip = 1.0<br>\n</li>\n<li>seq length = 32,768</li>\n</ul>\n<h2 id=\"rlhf\">RLHF</h2>\n<p>DPOOnline merging optimizers for boosting\nrewards and mitigating tax in alignmentOnline Merging\nOptimizeralignment tax</p>\n<h1 id=\"\"></h1>\n<h2 id=\"base\">base</h2>\n<p>base</p>\n<p>10.5B1.5B</p>\n<img src=\"/a8f8b641/eval_base_small.png\" class title=\"\">\n<p>27B</p>\n<img src=\"/a8f8b641/eval_base_7B.png\" class title=\"\">\n<p>332B57B-A14B</p>\n<img src=\"/a8f8b641/eval_base_large.png\" class title=\"\">\n<h2 id=\"instruction-tuned\">INSTRUCTION-TUNED</h2>\n<p>it</p>\n<p>10.5B1.5B</p>\n<img src=\"/a8f8b641/eval_chat_small.png\" class title=\"\">\n<p>27B</p>\n<img src=\"/a8f8b641/eval_chat_7B.png\" class title=\"\">\n<p>332B57B-A14B</p>\n<img src=\"/a8f8b641/eval_chat_large.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p>Qwen23</p>\n<p>1the Needle in a Haystack</p>\n<img src=\"/a8f8b641/eval_needle.png\" class title=\"\">\n<p>2NeedleBenchOpenCompass</p>\n<p></p>\n<p>3LV-Eval</p>\n<img src=\"/a8f8b641/eval_long.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<ul>\n<li>Qwen2\nMoEdenseupcycling<br>\n</li>\n<li>10T\ntoken12T0.5B<br>\n</li>\n<li></li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1QWEN2 TECHNICAL REPORT https://arxiv.org/abs/2407.10671</p>\n","length":4493,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>Qwen24dense1MoE0.5B57B</p>\n<h1 id=\"\"></h1>\n<p>Qwen25token</p>\n<img src=\"/a8f8b641/model.png\" class title=\"Qwen2\">\n<h2 id=\"tokenizer\">tokenizer</h2>\n<p>Qwen2Qwen1tokenizer</p>\n<p>151,643token3control\ntokenvocab\nsize151936</p>\n<h2 id=\"dense-model\">dense model</h2>\n<ul>\n<li>Qwen2GQAMHA<br>\n</li>\n<li>SwiGLURoPERMSNorm + pre-norm<br>\n</li>\n<li>BiasRoPE + Bias =\nQKVbiasRoPE<br>\n</li>\n<li>Training-free long-context scaling of large language\nmodelsDual Chunk AttentionDCAYaRNattention\nweightsrescale</li>\n</ul>\n<h2 id=\"moe-model\">MoE model</h2>\n<p>Qwen2-57B-A14Bfine-grained expertshared\nexpert</p>\n<p>Qwen2-57B-A14BQwen2-7BSparse upcycling: Training\nmixture-ofexperts from dense\ncheckpoints</p>\n<p>h_E,\nnMoEFFNh_FFNFFN\nn  h_E / h_FFN </p>\n<p>FFN\ncopyintermeidateshuffle</p>\n<p>50%50%</p>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>Qwen2<br>\n- Quality\nEnhancementQwen<br>\n- Data\nExpansionQwen1.5Qwen230<br>\n- Distribution\nImprovement</p>\n<p>Qwen27T0.5Bdense7TQwen2-57B-A14B4.5T0.5B12T7T12T</p>\n<h2 id=\"\"></h2>\n<p>409632,768</p>\n<p>RoPEbase10,0001,000,000YaRNDual\nChunk\nAttentionQwen2131,072</p>\n<h1 id=\"post-training\">POST-TRAINING</h1>\n<p>Qwen2SFTRLHFcodingmathematicslogical\nreasoninginstruction following  multilingual comprehension\n</p>\n<h2 id=\"\"></h2>\n<p>Qwen2scalable alignment with minimal\nhuman annotationTowards scalable automated alignment of LLMs: A\nsurvey</p>\n<p>collaborative data annotation \nautomated data synthesis</p>\n<p>1collaborative data annotation</p>\n<ul>\n<li>InsTag#InsTag: Instruction tagging for analyzing\nsupervised fine-tuning of large language\nmodelstagger<br>\n</li>\n<li>tagtagHow\nabilities in large language models are affected by supervised\nfine-tuning data composition<br>\n</li>\n<li>self-evolutionTree-Instruct:\nA preliminary study of the intrinsic relationship between complexity and\nalignmentQwen<br>\n</li>\n<li>Qwenresponseresponseresponsedemonstrationpreference</li>\n</ul>\n<p>2automated data synthesis</p>\n<ul>\n<li>Scaling\nrelationship on learning mathematical reasoning with large language\nmodelssolutionLLMresponseresponseresponse<br>\n</li>\n<li>LLMsolutionsolutionSelf-play\nwith execution feedback: Improving instruction-following capabilities of\nlarge language\nmodelsLLMPythonresponse<br>\n</li>\n<li>LLMLLMresponseLarge\nlanguage models are superpositions of all characters: Attaining\narbitrary role-play via\nself-alignment<br>\n</li>\n<li>Constitutional FeedbackConstitutional AI: Harmlessness from\nAI\nfeedbackresponseresponse</li>\n</ul>\n<h2 id=\"sft\">SFT</h2>\n<ul>\n<li>&gt;500,000<br>\n</li>\n<li>2epoch<br>\n</li>\n<li>lr = 7e-6decay7e-7<br>\n</li>\n<li>weight decay = 0.1<br>\n</li>\n<li>gradient clip = 1.0<br>\n</li>\n<li>seq length = 32,768</li>\n</ul>\n<h2 id=\"rlhf\">RLHF</h2>\n<p>DPOOnline merging optimizers for boosting\nrewards and mitigating tax in alignmentOnline Merging\nOptimizeralignment tax</p>\n<h1 id=\"\"></h1>\n<h2 id=\"base\">base</h2>\n<p>base</p>\n<p>10.5B1.5B</p>\n<img src=\"/a8f8b641/eval_base_small.png\" class title=\"\">\n<p>27B</p>\n<img src=\"/a8f8b641/eval_base_7B.png\" class title=\"\">\n<p>332B57B-A14B</p>\n<img src=\"/a8f8b641/eval_base_large.png\" class title=\"\">\n<h2 id=\"instruction-tuned\">INSTRUCTION-TUNED</h2>\n<p>it</p>\n<p>10.5B1.5B</p>\n<img src=\"/a8f8b641/eval_chat_small.png\" class title=\"\">\n<p>27B</p>\n<img src=\"/a8f8b641/eval_chat_7B.png\" class title=\"\">\n<p>332B57B-A14B</p>\n<img src=\"/a8f8b641/eval_chat_large.png\" class title=\"\">\n<h2 id=\"\"></h2>\n<p>Qwen23</p>\n<p>1the Needle in a Haystack</p>\n<img src=\"/a8f8b641/eval_needle.png\" class title=\"\">\n<p>2NeedleBenchOpenCompass</p>\n<p></p>\n<p>3LV-Eval</p>\n<img src=\"/a8f8b641/eval_long.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<ul>\n<li>Qwen2\nMoEdenseupcycling<br>\n</li>\n<li>10T\ntoken12T0.5B<br>\n</li>\n<li></li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1QWEN2 TECHNICAL REPORT https://arxiv.org/abs/2407.10671</p>\n"},{"title":"Yuan2.0Yuan2.0-M32","date":"2024-07-03T12:14:06.000Z","abbrlink":"3df0cd42","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nYuan2.0Yuan2.0-M32  \n\n# Yuan2.0  \n\nYuan2.023113  \n\n{% asset_img yuan2_intro.png Yuan2.0 %}  \n\n##   \n\nself-attentiontokentokenlocal dependencyshort of neighbouring local associations of tokens  \n\nYuan2.0attentionLocalized Filtering-based AttentionLFAconvolutiontoken  \n\n{% asset_img lfa.png LFA %}  \n\nconvolution  \n\n{% asset_img lfa_conv.png LFA Conv %}  \n\nAttentionAttention with EMALFAEMA  \n\n{% asset_img lfa_result.png LFA result %}  \n\nEMAMega: moving average equipped gated attentionEMA  \n\n##   \n\nYuan2.0  \n\n{% asset_img yuan2_pretrain_data.png pretrain data %}  \n\n  \n\n  \n- BaikeBOOK  \n- Code Instruct data4M instruction Python solution  \n- StarCoderheader\\<reponame\\>, \\<filename\\>, \\<gh_stars\\>codetokentokenizer  \n\n  \n- Code Instruction Datasetpython  \n- Math Instruction Dataset  \n- Chat Instruction Dataset  \n\n{% asset_img yuan2_chat_data.png chat %}  \n\nYuan2.0  \n\nSFT  \n\n{% asset_img yuan2_sft_hp.png sft %}  \n\n## Tokenizer  \n\nYuan2.0SentencePieceUnigramtokenizer  \n\nparalle1.6T135vocab size30000tokenizer  \n\n135tokenizertokenizervocabtokenbyte size  \n\ntokenizertoken50000token  \n\n>7  \n\n90003000050000token73417token  \n\narxiv tokenizerStarCoder tokenizer  LLaMA tokenizer134953tokenizer  \n\n##   \n\nYuan2.0loss  \n\n{% asset_img yuan2_train_curve.png  %}  \n\n# Yuan2.0-M32  \n\nYuan2.0-M32Yuan2.0-2BMoELFA32240B3.7B  \n\n{% asset_img m32_intro.png  %}  \n\n##   \n\nYuan2.0-M32router  \n\nroutertokentokena  \n\n  \n\n  \n\nYuan2.0-M32attention routerb  \n\n{% asset_img router.png attention router %}  \n\ntokenI=dYuan2.0-M32d=2048N  \n\n$$Q=WI,\\quad W\\in\\mathbb{R}^{N\\times d}$$  \n\n$$K=W^{\\prime}I,\\quad W^{\\prime}\\in\\mathbb{R}^{N\\times d}$$  \n\n$$V=W^{^{\\prime\\prime}}I,\\quad W^{^{\\prime\\prime}}\\in\\mathbb{R}^{N\\times d}$$  \n\n$$P=\\mathrm{Softmax}(QK^T)\\mathrm{V},\\quad P\\in R^N$$  \n\nPtop M  \n\nrouter30B10B  \n\n{% asset_img router_eval.png attention router %}  \n\nattention routerclassical router8shared expert router16214  \n\nscalability50B10B8/16/32  \n\n{% asset_img scalability.png scalability %}  \n\nYuan2.0-M32Yuan2.0tokenizer  \n\n##   \n\n  \n\n{% asset_img train_hp.png train hp %}  \n\nYuan2.0-M322T tokenlossloss1.22  \n\n{% asset_img pretrain.png pretrain %}  \n\n4k16kCodeLLamaRoPEbase10000500k1MNTK-aware  \n\n$$b^{\\prime}=b\\cdot s^{\\frac{|D|}{|D|-2}}$$  \n\nDhead sizeYuan2.0-M32head size1284k16ks=4base=40890  \n\nbase=40890base40000, 80000, 160000, 320000, 640000, 1280000, 2560000, 5120000, 1024000040890  \n\n##   \n\nYuan2.0-M32code generationmathMMLUAI2 Reasoning Challenge (ARC) benchmark  \n\n{% asset_img eval1.png  %}  \n\n{% asset_img eval2.png  %}  \n\n{% asset_img eval3.png  %}  \n\n{% asset_img eval4.png  %}  \n\n#   \n\n- Yuan2.0Yuan2.0-M32  \n-   \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1Yuan 2.0-M32: Mixture of Experts with Attention Router https://arxiv.org/abs/2405.17976  \n2YUAN 2.0: A Large Language Model with Localized Filtering-based Attention https://arxiv.org/ftp/arxiv/papers/2311/2311.15786.pdf  \n\n","source":"_posts/cs/nlp/2024/07/Yuan2-0-M32.md","raw":"---\ntitle: Yuan2.0Yuan2.0-M32\ndate: 2024-07-03 20:14:06\ntags:\n  - NLP \n  - LLM \n  - transformer \n  -  \n  - MoE\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 3df0cd42\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nYuan2.0Yuan2.0-M32  \n\n# Yuan2.0  \n\nYuan2.023113  \n\n{% asset_img yuan2_intro.png Yuan2.0 %}  \n\n##   \n\nself-attentiontokentokenlocal dependencyshort of neighbouring local associations of tokens  \n\nYuan2.0attentionLocalized Filtering-based AttentionLFAconvolutiontoken  \n\n{% asset_img lfa.png LFA %}  \n\nconvolution  \n\n{% asset_img lfa_conv.png LFA Conv %}  \n\nAttentionAttention with EMALFAEMA  \n\n{% asset_img lfa_result.png LFA result %}  \n\nEMAMega: moving average equipped gated attentionEMA  \n\n##   \n\nYuan2.0  \n\n{% asset_img yuan2_pretrain_data.png pretrain data %}  \n\n  \n\n  \n- BaikeBOOK  \n- Code Instruct data4M instruction Python solution  \n- StarCoderheader\\<reponame\\>, \\<filename\\>, \\<gh_stars\\>codetokentokenizer  \n\n  \n- Code Instruction Datasetpython  \n- Math Instruction Dataset  \n- Chat Instruction Dataset  \n\n{% asset_img yuan2_chat_data.png chat %}  \n\nYuan2.0  \n\nSFT  \n\n{% asset_img yuan2_sft_hp.png sft %}  \n\n## Tokenizer  \n\nYuan2.0SentencePieceUnigramtokenizer  \n\nparalle1.6T135vocab size30000tokenizer  \n\n135tokenizertokenizervocabtokenbyte size  \n\ntokenizertoken50000token  \n\n>7  \n\n90003000050000token73417token  \n\narxiv tokenizerStarCoder tokenizer  LLaMA tokenizer134953tokenizer  \n\n##   \n\nYuan2.0loss  \n\n{% asset_img yuan2_train_curve.png  %}  \n\n# Yuan2.0-M32  \n\nYuan2.0-M32Yuan2.0-2BMoELFA32240B3.7B  \n\n{% asset_img m32_intro.png  %}  \n\n##   \n\nYuan2.0-M32router  \n\nroutertokentokena  \n\n  \n\n  \n\nYuan2.0-M32attention routerb  \n\n{% asset_img router.png attention router %}  \n\ntokenI=dYuan2.0-M32d=2048N  \n\n$$Q=WI,\\quad W\\in\\mathbb{R}^{N\\times d}$$  \n\n$$K=W^{\\prime}I,\\quad W^{\\prime}\\in\\mathbb{R}^{N\\times d}$$  \n\n$$V=W^{^{\\prime\\prime}}I,\\quad W^{^{\\prime\\prime}}\\in\\mathbb{R}^{N\\times d}$$  \n\n$$P=\\mathrm{Softmax}(QK^T)\\mathrm{V},\\quad P\\in R^N$$  \n\nPtop M  \n\nrouter30B10B  \n\n{% asset_img router_eval.png attention router %}  \n\nattention routerclassical router8shared expert router16214  \n\nscalability50B10B8/16/32  \n\n{% asset_img scalability.png scalability %}  \n\nYuan2.0-M32Yuan2.0tokenizer  \n\n##   \n\n  \n\n{% asset_img train_hp.png train hp %}  \n\nYuan2.0-M322T tokenlossloss1.22  \n\n{% asset_img pretrain.png pretrain %}  \n\n4k16kCodeLLamaRoPEbase10000500k1MNTK-aware  \n\n$$b^{\\prime}=b\\cdot s^{\\frac{|D|}{|D|-2}}$$  \n\nDhead sizeYuan2.0-M32head size1284k16ks=4base=40890  \n\nbase=40890base40000, 80000, 160000, 320000, 640000, 1280000, 2560000, 5120000, 1024000040890  \n\n##   \n\nYuan2.0-M32code generationmathMMLUAI2 Reasoning Challenge (ARC) benchmark  \n\n{% asset_img eval1.png  %}  \n\n{% asset_img eval2.png  %}  \n\n{% asset_img eval3.png  %}  \n\n{% asset_img eval4.png  %}  \n\n#   \n\n- Yuan2.0Yuan2.0-M32  \n-   \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1Yuan 2.0-M32: Mixture of Experts with Attention Router https://arxiv.org/abs/2405.17976  \n2YUAN 2.0: A Large Language Model with Localized Filtering-based Attention https://arxiv.org/ftp/arxiv/papers/2311/2311.15786.pdf  \n\n","slug":"cs/nlp/2024/07/Yuan2-0-M32","published":1,"updated":"2024-07-18T12:22:47.447Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsn1002k0p4k3atkhqd7","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>Yuan2.0Yuan2.0-M32</p>\n<h1 id=\"yuan2.0\">Yuan2.0</h1>\n<p>Yuan2.023113</p>\n<img src=\"/3df0cd42/yuan2_intro.png\" class title=\"Yuan2.0\">\n<h2 id=\"\"></h2>\n<p>self-attentiontokentokenlocal\ndependencyshort of neighbouring local associations\nof tokens</p>\n<p>Yuan2.0attentionLocalized Filtering-based\nAttentionLFAconvolutiontoken</p>\n<img src=\"/3df0cd42/lfa.png\" class title=\"LFA\">\n<p>convolution</p>\n<img src=\"/3df0cd42/lfa_conv.png\" class title=\"LFA Conv\">\n<p>AttentionAttention with\nEMALFAEMA</p>\n<img src=\"/3df0cd42/lfa_result.png\" class title=\"LFA result\">\n<p>EMAMega: moving average equipped gated\nattentionEMA</p>\n<h2 id=\"\"></h2>\n<p>Yuan2.0</p>\n<img src=\"/3df0cd42/yuan2_pretrain_data.png\" class title=\"pretrain data\">\n<p></p>\n<p><br>\n- BaikeBOOK<br>\n- Code Instruct data4M instruction Python\nsolution<br>\n- StarCoderheader&lt;reponame&gt;, &lt;filename&gt;,\n&lt;gh_stars&gt;codetokentokenizer</p>\n<p><br>\n- Code Instruction Datasetpython<br>\n- Math Instruction Dataset<br>\n- Chat Instruction Dataset</p>\n<img src=\"/3df0cd42/yuan2_chat_data.png\" class title=\"chat\">\n<p>Yuan2.0</p>\n<p>SFT</p>\n<img src=\"/3df0cd42/yuan2_sft_hp.png\" class title=\"sft\">\n<h2 id=\"tokenizer\">Tokenizer</h2>\n<p>Yuan2.0SentencePieceUnigramtokenizer</p>\n<p>paralle1.6T135vocab\nsize30000tokenizer</p>\n<p>135tokenizertokenizervocabtokenbyte\nsize</p>\n<p>tokenizertoken50000token</p>\n<p>&gt;7</p>\n<p>90003000050000token73417token</p>\n<p>arxiv tokenizerStarCoder\ntokenizer  LLaMA\ntokenizer134953tokenizer</p>\n<h2 id=\"\"></h2>\n<p>Yuan2.0loss</p>\n<img src=\"/3df0cd42/yuan2_train_curve.png\" class title=\"\">\n<h1 id=\"yuan2.0-m32\">Yuan2.0-M32</h1>\n<p>Yuan2.0-M32Yuan2.0-2BMoELFA32240B3.7B</p>\n<img src=\"/3df0cd42/m32_intro.png\" class title=\"\">\n<h2 id=\"-1\"></h2>\n<p>Yuan2.0-M32router</p>\n<p>routertokentokena</p>\n<p></p>\n<p></p>\n<p>Yuan2.0-M32attention routerb</p>\n<img src=\"/3df0cd42/router.png\" class title=\"attention router\">\n<p>tokenI=dYuan2.0-M32d=2048N</p>\n<p><span class=\"math display\">\\[Q=WI,\\quad W\\in\\mathbb{R}^{N\\times\nd}\\]</span></p>\n<p><span class=\"math display\">\\[K=W^{\\prime}I,\\quad\nW^{\\prime}\\in\\mathbb{R}^{N\\times d}\\]</span></p>\n<p><span class=\"math display\">\\[V=W^{^{\\prime\\prime}}I,\\quad\nW^{^{\\prime\\prime}}\\in\\mathbb{R}^{N\\times d}\\]</span></p>\n<p><span class=\"math display\">\\[P=\\mathrm{Softmax}(QK^T)\\mathrm{V},\\quad\nP\\in R^N\\]</span></p>\n<p>Ptop M</p>\n<p>router30B10B</p>\n<img src=\"/3df0cd42/router_eval.png\" class title=\"attention router\">\n<p>attention routerclassical router8shared expert\nrouter16214</p>\n<p>scalability50B10B8/16/32</p>\n<img src=\"/3df0cd42/scalability.png\" class title=\"scalability\">\n<p>Yuan2.0-M32Yuan2.0tokenizer</p>\n<h2 id=\"-1\"></h2>\n<p></p>\n<img src=\"/3df0cd42/train_hp.png\" class title=\"train hp\">\n<p>Yuan2.0-M322T\ntokenlossloss1.22</p>\n<img src=\"/3df0cd42/pretrain.png\" class title=\"pretrain\">\n<p>4k16kCodeLLamaRoPEbase10000500k1MNTK-aware</p>\n<p><span class=\"math display\">\\[b^{\\prime}=b\\cdot\ns^{\\frac{|D|}{|D|-2}}\\]</span></p>\n<p>Dhead sizeYuan2.0-M32head\nsize1284k16ks=4base=40890</p>\n<p>base=40890base40000, 80000, 160000, 320000, 640000,\n1280000, 2560000, 5120000,\n1024000040890</p>\n<h2 id=\"\"></h2>\n<p>Yuan2.0-M32code generationmathMMLUAI2 Reasoning Challenge\n(ARC) benchmark</p>\n<img src=\"/3df0cd42/eval1.png\" class title=\"\">\n<img src=\"/3df0cd42/eval2.png\" class title=\"\">\n<img src=\"/3df0cd42/eval3.png\" class title=\"\">\n<img src=\"/3df0cd42/eval4.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<ul>\n<li>Yuan2.0Yuan2.0-M32<br>\n</li>\n<li></li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Yuan 2.0-M32: Mixture of Experts with Attention Router\nhttps://arxiv.org/abs/2405.17976<br>\n2YUAN 2.0: A Large Language Model with Localized Filtering-based\nAttention https://arxiv.org/ftp/arxiv/papers/2311/2311.15786.pdf</p>\n","length":3517,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>Yuan2.0Yuan2.0-M32</p>\n<h1 id=\"yuan2.0\">Yuan2.0</h1>\n<p>Yuan2.023113</p>\n<img src=\"/3df0cd42/yuan2_intro.png\" class title=\"Yuan2.0\">\n<h2 id=\"\"></h2>\n<p>self-attentiontokentokenlocal\ndependencyshort of neighbouring local associations\nof tokens</p>\n<p>Yuan2.0attentionLocalized Filtering-based\nAttentionLFAconvolutiontoken</p>\n<img src=\"/3df0cd42/lfa.png\" class title=\"LFA\">\n<p>convolution</p>\n<img src=\"/3df0cd42/lfa_conv.png\" class title=\"LFA Conv\">\n<p>AttentionAttention with\nEMALFAEMA</p>\n<img src=\"/3df0cd42/lfa_result.png\" class title=\"LFA result\">\n<p>EMAMega: moving average equipped gated\nattentionEMA</p>\n<h2 id=\"\"></h2>\n<p>Yuan2.0</p>\n<img src=\"/3df0cd42/yuan2_pretrain_data.png\" class title=\"pretrain data\">\n<p></p>\n<p><br>\n- BaikeBOOK<br>\n- Code Instruct data4M instruction Python\nsolution<br>\n- StarCoderheader&lt;reponame&gt;, &lt;filename&gt;,\n&lt;gh_stars&gt;codetokentokenizer</p>\n<p><br>\n- Code Instruction Datasetpython<br>\n- Math Instruction Dataset<br>\n- Chat Instruction Dataset</p>\n<img src=\"/3df0cd42/yuan2_chat_data.png\" class title=\"chat\">\n<p>Yuan2.0</p>\n<p>SFT</p>\n<img src=\"/3df0cd42/yuan2_sft_hp.png\" class title=\"sft\">\n<h2 id=\"tokenizer\">Tokenizer</h2>\n<p>Yuan2.0SentencePieceUnigramtokenizer</p>\n<p>paralle1.6T135vocab\nsize30000tokenizer</p>\n<p>135tokenizertokenizervocabtokenbyte\nsize</p>\n<p>tokenizertoken50000token</p>\n<p>&gt;7</p>\n<p>90003000050000token73417token</p>\n<p>arxiv tokenizerStarCoder\ntokenizer  LLaMA\ntokenizer134953tokenizer</p>\n<h2 id=\"\"></h2>\n<p>Yuan2.0loss</p>\n<img src=\"/3df0cd42/yuan2_train_curve.png\" class title=\"\">\n<h1 id=\"yuan2.0-m32\">Yuan2.0-M32</h1>\n<p>Yuan2.0-M32Yuan2.0-2BMoELFA32240B3.7B</p>\n<img src=\"/3df0cd42/m32_intro.png\" class title=\"\">\n<h2 id=\"-1\"></h2>\n<p>Yuan2.0-M32router</p>\n<p>routertokentokena</p>\n<p></p>\n<p></p>\n<p>Yuan2.0-M32attention routerb</p>\n<img src=\"/3df0cd42/router.png\" class title=\"attention router\">\n<p>tokenI=dYuan2.0-M32d=2048N</p>\n<p><span class=\"math display\">\\[Q=WI,\\quad W\\in\\mathbb{R}^{N\\times\nd}\\]</span></p>\n<p><span class=\"math display\">\\[K=W^{\\prime}I,\\quad\nW^{\\prime}\\in\\mathbb{R}^{N\\times d}\\]</span></p>\n<p><span class=\"math display\">\\[V=W^{^{\\prime\\prime}}I,\\quad\nW^{^{\\prime\\prime}}\\in\\mathbb{R}^{N\\times d}\\]</span></p>\n<p><span class=\"math display\">\\[P=\\mathrm{Softmax}(QK^T)\\mathrm{V},\\quad\nP\\in R^N\\]</span></p>\n<p>Ptop M</p>\n<p>router30B10B</p>\n<img src=\"/3df0cd42/router_eval.png\" class title=\"attention router\">\n<p>attention routerclassical router8shared expert\nrouter16214</p>\n<p>scalability50B10B8/16/32</p>\n<img src=\"/3df0cd42/scalability.png\" class title=\"scalability\">\n<p>Yuan2.0-M32Yuan2.0tokenizer</p>\n<h2 id=\"-1\"></h2>\n<p></p>\n<img src=\"/3df0cd42/train_hp.png\" class title=\"train hp\">\n<p>Yuan2.0-M322T\ntokenlossloss1.22</p>\n<img src=\"/3df0cd42/pretrain.png\" class title=\"pretrain\">\n<p>4k16kCodeLLamaRoPEbase10000500k1MNTK-aware</p>\n<p><span class=\"math display\">\\[b^{\\prime}=b\\cdot\ns^{\\frac{|D|}{|D|-2}}\\]</span></p>\n<p>Dhead sizeYuan2.0-M32head\nsize1284k16ks=4base=40890</p>\n<p>base=40890base40000, 80000, 160000, 320000, 640000,\n1280000, 2560000, 5120000,\n1024000040890</p>\n<h2 id=\"\"></h2>\n<p>Yuan2.0-M32code generationmathMMLUAI2 Reasoning Challenge\n(ARC) benchmark</p>\n<img src=\"/3df0cd42/eval1.png\" class title=\"\">\n<img src=\"/3df0cd42/eval2.png\" class title=\"\">\n<img src=\"/3df0cd42/eval3.png\" class title=\"\">\n<img src=\"/3df0cd42/eval4.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<ul>\n<li>Yuan2.0Yuan2.0-M32<br>\n</li>\n<li></li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1Yuan 2.0-M32: Mixture of Experts with Attention Router\nhttps://arxiv.org/abs/2405.17976<br>\n2YUAN 2.0: A Large Language Model with Localized Filtering-based\nAttention https://arxiv.org/ftp/arxiv/papers/2311/2311.15786.pdf</p>\n"},{"title":"bilibiliindex-1.9B","abbrlink":"770b63e1","date":"2024-07-05T08:16:59.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nbilibiliIndex-1.9B  \n- Index-1.9B base2.8T  \n- Index-1.9B purebase  \n- Index-1.9B chatbaseSFTDPO  \n- Index-1.9B characterchatRAG+fewshots  \n\n  \n\n#   \n\n1  \n\n3691.01B36base  \n\n{% asset_img layer_num.png  %}  \n\nactivationL * hidden size  \n\n2Norm-Head  \n\n>  LM-Head  LM-Head \n LM-Head   \n\nBaichuan2Norm-HeadLM-HeadNorm  \n\nNorm-HeadGradient Norm  \n\n{% asset_img norm_head.png norm head %}  \n\nNorm-HeadGradient Norm  \n\n#   \n\n##   \n\n  \n- 2.8T  \n-  = 45 = 6%  \n- STEM10%  \n\n  \n\n{% asset_img pt_data.png  %}  \n\n[https://github.com/google-research/deduplicate-text-datasets](https://github.com/google-research/deduplicate-text-datasets)  \n\n 15.6w   \n\n{% asset_img dedup.png  %}  \n\npackingattention-maskposition-id  \n\n##   \n\n1  \n\n- AdamWbeta_1=0.9beta_2=0.95eps=1e-8  \n- gradient clip=1.0  \n- weight decay=0.1  \n\n2lrscheduler  \n\nWSDlearning rate scheduler  \n- warmup 100 +   \n- decay  \n\ndecaylr5e-4decay1%5e-6decay400B  \n\n0.1BCosinLinearWSD1Tloss  \n\n{% asset_img scheduler.png scheduler %}  \n\n  \n- schedulervalid loss  \n- WSDSvalid lossdecay  \n-   \n\n3decay  \n\nWSDdecayloss  \n\n  \n- Cosine  \n- WSD  \n- Cosine + 10%  \n- WSD + 10%decay  \n\ndecayWSD  \n\n{% asset_img wsd_quality.png wsd quallity %}  \n\n4  \n\ndecay  \n\n  \n- index-1.9b-ablation-pureDecay ()  \n- index-1.9b-ablation-boostpure7%  \n\nMMLU  \n\n{% asset_img add_instruct_data.png  %}  \n\ndecay  \n\n#   \n\n## SFT  \n\n1  \n\n- 10M  \n-   \n-   \n- /  \n\n2  \n\n- lr=1e-5  \n-  system-query-response   \n\nSFTSFTresponsetoken4:6  \n\n{% asset_img sft.png SFT %}  \n\n## DPO  \n\nDPO\n\nprompt SFT  ppl   \n\nlr =  1e-6\nscheduler = cosine\nepoch = 1\ndpo beta = 1\n\n  \n\n{% asset_img dpo.png DPO %}  \n\n#   \n\nIndex-1.9B  \n- WSDdecay  \n- SFT  \n- Norm-Head  \n\n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1https://github.com/bilibili/Index-1.9B/blob/main/Index-1.9B%20%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A.pdf  \n\n","source":"_posts/cs/nlp/2024/07/bilibiliindex-1-9B.md","raw":"---\ntitle: bilibiliindex-1.9B\nabbrlink: 770b63e1\ndate: 2024-07-05 16:16:59\ntags:\n  - NLP \n  - LLM \n  - transformer \n  -  \ncategories:\n  - CS\n  - NLP\n  - LLM\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nbilibiliIndex-1.9B  \n- Index-1.9B base2.8T  \n- Index-1.9B purebase  \n- Index-1.9B chatbaseSFTDPO  \n- Index-1.9B characterchatRAG+fewshots  \n\n  \n\n#   \n\n1  \n\n3691.01B36base  \n\n{% asset_img layer_num.png  %}  \n\nactivationL * hidden size  \n\n2Norm-Head  \n\n>  LM-Head  LM-Head \n LM-Head   \n\nBaichuan2Norm-HeadLM-HeadNorm  \n\nNorm-HeadGradient Norm  \n\n{% asset_img norm_head.png norm head %}  \n\nNorm-HeadGradient Norm  \n\n#   \n\n##   \n\n  \n- 2.8T  \n-  = 45 = 6%  \n- STEM10%  \n\n  \n\n{% asset_img pt_data.png  %}  \n\n[https://github.com/google-research/deduplicate-text-datasets](https://github.com/google-research/deduplicate-text-datasets)  \n\n 15.6w   \n\n{% asset_img dedup.png  %}  \n\npackingattention-maskposition-id  \n\n##   \n\n1  \n\n- AdamWbeta_1=0.9beta_2=0.95eps=1e-8  \n- gradient clip=1.0  \n- weight decay=0.1  \n\n2lrscheduler  \n\nWSDlearning rate scheduler  \n- warmup 100 +   \n- decay  \n\ndecaylr5e-4decay1%5e-6decay400B  \n\n0.1BCosinLinearWSD1Tloss  \n\n{% asset_img scheduler.png scheduler %}  \n\n  \n- schedulervalid loss  \n- WSDSvalid lossdecay  \n-   \n\n3decay  \n\nWSDdecayloss  \n\n  \n- Cosine  \n- WSD  \n- Cosine + 10%  \n- WSD + 10%decay  \n\ndecayWSD  \n\n{% asset_img wsd_quality.png wsd quallity %}  \n\n4  \n\ndecay  \n\n  \n- index-1.9b-ablation-pureDecay ()  \n- index-1.9b-ablation-boostpure7%  \n\nMMLU  \n\n{% asset_img add_instruct_data.png  %}  \n\ndecay  \n\n#   \n\n## SFT  \n\n1  \n\n- 10M  \n-   \n-   \n- /  \n\n2  \n\n- lr=1e-5  \n-  system-query-response   \n\nSFTSFTresponsetoken4:6  \n\n{% asset_img sft.png SFT %}  \n\n## DPO  \n\nDPO\n\nprompt SFT  ppl   \n\nlr =  1e-6\nscheduler = cosine\nepoch = 1\ndpo beta = 1\n\n  \n\n{% asset_img dpo.png DPO %}  \n\n#   \n\nIndex-1.9B  \n- WSDdecay  \n- SFT  \n- Norm-Head  \n\n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1https://github.com/bilibili/Index-1.9B/blob/main/Index-1.9B%20%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A.pdf  \n\n","slug":"cs/nlp/2024/07/bilibiliindex-1-9B","published":1,"updated":"2024-07-05T11:51:37.531Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsn1002m0p4k0ski0myy","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>bilibiliIndex-1.9B<br>\n- Index-1.9B base2.8T<br>\n- Index-1.9B purebase<br>\n- Index-1.9B chatbaseSFTDPO<br>\n- Index-1.9B\ncharacterchatRAG+fewshots</p>\n<p></p>\n<h1 id=\"\"></h1>\n<p>1</p>\n<p>3691.01B36base</p>\n<img src=\"/770b63e1/layer_num.png\" class title=\"\">\n<p>activationL\n* hidden size</p>\n<p>2Norm-Head</p>\n<blockquote>\n<p> LM-Head\n LM-Head\n \nLM-Head </p>\n</blockquote>\n<p>Baichuan2Norm-HeadLM-HeadNorm</p>\n<p>Norm-HeadGradient Norm</p>\n<img src=\"/770b63e1/norm_head.png\" class title=\"norm head\">\n<p>Norm-HeadGradient\nNorm</p>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p><br>\n- 2.8T<br>\n-  = 45 = 6%<br>\n- STEM10%</p>\n<p></p>\n<img src=\"/770b63e1/pt_data.png\" class title=\"\">\n<p><a href=\"https://github.com/google-research/deduplicate-text-datasets\">https://github.com/google-research/deduplicate-text-datasets</a></p>\n<p>\n15.6w </p>\n<img src=\"/770b63e1/dedup.png\" class title=\"\">\n<p>packingattention-maskposition-id</p>\n<h2 id=\"\"></h2>\n<p>1</p>\n<ul>\n<li>AdamWbeta_1=0.9beta_2=0.95eps=1e-8<br>\n</li>\n<li>gradient clip=1.0<br>\n</li>\n<li>weight decay=0.1</li>\n</ul>\n<p>2lrscheduler</p>\n<p>WSDlearning rate\nscheduler<br>\n- warmup 100 + <br>\n- decay</p>\n<p>decaylr5e-4decay1%5e-6decay400B</p>\n<p>0.1BCosinLinearWSD1Tloss</p>\n<img src=\"/770b63e1/scheduler.png\" class title=\"scheduler\">\n<p><br>\n- schedulervalid loss<br>\n- WSDSvalid lossdecay<br>\n- </p>\n<p>3decay</p>\n<p>WSDdecayloss</p>\n<p><br>\n- Cosine<br>\n- WSD<br>\n- Cosine + 10%<br>\n- WSD + 10%decay</p>\n<p>decayWSD</p>\n<img src=\"/770b63e1/wsd_quality.png\" class title=\"wsd quallity\">\n<p>4</p>\n<p>decay</p>\n<p><br>\n- index-1.9b-ablation-pureDecay\n()<br>\n- index-1.9b-ablation-boostpure7%</p>\n<p>MMLU</p>\n<img src=\"/770b63e1/add_instruct_data.png\" class title=\"\">\n<p>decay</p>\n<h1 id=\"\"></h1>\n<h2 id=\"sft\">SFT</h2>\n<p>1</p>\n<ul>\n<li>10M<br>\n</li>\n<li><br>\n</li>\n<li><br>\n</li>\n<li>/</li>\n</ul>\n<p>2</p>\n<ul>\n<li>lr=1e-5<br>\n</li>\n<li> system-query-response </li>\n</ul>\n<p>SFTSFTresponsetoken4:6</p>\n<img src=\"/770b63e1/sft.png\" class title=\"SFT\">\n<h2 id=\"dpo\">DPO</h2>\n<p>DPO</p>\n<p>prompt\nSFT  ppl\n</p>\n<p>lr = 1e-6 scheduler = cosine epoch = 1 dpo beta = 1</p>\n<p></p>\n<img src=\"/770b63e1/dpo.png\" class title=\"DPO\">\n<h1 id=\"\"></h1>\n<p>Index-1.9B<br>\n-\nWSDdecay<br>\n- SFT<br>\n- Norm-Head</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1https://github.com/bilibili/Index-1.9B/blob/main/Index-1.9B%20%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A.pdf</p>\n","length":2846,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>bilibiliIndex-1.9B<br>\n- Index-1.9B base2.8T<br>\n- Index-1.9B purebase<br>\n- Index-1.9B chatbaseSFTDPO<br>\n- Index-1.9B\ncharacterchatRAG+fewshots</p>\n<p></p>\n<h1 id=\"\"></h1>\n<p>1</p>\n<p>3691.01B36base</p>\n<img src=\"/770b63e1/layer_num.png\" class title=\"\">\n<p>activationL\n* hidden size</p>\n<p>2Norm-Head</p>\n<blockquote>\n<p> LM-Head\n LM-Head\n \nLM-Head </p>\n</blockquote>\n<p>Baichuan2Norm-HeadLM-HeadNorm</p>\n<p>Norm-HeadGradient Norm</p>\n<img src=\"/770b63e1/norm_head.png\" class title=\"norm head\">\n<p>Norm-HeadGradient\nNorm</p>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p><br>\n- 2.8T<br>\n-  = 45 = 6%<br>\n- STEM10%</p>\n<p></p>\n<img src=\"/770b63e1/pt_data.png\" class title=\"\">\n<p><a href=\"https://github.com/google-research/deduplicate-text-datasets\">https://github.com/google-research/deduplicate-text-datasets</a></p>\n<p>\n15.6w </p>\n<img src=\"/770b63e1/dedup.png\" class title=\"\">\n<p>packingattention-maskposition-id</p>\n<h2 id=\"\"></h2>\n<p>1</p>\n<ul>\n<li>AdamWbeta_1=0.9beta_2=0.95eps=1e-8<br>\n</li>\n<li>gradient clip=1.0<br>\n</li>\n<li>weight decay=0.1</li>\n</ul>\n<p>2lrscheduler</p>\n<p>WSDlearning rate\nscheduler<br>\n- warmup 100 + <br>\n- decay</p>\n<p>decaylr5e-4decay1%5e-6decay400B</p>\n<p>0.1BCosinLinearWSD1Tloss</p>\n<img src=\"/770b63e1/scheduler.png\" class title=\"scheduler\">\n<p><br>\n- schedulervalid loss<br>\n- WSDSvalid lossdecay<br>\n- </p>\n<p>3decay</p>\n<p>WSDdecayloss</p>\n<p><br>\n- Cosine<br>\n- WSD<br>\n- Cosine + 10%<br>\n- WSD + 10%decay</p>\n<p>decayWSD</p>\n<img src=\"/770b63e1/wsd_quality.png\" class title=\"wsd quallity\">\n<p>4</p>\n<p>decay</p>\n<p><br>\n- index-1.9b-ablation-pureDecay\n()<br>\n- index-1.9b-ablation-boostpure7%</p>\n<p>MMLU</p>\n<img src=\"/770b63e1/add_instruct_data.png\" class title=\"\">\n<p>decay</p>\n<h1 id=\"\"></h1>\n<h2 id=\"sft\">SFT</h2>\n<p>1</p>\n<ul>\n<li>10M<br>\n</li>\n<li><br>\n</li>\n<li><br>\n</li>\n<li>/</li>\n</ul>\n<p>2</p>\n<ul>\n<li>lr=1e-5<br>\n</li>\n<li> system-query-response </li>\n</ul>\n<p>SFTSFTresponsetoken4:6</p>\n<img src=\"/770b63e1/sft.png\" class title=\"SFT\">\n<h2 id=\"dpo\">DPO</h2>\n<p>DPO</p>\n<p>prompt\nSFT  ppl\n</p>\n<p>lr = 1e-6 scheduler = cosine epoch = 1 dpo beta = 1</p>\n<p></p>\n<img src=\"/770b63e1/dpo.png\" class title=\"DPO\">\n<h1 id=\"\"></h1>\n<p>Index-1.9B<br>\n-\nWSDdecay<br>\n- SFT<br>\n- Norm-Head</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1https://github.com/bilibili/Index-1.9B/blob/main/Index-1.9B%20%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A.pdf</p>\n"},{"title":"MoEtop-p routing","abbrlink":"224c42da","date":"2024-07-15T12:34:00.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nAGIBangMoEdynamic routinggatingtop-k routingtop-p routing  \n\nMoE[MoE](http://www.linsight.cn/44e38c1b.html)  \n\n# routing  \n\n## top-k routing  \n\nMoEroutingtop-k routing k = 2tokenMoE2token drop   \n\nMoENexpertexpert $E=\\{e_{1},e_{2},..,e_{N}\\}$token xMoE  \n\n$$MoE(\\mathbf{x})=\\sum_{i=1}^Ng_i(\\mathbf{x})*e_i(\\mathbf{x})$$  \n\n$$g_i(\\mathbf{x})=\\begin{cases}\\frac{P_i}{\\sum_{j\\in TopK(\\mathbf{P})}P_j},&i\\in TopK(\\mathbf{P})\\\\0,&i\\notin TopK(\\mathbf{P})\\end{cases}$$  \n\n$$\\mathbf{P}=Softmax(\\mathbf{W_r}\\cdot\\mathbf{x}^T)$$  \n\ntop-k routingGoogleOutrageously large neural networks: The sparsely-gated mixture-of-experts layerLSTMGshardSwitch TransformerST-MoETaming sparsely activated transformer with stochastic expertsconstraint  \n\n## top-p routing  \n\ntop-k routingtokentokenMoE  \n\ntop-p routingtokenpgatingaccumulative confidencep  \n\n$$t=\\underset{k\\in\\{1...,N\\}}{argmin}\\sum_{j<=k}P_{i,j}\\geq p$$  \n\n$$g_i(\\mathbf{x})=\\begin{cases}P_i&e_i\\in S\\\\0,&e_i\\notin S\\end{cases}$$  \n\n$$S=\\{e_{I_1},e_{I_2}...e_{I_t}\\}$$  \n\ntop-k routingtop-p routing  \n\n{% asset_img top-p.png top-p %}  \n\n# Loss  \n\n## Dynamic Loss  \n\ntop-p routinggating  \n\np0.5MoE  \n\nMoE  \n\ndynamic lossP  \n\n$$Loss_d=-\\sum_{i=1}^NP_i*log(P_i)$$  \n\n## Load Balance Loss  \n\nMoE  \n\n$$Loss_b=N*\\sum_{i=1}^Nf_i*Q_i$$  \n\n$$f_i=\\frac{1}{M}\\sum_{j=1}^M1\\{e_i\\in S^j\\}$$  \n\n$$Q_i=\\frac{1}{M}\\sum_{j=1}^nP_i^j$$  \n\n$S^{j}$ jtoken  \n\n## Final Loss  \n\nloss  \n\n$$Loss=Loss_{lm}+\\alpha Loss_b+\\beta Loss_d$$\n\n $\\alpha=1e-2$$\\beta=1e-4$  \n\n#   \n\n##   \n\nRedPajama100Bcommon crawl (CC), C4, github, Wikipedia, books, arxiv  Stackexchange  \n\n##   \n\nLLaMA  \n- vocab size = 32000  \n- layer num = 24  \n- standard deviation = 0.006  \n- MHAhead num = 16head size = 64  \n\n5  \n- dense1hidden size = 1024 = 374M  \n- dense2hidden size = 1280 = 570M  \n- top-1 MoEhidden size = 1024 = 16 = 3.5B = 374M  \n- top-2 MoEhidden size = 1024 = 16 = 3.5B = 581M  \n- top-p MoEhidden size = 1024 = 16 = 3.5Bp = 0.4  \n\n  \n- AdamWbeta_1 = 0.9, beta_2 = 0.95  \n- weight decay = 0.1  \n- cosine schedule  \n- max lr = 3e-4final lr = 3e-5  \n- warmup = 2000 step  \n- context length = 2048  \n- batch size = 2048  \n\n5  \n\n{% asset_img perf.png performance %}  \n\ntop-p MoE1.76  \n\ntop-p MoEtop-2 MoE90%top-2 MoE0.7%  \n\n#   \n\n## p  \n\np0.1~0.7  \n\n{% asset_img diff_p.png p %}  \n\np0.10.2p0.3  \n\n##   \n\ntop-p MoE\n\n{% asset_img active_num.png  %}  \n\n60B2100B1T10T  \n\n## top-p MoE  \n\nBBHBIG-Bench Hard23BIG-Bench  \n\nBBH  \n\n{% asset_img task_expert.png  %}  \n\ntop-p MoEBBH  \n\ntop-p MoE  \n\n#   \n\ntop-p MoE  \n\nMoE  \n\n{% asset_img diff_layer.png  %}  \n\noverthinking  \n\nShallow-deep networks: Understanding and mitigating network overthinkingoverthinking  \n\nbudget  \n\n#   \n\n- MoE  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n# Reference  \n\n1Harder Tasks Need More Experts: Dynamic Routing in MoE Models https://arxiv.org/abs/2403.07652  \n","source":"_posts/cs/nlp/2024/07/routing.md","raw":"---\ntitle: MoEtop-p routing\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - MoE\n  - routing\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 224c42da\ndate: 2024-07-15 20:34:00\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nAGIBangMoEdynamic routinggatingtop-k routingtop-p routing  \n\nMoE[MoE](http://www.linsight.cn/44e38c1b.html)  \n\n# routing  \n\n## top-k routing  \n\nMoEroutingtop-k routing k = 2tokenMoE2token drop   \n\nMoENexpertexpert $E=\\{e_{1},e_{2},..,e_{N}\\}$token xMoE  \n\n$$MoE(\\mathbf{x})=\\sum_{i=1}^Ng_i(\\mathbf{x})*e_i(\\mathbf{x})$$  \n\n$$g_i(\\mathbf{x})=\\begin{cases}\\frac{P_i}{\\sum_{j\\in TopK(\\mathbf{P})}P_j},&i\\in TopK(\\mathbf{P})\\\\0,&i\\notin TopK(\\mathbf{P})\\end{cases}$$  \n\n$$\\mathbf{P}=Softmax(\\mathbf{W_r}\\cdot\\mathbf{x}^T)$$  \n\ntop-k routingGoogleOutrageously large neural networks: The sparsely-gated mixture-of-experts layerLSTMGshardSwitch TransformerST-MoETaming sparsely activated transformer with stochastic expertsconstraint  \n\n## top-p routing  \n\ntop-k routingtokentokenMoE  \n\ntop-p routingtokenpgatingaccumulative confidencep  \n\n$$t=\\underset{k\\in\\{1...,N\\}}{argmin}\\sum_{j<=k}P_{i,j}\\geq p$$  \n\n$$g_i(\\mathbf{x})=\\begin{cases}P_i&e_i\\in S\\\\0,&e_i\\notin S\\end{cases}$$  \n\n$$S=\\{e_{I_1},e_{I_2}...e_{I_t}\\}$$  \n\ntop-k routingtop-p routing  \n\n{% asset_img top-p.png top-p %}  \n\n# Loss  \n\n## Dynamic Loss  \n\ntop-p routinggating  \n\np0.5MoE  \n\nMoE  \n\ndynamic lossP  \n\n$$Loss_d=-\\sum_{i=1}^NP_i*log(P_i)$$  \n\n## Load Balance Loss  \n\nMoE  \n\n$$Loss_b=N*\\sum_{i=1}^Nf_i*Q_i$$  \n\n$$f_i=\\frac{1}{M}\\sum_{j=1}^M1\\{e_i\\in S^j\\}$$  \n\n$$Q_i=\\frac{1}{M}\\sum_{j=1}^nP_i^j$$  \n\n$S^{j}$ jtoken  \n\n## Final Loss  \n\nloss  \n\n$$Loss=Loss_{lm}+\\alpha Loss_b+\\beta Loss_d$$\n\n $\\alpha=1e-2$$\\beta=1e-4$  \n\n#   \n\n##   \n\nRedPajama100Bcommon crawl (CC), C4, github, Wikipedia, books, arxiv  Stackexchange  \n\n##   \n\nLLaMA  \n- vocab size = 32000  \n- layer num = 24  \n- standard deviation = 0.006  \n- MHAhead num = 16head size = 64  \n\n5  \n- dense1hidden size = 1024 = 374M  \n- dense2hidden size = 1280 = 570M  \n- top-1 MoEhidden size = 1024 = 16 = 3.5B = 374M  \n- top-2 MoEhidden size = 1024 = 16 = 3.5B = 581M  \n- top-p MoEhidden size = 1024 = 16 = 3.5Bp = 0.4  \n\n  \n- AdamWbeta_1 = 0.9, beta_2 = 0.95  \n- weight decay = 0.1  \n- cosine schedule  \n- max lr = 3e-4final lr = 3e-5  \n- warmup = 2000 step  \n- context length = 2048  \n- batch size = 2048  \n\n5  \n\n{% asset_img perf.png performance %}  \n\ntop-p MoE1.76  \n\ntop-p MoEtop-2 MoE90%top-2 MoE0.7%  \n\n#   \n\n## p  \n\np0.1~0.7  \n\n{% asset_img diff_p.png p %}  \n\np0.10.2p0.3  \n\n##   \n\ntop-p MoE\n\n{% asset_img active_num.png  %}  \n\n60B2100B1T10T  \n\n## top-p MoE  \n\nBBHBIG-Bench Hard23BIG-Bench  \n\nBBH  \n\n{% asset_img task_expert.png  %}  \n\ntop-p MoEBBH  \n\ntop-p MoE  \n\n#   \n\ntop-p MoE  \n\nMoE  \n\n{% asset_img diff_layer.png  %}  \n\noverthinking  \n\nShallow-deep networks: Understanding and mitigating network overthinkingoverthinking  \n\nbudget  \n\n#   \n\n- MoE  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n# Reference  \n\n1Harder Tasks Need More Experts: Dynamic Routing in MoE Models https://arxiv.org/abs/2403.07652  \n","slug":"cs/nlp/2024/07/routing","published":1,"updated":"2024-07-15T13:03:11.099Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsn1002q0p4k23kd4w80","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>AGIBangMoEdynamic\nroutinggatingtop-k routingtop-p\nrouting</p>\n<p>MoE<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a></p>\n<h1 id=\"routing\">routing</h1>\n<h2 id=\"top-k-routing\">top-k routing</h2>\n<p>MoEroutingtop-k routing k =\n2tokenMoE2token drop</p>\n<p>MoENexpertexpert <span class=\"math inline\">\\(E=\\{e_{1},e_{2},..,e_{N}\\}\\)</span>token\nxMoE</p>\n<p><span class=\"math display\">\\[MoE(\\mathbf{x})=\\sum_{i=1}^Ng_i(\\mathbf{x})*e_i(\\mathbf{x})\\]</span></p>\n<p><span class=\"math display\">\\[g_i(\\mathbf{x})=\\begin{cases}\\frac{P_i}{\\sum_{j\\in\nTopK(\\mathbf{P})}P_j},&amp;i\\in TopK(\\mathbf{P})\\\\0,&amp;i\\notin\nTopK(\\mathbf{P})\\end{cases}\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{P}=Softmax(\\mathbf{W_r}\\cdot\\mathbf{x}^T)\\]</span></p>\n<p>top-k routingGoogleOutrageously large neural networks: The\nsparsely-gated mixture-of-experts\nlayerLSTMGshardSwitch\nTransformerST-MoETaming sparsely activated transformer with\nstochastic\nexpertsconstraint</p>\n<h2 id=\"top-p-routing\">top-p routing</h2>\n<p>top-k\nroutingtokentokenMoE</p>\n<p>top-p\nroutingtokenpgatingaccumulative\nconfidencep</p>\n<p><span class=\"math display\">\\[t=\\underset{k\\in\\{1...,N\\}}{argmin}\\sum_{j&lt;=k}P_{i,j}\\geq\np\\]</span></p>\n<p><span class=\"math display\">\\[g_i(\\mathbf{x})=\\begin{cases}P_i&amp;e_i\\in\nS\\\\0,&amp;e_i\\notin S\\end{cases}\\]</span></p>\n<p><span class=\"math display\">\\[S=\\{e_{I_1},e_{I_2}...e_{I_t}\\}\\]</span></p>\n<p>top-k routingtop-p routing</p>\n<img src=\"/224c42da/top-p.png\" class title=\"top-p\">\n<h1 id=\"loss\">Loss</h1>\n<h2 id=\"dynamic-loss\">Dynamic Loss</h2>\n<p>top-p\nroutinggating</p>\n<p>p0.5MoE</p>\n<p>MoE</p>\n<p>dynamic\nlossP</p>\n<p><span class=\"math display\">\\[Loss_d=-\\sum_{i=1}^NP_i*log(P_i)\\]</span></p>\n<h2 id=\"load-balance-loss\">Load Balance Loss</h2>\n<p>MoE</p>\n<p><span class=\"math display\">\\[Loss_b=N*\\sum_{i=1}^Nf_i*Q_i\\]</span></p>\n<p><span class=\"math display\">\\[f_i=\\frac{1}{M}\\sum_{j=1}^M1\\{e_i\\in\nS^j\\}\\]</span></p>\n<p><span class=\"math display\">\\[Q_i=\\frac{1}{M}\\sum_{j=1}^nP_i^j\\]</span></p>\n<p><span class=\"math inline\">\\(S^{j}\\)</span>\njtoken</p>\n<h2 id=\"final-loss\">Final Loss</h2>\n<p>loss</p>\n<p><span class=\"math display\">\\[Loss=Loss_{lm}+\\alpha Loss_b+\\beta\nLoss_d\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha=1e-2\\)</span><span class=\"math inline\">\\(\\beta=1e-4\\)</span></p>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>RedPajama100Bcommon crawl (CC), C4, github,\nWikipedia, books, arxiv  Stackexchange</p>\n<h2 id=\"\"></h2>\n<p>LLaMA<br>\n- vocab size = 32000<br>\n- layer num = 24<br>\n- standard deviation = 0.006<br>\n- MHAhead num = 16head size = 64</p>\n<p>5<br>\n- dense1hidden size = 1024 = 374M<br>\n- dense2hidden size = 1280 = 570M<br>\n- top-1 MoEhidden size = 1024 = 16 =\n3.5B = 374M<br>\n- top-2 MoEhidden size = 1024 = 16 =\n3.5B = 581M<br>\n- top-p MoEhidden size = 1024 = 16 = 3.5Bp\n= 0.4</p>\n<p><br>\n- AdamWbeta_1 = 0.9, beta_2 = 0.95<br>\n- weight decay = 0.1<br>\n- cosine schedule<br>\n- max lr = 3e-4final lr = 3e-5<br>\n- warmup = 2000 step<br>\n- context length = 2048<br>\n- batch size = 2048</p>\n<p>5</p>\n<img src=\"/224c42da/perf.png\" class title=\"performance\">\n<p>top-p MoE1.76</p>\n<p>top-p MoEtop-2 MoE90%top-2\nMoE0.7%</p>\n<h1 id=\"\"></h1>\n<h2 id=\"p\">p</h2>\n<p>p0.1~0.7</p>\n<img src=\"/224c42da/diff_p.png\" class title=\"p\">\n<p>p0.10.2p0.3</p>\n<h2 id=\"\"></h2>\n<p>top-p\nMoE</p>\n<img src=\"/224c42da/active_num.png\" class title=\"\">\n<p>60B2100B1T10T</p>\n<h2 id=\"top-p-moe\">top-p MoE</h2>\n<p>BBHBIG-Bench Hard23BIG-Bench</p>\n<p>BBH</p>\n<img src=\"/224c42da/task_expert.png\" class title=\"\">\n<p>top-p MoEBBH</p>\n<p>top-p\nMoE</p>\n<h1 id=\"\"></h1>\n<p>top-p\nMoE</p>\n<p>MoE</p>\n<img src=\"/224c42da/diff_layer.png\" class title=\"\">\n<p>overthinking</p>\n<p>Shallow-deep networks: Understanding and mitigating network\noverthinkingoverthinking</p>\n<p>budget</p>\n<h1 id=\"\"></h1>\n<ul>\n<li>MoE</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Harder Tasks Need More Experts: Dynamic Routing in MoE Models\nhttps://arxiv.org/abs/2403.07652</p>\n","length":3933,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>AGIBangMoEdynamic\nroutinggatingtop-k routingtop-p\nrouting</p>\n<p>MoE<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a></p>\n<h1 id=\"routing\">routing</h1>\n<h2 id=\"top-k-routing\">top-k routing</h2>\n<p>MoEroutingtop-k routing k =\n2tokenMoE2token drop</p>\n<p>MoENexpertexpert <span class=\"math inline\">\\(E=\\{e_{1},e_{2},..,e_{N}\\}\\)</span>token\nxMoE</p>\n<p><span class=\"math display\">\\[MoE(\\mathbf{x})=\\sum_{i=1}^Ng_i(\\mathbf{x})*e_i(\\mathbf{x})\\]</span></p>\n<p><span class=\"math display\">\\[g_i(\\mathbf{x})=\\begin{cases}\\frac{P_i}{\\sum_{j\\in\nTopK(\\mathbf{P})}P_j},&amp;i\\in TopK(\\mathbf{P})\\\\0,&amp;i\\notin\nTopK(\\mathbf{P})\\end{cases}\\]</span></p>\n<p><span class=\"math display\">\\[\\mathbf{P}=Softmax(\\mathbf{W_r}\\cdot\\mathbf{x}^T)\\]</span></p>\n<p>top-k routingGoogleOutrageously large neural networks: The\nsparsely-gated mixture-of-experts\nlayerLSTMGshardSwitch\nTransformerST-MoETaming sparsely activated transformer with\nstochastic\nexpertsconstraint</p>\n<h2 id=\"top-p-routing\">top-p routing</h2>\n<p>top-k\nroutingtokentokenMoE</p>\n<p>top-p\nroutingtokenpgatingaccumulative\nconfidencep</p>\n<p><span class=\"math display\">\\[t=\\underset{k\\in\\{1...,N\\}}{argmin}\\sum_{j&lt;=k}P_{i,j}\\geq\np\\]</span></p>\n<p><span class=\"math display\">\\[g_i(\\mathbf{x})=\\begin{cases}P_i&amp;e_i\\in\nS\\\\0,&amp;e_i\\notin S\\end{cases}\\]</span></p>\n<p><span class=\"math display\">\\[S=\\{e_{I_1},e_{I_2}...e_{I_t}\\}\\]</span></p>\n<p>top-k routingtop-p routing</p>\n<img src=\"/224c42da/top-p.png\" class title=\"top-p\">\n<h1 id=\"loss\">Loss</h1>\n<h2 id=\"dynamic-loss\">Dynamic Loss</h2>\n<p>top-p\nroutinggating</p>\n<p>p0.5MoE</p>\n<p>MoE</p>\n<p>dynamic\nlossP</p>\n<p><span class=\"math display\">\\[Loss_d=-\\sum_{i=1}^NP_i*log(P_i)\\]</span></p>\n<h2 id=\"load-balance-loss\">Load Balance Loss</h2>\n<p>MoE</p>\n<p><span class=\"math display\">\\[Loss_b=N*\\sum_{i=1}^Nf_i*Q_i\\]</span></p>\n<p><span class=\"math display\">\\[f_i=\\frac{1}{M}\\sum_{j=1}^M1\\{e_i\\in\nS^j\\}\\]</span></p>\n<p><span class=\"math display\">\\[Q_i=\\frac{1}{M}\\sum_{j=1}^nP_i^j\\]</span></p>\n<p><span class=\"math inline\">\\(S^{j}\\)</span>\njtoken</p>\n<h2 id=\"final-loss\">Final Loss</h2>\n<p>loss</p>\n<p><span class=\"math display\">\\[Loss=Loss_{lm}+\\alpha Loss_b+\\beta\nLoss_d\\]</span></p>\n<p> <span class=\"math inline\">\\(\\alpha=1e-2\\)</span><span class=\"math inline\">\\(\\beta=1e-4\\)</span></p>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>RedPajama100Bcommon crawl (CC), C4, github,\nWikipedia, books, arxiv  Stackexchange</p>\n<h2 id=\"\"></h2>\n<p>LLaMA<br>\n- vocab size = 32000<br>\n- layer num = 24<br>\n- standard deviation = 0.006<br>\n- MHAhead num = 16head size = 64</p>\n<p>5<br>\n- dense1hidden size = 1024 = 374M<br>\n- dense2hidden size = 1280 = 570M<br>\n- top-1 MoEhidden size = 1024 = 16 =\n3.5B = 374M<br>\n- top-2 MoEhidden size = 1024 = 16 =\n3.5B = 581M<br>\n- top-p MoEhidden size = 1024 = 16 = 3.5Bp\n= 0.4</p>\n<p><br>\n- AdamWbeta_1 = 0.9, beta_2 = 0.95<br>\n- weight decay = 0.1<br>\n- cosine schedule<br>\n- max lr = 3e-4final lr = 3e-5<br>\n- warmup = 2000 step<br>\n- context length = 2048<br>\n- batch size = 2048</p>\n<p>5</p>\n<img src=\"/224c42da/perf.png\" class title=\"performance\">\n<p>top-p MoE1.76</p>\n<p>top-p MoEtop-2 MoE90%top-2\nMoE0.7%</p>\n<h1 id=\"\"></h1>\n<h2 id=\"p\">p</h2>\n<p>p0.1~0.7</p>\n<img src=\"/224c42da/diff_p.png\" class title=\"p\">\n<p>p0.10.2p0.3</p>\n<h2 id=\"\"></h2>\n<p>top-p\nMoE</p>\n<img src=\"/224c42da/active_num.png\" class title=\"\">\n<p>60B2100B1T10T</p>\n<h2 id=\"top-p-moe\">top-p MoE</h2>\n<p>BBHBIG-Bench Hard23BIG-Bench</p>\n<p>BBH</p>\n<img src=\"/224c42da/task_expert.png\" class title=\"\">\n<p>top-p MoEBBH</p>\n<p>top-p\nMoE</p>\n<h1 id=\"\"></h1>\n<p>top-p\nMoE</p>\n<p>MoE</p>\n<img src=\"/224c42da/diff_layer.png\" class title=\"\">\n<p>overthinking</p>\n<p>Shallow-deep networks: Understanding and mitigating network\noverthinkingoverthinking</p>\n<p>budget</p>\n<h1 id=\"\"></h1>\n<ul>\n<li>MoE</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Harder Tasks Need More Experts: Dynamic Routing in MoE Models\nhttps://arxiv.org/abs/2403.07652</p>\n"},{"title":" -- model soup","abbrlink":"bb8fcf21","date":"2024-07-30T12:33:25.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nDCLMLlama-3.1model soupModel soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time  \n\nmodel soupSWAEMA  \n\n#   \n\n  \n- 1checkpoint  \n- 2checkpoint  \n\n  \n- ensemble  \n- out-of-distribution data  \n\n# SWA & EMA  \n\nSWAEMA  \n\nStochastic Weight AveragingSWA  \n\nSWAcheckpoint  \n\n{% asset_img swa_1.png swa %}  \n\n{% asset_img swa_2.png swa %}  \n\nSWA  \n\nSGDlosswide flat regionwide flat regionvolumeSGD  \n\ntrain losstest errorwide flat region  \n\nSWA  \n\nSWASGDtrain losstest errorSWAtrain losstest error\n\n{% asset_img swa_3.png swa %}  \n\nEMASWAEMAPyTorch([https://zhuanlan.zhihu.com/p/68748778](https://zhuanlan.zhihu.com/p/68748778))  \n\n# model soup  \n\nmodel average  \n- What is being transferred in transfer learning?error landscape basin  \n- Rethinking the inception architecture for computer visionAveraging weights leads to wider optima and better generalizationSWAweight average  \n- No one representation to rule them all: Overlapping features of training methodsensemble  \n\nmodel soupmodel averageindependent runEMA/SWA  \n\n $[h_1,...h_k]$ $\\theta_0$ $[\\theta_1,...,\\theta_k]$ kcheckpointcheckpointkcheckpointmodel soup  \n\n3model soupuniform soupgreedy souplearned soup  \n\n{% asset_img method_soup.png model soup %}  \n\nuniform soup  \n\ngreedy soupkcheckpointcheckpointcheckpoint  \n\n{% asset_img algo.png model soup %}  \n\nuniform soupgreedy souplearned soup $\\alpha\\in\\mathbb{R}^k$ mixing coefficients$\\beta$ temperature scaling parameterlearned soup $\\alpha$  $\\beta$  \n\n$$\\arg\\min_{\\alpha\\in\\mathbb{R}^k,\\beta\\in\\mathbb{R}}\\sum_{j=1}^n\\ell\\Bigg(\\beta\\cdot f\\Bigg(x_j,\\sum_{i=1}^k\\alpha_i\\theta_i\\Bigg),y_j\\Bigg)$$  \n\nklearned soup  \n\ngreedy soup  \n\n# model soup  \n\nCLIPALIGNBASICtransformer  \n\n1Error landscape visualizations  \n\nCLIPImageNettraining losstest error  \n\n{% asset_img angle.png model soup %}  \n\nxyerror landscape  \n\n  \n- finetuned solution  \n- solution -- initialization --   \n\nsolutionmodel averagemodel soupsolution  \n\n{% asset_img angle_2.png model soup %}  \n\n2Ensemble comparison  \n\nmodel soupensemblelearning rate  \n\n{% asset_img compare.png model soup %}  \n\n  \n- lrensemblemodel soup  \n- lrensemblemodel soup  \n- lrensemblemodel souplr  \n- in-distributionensembledistribution shiftmodel soup  \n\n3One dimensional hyperparameter grids  \n\n  \n\noptimizeraugmentationlrlr  \n\n4  \n\nmodel soup  \n\n{% asset_img result.png model soup %}  \n\n  \n\n#   \n\n- model soupgreedy model soup  \n- model soupadapterLoRA  \n- r-drop  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n- MoE  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[denseMoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  \n[MoE--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  \n-   \n[Llama3.1--](https://www.linsight.cn/7d7294cb.html)  \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[--](https://www.linsight.cn/210dbccd.html)  \n-   \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n-   \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n-   \n[Llama3.1--post-training](https://www.linsight.cn/93328a2a.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n- Transformer  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n-   \n[(1)](http://www.linsight.cn/3345028a.html)\n[(2)](http://www.linsight.cn/ad0bba9d.html)\n[(3)](http://www.linsight.cn/1736008.html)\n[(4)](http://www.linsight.cn/1736008.html)\n[(5)](http://www.linsight.cn/336f2f3e.html)\n[(6)](http://www.linsight.cn/7c04944d.html)\n[(7)](https://www.linsight.cn/dd614e12.html)\n[(8)](https://www.linsight.cn/e287b9c3.html)  \n\n# Reference  \n\n1Averaging Weights Leads to Wider Optima and Better Generalization https://arxiv.org/abs/1803.05407  \n2Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time https://arxiv.org/abs/2203.05482  \n3Stochastic Weight Averaging in PyTorch https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/   \n4EMAPyTorch https://zhuanlan.zhihu.com/p/68748778  \n","source":"_posts/cs/nlp/2024/07/model-soup.md","raw":"---\ntitle:  -- model soup\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: bb8fcf21\ndate: 2024-07-30 20:33:25\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nDCLMLlama-3.1model soupModel soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time  \n\nmodel soupSWAEMA  \n\n#   \n\n  \n- 1checkpoint  \n- 2checkpoint  \n\n  \n- ensemble  \n- out-of-distribution data  \n\n# SWA & EMA  \n\nSWAEMA  \n\nStochastic Weight AveragingSWA  \n\nSWAcheckpoint  \n\n{% asset_img swa_1.png swa %}  \n\n{% asset_img swa_2.png swa %}  \n\nSWA  \n\nSGDlosswide flat regionwide flat regionvolumeSGD  \n\ntrain losstest errorwide flat region  \n\nSWA  \n\nSWASGDtrain losstest errorSWAtrain losstest error\n\n{% asset_img swa_3.png swa %}  \n\nEMASWAEMAPyTorch([https://zhuanlan.zhihu.com/p/68748778](https://zhuanlan.zhihu.com/p/68748778))  \n\n# model soup  \n\nmodel average  \n- What is being transferred in transfer learning?error landscape basin  \n- Rethinking the inception architecture for computer visionAveraging weights leads to wider optima and better generalizationSWAweight average  \n- No one representation to rule them all: Overlapping features of training methodsensemble  \n\nmodel soupmodel averageindependent runEMA/SWA  \n\n $[h_1,...h_k]$ $\\theta_0$ $[\\theta_1,...,\\theta_k]$ kcheckpointcheckpointkcheckpointmodel soup  \n\n3model soupuniform soupgreedy souplearned soup  \n\n{% asset_img method_soup.png model soup %}  \n\nuniform soup  \n\ngreedy soupkcheckpointcheckpointcheckpoint  \n\n{% asset_img algo.png model soup %}  \n\nuniform soupgreedy souplearned soup $\\alpha\\in\\mathbb{R}^k$ mixing coefficients$\\beta$ temperature scaling parameterlearned soup $\\alpha$  $\\beta$  \n\n$$\\arg\\min_{\\alpha\\in\\mathbb{R}^k,\\beta\\in\\mathbb{R}}\\sum_{j=1}^n\\ell\\Bigg(\\beta\\cdot f\\Bigg(x_j,\\sum_{i=1}^k\\alpha_i\\theta_i\\Bigg),y_j\\Bigg)$$  \n\nklearned soup  \n\ngreedy soup  \n\n# model soup  \n\nCLIPALIGNBASICtransformer  \n\n1Error landscape visualizations  \n\nCLIPImageNettraining losstest error  \n\n{% asset_img angle.png model soup %}  \n\nxyerror landscape  \n\n  \n- finetuned solution  \n- solution -- initialization --   \n\nsolutionmodel averagemodel soupsolution  \n\n{% asset_img angle_2.png model soup %}  \n\n2Ensemble comparison  \n\nmodel soupensemblelearning rate  \n\n{% asset_img compare.png model soup %}  \n\n  \n- lrensemblemodel soup  \n- lrensemblemodel soup  \n- lrensemblemodel souplr  \n- in-distributionensembledistribution shiftmodel soup  \n\n3One dimensional hyperparameter grids  \n\n  \n\noptimizeraugmentationlrlr  \n\n4  \n\nmodel soup  \n\n{% asset_img result.png model soup %}  \n\n  \n\n#   \n\n- model soupgreedy model soup  \n- model soupadapterLoRA  \n- r-drop  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n- MoE  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[denseMoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  \n[MoE--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  \n-   \n[Llama3.1--](https://www.linsight.cn/7d7294cb.html)  \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[--](https://www.linsight.cn/210dbccd.html)  \n-   \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n-   \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n-   \n[Llama3.1--post-training](https://www.linsight.cn/93328a2a.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n- Transformer  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n-   \n[(1)](http://www.linsight.cn/3345028a.html)\n[(2)](http://www.linsight.cn/ad0bba9d.html)\n[(3)](http://www.linsight.cn/1736008.html)\n[(4)](http://www.linsight.cn/1736008.html)\n[(5)](http://www.linsight.cn/336f2f3e.html)\n[(6)](http://www.linsight.cn/7c04944d.html)\n[(7)](https://www.linsight.cn/dd614e12.html)\n[(8)](https://www.linsight.cn/e287b9c3.html)  \n\n# Reference  \n\n1Averaging Weights Leads to Wider Optima and Better Generalization https://arxiv.org/abs/1803.05407  \n2Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time https://arxiv.org/abs/2203.05482  \n3Stochastic Weight Averaging in PyTorch https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/   \n4EMAPyTorch https://zhuanlan.zhihu.com/p/68748778  \n","slug":"cs/nlp/2024/07/model-soup","published":1,"updated":"2024-07-30T12:51:36.203Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsn2002t0p4k0mm281q5","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>DCLMLlama-3.1model soupModel soups:\naveraging weights of multiple fine-tuned models improves accuracy\nwithout increasing inference time</p>\n<p>model soupSWAEMA</p>\n<h1 id=\"\"></h1>\n<p><br>\n- 1checkpoint<br>\n- 2checkpoint</p>\n<p><br>\n-\nensemble<br>\n- out-of-distribution\ndata</p>\n<h1 id=\"swa-ema\">SWA &amp; EMA</h1>\n<p>SWAEMA</p>\n<p>Stochastic Weight\nAveragingSWA</p>\n<p>SWAcheckpoint</p>\n<img src=\"/bb8fcf21/swa_1.png\" class title=\"swa\">\n<img src=\"/bb8fcf21/swa_2.png\" class title=\"swa\">\n<p>SWA</p>\n<p>SGDlosswide flat\nregionwide flat\nregionvolumeSGD</p>\n<p>train losstest errorwide flat\nregion</p>\n<p>SWA</p>\n<p>SWASGDtrain losstest\nerrorSWAtrain\nlosstest\nerror</p>\n<img src=\"/bb8fcf21/swa_3.png\" class title=\"swa\">\n<p>EMASWAEMAPyTorch(<a href=\"https://zhuanlan.zhihu.com/p/68748778\">https://zhuanlan.zhihu.com/p/68748778</a>)</p>\n<h1 id=\"model-soup\">model soup</h1>\n<p>model average<br>\n- What is being transferred in transfer\nlearning?error\nlandscape basin<br>\n- Rethinking the inception architecture for computer\nvisionAveraging weights leads to wider optima and better\ngeneralizationSWAweight\naverage<br>\n- No one representation to rule them all: Overlapping features of\ntraining\nmethodsensemble</p>\n<p>model soupmodel\naverageindependent\nrunEMA/SWA</p>\n<p> <span class=\"math inline\">\\([h_1,...h_k]\\)</span>\n<span class=\"math inline\">\\(\\theta_0\\)</span> <span class=\"math inline\">\\([\\theta_1,...,\\theta_k]\\)</span>\nkcheckpointcheckpointkcheckpointmodel\nsoup</p>\n<p>3model soupuniform soupgreedy souplearned\nsoup</p>\n<img src=\"/bb8fcf21/method_soup.png\" class title=\"model soup\">\n<p>uniform soup</p>\n<p>greedy\nsoupkcheckpointcheckpointcheckpoint</p>\n<img src=\"/bb8fcf21/algo.png\" class title=\"model soup\">\n<p>uniform soupgreedy souplearned\nsoup <span class=\"math inline\">\\(\\alpha\\in\\mathbb{R}^k\\)</span> mixing\ncoefficients<span class=\"math inline\">\\(\\beta\\)</span> temperature\nscaling parameterlearned soup <span class=\"math inline\">\\(\\alpha\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span></p>\n<p><span class=\"math display\">\\[\\arg\\min_{\\alpha\\in\\mathbb{R}^k,\\beta\\in\\mathbb{R}}\\sum_{j=1}^n\\ell\\Bigg(\\beta\\cdot\nf\\Bigg(x_j,\\sum_{i=1}^k\\alpha_i\\theta_i\\Bigg),y_j\\Bigg)\\]</span></p>\n<p>klearned soup</p>\n<p>greedy soup</p>\n<h1 id=\"model-soup\">model soup</h1>\n<p>CLIPALIGNBASICtransformer</p>\n<p>1Error landscape visualizations</p>\n<p>CLIPImageNettraining losstest\nerror</p>\n<img src=\"/bb8fcf21/angle.png\" class title=\"model soup\">\n<p>xyerror\nlandscape</p>\n<p><br>\n- finetuned solution<br>\n- solution --\ninitialization --\n</p>\n<p>solutionmodel\naveragemodel\nsoupsolution</p>\n<img src=\"/bb8fcf21/angle_2.png\" class title=\"model soup\">\n<p>2Ensemble comparison</p>\n<p>model soupensemblelearning rate</p>\n<img src=\"/bb8fcf21/compare.png\" class title=\"model soup\">\n<p><br>\n- lrensemblemodel soup<br>\n- lrensemblemodel soup<br>\n- lrensemblemodel souplr<br>\n- in-distributionensembledistribution\nshiftmodel soup</p>\n<p>3One dimensional hyperparameter grids</p>\n<p></p>\n<p>optimizeraugmentationlrlr</p>\n<p>4</p>\n<p>model soup</p>\n<img src=\"/bb8fcf21/result.png\" class title=\"model soup\">\n<p></p>\n<h1 id=\"\"></h1>\n<ul>\n<li>model soupgreedy model\nsoup<br>\n</li>\n<li>model\nsoupadapterLoRA<br>\n</li>\n<li>r-drop</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n- MoE<br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/a0824e29.html\">denseMoE -- sparse\nupcycling</a><br>\n<a href=\"https://www.linsight.cn/2c8bbc7.html\">MoE--expert choice\nrouting</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/7d7294cb.html\">Llama3.1--</a><br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"https://www.linsight.cn/210dbccd.html\">--</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/93328a2a.html\">Llama3.1--post-training</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n- Transformer<br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a> <a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a> <a href=\"http://www.linsight.cn/1736008.html\">(3)</a> <a href=\"http://www.linsight.cn/1736008.html\">(4)</a> <a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a> <a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a> <a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a> <a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Averaging Weights Leads to Wider Optima and Better\nGeneralization https://arxiv.org/abs/1803.05407<br>\n2Model soups: averaging weights of multiple fine-tuned models\nimproves accuracy without increasing inference time\nhttps://arxiv.org/abs/2203.05482<br>\n3Stochastic Weight Averaging in PyTorch\nhttps://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/<br>\n4EMAPyTorch\nhttps://zhuanlan.zhihu.com/p/68748778</p>\n","length":4360,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>DCLMLlama-3.1model soupModel soups:\naveraging weights of multiple fine-tuned models improves accuracy\nwithout increasing inference time</p>\n<p>model soupSWAEMA</p>\n<h1 id=\"\"></h1>\n<p><br>\n- 1checkpoint<br>\n- 2checkpoint</p>\n<p><br>\n-\nensemble<br>\n- out-of-distribution\ndata</p>\n<h1 id=\"swa-ema\">SWA &amp; EMA</h1>\n<p>SWAEMA</p>\n<p>Stochastic Weight\nAveragingSWA</p>\n<p>SWAcheckpoint</p>\n<img src=\"/bb8fcf21/swa_1.png\" class title=\"swa\">\n<img src=\"/bb8fcf21/swa_2.png\" class title=\"swa\">\n<p>SWA</p>\n<p>SGDlosswide flat\nregionwide flat\nregionvolumeSGD</p>\n<p>train losstest errorwide flat\nregion</p>\n<p>SWA</p>\n<p>SWASGDtrain losstest\nerrorSWAtrain\nlosstest\nerror</p>\n<img src=\"/bb8fcf21/swa_3.png\" class title=\"swa\">\n<p>EMASWAEMAPyTorch(<a href=\"https://zhuanlan.zhihu.com/p/68748778\">https://zhuanlan.zhihu.com/p/68748778</a>)</p>\n<h1 id=\"model-soup\">model soup</h1>\n<p>model average<br>\n- What is being transferred in transfer\nlearning?error\nlandscape basin<br>\n- Rethinking the inception architecture for computer\nvisionAveraging weights leads to wider optima and better\ngeneralizationSWAweight\naverage<br>\n- No one representation to rule them all: Overlapping features of\ntraining\nmethodsensemble</p>\n<p>model soupmodel\naverageindependent\nrunEMA/SWA</p>\n<p> <span class=\"math inline\">\\([h_1,...h_k]\\)</span>\n<span class=\"math inline\">\\(\\theta_0\\)</span> <span class=\"math inline\">\\([\\theta_1,...,\\theta_k]\\)</span>\nkcheckpointcheckpointkcheckpointmodel\nsoup</p>\n<p>3model soupuniform soupgreedy souplearned\nsoup</p>\n<img src=\"/bb8fcf21/method_soup.png\" class title=\"model soup\">\n<p>uniform soup</p>\n<p>greedy\nsoupkcheckpointcheckpointcheckpoint</p>\n<img src=\"/bb8fcf21/algo.png\" class title=\"model soup\">\n<p>uniform soupgreedy souplearned\nsoup <span class=\"math inline\">\\(\\alpha\\in\\mathbb{R}^k\\)</span> mixing\ncoefficients<span class=\"math inline\">\\(\\beta\\)</span> temperature\nscaling parameterlearned soup <span class=\"math inline\">\\(\\alpha\\)</span>  <span class=\"math inline\">\\(\\beta\\)</span></p>\n<p><span class=\"math display\">\\[\\arg\\min_{\\alpha\\in\\mathbb{R}^k,\\beta\\in\\mathbb{R}}\\sum_{j=1}^n\\ell\\Bigg(\\beta\\cdot\nf\\Bigg(x_j,\\sum_{i=1}^k\\alpha_i\\theta_i\\Bigg),y_j\\Bigg)\\]</span></p>\n<p>klearned soup</p>\n<p>greedy soup</p>\n<h1 id=\"model-soup\">model soup</h1>\n<p>CLIPALIGNBASICtransformer</p>\n<p>1Error landscape visualizations</p>\n<p>CLIPImageNettraining losstest\nerror</p>\n<img src=\"/bb8fcf21/angle.png\" class title=\"model soup\">\n<p>xyerror\nlandscape</p>\n<p><br>\n- finetuned solution<br>\n- solution --\ninitialization --\n</p>\n<p>solutionmodel\naveragemodel\nsoupsolution</p>\n<img src=\"/bb8fcf21/angle_2.png\" class title=\"model soup\">\n<p>2Ensemble comparison</p>\n<p>model soupensemblelearning rate</p>\n<img src=\"/bb8fcf21/compare.png\" class title=\"model soup\">\n<p><br>\n- lrensemblemodel soup<br>\n- lrensemblemodel soup<br>\n- lrensemblemodel souplr<br>\n- in-distributionensembledistribution\nshiftmodel soup</p>\n<p>3One dimensional hyperparameter grids</p>\n<p></p>\n<p>optimizeraugmentationlrlr</p>\n<p>4</p>\n<p>model soup</p>\n<img src=\"/bb8fcf21/result.png\" class title=\"model soup\">\n<p></p>\n<h1 id=\"\"></h1>\n<ul>\n<li>model soupgreedy model\nsoup<br>\n</li>\n<li>model\nsoupadapterLoRA<br>\n</li>\n<li>r-drop</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n- MoE<br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/a0824e29.html\">denseMoE -- sparse\nupcycling</a><br>\n<a href=\"https://www.linsight.cn/2c8bbc7.html\">MoE--expert choice\nrouting</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/7d7294cb.html\">Llama3.1--</a><br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"https://www.linsight.cn/210dbccd.html\">--</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/93328a2a.html\">Llama3.1--post-training</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n- Transformer<br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a> <a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a> <a href=\"http://www.linsight.cn/1736008.html\">(3)</a> <a href=\"http://www.linsight.cn/1736008.html\">(4)</a> <a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a> <a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a> <a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a> <a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Averaging Weights Leads to Wider Optima and Better\nGeneralization https://arxiv.org/abs/1803.05407<br>\n2Model soups: averaging weights of multiple fine-tuned models\nimproves accuracy without increasing inference time\nhttps://arxiv.org/abs/2203.05482<br>\n3Stochastic Weight Averaging in PyTorch\nhttps://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/<br>\n4EMAPyTorch\nhttps://zhuanlan.zhihu.com/p/68748778</p>\n"},{"title":"denseMoE -- sparse upcycling","abbrlink":"a0824e29","date":"2024-07-19T13:03:12.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\ndenseMoEdenseGooglesparse upcycling2022T5Vision Transformer  \n\nsparse upcycling  \n- dense  \n- denseMoEMoEdensedenseMoE  \n\n  \n\n#   \n\ntransformersparse upcycling  \n\n{% asset_img intro.png upcycling %}  \n\nMLPMoElayernormattentiondensecopyMoE  \n\n  \n- 2MoE  \n- MoEdense  \n- MoE32FLOPSlarger initial quality drop relative to baseline dense modelquality dropexpert  \n- expertMLP  \n- routerstandard deviation=0.02zero-mean normal distribution  \n- encoderexpert choice routingcapacity factor C = 2capacity factor  \n- decodertoken choice routingtop-k routingk=2auxiliary loss0.01decodertop-k routing\"to avoid train time (full batch teacher forcing) versus inference time (single token auto-regressive decoding) discrepancies\"expert choice routing  \n\nMoEdensebatch sizelearning rate scheduleweight decay  \n\nlearning rate scheduleinverse square root learning rate scheduleMoEdenseschedule  \n\n  \n\n{% asset_img models.png  %}  \n\n#   \n\n## CORE RESULTS  \n\n1dense vs upcycling  \n\nupcyclingdense  \n\n{% asset_img 1.png  %}  \n\n2  \n\n  \n\n{% asset_img 2.png  %}  \n\nMoE  \n\n3MoE from scratch vs upcycling  \n\nMoEupcycling  \n\n{% asset_img 3.png  %}  \n\n- MoElr  \n- upcycling  \n- upcyclingdense1.2upcycling<=densesparse upcycling  \n\n4sparse upcycling vs dense upcycling  \n\nScaling language models: Methods, analysis & insights from training gopherdepth tilingdense upcycling  sparse upcyclingsparse upcycling  \n\n{% asset_img 4.png  %}  \n\ndepth tiling  \n\n##   \n\n1Amount of dense pretraining  \n\nupcyclingdensestepdensecheckpointupcycling200kstep  \n\n{% asset_img a1.png  %}  \n\ncheckpointMoE  \n\n2Router type  \n\nrouterexpert choicetoken choice  \n\n{% asset_img a2.png  %}  \n\nstepexpert choicetoken choiceexpert choice routing  \n\n3Expert capacity factor  \n\ntoken\n\ncapacity factor  \n\n{% asset_img a3.png  %}  \n\nC = 2  \n\n\n4Number of MoE layers  \n\nMoE  \n\nMoE1MLPMoE  \n\n{% asset_img a4.png  %}  \n\nMoE5~6MoE40%~50%  \n\n5Initialization of experts  \n\ndenseMLP  \n\n{% asset_img a5.png  %}  \n\ndense  \n\n6Number of experts  \n\n2~128  \n\n{% asset_img a6.png  %}  \n\n  \n\n##   \n\n1optimizer  \n\nvisiondenseoptimizerMoE  \n\n2router normalization  \n\ndenseMoEperformace droprouternormalizationtokenweight1\n\nexperttokenvanishing routing gradients  \n\nrouter normalizationnormalizationMoEroutertoken choice routing  \n\nMoErouter normalization  \n\n#   \n\n- MoEMoE  \n- MoE  \n- MoEtoken choice routing  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n[(8)](https://www.linsight.cn/e287b9c3.html)  \n\n\n# Reference  \n\n1SPARSE UPCYCLING: TRAINING MIXTURE-OF-EXPERTS FROM DENSE CHECKPOINTS https://arxiv.org/abs/2212.05055  \n","source":"_posts/cs/nlp/2024/07/upcycling.md","raw":"---\ntitle: denseMoE -- sparse upcycling\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - MoE\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: a0824e29\ndate: 2024-07-19 21:03:12\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\ndenseMoEdenseGooglesparse upcycling2022T5Vision Transformer  \n\nsparse upcycling  \n- dense  \n- denseMoEMoEdensedenseMoE  \n\n  \n\n#   \n\ntransformersparse upcycling  \n\n{% asset_img intro.png upcycling %}  \n\nMLPMoElayernormattentiondensecopyMoE  \n\n  \n- 2MoE  \n- MoEdense  \n- MoE32FLOPSlarger initial quality drop relative to baseline dense modelquality dropexpert  \n- expertMLP  \n- routerstandard deviation=0.02zero-mean normal distribution  \n- encoderexpert choice routingcapacity factor C = 2capacity factor  \n- decodertoken choice routingtop-k routingk=2auxiliary loss0.01decodertop-k routing\"to avoid train time (full batch teacher forcing) versus inference time (single token auto-regressive decoding) discrepancies\"expert choice routing  \n\nMoEdensebatch sizelearning rate scheduleweight decay  \n\nlearning rate scheduleinverse square root learning rate scheduleMoEdenseschedule  \n\n  \n\n{% asset_img models.png  %}  \n\n#   \n\n## CORE RESULTS  \n\n1dense vs upcycling  \n\nupcyclingdense  \n\n{% asset_img 1.png  %}  \n\n2  \n\n  \n\n{% asset_img 2.png  %}  \n\nMoE  \n\n3MoE from scratch vs upcycling  \n\nMoEupcycling  \n\n{% asset_img 3.png  %}  \n\n- MoElr  \n- upcycling  \n- upcyclingdense1.2upcycling<=densesparse upcycling  \n\n4sparse upcycling vs dense upcycling  \n\nScaling language models: Methods, analysis & insights from training gopherdepth tilingdense upcycling  sparse upcyclingsparse upcycling  \n\n{% asset_img 4.png  %}  \n\ndepth tiling  \n\n##   \n\n1Amount of dense pretraining  \n\nupcyclingdensestepdensecheckpointupcycling200kstep  \n\n{% asset_img a1.png  %}  \n\ncheckpointMoE  \n\n2Router type  \n\nrouterexpert choicetoken choice  \n\n{% asset_img a2.png  %}  \n\nstepexpert choicetoken choiceexpert choice routing  \n\n3Expert capacity factor  \n\ntoken\n\ncapacity factor  \n\n{% asset_img a3.png  %}  \n\nC = 2  \n\n\n4Number of MoE layers  \n\nMoE  \n\nMoE1MLPMoE  \n\n{% asset_img a4.png  %}  \n\nMoE5~6MoE40%~50%  \n\n5Initialization of experts  \n\ndenseMLP  \n\n{% asset_img a5.png  %}  \n\ndense  \n\n6Number of experts  \n\n2~128  \n\n{% asset_img a6.png  %}  \n\n  \n\n##   \n\n1optimizer  \n\nvisiondenseoptimizerMoE  \n\n2router normalization  \n\ndenseMoEperformace droprouternormalizationtokenweight1\n\nexperttokenvanishing routing gradients  \n\nrouter normalizationnormalizationMoEroutertoken choice routing  \n\nMoErouter normalization  \n\n#   \n\n- MoEMoE  \n- MoE  \n- MoEtoken choice routing  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n[(8)](https://www.linsight.cn/e287b9c3.html)  \n\n\n# Reference  \n\n1SPARSE UPCYCLING: TRAINING MIXTURE-OF-EXPERTS FROM DENSE CHECKPOINTS https://arxiv.org/abs/2212.05055  \n","slug":"cs/nlp/2024/07/upcycling","published":1,"updated":"2024-07-19T14:21:50.417Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsn2002y0p4k0s0s5f0u","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>denseMoEdenseGooglesparse\nupcycling2022T5Vision\nTransformer</p>\n<p>sparse upcycling<br>\n- dense<br>\n-\ndenseMoEMoEdensedenseMoE</p>\n<p></p>\n<h1 id=\"\"></h1>\n<p>transformersparse upcycling</p>\n<img src=\"/a0824e29/intro.png\" class title=\"upcycling\">\n<p>MLPMoElayernormattentiondensecopyMoE</p>\n<p><br>\n- 2MoE<br>\n- MoEdense<br>\n-\nMoE32FLOPSlarger\ninitial quality drop relative to baseline dense\nmodelquality\ndropexpert<br>\n- expertMLP<br>\n- routerstandard deviation=0.02zero-mean normal\ndistribution<br>\n- encoderexpert choice routingcapacity factor C =\n2capacity factor<br>\n- decodertoken choice routingtop-k\nroutingk=2auxiliary\nloss0.01decodertop-k routing\"to\navoid train time (full batch teacher forcing) versus inference time\n(single token auto-regressive decoding) discrepancies\"expert choice\nrouting</p>\n<p>MoEdensebatch sizelearning rate\nscheduleweight decay</p>\n<p>learning rate scheduleinverse square root learning rate\nscheduleMoEdenseschedule</p>\n<p></p>\n<img src=\"/a0824e29/models.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<h2 id=\"core-results\">CORE RESULTS</h2>\n<p>1dense vs upcycling</p>\n<p>upcyclingdense</p>\n<img src=\"/a0824e29/1.png\" class title=\"\">\n<p>2</p>\n<p></p>\n<img src=\"/a0824e29/2.png\" class title=\"\">\n<p>MoE</p>\n<p>3MoE from scratch vs upcycling</p>\n<p>MoEupcycling</p>\n<img src=\"/a0824e29/3.png\" class title=\"\">\n<ul>\n<li>MoElr<br>\n</li>\n<li>upcycling<br>\n</li>\n<li>upcyclingdense1.2upcycling&lt;=densesparse\nupcycling</li>\n</ul>\n<p>4sparse upcycling vs dense upcycling</p>\n<p>Scaling language models: Methods, analysis &amp; insights from\ntraining gopherdepth tilingdense upcycling  sparse\nupcyclingsparse\nupcycling</p>\n<img src=\"/a0824e29/4.png\" class title=\"\">\n<p>depth tiling</p>\n<h2 id=\"\"></h2>\n<p>1Amount of dense pretraining</p>\n<p>upcyclingdensestepdensecheckpointupcycling200kstep</p>\n<img src=\"/a0824e29/a1.png\" class title=\"\">\n<p>checkpointMoE</p>\n<p>2Router type</p>\n<p>routerexpert choicetoken choice</p>\n<img src=\"/a0824e29/a2.png\" class title=\"\">\n<p>stepexpert choicetoken\nchoiceexpert choice\nrouting</p>\n<p>3Expert capacity factor</p>\n<p>token</p>\n<p>capacity factor</p>\n<img src=\"/a0824e29/a3.png\" class title=\"\">\n<p>C =\n2</p>\n<p>4Number of MoE layers</p>\n<p>MoE</p>\n<p>MoE1MLPMoE</p>\n<img src=\"/a0824e29/a4.png\" class title=\"\">\n<p>MoE5<sub>6MoE40%</sub>50%</p>\n<p>5Initialization of experts</p>\n<p>denseMLP</p>\n<img src=\"/a0824e29/a5.png\" class title=\"\">\n<p>dense</p>\n<p>6Number of experts</p>\n<p>2~128</p>\n<img src=\"/a0824e29/a6.png\" class title=\"\">\n<p></p>\n<h2 id=\"\"></h2>\n<p>1optimizer</p>\n<p>visiondenseoptimizerMoE</p>\n<p>2router normalization</p>\n<p>denseMoEperformace\ndroprouternormalizationtokenweight1</p>\n<p>experttokenvanishing\nrouting gradients</p>\n<p>router\nnormalizationnormalizationMoEroutertoken\nchoice routing</p>\n<p>MoErouter\nnormalization</p>\n<h1 id=\"\"></h1>\n<ul>\n<li>MoEMoE<br>\n</li>\n<li>MoE<br>\n</li>\n<li>MoEtoken choice\nrouting</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a><br>\n<a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1SPARSE UPCYCLING: TRAINING MIXTURE-OF-EXPERTS FROM DENSE\nCHECKPOINTS https://arxiv.org/abs/2212.05055</p>\n","length":3965,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>denseMoEdenseGooglesparse\nupcycling2022T5Vision\nTransformer</p>\n<p>sparse upcycling<br>\n- dense<br>\n-\ndenseMoEMoEdensedenseMoE</p>\n<p></p>\n<h1 id=\"\"></h1>\n<p>transformersparse upcycling</p>\n<img src=\"/a0824e29/intro.png\" class title=\"upcycling\">\n<p>MLPMoElayernormattentiondensecopyMoE</p>\n<p><br>\n- 2MoE<br>\n- MoEdense<br>\n-\nMoE32FLOPSlarger\ninitial quality drop relative to baseline dense\nmodelquality\ndropexpert<br>\n- expertMLP<br>\n- routerstandard deviation=0.02zero-mean normal\ndistribution<br>\n- encoderexpert choice routingcapacity factor C =\n2capacity factor<br>\n- decodertoken choice routingtop-k\nroutingk=2auxiliary\nloss0.01decodertop-k routing\"to\navoid train time (full batch teacher forcing) versus inference time\n(single token auto-regressive decoding) discrepancies\"expert choice\nrouting</p>\n<p>MoEdensebatch sizelearning rate\nscheduleweight decay</p>\n<p>learning rate scheduleinverse square root learning rate\nscheduleMoEdenseschedule</p>\n<p></p>\n<img src=\"/a0824e29/models.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<h2 id=\"core-results\">CORE RESULTS</h2>\n<p>1dense vs upcycling</p>\n<p>upcyclingdense</p>\n<img src=\"/a0824e29/1.png\" class title=\"\">\n<p>2</p>\n<p></p>\n<img src=\"/a0824e29/2.png\" class title=\"\">\n<p>MoE</p>\n<p>3MoE from scratch vs upcycling</p>\n<p>MoEupcycling</p>\n<img src=\"/a0824e29/3.png\" class title=\"\">\n<ul>\n<li>MoElr<br>\n</li>\n<li>upcycling<br>\n</li>\n<li>upcyclingdense1.2upcycling&lt;=densesparse\nupcycling</li>\n</ul>\n<p>4sparse upcycling vs dense upcycling</p>\n<p>Scaling language models: Methods, analysis &amp; insights from\ntraining gopherdepth tilingdense upcycling  sparse\nupcyclingsparse\nupcycling</p>\n<img src=\"/a0824e29/4.png\" class title=\"\">\n<p>depth tiling</p>\n<h2 id=\"\"></h2>\n<p>1Amount of dense pretraining</p>\n<p>upcyclingdensestepdensecheckpointupcycling200kstep</p>\n<img src=\"/a0824e29/a1.png\" class title=\"\">\n<p>checkpointMoE</p>\n<p>2Router type</p>\n<p>routerexpert choicetoken choice</p>\n<img src=\"/a0824e29/a2.png\" class title=\"\">\n<p>stepexpert choicetoken\nchoiceexpert choice\nrouting</p>\n<p>3Expert capacity factor</p>\n<p>token</p>\n<p>capacity factor</p>\n<img src=\"/a0824e29/a3.png\" class title=\"\">\n<p>C =\n2</p>\n<p>4Number of MoE layers</p>\n<p>MoE</p>\n<p>MoE1MLPMoE</p>\n<img src=\"/a0824e29/a4.png\" class title=\"\">\n<p>MoE5<sub>6MoE40%</sub>50%</p>\n<p>5Initialization of experts</p>\n<p>denseMLP</p>\n<img src=\"/a0824e29/a5.png\" class title=\"\">\n<p>dense</p>\n<p>6Number of experts</p>\n<p>2~128</p>\n<img src=\"/a0824e29/a6.png\" class title=\"\">\n<p></p>\n<h2 id=\"\"></h2>\n<p>1optimizer</p>\n<p>visiondenseoptimizerMoE</p>\n<p>2router normalization</p>\n<p>denseMoEperformace\ndroprouternormalizationtokenweight1</p>\n<p>experttokenvanishing\nrouting gradients</p>\n<p>router\nnormalizationnormalizationMoEroutertoken\nchoice routing</p>\n<p>MoErouter\nnormalization</p>\n<h1 id=\"\"></h1>\n<ul>\n<li>MoEMoE<br>\n</li>\n<li>MoE<br>\n</li>\n<li>MoEtoken choice\nrouting</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a><br>\n<a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1SPARSE UPCYCLING: TRAINING MIXTURE-OF-EXPERTS FROM DENSE\nCHECKPOINTS https://arxiv.org/abs/2212.05055</p>\n"},{"title":"MoE","abbrlink":"5e1d14b3","date":"2024-07-16T12:14:40.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nMoE  \n\n# MoE  \n\nSparse Mixture-of-ExpertsNkk < N  \n\n$$\\mathbf{y}=\\sum_{n\\in N}g_n(\\mathbf{x};\\mathbf{G},\\mathbf{k})E_n(\\mathbf{x})$$  \n\n$$\\mathrm{Expert}(x)=W_\\text{down}(W_\\text{up}x\\odot\\mathrm{Act}(W_\\text{gate}x))$$  \n\n$$W_{\\mathrm{up}},W_{\\mathrm{gate}}\\in\\mathbb{R}^{d_{\\mathrm{mid}}\\times d_{\\mathrm{hid}}}$$  \n\n$$W_{\\mathrm{down}}\\in\\mathbb{R}^{d_{\\mathrm{hid}}\\times d_{\\mathrm{mid}}}$$  \n\n $W_{\\mathrm{up}}[i,:]$  $W_{\\mathrm{gate}}[i,:]$  $W_{\\mathrm{down}}[:,i]$ neurond_midneuron  \n\n#   \n\nMixtral 8x7BDeepSeekMoE  Grok-1MoEMistral 7Bdense  \n\n  \n\n{% asset_img models.png  %}  \n\ncosine similarity  \n\n# Analysis of Static Parameters  \n\n1MoE experts2gatingMoE  \n\n## MoE experts  \n\nTransformer feed-forward layers are keyvalue memoriesEmpirical study on updating key-value memories in transformer feed-forward layersexpertprojection matriceskeysvalues  \n- W_downpossible outputs  \n- W_uppossible outputs  \n- W_gateneuron  \n\nexpertsmatrix levelneuron level  \n\n1matrix level  \n\nDeepSeekMoEshared expertPCA2  \n\n{% asset_img matrix_level.png matrix level %}  \n\n  \n- DeepSeekMoEGrok-1MixtralDeepSeekMoEGrok-1Mixtral  \n- Mixtral  \n-   \n\n2neuron level  \n\nmatrix levelneuronneuronneuron levelJonker-Volgenant  \n\nKendalls coefficientKendalls coefficient1-10  \n\n{% asset_img t2.png neuron level %}  \n\nMixtral  \n\n## Gate Embedding  \n\ngatinggate embeddinggate embeddingmatrices  \n\ngate embeddingXW_upW_gateW_downYlinear regressionsquare of Pearson correlation coefficients  \n\n{% asset_img gating_1.png gating %}  \n\n  \n\n{% asset_img gating_2.png gating %}  \n\n  \n- gate embeddingW_gate  \n- MixtralDeepSeekMoEXY_gateGrok-1>25  \n- gate embeddingW_gateneuron  \n\n## Summary  \n\n-   \n- W_upW_gateW_down  \n\n# Analysis of Dynamic Behaviours  \n\n6token1100tokenemmm  \n\n## Outputs of Experts  \n\nMoE  \n\n  \n\n{% asset_img dynamic.png  %}  \n\nangular similarity  \n\n$$\\text{angular sim}=1-\\frac{\\arccos{(\\text{cosine sim})}}{\\pi}$$  \n\nMixtralnorm  \n\nDeepSeekMixtral  \n\nGrokGrokexpert size  \n\n## Norms of Expert Outputs and Gate Scores  \n\nexpertsL2 normgating decision  \n\ngate scorenorm  \n\n{% asset_img norm.png norm %}  \n\nMixtralexpertfeature vector normCompetesmoeeffective training of sparse mixture of experts via competitionnormImproved transformer pretraining with extra normalization  \n\nDeepSeekMixtralDeepSeekgatingnormtop-1norm  \n\nGrokGrokgatingnormGeLUgatingnormMixtralDeepSeekGroknormnorm1  \n\n## Summary  \n\n- MixtralDeepSeek  \n- expert outputheat mapneuron-levelheat map  \n- MixtralDeepSeeklarge norm outputexpert  \n\n# Suggestions  \n\n  \n- Neuron-level expertsgate embeddingW_gateneurongate embeddingW_gateneuron  \n- Model architecture//gatingnormnorm  \n- Correlation measurementweight matricestokenweight matricesoverview  \n- Training schemeDeepSeekGrokdenseMixtra  \n\n#   \n\n- MoE  \n-   \n- MoE  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n# Reference  \n\n1A Closer Look into Mixture-of-Experts in Large Language Models https://arxiv.org/abs/2406.18219  \n","source":"_posts/cs/nlp/2024/07/MoE.md","raw":"---\ntitle: MoE\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - MoE\ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 5e1d14b3\ndate: 2024-07-16 20:14:40\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nMoE  \n\n# MoE  \n\nSparse Mixture-of-ExpertsNkk < N  \n\n$$\\mathbf{y}=\\sum_{n\\in N}g_n(\\mathbf{x};\\mathbf{G},\\mathbf{k})E_n(\\mathbf{x})$$  \n\n$$\\mathrm{Expert}(x)=W_\\text{down}(W_\\text{up}x\\odot\\mathrm{Act}(W_\\text{gate}x))$$  \n\n$$W_{\\mathrm{up}},W_{\\mathrm{gate}}\\in\\mathbb{R}^{d_{\\mathrm{mid}}\\times d_{\\mathrm{hid}}}$$  \n\n$$W_{\\mathrm{down}}\\in\\mathbb{R}^{d_{\\mathrm{hid}}\\times d_{\\mathrm{mid}}}$$  \n\n $W_{\\mathrm{up}}[i,:]$  $W_{\\mathrm{gate}}[i,:]$  $W_{\\mathrm{down}}[:,i]$ neurond_midneuron  \n\n#   \n\nMixtral 8x7BDeepSeekMoE  Grok-1MoEMistral 7Bdense  \n\n  \n\n{% asset_img models.png  %}  \n\ncosine similarity  \n\n# Analysis of Static Parameters  \n\n1MoE experts2gatingMoE  \n\n## MoE experts  \n\nTransformer feed-forward layers are keyvalue memoriesEmpirical study on updating key-value memories in transformer feed-forward layersexpertprojection matriceskeysvalues  \n- W_downpossible outputs  \n- W_uppossible outputs  \n- W_gateneuron  \n\nexpertsmatrix levelneuron level  \n\n1matrix level  \n\nDeepSeekMoEshared expertPCA2  \n\n{% asset_img matrix_level.png matrix level %}  \n\n  \n- DeepSeekMoEGrok-1MixtralDeepSeekMoEGrok-1Mixtral  \n- Mixtral  \n-   \n\n2neuron level  \n\nmatrix levelneuronneuronneuron levelJonker-Volgenant  \n\nKendalls coefficientKendalls coefficient1-10  \n\n{% asset_img t2.png neuron level %}  \n\nMixtral  \n\n## Gate Embedding  \n\ngatinggate embeddinggate embeddingmatrices  \n\ngate embeddingXW_upW_gateW_downYlinear regressionsquare of Pearson correlation coefficients  \n\n{% asset_img gating_1.png gating %}  \n\n  \n\n{% asset_img gating_2.png gating %}  \n\n  \n- gate embeddingW_gate  \n- MixtralDeepSeekMoEXY_gateGrok-1>25  \n- gate embeddingW_gateneuron  \n\n## Summary  \n\n-   \n- W_upW_gateW_down  \n\n# Analysis of Dynamic Behaviours  \n\n6token1100tokenemmm  \n\n## Outputs of Experts  \n\nMoE  \n\n  \n\n{% asset_img dynamic.png  %}  \n\nangular similarity  \n\n$$\\text{angular sim}=1-\\frac{\\arccos{(\\text{cosine sim})}}{\\pi}$$  \n\nMixtralnorm  \n\nDeepSeekMixtral  \n\nGrokGrokexpert size  \n\n## Norms of Expert Outputs and Gate Scores  \n\nexpertsL2 normgating decision  \n\ngate scorenorm  \n\n{% asset_img norm.png norm %}  \n\nMixtralexpertfeature vector normCompetesmoeeffective training of sparse mixture of experts via competitionnormImproved transformer pretraining with extra normalization  \n\nDeepSeekMixtralDeepSeekgatingnormtop-1norm  \n\nGrokGrokgatingnormGeLUgatingnormMixtralDeepSeekGroknormnorm1  \n\n## Summary  \n\n- MixtralDeepSeek  \n- expert outputheat mapneuron-levelheat map  \n- MixtralDeepSeeklarge norm outputexpert  \n\n# Suggestions  \n\n  \n- Neuron-level expertsgate embeddingW_gateneurongate embeddingW_gateneuron  \n- Model architecture//gatingnormnorm  \n- Correlation measurementweight matricestokenweight matricesoverview  \n- Training schemeDeepSeekGrokdenseMixtra  \n\n#   \n\n- MoE  \n-   \n- MoE  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n# Reference  \n\n1A Closer Look into Mixture-of-Experts in Large Language Models https://arxiv.org/abs/2406.18219  \n","slug":"cs/nlp/2024/07/MoE","published":1,"updated":"2024-07-16T12:33:17.731Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsn200310p4k2z6uej8j","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>MoE</p>\n<h1 id=\"moe\">MoE</h1>\n<p>Sparse Mixture-of-ExpertsNkk &lt;\nN</p>\n<p><span class=\"math display\">\\[\\mathbf{y}=\\sum_{n\\in\nN}g_n(\\mathbf{x};\\mathbf{G},\\mathbf{k})E_n(\\mathbf{x})\\]</span></p>\n<p><span class=\"math display\">\\[\\mathrm{Expert}(x)=W_\\text{down}(W_\\text{up}x\\odot\\mathrm{Act}(W_\\text{gate}x))\\]</span></p>\n<p><span class=\"math display\">\\[W_{\\mathrm{up}},W_{\\mathrm{gate}}\\in\\mathbb{R}^{d_{\\mathrm{mid}}\\times\nd_{\\mathrm{hid}}}\\]</span></p>\n<p><span class=\"math display\">\\[W_{\\mathrm{down}}\\in\\mathbb{R}^{d_{\\mathrm{hid}}\\times\nd_{\\mathrm{mid}}}\\]</span></p>\n<p> <span class=\"math inline\">\\(W_{\\mathrm{up}}[i,:]\\)</span> \n<span class=\"math inline\">\\(W_{\\mathrm{gate}}[i,:]\\)</span>\n <span class=\"math inline\">\\(W_{\\mathrm{down}}[:,i]\\)</span>\nneurond_midneuron</p>\n<h1 id=\"\"></h1>\n<p>Mixtral 8x7BDeepSeekMoE \nGrok-1MoEMistral\n7Bdense</p>\n<p></p>\n<img src=\"/5e1d14b3/models.png\" class title=\"\">\n<p>cosine similarity</p>\n<h1 id=\"analysis-of-static-parameters\">Analysis of Static\nParameters</h1>\n<p>1MoE\nexperts2gatingMoE</p>\n<h2 id=\"moe-experts\">MoE experts</h2>\n<p>Transformer feed-forward layers are keyvalue\nmemoriesEmpirical study on updating key-value memories in\ntransformer feed-forward layersexpertprojection\nmatriceskeysvalues<br>\n- W_downpossible outputs<br>\n- W_uppossible outputs<br>\n- W_gateneuron</p>\n<p>expertsmatrix levelneuron level</p>\n<p>1matrix level</p>\n<p>DeepSeekMoEshared\nexpertPCA2</p>\n<img src=\"/5e1d14b3/matrix_level.png\" class title=\"matrix level\">\n<p><br>\n-\nDeepSeekMoEGrok-1MixtralDeepSeekMoEGrok-1Mixtral<br>\n-\nMixtral<br>\n-\n</p>\n<p>2neuron level</p>\n<p>matrix\nlevelneuronneuronneuron\nlevelJonker-Volgenant</p>\n<p>Kendalls coefficientKendalls\ncoefficient1-10</p>\n<img src=\"/5e1d14b3/t2.png\" class title=\"neuron level\">\n<p>Mixtral</p>\n<h2 id=\"gate-embedding\">Gate Embedding</h2>\n<p>gatinggate\nembeddinggate\nembeddingmatrices</p>\n<p>gate\nembeddingXW_upW_gateW_downYlinear\nregressionsquare of Pearson correlation\ncoefficients</p>\n<img src=\"/5e1d14b3/gating_1.png\" class title=\"gating\">\n<p></p>\n<img src=\"/5e1d14b3/gating_2.png\" class title=\"gating\">\n<p><br>\n- gate embeddingW_gate<br>\n-\nMixtralDeepSeekMoEXY_gateGrok-1&gt;25<br>\n- gate\nembeddingW_gateneuron</p>\n<h2 id=\"summary\">Summary</h2>\n<ul>\n<li><br>\n</li>\n<li>W_upW_gateW_down</li>\n</ul>\n<h1 id=\"analysis-of-dynamic-behaviours\">Analysis of Dynamic\nBehaviours</h1>\n<p>6token1100tokenemmm</p>\n<h2 id=\"outputs-of-experts\">Outputs of Experts</h2>\n<p>MoE</p>\n<p></p>\n<img src=\"/5e1d14b3/dynamic.png\" class title=\"\">\n<p>angular similarity</p>\n<p><span class=\"math display\">\\[\\text{angular\nsim}=1-\\frac{\\arccos{(\\text{cosine sim})}}{\\pi}\\]</span></p>\n<p>Mixtralnorm</p>\n<p>DeepSeekMixtral</p>\n<p>GrokGrokexpert\nsize</p>\n<h2 id=\"norms-of-expert-outputs-and-gate-scores\">Norms of Expert Outputs\nand Gate Scores</h2>\n<p>expertsL2\nnormgating decision</p>\n<p>gate scorenorm</p>\n<img src=\"/5e1d14b3/norm.png\" class title=\"norm\">\n<p>Mixtralexpertfeature vector\nnormCompetesmoeeffective training of sparse\nmixture of experts via\ncompetitionnormImproved\ntransformer pretraining with extra normalization</p>\n<p>DeepSeekMixtralDeepSeekgatingnormtop-1norm</p>\n<p>GrokGrokgatingnormGeLUgatingnormMixtralDeepSeekGroknormnorm1</p>\n<h2 id=\"summary-1\">Summary</h2>\n<ul>\n<li>MixtralDeepSeek<br>\n</li>\n<li>expert outputheat mapneuron-levelheat\nmap<br>\n</li>\n<li>MixtralDeepSeeklarge norm\noutputexpert</li>\n</ul>\n<h1 id=\"suggestions\">Suggestions</h1>\n<p><br>\n- Neuron-level expertsgate\nembeddingW_gateneurongate\nembeddingW_gateneuron<br>\n- Model\narchitecture//gatingnormnorm<br>\n- Correlation measurementweight\nmatricestokenweight\nmatricesoverview<br>\n- Training\nschemeDeepSeekGrokdenseMixtra</p>\n<h1 id=\"\"></h1>\n<ul>\n<li>MoE<br>\n</li>\n<li><br>\n</li>\n<li>MoE</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1A Closer Look into Mixture-of-Experts in Large Language Models\nhttps://arxiv.org/abs/2406.18219</p>\n","length":4717,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>MoE</p>\n<h1 id=\"moe\">MoE</h1>\n<p>Sparse Mixture-of-ExpertsNkk &lt;\nN</p>\n<p><span class=\"math display\">\\[\\mathbf{y}=\\sum_{n\\in\nN}g_n(\\mathbf{x};\\mathbf{G},\\mathbf{k})E_n(\\mathbf{x})\\]</span></p>\n<p><span class=\"math display\">\\[\\mathrm{Expert}(x)=W_\\text{down}(W_\\text{up}x\\odot\\mathrm{Act}(W_\\text{gate}x))\\]</span></p>\n<p><span class=\"math display\">\\[W_{\\mathrm{up}},W_{\\mathrm{gate}}\\in\\mathbb{R}^{d_{\\mathrm{mid}}\\times\nd_{\\mathrm{hid}}}\\]</span></p>\n<p><span class=\"math display\">\\[W_{\\mathrm{down}}\\in\\mathbb{R}^{d_{\\mathrm{hid}}\\times\nd_{\\mathrm{mid}}}\\]</span></p>\n<p> <span class=\"math inline\">\\(W_{\\mathrm{up}}[i,:]\\)</span> \n<span class=\"math inline\">\\(W_{\\mathrm{gate}}[i,:]\\)</span>\n <span class=\"math inline\">\\(W_{\\mathrm{down}}[:,i]\\)</span>\nneurond_midneuron</p>\n<h1 id=\"\"></h1>\n<p>Mixtral 8x7BDeepSeekMoE \nGrok-1MoEMistral\n7Bdense</p>\n<p></p>\n<img src=\"/5e1d14b3/models.png\" class title=\"\">\n<p>cosine similarity</p>\n<h1 id=\"analysis-of-static-parameters\">Analysis of Static\nParameters</h1>\n<p>1MoE\nexperts2gatingMoE</p>\n<h2 id=\"moe-experts\">MoE experts</h2>\n<p>Transformer feed-forward layers are keyvalue\nmemoriesEmpirical study on updating key-value memories in\ntransformer feed-forward layersexpertprojection\nmatriceskeysvalues<br>\n- W_downpossible outputs<br>\n- W_uppossible outputs<br>\n- W_gateneuron</p>\n<p>expertsmatrix levelneuron level</p>\n<p>1matrix level</p>\n<p>DeepSeekMoEshared\nexpertPCA2</p>\n<img src=\"/5e1d14b3/matrix_level.png\" class title=\"matrix level\">\n<p><br>\n-\nDeepSeekMoEGrok-1MixtralDeepSeekMoEGrok-1Mixtral<br>\n-\nMixtral<br>\n-\n</p>\n<p>2neuron level</p>\n<p>matrix\nlevelneuronneuronneuron\nlevelJonker-Volgenant</p>\n<p>Kendalls coefficientKendalls\ncoefficient1-10</p>\n<img src=\"/5e1d14b3/t2.png\" class title=\"neuron level\">\n<p>Mixtral</p>\n<h2 id=\"gate-embedding\">Gate Embedding</h2>\n<p>gatinggate\nembeddinggate\nembeddingmatrices</p>\n<p>gate\nembeddingXW_upW_gateW_downYlinear\nregressionsquare of Pearson correlation\ncoefficients</p>\n<img src=\"/5e1d14b3/gating_1.png\" class title=\"gating\">\n<p></p>\n<img src=\"/5e1d14b3/gating_2.png\" class title=\"gating\">\n<p><br>\n- gate embeddingW_gate<br>\n-\nMixtralDeepSeekMoEXY_gateGrok-1&gt;25<br>\n- gate\nembeddingW_gateneuron</p>\n<h2 id=\"summary\">Summary</h2>\n<ul>\n<li><br>\n</li>\n<li>W_upW_gateW_down</li>\n</ul>\n<h1 id=\"analysis-of-dynamic-behaviours\">Analysis of Dynamic\nBehaviours</h1>\n<p>6token1100tokenemmm</p>\n<h2 id=\"outputs-of-experts\">Outputs of Experts</h2>\n<p>MoE</p>\n<p></p>\n<img src=\"/5e1d14b3/dynamic.png\" class title=\"\">\n<p>angular similarity</p>\n<p><span class=\"math display\">\\[\\text{angular\nsim}=1-\\frac{\\arccos{(\\text{cosine sim})}}{\\pi}\\]</span></p>\n<p>Mixtralnorm</p>\n<p>DeepSeekMixtral</p>\n<p>GrokGrokexpert\nsize</p>\n<h2 id=\"norms-of-expert-outputs-and-gate-scores\">Norms of Expert Outputs\nand Gate Scores</h2>\n<p>expertsL2\nnormgating decision</p>\n<p>gate scorenorm</p>\n<img src=\"/5e1d14b3/norm.png\" class title=\"norm\">\n<p>Mixtralexpertfeature vector\nnormCompetesmoeeffective training of sparse\nmixture of experts via\ncompetitionnormImproved\ntransformer pretraining with extra normalization</p>\n<p>DeepSeekMixtralDeepSeekgatingnormtop-1norm</p>\n<p>GrokGrokgatingnormGeLUgatingnormMixtralDeepSeekGroknormnorm1</p>\n<h2 id=\"summary-1\">Summary</h2>\n<ul>\n<li>MixtralDeepSeek<br>\n</li>\n<li>expert outputheat mapneuron-levelheat\nmap<br>\n</li>\n<li>MixtralDeepSeeklarge norm\noutputexpert</li>\n</ul>\n<h1 id=\"suggestions\">Suggestions</h1>\n<p><br>\n- Neuron-level expertsgate\nembeddingW_gateneurongate\nembeddingW_gateneuron<br>\n- Model\narchitecture//gatingnormnorm<br>\n- Correlation measurementweight\nmatricestokenweight\nmatricesoverview<br>\n- Training\nschemeDeepSeekGrokdenseMixtra</p>\n<h1 id=\"\"></h1>\n<ul>\n<li>MoE<br>\n</li>\n<li><br>\n</li>\n<li>MoE</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1A Closer Look into Mixture-of-Experts in Large Language Models\nhttps://arxiv.org/abs/2406.18219</p>\n"},{"title":"(8)","abbrlink":"e287b9c3","date":"2024-07-18T12:11:05.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~  \n\n~\n\n***  \n\n# 1.GeLU  \n\nGeLU2016Gaussian Error Linear Units (GELUs)  \n\nReLUReLUReLUdropout0  \n\nGeLU0  \n\nGeLU(x) = x*P(Xx)=x*(x)  \n\n(x)GeLU  \n\n{% asset_img 1.png  %}  \n\nGeLUReLU0  \n\n# 2.GshardSwitch TransformerMoEexpert capacityexpertbatchtokenexpert capacity  \n\nexpert capacitybatchtokentokenexperttokenoverflowoverflowtoken  \n\nbatchNtokenEexperttoken2expertexpert capacity = 2N/Etokenoverflowexpert capacityoverflowexpert  \n\nexpert capacity  \n\n{% asset_img 2.png  %}  \n\n# 3.recomputation  \nbatch size483224OOM131643421242recomputation  \n\n  \n\n\n\n# 4.MoEexpert  \n\nexpertexpert16128120643284,426,165,368  \n\n8  \n\n# 5.MoEexpert  \n\n1tokenbatch size  \n\n2  \n\n3  \n\n4  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  ","source":"_posts/cs/nlp/2024/07/-8.md","raw":"---\ntitle: (8)\ntags:\n  - NLP\n  - LLM\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: e287b9c3\ndate: 2024-07-18 20:11:05\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~  \n\n~\n\n***  \n\n# 1.GeLU  \n\nGeLU2016Gaussian Error Linear Units (GELUs)  \n\nReLUReLUReLUdropout0  \n\nGeLU0  \n\nGeLU(x) = x*P(Xx)=x*(x)  \n\n(x)GeLU  \n\n{% asset_img 1.png  %}  \n\nGeLUReLU0  \n\n# 2.GshardSwitch TransformerMoEexpert capacityexpertbatchtokenexpert capacity  \n\nexpert capacitybatchtokentokenexperttokenoverflowoverflowtoken  \n\nbatchNtokenEexperttoken2expertexpert capacity = 2N/Etokenoverflowexpert capacityoverflowexpert  \n\nexpert capacity  \n\n{% asset_img 2.png  %}  \n\n# 3.recomputation  \nbatch size483224OOM131643421242recomputation  \n\n  \n\n\n\n# 4.MoEexpert  \n\nexpertexpert16128120643284,426,165,368  \n\n8  \n\n# 5.MoEexpert  \n\n1tokenbatch size  \n\n2  \n\n3  \n\n4  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  ","slug":"cs/nlp/2024/07/-8","published":1,"updated":"2024-07-18T12:59:18.544Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsn300360p4k1je1669v","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM~</p>\n<p>~</p>\n<hr>\n<h1 id=\"gelu\">1.GeLU</h1>\n<p>GeLU2016Gaussian Error Linear Units\n(GELUs)</p>\n<p>ReLUReLUReLUdropout0</p>\n<p>GeLU0</p>\n<p>GeLU(x) = x<em>P(Xx)=x</em>(x)</p>\n<p>(x)GeLU</p>\n<img src=\"/e287b9c3/1.png\" class title=\"\">\n<p>GeLUReLU0</p>\n<h1 id=\"gshardswitch-transformermoeexpert-capacityexpertbatchtokenexpert-capacity\">2.GshardSwitch\nTransformerMoEexpert\ncapacityexpertbatchtokenexpert\ncapacity</h1>\n<p>expert\ncapacitybatchtokentokenexperttokenoverflowoverflowtoken</p>\n<p>batchNtokenEexperttoken2expertexpert\ncapacity =\n2N/Etokenoverflowexpert\ncapacityoverflowexpert</p>\n<p>expert capacity</p>\n<img src=\"/e287b9c3/2.png\" class title=\"\">\n<h1 id=\"recomputation\">3.recomputation</h1>\n<p>batch\nsize483224OOM131643421242recomputation</p>\n<p></p>\n<h1 id=\"moeexpert\">4.MoEexpert</h1>\n<p>expertexpert16128120643284,426,165,368</p>\n<p>8</p>\n<h1 id=\"moeexpert\">5.MoEexpert</h1>\n<p>1tokenbatch\nsize</p>\n<p>2</p>\n<p>3</p>\n<p>4</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n","length":2531,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM~</p>\n<p>~</p>\n<hr>\n<h1 id=\"gelu\">1.GeLU</h1>\n<p>GeLU2016Gaussian Error Linear Units\n(GELUs)</p>\n<p>ReLUReLUReLUdropout0</p>\n<p>GeLU0</p>\n<p>GeLU(x) = x<em>P(Xx)=x</em>(x)</p>\n<p>(x)GeLU</p>\n<img src=\"/e287b9c3/1.png\" class title=\"\">\n<p>GeLUReLU0</p>\n<h1 id=\"gshardswitch-transformermoeexpert-capacityexpertbatchtokenexpert-capacity\">2.GshardSwitch\nTransformerMoEexpert\ncapacityexpertbatchtokenexpert\ncapacity</h1>\n<p>expert\ncapacitybatchtokentokenexperttokenoverflowoverflowtoken</p>\n<p>batchNtokenEexperttoken2expertexpert\ncapacity =\n2N/Etokenoverflowexpert\ncapacityoverflowexpert</p>\n<p>expert capacity</p>\n<img src=\"/e287b9c3/2.png\" class title=\"\">\n<h1 id=\"recomputation\">3.recomputation</h1>\n<p>batch\nsize483224OOM131643421242recomputation</p>\n<p></p>\n<h1 id=\"moeexpert\">4.MoEexpert</h1>\n<p>expertexpert16128120643284,426,165,368</p>\n<p>8</p>\n<h1 id=\"moeexpert\">5.MoEexpert</h1>\n<p>1tokenbatch\nsize</p>\n<p>2</p>\n<p>3</p>\n<p>4</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n"},{"title":"--AFM","abbrlink":"1e34e252","date":"2024-07-31T14:28:10.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nWWDC24Apple IntelligenceApple Intelligence Foundation Language Models -- 3BAFM-on-deviceAFM-serverAFM=Apple Foundation ModelAFM-server  \n\n#   \n\nOpenELM  \n\n{% asset_img afm.png afm %}  \n\n  \n- embedding  \n- Small-scale proxies for large-scale transformer training instabilitiesQuery/key normalization  \n- RoPEbase frequency500k  \n\ntokenizerSentencePieceBPEAFM-server100kAFM-on-device49k  \n\n#   \n\n##   \n\n- Applebot  \n-   \n\npersonally identifiable informationPII  \n\n  \n\n1  \n\npipeline  \n- Safarireader modeBoilerpipe  \n- +model based  \n- locality-sensitive n-gram hashing  \n- Large language model-guided document selectionDatacomp-lm: In search of the next generation of training sets for language models  \n- Decontaminationn-gram811benchmark  \n\n2  \n\n  \n\n3  \n\ngithub14PIIDecontamination  \n\n4  \n\n3BQA14Btutorialseminarfilterquality filterfilter  \n\n5  \n\n  \n\n##   \n\nAFM3stage  \n- core  \n- continuedcodemath  \n- context-lengtheningcontinued  \n\nstageSmall-scale proxies for large-scale transformer training instabilitiesParam (simple)  \n\n### Core pre-training  \n\nAFM-server0AFM-on-device+  \n\n1AFM-server  \n\n- 6.3T  \n- sequence length = 4096  \n- batch size = 4096  \n- weight decay = 3.16e-4  \n- cosine lr schedule, max lr = 0.01, min lr = 0.5% max lr  \n- warmup step = 5000  \n\nbatch sizescaling lawbatch sizebatch sizescaling lawbatch size30724096  \n\nproxy modellrlr0.01~0.020.01Paramlr  \n\nRMSProp with momentumAdamW  \n\ncore trainingbaseline  \n- AdamWbeta_1 = 0.9beta_2 = 0.95  \n- weight decay = 1e-4  \n- lr decay0.0001  \n- batch size = 1024  \n\nAFM-on-device3.1T  \n\n  \n\n{% asset_img core_ablation.png afm %}  \n\nAFMcore trainingbaseline  \n\n2AFM-on-device  \n\nAFM-on-device6.4BAFM-serverstructural pruningdistillation  \n\nstructural pruningStructured pruning of large language modelsSheared llama: Accelerating language model pre-training via structured pruning  \n- FFNprune  \n- Soft-Top-K maskingConditional adapters: Parameter-efficient transfer learning with fast inference  \n- core trainingdata mix188Bpruning mask  \n\ncoretarget label0.9 * teacher top-1 prediction + 0.1 * true label  \n\n6.3T  \n\npruningdistillation  \n\n{% asset_img distill.png afm %}  \n\nprune + distill5training costbaseline  \n\n### Continued pre-training  \n\nstagemathcode1T token  \n\n  \n- sequence length = 8192  \n- max lr = 3e-4min lr = 0.1% max lr  \n- weight decay = 1e-5  \n- warmup step = 1000  \n\ncore training  \n\nAFM-on-deviceAFM-server  \n\n3Context lengthening  \n\n100B  \n- sequence length = 32768  \n- RoPE base frequency 500k --> 6315089Scaling laws of rope-based extrapolation  \n- QA  \n\n###   \n\nAFM-on-deviceAFM-serverinternalformulation  \n\n{% asset_img pretrain_1.png afm %}  \n\n{% asset_img pretrain_2.png afm %}  \n\ncontinued pre-trainingmathcode  \n\n# Post-Training  \n\nAFMpost-trainingSFTRLHFiTecMDLOO  \n\n##   \n\npost-training  \n\n###   \n\nreward modelprompt set  \n\n3  \n\n1Mathematics  \n\nstage  \n-   \n-   \n\npromptprompt  \n- Problem rephrase and reversionMetamath: Bootstrap your own mathematical questions for large language models  \n- Problem evolutionWizardLM: Empowering large language models to follow complex instructions  \n\n2Tool use  \n\nsingle-toolmulti-toolmulti-steporacle tool  \n\ntool intent detection  \n\n3Coding  \n\n71self-instructrejection sampling  \n\nsolutionsolutionsolution  \n\n12k  \n\n## SFT  \n\n1  \n\n + rejection samplingSFT  \n\n2  \n\n  \n\n3  \n\nconstant lrAFM-serverAFM-on-devicelr5e62e5  \n\n0.1dropout rate  \n\ncheckpointevalRMbest-of-Ncheckpoint  \n\n## RLHF  \n\nRLHF  \n\n### RM  \n\nRM  \n- promptresponseLlama-33  \n- significantly better, better, slightly better, negligibly better  \n- response3  \n\nRMnon-padding tokenembeddinglinear4MLPheadlinear4MLP4 $u_\\phi^\\mathrm{if},u_\\phi^\\mathrm{verb},u_\\phi^\\mathrm{truth},u_\\phi^\\mathrm{harm}$  \n\nRMsoft label loss functionregularization termRM  \n\n1Soft label loss  \n\nBradley-Terry modely_cc=choseny_rr=rejected\n\n$$\\sigma(r_\\phi(x,y_c)-r_\\phi(x,y_r))$$  \n\nresponse  \n\nltarget preference probability p_lsoft label loss  \n\n$$\\begin{aligned}\nL_{\\mathrm{ranking}}(\\phi)=& -p_\\ell\\log(\\sigma(r_\\phi(x,y_c)-r_\\phi(x,y_r))  \\\\\n&-\\left(1-p_\\ell\\right)\\log(\\sigma(r_\\phi(x,y_r)-r_\\phi(x,y_c))\n\\end{aligned}$$  \n\np_lp_l0.950.850.750.65  \n\n2Single-sided grading as regularization  \n\nregularization loss  \n\n$$\\begin{aligned}L_{\\mathrm{regu}}(\\phi)&=\\sum_{\\text{grade}\\in\\text{if,verb,truth,harm}}\\left(\\text{cross}_\\text{entropy}(u_\\phi^\\mathrm{grade}(x,y_c),z_c^\\mathrm{grade})\\right.\\\\&+\\text{cross}_\\text{entropy}(u_\\phi^\\mathrm{grade}(x,y_r),z_r^\\mathrm{grade})\\Big)\\end{aligned}$$  \n\nz  \n\nRMloss  \n\n$$L_\\text{ranking}(\\phi)+\\lambda L_\\text{regu}(\\phi)$$  \n\n### Iterative teaching committeeiTeC  \n\niterative RLHF  \n\nAFMRLHFrefresh online human preference data collection using a diverse set of the best performing models  \n\nSFTDPO/IPORLmodel committeemodel committee  \n\nRMmodel comittee  \n\nonline RLHFDPOIPOrejection samplingmodel comitteeRM  \n\n### Online RLHF algorithm: MDLOO  \n\nRLHF  \n\n$$\\max_\\theta\\mathbb{E}_{x\\sim\\mathcal{D},y\\sim\\pi_\\theta(\\cdot|x)}\\left[r_\\phi(x,y)-\\beta D_{\\mathrm{KL}}\\left(\\pi_\\theta(\\cdot|x)\\|\\pi_{\\mathrm{ref}}(\\cdot|x)\\right)\\right]$$  \n\nreward function\n\n$$R(x,y)=r_\\phi(x,y)-\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}$$  \n\nreward functionexpectation  \n\nRLHFresponseactioncritictokenreward  \n\n  \n\n1Leave-One-Out (LOO) estimator of the advantage  \n\nkiterationpolicy modelnpromptpromptK  \n\nadvantage  \n\n$$A_k(x,y_i)=R(x,y_i)-\\mathbb{E}_{y\\sim\\pi_{\\theta_k}(\\cdot|x)}[R(x,y)]$$  \n\nleave-one-out (LOO) $A_k(x,y_i)$prompt xK-1response  \n\n$$\\widehat{A}_k(x,y_i)=R(x,y_i)-\\frac{1}{K-1}\\sum_{j\\neq i}R(x,y_j)$$  \n\nBack to basics: Revisiting reinforce style optimization for learning from human feedback in LLMsadvantageRLHF  \n\n2Mirror descent policy optimization (MDPO)  \n\nclipping-based PPOKL divergenceregularization  \n\n# Apple Intelligence  \n\nAFMApple IntelligenceApple IntelligenceiPhoneiPadMac  \n\npost-trainingadapteradapter  \n\n  \n\n{% asset_img intelligence.png afm %}  \n\n## accuracy-recovery adapter  \n\n1  \n\npost-training4-bit  \n\n16-bitLoRALoRAaccuracy-recovery adapter  \n\naccuracy-recovery adapterpre-trainingpost-trainingMB10B  \n\nrank 16rankLoRA81632  \n\naccuracy-recovery adapter  \n\n{% asset_img recover.png afm %}  \n\nrank 16adapteradapteraccuracy-recovery adapter  \n\n2Quantization schemes  \n\nblock size3264accuracy-recovery adapterblock size100k  \n\nAFMembeddingsharedembedding8-bitper-channel quantization  \n\n3  \n\n2-bit3.5~3.7bpw  \n\n## task-specific adapter  \n\naccuracy-recovery adapter4-bit  \n\nsummarizationemailmessagenotificationAFM-serveradapter  \n\n#   \n\n- pre-trainingpost-trainingRMSProp with momentum  \n- accuracy-recovery adapterfollow  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n- MoE  \n<p style=\"line-height: 1.2;\"><small>\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[denseMoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  \n[MoE--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  \n</small></p>\n\n-   \n<p style=\"line-height: 1.2;\"><small>\n[Llama3.1--](https://www.linsight.cn/7d7294cb.html)  \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n</small></p>\n\n-   \n<p style=\"line-height: 1.2;\"><small>\n[--](https://www.linsight.cn/210dbccd.html)  \n</small></p>\n\n-   \n<p style=\"line-height: 1.2;\"><small>\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n</small></p>\n\n-   \n<p style=\"line-height: 1.2;\"><small>\n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n</small></p>\n\n-   \n<p style=\"line-height: 1.2;\"><small>\n[Llama3.1--post-training](https://www.linsight.cn/93328a2a.html)  \n[ -- model soup](https://www.linsight.cn/bb8fcf21.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n</small></p>\n\n- Transformer  \n<p style=\"line-height: 1.2;\"><small>\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n</small></p>\n\n-   \n<p style=\"line-height: 1.2;\"><small>\n[(1)](http://www.linsight.cn/3345028a.html)\n[(2)](http://www.linsight.cn/ad0bba9d.html)\n[(3)](http://www.linsight.cn/1736008.html)\n[(4)](http://www.linsight.cn/1736008.html)\n[(5)](http://www.linsight.cn/336f2f3e.html)\n[(6)](http://www.linsight.cn/7c04944d.html)\n[(7)](https://www.linsight.cn/dd614e12.html)\n[(8)](https://www.linsight.cn/e287b9c3.html)  \n</small></p>\n\n# Reference  \n\n1Apple Intelligence Foundation Language Models https://machinelearning.apple.com/papers/apple_intelligence_foundation_language_models.pdf  \n","source":"_posts/cs/nlp/2024/07/AFM.md","raw":"---\ntitle: --AFM\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \n  - post-training\n  - SFT\n  - DPO\n  - RM\n  - RS\n  - \n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 1e34e252\ndate: 2024-07-31 22:28:10\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nWWDC24Apple IntelligenceApple Intelligence Foundation Language Models -- 3BAFM-on-deviceAFM-serverAFM=Apple Foundation ModelAFM-server  \n\n#   \n\nOpenELM  \n\n{% asset_img afm.png afm %}  \n\n  \n- embedding  \n- Small-scale proxies for large-scale transformer training instabilitiesQuery/key normalization  \n- RoPEbase frequency500k  \n\ntokenizerSentencePieceBPEAFM-server100kAFM-on-device49k  \n\n#   \n\n##   \n\n- Applebot  \n-   \n\npersonally identifiable informationPII  \n\n  \n\n1  \n\npipeline  \n- Safarireader modeBoilerpipe  \n- +model based  \n- locality-sensitive n-gram hashing  \n- Large language model-guided document selectionDatacomp-lm: In search of the next generation of training sets for language models  \n- Decontaminationn-gram811benchmark  \n\n2  \n\n  \n\n3  \n\ngithub14PIIDecontamination  \n\n4  \n\n3BQA14Btutorialseminarfilterquality filterfilter  \n\n5  \n\n  \n\n##   \n\nAFM3stage  \n- core  \n- continuedcodemath  \n- context-lengtheningcontinued  \n\nstageSmall-scale proxies for large-scale transformer training instabilitiesParam (simple)  \n\n### Core pre-training  \n\nAFM-server0AFM-on-device+  \n\n1AFM-server  \n\n- 6.3T  \n- sequence length = 4096  \n- batch size = 4096  \n- weight decay = 3.16e-4  \n- cosine lr schedule, max lr = 0.01, min lr = 0.5% max lr  \n- warmup step = 5000  \n\nbatch sizescaling lawbatch sizebatch sizescaling lawbatch size30724096  \n\nproxy modellrlr0.01~0.020.01Paramlr  \n\nRMSProp with momentumAdamW  \n\ncore trainingbaseline  \n- AdamWbeta_1 = 0.9beta_2 = 0.95  \n- weight decay = 1e-4  \n- lr decay0.0001  \n- batch size = 1024  \n\nAFM-on-device3.1T  \n\n  \n\n{% asset_img core_ablation.png afm %}  \n\nAFMcore trainingbaseline  \n\n2AFM-on-device  \n\nAFM-on-device6.4BAFM-serverstructural pruningdistillation  \n\nstructural pruningStructured pruning of large language modelsSheared llama: Accelerating language model pre-training via structured pruning  \n- FFNprune  \n- Soft-Top-K maskingConditional adapters: Parameter-efficient transfer learning with fast inference  \n- core trainingdata mix188Bpruning mask  \n\ncoretarget label0.9 * teacher top-1 prediction + 0.1 * true label  \n\n6.3T  \n\npruningdistillation  \n\n{% asset_img distill.png afm %}  \n\nprune + distill5training costbaseline  \n\n### Continued pre-training  \n\nstagemathcode1T token  \n\n  \n- sequence length = 8192  \n- max lr = 3e-4min lr = 0.1% max lr  \n- weight decay = 1e-5  \n- warmup step = 1000  \n\ncore training  \n\nAFM-on-deviceAFM-server  \n\n3Context lengthening  \n\n100B  \n- sequence length = 32768  \n- RoPE base frequency 500k --> 6315089Scaling laws of rope-based extrapolation  \n- QA  \n\n###   \n\nAFM-on-deviceAFM-serverinternalformulation  \n\n{% asset_img pretrain_1.png afm %}  \n\n{% asset_img pretrain_2.png afm %}  \n\ncontinued pre-trainingmathcode  \n\n# Post-Training  \n\nAFMpost-trainingSFTRLHFiTecMDLOO  \n\n##   \n\npost-training  \n\n###   \n\nreward modelprompt set  \n\n3  \n\n1Mathematics  \n\nstage  \n-   \n-   \n\npromptprompt  \n- Problem rephrase and reversionMetamath: Bootstrap your own mathematical questions for large language models  \n- Problem evolutionWizardLM: Empowering large language models to follow complex instructions  \n\n2Tool use  \n\nsingle-toolmulti-toolmulti-steporacle tool  \n\ntool intent detection  \n\n3Coding  \n\n71self-instructrejection sampling  \n\nsolutionsolutionsolution  \n\n12k  \n\n## SFT  \n\n1  \n\n + rejection samplingSFT  \n\n2  \n\n  \n\n3  \n\nconstant lrAFM-serverAFM-on-devicelr5e62e5  \n\n0.1dropout rate  \n\ncheckpointevalRMbest-of-Ncheckpoint  \n\n## RLHF  \n\nRLHF  \n\n### RM  \n\nRM  \n- promptresponseLlama-33  \n- significantly better, better, slightly better, negligibly better  \n- response3  \n\nRMnon-padding tokenembeddinglinear4MLPheadlinear4MLP4 $u_\\phi^\\mathrm{if},u_\\phi^\\mathrm{verb},u_\\phi^\\mathrm{truth},u_\\phi^\\mathrm{harm}$  \n\nRMsoft label loss functionregularization termRM  \n\n1Soft label loss  \n\nBradley-Terry modely_cc=choseny_rr=rejected\n\n$$\\sigma(r_\\phi(x,y_c)-r_\\phi(x,y_r))$$  \n\nresponse  \n\nltarget preference probability p_lsoft label loss  \n\n$$\\begin{aligned}\nL_{\\mathrm{ranking}}(\\phi)=& -p_\\ell\\log(\\sigma(r_\\phi(x,y_c)-r_\\phi(x,y_r))  \\\\\n&-\\left(1-p_\\ell\\right)\\log(\\sigma(r_\\phi(x,y_r)-r_\\phi(x,y_c))\n\\end{aligned}$$  \n\np_lp_l0.950.850.750.65  \n\n2Single-sided grading as regularization  \n\nregularization loss  \n\n$$\\begin{aligned}L_{\\mathrm{regu}}(\\phi)&=\\sum_{\\text{grade}\\in\\text{if,verb,truth,harm}}\\left(\\text{cross}_\\text{entropy}(u_\\phi^\\mathrm{grade}(x,y_c),z_c^\\mathrm{grade})\\right.\\\\&+\\text{cross}_\\text{entropy}(u_\\phi^\\mathrm{grade}(x,y_r),z_r^\\mathrm{grade})\\Big)\\end{aligned}$$  \n\nz  \n\nRMloss  \n\n$$L_\\text{ranking}(\\phi)+\\lambda L_\\text{regu}(\\phi)$$  \n\n### Iterative teaching committeeiTeC  \n\niterative RLHF  \n\nAFMRLHFrefresh online human preference data collection using a diverse set of the best performing models  \n\nSFTDPO/IPORLmodel committeemodel committee  \n\nRMmodel comittee  \n\nonline RLHFDPOIPOrejection samplingmodel comitteeRM  \n\n### Online RLHF algorithm: MDLOO  \n\nRLHF  \n\n$$\\max_\\theta\\mathbb{E}_{x\\sim\\mathcal{D},y\\sim\\pi_\\theta(\\cdot|x)}\\left[r_\\phi(x,y)-\\beta D_{\\mathrm{KL}}\\left(\\pi_\\theta(\\cdot|x)\\|\\pi_{\\mathrm{ref}}(\\cdot|x)\\right)\\right]$$  \n\nreward function\n\n$$R(x,y)=r_\\phi(x,y)-\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}$$  \n\nreward functionexpectation  \n\nRLHFresponseactioncritictokenreward  \n\n  \n\n1Leave-One-Out (LOO) estimator of the advantage  \n\nkiterationpolicy modelnpromptpromptK  \n\nadvantage  \n\n$$A_k(x,y_i)=R(x,y_i)-\\mathbb{E}_{y\\sim\\pi_{\\theta_k}(\\cdot|x)}[R(x,y)]$$  \n\nleave-one-out (LOO) $A_k(x,y_i)$prompt xK-1response  \n\n$$\\widehat{A}_k(x,y_i)=R(x,y_i)-\\frac{1}{K-1}\\sum_{j\\neq i}R(x,y_j)$$  \n\nBack to basics: Revisiting reinforce style optimization for learning from human feedback in LLMsadvantageRLHF  \n\n2Mirror descent policy optimization (MDPO)  \n\nclipping-based PPOKL divergenceregularization  \n\n# Apple Intelligence  \n\nAFMApple IntelligenceApple IntelligenceiPhoneiPadMac  \n\npost-trainingadapteradapter  \n\n  \n\n{% asset_img intelligence.png afm %}  \n\n## accuracy-recovery adapter  \n\n1  \n\npost-training4-bit  \n\n16-bitLoRALoRAaccuracy-recovery adapter  \n\naccuracy-recovery adapterpre-trainingpost-trainingMB10B  \n\nrank 16rankLoRA81632  \n\naccuracy-recovery adapter  \n\n{% asset_img recover.png afm %}  \n\nrank 16adapteradapteraccuracy-recovery adapter  \n\n2Quantization schemes  \n\nblock size3264accuracy-recovery adapterblock size100k  \n\nAFMembeddingsharedembedding8-bitper-channel quantization  \n\n3  \n\n2-bit3.5~3.7bpw  \n\n## task-specific adapter  \n\naccuracy-recovery adapter4-bit  \n\nsummarizationemailmessagenotificationAFM-serveradapter  \n\n#   \n\n- pre-trainingpost-trainingRMSProp with momentum  \n- accuracy-recovery adapterfollow  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n\n- MoE  \n<p style=\"line-height: 1.2;\"><small>\n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[denseMoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  \n[MoE--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  \n</small></p>\n\n-   \n<p style=\"line-height: 1.2;\"><small>\n[Llama3.1--](https://www.linsight.cn/7d7294cb.html)  \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n</small></p>\n\n-   \n<p style=\"line-height: 1.2;\"><small>\n[--](https://www.linsight.cn/210dbccd.html)  \n</small></p>\n\n-   \n<p style=\"line-height: 1.2;\"><small>\n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n</small></p>\n\n-   \n<p style=\"line-height: 1.2;\"><small>\n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n</small></p>\n\n-   \n<p style=\"line-height: 1.2;\"><small>\n[Llama3.1--post-training](https://www.linsight.cn/93328a2a.html)  \n[ -- model soup](https://www.linsight.cn/bb8fcf21.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n</small></p>\n\n- Transformer  \n<p style=\"line-height: 1.2;\"><small>\n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n</small></p>\n\n-   \n<p style=\"line-height: 1.2;\"><small>\n[(1)](http://www.linsight.cn/3345028a.html)\n[(2)](http://www.linsight.cn/ad0bba9d.html)\n[(3)](http://www.linsight.cn/1736008.html)\n[(4)](http://www.linsight.cn/1736008.html)\n[(5)](http://www.linsight.cn/336f2f3e.html)\n[(6)](http://www.linsight.cn/7c04944d.html)\n[(7)](https://www.linsight.cn/dd614e12.html)\n[(8)](https://www.linsight.cn/e287b9c3.html)  \n</small></p>\n\n# Reference  \n\n1Apple Intelligence Foundation Language Models https://machinelearning.apple.com/papers/apple_intelligence_foundation_language_models.pdf  \n","slug":"cs/nlp/2024/07/AFM","published":1,"updated":"2024-08-01T12:49:54.506Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsn300390p4k1bq9epn3","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>WWDC24Apple\nIntelligenceApple Intelligence\nFoundation Language Models --\n3BAFM-on-deviceAFM-serverAFM=Apple\nFoundation ModelAFM-server</p>\n<h1 id=\"\"></h1>\n<p>OpenELM</p>\n<img src=\"/1e34e252/afm.png\" class title=\"afm\">\n<p><br>\n- embedding<br>\n- Small-scale proxies for large-scale transformer training\ninstabilitiesQuery/key normalization<br>\n- RoPEbase frequency500k</p>\n<p>tokenizerSentencePieceBPEAFM-server100kAFM-on-device49k</p>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<ul>\n<li>Applebot<br>\n</li>\n<li></li>\n</ul>\n<p>personally\nidentifiable informationPII</p>\n<p></p>\n<p>1</p>\n<p>pipeline<br>\n- Safarireader modeBoilerpipe<br>\n- +model based<br>\n- locality-sensitive n-gram hashing<br>\n- Large language model-guided document\nselectionDatacomp-lm: In search of the next generation of training\nsets for language models<br>\n-\nDecontaminationn-gram811benchmark</p>\n<p>2</p>\n<p></p>\n<p>3</p>\n<p>github14PIIDecontamination</p>\n<p>4</p>\n<p>3BQA14Btutorialseminarfilterquality\nfilterfilter</p>\n<p>5</p>\n<p></p>\n<h2 id=\"\"></h2>\n<p>AFM3stage<br>\n- core<br>\n- continuedcodemath<br>\n-\ncontext-lengtheningcontinued</p>\n<p>stageSmall-scale proxies for large-scale\ntransformer training instabilitiesParam\n(simple)</p>\n<h3 id=\"core-pre-training\">Core pre-training</h3>\n<p>AFM-server0AFM-on-device+</p>\n<p>1AFM-server</p>\n<ul>\n<li>6.3T<br>\n</li>\n<li>sequence length = 4096<br>\n</li>\n<li>batch size = 4096<br>\n</li>\n<li>weight decay = 3.16e-4<br>\n</li>\n<li>cosine lr schedule, max lr = 0.01, min lr = 0.5% max lr<br>\n</li>\n<li>warmup step = 5000</li>\n</ul>\n<p>batch sizescaling\nlawbatch\nsizebatch\nsizescaling\nlawbatch\nsize30724096</p>\n<p>proxy\nmodellrlr0.01~0.020.01Paramlr</p>\n<p>RMSProp with\nmomentumAdamW</p>\n<p>core\ntrainingbaseline<br>\n- AdamWbeta_1 = 0.9beta_2 = 0.95<br>\n- weight decay = 1e-4<br>\n- lr decay0.0001<br>\n- batch size = 1024</p>\n<p>AFM-on-device3.1T</p>\n<p></p>\n<img src=\"/1e34e252/core_ablation.png\" class title=\"afm\">\n<p>AFMcore\ntrainingbaseline</p>\n<p>2AFM-on-device</p>\n<p>AFM-on-device6.4BAFM-serverstructural\npruningdistillation</p>\n<p>structural pruningStructured pruning of large language\nmodelsSheared llama: Accelerating language model pre-training via\nstructured pruning<br>\n- FFNprune<br>\n- Soft-Top-K maskingConditional adapters: Parameter-efficient\ntransfer learning with fast inference<br>\n- core trainingdata mix188Bpruning mask</p>\n<p>coretarget\nlabel0.9 * teacher top-1 prediction + 0.1 * true label</p>\n<p>6.3T</p>\n<p>pruningdistillation</p>\n<img src=\"/1e34e252/distill.png\" class title=\"afm\">\n<p>prune + distill5training\ncostbaseline</p>\n<h3 id=\"continued-pre-training\">Continued pre-training</h3>\n<p>stagemathcode1T\ntoken</p>\n<p><br>\n- sequence length = 8192<br>\n- max lr = 3e-4min lr = 0.1% max lr<br>\n- weight decay = 1e-5<br>\n- warmup step = 1000</p>\n<p>core training</p>\n<p>AFM-on-deviceAFM-server</p>\n<p>3Context lengthening</p>\n<p>100B<br>\n- sequence length = 32768<br>\n- RoPE base frequency 500k --&gt; 6315089Scaling laws of rope-based\nextrapolation<br>\n- QA</p>\n<h3 id=\"\"></h3>\n<p>AFM-on-deviceAFM-serverinternalformulation</p>\n<img src=\"/1e34e252/pretrain_1.png\" class title=\"afm\">\n<img src=\"/1e34e252/pretrain_2.png\" class title=\"afm\">\n<p>continued\npre-trainingmathcode</p>\n<h1 id=\"post-training\">Post-Training</h1>\n<p>AFMpost-trainingSFTRLHFiTecMDLOO</p>\n<h2 id=\"-1\"></h2>\n<p>post-training</p>\n<h3 id=\"\"></h3>\n<p>reward modelprompt\nset</p>\n<p>3</p>\n<p>1Mathematics</p>\n<p>stage<br>\n- <br>\n- </p>\n<p>promptprompt<br>\n- Problem rephrase and reversionMetamath: Bootstrap your own\nmathematical questions for large language models<br>\n- Problem evolutionWizardLM: Empowering large\nlanguage models to follow complex\ninstructions</p>\n<p>2Tool use</p>\n<p>single-toolmulti-toolmulti-steporacle\ntool</p>\n<p>tool intent detection</p>\n<p>3Coding</p>\n<p>71self-instructrejection\nsampling</p>\n<p>solutionsolutionsolution</p>\n<p>12k</p>\n<h2 id=\"sft\">SFT</h2>\n<p>1</p>\n<p> + rejection\nsamplingSFT</p>\n<p>2</p>\n<p></p>\n<p>3</p>\n<p>constant\nlrAFM-serverAFM-on-devicelr5e62e5</p>\n<p>0.1dropout rate</p>\n<p>checkpointevalRMbest-of-Ncheckpoint</p>\n<h2 id=\"rlhf\">RLHF</h2>\n<p>RLHF</p>\n<h3 id=\"rm\">RM</h3>\n<p>RM<br>\n- promptresponseLlama-33<br>\n- significantly better, better, slightly better, negligibly\nbetter<br>\n-\nresponse3</p>\n<p>RMnon-padding\ntokenembeddinglinear4MLPheadlinear4MLP4\n<span class=\"math inline\">\\(u_\\phi^\\mathrm{if},u_\\phi^\\mathrm{verb},u_\\phi^\\mathrm{truth},u_\\phi^\\mathrm{harm}\\)</span></p>\n<p>RMsoft label loss\nfunctionregularization\ntermRM</p>\n<p>1Soft label loss</p>\n<p>Bradley-Terry\nmodely_cc=choseny_rr=rejected</p>\n<p><span class=\"math display\">\\[\\sigma(r_\\phi(x,y_c)-r_\\phi(x,y_r))\\]</span></p>\n<p>response</p>\n<p>ltarget preference\nprobability p_lsoft label loss</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nL_{\\mathrm{ranking}}(\\phi)=&amp;\n-p_\\ell\\log(\\sigma(r_\\phi(x,y_c)-r_\\phi(x,y_r))  \\\\\n&amp;-\\left(1-p_\\ell\\right)\\log(\\sigma(r_\\phi(x,y_r)-r_\\phi(x,y_c))\n\\end{aligned}\\]</span></p>\n<p>p_lp_l0.950.850.750.65</p>\n<p>2Single-sided grading as regularization</p>\n<p>regularization loss</p>\n<p><span class=\"math display\">\\[\\begin{aligned}L_{\\mathrm{regu}}(\\phi)&amp;=\\sum_{\\text{grade}\\in\\text{if,verb,truth,harm}}\\left(\\text{cross}_\\text{entropy}(u_\\phi^\\mathrm{grade}(x,y_c),z_c^\\mathrm{grade})\\right.\\\\&amp;+\\text{cross}_\\text{entropy}(u_\\phi^\\mathrm{grade}(x,y_r),z_r^\\mathrm{grade})\\Big)\\end{aligned}\\]</span></p>\n<p>z</p>\n<p>RMloss</p>\n<p><span class=\"math display\">\\[L_\\text{ranking}(\\phi)+\\lambda\nL_\\text{regu}(\\phi)\\]</span></p>\n<h3 id=\"iterative-teaching-committeeitec\">Iterative teaching\ncommitteeiTeC</h3>\n<p>iterative RLHF</p>\n<p>AFMRLHFrefresh online human\npreference data collection using a diverse set of the best performing\nmodels</p>\n<p>SFTDPO/IPORLmodel\ncommitteemodel committee</p>\n<p>RMmodel\ncomittee</p>\n<p>online\nRLHFDPOIPOrejection\nsamplingmodel\ncomitteeRM</p>\n<h3 id=\"online-rlhf-algorithm-mdloo\">Online RLHF algorithm: MDLOO</h3>\n<p>RLHF</p>\n<p><span class=\"math display\">\\[\\max_\\theta\\mathbb{E}_{x\\sim\\mathcal{D},y\\sim\\pi_\\theta(\\cdot|x)}\\left[r_\\phi(x,y)-\\beta\nD_{\\mathrm{KL}}\\left(\\pi_\\theta(\\cdot|x)\\|\\pi_{\\mathrm{ref}}(\\cdot|x)\\right)\\right]\\]</span></p>\n<p>reward function</p>\n<p><span class=\"math display\">\\[R(x,y)=r_\\phi(x,y)-\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}\\]</span></p>\n<p>reward functionexpectation</p>\n<p>RLHFresponseactioncritictokenreward</p>\n<p></p>\n<p>1Leave-One-Out (LOO) estimator of the advantage</p>\n<p>kiterationpolicy\nmodelnpromptpromptK</p>\n<p>advantage</p>\n<p><span class=\"math display\">\\[A_k(x,y_i)=R(x,y_i)-\\mathbb{E}_{y\\sim\\pi_{\\theta_k}(\\cdot|x)}[R(x,y)]\\]</span></p>\n<p>leave-one-out (LOO) <span class=\"math inline\">\\(A_k(x,y_i)\\)</span>prompt\nxK-1response</p>\n<p><span class=\"math display\">\\[\\widehat{A}_k(x,y_i)=R(x,y_i)-\\frac{1}{K-1}\\sum_{j\\neq\ni}R(x,y_j)\\]</span></p>\n<p>Back to basics: Revisiting reinforce style optimization for\nlearning from human feedback in\nLLMsadvantageRLHF</p>\n<p>2Mirror descent policy optimization (MDPO)</p>\n<p>clipping-based PPOKL\ndivergenceregularization</p>\n<h1 id=\"apple-intelligence\">Apple Intelligence</h1>\n<p>AFMApple IntelligenceApple\nIntelligenceiPhoneiPadMac</p>\n<p>post-trainingadapteradapter</p>\n<p></p>\n<img src=\"/1e34e252/intelligence.png\" class title=\"afm\">\n<h2 id=\"accuracy-recovery-adapter\">accuracy-recovery adapter</h2>\n<p>1</p>\n<p>post-training4-bit</p>\n<p>16-bitLoRALoRAaccuracy-recovery\nadapter</p>\n<p>accuracy-recovery\nadapterpre-trainingpost-trainingMB10B</p>\n<p>rank\n16rankLoRA81632</p>\n<p>accuracy-recovery\nadapter</p>\n<img src=\"/1e34e252/recover.png\" class title=\"afm\">\n<p>rank\n16adapteradapteraccuracy-recovery\nadapter</p>\n<p>2Quantization schemes</p>\n<p>block\nsize3264accuracy-recovery\nadapterblock\nsize100k</p>\n<p>AFMembeddingsharedembedding8-bitper-channel\nquantization</p>\n<p>3</p>\n<p>2-bit3.5~3.7bpw</p>\n<h2 id=\"task-specific-adapter\">task-specific adapter</h2>\n<p>accuracy-recovery\nadapter4-bit</p>\n<p>summarizationemailmessagenotificationAFM-serveradapter</p>\n<h1 id=\"\"></h1>\n<ul>\n<li>pre-trainingpost-trainingRMSProp\nwith momentum<br>\n</li>\n<li>accuracy-recovery\nadapterfollow</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<ul>\n<li>MoE<br>\n\n<p style=\"line-height: 1.2;\">\n<small> <a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/a0824e29.html\">denseMoE -- sparse\nupcycling</a><br>\n<a href=\"https://www.linsight.cn/2c8bbc7.html\">MoE--expert choice\nrouting</a><br>\n</small>\n</p></li>\n<li><br>\n\n<p style=\"line-height: 1.2;\">\n<small> <a href=\"https://www.linsight.cn/7d7294cb.html\">Llama3.1--</a><br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n</small>\n</p></li>\n<li><br>\n\n<p style=\"line-height: 1.2;\">\n<small> <a href=\"https://www.linsight.cn/210dbccd.html\">--</a><br>\n</small>\n</p></li>\n<li><br>\n\n<p style=\"line-height: 1.2;\">\n<small> <a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n</small>\n</p></li>\n<li><br>\n\n<p style=\"line-height: 1.2;\">\n<small> <a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n</small>\n</p></li>\n<li><br>\n\n<p style=\"line-height: 1.2;\">\n<small> <a href=\"https://www.linsight.cn/93328a2a.html\">Llama3.1--post-training</a><br>\n<a href=\"https://www.linsight.cn/bb8fcf21.html\"> -- model\nsoup</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n</small>\n</p></li>\n<li>Transformer<br>\n\n<p style=\"line-height: 1.2;\">\n<small> <a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n</small>\n</p></li>\n<li><br>\n\n<p style=\"line-height: 1.2;\">\n<small> <a href=\"http://www.linsight.cn/3345028a.html\">(1)</a> <a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a> <a href=\"http://www.linsight.cn/1736008.html\">(3)</a> <a href=\"http://www.linsight.cn/1736008.html\">(4)</a> <a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a> <a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a> <a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a> <a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a><br>\n</small>\n</p></li>\n</ul>\n<h1 id=\"reference\">Reference</h1>\n<p>1Apple Intelligence Foundation Language Models\nhttps://machinelearning.apple.com/papers/apple_intelligence_foundation_language_models.pdf</p>\n","length":9757,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>WWDC24Apple\nIntelligenceApple Intelligence\nFoundation Language Models --\n3BAFM-on-deviceAFM-serverAFM=Apple\nFoundation ModelAFM-server</p>\n<h1 id=\"\"></h1>\n<p>OpenELM</p>\n<img src=\"/1e34e252/afm.png\" class title=\"afm\">\n<p><br>\n- embedding<br>\n- Small-scale proxies for large-scale transformer training\ninstabilitiesQuery/key normalization<br>\n- RoPEbase frequency500k</p>\n<p>tokenizerSentencePieceBPEAFM-server100kAFM-on-device49k</p>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<ul>\n<li>Applebot<br>\n</li>\n<li></li>\n</ul>\n<p>personally\nidentifiable informationPII</p>\n<p></p>\n<p>1</p>\n<p>pipeline<br>\n- Safarireader modeBoilerpipe<br>\n- +model based<br>\n- locality-sensitive n-gram hashing<br>\n- Large language model-guided document\nselectionDatacomp-lm: In search of the next generation of training\nsets for language models<br>\n-\nDecontaminationn-gram811benchmark</p>\n<p>2</p>\n<p></p>\n<p>3</p>\n<p>github14PIIDecontamination</p>\n<p>4</p>\n<p>3BQA14Btutorialseminarfilterquality\nfilterfilter</p>\n<p>5</p>\n<p></p>\n<h2 id=\"\"></h2>\n<p>AFM3stage<br>\n- core<br>\n- continuedcodemath<br>\n-\ncontext-lengtheningcontinued</p>\n<p>stageSmall-scale proxies for large-scale\ntransformer training instabilitiesParam\n(simple)</p>\n<h3 id=\"core-pre-training\">Core pre-training</h3>\n<p>AFM-server0AFM-on-device+</p>\n<p>1AFM-server</p>\n<ul>\n<li>6.3T<br>\n</li>\n<li>sequence length = 4096<br>\n</li>\n<li>batch size = 4096<br>\n</li>\n<li>weight decay = 3.16e-4<br>\n</li>\n<li>cosine lr schedule, max lr = 0.01, min lr = 0.5% max lr<br>\n</li>\n<li>warmup step = 5000</li>\n</ul>\n<p>batch sizescaling\nlawbatch\nsizebatch\nsizescaling\nlawbatch\nsize30724096</p>\n<p>proxy\nmodellrlr0.01~0.020.01Paramlr</p>\n<p>RMSProp with\nmomentumAdamW</p>\n<p>core\ntrainingbaseline<br>\n- AdamWbeta_1 = 0.9beta_2 = 0.95<br>\n- weight decay = 1e-4<br>\n- lr decay0.0001<br>\n- batch size = 1024</p>\n<p>AFM-on-device3.1T</p>\n<p></p>\n<img src=\"/1e34e252/core_ablation.png\" class title=\"afm\">\n<p>AFMcore\ntrainingbaseline</p>\n<p>2AFM-on-device</p>\n<p>AFM-on-device6.4BAFM-serverstructural\npruningdistillation</p>\n<p>structural pruningStructured pruning of large language\nmodelsSheared llama: Accelerating language model pre-training via\nstructured pruning<br>\n- FFNprune<br>\n- Soft-Top-K maskingConditional adapters: Parameter-efficient\ntransfer learning with fast inference<br>\n- core trainingdata mix188Bpruning mask</p>\n<p>coretarget\nlabel0.9 * teacher top-1 prediction + 0.1 * true label</p>\n<p>6.3T</p>\n<p>pruningdistillation</p>\n<img src=\"/1e34e252/distill.png\" class title=\"afm\">\n<p>prune + distill5training\ncostbaseline</p>\n<h3 id=\"continued-pre-training\">Continued pre-training</h3>\n<p>stagemathcode1T\ntoken</p>\n<p><br>\n- sequence length = 8192<br>\n- max lr = 3e-4min lr = 0.1% max lr<br>\n- weight decay = 1e-5<br>\n- warmup step = 1000</p>\n<p>core training</p>\n<p>AFM-on-deviceAFM-server</p>\n<p>3Context lengthening</p>\n<p>100B<br>\n- sequence length = 32768<br>\n- RoPE base frequency 500k --&gt; 6315089Scaling laws of rope-based\nextrapolation<br>\n- QA</p>\n<h3 id=\"\"></h3>\n<p>AFM-on-deviceAFM-serverinternalformulation</p>\n<img src=\"/1e34e252/pretrain_1.png\" class title=\"afm\">\n<img src=\"/1e34e252/pretrain_2.png\" class title=\"afm\">\n<p>continued\npre-trainingmathcode</p>\n<h1 id=\"post-training\">Post-Training</h1>\n<p>AFMpost-trainingSFTRLHFiTecMDLOO</p>\n<h2 id=\"-1\"></h2>\n<p>post-training</p>\n<h3 id=\"\"></h3>\n<p>reward modelprompt\nset</p>\n<p>3</p>\n<p>1Mathematics</p>\n<p>stage<br>\n- <br>\n- </p>\n<p>promptprompt<br>\n- Problem rephrase and reversionMetamath: Bootstrap your own\nmathematical questions for large language models<br>\n- Problem evolutionWizardLM: Empowering large\nlanguage models to follow complex\ninstructions</p>\n<p>2Tool use</p>\n<p>single-toolmulti-toolmulti-steporacle\ntool</p>\n<p>tool intent detection</p>\n<p>3Coding</p>\n<p>71self-instructrejection\nsampling</p>\n<p>solutionsolutionsolution</p>\n<p>12k</p>\n<h2 id=\"sft\">SFT</h2>\n<p>1</p>\n<p> + rejection\nsamplingSFT</p>\n<p>2</p>\n<p></p>\n<p>3</p>\n<p>constant\nlrAFM-serverAFM-on-devicelr5e62e5</p>\n<p>0.1dropout rate</p>\n<p>checkpointevalRMbest-of-Ncheckpoint</p>\n<h2 id=\"rlhf\">RLHF</h2>\n<p>RLHF</p>\n<h3 id=\"rm\">RM</h3>\n<p>RM<br>\n- promptresponseLlama-33<br>\n- significantly better, better, slightly better, negligibly\nbetter<br>\n-\nresponse3</p>\n<p>RMnon-padding\ntokenembeddinglinear4MLPheadlinear4MLP4\n<span class=\"math inline\">\\(u_\\phi^\\mathrm{if},u_\\phi^\\mathrm{verb},u_\\phi^\\mathrm{truth},u_\\phi^\\mathrm{harm}\\)</span></p>\n<p>RMsoft label loss\nfunctionregularization\ntermRM</p>\n<p>1Soft label loss</p>\n<p>Bradley-Terry\nmodely_cc=choseny_rr=rejected</p>\n<p><span class=\"math display\">\\[\\sigma(r_\\phi(x,y_c)-r_\\phi(x,y_r))\\]</span></p>\n<p>response</p>\n<p>ltarget preference\nprobability p_lsoft label loss</p>\n<p><span class=\"math display\">\\[\\begin{aligned}\nL_{\\mathrm{ranking}}(\\phi)=&amp;\n-p_\\ell\\log(\\sigma(r_\\phi(x,y_c)-r_\\phi(x,y_r))  \\\\\n&amp;-\\left(1-p_\\ell\\right)\\log(\\sigma(r_\\phi(x,y_r)-r_\\phi(x,y_c))\n\\end{aligned}\\]</span></p>\n<p>p_lp_l0.950.850.750.65</p>\n<p>2Single-sided grading as regularization</p>\n<p>regularization loss</p>\n<p><span class=\"math display\">\\[\\begin{aligned}L_{\\mathrm{regu}}(\\phi)&amp;=\\sum_{\\text{grade}\\in\\text{if,verb,truth,harm}}\\left(\\text{cross}_\\text{entropy}(u_\\phi^\\mathrm{grade}(x,y_c),z_c^\\mathrm{grade})\\right.\\\\&amp;+\\text{cross}_\\text{entropy}(u_\\phi^\\mathrm{grade}(x,y_r),z_r^\\mathrm{grade})\\Big)\\end{aligned}\\]</span></p>\n<p>z</p>\n<p>RMloss</p>\n<p><span class=\"math display\">\\[L_\\text{ranking}(\\phi)+\\lambda\nL_\\text{regu}(\\phi)\\]</span></p>\n<h3 id=\"iterative-teaching-committeeitec\">Iterative teaching\ncommitteeiTeC</h3>\n<p>iterative RLHF</p>\n<p>AFMRLHFrefresh online human\npreference data collection using a diverse set of the best performing\nmodels</p>\n<p>SFTDPO/IPORLmodel\ncommitteemodel committee</p>\n<p>RMmodel\ncomittee</p>\n<p>online\nRLHFDPOIPOrejection\nsamplingmodel\ncomitteeRM</p>\n<h3 id=\"online-rlhf-algorithm-mdloo\">Online RLHF algorithm: MDLOO</h3>\n<p>RLHF</p>\n<p><span class=\"math display\">\\[\\max_\\theta\\mathbb{E}_{x\\sim\\mathcal{D},y\\sim\\pi_\\theta(\\cdot|x)}\\left[r_\\phi(x,y)-\\beta\nD_{\\mathrm{KL}}\\left(\\pi_\\theta(\\cdot|x)\\|\\pi_{\\mathrm{ref}}(\\cdot|x)\\right)\\right]\\]</span></p>\n<p>reward function</p>\n<p><span class=\"math display\">\\[R(x,y)=r_\\phi(x,y)-\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}\\]</span></p>\n<p>reward functionexpectation</p>\n<p>RLHFresponseactioncritictokenreward</p>\n<p></p>\n<p>1Leave-One-Out (LOO) estimator of the advantage</p>\n<p>kiterationpolicy\nmodelnpromptpromptK</p>\n<p>advantage</p>\n<p><span class=\"math display\">\\[A_k(x,y_i)=R(x,y_i)-\\mathbb{E}_{y\\sim\\pi_{\\theta_k}(\\cdot|x)}[R(x,y)]\\]</span></p>\n<p>leave-one-out (LOO) <span class=\"math inline\">\\(A_k(x,y_i)\\)</span>prompt\nxK-1response</p>\n<p><span class=\"math display\">\\[\\widehat{A}_k(x,y_i)=R(x,y_i)-\\frac{1}{K-1}\\sum_{j\\neq\ni}R(x,y_j)\\]</span></p>\n<p>Back to basics: Revisiting reinforce style optimization for\nlearning from human feedback in\nLLMsadvantageRLHF</p>\n<p>2Mirror descent policy optimization (MDPO)</p>\n<p>clipping-based PPOKL\ndivergenceregularization</p>\n<h1 id=\"apple-intelligence\">Apple Intelligence</h1>\n<p>AFMApple IntelligenceApple\nIntelligenceiPhoneiPadMac</p>\n<p>post-trainingadapteradapter</p>\n<p></p>\n<img src=\"/1e34e252/intelligence.png\" class title=\"afm\">\n<h2 id=\"accuracy-recovery-adapter\">accuracy-recovery adapter</h2>\n<p>1</p>\n<p>post-training4-bit</p>\n<p>16-bitLoRALoRAaccuracy-recovery\nadapter</p>\n<p>accuracy-recovery\nadapterpre-trainingpost-trainingMB10B</p>\n<p>rank\n16rankLoRA81632</p>\n<p>accuracy-recovery\nadapter</p>\n<img src=\"/1e34e252/recover.png\" class title=\"afm\">\n<p>rank\n16adapteradapteraccuracy-recovery\nadapter</p>\n<p>2Quantization schemes</p>\n<p>block\nsize3264accuracy-recovery\nadapterblock\nsize100k</p>\n<p>AFMembeddingsharedembedding8-bitper-channel\nquantization</p>\n<p>3</p>\n<p>2-bit3.5~3.7bpw</p>\n<h2 id=\"task-specific-adapter\">task-specific adapter</h2>\n<p>accuracy-recovery\nadapter4-bit</p>\n<p>summarizationemailmessagenotificationAFM-serveradapter</p>\n<h1 id=\"\"></h1>\n<ul>\n<li>pre-trainingpost-trainingRMSProp\nwith momentum<br>\n</li>\n<li>accuracy-recovery\nadapterfollow</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p></p>\n<ul>\n<li>MoE<br>\n\n<p style=\"line-height: 1.2;\">\n<small> <a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/a0824e29.html\">denseMoE -- sparse\nupcycling</a><br>\n<a href=\"https://www.linsight.cn/2c8bbc7.html\">MoE--expert choice\nrouting</a><br>\n</small>\n</p></li>\n<li><br>\n\n<p style=\"line-height: 1.2;\">\n<small> <a href=\"https://www.linsight.cn/7d7294cb.html\">Llama3.1--</a><br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n</small>\n</p></li>\n<li><br>\n\n<p style=\"line-height: 1.2;\">\n<small> <a href=\"https://www.linsight.cn/210dbccd.html\">--</a><br>\n</small>\n</p></li>\n<li><br>\n\n<p style=\"line-height: 1.2;\">\n<small> <a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n</small>\n</p></li>\n<li><br>\n\n<p style=\"line-height: 1.2;\">\n<small> <a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n</small>\n</p></li>\n<li><br>\n\n<p style=\"line-height: 1.2;\">\n<small> <a href=\"https://www.linsight.cn/93328a2a.html\">Llama3.1--post-training</a><br>\n<a href=\"https://www.linsight.cn/bb8fcf21.html\"> -- model\nsoup</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n</small>\n</p></li>\n<li>Transformer<br>\n\n<p style=\"line-height: 1.2;\">\n<small> <a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n</small>\n</p></li>\n<li><br>\n\n<p style=\"line-height: 1.2;\">\n<small> <a href=\"http://www.linsight.cn/3345028a.html\">(1)</a> <a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a> <a href=\"http://www.linsight.cn/1736008.html\">(3)</a> <a href=\"http://www.linsight.cn/1736008.html\">(4)</a> <a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a> <a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a> <a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a> <a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a><br>\n</small>\n</p></li>\n</ul>\n<h1 id=\"reference\">Reference</h1>\n<p>1Apple Intelligence Foundation Language Models\nhttps://machinelearning.apple.com/papers/apple_intelligence_foundation_language_models.pdf</p>\n"},{"title":"--","abbrlink":"210dbccd","date":"2024-07-23T13:23:22.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nLLM90%  \n\nDataset Decomposition  \n\n#   \n\n## concat-and-chunk  \n\ntokentokenconcat-and-chunk  \n\n  \n- next token prediction  \n- attention  \n-   \n\nconcat-and-chunk document masking (DM) best-fit sequence packingin-context pretraining  \n\n## Variable Sequence Length  \n\nDataset DecompositionDD  \n\n  \n- bucketbucketsequence  \n- bucketsequencesubsequence  \n- buckettoken  \n\nl $l=2^{i_1}+2^{i_2}+\\ldots+2^{i_k}$ ksubsequence2Variable Sequence LengthVSL  \n\n200128 + 64 + 8bucketD_i  \n\n{% asset_img buckets.png  %}  \n\nbuckettokenb512token2^ibucket$2^i\\leq l<2^{i+1}$   \n\n{% asset_img dist.png  %}  \n\ncconcat-and-chunkVSLcontext length  \n\n#   \n\nVSLbatchtokenbbucketbucket isample = b / 2^i  \n\ntokenlearning rate  \n\nOpenLM  \n\n{% asset_img model.png  %}  \n\nRoPEbase frequency10,000100,000base frequency100k  \n\n{% asset_img base_freq.png base frequence %}  \n\nregular language modeling benchmarkslong context task  \n\nregular  \n- Commonsense Reasoning (CSR): PIQA-0-shotCOPA-0-shotOpenBookQA-10-shots  \n- Language Understanding (LU): Lambada-OpenAIHellaswag-0-shotWinograd-3-shotsWinoGrande-5-shots  \n- Reading Comprehension (RC): SQuAD-3-shotsBoolQ-0-shotCoQA-0-shot  \n- World Knowledge (WK): Jeopardy-3-shotsArcEasy-3-shotsArcChallenge-3-shotsWikiDataQA-3-shots  \n\nlong context  \n- Multi-Document Question Answering (MDQA)NaturalQuestions-OpenWikipedia  \n- TOEFL  \n- QuALITY  \n\n##   \n\nbatch size = 8 * 8192OpenLM-1B/3B/7B $2^6$  $2^13$   \n\n  \n\n{% asset_img efficiency.png  %}  \n\ntraining overhead  \n\nconcat-and-chunkVSL  \n\n8kVSL20%  \n\n## Sequence length bias  \n\n  \n\nOpenLM-1Ba  \n\n{% asset_img bias.png Sequence length bias %}  \n\nreasoning, language understandingworld knowledgeU  \n\nb  \n\nregular  \n\n$\\mathcal{D}_{13\\to10}$  $\\mathcal{D}_{7\\to10}$8k81k81281kc  \n\n $\\mathcal{D}_{13\\to10}$  $\\mathcal{D}_{13}$ 2.6  \n\n $\\mathcal{D}_{13\\to10}$  $\\mathcal{D}_{10}$   \n\n$\\mathcal{D}_{7\\to10}$  $\\mathcal{D}_{7}$   \n\n## Data mixture  \n\n7token  \n\n{% asset_img mixture.png Data mixture %}  \n\nmixtureMDQAMDQA1k-onlyregular  \n\nNaturalregularMDQAVSL  \n\n## Length-based curriculum  \n\n  \n\ncurriculum learning  \n\n{% asset_img curriculum.png Length-based curriculum %}  \n\nlrepoch  \n\nGrow-P2 curriculum  \n\ncurriculum learningThe stability-efficiency dilemma: Investigating sequence length warmup for training gpt models  \n\n## Scaling  \n\naVSLregular  \n\n{% asset_img scaling.png Scaling %}  \n\n  \n\nbVSL  \n\n## sota  \n\nVSLdocument masking (DM) best-fit sequence packingin-context pretraining  \n\n{% asset_img sota.png sota %}  \n\nDMbaseline-8kregularEffective long-context scaling of foundation modelsDM  \n\nbest-fit sequence packingregular  \n\nin-context pretraining  \n\nVSL  \n\n#   \n\n-   \n-   \n- DeepSeek-V2LLAMA3.1DCLMcurriculum learning  \n\n***  \n\n~  \n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n- MoE  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[denseMoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  \n[MoE--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  \n-   \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n-   \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n-   \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n-   \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n- Transformer  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n-   \n[(1)](http://www.linsight.cn/3345028a.html)\n[(2)](http://www.linsight.cn/ad0bba9d.html)\n[(3)](http://www.linsight.cn/1736008.html)\n[(4)](http://www.linsight.cn/1736008.html)\n[(5)](http://www.linsight.cn/336f2f3e.html)\n[(6)](http://www.linsight.cn/7c04944d.html)\n[(7)](https://www.linsight.cn/dd614e12.html)\n[(8)](https://www.linsight.cn/e287b9c3.html)  \n\n\n# Reference  \n\n1Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum https://arxiv.org/abs/2405.13226  \n","source":"_posts/cs/nlp/2024/07/-.md","raw":"---\ntitle: --\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 210dbccd\ndate: 2024-07-23 21:23:22\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nLLM90%  \n\nDataset Decomposition  \n\n#   \n\n## concat-and-chunk  \n\ntokentokenconcat-and-chunk  \n\n  \n- next token prediction  \n- attention  \n-   \n\nconcat-and-chunk document masking (DM) best-fit sequence packingin-context pretraining  \n\n## Variable Sequence Length  \n\nDataset DecompositionDD  \n\n  \n- bucketbucketsequence  \n- bucketsequencesubsequence  \n- buckettoken  \n\nl $l=2^{i_1}+2^{i_2}+\\ldots+2^{i_k}$ ksubsequence2Variable Sequence LengthVSL  \n\n200128 + 64 + 8bucketD_i  \n\n{% asset_img buckets.png  %}  \n\nbuckettokenb512token2^ibucket$2^i\\leq l<2^{i+1}$   \n\n{% asset_img dist.png  %}  \n\ncconcat-and-chunkVSLcontext length  \n\n#   \n\nVSLbatchtokenbbucketbucket isample = b / 2^i  \n\ntokenlearning rate  \n\nOpenLM  \n\n{% asset_img model.png  %}  \n\nRoPEbase frequency10,000100,000base frequency100k  \n\n{% asset_img base_freq.png base frequence %}  \n\nregular language modeling benchmarkslong context task  \n\nregular  \n- Commonsense Reasoning (CSR): PIQA-0-shotCOPA-0-shotOpenBookQA-10-shots  \n- Language Understanding (LU): Lambada-OpenAIHellaswag-0-shotWinograd-3-shotsWinoGrande-5-shots  \n- Reading Comprehension (RC): SQuAD-3-shotsBoolQ-0-shotCoQA-0-shot  \n- World Knowledge (WK): Jeopardy-3-shotsArcEasy-3-shotsArcChallenge-3-shotsWikiDataQA-3-shots  \n\nlong context  \n- Multi-Document Question Answering (MDQA)NaturalQuestions-OpenWikipedia  \n- TOEFL  \n- QuALITY  \n\n##   \n\nbatch size = 8 * 8192OpenLM-1B/3B/7B $2^6$  $2^13$   \n\n  \n\n{% asset_img efficiency.png  %}  \n\ntraining overhead  \n\nconcat-and-chunkVSL  \n\n8kVSL20%  \n\n## Sequence length bias  \n\n  \n\nOpenLM-1Ba  \n\n{% asset_img bias.png Sequence length bias %}  \n\nreasoning, language understandingworld knowledgeU  \n\nb  \n\nregular  \n\n$\\mathcal{D}_{13\\to10}$  $\\mathcal{D}_{7\\to10}$8k81k81281kc  \n\n $\\mathcal{D}_{13\\to10}$  $\\mathcal{D}_{13}$ 2.6  \n\n $\\mathcal{D}_{13\\to10}$  $\\mathcal{D}_{10}$   \n\n$\\mathcal{D}_{7\\to10}$  $\\mathcal{D}_{7}$   \n\n## Data mixture  \n\n7token  \n\n{% asset_img mixture.png Data mixture %}  \n\nmixtureMDQAMDQA1k-onlyregular  \n\nNaturalregularMDQAVSL  \n\n## Length-based curriculum  \n\n  \n\ncurriculum learning  \n\n{% asset_img curriculum.png Length-based curriculum %}  \n\nlrepoch  \n\nGrow-P2 curriculum  \n\ncurriculum learningThe stability-efficiency dilemma: Investigating sequence length warmup for training gpt models  \n\n## Scaling  \n\naVSLregular  \n\n{% asset_img scaling.png Scaling %}  \n\n  \n\nbVSL  \n\n## sota  \n\nVSLdocument masking (DM) best-fit sequence packingin-context pretraining  \n\n{% asset_img sota.png sota %}  \n\nDMbaseline-8kregularEffective long-context scaling of foundation modelsDM  \n\nbest-fit sequence packingregular  \n\nin-context pretraining  \n\nVSL  \n\n#   \n\n-   \n-   \n- DeepSeek-V2LLAMA3.1DCLMcurriculum learning  \n\n***  \n\n~  \n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n- MoE  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[denseMoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  \n[MoE--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  \n-   \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n-   \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n-   \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n-   \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n- Transformer  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n-   \n[(1)](http://www.linsight.cn/3345028a.html)\n[(2)](http://www.linsight.cn/ad0bba9d.html)\n[(3)](http://www.linsight.cn/1736008.html)\n[(4)](http://www.linsight.cn/1736008.html)\n[(5)](http://www.linsight.cn/336f2f3e.html)\n[(6)](http://www.linsight.cn/7c04944d.html)\n[(7)](https://www.linsight.cn/dd614e12.html)\n[(8)](https://www.linsight.cn/e287b9c3.html)  \n\n\n# Reference  \n\n1Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum https://arxiv.org/abs/2405.13226  \n","slug":"cs/nlp/2024/07/-","published":1,"updated":"2024-07-24T14:20:20.308Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsns00gh0p4k0itoh1bc","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>LLM90%</p>\n<p>Dataset\nDecomposition</p>\n<h1 id=\"\"></h1>\n<h2 id=\"concat-and-chunk\">concat-and-chunk</h2>\n<p>tokentokenconcat-and-chunk</p>\n<p><br>\n-\nnext\ntoken prediction<br>\n-\nattention<br>\n-\n</p>\n<p>concat-and-chunk document masking\n(DM) best-fit sequence packingin-context\npretraining</p>\n<h2 id=\"variable-sequence-length\">Variable Sequence Length</h2>\n<p>Dataset DecompositionDD</p>\n<p><br>\n-\nbucketbucketsequence<br>\n- bucketsequencesubsequence<br>\n- buckettoken</p>\n<p>l\n<span class=\"math inline\">\\(l=2^{i_1}+2^{i_2}+\\ldots+2^{i_k}\\)</span>\nksubsequence2Variable Sequence\nLengthVSL</p>\n<p>200128 + 64 +\n8bucketD_i</p>\n<img src=\"/210dbccd/buckets.png\" class title=\"\">\n<p>buckettokenb512token2^ibucket<span class=\"math inline\">\\(2^i\\leq l&lt;2^{i+1}\\)</span> </p>\n<img src=\"/210dbccd/dist.png\" class title=\"\">\n<p>cconcat-and-chunkVSLcontext\nlength</p>\n<h1 id=\"\"></h1>\n<p>VSLbatchtokenbbucketbucket\nisample = b /\n2^i</p>\n<p>tokenlearning\nrate</p>\n<p>OpenLM</p>\n<img src=\"/210dbccd/model.png\" class title=\"\">\n<p>RoPEbase frequency10,000100,000base\nfrequency100k</p>\n<img src=\"/210dbccd/base_freq.png\" class title=\"base frequence\">\n<p>regular language modeling benchmarkslong\ncontext task</p>\n<p>regular<br>\n- Commonsense Reasoning (CSR):\nPIQA-0-shotCOPA-0-shotOpenBookQA-10-shots<br>\n- Language Understanding (LU):\nLambada-OpenAIHellaswag-0-shotWinograd-3-shotsWinoGrande-5-shots<br>\n- Reading Comprehension (RC):\nSQuAD-3-shotsBoolQ-0-shotCoQA-0-shot<br>\n- World Knowledge (WK):\nJeopardy-3-shotsArcEasy-3-shotsArcChallenge-3-shotsWikiDataQA-3-shots</p>\n<p>long context<br>\n- Multi-Document Question Answering\n(MDQA)NaturalQuestions-OpenWikipedia<br>\n- TOEFL<br>\n- QuALITY</p>\n<h2 id=\"\"></h2>\n<p>batch size = 8 * 8192OpenLM-1B/3B/7B <span class=\"math inline\">\\(2^6\\)</span>  <span class=\"math inline\">\\(2^13\\)</span> </p>\n<p></p>\n<img src=\"/210dbccd/efficiency.png\" class title=\"\">\n<p>training overhead</p>\n<p>concat-and-chunkVSL</p>\n<p>8kVSL20%</p>\n<h2 id=\"sequence-length-bias\">Sequence length bias</h2>\n<p></p>\n<p>OpenLM-1Ba</p>\n<img src=\"/210dbccd/bias.png\" class title=\"Sequence length bias\">\n<p>reasoning, language understandingworld\nknowledgeU</p>\n<p>b</p>\n<p>regular</p>\n<p><span class=\"math inline\">\\(\\mathcal{D}_{13\\to10}\\)</span>  <span class=\"math inline\">\\(\\mathcal{D}_{7\\to10}\\)</span>8k81k81281kc</p>\n<p> <span class=\"math inline\">\\(\\mathcal{D}_{13\\to10}\\)</span> \n<span class=\"math inline\">\\(\\mathcal{D}_{13}\\)</span>\n2.6</p>\n<p> <span class=\"math inline\">\\(\\mathcal{D}_{13\\to10}\\)</span> \n<span class=\"math inline\">\\(\\mathcal{D}_{10}\\)</span>\n</p>\n<p><span class=\"math inline\">\\(\\mathcal{D}_{7\\to10}\\)</span> \n<span class=\"math inline\">\\(\\mathcal{D}_{7}\\)</span>\n</p>\n<h2 id=\"data-mixture\">Data mixture</h2>\n<p>7token</p>\n<img src=\"/210dbccd/mixture.png\" class title=\"Data mixture\">\n<p>mixtureMDQAMDQA1k-onlyregular</p>\n<p>NaturalregularMDQAVSL</p>\n<h2 id=\"length-based-curriculum\">Length-based curriculum</h2>\n<p></p>\n<p>curriculum\nlearning</p>\n<img src=\"/210dbccd/curriculum.png\" class title=\"Length-based curriculum\">\n<p>lrepoch</p>\n<p>Grow-P2 curriculum</p>\n<p>curriculum learningThe\nstability-efficiency dilemma: Investigating sequence length warmup for\ntraining gpt\nmodels</p>\n<h2 id=\"scaling\">Scaling</h2>\n<p>aVSLregular</p>\n<img src=\"/210dbccd/scaling.png\" class title=\"Scaling\">\n<p></p>\n<p>bVSL</p>\n<h2 id=\"sota\">sota</h2>\n<p>VSLdocument masking (DM) best-fit sequence packingin-context\npretraining</p>\n<img src=\"/210dbccd/sota.png\" class title=\"sota\">\n<p>DMbaseline-8kregularEffective\nlong-context scaling of foundation\nmodelsDM</p>\n<p>best-fit sequence\npackingregular</p>\n<p>in-context\npretraining</p>\n<p>VSL</p>\n<h1 id=\"\"></h1>\n<ul>\n<li><br>\n</li>\n<li><br>\n</li>\n<li>DeepSeek-V2LLAMA3.1DCLMcurriculum\nlearning</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n- MoE<br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/a0824e29.html\">denseMoE -- sparse\nupcycling</a><br>\n<a href=\"https://www.linsight.cn/2c8bbc7.html\">MoE--expert choice\nrouting</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n- Transformer<br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a> <a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a> <a href=\"http://www.linsight.cn/1736008.html\">(3)</a> <a href=\"http://www.linsight.cn/1736008.html\">(4)</a> <a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a> <a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a> <a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a> <a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Dataset Decomposition: Faster LLM Training with Variable\nSequence Length Curriculum https://arxiv.org/abs/2405.13226</p>\n","length":4908,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>LLM90%</p>\n<p>Dataset\nDecomposition</p>\n<h1 id=\"\"></h1>\n<h2 id=\"concat-and-chunk\">concat-and-chunk</h2>\n<p>tokentokenconcat-and-chunk</p>\n<p><br>\n-\nnext\ntoken prediction<br>\n-\nattention<br>\n-\n</p>\n<p>concat-and-chunk document masking\n(DM) best-fit sequence packingin-context\npretraining</p>\n<h2 id=\"variable-sequence-length\">Variable Sequence Length</h2>\n<p>Dataset DecompositionDD</p>\n<p><br>\n-\nbucketbucketsequence<br>\n- bucketsequencesubsequence<br>\n- buckettoken</p>\n<p>l\n<span class=\"math inline\">\\(l=2^{i_1}+2^{i_2}+\\ldots+2^{i_k}\\)</span>\nksubsequence2Variable Sequence\nLengthVSL</p>\n<p>200128 + 64 +\n8bucketD_i</p>\n<img src=\"/210dbccd/buckets.png\" class title=\"\">\n<p>buckettokenb512token2^ibucket<span class=\"math inline\">\\(2^i\\leq l&lt;2^{i+1}\\)</span> </p>\n<img src=\"/210dbccd/dist.png\" class title=\"\">\n<p>cconcat-and-chunkVSLcontext\nlength</p>\n<h1 id=\"\"></h1>\n<p>VSLbatchtokenbbucketbucket\nisample = b /\n2^i</p>\n<p>tokenlearning\nrate</p>\n<p>OpenLM</p>\n<img src=\"/210dbccd/model.png\" class title=\"\">\n<p>RoPEbase frequency10,000100,000base\nfrequency100k</p>\n<img src=\"/210dbccd/base_freq.png\" class title=\"base frequence\">\n<p>regular language modeling benchmarkslong\ncontext task</p>\n<p>regular<br>\n- Commonsense Reasoning (CSR):\nPIQA-0-shotCOPA-0-shotOpenBookQA-10-shots<br>\n- Language Understanding (LU):\nLambada-OpenAIHellaswag-0-shotWinograd-3-shotsWinoGrande-5-shots<br>\n- Reading Comprehension (RC):\nSQuAD-3-shotsBoolQ-0-shotCoQA-0-shot<br>\n- World Knowledge (WK):\nJeopardy-3-shotsArcEasy-3-shotsArcChallenge-3-shotsWikiDataQA-3-shots</p>\n<p>long context<br>\n- Multi-Document Question Answering\n(MDQA)NaturalQuestions-OpenWikipedia<br>\n- TOEFL<br>\n- QuALITY</p>\n<h2 id=\"\"></h2>\n<p>batch size = 8 * 8192OpenLM-1B/3B/7B <span class=\"math inline\">\\(2^6\\)</span>  <span class=\"math inline\">\\(2^13\\)</span> </p>\n<p></p>\n<img src=\"/210dbccd/efficiency.png\" class title=\"\">\n<p>training overhead</p>\n<p>concat-and-chunkVSL</p>\n<p>8kVSL20%</p>\n<h2 id=\"sequence-length-bias\">Sequence length bias</h2>\n<p></p>\n<p>OpenLM-1Ba</p>\n<img src=\"/210dbccd/bias.png\" class title=\"Sequence length bias\">\n<p>reasoning, language understandingworld\nknowledgeU</p>\n<p>b</p>\n<p>regular</p>\n<p><span class=\"math inline\">\\(\\mathcal{D}_{13\\to10}\\)</span>  <span class=\"math inline\">\\(\\mathcal{D}_{7\\to10}\\)</span>8k81k81281kc</p>\n<p> <span class=\"math inline\">\\(\\mathcal{D}_{13\\to10}\\)</span> \n<span class=\"math inline\">\\(\\mathcal{D}_{13}\\)</span>\n2.6</p>\n<p> <span class=\"math inline\">\\(\\mathcal{D}_{13\\to10}\\)</span> \n<span class=\"math inline\">\\(\\mathcal{D}_{10}\\)</span>\n</p>\n<p><span class=\"math inline\">\\(\\mathcal{D}_{7\\to10}\\)</span> \n<span class=\"math inline\">\\(\\mathcal{D}_{7}\\)</span>\n</p>\n<h2 id=\"data-mixture\">Data mixture</h2>\n<p>7token</p>\n<img src=\"/210dbccd/mixture.png\" class title=\"Data mixture\">\n<p>mixtureMDQAMDQA1k-onlyregular</p>\n<p>NaturalregularMDQAVSL</p>\n<h2 id=\"length-based-curriculum\">Length-based curriculum</h2>\n<p></p>\n<p>curriculum\nlearning</p>\n<img src=\"/210dbccd/curriculum.png\" class title=\"Length-based curriculum\">\n<p>lrepoch</p>\n<p>Grow-P2 curriculum</p>\n<p>curriculum learningThe\nstability-efficiency dilemma: Investigating sequence length warmup for\ntraining gpt\nmodels</p>\n<h2 id=\"scaling\">Scaling</h2>\n<p>aVSLregular</p>\n<img src=\"/210dbccd/scaling.png\" class title=\"Scaling\">\n<p></p>\n<p>bVSL</p>\n<h2 id=\"sota\">sota</h2>\n<p>VSLdocument masking (DM) best-fit sequence packingin-context\npretraining</p>\n<img src=\"/210dbccd/sota.png\" class title=\"sota\">\n<p>DMbaseline-8kregularEffective\nlong-context scaling of foundation\nmodelsDM</p>\n<p>best-fit sequence\npackingregular</p>\n<p>in-context\npretraining</p>\n<p>VSL</p>\n<h1 id=\"\"></h1>\n<ul>\n<li><br>\n</li>\n<li><br>\n</li>\n<li>DeepSeek-V2LLAMA3.1DCLMcurriculum\nlearning</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n- MoE<br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/a0824e29.html\">denseMoE -- sparse\nupcycling</a><br>\n<a href=\"https://www.linsight.cn/2c8bbc7.html\">MoE--expert choice\nrouting</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n- Transformer<br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a> <a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a> <a href=\"http://www.linsight.cn/1736008.html\">(3)</a> <a href=\"http://www.linsight.cn/1736008.html\">(4)</a> <a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a> <a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a> <a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a> <a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Dataset Decomposition: Faster LLM Training with Variable\nSequence Length Curriculum https://arxiv.org/abs/2405.13226</p>\n"},{"title":"OpenELM","abbrlink":"f845f3e4","date":"2024-07-02T09:14:36.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nOpenELM0.27B0.45B1.08B3.04Bcheckpointtraining loghttps://github.com/apple/corenet  \n\nOpenELM-1BOLMo  \n\n{% asset_img intro.png OpenELM %}  \n\nOpenELM  \n\n#   \n\nOpenELM  \n- linearbias  \n- pre-norm + RMSNorm  \n- RoPE  \n- GQA  \n- SwiGLU FFN  \n- LLAMAtokenizer  \n\nDelight: Deep and light-weight transformerOpenELMlayer-wise scaling  \n\nhidden sizescaling  \n\nN $d_{model}$MHA $n_{h}$  $d_h=\\frac{d_{model}}{n_h}$FFN $d_{\\mathrm{FFN}}=m\\cdot d_{model}$mFFN multiplier  \n\nlayer-wise scaling $\\alpha$  $\\beta$ attention head $n_{h}$ FFN multiplier m  \n\ni  \n\n{% asset_img formula.png  %}  \n\n $\\alpha_{min}$  $\\alpha_{max}$ $\\beta_{min}$  $\\beta_{max}$  $\\alpha_{min}=0.5$$\\alpha_{max}=1.0$$\\beta_{min}=0.5$$\\beta_{max}=4.0$  \n\n  \n\n#   \n\n  \n- RefinedWeb  \n- deduplicated PILE  \n- a subset of RedPajama  \n- a subset of Dolma v1.6  \n\n1.8T tokentoken  \n\n{% asset_img data.png  %}  \n\n200 character256 token  \n\n#   \n\n  \n- 350k step  \n- AdamW optimizer  \n- cosine learning rate schedulewarmup=5k  \n- weight decay = 0.1  \n- gradient clipping = 1.0  \n\n  \n\n{% asset_img pretrain_hp.png  %}  \n\n  \n\n{% asset_img sft_hp.png  %}  \n\n# evaluation  \n\nOpenELM  \n- Standard zero-shot tasks  \n- OpenLLM leaderboard tasks  \n- LLM360 leaderboard tasks  \n\n{% asset_img eval_1.png  %}  \n\npretrained modelStandard zero-shot tasks7checkpoint  \n\n{% asset_img eval_2.png  %}  \n\n  \n\n55000 stepcheckpointcheckpointcheckpointnoise  \n\n  \n\nOpenELM  \n\n{% asset_img eval_3.png  %}  \n\nOpenELM  \n\n{% asset_img sft_result.png  %}  \n\nOpenELMLoRADoRA  \n\n{% asset_img peft_eval.png  %}  \n\n#   \n\nOpenELMlayer-wise scaling  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1OpenELM: An Efficient Language Model Family with Open Training and Inference Framework https://arxiv.org/abs/2404.14619  \n","source":"_posts/cs/nlp/2024/07/OpenELM.md","raw":"---\ntitle: OpenELM\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: f845f3e4\ndate: 2024-07-02 17:14:36\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nOpenELM0.27B0.45B1.08B3.04Bcheckpointtraining loghttps://github.com/apple/corenet  \n\nOpenELM-1BOLMo  \n\n{% asset_img intro.png OpenELM %}  \n\nOpenELM  \n\n#   \n\nOpenELM  \n- linearbias  \n- pre-norm + RMSNorm  \n- RoPE  \n- GQA  \n- SwiGLU FFN  \n- LLAMAtokenizer  \n\nDelight: Deep and light-weight transformerOpenELMlayer-wise scaling  \n\nhidden sizescaling  \n\nN $d_{model}$MHA $n_{h}$  $d_h=\\frac{d_{model}}{n_h}$FFN $d_{\\mathrm{FFN}}=m\\cdot d_{model}$mFFN multiplier  \n\nlayer-wise scaling $\\alpha$  $\\beta$ attention head $n_{h}$ FFN multiplier m  \n\ni  \n\n{% asset_img formula.png  %}  \n\n $\\alpha_{min}$  $\\alpha_{max}$ $\\beta_{min}$  $\\beta_{max}$  $\\alpha_{min}=0.5$$\\alpha_{max}=1.0$$\\beta_{min}=0.5$$\\beta_{max}=4.0$  \n\n  \n\n#   \n\n  \n- RefinedWeb  \n- deduplicated PILE  \n- a subset of RedPajama  \n- a subset of Dolma v1.6  \n\n1.8T tokentoken  \n\n{% asset_img data.png  %}  \n\n200 character256 token  \n\n#   \n\n  \n- 350k step  \n- AdamW optimizer  \n- cosine learning rate schedulewarmup=5k  \n- weight decay = 0.1  \n- gradient clipping = 1.0  \n\n  \n\n{% asset_img pretrain_hp.png  %}  \n\n  \n\n{% asset_img sft_hp.png  %}  \n\n# evaluation  \n\nOpenELM  \n- Standard zero-shot tasks  \n- OpenLLM leaderboard tasks  \n- LLM360 leaderboard tasks  \n\n{% asset_img eval_1.png  %}  \n\npretrained modelStandard zero-shot tasks7checkpoint  \n\n{% asset_img eval_2.png  %}  \n\n  \n\n55000 stepcheckpointcheckpointcheckpointnoise  \n\n  \n\nOpenELM  \n\n{% asset_img eval_3.png  %}  \n\nOpenELM  \n\n{% asset_img sft_result.png  %}  \n\nOpenELMLoRADoRA  \n\n{% asset_img peft_eval.png  %}  \n\n#   \n\nOpenELMlayer-wise scaling  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n[(1)](http://www.linsight.cn/3345028a.html)  \n[(2)](http://www.linsight.cn/ad0bba9d.html)  \n[(3)](http://www.linsight.cn/1736008.html)  \n[(4)](http://www.linsight.cn/1736008.html)  \n[(5)](http://www.linsight.cn/336f2f3e.html)  \n[(6)](http://www.linsight.cn/7c04944d.html)  \n[(7)](https://www.linsight.cn/dd614e12.html)  \n\n***  \n\n# Reference  \n\n1OpenELM: An Efficient Language Model Family with Open Training and Inference Framework https://arxiv.org/abs/2404.14619  \n","slug":"cs/nlp/2024/07/OpenELM","published":1,"updated":"2024-07-03T11:40:00.701Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsns00gi0p4k5muigr4a","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>OpenELM0.27B0.45B1.08B3.04Bcheckpointtraining\nloghttps://github.com/apple/corenet</p>\n<p>OpenELM-1BOLMo</p>\n<img src=\"/f845f3e4/intro.png\" class title=\"OpenELM\">\n<p>OpenELM</p>\n<h1 id=\"\"></h1>\n<p>OpenELM<br>\n- linearbias<br>\n- pre-norm + RMSNorm<br>\n- RoPE<br>\n- GQA<br>\n- SwiGLU FFN<br>\n- LLAMAtokenizer</p>\n<p>Delight:\nDeep and light-weight transformerOpenELMlayer-wise\nscaling</p>\n<p>hidden\nsizescaling</p>\n<p>N <span class=\"math inline\">\\(d_{model}\\)</span>MHA <span class=\"math inline\">\\(n_{h}\\)</span>  <span class=\"math inline\">\\(d_h=\\frac{d_{model}}{n_h}\\)</span>FFN\n<span class=\"math inline\">\\(d_{\\mathrm{FFN}}=m\\cdot\nd_{model}\\)</span>mFFN multiplier</p>\n<p>layer-wise scaling <span class=\"math inline\">\\(\\alpha\\)</span> \n<span class=\"math inline\">\\(\\beta\\)</span>\nattention head <span class=\"math inline\">\\(n_{h}\\)</span> FFN multiplier m</p>\n<p>i</p>\n<img src=\"/f845f3e4/formula.png\" class title=\"\">\n<p> <span class=\"math inline\">\\(\\alpha_{min}\\)</span>  <span class=\"math inline\">\\(\\alpha_{max}\\)</span>\n<span class=\"math inline\">\\(\\beta_{min}\\)</span>  <span class=\"math inline\">\\(\\beta_{max}\\)</span>\n <span class=\"math inline\">\\(\\alpha_{min}=0.5\\)</span><span class=\"math inline\">\\(\\alpha_{max}=1.0\\)</span><span class=\"math inline\">\\(\\beta_{min}=0.5\\)</span><span class=\"math inline\">\\(\\beta_{max}=4.0\\)</span></p>\n<p></p>\n<h1 id=\"\"></h1>\n<p><br>\n- RefinedWeb<br>\n- deduplicated PILE<br>\n- a subset of RedPajama<br>\n- a subset of Dolma v1.6</p>\n<p>1.8T tokentoken</p>\n<img src=\"/f845f3e4/data.png\" class title=\"\">\n<p>200 character256\ntoken</p>\n<h1 id=\"\"></h1>\n<p><br>\n- 350k step<br>\n- AdamW optimizer<br>\n- cosine learning rate schedulewarmup=5k<br>\n- weight decay = 0.1<br>\n- gradient clipping = 1.0</p>\n<p></p>\n<img src=\"/f845f3e4/pretrain_hp.png\" class title=\"\">\n<p></p>\n<img src=\"/f845f3e4/sft_hp.png\" class title=\"\">\n<h1 id=\"evaluation\">evaluation</h1>\n<p>OpenELM<br>\n- Standard zero-shot tasks<br>\n- OpenLLM leaderboard tasks<br>\n- LLM360 leaderboard tasks</p>\n<img src=\"/f845f3e4/eval_1.png\" class title=\"\">\n<p>pretrained modelStandard zero-shot\ntasks7checkpoint</p>\n<img src=\"/f845f3e4/eval_2.png\" class title=\"\">\n<p></p>\n<p>55000\nstepcheckpointcheckpointcheckpointnoise</p>\n<p></p>\n<p>OpenELM</p>\n<img src=\"/f845f3e4/eval_3.png\" class title=\"\">\n<p>OpenELM</p>\n<img src=\"/f845f3e4/sft_result.png\" class title=\"\">\n<p>OpenELMLoRADoRA</p>\n<img src=\"/f845f3e4/peft_eval.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<p>OpenELMlayer-wise\nscaling</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1OpenELM: An Efficient Language Model Family with Open Training\nand Inference Framework https://arxiv.org/abs/2404.14619</p>\n","length":2368,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>OpenELM0.27B0.45B1.08B3.04Bcheckpointtraining\nloghttps://github.com/apple/corenet</p>\n<p>OpenELM-1BOLMo</p>\n<img src=\"/f845f3e4/intro.png\" class title=\"OpenELM\">\n<p>OpenELM</p>\n<h1 id=\"\"></h1>\n<p>OpenELM<br>\n- linearbias<br>\n- pre-norm + RMSNorm<br>\n- RoPE<br>\n- GQA<br>\n- SwiGLU FFN<br>\n- LLAMAtokenizer</p>\n<p>Delight:\nDeep and light-weight transformerOpenELMlayer-wise\nscaling</p>\n<p>hidden\nsizescaling</p>\n<p>N <span class=\"math inline\">\\(d_{model}\\)</span>MHA <span class=\"math inline\">\\(n_{h}\\)</span>  <span class=\"math inline\">\\(d_h=\\frac{d_{model}}{n_h}\\)</span>FFN\n<span class=\"math inline\">\\(d_{\\mathrm{FFN}}=m\\cdot\nd_{model}\\)</span>mFFN multiplier</p>\n<p>layer-wise scaling <span class=\"math inline\">\\(\\alpha\\)</span> \n<span class=\"math inline\">\\(\\beta\\)</span>\nattention head <span class=\"math inline\">\\(n_{h}\\)</span> FFN multiplier m</p>\n<p>i</p>\n<img src=\"/f845f3e4/formula.png\" class title=\"\">\n<p> <span class=\"math inline\">\\(\\alpha_{min}\\)</span>  <span class=\"math inline\">\\(\\alpha_{max}\\)</span>\n<span class=\"math inline\">\\(\\beta_{min}\\)</span>  <span class=\"math inline\">\\(\\beta_{max}\\)</span>\n <span class=\"math inline\">\\(\\alpha_{min}=0.5\\)</span><span class=\"math inline\">\\(\\alpha_{max}=1.0\\)</span><span class=\"math inline\">\\(\\beta_{min}=0.5\\)</span><span class=\"math inline\">\\(\\beta_{max}=4.0\\)</span></p>\n<p></p>\n<h1 id=\"\"></h1>\n<p><br>\n- RefinedWeb<br>\n- deduplicated PILE<br>\n- a subset of RedPajama<br>\n- a subset of Dolma v1.6</p>\n<p>1.8T tokentoken</p>\n<img src=\"/f845f3e4/data.png\" class title=\"\">\n<p>200 character256\ntoken</p>\n<h1 id=\"\"></h1>\n<p><br>\n- 350k step<br>\n- AdamW optimizer<br>\n- cosine learning rate schedulewarmup=5k<br>\n- weight decay = 0.1<br>\n- gradient clipping = 1.0</p>\n<p></p>\n<img src=\"/f845f3e4/pretrain_hp.png\" class title=\"\">\n<p></p>\n<img src=\"/f845f3e4/sft_hp.png\" class title=\"\">\n<h1 id=\"evaluation\">evaluation</h1>\n<p>OpenELM<br>\n- Standard zero-shot tasks<br>\n- OpenLLM leaderboard tasks<br>\n- LLM360 leaderboard tasks</p>\n<img src=\"/f845f3e4/eval_1.png\" class title=\"\">\n<p>pretrained modelStandard zero-shot\ntasks7checkpoint</p>\n<img src=\"/f845f3e4/eval_2.png\" class title=\"\">\n<p></p>\n<p>55000\nstepcheckpointcheckpointcheckpointnoise</p>\n<p></p>\n<p>OpenELM</p>\n<img src=\"/f845f3e4/eval_3.png\" class title=\"\">\n<p>OpenELM</p>\n<img src=\"/f845f3e4/sft_result.png\" class title=\"\">\n<p>OpenELMLoRADoRA</p>\n<img src=\"/f845f3e4/peft_eval.png\" class title=\"\">\n<h1 id=\"\"></h1>\n<p>OpenELMlayer-wise\nscaling</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a><br>\n<a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(3)</a><br>\n<a href=\"http://www.linsight.cn/1736008.html\">(4)</a><br>\n<a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a><br>\n<a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a><br>\n<a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a></p>\n<hr>\n<h1 id=\"reference\">Reference</h1>\n<p>1OpenELM: An Efficient Language Model Family with Open Training\nand Inference Framework https://arxiv.org/abs/2404.14619</p>\n"},{"title":"","abbrlink":"9c593ccd","date":"2024-08-16T18:37:42.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\n  \n\n[AFM](https://www.linsight.cn/1e34e252.html)BertBertBFMBert-based Foundation Model  \n\n  \n\n#   \n\nNLU  \n\n##   \n\nSiri  \n\n1  \n\niotquery  \n\n/AA  \n\nNLUstraightforwardintent + slotintentslotintentslotintentslotintentslot88intentslotslot  \n\nintentintentintentslotNER  \n\nintentintentquery  \n\nNLUintent + NERslot  \n\n2  \n\n5000C  \n\nQQQA  \n\nagent + RAG  \n\nNER/  \n\n3  \n\nChatGPT  \n\nbotQA  \n\n  \n\n  \n\n  \n\n4  \n\n34  \n\n  \n\nNER/  \n\n+  \n\n##   \n\nintentintent  \n- intent  \n- caseintentintent  \n\nintentintent  \n\n{% asset_img system_1.png  %}  \n\nAintentNERslotiot  \n\nrank  \n\nintent  \n\n  \n- rank  \n-   \n\n  \n\nfasttextcpu  \n\ntextcnnlstm<=50 token  \n\nBertNLUBertNERtextcnnlstm -- Bert  \n\nBertBertCPUBert-basep99200msGPU  \n\nBertNER  \n\n#   \n\nadapter/LoRAadapteradapterBert  \n\nadapter  \n\nA Flexible Multi-Task Model for BERT Serving  \n\n  \n\n{% asset_img xiaomi_1.png  %}  \n\ntop-ktasktaskstudentteacherstudentBertfrozenbottom + topBert12Bert-base  \n\nstudentBertBert  \n\n  \n\n1teacher model1012  \n\n{% asset_img xiaomi_2.png  %}  \n\n[4,10]  \n\n2studentteacherBert frozen  \n\n#   \n\n  \n\nBFM  \n\nBFM +   \n\nBFM121012 * 10 = 120BFM +   \n\n|  |  | % | \n| ---- | ---- | ---- |\n| 8 | 12 + 8 * 10 = 92 | 23% |\n| 7 | 12 + 7 * 10 = 82 | 32% |\n| 6 | 12 + 6 * 10 = 72 | 40% |\n| 5 | 12 + 5 * 10 = 62 | 48% |\n| 4 | 12 + 4 * 10 = 52 | 56% |\n| 3 | 12 + 3 * 10 = 42 | 65% |\n| 2 | 12 + 2 * 10 = 32 | 73% |\n| 1 | 12 + 1 * 10 = 22 | 82% |\n\n[16]<=3  \n\nfrozenBert1~6hidden size76810247~12hidden size512  \n\n{% asset_img bfm.png  %}  \n\nhidden size67bottleneck  \n\nOpenELMOpenELM  \n\nCPU  \n\nhidden size = 512Bert  \n\n|  |  | % | \n| ---- | ---- | ---- |\n| 6 | 12 + 6 * 10 * 0.5 = 42 | 65% |\n| 5 | 12 + 5 * 10 * 0.5 = 37 | 69% |\n| 4 | 12 + 4 * 10 * 0.5 = 32 | 73% |\n| 3 | 12 + 3 * 10 * 0.5 = 27 | 78% |\n| 2 | 12 + 2 * 10 * 0.5 = 22 | 82% |\n| 1 | 12 + 1 * 10 * 0.5 = 17 | 85% |\n\n#   \n\nBert  \n\nRoBertaMLMNSPRoBertawhile word maskmask  \n\nLACLexical Analysis of Chinese  \n\n200GCLUE100G  \n\nbatch sizelearning rateoptimizerloss  \n\nNERRoBerta  \n\n#   \n\n  \n\n##   \n\ndomain-adaptive pretrainingDAPTtask-adaptive pretrainingTAPT  \n\nTAPTDAPTdomaindomain  \n\n1B  \n\nDAPTTAPTDon't Stop Pretraining: Adapt Language Models to Domains and Tasks  \n\n/  \n\n  \n\nTraining Language Models with Memory Augmentation  \n\n## task-specific embedding  \n\ntask-specific embedding  \n\ntask-specific embedding  \n\n{% asset_img task_emb.png  %}  \n\nembedding  \n\ntask-specific embeddingBertfasttext  \n\n##   \n\n  \n- teacherteacher  \n- student + frozen12  \n\nstudentteacherlogitsground truthloss  \n\n##   \n\n  \n\n1FGMFast Gradient Method  \n\n  \n-   \n-   \n\nFGM  \n\n2multi-sample dropout  \n\nmulti-sample dropoutdropout  \n\n3r-drop  \n\nmulti-sample dropoutr-dropdropoutKLauxiliary loss  \n\nNoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better  \n\noptunabatch sizelearning rate  \n\n#   \n\n1  \n\nBFMGPU61  \n\nBFM6Bert  \n\n2  \n\nonnx + fp16GPUp9910msGPUCPU2  \n\nonnxonnxfp16logits1%differenceNER5%lstmBert  \n\n#   \n\nBFMNERfasttext/textcnn/lstm3%  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n- MoE  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[denseMoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  \n[MoE--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  \n-   \n[--AFM](https://www.linsight.cn/1e34e252.html)  \n[--MobileLLM](https://www.linsight.cn/5ac36d34.html)  \n[phi](https://www.linsight.cn/fe13b56f.html)  \n-   \n[Llama3.1--](https://www.linsight.cn/7d7294cb.html)  \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n-   \n[--](https://www.linsight.cn/210dbccd.html)  \n-   \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n-   \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n-   \n[Llama3.1--post-training](https://www.linsight.cn/93328a2a.html)  \n[ -- model soup](https://www.linsight.cn/bb8fcf21.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n- Transformer  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n-   \n[(1)](http://www.linsight.cn/3345028a.html)\n[(2)](http://www.linsight.cn/ad0bba9d.html)\n[(3)](http://www.linsight.cn/1736008.html)\n[(4)](http://www.linsight.cn/1736008.html)\n[(5)](http://www.linsight.cn/336f2f3e.html)\n[(6)](http://www.linsight.cn/7c04944d.html)\n[(7)](https://www.linsight.cn/dd614e12.html)\n[(8)](https://www.linsight.cn/e287b9c3.html)\n[(9)](https://www.linsight.cn/fb9c8882.html)  \n\n# Reference  \n\n1A Flexible Multi-Task Model for BERT Serving https://arxiv.org/pdf/2107.05377  \n2Don't Stop Pretraining: Adapt Language Models to Domains and Tasks https://arxiv.org/abs/2004.10964  \n3OpenELM: An Efficient Language Model Family with Open Training and Inference Framework https://arxiv.org/abs/2404.14619  \n4Apple Intelligence Foundation Language Models https://arxiv.org/pdf/2407.21075  \n5Stochastic Weight Averaging in PyTorch https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/   \n6Multi-Sample Dropout for Accelerated Training and Better Generalization https://arxiv.org/abs/1905.09788  \n7Adversarial Training Methods for Semi-Supervised Text Classification https://arxiv.org/abs/1605.07725  \n8Training Language Models with Memory Augmentation https://aclanthology.org/2022.emnlp-main.382.pdf  \n9R-Drop: Regularized Dropout for Neural Networks https://arxiv.org/abs/2106.14448  \n10NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better https://arxiv.org/abs/2202.12024  \n","source":"_posts/cs/nlp/2024/08/.md","raw":"---\ntitle: \ntags:\n  - NLP\n  - LLM\n  - transformer\n  - Bert\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 9c593ccd\ndate: 2024-08-17 02:37:42\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\n  \n\n[AFM](https://www.linsight.cn/1e34e252.html)BertBertBFMBert-based Foundation Model  \n\n  \n\n#   \n\nNLU  \n\n##   \n\nSiri  \n\n1  \n\niotquery  \n\n/AA  \n\nNLUstraightforwardintent + slotintentslotintentslotintentslotintentslot88intentslotslot  \n\nintentintentintentslotNER  \n\nintentintentquery  \n\nNLUintent + NERslot  \n\n2  \n\n5000C  \n\nQQQA  \n\nagent + RAG  \n\nNER/  \n\n3  \n\nChatGPT  \n\nbotQA  \n\n  \n\n  \n\n  \n\n4  \n\n34  \n\n  \n\nNER/  \n\n+  \n\n##   \n\nintentintent  \n- intent  \n- caseintentintent  \n\nintentintent  \n\n{% asset_img system_1.png  %}  \n\nAintentNERslotiot  \n\nrank  \n\nintent  \n\n  \n- rank  \n-   \n\n  \n\nfasttextcpu  \n\ntextcnnlstm<=50 token  \n\nBertNLUBertNERtextcnnlstm -- Bert  \n\nBertBertCPUBert-basep99200msGPU  \n\nBertNER  \n\n#   \n\nadapter/LoRAadapteradapterBert  \n\nadapter  \n\nA Flexible Multi-Task Model for BERT Serving  \n\n  \n\n{% asset_img xiaomi_1.png  %}  \n\ntop-ktasktaskstudentteacherstudentBertfrozenbottom + topBert12Bert-base  \n\nstudentBertBert  \n\n  \n\n1teacher model1012  \n\n{% asset_img xiaomi_2.png  %}  \n\n[4,10]  \n\n2studentteacherBert frozen  \n\n#   \n\n  \n\nBFM  \n\nBFM +   \n\nBFM121012 * 10 = 120BFM +   \n\n|  |  | % | \n| ---- | ---- | ---- |\n| 8 | 12 + 8 * 10 = 92 | 23% |\n| 7 | 12 + 7 * 10 = 82 | 32% |\n| 6 | 12 + 6 * 10 = 72 | 40% |\n| 5 | 12 + 5 * 10 = 62 | 48% |\n| 4 | 12 + 4 * 10 = 52 | 56% |\n| 3 | 12 + 3 * 10 = 42 | 65% |\n| 2 | 12 + 2 * 10 = 32 | 73% |\n| 1 | 12 + 1 * 10 = 22 | 82% |\n\n[16]<=3  \n\nfrozenBert1~6hidden size76810247~12hidden size512  \n\n{% asset_img bfm.png  %}  \n\nhidden size67bottleneck  \n\nOpenELMOpenELM  \n\nCPU  \n\nhidden size = 512Bert  \n\n|  |  | % | \n| ---- | ---- | ---- |\n| 6 | 12 + 6 * 10 * 0.5 = 42 | 65% |\n| 5 | 12 + 5 * 10 * 0.5 = 37 | 69% |\n| 4 | 12 + 4 * 10 * 0.5 = 32 | 73% |\n| 3 | 12 + 3 * 10 * 0.5 = 27 | 78% |\n| 2 | 12 + 2 * 10 * 0.5 = 22 | 82% |\n| 1 | 12 + 1 * 10 * 0.5 = 17 | 85% |\n\n#   \n\nBert  \n\nRoBertaMLMNSPRoBertawhile word maskmask  \n\nLACLexical Analysis of Chinese  \n\n200GCLUE100G  \n\nbatch sizelearning rateoptimizerloss  \n\nNERRoBerta  \n\n#   \n\n  \n\n##   \n\ndomain-adaptive pretrainingDAPTtask-adaptive pretrainingTAPT  \n\nTAPTDAPTdomaindomain  \n\n1B  \n\nDAPTTAPTDon't Stop Pretraining: Adapt Language Models to Domains and Tasks  \n\n/  \n\n  \n\nTraining Language Models with Memory Augmentation  \n\n## task-specific embedding  \n\ntask-specific embedding  \n\ntask-specific embedding  \n\n{% asset_img task_emb.png  %}  \n\nembedding  \n\ntask-specific embeddingBertfasttext  \n\n##   \n\n  \n- teacherteacher  \n- student + frozen12  \n\nstudentteacherlogitsground truthloss  \n\n##   \n\n  \n\n1FGMFast Gradient Method  \n\n  \n-   \n-   \n\nFGM  \n\n2multi-sample dropout  \n\nmulti-sample dropoutdropout  \n\n3r-drop  \n\nmulti-sample dropoutr-dropdropoutKLauxiliary loss  \n\nNoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better  \n\noptunabatch sizelearning rate  \n\n#   \n\n1  \n\nBFMGPU61  \n\nBFM6Bert  \n\n2  \n\nonnx + fp16GPUp9910msGPUCPU2  \n\nonnxonnxfp16logits1%differenceNER5%lstmBert  \n\n#   \n\nBFMNERfasttext/textcnn/lstm3%  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n- MoE  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[denseMoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  \n[MoE--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  \n-   \n[--AFM](https://www.linsight.cn/1e34e252.html)  \n[--MobileLLM](https://www.linsight.cn/5ac36d34.html)  \n[phi](https://www.linsight.cn/fe13b56f.html)  \n-   \n[Llama3.1--](https://www.linsight.cn/7d7294cb.html)  \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n-   \n[--](https://www.linsight.cn/210dbccd.html)  \n-   \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n-   \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n-   \n[Llama3.1--post-training](https://www.linsight.cn/93328a2a.html)  \n[ -- model soup](https://www.linsight.cn/bb8fcf21.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n- Transformer  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n-   \n[(1)](http://www.linsight.cn/3345028a.html)\n[(2)](http://www.linsight.cn/ad0bba9d.html)\n[(3)](http://www.linsight.cn/1736008.html)\n[(4)](http://www.linsight.cn/1736008.html)\n[(5)](http://www.linsight.cn/336f2f3e.html)\n[(6)](http://www.linsight.cn/7c04944d.html)\n[(7)](https://www.linsight.cn/dd614e12.html)\n[(8)](https://www.linsight.cn/e287b9c3.html)\n[(9)](https://www.linsight.cn/fb9c8882.html)  \n\n# Reference  \n\n1A Flexible Multi-Task Model for BERT Serving https://arxiv.org/pdf/2107.05377  \n2Don't Stop Pretraining: Adapt Language Models to Domains and Tasks https://arxiv.org/abs/2004.10964  \n3OpenELM: An Efficient Language Model Family with Open Training and Inference Framework https://arxiv.org/abs/2404.14619  \n4Apple Intelligence Foundation Language Models https://arxiv.org/pdf/2407.21075  \n5Stochastic Weight Averaging in PyTorch https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/   \n6Multi-Sample Dropout for Accelerated Training and Better Generalization https://arxiv.org/abs/1905.09788  \n7Adversarial Training Methods for Semi-Supervised Text Classification https://arxiv.org/abs/1605.07725  \n8Training Language Models with Memory Augmentation https://aclanthology.org/2022.emnlp-main.382.pdf  \n9R-Drop: Regularized Dropout for Neural Networks https://arxiv.org/abs/2106.14448  \n10NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better https://arxiv.org/abs/2202.12024  \n","slug":"cs/nlp/2024/08/","published":1,"updated":"2024-08-17T12:45:53.497Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsnt00gk0p4kbe1j6qt9","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p></p>\n<p><a href=\"https://www.linsight.cn/1e34e252.html\">AFM</a>BertBertBFMBert-based\nFoundation Model</p>\n<p></p>\n<h1 id=\"\"></h1>\n<p>NLU</p>\n<h2 id=\"\"></h2>\n<p>Siri</p>\n<p>1</p>\n<p>iotquery</p>\n<p>/AA</p>\n<p>NLUstraightforwardintent\n+\nslotintentslotintentslotintentslotintentslot88intentslotslot</p>\n<p>intentintentintentslotNER</p>\n<p>intentintentquery</p>\n<p>NLUintent +\nNERslot</p>\n<p>2</p>\n<p>5000C</p>\n<p>QQQA</p>\n<p>agent\n+ RAG</p>\n<p>NER/</p>\n<p>3</p>\n<p>ChatGPT</p>\n<p>botQA</p>\n<p></p>\n<p></p>\n<p></p>\n<p>4</p>\n<p>34</p>\n<p></p>\n<p>NER/</p>\n<p>+</p>\n<h2 id=\"\"></h2>\n<p>intentintent<br>\n- intent<br>\n-\ncaseintentintent</p>\n<p>intentintent</p>\n<img src=\"/9c593ccd/system_1.png\" class title=\"\">\n<p>AintentNERslotiot</p>\n<p>rank</p>\n<p>intent</p>\n<p><br>\n-\nrank<br>\n- </p>\n<p></p>\n<p>fasttextcpu</p>\n<p>textcnnlstm&lt;=50\ntoken</p>\n<p>BertNLUBertNERtextcnnlstm\n-- Bert</p>\n<p>BertBertCPUBert-basep99200msGPU</p>\n<p>BertNER</p>\n<h1 id=\"\"></h1>\n<p>adapter/LoRAadapteradapterBert</p>\n<p>adapter</p>\n<p>A Flexible Multi-Task Model for BERT\nServing</p>\n<p></p>\n<img src=\"/9c593ccd/xiaomi_1.png\" class title=\"\">\n<p>top-ktasktaskstudentteacherstudentBertfrozenbottom\n+ topBert12Bert-base</p>\n<p>studentBertBert</p>\n<p></p>\n<p>1teacher\nmodel1012</p>\n<img src=\"/9c593ccd/xiaomi_2.png\" class title=\"\">\n<p>[4,10]</p>\n<p>2studentteacherBert frozen</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>BFM</p>\n<p>BFM +\n</p>\n<p>BFM121012\n* 10 = 120BFM + </p>\n<table>\n<thead>\n<tr class=\"header\">\n<th></th>\n<th></th>\n<th>%</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td>8</td>\n<td>12 + 8 * 10 = 92</td>\n<td>23%</td>\n</tr>\n<tr class=\"even\">\n<td>7</td>\n<td>12 + 7 * 10 = 82</td>\n<td>32%</td>\n</tr>\n<tr class=\"odd\">\n<td>6</td>\n<td>12 + 6 * 10 = 72</td>\n<td>40%</td>\n</tr>\n<tr class=\"even\">\n<td>5</td>\n<td>12 + 5 * 10 = 62</td>\n<td>48%</td>\n</tr>\n<tr class=\"odd\">\n<td>4</td>\n<td>12 + 4 * 10 = 52</td>\n<td>56%</td>\n</tr>\n<tr class=\"even\">\n<td>3</td>\n<td>12 + 3 * 10 = 42</td>\n<td>65%</td>\n</tr>\n<tr class=\"odd\">\n<td>2</td>\n<td>12 + 2 * 10 = 32</td>\n<td>73%</td>\n</tr>\n<tr class=\"even\">\n<td>1</td>\n<td>12 + 1 * 10 = 22</td>\n<td>82%</td>\n</tr>\n</tbody>\n</table>\n<p>[16]&lt;=3</p>\n<p>frozenBert1~6hidden\nsize76810247~12hidden\nsize512</p>\n<img src=\"/9c593ccd/bfm.png\" class title=\"\">\n<p>hidden\nsize67bottleneck</p>\n<p>OpenELMOpenELM</p>\n<p>CPU</p>\n<p>hidden\nsize = 512Bert</p>\n<table>\n<thead>\n<tr class=\"header\">\n<th></th>\n<th></th>\n<th>%</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td>6</td>\n<td>12 + 6 * 10 * 0.5 = 42</td>\n<td>65%</td>\n</tr>\n<tr class=\"even\">\n<td>5</td>\n<td>12 + 5 * 10 * 0.5 = 37</td>\n<td>69%</td>\n</tr>\n<tr class=\"odd\">\n<td>4</td>\n<td>12 + 4 * 10 * 0.5 = 32</td>\n<td>73%</td>\n</tr>\n<tr class=\"even\">\n<td>3</td>\n<td>12 + 3 * 10 * 0.5 = 27</td>\n<td>78%</td>\n</tr>\n<tr class=\"odd\">\n<td>2</td>\n<td>12 + 2 * 10 * 0.5 = 22</td>\n<td>82%</td>\n</tr>\n<tr class=\"even\">\n<td>1</td>\n<td>12 + 1 * 10 * 0.5 = 17</td>\n<td>85%</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"\"></h1>\n<p>Bert</p>\n<p>RoBertaMLMNSPRoBertawhile\nword maskmask</p>\n<p>LACLexical Analysis of\nChinese</p>\n<p>200GCLUE100G</p>\n<p>batch sizelearning\nrateoptimizerloss</p>\n<p>NERRoBerta</p>\n<h1 id=\"\"></h1>\n<p></p>\n<h2 id=\"\"></h2>\n<p>domain-adaptive\npretrainingDAPTtask-adaptive pretrainingTAPT</p>\n<p>TAPTDAPTdomaindomain</p>\n<p>1B</p>\n<p>DAPTTAPTDon't Stop Pretraining: Adapt\nLanguage Models to Domains and Tasks</p>\n<p>/</p>\n<p></p>\n<p>Training Language Models with Memory\nAugmentation</p>\n<h2 id=\"task-specific-embedding\">task-specific embedding</h2>\n<p>task-specific\nembedding</p>\n<p>task-specific embedding</p>\n<img src=\"/9c593ccd/task_emb.png\" class title=\"\">\n<p>embedding</p>\n<p>task-specific embeddingBertfasttext</p>\n<h2 id=\"\"></h2>\n<p><br>\n-\nteacherteacher<br>\n- student +\nfrozen12</p>\n<p>studentteacherlogitsground\ntruthloss</p>\n<h2 id=\"\"></h2>\n<p></p>\n<p>1FGMFast Gradient Method</p>\n<p><br>\n- <br>\n- </p>\n<p>FGM</p>\n<p>2multi-sample dropout</p>\n<p>multi-sample\ndropoutdropout</p>\n<p>3r-drop</p>\n<p>multi-sample\ndropoutr-dropdropoutKLauxiliary\nloss</p>\n<p>NoisyTune: A Little Noise Can\nHelp You Finetune Pretrained Language Models\nBetter</p>\n<p>optunabatch\nsizelearning\nrate</p>\n<h1 id=\"\"></h1>\n<p>1</p>\n<p>BFMGPU61</p>\n<p>BFM6Bert</p>\n<p>2</p>\n<p>onnx +\nfp16GPUp9910msGPUCPU2</p>\n<p>onnxonnxfp16logits1%differenceNER5%lstmBert</p>\n<h1 id=\"\"></h1>\n<p>BFMNERfasttext/textcnn/lstm3%</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n- MoE<br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/a0824e29.html\">denseMoE -- sparse\nupcycling</a><br>\n<a href=\"https://www.linsight.cn/2c8bbc7.html\">MoE--expert choice\nrouting</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/1e34e252.html\">--AFM</a><br>\n<a href=\"https://www.linsight.cn/5ac36d34.html\">--MobileLLM</a><br>\n<a href=\"https://www.linsight.cn/fe13b56f.html\">phi</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/7d7294cb.html\">Llama3.1--</a><br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/210dbccd.html\">--</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/93328a2a.html\">Llama3.1--post-training</a><br>\n<a href=\"https://www.linsight.cn/bb8fcf21.html\"> -- model\nsoup</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n- Transformer<br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a> <a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a> <a href=\"http://www.linsight.cn/1736008.html\">(3)</a> <a href=\"http://www.linsight.cn/1736008.html\">(4)</a> <a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a> <a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a> <a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a> <a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a> <a href=\"https://www.linsight.cn/fb9c8882.html\">(9)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1A Flexible Multi-Task Model for BERT Serving\nhttps://arxiv.org/pdf/2107.05377<br>\n2Don't Stop Pretraining: Adapt Language Models to Domains and Tasks\nhttps://arxiv.org/abs/2004.10964<br>\n3OpenELM: An Efficient Language Model Family with Open Training and\nInference Framework https://arxiv.org/abs/2404.14619<br>\n4Apple Intelligence Foundation Language Models\nhttps://arxiv.org/pdf/2407.21075<br>\n5Stochastic Weight Averaging in PyTorch\nhttps://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/<br>\n6Multi-Sample Dropout for Accelerated Training and Better\nGeneralization https://arxiv.org/abs/1905.09788<br>\n7Adversarial Training Methods for Semi-Supervised Text\nClassification https://arxiv.org/abs/1605.07725<br>\n8Training Language Models with Memory Augmentation\nhttps://aclanthology.org/2022.emnlp-main.382.pdf<br>\n9R-Drop: Regularized Dropout for Neural Networks\nhttps://arxiv.org/abs/2106.14448<br>\n10NoisyTune: A Little Noise Can Help You Finetune Pretrained\nLanguage Models Better https://arxiv.org/abs/2202.12024</p>\n","length":9057,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p></p>\n<p><a href=\"https://www.linsight.cn/1e34e252.html\">AFM</a>BertBertBFMBert-based\nFoundation Model</p>\n<p></p>\n<h1 id=\"\"></h1>\n<p>NLU</p>\n<h2 id=\"\"></h2>\n<p>Siri</p>\n<p>1</p>\n<p>iotquery</p>\n<p>/AA</p>\n<p>NLUstraightforwardintent\n+\nslotintentslotintentslotintentslotintentslot88intentslotslot</p>\n<p>intentintentintentslotNER</p>\n<p>intentintentquery</p>\n<p>NLUintent +\nNERslot</p>\n<p>2</p>\n<p>5000C</p>\n<p>QQQA</p>\n<p>agent\n+ RAG</p>\n<p>NER/</p>\n<p>3</p>\n<p>ChatGPT</p>\n<p>botQA</p>\n<p></p>\n<p></p>\n<p></p>\n<p>4</p>\n<p>34</p>\n<p></p>\n<p>NER/</p>\n<p>+</p>\n<h2 id=\"\"></h2>\n<p>intentintent<br>\n- intent<br>\n-\ncaseintentintent</p>\n<p>intentintent</p>\n<img src=\"/9c593ccd/system_1.png\" class title=\"\">\n<p>AintentNERslotiot</p>\n<p>rank</p>\n<p>intent</p>\n<p><br>\n-\nrank<br>\n- </p>\n<p></p>\n<p>fasttextcpu</p>\n<p>textcnnlstm&lt;=50\ntoken</p>\n<p>BertNLUBertNERtextcnnlstm\n-- Bert</p>\n<p>BertBertCPUBert-basep99200msGPU</p>\n<p>BertNER</p>\n<h1 id=\"\"></h1>\n<p>adapter/LoRAadapteradapterBert</p>\n<p>adapter</p>\n<p>A Flexible Multi-Task Model for BERT\nServing</p>\n<p></p>\n<img src=\"/9c593ccd/xiaomi_1.png\" class title=\"\">\n<p>top-ktasktaskstudentteacherstudentBertfrozenbottom\n+ topBert12Bert-base</p>\n<p>studentBertBert</p>\n<p></p>\n<p>1teacher\nmodel1012</p>\n<img src=\"/9c593ccd/xiaomi_2.png\" class title=\"\">\n<p>[4,10]</p>\n<p>2studentteacherBert frozen</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>BFM</p>\n<p>BFM +\n</p>\n<p>BFM121012\n* 10 = 120BFM + </p>\n<table>\n<thead>\n<tr class=\"header\">\n<th></th>\n<th></th>\n<th>%</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td>8</td>\n<td>12 + 8 * 10 = 92</td>\n<td>23%</td>\n</tr>\n<tr class=\"even\">\n<td>7</td>\n<td>12 + 7 * 10 = 82</td>\n<td>32%</td>\n</tr>\n<tr class=\"odd\">\n<td>6</td>\n<td>12 + 6 * 10 = 72</td>\n<td>40%</td>\n</tr>\n<tr class=\"even\">\n<td>5</td>\n<td>12 + 5 * 10 = 62</td>\n<td>48%</td>\n</tr>\n<tr class=\"odd\">\n<td>4</td>\n<td>12 + 4 * 10 = 52</td>\n<td>56%</td>\n</tr>\n<tr class=\"even\">\n<td>3</td>\n<td>12 + 3 * 10 = 42</td>\n<td>65%</td>\n</tr>\n<tr class=\"odd\">\n<td>2</td>\n<td>12 + 2 * 10 = 32</td>\n<td>73%</td>\n</tr>\n<tr class=\"even\">\n<td>1</td>\n<td>12 + 1 * 10 = 22</td>\n<td>82%</td>\n</tr>\n</tbody>\n</table>\n<p>[16]&lt;=3</p>\n<p>frozenBert1~6hidden\nsize76810247~12hidden\nsize512</p>\n<img src=\"/9c593ccd/bfm.png\" class title=\"\">\n<p>hidden\nsize67bottleneck</p>\n<p>OpenELMOpenELM</p>\n<p>CPU</p>\n<p>hidden\nsize = 512Bert</p>\n<table>\n<thead>\n<tr class=\"header\">\n<th></th>\n<th></th>\n<th>%</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td>6</td>\n<td>12 + 6 * 10 * 0.5 = 42</td>\n<td>65%</td>\n</tr>\n<tr class=\"even\">\n<td>5</td>\n<td>12 + 5 * 10 * 0.5 = 37</td>\n<td>69%</td>\n</tr>\n<tr class=\"odd\">\n<td>4</td>\n<td>12 + 4 * 10 * 0.5 = 32</td>\n<td>73%</td>\n</tr>\n<tr class=\"even\">\n<td>3</td>\n<td>12 + 3 * 10 * 0.5 = 27</td>\n<td>78%</td>\n</tr>\n<tr class=\"odd\">\n<td>2</td>\n<td>12 + 2 * 10 * 0.5 = 22</td>\n<td>82%</td>\n</tr>\n<tr class=\"even\">\n<td>1</td>\n<td>12 + 1 * 10 * 0.5 = 17</td>\n<td>85%</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"\"></h1>\n<p>Bert</p>\n<p>RoBertaMLMNSPRoBertawhile\nword maskmask</p>\n<p>LACLexical Analysis of\nChinese</p>\n<p>200GCLUE100G</p>\n<p>batch sizelearning\nrateoptimizerloss</p>\n<p>NERRoBerta</p>\n<h1 id=\"\"></h1>\n<p></p>\n<h2 id=\"\"></h2>\n<p>domain-adaptive\npretrainingDAPTtask-adaptive pretrainingTAPT</p>\n<p>TAPTDAPTdomaindomain</p>\n<p>1B</p>\n<p>DAPTTAPTDon't Stop Pretraining: Adapt\nLanguage Models to Domains and Tasks</p>\n<p>/</p>\n<p></p>\n<p>Training Language Models with Memory\nAugmentation</p>\n<h2 id=\"task-specific-embedding\">task-specific embedding</h2>\n<p>task-specific\nembedding</p>\n<p>task-specific embedding</p>\n<img src=\"/9c593ccd/task_emb.png\" class title=\"\">\n<p>embedding</p>\n<p>task-specific embeddingBertfasttext</p>\n<h2 id=\"\"></h2>\n<p><br>\n-\nteacherteacher<br>\n- student +\nfrozen12</p>\n<p>studentteacherlogitsground\ntruthloss</p>\n<h2 id=\"\"></h2>\n<p></p>\n<p>1FGMFast Gradient Method</p>\n<p><br>\n- <br>\n- </p>\n<p>FGM</p>\n<p>2multi-sample dropout</p>\n<p>multi-sample\ndropoutdropout</p>\n<p>3r-drop</p>\n<p>multi-sample\ndropoutr-dropdropoutKLauxiliary\nloss</p>\n<p>NoisyTune: A Little Noise Can\nHelp You Finetune Pretrained Language Models\nBetter</p>\n<p>optunabatch\nsizelearning\nrate</p>\n<h1 id=\"\"></h1>\n<p>1</p>\n<p>BFMGPU61</p>\n<p>BFM6Bert</p>\n<p>2</p>\n<p>onnx +\nfp16GPUp9910msGPUCPU2</p>\n<p>onnxonnxfp16logits1%differenceNER5%lstmBert</p>\n<h1 id=\"\"></h1>\n<p>BFMNERfasttext/textcnn/lstm3%</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n- MoE<br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/a0824e29.html\">denseMoE -- sparse\nupcycling</a><br>\n<a href=\"https://www.linsight.cn/2c8bbc7.html\">MoE--expert choice\nrouting</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/1e34e252.html\">--AFM</a><br>\n<a href=\"https://www.linsight.cn/5ac36d34.html\">--MobileLLM</a><br>\n<a href=\"https://www.linsight.cn/fe13b56f.html\">phi</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/7d7294cb.html\">Llama3.1--</a><br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/210dbccd.html\">--</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/93328a2a.html\">Llama3.1--post-training</a><br>\n<a href=\"https://www.linsight.cn/bb8fcf21.html\"> -- model\nsoup</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n- Transformer<br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a> <a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a> <a href=\"http://www.linsight.cn/1736008.html\">(3)</a> <a href=\"http://www.linsight.cn/1736008.html\">(4)</a> <a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a> <a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a> <a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a> <a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a> <a href=\"https://www.linsight.cn/fb9c8882.html\">(9)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1A Flexible Multi-Task Model for BERT Serving\nhttps://arxiv.org/pdf/2107.05377<br>\n2Don't Stop Pretraining: Adapt Language Models to Domains and Tasks\nhttps://arxiv.org/abs/2004.10964<br>\n3OpenELM: An Efficient Language Model Family with Open Training and\nInference Framework https://arxiv.org/abs/2404.14619<br>\n4Apple Intelligence Foundation Language Models\nhttps://arxiv.org/pdf/2407.21075<br>\n5Stochastic Weight Averaging in PyTorch\nhttps://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/<br>\n6Multi-Sample Dropout for Accelerated Training and Better\nGeneralization https://arxiv.org/abs/1905.09788<br>\n7Adversarial Training Methods for Semi-Supervised Text\nClassification https://arxiv.org/abs/1605.07725<br>\n8Training Language Models with Memory Augmentation\nhttps://aclanthology.org/2022.emnlp-main.382.pdf<br>\n9R-Drop: Regularized Dropout for Neural Networks\nhttps://arxiv.org/abs/2106.14448<br>\n10NoisyTune: A Little Noise Can Help You Finetune Pretrained\nLanguage Models Better https://arxiv.org/abs/2202.12024</p>\n"},{"title":"--MobileLLM","abbrlink":"5ac36d34","date":"2024-08-02T14:46:21.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nCNNMetaMobileLLM125M350MMobileNet  \n\n{% asset_img mobilellm.png mobilellm %}  \n\n#   \n\n5%GPT-41H100  \n\nLlama-2-7B4-bit0.1J/token per million parametersEie: Efficient inference engine on compressed deep neural networkTowards energyproportional\ndatacenter memory with mobile dram4-bit Llama-2-7BiPhone2125M350M  \n\nmemory hierarchy7Bapp  \n\n{% asset_img device.png mobilellm %}  \n\nMeta1B  \n\n#   \n\n  \n\ntransformer decoderMobileLLM4  \n- 1deep and thin  \n- 2embedding sharing  \n- 3SwiGLU  \n- 4GQA  \n\nzero-shot common sense reasoning  \n\n{% asset_img structure.png mobilellm %}  \n\n  \n\n{% asset_img structure_ablation.png mobilellm %}  \n\n  \n\n## depth vs width  \n\n  \n\n{% asset_img deep.png mobilellm %}  \n\n  \n\n{% asset_img deep_ablation.png mobilellm %}  \n\nzero-shot3010  \n\n1B201B  \n\n## embedding sharing  \n\nembeddingLlama-7B3.7%Llama-70B0.7%embedding sharing  \n\n125Membedding20%embedding dimension = 512vocab = 32kembedding sharing  \n\n30135Membedding16M0.2embedding3032125M135M0.4  \n\n{% asset_img emb.png mobilellm %}  \n\nembedding  \n\n## GQA  \n\n  \n\n> The trade-off between more semantics per\nhead dimension and more non-linear combinations of multiple\nheads is a key consideration in choosing the head size.\n\nGQAKV cacheGQA  \n\nhead sizekv head125M350M  \n\n{% asset_img head.png mobilellm %}  \n\n16query headkv head4350M0.210%  \n\n## Layer Sharing  \n\nMetalayer sharing  \n\nAlbertlayer sharinglayer sharingMobileLLM-LS  \n\nlayer sharing  \n\n{% asset_img share.png mobilellm %}  \n\n  \n\n{% asset_img share_2.png mobilellm %}  \n\nrepeat-all-overMetaimmediate block-wiseSRAM  \n\nblock-wise  \n\n{% asset_img repeat.png mobilellm %}  \n\nrepeat2  \n\n#   \n\n##   \n\n125M350M  \n\n{% asset_img result.png mobilellm %}  \n\n  \n\n## scale up  \n\nMeta\n\n{% asset_img model.png mobilellm %}  \n\n  \n\n{% asset_img zero_shot.png mobilellm %}  \n\n1B  \n\n##   \n\nMetaLlama-2-7B  \n\n  \n\n{% asset_img kd.png mobilellm %}  \n\n#   \n\n-   \n- scaling law  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n- MoE  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[denseMoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  \n[MoE--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  \n-   \n[--AFM](https://www.linsight.cn/1e34e252.html)  \n-   \n[Llama3.1--](https://www.linsight.cn/7d7294cb.html)  \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n-   \n[--](https://www.linsight.cn/210dbccd.html)  \n-   \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n-   \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n-   \n[Llama3.1--post-training](https://www.linsight.cn/93328a2a.html)  \n[ -- model soup](https://www.linsight.cn/bb8fcf21.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n- Transformer  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n-   \n[(1)](http://www.linsight.cn/3345028a.html)\n[(2)](http://www.linsight.cn/ad0bba9d.html)\n[(3)](http://www.linsight.cn/1736008.html)\n[(4)](http://www.linsight.cn/1736008.html)\n[(5)](http://www.linsight.cn/336f2f3e.html)\n[(6)](http://www.linsight.cn/7c04944d.html)\n[(7)](https://www.linsight.cn/dd614e12.html)\n[(8)](https://www.linsight.cn/e287b9c3.html)  \n\n# Reference  \n\n1MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases https://arxiv.org/abs/2402.14905  \n","source":"_posts/cs/nlp/2024/08/-MobileLLM.md","raw":"---\ntitle: --MobileLLM\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - Meta\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: 5ac36d34\ndate: 2024-08-02 22:46:21\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nCNNMetaMobileLLM125M350MMobileNet  \n\n{% asset_img mobilellm.png mobilellm %}  \n\n#   \n\n5%GPT-41H100  \n\nLlama-2-7B4-bit0.1J/token per million parametersEie: Efficient inference engine on compressed deep neural networkTowards energyproportional\ndatacenter memory with mobile dram4-bit Llama-2-7BiPhone2125M350M  \n\nmemory hierarchy7Bapp  \n\n{% asset_img device.png mobilellm %}  \n\nMeta1B  \n\n#   \n\n  \n\ntransformer decoderMobileLLM4  \n- 1deep and thin  \n- 2embedding sharing  \n- 3SwiGLU  \n- 4GQA  \n\nzero-shot common sense reasoning  \n\n{% asset_img structure.png mobilellm %}  \n\n  \n\n{% asset_img structure_ablation.png mobilellm %}  \n\n  \n\n## depth vs width  \n\n  \n\n{% asset_img deep.png mobilellm %}  \n\n  \n\n{% asset_img deep_ablation.png mobilellm %}  \n\nzero-shot3010  \n\n1B201B  \n\n## embedding sharing  \n\nembeddingLlama-7B3.7%Llama-70B0.7%embedding sharing  \n\n125Membedding20%embedding dimension = 512vocab = 32kembedding sharing  \n\n30135Membedding16M0.2embedding3032125M135M0.4  \n\n{% asset_img emb.png mobilellm %}  \n\nembedding  \n\n## GQA  \n\n  \n\n> The trade-off between more semantics per\nhead dimension and more non-linear combinations of multiple\nheads is a key consideration in choosing the head size.\n\nGQAKV cacheGQA  \n\nhead sizekv head125M350M  \n\n{% asset_img head.png mobilellm %}  \n\n16query headkv head4350M0.210%  \n\n## Layer Sharing  \n\nMetalayer sharing  \n\nAlbertlayer sharinglayer sharingMobileLLM-LS  \n\nlayer sharing  \n\n{% asset_img share.png mobilellm %}  \n\n  \n\n{% asset_img share_2.png mobilellm %}  \n\nrepeat-all-overMetaimmediate block-wiseSRAM  \n\nblock-wise  \n\n{% asset_img repeat.png mobilellm %}  \n\nrepeat2  \n\n#   \n\n##   \n\n125M350M  \n\n{% asset_img result.png mobilellm %}  \n\n  \n\n## scale up  \n\nMeta\n\n{% asset_img model.png mobilellm %}  \n\n  \n\n{% asset_img zero_shot.png mobilellm %}  \n\n1B  \n\n##   \n\nMetaLlama-2-7B  \n\n  \n\n{% asset_img kd.png mobilellm %}  \n\n#   \n\n-   \n- scaling law  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n- MoE  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[denseMoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  \n[MoE--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  \n-   \n[--AFM](https://www.linsight.cn/1e34e252.html)  \n-   \n[Llama3.1--](https://www.linsight.cn/7d7294cb.html)  \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n-   \n[--](https://www.linsight.cn/210dbccd.html)  \n-   \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n-   \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n-   \n[Llama3.1--post-training](https://www.linsight.cn/93328a2a.html)  \n[ -- model soup](https://www.linsight.cn/bb8fcf21.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n- Transformer  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n-   \n[(1)](http://www.linsight.cn/3345028a.html)\n[(2)](http://www.linsight.cn/ad0bba9d.html)\n[(3)](http://www.linsight.cn/1736008.html)\n[(4)](http://www.linsight.cn/1736008.html)\n[(5)](http://www.linsight.cn/336f2f3e.html)\n[(6)](http://www.linsight.cn/7c04944d.html)\n[(7)](https://www.linsight.cn/dd614e12.html)\n[(8)](https://www.linsight.cn/e287b9c3.html)  \n\n# Reference  \n\n1MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases https://arxiv.org/abs/2402.14905  \n","slug":"cs/nlp/2024/08/-MobileLLM","published":1,"updated":"2024-08-02T15:08:59.571Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsnt00gm0p4k5nza3nwg","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>CNNMetaMobileLLM125M350MMobileNet</p>\n<img src=\"/5ac36d34/mobilellm.png\" class title=\"mobilellm\">\n<h1 id=\"\"></h1>\n<p>5%GPT-41H100</p>\n<p>Llama-2-7B4-bit0.1J/token\nper million parametersEie: Efficient inference engine on\ncompressed deep neural networkTowards energyproportional\ndatacenter memory with mobile dram4-bit\nLlama-2-7BiPhone2125M350M</p>\n<p>memory\nhierarchy7Bapp</p>\n<img src=\"/5ac36d34/device.png\" class title=\"mobilellm\">\n<p>Meta1B</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>transformer\ndecoderMobileLLM4<br>\n- 1deep and thin<br>\n- 2embedding sharing<br>\n- 3SwiGLU<br>\n- 4GQA</p>\n<p>zero-shot common sense reasoning</p>\n<img src=\"/5ac36d34/structure.png\" class title=\"mobilellm\">\n<p></p>\n<img src=\"/5ac36d34/structure_ablation.png\" class title=\"mobilellm\">\n<p></p>\n<h2 id=\"depth-vs-width\">depth vs width</h2>\n<p></p>\n<img src=\"/5ac36d34/deep.png\" class title=\"mobilellm\">\n<p></p>\n<img src=\"/5ac36d34/deep_ablation.png\" class title=\"mobilellm\">\n<p>zero-shot3010</p>\n<p>1B201B</p>\n<h2 id=\"embedding-sharing\">embedding sharing</h2>\n<p>embeddingLlama-7B3.7%Llama-70B0.7%embedding\nsharing</p>\n<p>125Membedding20%embedding\ndimension = 512vocab = 32kembedding\nsharing</p>\n<p>30135Membedding16M0.2embedding3032125M135M0.4</p>\n<img src=\"/5ac36d34/emb.png\" class title=\"mobilellm\">\n<p>embedding</p>\n<h2 id=\"gqa\">GQA</h2>\n<p></p>\n<blockquote>\n<p>The trade-off between more semantics per head dimension and more\nnon-linear combinations of multiple heads is a key consideration in\nchoosing the head size.</p>\n</blockquote>\n<p>GQAKV\ncacheGQA</p>\n<p>head sizekv head125M350M</p>\n<img src=\"/5ac36d34/head.png\" class title=\"mobilellm\">\n<p>16query headkv\nhead4350M0.210%</p>\n<h2 id=\"layer-sharing\">Layer Sharing</h2>\n<p>Metalayer\nsharing</p>\n<p>Albertlayer\nsharinglayer\nsharingMobileLLM-LS</p>\n<p>layer sharing</p>\n<img src=\"/5ac36d34/share.png\" class title=\"mobilellm\">\n<p></p>\n<img src=\"/5ac36d34/share_2.png\" class title=\"mobilellm\">\n<p>repeat-all-overMetaimmediate\nblock-wiseSRAM</p>\n<p>block-wise</p>\n<img src=\"/5ac36d34/repeat.png\" class title=\"mobilellm\">\n<p>repeat2</p>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>125M350M</p>\n<img src=\"/5ac36d34/result.png\" class title=\"mobilellm\">\n<p></p>\n<h2 id=\"scale-up\">scale up</h2>\n<p>Meta</p>\n<img src=\"/5ac36d34/model.png\" class title=\"mobilellm\">\n<p></p>\n<img src=\"/5ac36d34/zero_shot.png\" class title=\"mobilellm\">\n<p>1B</p>\n<h2 id=\"\"></h2>\n<p>MetaLlama-2-7B</p>\n<p></p>\n<img src=\"/5ac36d34/kd.png\" class title=\"mobilellm\">\n<h1 id=\"\"></h1>\n<ul>\n<li><br>\n</li>\n<li>scaling\nlaw</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n- MoE<br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/a0824e29.html\">denseMoE -- sparse\nupcycling</a><br>\n<a href=\"https://www.linsight.cn/2c8bbc7.html\">MoE--expert choice\nrouting</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/1e34e252.html\">--AFM</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/7d7294cb.html\">Llama3.1--</a><br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/210dbccd.html\">--</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/93328a2a.html\">Llama3.1--post-training</a><br>\n<a href=\"https://www.linsight.cn/bb8fcf21.html\"> -- model\nsoup</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n- Transformer<br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a> <a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a> <a href=\"http://www.linsight.cn/1736008.html\">(3)</a> <a href=\"http://www.linsight.cn/1736008.html\">(4)</a> <a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a> <a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a> <a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a> <a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1MobileLLM: Optimizing Sub-billion Parameter Language Models for\nOn-Device Use Cases https://arxiv.org/abs/2402.14905</p>\n","length":3427,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>CNNMetaMobileLLM125M350MMobileNet</p>\n<img src=\"/5ac36d34/mobilellm.png\" class title=\"mobilellm\">\n<h1 id=\"\"></h1>\n<p>5%GPT-41H100</p>\n<p>Llama-2-7B4-bit0.1J/token\nper million parametersEie: Efficient inference engine on\ncompressed deep neural networkTowards energyproportional\ndatacenter memory with mobile dram4-bit\nLlama-2-7BiPhone2125M350M</p>\n<p>memory\nhierarchy7Bapp</p>\n<img src=\"/5ac36d34/device.png\" class title=\"mobilellm\">\n<p>Meta1B</p>\n<h1 id=\"\"></h1>\n<p></p>\n<p>transformer\ndecoderMobileLLM4<br>\n- 1deep and thin<br>\n- 2embedding sharing<br>\n- 3SwiGLU<br>\n- 4GQA</p>\n<p>zero-shot common sense reasoning</p>\n<img src=\"/5ac36d34/structure.png\" class title=\"mobilellm\">\n<p></p>\n<img src=\"/5ac36d34/structure_ablation.png\" class title=\"mobilellm\">\n<p></p>\n<h2 id=\"depth-vs-width\">depth vs width</h2>\n<p></p>\n<img src=\"/5ac36d34/deep.png\" class title=\"mobilellm\">\n<p></p>\n<img src=\"/5ac36d34/deep_ablation.png\" class title=\"mobilellm\">\n<p>zero-shot3010</p>\n<p>1B201B</p>\n<h2 id=\"embedding-sharing\">embedding sharing</h2>\n<p>embeddingLlama-7B3.7%Llama-70B0.7%embedding\nsharing</p>\n<p>125Membedding20%embedding\ndimension = 512vocab = 32kembedding\nsharing</p>\n<p>30135Membedding16M0.2embedding3032125M135M0.4</p>\n<img src=\"/5ac36d34/emb.png\" class title=\"mobilellm\">\n<p>embedding</p>\n<h2 id=\"gqa\">GQA</h2>\n<p></p>\n<blockquote>\n<p>The trade-off between more semantics per head dimension and more\nnon-linear combinations of multiple heads is a key consideration in\nchoosing the head size.</p>\n</blockquote>\n<p>GQAKV\ncacheGQA</p>\n<p>head sizekv head125M350M</p>\n<img src=\"/5ac36d34/head.png\" class title=\"mobilellm\">\n<p>16query headkv\nhead4350M0.210%</p>\n<h2 id=\"layer-sharing\">Layer Sharing</h2>\n<p>Metalayer\nsharing</p>\n<p>Albertlayer\nsharinglayer\nsharingMobileLLM-LS</p>\n<p>layer sharing</p>\n<img src=\"/5ac36d34/share.png\" class title=\"mobilellm\">\n<p></p>\n<img src=\"/5ac36d34/share_2.png\" class title=\"mobilellm\">\n<p>repeat-all-overMetaimmediate\nblock-wiseSRAM</p>\n<p>block-wise</p>\n<img src=\"/5ac36d34/repeat.png\" class title=\"mobilellm\">\n<p>repeat2</p>\n<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>125M350M</p>\n<img src=\"/5ac36d34/result.png\" class title=\"mobilellm\">\n<p></p>\n<h2 id=\"scale-up\">scale up</h2>\n<p>Meta</p>\n<img src=\"/5ac36d34/model.png\" class title=\"mobilellm\">\n<p></p>\n<img src=\"/5ac36d34/zero_shot.png\" class title=\"mobilellm\">\n<p>1B</p>\n<h2 id=\"\"></h2>\n<p>MetaLlama-2-7B</p>\n<p></p>\n<img src=\"/5ac36d34/kd.png\" class title=\"mobilellm\">\n<h1 id=\"\"></h1>\n<ul>\n<li><br>\n</li>\n<li>scaling\nlaw</li>\n</ul>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n- MoE<br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/a0824e29.html\">denseMoE -- sparse\nupcycling</a><br>\n<a href=\"https://www.linsight.cn/2c8bbc7.html\">MoE--expert choice\nrouting</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/1e34e252.html\">--AFM</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/7d7294cb.html\">Llama3.1--</a><br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/210dbccd.html\">--</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/93328a2a.html\">Llama3.1--post-training</a><br>\n<a href=\"https://www.linsight.cn/bb8fcf21.html\"> -- model\nsoup</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n- Transformer<br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a> <a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a> <a href=\"http://www.linsight.cn/1736008.html\">(3)</a> <a href=\"http://www.linsight.cn/1736008.html\">(4)</a> <a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a> <a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a> <a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a> <a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1MobileLLM: Optimizing Sub-billion Parameter Language Models for\nOn-Device Use Cases https://arxiv.org/abs/2402.14905</p>\n"},{"title":"phi","abbrlink":"fe13b56f","date":"2024-08-13T12:41:06.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nphi  \n\n# phi-1  \n\nphi-1350Mphi-1-small1.3Bphi-1-base  \n\nSLM/LLMphi-17B  \n- 6Bweb  \n- 1BGPT-3.5  \n\nA100*84  \n\nphi-1codepythonphi-1-baseLM/LMHumanEvalMBPP  \n\n{% asset_img phi_1_result.png phi %}  \n\n##   \n\nphi-1decoder-onlyphi-1  \n\n|   | phi-1-base | phi-1-small |\n| ---- | ---- | ---- |\n|  | 1.3B | 350M |\n|  | 24 | 20 |\n| hidden size | 2048 | 1024 |\n| intermediate size | 8192 | 4096 |\n| attention head num | 32 | 16 |\n\nMHARoPEdimension3264  \n\ntokenizercodegen-350M-mono  \n\n##   \n\nscaling lawphi-1  \n\nTinystories: How small can language models be and still speak coherent english?scaling law  \n\nTextbooks Are All You Needphi-1textbook  \n\n1  \n\nThe StackStackOverflowCodeContest  \n- not self-contained  \n- not meaningfulGUI  \n-   \n- topic  \n\n{% asset_img phi_1_code_case.png phi %}  \n\n  \n\ntextbook  \n\n2  \n\n  \n\nThe StackStackOverflowpython35M35B100kGPT-4determine its educational value for a student whose goal is to learn basic coding concepts  \n\nprompt  \n\n100kcodegenoutput embeddingfeaturerandom forest classifier35M  \n\nstep96k vs 36k  \n\n{% asset_img phi_1_compare.png phi %}  \n\n3synthetic textbook dataset  \n\n  \n\n\n- topiccase  \n- LLM  \n\nphi-1TinystoriespromptGPT-3.5topic1B token  \n\n  \n\n{% asset_img phi_1_example_1.png phi %}  \n\n4CodeExercises dataset  \n\nSFT  \n\nGPT-3.5180M tokenfunction name  \n\n{% asset_img phi_1_example_2.png phi %}  \n\n##   \n\nphi-1  \n- AdamW optimizerweight decay = 0.1  \n- linear-warmup-linear-decay learning rate schedule  \n- dropout = 0.1  \n-  = 2048  \n-  = fp16  \n\n36000 step24000 stepcheckpoint7B8epoch50B  \n- batch size = 1024  \n- max lr = 1e-3  \n- warmup step = 750  \n\n6000step  \n- batch size = 256  \n- max lr = 1e-4  \n- warmup step = 50  \n\n  \n\n##   \n\n- phi-1prompt  \n- Meta  \n- scaling up  \n\n# phi-1.5  \n\nphi-1.5phi-1phi-1 + common sense reasoninghow small can a LLM be to achieve certain capabilities  \n\n{% asset_img phi_15_result.png phi %}  \n\n##   \n\nphi-1.5phi-17B token20Bcommon sense reasoninggeneral knowledge  \n\n20B20k topicspromptweb datasetsample  \n\nIt requires intricate iterations, strategic topic selection, and a deep understanding of knowledge gaps to ensure quality and diversity of the data.  \n\n##   \n\nphi-1.5  \n-   \n- max lr = 2e-4  \n- no warmup  \n- AdamWbeta_1 = 0.9, beta_2 = 0.98, epsilon = 1e-7  \n- DeepSpeed ZeRO stage 2  \n- fp16  \n- batch size = 2048  \n\n150Bepoch20%phi-180%  \n\n## filtered web data  \n\ntraditional web dataphi-1.5-web-onlyweb dataphi-1.5-webphi-1web data2:4:4filtered web data95B88BFalcon refined web dataset7BThe StackStackOverflow  \n\n##   \n\nphi-1.5phi-1.5-webphi-1.5-web-onlycommon sensebenchmark  \n\n{% asset_img phi_15_bench_1.png phi %}  \n\n- phi-1.5-web-only  \n- phi-1.5-webphi-1.5  \n\nlanguage understanding taskphi-1.5  \n\n{% asset_img phi_15_bench_2.png phi %}  \n\nreasoning ability  \n\n{% asset_img phi_15_bench_3.png phi %}  \n\n- phi-1.5reasoning  \n- phi-1.5-webphi-1.5web datareasoning  \n- phi-1.5phi-1  \n\n# phi-2  \n\nphi-22.7Bphi-1.5scale up  \n\n## scale up  \n\nphi-1-smallphi-1-basetrain from scratch  \n\n{% asset_img phi_2_0.png phi %}  \n\nhidden size  \n\n1scaling number of layers  \n\nScaling language models: Methods, analysis & insights from training gopherphi-1.520-->24  \n\nround_int(range(num_layers_new)/num_layers_new * num_layers_old)  \n\n2Scaling attention layer dimensions  \n\nQKVweight reuseWR  \n\n{% asset_img phi_2_1.png phi %}  \n\ntiling  \n\n{% asset_img phi_2_2.png phi %}  \n\nWRWR + tiling  \n\n{% asset_img phi_2_3.png phi %}  \n\n## phi-2  \n\nWR + tilingphi-1.5phi-22.7B  \n\n{% asset_img phi_2.png phi %}  \n\n# phi-3  \n\nphi-33  \n- phi-3-mini3.8B  \n- phi-3-small7B  \n- phi-3-medium14B  \n\n##   \n\n- phi-3Llama-2  \n- (minismall & medium)  \n- LongRoPE128k  \n- phi-3-smallGQA  \n\nblocksparse attentionKV cacheKV block  \n\n{% asset_img phi_3_sparse.png phi %}  \n\n##  &   \n\nTextbooks Are All You Needphi-3heavily filtered publicly web dataeducational level  \n\nphase  \n- phase1web sources &   \n- phase2heavily filtered webdataphase-1reasoning  \n\nphi-3-mini3.3T  \n\ncompute optimaldata optimal  \n\n  \n\nbenchmarkphi-3-medium14Bphi-3-small7Bphi-3-smallphi-3-miniphi-3-mediumdata optimal  \n\n##   \n\nMMLUphiLlama-2  \n\n{% asset_img phi_3_result.png phi %}  \n\nphidata optimal regime  \n\n#   \n\nphi/  \n\nA Careful Examination of Large Language Model Performance on Grade School ArithmeticGSM8kGSM1kGSM1kGSM8k  \n\nGSM8kGSM1kGSM8kGSM1k  \n\nMixtralphiLlama  \n\n{% asset_img overfit.png phi %}  \n\nphi-3GSM1kGSM8kgapphi-3  \n\nphiChatGPT  \n\nClaudeGPT-4Llama  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n- MoE  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[denseMoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  \n[MoE--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  \n-   \n[--AFM](https://www.linsight.cn/1e34e252.html)  \n[--MobileLLM](https://www.linsight.cn/5ac36d34.html)  \n-   \n[Llama3.1--](https://www.linsight.cn/7d7294cb.html)  \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n-   \n[--](https://www.linsight.cn/210dbccd.html)  \n-   \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n-   \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n-   \n[Llama3.1--post-training](https://www.linsight.cn/93328a2a.html)  \n[ -- model soup](https://www.linsight.cn/bb8fcf21.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n- Transformer  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n-   \n[(1)](http://www.linsight.cn/3345028a.html)\n[(2)](http://www.linsight.cn/ad0bba9d.html)\n[(3)](http://www.linsight.cn/1736008.html)\n[(4)](http://www.linsight.cn/1736008.html)\n[(5)](http://www.linsight.cn/336f2f3e.html)\n[(6)](http://www.linsight.cn/7c04944d.html)\n[(7)](https://www.linsight.cn/dd614e12.html)\n[(8)](https://www.linsight.cn/e287b9c3.html)\n[(9)](https://www.linsight.cn/fb9c8882.html)  \n\n# Reference  \n\n1Textbooks Are All You Need https://arxiv.org/abs/2306.11644  \n2Textbooks Are All You Need II: phi-1.5 technical report https://arxiv.org/abs/2309.05463  \n3Phi-2: The Surprising Power of Small Language Models https://nips.cc/media/neurips-2023/Slides/83968_5GxuY2z.pdf  \n4Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone https://arxiv.org/abs/2404.14219  \n5A Careful Examination of Large Language Model Performance on Grade School Arithmetic https://arxiv.org/abs/2405.00332  \n","source":"_posts/cs/nlp/2024/08/phi.md","raw":"---\ntitle: phi\ntags:\n  - NLP\n  - LLM\n  - transformer\n  - \n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: fe13b56f\ndate: 2024-08-13 20:41:06\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\n***  \n\nphi  \n\n# phi-1  \n\nphi-1350Mphi-1-small1.3Bphi-1-base  \n\nSLM/LLMphi-17B  \n- 6Bweb  \n- 1BGPT-3.5  \n\nA100*84  \n\nphi-1codepythonphi-1-baseLM/LMHumanEvalMBPP  \n\n{% asset_img phi_1_result.png phi %}  \n\n##   \n\nphi-1decoder-onlyphi-1  \n\n|   | phi-1-base | phi-1-small |\n| ---- | ---- | ---- |\n|  | 1.3B | 350M |\n|  | 24 | 20 |\n| hidden size | 2048 | 1024 |\n| intermediate size | 8192 | 4096 |\n| attention head num | 32 | 16 |\n\nMHARoPEdimension3264  \n\ntokenizercodegen-350M-mono  \n\n##   \n\nscaling lawphi-1  \n\nTinystories: How small can language models be and still speak coherent english?scaling law  \n\nTextbooks Are All You Needphi-1textbook  \n\n1  \n\nThe StackStackOverflowCodeContest  \n- not self-contained  \n- not meaningfulGUI  \n-   \n- topic  \n\n{% asset_img phi_1_code_case.png phi %}  \n\n  \n\ntextbook  \n\n2  \n\n  \n\nThe StackStackOverflowpython35M35B100kGPT-4determine its educational value for a student whose goal is to learn basic coding concepts  \n\nprompt  \n\n100kcodegenoutput embeddingfeaturerandom forest classifier35M  \n\nstep96k vs 36k  \n\n{% asset_img phi_1_compare.png phi %}  \n\n3synthetic textbook dataset  \n\n  \n\n\n- topiccase  \n- LLM  \n\nphi-1TinystoriespromptGPT-3.5topic1B token  \n\n  \n\n{% asset_img phi_1_example_1.png phi %}  \n\n4CodeExercises dataset  \n\nSFT  \n\nGPT-3.5180M tokenfunction name  \n\n{% asset_img phi_1_example_2.png phi %}  \n\n##   \n\nphi-1  \n- AdamW optimizerweight decay = 0.1  \n- linear-warmup-linear-decay learning rate schedule  \n- dropout = 0.1  \n-  = 2048  \n-  = fp16  \n\n36000 step24000 stepcheckpoint7B8epoch50B  \n- batch size = 1024  \n- max lr = 1e-3  \n- warmup step = 750  \n\n6000step  \n- batch size = 256  \n- max lr = 1e-4  \n- warmup step = 50  \n\n  \n\n##   \n\n- phi-1prompt  \n- Meta  \n- scaling up  \n\n# phi-1.5  \n\nphi-1.5phi-1phi-1 + common sense reasoninghow small can a LLM be to achieve certain capabilities  \n\n{% asset_img phi_15_result.png phi %}  \n\n##   \n\nphi-1.5phi-17B token20Bcommon sense reasoninggeneral knowledge  \n\n20B20k topicspromptweb datasetsample  \n\nIt requires intricate iterations, strategic topic selection, and a deep understanding of knowledge gaps to ensure quality and diversity of the data.  \n\n##   \n\nphi-1.5  \n-   \n- max lr = 2e-4  \n- no warmup  \n- AdamWbeta_1 = 0.9, beta_2 = 0.98, epsilon = 1e-7  \n- DeepSpeed ZeRO stage 2  \n- fp16  \n- batch size = 2048  \n\n150Bepoch20%phi-180%  \n\n## filtered web data  \n\ntraditional web dataphi-1.5-web-onlyweb dataphi-1.5-webphi-1web data2:4:4filtered web data95B88BFalcon refined web dataset7BThe StackStackOverflow  \n\n##   \n\nphi-1.5phi-1.5-webphi-1.5-web-onlycommon sensebenchmark  \n\n{% asset_img phi_15_bench_1.png phi %}  \n\n- phi-1.5-web-only  \n- phi-1.5-webphi-1.5  \n\nlanguage understanding taskphi-1.5  \n\n{% asset_img phi_15_bench_2.png phi %}  \n\nreasoning ability  \n\n{% asset_img phi_15_bench_3.png phi %}  \n\n- phi-1.5reasoning  \n- phi-1.5-webphi-1.5web datareasoning  \n- phi-1.5phi-1  \n\n# phi-2  \n\nphi-22.7Bphi-1.5scale up  \n\n## scale up  \n\nphi-1-smallphi-1-basetrain from scratch  \n\n{% asset_img phi_2_0.png phi %}  \n\nhidden size  \n\n1scaling number of layers  \n\nScaling language models: Methods, analysis & insights from training gopherphi-1.520-->24  \n\nround_int(range(num_layers_new)/num_layers_new * num_layers_old)  \n\n2Scaling attention layer dimensions  \n\nQKVweight reuseWR  \n\n{% asset_img phi_2_1.png phi %}  \n\ntiling  \n\n{% asset_img phi_2_2.png phi %}  \n\nWRWR + tiling  \n\n{% asset_img phi_2_3.png phi %}  \n\n## phi-2  \n\nWR + tilingphi-1.5phi-22.7B  \n\n{% asset_img phi_2.png phi %}  \n\n# phi-3  \n\nphi-33  \n- phi-3-mini3.8B  \n- phi-3-small7B  \n- phi-3-medium14B  \n\n##   \n\n- phi-3Llama-2  \n- (minismall & medium)  \n- LongRoPE128k  \n- phi-3-smallGQA  \n\nblocksparse attentionKV cacheKV block  \n\n{% asset_img phi_3_sparse.png phi %}  \n\n##  &   \n\nTextbooks Are All You Needphi-3heavily filtered publicly web dataeducational level  \n\nphase  \n- phase1web sources &   \n- phase2heavily filtered webdataphase-1reasoning  \n\nphi-3-mini3.3T  \n\ncompute optimaldata optimal  \n\n  \n\nbenchmarkphi-3-medium14Bphi-3-small7Bphi-3-smallphi-3-miniphi-3-mediumdata optimal  \n\n##   \n\nMMLUphiLlama-2  \n\n{% asset_img phi_3_result.png phi %}  \n\nphidata optimal regime  \n\n#   \n\nphi/  \n\nA Careful Examination of Large Language Model Performance on Grade School ArithmeticGSM8kGSM1kGSM1kGSM8k  \n\nGSM8kGSM1kGSM8kGSM1k  \n\nMixtralphiLlama  \n\n{% asset_img overfit.png phi %}  \n\nphi-3GSM1kGSM8kgapphi-3  \n\nphiChatGPT  \n\nClaudeGPT-4Llama  \n\n***  \n\n~\n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n- MoE  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[denseMoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  \n[MoE--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  \n-   \n[--AFM](https://www.linsight.cn/1e34e252.html)  \n[--MobileLLM](https://www.linsight.cn/5ac36d34.html)  \n-   \n[Llama3.1--](https://www.linsight.cn/7d7294cb.html)  \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n-   \n[--](https://www.linsight.cn/210dbccd.html)  \n-   \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n-   \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n-   \n[Llama3.1--post-training](https://www.linsight.cn/93328a2a.html)  \n[ -- model soup](https://www.linsight.cn/bb8fcf21.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n- Transformer  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n-   \n[(1)](http://www.linsight.cn/3345028a.html)\n[(2)](http://www.linsight.cn/ad0bba9d.html)\n[(3)](http://www.linsight.cn/1736008.html)\n[(4)](http://www.linsight.cn/1736008.html)\n[(5)](http://www.linsight.cn/336f2f3e.html)\n[(6)](http://www.linsight.cn/7c04944d.html)\n[(7)](https://www.linsight.cn/dd614e12.html)\n[(8)](https://www.linsight.cn/e287b9c3.html)\n[(9)](https://www.linsight.cn/fb9c8882.html)  \n\n# Reference  \n\n1Textbooks Are All You Need https://arxiv.org/abs/2306.11644  \n2Textbooks Are All You Need II: phi-1.5 technical report https://arxiv.org/abs/2309.05463  \n3Phi-2: The Surprising Power of Small Language Models https://nips.cc/media/neurips-2023/Slides/83968_5GxuY2z.pdf  \n4Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone https://arxiv.org/abs/2404.14219  \n5A Careful Examination of Large Language Model Performance on Grade School Arithmetic https://arxiv.org/abs/2405.00332  \n","slug":"cs/nlp/2024/08/phi","published":1,"updated":"2024-08-13T13:33:05.983Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsnt00gp0p4k1f5c31jk","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>phi</p>\n<h1 id=\"phi-1\">phi-1</h1>\n<p>phi-1350Mphi-1-small1.3Bphi-1-base</p>\n<p>SLM/LLMphi-17B<br>\n- 6Bweb<br>\n- 1BGPT-3.5</p>\n<p>A100*84</p>\n<p>phi-1codepythonphi-1-baseLM/LMHumanEvalMBPP</p>\n<img src=\"/fe13b56f/phi_1_result.png\" class title=\"phi\">\n<h2 id=\"\"></h2>\n<p>phi-1decoder-onlyphi-1</p>\n<table>\n<thead>\n<tr class=\"header\">\n<th></th>\n<th>phi-1-base</th>\n<th>phi-1-small</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td></td>\n<td>1.3B</td>\n<td>350M</td>\n</tr>\n<tr class=\"even\">\n<td></td>\n<td>24</td>\n<td>20</td>\n</tr>\n<tr class=\"odd\">\n<td>hidden size</td>\n<td>2048</td>\n<td>1024</td>\n</tr>\n<tr class=\"even\">\n<td>intermediate size</td>\n<td>8192</td>\n<td>4096</td>\n</tr>\n<tr class=\"odd\">\n<td>attention head num</td>\n<td>32</td>\n<td>16</td>\n</tr>\n</tbody>\n</table>\n<p>MHARoPEdimension3264</p>\n<p>tokenizercodegen-350M-mono</p>\n<h2 id=\"\"></h2>\n<p>scaling\nlawphi-1</p>\n<p>Tinystories: How small can language models be and still speak\ncoherent english?scaling\nlaw</p>\n<p>Textbooks Are All You\nNeedphi-1textbook</p>\n<p>1</p>\n<p>The\nStackStackOverflowCodeContest<br>\n- not\nself-contained<br>\n- not\nmeaningfulGUI<br>\n-\n<br>\n- topic</p>\n<img src=\"/fe13b56f/phi_1_code_case.png\" class title=\"phi\">\n<p></p>\n<p>textbook</p>\n<p>2</p>\n<p></p>\n<p>The\nStackStackOverflowpython35M35B100kGPT-4determine\nits educational value for a student whose goal is to learn basic coding\nconcepts</p>\n<p>prompt</p>\n<p>100kcodegenoutput\nembeddingfeaturerandom forest\nclassifier35M</p>\n<p>step96k\nvs 36k</p>\n<img src=\"/fe13b56f/phi_1_compare.png\" class title=\"phi\">\n<p>3synthetic textbook dataset</p>\n<p></p>\n<p> -\ntopiccase<br>\n-\nLLM</p>\n<p>phi-1TinystoriespromptGPT-3.5topic1B\ntoken</p>\n<p></p>\n<img src=\"/fe13b56f/phi_1_example_1.png\" class title=\"phi\">\n<p>4CodeExercises dataset</p>\n<p>SFT</p>\n<p>GPT-3.5180M\ntokenfunction\nname</p>\n<img src=\"/fe13b56f/phi_1_example_2.png\" class title=\"phi\">\n<h2 id=\"\"></h2>\n<p>phi-1<br>\n- AdamW optimizerweight decay = 0.1<br>\n- linear-warmup-linear-decay learning rate schedule<br>\n- dropout = 0.1<br>\n-  = 2048<br>\n-  = fp16</p>\n<p>36000 step24000\nstepcheckpoint7B8epoch50B<br>\n- batch size = 1024<br>\n- max lr = 1e-3<br>\n- warmup step = 750</p>\n<p>6000step<br>\n- batch size = 256<br>\n- max lr = 1e-4<br>\n- warmup step = 50</p>\n<p></p>\n<h2 id=\"\"></h2>\n<ul>\n<li>phi-1prompt<br>\n</li>\n<li>Meta<br>\n</li>\n<li>scaling\nup</li>\n</ul>\n<h1 id=\"phi-1.5\">phi-1.5</h1>\n<p>phi-1.5phi-1phi-1\n+ common sense reasoninghow small can a LLM be to achieve certain\ncapabilities</p>\n<img src=\"/fe13b56f/phi_15_result.png\" class title=\"phi\">\n<h2 id=\"-1\"></h2>\n<p>phi-1.5phi-17B\ntoken20Bcommon sense\nreasoninggeneral knowledge</p>\n<p>20B20k topicspromptweb\ndatasetsample</p>\n<p>It requires intricate iterations,\nstrategic topic selection, and a deep understanding of knowledge gaps to\nensure quality and diversity of the data.</p>\n<h2 id=\"-1\"></h2>\n<p>phi-1.5<br>\n- <br>\n- max lr = 2e-4<br>\n- no warmup<br>\n- AdamWbeta_1 = 0.9, beta_2 = 0.98, epsilon = 1e-7<br>\n- DeepSpeed ZeRO stage 2<br>\n- fp16<br>\n- batch size = 2048</p>\n<p>150Bepoch20%phi-180%</p>\n<h2 id=\"filtered-web-data\">filtered web data</h2>\n<p>traditional web\ndataphi-1.5-web-onlyweb\ndataphi-1.5-webphi-1web\ndata2:4:4filtered web data95B88BFalcon\nrefined web dataset7BThe StackStackOverflow</p>\n<h2 id=\"\"></h2>\n<p>phi-1.5phi-1.5-webphi-1.5-web-onlycommon\nsensebenchmark</p>\n<img src=\"/fe13b56f/phi_15_bench_1.png\" class title=\"phi\">\n<ul>\n<li>phi-1.5-web-only<br>\n</li>\n<li>phi-1.5-webphi-1.5</li>\n</ul>\n<p>language understanding taskphi-1.5</p>\n<img src=\"/fe13b56f/phi_15_bench_2.png\" class title=\"phi\">\n<p>reasoning\nability</p>\n<img src=\"/fe13b56f/phi_15_bench_3.png\" class title=\"phi\">\n<ul>\n<li>phi-1.5reasoning<br>\n</li>\n<li>phi-1.5-webphi-1.5web\ndatareasoning<br>\n</li>\n<li>phi-1.5phi-1</li>\n</ul>\n<h1 id=\"phi-2\">phi-2</h1>\n<p>phi-22.7Bphi-1.5scale up</p>\n<h2 id=\"scale-up\">scale up</h2>\n<p>phi-1-smallphi-1-basetrain from\nscratch</p>\n<img src=\"/fe13b56f/phi_2_0.png\" class title=\"phi\">\n<p>hidden\nsize</p>\n<p>1scaling number of layers</p>\n<p>Scaling language models: Methods, analysis &amp; insights from\ntraining\ngopherphi-1.520--&gt;24</p>\n<p>round_int(range(num_layers_new)/num_layers_new * num_layers_old)</p>\n<p>2Scaling attention layer dimensions</p>\n<p>QKVweight\nreuseWR</p>\n<img src=\"/fe13b56f/phi_2_1.png\" class title=\"phi\">\n<p>tiling</p>\n<img src=\"/fe13b56f/phi_2_2.png\" class title=\"phi\">\n<p>WRWR + tiling</p>\n<img src=\"/fe13b56f/phi_2_3.png\" class title=\"phi\">\n<h2 id=\"phi-2-1\">phi-2</h2>\n<p>WR + tilingphi-1.5phi-22.7B</p>\n<img src=\"/fe13b56f/phi_2.png\" class title=\"phi\">\n<h1 id=\"phi-3\">phi-3</h1>\n<p>phi-33<br>\n- phi-3-mini3.8B<br>\n- phi-3-small7B<br>\n- phi-3-medium14B</p>\n<h2 id=\"-1\"></h2>\n<ul>\n<li>phi-3Llama-2<br>\n</li>\n<li>(minismall &amp; medium)<br>\n</li>\n<li>LongRoPE128k<br>\n</li>\n<li>phi-3-smallGQA</li>\n</ul>\n<p>blocksparse attentionKV\ncacheKV\nblock</p>\n<img src=\"/fe13b56f/phi_3_sparse.png\" class title=\"phi\">\n<h2 id=\"-\"> &amp; </h2>\n<p>Textbooks Are All You Needphi-3heavily\nfiltered publicly web dataeducational\nlevel</p>\n<p>phase<br>\n- phase1web sources &amp; <br>\n- phase2heavily filtered\nwebdataphase-1reasoning</p>\n<p>phi-3-mini3.3T</p>\n<p>compute optimaldata\noptimal</p>\n<p></p>\n<p>benchmarkphi-3-medium14Bphi-3-small7Bphi-3-smallphi-3-miniphi-3-mediumdata\noptimal</p>\n<h2 id=\"-1\"></h2>\n<p>MMLUphiLlama-2</p>\n<img src=\"/fe13b56f/phi_3_result.png\" class title=\"phi\">\n<p>phidata optimal regime</p>\n<h1 id=\"\"></h1>\n<p>phi/</p>\n<p>A Careful Examination of Large Language Model Performance on Grade\nSchool\nArithmeticGSM8kGSM1kGSM1kGSM8k</p>\n<p>GSM8kGSM1kGSM8kGSM1k</p>\n<p>MixtralphiLlama</p>\n<img src=\"/fe13b56f/overfit.png\" class title=\"phi\">\n<p>phi-3GSM1kGSM8kgapphi-3</p>\n<p>phiChatGPT</p>\n<p>ClaudeGPT-4Llama</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n- MoE<br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/a0824e29.html\">denseMoE -- sparse\nupcycling</a><br>\n<a href=\"https://www.linsight.cn/2c8bbc7.html\">MoE--expert choice\nrouting</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/1e34e252.html\">--AFM</a><br>\n<a href=\"https://www.linsight.cn/5ac36d34.html\">--MobileLLM</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/7d7294cb.html\">Llama3.1--</a><br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/210dbccd.html\">--</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/93328a2a.html\">Llama3.1--post-training</a><br>\n<a href=\"https://www.linsight.cn/bb8fcf21.html\"> -- model\nsoup</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n- Transformer<br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a> <a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a> <a href=\"http://www.linsight.cn/1736008.html\">(3)</a> <a href=\"http://www.linsight.cn/1736008.html\">(4)</a> <a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a> <a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a> <a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a> <a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a> <a href=\"https://www.linsight.cn/fb9c8882.html\">(9)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Textbooks Are All You Need\nhttps://arxiv.org/abs/2306.11644<br>\n2Textbooks Are All You Need II: phi-1.5 technical report\nhttps://arxiv.org/abs/2309.05463<br>\n3Phi-2: The Surprising Power of Small Language Models\nhttps://nips.cc/media/neurips-2023/Slides/83968_5GxuY2z.pdf<br>\n4Phi-3 Technical Report: A Highly Capable Language Model Locally on\nYour Phone https://arxiv.org/abs/2404.14219<br>\n5A Careful Examination of Large Language Model Performance on Grade\nSchool Arithmetic https://arxiv.org/abs/2405.00332</p>\n","length":7062,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<hr>\n<p>phi</p>\n<h1 id=\"phi-1\">phi-1</h1>\n<p>phi-1350Mphi-1-small1.3Bphi-1-base</p>\n<p>SLM/LLMphi-17B<br>\n- 6Bweb<br>\n- 1BGPT-3.5</p>\n<p>A100*84</p>\n<p>phi-1codepythonphi-1-baseLM/LMHumanEvalMBPP</p>\n<img src=\"/fe13b56f/phi_1_result.png\" class title=\"phi\">\n<h2 id=\"\"></h2>\n<p>phi-1decoder-onlyphi-1</p>\n<table>\n<thead>\n<tr class=\"header\">\n<th></th>\n<th>phi-1-base</th>\n<th>phi-1-small</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td></td>\n<td>1.3B</td>\n<td>350M</td>\n</tr>\n<tr class=\"even\">\n<td></td>\n<td>24</td>\n<td>20</td>\n</tr>\n<tr class=\"odd\">\n<td>hidden size</td>\n<td>2048</td>\n<td>1024</td>\n</tr>\n<tr class=\"even\">\n<td>intermediate size</td>\n<td>8192</td>\n<td>4096</td>\n</tr>\n<tr class=\"odd\">\n<td>attention head num</td>\n<td>32</td>\n<td>16</td>\n</tr>\n</tbody>\n</table>\n<p>MHARoPEdimension3264</p>\n<p>tokenizercodegen-350M-mono</p>\n<h2 id=\"\"></h2>\n<p>scaling\nlawphi-1</p>\n<p>Tinystories: How small can language models be and still speak\ncoherent english?scaling\nlaw</p>\n<p>Textbooks Are All You\nNeedphi-1textbook</p>\n<p>1</p>\n<p>The\nStackStackOverflowCodeContest<br>\n- not\nself-contained<br>\n- not\nmeaningfulGUI<br>\n-\n<br>\n- topic</p>\n<img src=\"/fe13b56f/phi_1_code_case.png\" class title=\"phi\">\n<p></p>\n<p>textbook</p>\n<p>2</p>\n<p></p>\n<p>The\nStackStackOverflowpython35M35B100kGPT-4determine\nits educational value for a student whose goal is to learn basic coding\nconcepts</p>\n<p>prompt</p>\n<p>100kcodegenoutput\nembeddingfeaturerandom forest\nclassifier35M</p>\n<p>step96k\nvs 36k</p>\n<img src=\"/fe13b56f/phi_1_compare.png\" class title=\"phi\">\n<p>3synthetic textbook dataset</p>\n<p></p>\n<p> -\ntopiccase<br>\n-\nLLM</p>\n<p>phi-1TinystoriespromptGPT-3.5topic1B\ntoken</p>\n<p></p>\n<img src=\"/fe13b56f/phi_1_example_1.png\" class title=\"phi\">\n<p>4CodeExercises dataset</p>\n<p>SFT</p>\n<p>GPT-3.5180M\ntokenfunction\nname</p>\n<img src=\"/fe13b56f/phi_1_example_2.png\" class title=\"phi\">\n<h2 id=\"\"></h2>\n<p>phi-1<br>\n- AdamW optimizerweight decay = 0.1<br>\n- linear-warmup-linear-decay learning rate schedule<br>\n- dropout = 0.1<br>\n-  = 2048<br>\n-  = fp16</p>\n<p>36000 step24000\nstepcheckpoint7B8epoch50B<br>\n- batch size = 1024<br>\n- max lr = 1e-3<br>\n- warmup step = 750</p>\n<p>6000step<br>\n- batch size = 256<br>\n- max lr = 1e-4<br>\n- warmup step = 50</p>\n<p></p>\n<h2 id=\"\"></h2>\n<ul>\n<li>phi-1prompt<br>\n</li>\n<li>Meta<br>\n</li>\n<li>scaling\nup</li>\n</ul>\n<h1 id=\"phi-1.5\">phi-1.5</h1>\n<p>phi-1.5phi-1phi-1\n+ common sense reasoninghow small can a LLM be to achieve certain\ncapabilities</p>\n<img src=\"/fe13b56f/phi_15_result.png\" class title=\"phi\">\n<h2 id=\"-1\"></h2>\n<p>phi-1.5phi-17B\ntoken20Bcommon sense\nreasoninggeneral knowledge</p>\n<p>20B20k topicspromptweb\ndatasetsample</p>\n<p>It requires intricate iterations,\nstrategic topic selection, and a deep understanding of knowledge gaps to\nensure quality and diversity of the data.</p>\n<h2 id=\"-1\"></h2>\n<p>phi-1.5<br>\n- <br>\n- max lr = 2e-4<br>\n- no warmup<br>\n- AdamWbeta_1 = 0.9, beta_2 = 0.98, epsilon = 1e-7<br>\n- DeepSpeed ZeRO stage 2<br>\n- fp16<br>\n- batch size = 2048</p>\n<p>150Bepoch20%phi-180%</p>\n<h2 id=\"filtered-web-data\">filtered web data</h2>\n<p>traditional web\ndataphi-1.5-web-onlyweb\ndataphi-1.5-webphi-1web\ndata2:4:4filtered web data95B88BFalcon\nrefined web dataset7BThe StackStackOverflow</p>\n<h2 id=\"\"></h2>\n<p>phi-1.5phi-1.5-webphi-1.5-web-onlycommon\nsensebenchmark</p>\n<img src=\"/fe13b56f/phi_15_bench_1.png\" class title=\"phi\">\n<ul>\n<li>phi-1.5-web-only<br>\n</li>\n<li>phi-1.5-webphi-1.5</li>\n</ul>\n<p>language understanding taskphi-1.5</p>\n<img src=\"/fe13b56f/phi_15_bench_2.png\" class title=\"phi\">\n<p>reasoning\nability</p>\n<img src=\"/fe13b56f/phi_15_bench_3.png\" class title=\"phi\">\n<ul>\n<li>phi-1.5reasoning<br>\n</li>\n<li>phi-1.5-webphi-1.5web\ndatareasoning<br>\n</li>\n<li>phi-1.5phi-1</li>\n</ul>\n<h1 id=\"phi-2\">phi-2</h1>\n<p>phi-22.7Bphi-1.5scale up</p>\n<h2 id=\"scale-up\">scale up</h2>\n<p>phi-1-smallphi-1-basetrain from\nscratch</p>\n<img src=\"/fe13b56f/phi_2_0.png\" class title=\"phi\">\n<p>hidden\nsize</p>\n<p>1scaling number of layers</p>\n<p>Scaling language models: Methods, analysis &amp; insights from\ntraining\ngopherphi-1.520--&gt;24</p>\n<p>round_int(range(num_layers_new)/num_layers_new * num_layers_old)</p>\n<p>2Scaling attention layer dimensions</p>\n<p>QKVweight\nreuseWR</p>\n<img src=\"/fe13b56f/phi_2_1.png\" class title=\"phi\">\n<p>tiling</p>\n<img src=\"/fe13b56f/phi_2_2.png\" class title=\"phi\">\n<p>WRWR + tiling</p>\n<img src=\"/fe13b56f/phi_2_3.png\" class title=\"phi\">\n<h2 id=\"phi-2-1\">phi-2</h2>\n<p>WR + tilingphi-1.5phi-22.7B</p>\n<img src=\"/fe13b56f/phi_2.png\" class title=\"phi\">\n<h1 id=\"phi-3\">phi-3</h1>\n<p>phi-33<br>\n- phi-3-mini3.8B<br>\n- phi-3-small7B<br>\n- phi-3-medium14B</p>\n<h2 id=\"-1\"></h2>\n<ul>\n<li>phi-3Llama-2<br>\n</li>\n<li>(minismall &amp; medium)<br>\n</li>\n<li>LongRoPE128k<br>\n</li>\n<li>phi-3-smallGQA</li>\n</ul>\n<p>blocksparse attentionKV\ncacheKV\nblock</p>\n<img src=\"/fe13b56f/phi_3_sparse.png\" class title=\"phi\">\n<h2 id=\"-\"> &amp; </h2>\n<p>Textbooks Are All You Needphi-3heavily\nfiltered publicly web dataeducational\nlevel</p>\n<p>phase<br>\n- phase1web sources &amp; <br>\n- phase2heavily filtered\nwebdataphase-1reasoning</p>\n<p>phi-3-mini3.3T</p>\n<p>compute optimaldata\noptimal</p>\n<p></p>\n<p>benchmarkphi-3-medium14Bphi-3-small7Bphi-3-smallphi-3-miniphi-3-mediumdata\noptimal</p>\n<h2 id=\"-1\"></h2>\n<p>MMLUphiLlama-2</p>\n<img src=\"/fe13b56f/phi_3_result.png\" class title=\"phi\">\n<p>phidata optimal regime</p>\n<h1 id=\"\"></h1>\n<p>phi/</p>\n<p>A Careful Examination of Large Language Model Performance on Grade\nSchool\nArithmeticGSM8kGSM1kGSM1kGSM8k</p>\n<p>GSM8kGSM1kGSM8kGSM1k</p>\n<p>MixtralphiLlama</p>\n<img src=\"/fe13b56f/overfit.png\" class title=\"phi\">\n<p>phi-3GSM1kGSM8kgapphi-3</p>\n<p>phiChatGPT</p>\n<p>ClaudeGPT-4Llama</p>\n<hr>\n<p>~</p>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n- MoE<br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/a0824e29.html\">denseMoE -- sparse\nupcycling</a><br>\n<a href=\"https://www.linsight.cn/2c8bbc7.html\">MoE--expert choice\nrouting</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/1e34e252.html\">--AFM</a><br>\n<a href=\"https://www.linsight.cn/5ac36d34.html\">--MobileLLM</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/7d7294cb.html\">Llama3.1--</a><br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/210dbccd.html\">--</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/93328a2a.html\">Llama3.1--post-training</a><br>\n<a href=\"https://www.linsight.cn/bb8fcf21.html\"> -- model\nsoup</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n- Transformer<br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a> <a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a> <a href=\"http://www.linsight.cn/1736008.html\">(3)</a> <a href=\"http://www.linsight.cn/1736008.html\">(4)</a> <a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a> <a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a> <a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a> <a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a> <a href=\"https://www.linsight.cn/fb9c8882.html\">(9)</a></p>\n<h1 id=\"reference\">Reference</h1>\n<p>1Textbooks Are All You Need\nhttps://arxiv.org/abs/2306.11644<br>\n2Textbooks Are All You Need II: phi-1.5 technical report\nhttps://arxiv.org/abs/2309.05463<br>\n3Phi-2: The Surprising Power of Small Language Models\nhttps://nips.cc/media/neurips-2023/Slides/83968_5GxuY2z.pdf<br>\n4Phi-3 Technical Report: A Highly Capable Language Model Locally on\nYour Phone https://arxiv.org/abs/2404.14219<br>\n5A Careful Examination of Large Language Model Performance on Grade\nSchool Arithmetic https://arxiv.org/abs/2405.00332</p>\n"},{"title":"(9)","abbrlink":"fb9c8882","date":"2024-08-06T13:28:26.000Z","_content":"\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~  \n\n~\n\n***  \n\n# 1.  \n\n1LoRAadaptor  \n\n2flash attentionpaged attentionring attention  \n\n3ZeROrecomputationcpu-offload  \n\n4  \n\n# 2.MoEdense  \n\nfloat32/float16/bfloat16MoEgating functionexponentialexponentialgating functionMoE  \n\n# 3.T  \n\nstudent modelteacher modelsoftmaxteacher modelsoftmaxT>1  \n\nteacher modelgroud truthone-hot label []teacher model  [0.7, 0.2, 0.1] 0.70.20.1one-hot label  \n\nTteacher modellabelsoftstudent model  \n\n# 4.  \n\n1.  \n\n2.MoE  \n\n3.early-exit  \n\n4.  \n\n# 5.LLAMA1/2intermiedia size/hidden sizeBert48/3  \n\nLLAMASwiGLU FFNBertFFNintermediate size4hidden size8/3  \n\n***  \n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n- MoE  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[denseMoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  \n[MoE--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  \n-   \n[--AFM](https://www.linsight.cn/1e34e252.html)  \n[--MobileLLM](https://www.linsight.cn/5ac36d34.html)  \n-   \n[Llama3.1--](https://www.linsight.cn/7d7294cb.html)  \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n-   \n[--](https://www.linsight.cn/210dbccd.html)  \n-   \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n-   \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n-   \n[Llama3.1--post-training](https://www.linsight.cn/93328a2a.html)  \n[ -- model soup](https://www.linsight.cn/bb8fcf21.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n- Transformer  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n-   \n[(1)](http://www.linsight.cn/3345028a.html)\n[(2)](http://www.linsight.cn/ad0bba9d.html)\n[(3)](http://www.linsight.cn/1736008.html)\n[(4)](http://www.linsight.cn/1736008.html)\n[(5)](http://www.linsight.cn/336f2f3e.html)\n[(6)](http://www.linsight.cn/7c04944d.html)\n[(7)](https://www.linsight.cn/dd614e12.html)\n[(8)](https://www.linsight.cn/e287b9c3.html)  \n","source":"_posts/cs/nlp/2024/08/-9.md","raw":"---\ntitle: (9)\ntags:\n  - NLP\n  - LLM\n  - \ncategories:\n  - CS\n  - NLP\n  - LLM\nabbrlink: fb9c8882\ndate: 2024-08-06 21:28:26\n---\n\n  /  / [linsight.cn](http://www.linsight.cn/)   \n\nLLM~  \n\n~\n\n***  \n\n# 1.  \n\n1LoRAadaptor  \n\n2flash attentionpaged attentionring attention  \n\n3ZeROrecomputationcpu-offload  \n\n4  \n\n# 2.MoEdense  \n\nfloat32/float16/bfloat16MoEgating functionexponentialexponentialgating functionMoE  \n\n# 3.T  \n\nstudent modelteacher modelsoftmaxteacher modelsoftmaxT>1  \n\nteacher modelgroud truthone-hot label []teacher model  [0.7, 0.2, 0.1] 0.70.20.1one-hot label  \n\nTteacher modellabelsoftstudent model  \n\n# 4.  \n\n1.  \n\n2.MoE  \n\n3.early-exit  \n\n4.  \n\n# 5.LLAMA1/2intermiedia size/hidden sizeBert48/3  \n\nLLAMASwiGLU FFNBertFFNintermediate size4hidden size8/3  \n\n***  \n\n[http://www.linsight.cn/](http://www.linsight.cn/)  \n[Linsight](https://www.zhihu.com/people/us4ever)  \nLinsight  \n![](/images/qrcode.jpg)  \n\n***  \n\n  \n- MoE  \n[MoE](http://www.linsight.cn/44e38c1b.html)  \n[DeepSeek-V2MLA](https://www.linsight.cn/83c49df0.html)  \n[-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  \n[10wJetMoE](https://www.linsight.cn/f3acf042.html)  \n[MoEtop-p routing](https://www.linsight.cn/224c42da.html)  \n[MoE](https://www.linsight.cn/5e1d14b3.html)  \n[denseMoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  \n[MoE--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  \n-   \n[--AFM](https://www.linsight.cn/1e34e252.html)  \n[--MobileLLM](https://www.linsight.cn/5ac36d34.html)  \n-   \n[Llama3.1--](https://www.linsight.cn/7d7294cb.html)  \n[Qwen2](https://www.linsight.cn/a8f8b641.html)  \n[Yi-](http://www.linsight.cn/41b6a819.html)  \n[MiniCPM](https://www.linsight.cn/376db710.html)  \n[GLM4](https://www.linsight.cn/a5206abd.html)  \n[Gemma2](https://www.linsight.cn/cf3f1f81.html)  \n[OpenELM](https://www.linsight.cn/f845f3e4.html)  \n[Yuan2.0Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  \n[bilibiliindex-1.9B](https://www.linsight.cn/770b63e1.html)  \n[loss](https://www.linsight.cn/f5fb75e4.html)  \n-   \n[--](https://www.linsight.cn/210dbccd.html)  \n-   \n[LLM](http://www.linsight.cn/c4da56c0.html)  \n[](http://www.linsight.cn/cc852861.html)  \n[-](http://www.linsight.cn/45ee1a6d.html)  \n-   \n[-](http://www.linsight.cn/f5c015c.html)  \n[-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  \n-   \n[Llama3.1--post-training](https://www.linsight.cn/93328a2a.html)  \n[ -- model soup](https://www.linsight.cn/bb8fcf21.html)  \n[-DPO](http://www.linsight.cn/473f2b43.html)  \n[-ODPO](http://www.linsight.cn/da871ebe.html)  \n[-simPO](http://www.linsight.cn/280fa97a.html)  \n[-IPO](http://www.linsight.cn/4fe7b810.html)  \n- Transformer  \n[Attention:MHA,MQAGQA](http://www.linsight.cn/3dc22f96.html)  \n[LLMICL](https://www.linsight.cn/7381cae3.html)  \n[transformernormalization](http://www.linsight.cn/6a40bfa5.html)  \n[normalization-](http://www.linsight.cn/b70b4a2d.html)  \n[:sliding window attention](http://www.linsight.cn/c61d17e3.html)  \n[LLM:RoPE](http://www.linsight.cn/a051710f.html)  \n[RoPE](https://www.linsight.cn/f0902f1a.html)  \n-   \n[(1)](http://www.linsight.cn/3345028a.html)\n[(2)](http://www.linsight.cn/ad0bba9d.html)\n[(3)](http://www.linsight.cn/1736008.html)\n[(4)](http://www.linsight.cn/1736008.html)\n[(5)](http://www.linsight.cn/336f2f3e.html)\n[(6)](http://www.linsight.cn/7c04944d.html)\n[(7)](https://www.linsight.cn/dd614e12.html)\n[(8)](https://www.linsight.cn/e287b9c3.html)  \n","slug":"cs/nlp/2024/08/-9","published":1,"updated":"2024-08-06T14:05:10.936Z","comments":1,"layout":"post","photos":[],"_id":"clzy4tsnu00gt0p4k1o8y5ky2","content":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM~</p>\n<p>~</p>\n<hr>\n<h1 id=\"\">1.</h1>\n<p>1LoRAadaptor</p>\n<p>2flash attentionpaged attentionring\nattention</p>\n<p>3ZeROrecomputationcpu-offload</p>\n<p>4</p>\n<h1 id=\"moedense\">2.MoEdense</h1>\n<p>float32/float16/bfloat16MoEgating\nfunctionexponentialexponentialgating\nfunctionMoE</p>\n<h1 id=\"t\">3.T</h1>\n<p>student modelteacher\nmodelsoftmaxteacher\nmodelsoftmaxT&gt;1</p>\n<p>teacher\nmodelgroud\ntruthone-hot\nlabel\n[]teacher model  [0.7, 0.2, 0.1]\n0.70.20.1one-hot\nlabel</p>\n<p>Tteacher\nmodellabelsoftstudent\nmodel</p>\n<h1 id=\"\">4.</h1>\n<p>1.</p>\n<p>2.MoE</p>\n<p>3.early-exit</p>\n<p>4.</p>\n<h1 id=\"llama12intermiedia-sizehidden-sizebert483\">5.LLAMA1/2intermiedia\nsize/hidden sizeBert48/3</h1>\n<p>LLAMASwiGLU\nFFNBertFFNintermediate\nsize4hidden size8/3</p>\n<hr>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n- MoE<br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/a0824e29.html\">denseMoE -- sparse\nupcycling</a><br>\n<a href=\"https://www.linsight.cn/2c8bbc7.html\">MoE--expert choice\nrouting</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/1e34e252.html\">--AFM</a><br>\n<a href=\"https://www.linsight.cn/5ac36d34.html\">--MobileLLM</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/7d7294cb.html\">Llama3.1--</a><br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/210dbccd.html\">--</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/93328a2a.html\">Llama3.1--post-training</a><br>\n<a href=\"https://www.linsight.cn/bb8fcf21.html\"> -- model\nsoup</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n- Transformer<br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a> <a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a> <a href=\"http://www.linsight.cn/1736008.html\">(3)</a> <a href=\"http://www.linsight.cn/1736008.html\">(4)</a> <a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a> <a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a> <a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a> <a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a></p>\n","length":1910,"excerpt":"","more":"<p>  /  / <a href=\"http://www.linsight.cn/\">linsight.cn</a> </p>\n<p>LLM~</p>\n<p>~</p>\n<hr>\n<h1 id=\"\">1.</h1>\n<p>1LoRAadaptor</p>\n<p>2flash attentionpaged attentionring\nattention</p>\n<p>3ZeROrecomputationcpu-offload</p>\n<p>4</p>\n<h1 id=\"moedense\">2.MoEdense</h1>\n<p>float32/float16/bfloat16MoEgating\nfunctionexponentialexponentialgating\nfunctionMoE</p>\n<h1 id=\"t\">3.T</h1>\n<p>student modelteacher\nmodelsoftmaxteacher\nmodelsoftmaxT&gt;1</p>\n<p>teacher\nmodelgroud\ntruthone-hot\nlabel\n[]teacher model  [0.7, 0.2, 0.1]\n0.70.20.1one-hot\nlabel</p>\n<p>Tteacher\nmodellabelsoftstudent\nmodel</p>\n<h1 id=\"\">4.</h1>\n<p>1.</p>\n<p>2.MoE</p>\n<p>3.early-exit</p>\n<p>4.</p>\n<h1 id=\"llama12intermiedia-sizehidden-sizebert483\">5.LLAMA1/2intermiedia\nsize/hidden sizeBert48/3</h1>\n<p>LLAMASwiGLU\nFFNBertFFNintermediate\nsize4hidden size8/3</p>\n<hr>\n<p><a href=\"http://www.linsight.cn/\">http://www.linsight.cn/</a><br>\n<a href=\"https://www.zhihu.com/people/us4ever\">Linsight</a><br>\nLinsight<br>\n<img src=\"/images/qrcode.jpg\"></p>\n<hr>\n<p><br>\n- MoE<br>\n<a href=\"http://www.linsight.cn/44e38c1b.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/83c49df0.html\">DeepSeek-V2MLA</a><br>\n<a href=\"https://www.linsight.cn/1d5bcd45.html\">-SkyworkMoE</a><br>\n<a href=\"https://www.linsight.cn/f3acf042.html\">10wJetMoE</a><br>\n<a href=\"https://www.linsight.cn/224c42da.html\">MoEtop-p\nrouting</a><br>\n<a href=\"https://www.linsight.cn/5e1d14b3.html\">MoE</a><br>\n<a href=\"https://www.linsight.cn/a0824e29.html\">denseMoE -- sparse\nupcycling</a><br>\n<a href=\"https://www.linsight.cn/2c8bbc7.html\">MoE--expert choice\nrouting</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/1e34e252.html\">--AFM</a><br>\n<a href=\"https://www.linsight.cn/5ac36d34.html\">--MobileLLM</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/7d7294cb.html\">Llama3.1--</a><br>\n<a href=\"https://www.linsight.cn/a8f8b641.html\">Qwen2</a><br>\n<a href=\"http://www.linsight.cn/41b6a819.html\">Yi-</a><br>\n<a href=\"https://www.linsight.cn/376db710.html\">MiniCPM</a><br>\n<a href=\"https://www.linsight.cn/a5206abd.html\">GLM4</a><br>\n<a href=\"https://www.linsight.cn/cf3f1f81.html\">Gemma2</a><br>\n<a href=\"https://www.linsight.cn/f845f3e4.html\">OpenELM</a><br>\n<a href=\"https://www.linsight.cn/3df0cd42.html\">Yuan2.0Yuan2.0-M32</a><br>\n<a href=\"https://www.linsight.cn/770b63e1.html\">bilibiliindex-1.9B</a><br>\n<a href=\"https://www.linsight.cn/f5fb75e4.html\">loss</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/210dbccd.html\">--</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/c4da56c0.html\">LLM</a><br>\n<a href=\"http://www.linsight.cn/cc852861.html\"></a><br>\n<a href=\"http://www.linsight.cn/45ee1a6d.html\">-</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/f5c015c.html\">-</a><br>\n<a href=\"https://www.linsight.cn/7bbe2df6.html\">-MEDUSA</a><br>\n- <br>\n<a href=\"https://www.linsight.cn/93328a2a.html\">Llama3.1--post-training</a><br>\n<a href=\"https://www.linsight.cn/bb8fcf21.html\"> -- model\nsoup</a><br>\n<a href=\"http://www.linsight.cn/473f2b43.html\">-DPO</a><br>\n<a href=\"http://www.linsight.cn/da871ebe.html\">-ODPO</a><br>\n<a href=\"http://www.linsight.cn/280fa97a.html\">-simPO</a><br>\n<a href=\"http://www.linsight.cn/4fe7b810.html\">-IPO</a><br>\n- Transformer<br>\n<a href=\"http://www.linsight.cn/3dc22f96.html\">Attention:MHA,MQAGQA</a><br>\n<a href=\"https://www.linsight.cn/7381cae3.html\">LLMICL</a><br>\n<a href=\"http://www.linsight.cn/6a40bfa5.html\">transformernormalization</a><br>\n<a href=\"http://www.linsight.cn/b70b4a2d.html\">normalization-</a><br>\n<a href=\"http://www.linsight.cn/c61d17e3.html\">:sliding\nwindow attention</a><br>\n<a href=\"http://www.linsight.cn/a051710f.html\">LLM:RoPE</a><br>\n<a href=\"https://www.linsight.cn/f0902f1a.html\">RoPE</a><br>\n- <br>\n<a href=\"http://www.linsight.cn/3345028a.html\">(1)</a> <a href=\"http://www.linsight.cn/ad0bba9d.html\">(2)</a> <a href=\"http://www.linsight.cn/1736008.html\">(3)</a> <a href=\"http://www.linsight.cn/1736008.html\">(4)</a> <a href=\"http://www.linsight.cn/336f2f3e.html\">(5)</a> <a href=\"http://www.linsight.cn/7c04944d.html\">(6)</a> <a href=\"https://www.linsight.cn/dd614e12.html\">(7)</a> <a href=\"https://www.linsight.cn/e287b9c3.html\">(8)</a></p>\n"}],"PostAsset":[{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/complex_number.png","slug":"complex_number.png","post":"clzy4tsmh00010p4k4cqqejre","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/remote_attenuation.png","slug":"remote_attenuation.png","post":"clzy4tsmh00010p4k4cqqejre","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLMRoPE/rope.png","slug":"rope.png","post":"clzy4tsmh00010p4k4cqqejre","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/RoPE/1.png","slug":"1.png","post":"clzy4tsmn00090p4k39vfat2y","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/RoPE/2.png","slug":"2.png","post":"clzy4tsmn00090p4k39vfat2y","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/RoPE/3.png","slug":"3.png","post":"clzy4tsmn00090p4k39vfat2y","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/RoPE/4.png","slug":"4.png","post":"clzy4tsmn00090p4k39vfat2y","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/RoPE/5.png","slug":"5.png","post":"clzy4tsmn00090p4k39vfat2y","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi.png","slug":"meta_pi.png","post":"clzy4tsmj00030p4ka4mbe3g6","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi_explanation.png","slug":"meta_pi_explanation.png","post":"clzy4tsmj00030p4ka4mbe3g6","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_pi_nosft.png","slug":"meta_pi_nosft.png","post":"clzy4tsmj00030p4ka4mbe3g6","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/meta_rope_ext.png","slug":"meta_rope_ext.png","post":"clzy4tsmj00030p4ka4mbe3g6","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/mix_precision_fp16.png","slug":"mix_precision_fp16.png","post":"clzy4tsmj00030p4ka4mbe3g6","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/02/LLM/rope_matrix.png","slug":"rope_matrix.png","post":"clzy4tsmj00030p4ka4mbe3g6","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/loss/downstream_dataset.png","slug":"downstream_dataset.png","post":"clzy4tsmp000c0p4k9p4u62qk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/loss/downstream_dataset_num.png","slug":"downstream_dataset_num.png","post":"clzy4tsmp000c0p4k9p4u62qk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/loss/eng_data.png","slug":"eng_data.png","post":"clzy4tsmp000c0p4k9p4u62qk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/loss/exp1_compute.png","slug":"exp1_compute.png","post":"clzy4tsmp000c0p4k9p4u62qk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/loss/exp1_param.png","slug":"exp1_param.png","post":"clzy4tsmp000c0p4k9p4u62qk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/loss/exp1_plot.png","slug":"exp1_plot.png","post":"clzy4tsmp000c0p4k9p4u62qk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/loss/exp2_param.png","slug":"exp2_param.png","post":"clzy4tsmp000c0p4k9p4u62qk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/loss/exp2_plot.png","slug":"exp2_plot.png","post":"clzy4tsmp000c0p4k9p4u62qk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/loss/exp3_plot.png","slug":"exp3_plot.png","post":"clzy4tsmp000c0p4k9p4u62qk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/loss/metrics.png","slug":"metrics.png","post":"clzy4tsmp000c0p4k9p4u62qk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/-IPO/curve.png","slug":"curve.png","post":"clzy4tsmq000g0p4kf42o7x9n","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/LLM/3.png","slug":"3.png","post":"clzy4tsmp000d0p4k8ivqhhwc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/LLM/4.png","slug":"4.png","post":"clzy4tsmp000d0p4k8ivqhhwc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/LLM/5.png","slug":"5.png","post":"clzy4tsmp000d0p4k8ivqhhwc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/LLM/6.png","slug":"6.png","post":"clzy4tsmp000d0p4k8ivqhhwc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/LLM/ditto_1.png","slug":"ditto_1.png","post":"clzy4tsmp000d0p4k8ivqhhwc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/LLM/ditto_2.png","slug":"ditto_2.png","post":"clzy4tsmp000d0p4k8ivqhhwc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/-7/lora.png","slug":"lora.png","post":"clzy4tsmq000h0p4kf1bk55ty","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/10wJetMoE/data1.png","slug":"data1.png","post":"clzy4tsmr000l0p4k37uv5eye","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/10wJetMoE/data2.png","slug":"data2.png","post":"clzy4tsmr000l0p4k37uv5eye","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/10wJetMoE/evaluation.png","slug":"evaluation.png","post":"clzy4tsmr000l0p4k37uv5eye","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/10wJetMoE/model_param.png","slug":"model_param.png","post":"clzy4tsmr000l0p4k37uv5eye","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/10wJetMoE/mtbench.png","slug":"mtbench.png","post":"clzy4tsmr000l0p4k37uv5eye","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/10wJetMoE/structure.png","slug":"structure.png","post":"clzy4tsmr000l0p4k37uv5eye","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/-MEDUSA/construct_tree.png","slug":"construct_tree.png","post":"clzy4tsmr000k0p4k1ioxckic","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/-MEDUSA/exp1.png","slug":"exp1.png","post":"clzy4tsmr000k0p4k1ioxckic","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/-MEDUSA/intro.png","slug":"intro.png","post":"clzy4tsmr000k0p4k1ioxckic","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/-MEDUSA/speed.png","slug":"speed.png","post":"clzy4tsmr000k0p4k1ioxckic","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/-MEDUSA/threshold.png","slug":"threshold.png","post":"clzy4tsmr000k0p4k1ioxckic","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/-MEDUSA/tree_attention.png","slug":"tree_attention.png","post":"clzy4tsmr000k0p4k1ioxckic","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/-MEDUSA/tree_attention_exp.png","slug":"tree_attention_exp.png","post":"clzy4tsmr000k0p4k1ioxckic","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/big_bird_attention.png","slug":"big_bird_attention.png","post":"clzy4tsms000p0p4kcps7453c","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/dilated_conv.png","slug":"dilated_conv.png","post":"clzy4tsms000p0p4kcps7453c","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/longformer_attention.png","slug":"longformer_attention.png","post":"clzy4tsms000p0p4kcps7453c","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_architechture.png","slug":"mistral_architechture.png","post":"clzy4tsms000p0p4kcps7453c","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_large_performance.jpeg","slug":"mistral_large_performance.jpeg","post":"clzy4tsms000p0p4kcps7453c","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_perf.png","slug":"mistral_perf.png","post":"clzy4tsms000p0p4kcps7453c","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/mistral_swa.png","slug":"mistral_swa.png","post":"clzy4tsms000p0p4kcps7453c","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/ms_invest_mistral.png","slug":"ms_invest_mistral.png","post":"clzy4tsms000p0p4kcps7453c","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/prefill_and_chunking.png","slug":"prefill_and_chunking.png","post":"clzy4tsms000p0p4kcps7453c","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/receptive_field_cnn.png","slug":"receptive_field_cnn.png","post":"clzy4tsms000p0p4kcps7453c","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/LLM-sliding-window-attention/rolling_buffer.png","slug":"rolling_buffer.png","post":"clzy4tsms000p0p4kcps7453c","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/GLM4/all_tools.png","slug":"all_tools.png","post":"clzy4tsmm00070p4k05cz5zt9","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/GLM4/glm.png","slug":"glm.png","post":"clzy4tsmm00070p4k05cz5zt9","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_algo.png","slug":"bn_algo.png","post":"clzy4tsms000r0p4k1fhha0b5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_and_ln.png","slug":"bn_and_ln.png","post":"clzy4tsms000r0p4k1fhha0b5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_ics.png","slug":"bn_ics.png","post":"clzy4tsms000r0p4k1fhha0b5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bn_ln_gn_in.png","slug":"bn_ln_gn_in.png","post":"clzy4tsms000r0p4k1fhha0b5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/bs_bn.png","slug":"bs_bn.png","post":"clzy4tsms000r0p4k1fhha0b5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/deepnorm.png","slug":"deepnorm.png","post":"clzy4tsms000r0p4k1fhha0b5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/deepnorm_result.png","slug":"deepnorm_result.png","post":"clzy4tsms000r0p4k1fhha0b5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ellipse_1.png","slug":"ellipse_1.png","post":"clzy4tsms000r0p4k1fhha0b5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ellipse_2.png","slug":"ellipse_2.png","post":"clzy4tsms000r0p4k1fhha0b5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ics_define.png","slug":"ics_define.png","post":"clzy4tsms000r0p4k1fhha0b5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/ics_measure.png","slug":"ics_measure.png","post":"clzy4tsms000r0p4k1fhha0b5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/lossfunc_surface.jpeg","slug":"lossfunc_surface.jpeg","post":"clzy4tsms000r0p4k1fhha0b5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/postnorm_prenorm.png","slug":"postnorm_prenorm.png","post":"clzy4tsms000r0p4k1fhha0b5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/prmsnorm.png","slug":"prmsnorm.png","post":"clzy4tsms000r0p4k1fhha0b5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/realformer.png","slug":"realformer.png","post":"clzy4tsms000r0p4k1fhha0b5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/realformer_attention.png","slug":"realformer_attention.png","post":"clzy4tsms000r0p4k1fhha0b5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/rmsnorm.png","slug":"rmsnorm.png","post":"clzy4tsms000r0p4k1fhha0b5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/rmsnorm_eff.png","slug":"rmsnorm_eff.png","post":"clzy4tsms000r0p4k1fhha0b5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/sigmoid.png","slug":"sigmoid.png","post":"clzy4tsms000r0p4k1fhha0b5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Transformernormalization/warmup_effect.png","slug":"warmup_effect.png","post":"clzy4tsms000r0p4k1fhha0b5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/9B.png","slug":"9B.png","post":"clzy4tsmt000x0p4k0a8r1fh9","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/base_model_eval.png","slug":"base_model_eval.png","post":"clzy4tsmt000x0p4k0a8r1fh9","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/cover.png","slug":"cover.png","post":"clzy4tsmt000x0p4k0a8r1fh9","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/eval.png","slug":"eval.png","post":"clzy4tsmt000x0p4k0a8r1fh9","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/ict.png","slug":"ict.png","post":"clzy4tsmt000x0p4k0a8r1fh9","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/long_context_result.png","slug":"long_context_result.png","post":"clzy4tsmt000x0p4k0a8r1fh9","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/model.png","slug":"model.png","post":"clzy4tsmt000x0p4k0a8r1fh9","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/multimodal.png","slug":"multimodal.png","post":"clzy4tsmt000x0p4k0a8r1fh9","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/perf.png","slug":"perf.png","post":"clzy4tsmt000x0p4k0a8r1fh9","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/pretrain_data_dist.png","slug":"pretrain_data_dist.png","post":"clzy4tsmt000x0p4k0a8r1fh9","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/pretrain_data_pipeline.png","slug":"pretrain_data_pipeline.png","post":"clzy4tsmt000x0p4k0a8r1fh9","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/sft.png","slug":"sft.png","post":"clzy4tsmt000x0p4k0a8r1fh9","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Yi-/third_party.png","slug":"third_party.png","post":"clzy4tsmt000x0p4k0a8r1fh9","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/1.png","slug":"1.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/10.png","slug":"10.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/11.png","slug":"11.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/12.png","slug":"12.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/13.png","slug":"13.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/14.png","slug":"14.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/15.png","slug":"15.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/16.png","slug":"16.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/17.png","slug":"17.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/18.png","slug":"18.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/19.png","slug":"19.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/2.png","slug":"2.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/20.png","slug":"20.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/21.png","slug":"21.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/22.png","slug":"22.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/23.png","slug":"23.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/24.png","slug":"24.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/25.png","slug":"25.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/26.png","slug":"26.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/27.png","slug":"27.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/28.png","slug":"28.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/3.png","slug":"3.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/4.png","slug":"4.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/5.png","slug":"5.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/6.png","slug":"6.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/7.png","slug":"7.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/8.png","slug":"8.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/ChatGPT/9.png","slug":"9.png","post":"clzy4tsmu00130p4k6fjxdg86","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/GQA.png","slug":"GQA.png","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/GQA_result_1.png","slug":"GQA_result_1.png","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/MQA.png","slug":"MQA.png","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/MQA.webp","slug":"MQA.webp","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Scaled-dot-product-self-attention.pbm","slug":"Scaled-dot-product-self-attention.pbm","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/Scaled-dot-product-self-attention.png","slug":"Scaled-dot-product-self-attention.png","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/attention_calculation.png","slug":"attention_calculation.png","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/cnn_heatmap.png","slug":"cnn_heatmap.png","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/decoder.png","slug":"decoder.png","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/encoder.png","slug":"encoder.png","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/gpu_cache.png","slug":"gpu_cache.png","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/lihongyi_self_attention.png","slug":"lihongyi_self_attention.png","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/llama2_qga.png","slug":"llama2_qga.png","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/mqa_result_1.png","slug":"mqa_result_1.png","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/mqa_result_3.png","slug":"mqa_result_3.png","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/multihead_attention.png","slug":"multihead_attention.png","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/seq2seq.png","slug":"seq2seq.png","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/seq2seq_attention.png","slug":"seq2seq_attention.png","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/softmax.png","slug":"softmax.png","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/sram_dram.png","slug":"sram_dram.png","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/Attention-MHA-MQAGQA/transformer_structure.png","slug":"transformer_structure.png","post":"clzy4tsmv00180p4kgx3k0lrt","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/128k_result.png","slug":"128k_result.png","post":"clzy4tsmm00080p4k7vhx37f3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/2_stage.png","slug":"2_stage.png","post":"clzy4tsmm00080p4k7vhx37f3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/batch_size.png","slug":"batch_size.png","post":"clzy4tsmm00080p4k7vhx37f3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/batch_size_2.png","slug":"batch_size_2.png","post":"clzy4tsmm00080p4k7vhx37f3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/cos_loss.png","slug":"cos_loss.png","post":"clzy4tsmm00080p4k7vhx37f3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/cos_lr.png","slug":"cos_lr.png","post":"clzy4tsmm00080p4k7vhx37f3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/data.png","slug":"data.png","post":"clzy4tsmm00080p4k7vhx37f3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/eval.png","slug":"eval.png","post":"clzy4tsmm00080p4k7vhx37f3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/exp_model.png","slug":"exp_model.png","post":"clzy4tsmm00080p4k7vhx37f3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/layers.png","slug":"layers.png","post":"clzy4tsmm00080p4k7vhx37f3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/learning_rate.png","slug":"learning_rate.png","post":"clzy4tsmm00080p4k7vhx37f3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/moe_result.png","slug":"moe_result.png","post":"clzy4tsmm00080p4k7vhx37f3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/param_search.png","slug":"param_search.png","post":"clzy4tsmm00080p4k7vhx37f3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/param_search_2.png","slug":"param_search_2.png","post":"clzy4tsmm00080p4k7vhx37f3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/scaling_law.png","slug":"scaling_law.png","post":"clzy4tsmm00080p4k7vhx37f3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/tokenizer.png","slug":"tokenizer.png","post":"clzy4tsmm00080p4k7vhx37f3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/train_loss.png","slug":"train_loss.png","post":"clzy4tsmm00080p4k7vhx37f3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/wsd_exp1.png","slug":"wsd_exp1.png","post":"clzy4tsmm00080p4k7vhx37f3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/wsd_exp2.png","slug":"wsd_exp2.png","post":"clzy4tsmm00080p4k7vhx37f3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/MiniCPM/wsd_update.png","slug":"wsd_update.png","post":"clzy4tsmm00080p4k7vhx37f3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/dpo_loss_code.png","slug":"dpo_loss_code.png","post":"clzy4tsmv001a0p4kfweq0629","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/gradient.png","slug":"gradient.png","post":"clzy4tsmv001a0p4kfweq0629","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/intro.png","slug":"intro.png","post":"clzy4tsmv001a0p4kfweq0629","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/result_1.png","slug":"result_1.png","post":"clzy4tsmv001a0p4kfweq0629","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/result_2.png","slug":"result_2.png","post":"clzy4tsmv001a0p4kfweq0629","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/result_3.png","slug":"result_3.png","post":"clzy4tsmv001a0p4kfweq0629","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-DPO/result_4.png","slug":"result_4.png","post":"clzy4tsmv001a0p4kfweq0629","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/alpha.png","slug":"alpha.png","post":"clzy4tsmv001c0p4ketw1b4vf","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/odpo_intro.png","slug":"odpo_intro.png","post":"clzy4tsmv001c0p4ketw1b4vf","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/scaling_function.png","slug":"scaling_function.png","post":"clzy4tsmv001c0p4ketw1b4vf","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/sentiment_control.png","slug":"sentiment_control.png","post":"clzy4tsmv001c0p4ketw1b4vf","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/summarization.png","slug":"summarization.png","post":"clzy4tsmv001c0p4ketw1b4vf","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-ODPO/toxicity_control.png","slug":"toxicity_control.png","post":"clzy4tsmv001c0p4ketw1b4vf","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/acce_alog.png","slug":"acce_alog.png","post":"clzy4tsmw001e0p4k5v58h187","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/acce_draft_model_param.png","slug":"acce_draft_model_param.png","post":"clzy4tsmw001e0p4k5v58h187","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/acce_k.png","slug":"acce_k.png","post":"clzy4tsmw001e0p4k5v58h187","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_alpha.png","slug":"fi_alpha.png","post":"clzy4tsmw001e0p4k5v58h187","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_choose_gamma.png","slug":"fi_choose_gamma.png","post":"clzy4tsmw001e0p4k5v58h187","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_example.png","slug":"fi_example.png","post":"clzy4tsmw001e0p4k5v58h187","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_expected_token_num.png","slug":"fi_expected_token_num.png","post":"clzy4tsmw001e0p4k5v58h187","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_sd_algo.png","slug":"fi_sd_algo.png","post":"clzy4tsmw001e0p4k5v58h187","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_speed_and_op.png","slug":"fi_speed_and_op.png","post":"clzy4tsmw001e0p4k5v58h187","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_speed_and_op_table.png","slug":"fi_speed_and_op_table.png","post":"clzy4tsmw001e0p4k5v58h187","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_t5_result.png","slug":"fi_t5_result.png","post":"clzy4tsmw001e0p4k5v58h187","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/fi_walltime.png","slug":"fi_walltime.png","post":"clzy4tsmw001e0p4k5v58h187","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/formula.png","slug":"formula.png","post":"clzy4tsmw001e0p4k5v58h187","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/speculative_decoding.png","slug":"speculative_decoding.png","post":"clzy4tsmw001e0p4k5v58h187","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-5/bfloat16.jpeg","slug":"bfloat16.jpeg","post":"clzy4tsmw001g0p4k0k4vhutb","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-5/ntk_by_parts.png","slug":"ntk_by_parts.png","post":"clzy4tsmw001g0p4k0k4vhutb","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-5/yarn.png","slug":"yarn.png","post":"clzy4tsmw001g0p4k0k4vhutb","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/SkyworkMoE/100B.png","slug":"100B.png","post":"clzy4tsms000o0p4kcl5g8m2j","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/SkyworkMoE/diff_dense.png","slug":"diff_dense.png","post":"clzy4tsms000o0p4kcl5g8m2j","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/SkyworkMoE/exp_1.png","slug":"exp_1.png","post":"clzy4tsms000o0p4kcl5g8m2j","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/SkyworkMoE/gate_dist.png","slug":"gate_dist.png","post":"clzy4tsms000o0p4kcl5g8m2j","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/SkyworkMoE/lr_exp.png","slug":"lr_exp.png","post":"clzy4tsms000o0p4kcl5g8m2j","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/SkyworkMoE/lr_result.png","slug":"lr_result.png","post":"clzy4tsms000o0p4kcl5g8m2j","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/SkyworkMoE/normaization.png","slug":"normaization.png","post":"clzy4tsms000o0p4kcl5g8m2j","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/SkyworkMoE/perf.png","slug":"perf.png","post":"clzy4tsms000o0p4kcl5g8m2j","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/06/SkyworkMoE/structure.png","slug":"structure.png","post":"clzy4tsms000o0p4kcl5g8m2j","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/ablation.png","slug":"ablation.png","post":"clzy4tsmx001l0p4k0q3edf3p","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/benchmark.png","slug":"benchmark.png","post":"clzy4tsmx001l0p4k0q3edf3p","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/contingency_table.png","slug":"contingency_table.png","post":"clzy4tsmx001l0p4k0q3edf3p","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/dpo_correlation.png","slug":"dpo_correlation.png","post":"clzy4tsmx001l0p4k0q3edf3p","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/gradient.png","slug":"gradient.png","post":"clzy4tsmx001l0p4k0q3edf3p","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/hyperparameters.png","slug":"hyperparameters.png","post":"clzy4tsmx001l0p4k0q3edf3p","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/intro.png","slug":"intro.png","post":"clzy4tsmx001l0p4k0q3edf3p","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/ln.png","slug":"ln.png","post":"clzy4tsmx001l0p4k0q3edf3p","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/ln_effect.png","slug":"ln_effect.png","post":"clzy4tsmx001l0p4k0q3edf3p","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/main_results.png","slug":"main_results.png","post":"clzy4tsmx001l0p4k0q3edf3p","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/margin_dist.png","slug":"margin_dist.png","post":"clzy4tsmx001l0p4k0q3edf3p","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/reward_accuracy.png","slug":"reward_accuracy.png","post":"clzy4tsmx001l0p4k0q3edf3p","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/reward_accuracy_compare.png","slug":"reward_accuracy_compare.png","post":"clzy4tsmx001l0p4k0q3edf3p","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/simpo_contingency.png","slug":"simpo_contingency.png","post":"clzy4tsmx001l0p4k0q3edf3p","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-simPO/simpo_hyperparameters.png","slug":"simpo_hyperparameters.png","post":"clzy4tsmx001l0p4k0q3edf3p","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/digimon.png","slug":"digimon.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_booksum.png","slug":"infini_attention_booksum.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_compare.png","slug":"infini_attention_compare.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_gating.png","slug":"infini_attention_gating.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_language_modeling.png","slug":"infini_attention_language_modeling.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_passkey.png","slug":"infini_attention_passkey.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_process.png","slug":"infini_attention_process.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/infini_attention_structure.png","slug":"infini_attention_structure.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_ablation.png","slug":"lm_infinite_ablation.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_attention_entropy.png","slug":"lm_infinite_attention_entropy.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_attention_logits_explode.png","slug":"lm_infinite_attention_logits_explode.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_design.png","slug":"lm_infinite_design.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_downstream.png","slug":"lm_infinite_downstream.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_middle_k.png","slug":"lm_infinite_middle_k.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_ppl_200m.png","slug":"lm_infinite_ppl_200m.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_ppl_figure.png","slug":"lm_infinite_ppl_figure.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_starting_tokens.png","slug":"lm_infinite_starting_tokens.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/lm_infinite_starting_tokens_num.png","slug":"lm_infinite_starting_tokens_num.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/streamingllm_compare.png","slug":"streamingllm_compare.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/streamingllm_model_ppl.png","slug":"streamingllm_model_ppl.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/stremingllm_attention_sink.png","slug":"stremingllm_attention_sink.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/stremingllm_exp.png","slug":"stremingllm_exp.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/stremingllm_init_token_num.png","slug":"stremingllm_init_token_num.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/stremingllm_kv_cache.png","slug":"stremingllm_kv_cache.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/stremingllm_perf_4m.png","slug":"stremingllm_perf_4m.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/xl_attention.png","slug":"xl_attention.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05/-/xl_vanilla_sw.png","slug":"xl_vanilla_sw.png","post":"clzy4tsmw001i0p4k2olbb79a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/cover.jpeg","slug":"cover.jpeg","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_infer_efficiency.png","slug":"dbrx_infer_efficiency.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_long_perf_1.png","slug":"dbrx_long_perf_1.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_long_perf_2.png","slug":"dbrx_long_perf_2.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_perf.png","slug":"dbrx_perf.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_train_efficiency.png","slug":"dbrx_train_efficiency.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_vs_closed_models.png","slug":"dbrx_vs_closed_models.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/dbrx_vs_open_models.png","slug":"dbrx_vs_open_models.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_16b_perf_1.png","slug":"ds_16b_perf_1.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_16b_perf_2.png","slug":"ds_16b_perf_2.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_2b_less_expert.png","slug":"ds_2b_less_expert.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_model_param.png","slug":"ds_model_param.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_145b.png","slug":"ds_moe_145b.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_ablation.png","slug":"ds_moe_ablation.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_comparison.png","slug":"ds_moe_comparison.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_expert_specialization.png","slug":"ds_moe_expert_specialization.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_less_activated_expert.png","slug":"ds_moe_less_activated_expert.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_perf.png","slug":"ds_moe_perf.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_sft.png","slug":"ds_moe_sft.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_structure.png","slug":"ds_moe_structure.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_upper_bound_13b.png","slug":"ds_moe_upper_bound_13b.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/ds_moe_upper_bound_2b.png","slug":"ds_moe_upper_bound_2b.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_compare_gpt3.png","slug":"glam_compare_gpt3.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_compare_gpt3_2.png","slug":"glam_compare_gpt3_2.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_family.png","slug":"glam_family.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_model.png","slug":"glam_model.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_perf.png","slug":"glam_perf.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/glam_related_model.png","slug":"glam_related_model.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_algo_1.png","slug":"gshard_algo_1.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_model.png","slug":"gshard_model.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_moe_family.png","slug":"gshard_moe_family.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_perf.png","slug":"gshard_perf.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/gshard_result.png","slug":"gshard_result.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/mistral_8_22b_code.png","slug":"mistral_8_22b_code.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/mistral_8_22b_multiling.png","slug":"mistral_8_22b_multiling.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/mistral_8_22b_reasoning.png","slug":"mistral_8_22b_reasoning.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/mistral_8_7b_active_perf.png","slug":"mistral_8_7b_active_perf.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/mistral_8_7b_perf.png","slug":"mistral_8_7b_perf.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/modular_connectionist.png","slug":"modular_connectionist.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/qwen1.5_moe_params.png","slug":"qwen1.5_moe_params.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/qwen1.5_moe_perf.png","slug":"qwen1.5_moe_perf.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/qwen1.5_moe_tps.png","slug":"qwen1.5_moe_tps.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe.png","slug":"rnn_moe.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_137b.png","slug":"rnn_moe_137b.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_hierarchical_gating.png","slug":"rnn_moe_hierarchical_gating.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_load_function.png","slug":"rnn_moe_load_function.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_perf.png","slug":"rnn_moe_perf.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/rnn_moe_specilized.png","slug":"rnn_moe_specilized.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/softplus.png","slug":"softplus.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_capacity_factor.png","slug":"st_moe_capacity_factor.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_capacity_factor_speed.png","slug":"st_moe_capacity_factor_speed.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_encoder_specialization.png","slug":"st_moe_encoder_specialization.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_models.png","slug":"st_moe_models.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_more_add_bias.png","slug":"st_moe_more_add_bias.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_more_add_noise.png","slug":"st_moe_more_add_noise.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_more_dense_layer.png","slug":"st_moe_more_dense_layer.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_multiling_specialization.png","slug":"st_moe_multiling_specialization.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_perf.png","slug":"st_moe_perf.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_remove_multiplications.png","slug":"st_moe_remove_multiplications.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_round_error.png","slug":"st_moe_round_error.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/st_moe_z_loss_result.png","slug":"st_moe_z_loss_result.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_capacity_effect.png","slug":"switch_transformer_capacity_effect.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_diff_expert_capacity.png","slug":"switch_transformer_diff_expert_capacity.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_distill.png","slug":"switch_transformer_distill.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_distill_diff_model.png","slug":"switch_transformer_distill_diff_model.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_distill_sft.png","slug":"switch_transformer_distill_sft.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_dropout.png","slug":"switch_transformer_dropout.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_init.png","slug":"switch_transformer_init.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_pretrain_result.png","slug":"switch_transformer_pretrain_result.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_scaling_dense.png","slug":"switch_transformer_scaling_dense.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_scaling_step.png","slug":"switch_transformer_scaling_step.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_scaling_time.png","slug":"switch_transformer_scaling_time.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_sft_result.png","slug":"switch_transformer_sft_result.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/switch_transformer_structure.png","slug":"switch_transformer_structure.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/vanilla_moe.png","slug":"vanilla_moe.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/vanilla_moe_result.png","slug":"vanilla_moe_result.png","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/03/MoE-/xiaomi_moe.jpg","slug":"xiaomi_moe.jpg","post":"clzy4tsmu00160p4k621p61y5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//add_money.jpg","slug":"add_money.jpg","post":"clzy4tsmx001r0p4kfkhz57lw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//eng_config.png","slug":"eng_config.png","post":"clzy4tsmx001r0p4kfkhz57lw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//eng_data.png","slug":"eng_data.png","post":"clzy4tsmx001r0p4kfkhz57lw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//eng_data_dist.png","slug":"eng_data_dist.png","post":"clzy4tsmx001r0p4kfkhz57lw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//eng_needle_comp.png","slug":"eng_needle_comp.png","post":"clzy4tsmx001r0p4kfkhz57lw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//eng_ppl.png","slug":"eng_ppl.png","post":"clzy4tsmx001r0p4kfkhz57lw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//eng_sample.png","slug":"eng_sample.png","post":"clzy4tsmx001r0p4kfkhz57lw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//eng_tokens.png","slug":"eng_tokens.png","post":"clzy4tsmx001r0p4kfkhz57lw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_dataset.png","slug":"paraphrasing_dataset.png","post":"clzy4tsmx001r0p4kfkhz57lw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_dataset_dist.png","slug":"paraphrasing_dataset_dist.png","post":"clzy4tsmx001r0p4kfkhz57lw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_example.png","slug":"paraphrasing_example.png","post":"clzy4tsmx001r0p4kfkhz57lw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_intro.png","slug":"paraphrasing_intro.png","post":"clzy4tsmx001r0p4kfkhz57lw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_lost.png","slug":"paraphrasing_lost.png","post":"clzy4tsmx001r0p4kfkhz57lw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_perf.png","slug":"paraphrasing_perf.png","post":"clzy4tsmx001r0p4kfkhz57lw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//paraphrasing_quality.png","slug":"paraphrasing_quality.png","post":"clzy4tsmx001r0p4kfkhz57lw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//pose_method.png","slug":"pose_method.png","post":"clzy4tsmx001r0p4kfkhz57lw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//pose_passkey.png","slug":"pose_passkey.png","post":"clzy4tsmx001r0p4kfkhz57lw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/05//pose_ppl.png","slug":"pose_ppl.png","post":"clzy4tsmx001r0p4kfkhz57lw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/bn_and_ln.png","slug":"bn_and_ln.png","post":"clzy4tsmy001u0p4kbhyla7a5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/cv_batchnorm.png","slug":"cv_batchnorm.png","post":"clzy4tsmy001u0p4kbhyla7a5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/cv_layernorm.jpeg","slug":"cv_layernorm.jpeg","post":"clzy4tsmy001u0p4kbhyla7a5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/04/normalization-/norm_in_nlp.png","slug":"norm_in_nlp.png","post":"clzy4tsmy001u0p4kbhyla7a5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/04/-4/transformer.png","slug":"transformer.png","post":"clzy4tsmy001z0p4kavkaedcm","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Llama-3-1-/eval.png","slug":"eval.png","post":"clzy4tsmz00280p4k2nfvggv5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Llama-3-1-/llama3.png","slug":"llama3.png","post":"clzy4tsmz00280p4k2nfvggv5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Llama-3-1-/model.png","slug":"model.png","post":"clzy4tsmz00280p4k2nfvggv5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Llama-3-1-/scaling_law.png","slug":"scaling_law.png","post":"clzy4tsmz00280p4k2nfvggv5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Llama-3-1-/scaling_law_exp.png","slug":"scaling_law_exp.png","post":"clzy4tsmz00280p4k2nfvggv5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/ablation_1.png","slug":"ablation_1.png","post":"clzy4tsmz00250p4k2ui96aqa","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/ablation_2.png","slug":"ablation_2.png","post":"clzy4tsmz00250p4k2ui96aqa","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/ablation_3.png","slug":"ablation_3.png","post":"clzy4tsmz00250p4k2ui96aqa","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/ablation_4.png","slug":"ablation_4.png","post":"clzy4tsmz00250p4k2ui96aqa","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/ablation_5.png","slug":"ablation_5.png","post":"clzy4tsmz00250p4k2ui96aqa","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/ablation_6.png","slug":"ablation_6.png","post":"clzy4tsmz00250p4k2ui96aqa","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/eval1.png","slug":"eval1.png","post":"clzy4tsmz00250p4k2ui96aqa","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/eval2.png","slug":"eval2.png","post":"clzy4tsmz00250p4k2ui96aqa","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/example.png","slug":"example.png","post":"clzy4tsmz00250p4k2ui96aqa","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/format.png","slug":"format.png","post":"clzy4tsmz00250p4k2ui96aqa","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/intro.png","slug":"intro.png","post":"clzy4tsmz00250p4k2ui96aqa","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Gemma2/model.png","slug":"model.png","post":"clzy4tsmz00250p4k2ui96aqa","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/GQA.png","slug":"GQA.png","post":"clzy4tsmz00220p4k0zzh79l1","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/GQA_compare_MHA.png","slug":"GQA_compare_MHA.png","post":"clzy4tsmz00220p4k0zzh79l1","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/MLA.png","slug":"MLA.png","post":"clzy4tsmz00220p4k0zzh79l1","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/MLA_cache.png","slug":"MLA_cache.png","post":"clzy4tsmz00220p4k0zzh79l1","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/MLA_formula.png","slug":"MLA_formula.png","post":"clzy4tsmz00220p4k0zzh79l1","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/MLA_perf.png","slug":"MLA_perf.png","post":"clzy4tsmz00220p4k0zzh79l1","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/align_eval.png","slug":"align_eval.png","post":"clzy4tsmz00220p4k0zzh79l1","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/intro.png","slug":"intro.png","post":"clzy4tsmz00220p4k0zzh79l1","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/lite_eval_1.png","slug":"lite_eval_1.png","post":"clzy4tsmz00220p4k0zzh79l1","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/lite_eval_2.png","slug":"lite_eval_2.png","post":"clzy4tsmz00220p4k0zzh79l1","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/model.png","slug":"model.png","post":"clzy4tsmz00220p4k0zzh79l1","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/needle.png","slug":"needle.png","post":"clzy4tsmz00220p4k0zzh79l1","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/DeepSeek-V2MLA/pt_eval.png","slug":"pt_eval.png","post":"clzy4tsmz00220p4k0zzh79l1","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Llama3-1-post-training/code_sample.png","slug":"code_sample.png","post":"clzy4tsn0002a0p4kh8jtd4dw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Llama3-1-post-training/file_upload.png","slug":"file_upload.png","post":"clzy4tsn0002a0p4kh8jtd4dw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Llama3-1-post-training/multi_step_tool.png","slug":"multi_step_tool.png","post":"clzy4tsn0002a0p4kh8jtd4dw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Llama3-1-post-training/post_training.png","slug":"post_training.png","post":"clzy4tsn0002a0p4kh8jtd4dw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Llama3-1-post-training/preference_data.png","slug":"preference_data.png","post":"clzy4tsn0002a0p4kh8jtd4dw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Llama3-1-post-training/sft_data.png","slug":"sft_data.png","post":"clzy4tsn0002a0p4kh8jtd4dw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Llama3-1-post-training/steerability.png","slug":"steerability.png","post":"clzy4tsn0002a0p4kh8jtd4dw","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/MoE-expert-choice-routing/capped.png","slug":"capped.png","post":"clzy4tsn0002e0p4k3m414qtc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/MoE-expert-choice-routing/cf.png","slug":"cf.png","post":"clzy4tsn0002e0p4k3m414qtc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/MoE-expert-choice-routing/dense.png","slug":"dense.png","post":"clzy4tsn0002e0p4k3m414qtc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/MoE-expert-choice-routing/dist.png","slug":"dist.png","post":"clzy4tsn0002e0p4k3m414qtc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/MoE-expert-choice-routing/efficiency.png","slug":"efficiency.png","post":"clzy4tsn0002e0p4k3m414qtc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/MoE-expert-choice-routing/expert_num.png","slug":"expert_num.png","post":"clzy4tsn0002e0p4k3m414qtc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/MoE-expert-choice-routing/intro.png","slug":"intro.png","post":"clzy4tsn0002e0p4k3m414qtc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/MoE-expert-choice-routing/model.png","slug":"model.png","post":"clzy4tsn0002e0p4k3m414qtc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Qwen2/eval_base_7B.png","slug":"eval_base_7B.png","post":"clzy4tsn0002g0p4kcz8k0zgz","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Qwen2/eval_base_large.png","slug":"eval_base_large.png","post":"clzy4tsn0002g0p4kcz8k0zgz","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Qwen2/eval_base_small.png","slug":"eval_base_small.png","post":"clzy4tsn0002g0p4kcz8k0zgz","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Qwen2/eval_chat_7B.png","slug":"eval_chat_7B.png","post":"clzy4tsn0002g0p4kcz8k0zgz","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Qwen2/eval_chat_large.png","slug":"eval_chat_large.png","post":"clzy4tsn0002g0p4kcz8k0zgz","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Qwen2/eval_chat_small.png","slug":"eval_chat_small.png","post":"clzy4tsn0002g0p4kcz8k0zgz","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Qwen2/eval_long.png","slug":"eval_long.png","post":"clzy4tsn0002g0p4kcz8k0zgz","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Qwen2/eval_needle.png","slug":"eval_needle.png","post":"clzy4tsn0002g0p4kcz8k0zgz","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Qwen2/model.png","slug":"model.png","post":"clzy4tsn0002g0p4kcz8k0zgz","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/eval1.png","slug":"eval1.png","post":"clzy4tsn1002k0p4k3atkhqd7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/eval2.png","slug":"eval2.png","post":"clzy4tsn1002k0p4k3atkhqd7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/eval3.png","slug":"eval3.png","post":"clzy4tsn1002k0p4k3atkhqd7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/eval4.png","slug":"eval4.png","post":"clzy4tsn1002k0p4k3atkhqd7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/lfa.png","slug":"lfa.png","post":"clzy4tsn1002k0p4k3atkhqd7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/lfa_conv.png","slug":"lfa_conv.png","post":"clzy4tsn1002k0p4k3atkhqd7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/lfa_result.png","slug":"lfa_result.png","post":"clzy4tsn1002k0p4k3atkhqd7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/m32_intro.png","slug":"m32_intro.png","post":"clzy4tsn1002k0p4k3atkhqd7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/pretrain.png","slug":"pretrain.png","post":"clzy4tsn1002k0p4k3atkhqd7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/router.png","slug":"router.png","post":"clzy4tsn1002k0p4k3atkhqd7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/router_eval.png","slug":"router_eval.png","post":"clzy4tsn1002k0p4k3atkhqd7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/scalability.png","slug":"scalability.png","post":"clzy4tsn1002k0p4k3atkhqd7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/train_hp.png","slug":"train_hp.png","post":"clzy4tsn1002k0p4k3atkhqd7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/yuan2_chat_data.png","slug":"yuan2_chat_data.png","post":"clzy4tsn1002k0p4k3atkhqd7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/yuan2_intro.png","slug":"yuan2_intro.png","post":"clzy4tsn1002k0p4k3atkhqd7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/yuan2_pretrain_data.png","slug":"yuan2_pretrain_data.png","post":"clzy4tsn1002k0p4k3atkhqd7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/yuan2_sft_hp.png","slug":"yuan2_sft_hp.png","post":"clzy4tsn1002k0p4k3atkhqd7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/Yuan2-0-M32/yuan2_train_curve.png","slug":"yuan2_train_curve.png","post":"clzy4tsn1002k0p4k3atkhqd7","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/bilibiliindex-1-9B/add_instruct_data.png","slug":"add_instruct_data.png","post":"clzy4tsn1002m0p4k0ski0myy","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/bilibiliindex-1-9B/dedup.png","slug":"dedup.png","post":"clzy4tsn1002m0p4k0ski0myy","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/bilibiliindex-1-9B/dpo.png","slug":"dpo.png","post":"clzy4tsn1002m0p4k0ski0myy","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/bilibiliindex-1-9B/layer_num.png","slug":"layer_num.png","post":"clzy4tsn1002m0p4k0ski0myy","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/bilibiliindex-1-9B/norm_head.png","slug":"norm_head.png","post":"clzy4tsn1002m0p4k0ski0myy","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/bilibiliindex-1-9B/pt_data.png","slug":"pt_data.png","post":"clzy4tsn1002m0p4k0ski0myy","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/bilibiliindex-1-9B/scheduler.png","slug":"scheduler.png","post":"clzy4tsn1002m0p4k0ski0myy","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/bilibiliindex-1-9B/sft.png","slug":"sft.png","post":"clzy4tsn1002m0p4k0ski0myy","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/bilibiliindex-1-9B/wsd_quality.png","slug":"wsd_quality.png","post":"clzy4tsn1002m0p4k0ski0myy","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/routing/active_num.png","slug":"active_num.png","post":"clzy4tsn1002q0p4k23kd4w80","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/routing/diff_layer.png","slug":"diff_layer.png","post":"clzy4tsn1002q0p4k23kd4w80","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/routing/diff_p.png","slug":"diff_p.png","post":"clzy4tsn1002q0p4k23kd4w80","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/routing/perf.png","slug":"perf.png","post":"clzy4tsn1002q0p4k23kd4w80","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/routing/task_expert.png","slug":"task_expert.png","post":"clzy4tsn1002q0p4k23kd4w80","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/routing/top-p.png","slug":"top-p.png","post":"clzy4tsn1002q0p4k23kd4w80","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/model-soup/algo.png","slug":"algo.png","post":"clzy4tsn2002t0p4k0mm281q5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/model-soup/angle.png","slug":"angle.png","post":"clzy4tsn2002t0p4k0mm281q5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/model-soup/angle_2.png","slug":"angle_2.png","post":"clzy4tsn2002t0p4k0mm281q5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/model-soup/compare.png","slug":"compare.png","post":"clzy4tsn2002t0p4k0mm281q5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/model-soup/method_soup.png","slug":"method_soup.png","post":"clzy4tsn2002t0p4k0mm281q5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/model-soup/result.png","slug":"result.png","post":"clzy4tsn2002t0p4k0mm281q5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/model-soup/swa_1.png","slug":"swa_1.png","post":"clzy4tsn2002t0p4k0mm281q5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/model-soup/swa_2.png","slug":"swa_2.png","post":"clzy4tsn2002t0p4k0mm281q5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/model-soup/swa_3.png","slug":"swa_3.png","post":"clzy4tsn2002t0p4k0mm281q5","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/1.png","slug":"1.png","post":"clzy4tsn2002y0p4k0s0s5f0u","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/2.png","slug":"2.png","post":"clzy4tsn2002y0p4k0s0s5f0u","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/3.png","slug":"3.png","post":"clzy4tsn2002y0p4k0s0s5f0u","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/4.png","slug":"4.png","post":"clzy4tsn2002y0p4k0s0s5f0u","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/a1.png","slug":"a1.png","post":"clzy4tsn2002y0p4k0s0s5f0u","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/a2.png","slug":"a2.png","post":"clzy4tsn2002y0p4k0s0s5f0u","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/a3.png","slug":"a3.png","post":"clzy4tsn2002y0p4k0s0s5f0u","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/a4.png","slug":"a4.png","post":"clzy4tsn2002y0p4k0s0s5f0u","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/a5.png","slug":"a5.png","post":"clzy4tsn2002y0p4k0s0s5f0u","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/a6.png","slug":"a6.png","post":"clzy4tsn2002y0p4k0s0s5f0u","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/intro.png","slug":"intro.png","post":"clzy4tsn2002y0p4k0s0s5f0u","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/upcycling/models.png","slug":"models.png","post":"clzy4tsn2002y0p4k0s0s5f0u","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/-8/1.png","slug":"1.png","post":"clzy4tsn300360p4k1je1669v","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/-8/2.png","slug":"2.png","post":"clzy4tsn300360p4k1je1669v","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/AFM/afm.png","slug":"afm.png","post":"clzy4tsn300390p4k1bq9epn3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/AFM/core_ablation.png","slug":"core_ablation.png","post":"clzy4tsn300390p4k1bq9epn3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/AFM/distill.png","slug":"distill.png","post":"clzy4tsn300390p4k1bq9epn3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/AFM/intelligence.png","slug":"intelligence.png","post":"clzy4tsn300390p4k1bq9epn3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/AFM/pretrain_1.png","slug":"pretrain_1.png","post":"clzy4tsn300390p4k1bq9epn3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/AFM/pretrain_2.png","slug":"pretrain_2.png","post":"clzy4tsn300390p4k1bq9epn3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/AFM/recover.png","slug":"recover.png","post":"clzy4tsn300390p4k1bq9epn3","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/MoE/dynamic.png","slug":"dynamic.png","post":"clzy4tsn200310p4k2z6uej8j","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/MoE/gating_1.png","slug":"gating_1.png","post":"clzy4tsn200310p4k2z6uej8j","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/MoE/gating_2.png","slug":"gating_2.png","post":"clzy4tsn200310p4k2z6uej8j","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/MoE/matrix_level.png","slug":"matrix_level.png","post":"clzy4tsn200310p4k2z6uej8j","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/MoE/models.png","slug":"models.png","post":"clzy4tsn200310p4k2z6uej8j","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/MoE/norm.png","slug":"norm.png","post":"clzy4tsn200310p4k2z6uej8j","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/MoE/t2.png","slug":"t2.png","post":"clzy4tsn200310p4k2z6uej8j","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/-/base_freq.png","slug":"base_freq.png","post":"clzy4tsns00gh0p4k0itoh1bc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/-/bias.png","slug":"bias.png","post":"clzy4tsns00gh0p4k0itoh1bc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/-/buckets.png","slug":"buckets.png","post":"clzy4tsns00gh0p4k0itoh1bc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/-/curriculum.png","slug":"curriculum.png","post":"clzy4tsns00gh0p4k0itoh1bc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/-/dist.png","slug":"dist.png","post":"clzy4tsns00gh0p4k0itoh1bc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/-/efficiency.png","slug":"efficiency.png","post":"clzy4tsns00gh0p4k0itoh1bc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/-/mixture.png","slug":"mixture.png","post":"clzy4tsns00gh0p4k0itoh1bc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/-/model.png","slug":"model.png","post":"clzy4tsns00gh0p4k0itoh1bc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/-/scaling.png","slug":"scaling.png","post":"clzy4tsns00gh0p4k0itoh1bc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/-/sota.png","slug":"sota.png","post":"clzy4tsns00gh0p4k0itoh1bc","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/data.png","slug":"data.png","post":"clzy4tsns00gi0p4k5muigr4a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/eval_1.png","slug":"eval_1.png","post":"clzy4tsns00gi0p4k5muigr4a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/eval_2.png","slug":"eval_2.png","post":"clzy4tsns00gi0p4k5muigr4a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/eval_3.png","slug":"eval_3.png","post":"clzy4tsns00gi0p4k5muigr4a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/formula.png","slug":"formula.png","post":"clzy4tsns00gi0p4k5muigr4a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/intro.png","slug":"intro.png","post":"clzy4tsns00gi0p4k5muigr4a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/peft_eval.png","slug":"peft_eval.png","post":"clzy4tsns00gi0p4k5muigr4a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/pretrain_hp.png","slug":"pretrain_hp.png","post":"clzy4tsns00gi0p4k5muigr4a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/sft_hp.png","slug":"sft_hp.png","post":"clzy4tsns00gi0p4k5muigr4a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/07/OpenELM/sft_result.png","slug":"sft_result.png","post":"clzy4tsns00gi0p4k5muigr4a","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08//bfm.png","slug":"bfm.png","post":"clzy4tsnt00gk0p4kbe1j6qt9","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08//system_1.png","slug":"system_1.png","post":"clzy4tsnt00gk0p4kbe1j6qt9","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08//task_emb.png","slug":"task_emb.png","post":"clzy4tsnt00gk0p4kbe1j6qt9","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08//xiaomi_1.png","slug":"xiaomi_1.png","post":"clzy4tsnt00gk0p4kbe1j6qt9","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08//xiaomi_2.png","slug":"xiaomi_2.png","post":"clzy4tsnt00gk0p4kbe1j6qt9","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/phi/overfit.png","slug":"overfit.png","post":"clzy4tsnt00gp0p4k1f5c31jk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_15_bench_1.png","slug":"phi_15_bench_1.png","post":"clzy4tsnt00gp0p4k1f5c31jk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_15_bench_2.png","slug":"phi_15_bench_2.png","post":"clzy4tsnt00gp0p4k1f5c31jk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_15_bench_3.png","slug":"phi_15_bench_3.png","post":"clzy4tsnt00gp0p4k1f5c31jk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_15_result.png","slug":"phi_15_result.png","post":"clzy4tsnt00gp0p4k1f5c31jk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_1_code_case.png","slug":"phi_1_code_case.png","post":"clzy4tsnt00gp0p4k1f5c31jk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_1_compare.png","slug":"phi_1_compare.png","post":"clzy4tsnt00gp0p4k1f5c31jk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_1_example_1.png","slug":"phi_1_example_1.png","post":"clzy4tsnt00gp0p4k1f5c31jk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_1_example_2.png","slug":"phi_1_example_2.png","post":"clzy4tsnt00gp0p4k1f5c31jk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_1_result.png","slug":"phi_1_result.png","post":"clzy4tsnt00gp0p4k1f5c31jk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_2.png","slug":"phi_2.png","post":"clzy4tsnt00gp0p4k1f5c31jk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_2_0.png","slug":"phi_2_0.png","post":"clzy4tsnt00gp0p4k1f5c31jk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_2_1.png","slug":"phi_2_1.png","post":"clzy4tsnt00gp0p4k1f5c31jk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_2_2.png","slug":"phi_2_2.png","post":"clzy4tsnt00gp0p4k1f5c31jk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_2_3.png","slug":"phi_2_3.png","post":"clzy4tsnt00gp0p4k1f5c31jk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_3_result.png","slug":"phi_3_result.png","post":"clzy4tsnt00gp0p4k1f5c31jk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/phi/phi_3_sparse.png","slug":"phi_3_sparse.png","post":"clzy4tsnt00gp0p4k1f5c31jk","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/deep.png","slug":"deep.png","post":"clzy4tsnt00gm0p4k5nza3nwg","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/deep_ablation.png","slug":"deep_ablation.png","post":"clzy4tsnt00gm0p4k5nza3nwg","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/device.png","slug":"device.png","post":"clzy4tsnt00gm0p4k5nza3nwg","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/emb.png","slug":"emb.png","post":"clzy4tsnt00gm0p4k5nza3nwg","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/head.png","slug":"head.png","post":"clzy4tsnt00gm0p4k5nza3nwg","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/kd.png","slug":"kd.png","post":"clzy4tsnt00gm0p4k5nza3nwg","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/mobilellm.png","slug":"mobilellm.png","post":"clzy4tsnt00gm0p4k5nza3nwg","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/model.png","slug":"model.png","post":"clzy4tsnt00gm0p4k5nza3nwg","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/repeat.png","slug":"repeat.png","post":"clzy4tsnt00gm0p4k5nza3nwg","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/result.png","slug":"result.png","post":"clzy4tsnt00gm0p4k5nza3nwg","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/share.png","slug":"share.png","post":"clzy4tsnt00gm0p4k5nza3nwg","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/share_2.png","slug":"share_2.png","post":"clzy4tsnt00gm0p4k5nza3nwg","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/structure.png","slug":"structure.png","post":"clzy4tsnt00gm0p4k5nza3nwg","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/structure_ablation.png","slug":"structure_ablation.png","post":"clzy4tsnt00gm0p4k5nza3nwg","modified":0,"renderable":0},{"_id":"source/_posts/cs/nlp/2024/08/-MobileLLM/zero_shot.png","slug":"zero_shot.png","post":"clzy4tsnt00gm0p4k5nza3nwg","modified":0,"renderable":0}],"PostCategory":[{"post_id":"clzy4tsmx001r0p4kfkhz57lw","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsmy00200p4k8adga5co"},{"post_id":"clzy4tsmx001r0p4kfkhz57lw","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsmz00230p4kbk9nabu7"},{"post_id":"clzy4tsmx001r0p4kfkhz57lw","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsmz00260p4kf2cm3xu5"},{"post_id":"clzy4tsmr000k0p4k1ioxckic","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsn000290p4kfk7wa9wr"},{"post_id":"clzy4tsmr000k0p4k1ioxckic","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsn0002c0p4kakdq09jk"},{"post_id":"clzy4tsmr000k0p4k1ioxckic","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsn0002f0p4k93p6axcn"},{"post_id":"clzy4tsmy001u0p4kbhyla7a5","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsn1002i0p4kcgqb5d1d"},{"post_id":"clzy4tsmy001u0p4kbhyla7a5","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsn1002l0p4ka6p1eycr"},{"post_id":"clzy4tsmy001u0p4kbhyla7a5","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsn1002n0p4kdda50t7o"},{"post_id":"clzy4tsmy001x0p4ka1x236kq","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsn2002r0p4k0bvecwcx"},{"post_id":"clzy4tsmy001x0p4ka1x236kq","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsn2002u0p4kaabc1w5n"},{"post_id":"clzy4tsmy001x0p4ka1x236kq","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsn2002z0p4khca94pcz"},{"post_id":"clzy4tsmm00080p4k7vhx37f3","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsn300320p4kc0lzhm2f"},{"post_id":"clzy4tsmm00080p4k7vhx37f3","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsn300370p4k3detaxih"},{"post_id":"clzy4tsmm00080p4k7vhx37f3","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsn3003a0p4kd1mfbim3"},{"post_id":"clzy4tsmy001z0p4kavkaedcm","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsn4003e0p4k12j2643e"},{"post_id":"clzy4tsmy001z0p4kavkaedcm","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsn4003g0p4k20incl0v"},{"post_id":"clzy4tsmy001z0p4kavkaedcm","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsn4003k0p4k1rrhbf8l"},{"post_id":"clzy4tsmz00220p4k0zzh79l1","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsn4003m0p4k32l108nj"},{"post_id":"clzy4tsmz00220p4k0zzh79l1","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsn4003q0p4k9ra083tv"},{"post_id":"clzy4tsmz00220p4k0zzh79l1","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsn4003s0p4k58ku263f"},{"post_id":"clzy4tsmr000l0p4k37uv5eye","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsn5003w0p4k07fbezc5"},{"post_id":"clzy4tsmr000l0p4k37uv5eye","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsn5003y0p4kes101ug8"},{"post_id":"clzy4tsmr000l0p4k37uv5eye","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsn500420p4k2maser4k"},{"post_id":"clzy4tsmz00250p4k2ui96aqa","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsn500440p4k75dk83bu"},{"post_id":"clzy4tsmz00250p4k2ui96aqa","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsn500480p4kdm594scm"},{"post_id":"clzy4tsmz00250p4k2ui96aqa","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsn5004a0p4kdojh0wbl"},{"post_id":"clzy4tsmz00280p4k2nfvggv5","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsn5004e0p4kgno707oi"},{"post_id":"clzy4tsmz00280p4k2nfvggv5","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsn5004f0p4k2xxs3f07"},{"post_id":"clzy4tsmz00280p4k2nfvggv5","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsn6004i0p4k7984gqem"},{"post_id":"clzy4tsms000o0p4kcl5g8m2j","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsn6004k0p4k0mv6gdue"},{"post_id":"clzy4tsms000o0p4kcl5g8m2j","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsn6004o0p4k0slse221"},{"post_id":"clzy4tsms000o0p4kcl5g8m2j","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsn6004q0p4k0m939ojz"},{"post_id":"clzy4tsn0002a0p4kh8jtd4dw","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsn6004u0p4k9r827lq6"},{"post_id":"clzy4tsn0002a0p4kh8jtd4dw","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsn6004w0p4kbkli4l6b"},{"post_id":"clzy4tsn0002a0p4kh8jtd4dw","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsn600500p4k4q8b0ldb"},{"post_id":"clzy4tsn0002e0p4k3m414qtc","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsn600520p4kbbvk30z4"},{"post_id":"clzy4tsn0002e0p4k3m414qtc","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsn600550p4k6w8fcxs0"},{"post_id":"clzy4tsn0002e0p4k3m414qtc","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsn700580p4k0sz69vhm"},{"post_id":"clzy4tsmh00010p4k4cqqejre","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsn7005a0p4k2549eet8"},{"post_id":"clzy4tsmh00010p4k4cqqejre","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsn7005c0p4keki00owc"},{"post_id":"clzy4tsmh00010p4k4cqqejre","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsn7005f0p4k8emwb12h"},{"post_id":"clzy4tsn0002g0p4kcz8k0zgz","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsn7005i0p4k6gz2fk8l"},{"post_id":"clzy4tsn0002g0p4kcz8k0zgz","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsn7005l0p4kd8d6cf54"},{"post_id":"clzy4tsn0002g0p4kcz8k0zgz","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsn7005o0p4k3nxv3o3l"},{"post_id":"clzy4tsn1002k0p4k3atkhqd7","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsn7005r0p4kaxphgqyr"},{"post_id":"clzy4tsn1002k0p4k3atkhqd7","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsn8005t0p4kfvyc5ac6"},{"post_id":"clzy4tsn1002k0p4k3atkhqd7","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsn8005v0p4k4eqv514j"},{"post_id":"clzy4tsms000p0p4kcps7453c","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsn8005x0p4k8kn57uuc"},{"post_id":"clzy4tsms000p0p4kcps7453c","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsn800600p4k8tv43f0g"},{"post_id":"clzy4tsms000p0p4kcps7453c","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsn800620p4k506jfq5i"},{"post_id":"clzy4tsn1002m0p4k0ski0myy","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsn800650p4kczc76xoa"},{"post_id":"clzy4tsn1002m0p4k0ski0myy","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsn800670p4kbf588mt3"},{"post_id":"clzy4tsn1002m0p4k0ski0myy","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnb006a0p4kh0q8dlk4"},{"post_id":"clzy4tsn1002q0p4k23kd4w80","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnb006c0p4k26ne90gh"},{"post_id":"clzy4tsn1002q0p4k23kd4w80","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnb006f0p4k9bkwcln5"},{"post_id":"clzy4tsn1002q0p4k23kd4w80","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnb006h0p4kgmhz2wiv"},{"post_id":"clzy4tsms000r0p4k1fhha0b5","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnc006k0p4kbovo3nef"},{"post_id":"clzy4tsms000r0p4k1fhha0b5","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnc006m0p4khg40fzze"},{"post_id":"clzy4tsms000r0p4k1fhha0b5","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnc006p0p4k8mnler57"},{"post_id":"clzy4tsn2002t0p4k0mm281q5","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnc006r0p4kdsn67kcy"},{"post_id":"clzy4tsn2002t0p4k0mm281q5","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnc006t0p4kbzixgjbp"},{"post_id":"clzy4tsn2002t0p4k0mm281q5","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnc006w0p4kakhzccsb"},{"post_id":"clzy4tsn2002y0p4k0s0s5f0u","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnc006y0p4k06lh1djy"},{"post_id":"clzy4tsn2002y0p4k0s0s5f0u","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnc00710p4kgnu26114"},{"post_id":"clzy4tsn2002y0p4k0s0s5f0u","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnc00730p4kaqhha6jh"},{"post_id":"clzy4tsmn00090p4k39vfat2y","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnc00760p4k33rl87i3"},{"post_id":"clzy4tsmn00090p4k39vfat2y","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnc00780p4kcpeheunj"},{"post_id":"clzy4tsmn00090p4k39vfat2y","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnc007b0p4ka103fywt"},{"post_id":"clzy4tsn200310p4k2z6uej8j","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnc007d0p4k5l2b42hz"},{"post_id":"clzy4tsn200310p4k2z6uej8j","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnc007f0p4k4eu1ee2e"},{"post_id":"clzy4tsn200310p4k2z6uej8j","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnd007i0p4khmvv79jd"},{"post_id":"clzy4tsn300360p4k1je1669v","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnd007k0p4k1g480qsc"},{"post_id":"clzy4tsn300360p4k1je1669v","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnd007n0p4kbgaf48gg"},{"post_id":"clzy4tsn300360p4k1je1669v","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnd007p0p4k443y15gx"},{"post_id":"clzy4tsmt000u0p4kenajh0xm","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnd007s0p4k80wj3stc"},{"post_id":"clzy4tsmt000u0p4kenajh0xm","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnd007u0p4khno34lvu"},{"post_id":"clzy4tsmt000u0p4kenajh0xm","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnd007x0p4k988acn6m"},{"post_id":"clzy4tsn300390p4k1bq9epn3","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnd007z0p4kcd13b0it"},{"post_id":"clzy4tsn300390p4k1bq9epn3","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnd00820p4kbxugcrjv"},{"post_id":"clzy4tsn300390p4k1bq9epn3","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnd00840p4k7oy75nk9"},{"post_id":"clzy4tsmt000x0p4k0a8r1fh9","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnd00870p4kc8rsc7s1"},{"post_id":"clzy4tsmt000x0p4k0a8r1fh9","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsne00890p4kc6rg4w2t"},{"post_id":"clzy4tsmt000x0p4k0a8r1fh9","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsne008c0p4kdxwecvaz"},{"post_id":"clzy4tsmp000c0p4k9p4u62qk","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsne008e0p4k0rsm3j2g"},{"post_id":"clzy4tsmp000c0p4k9p4u62qk","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsne008h0p4kfasl8pnm"},{"post_id":"clzy4tsmp000c0p4k9p4u62qk","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsne008j0p4khmiqcglw"},{"post_id":"clzy4tsmt00100p4ketyogyon","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsne008l0p4k26gtbgz9"},{"post_id":"clzy4tsmt00100p4ketyogyon","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsne008o0p4k5x2d7zrb"},{"post_id":"clzy4tsmt00100p4ketyogyon","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsne008q0p4kcshwbi5e"},{"post_id":"clzy4tsmu00130p4k6fjxdg86","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsne008t0p4k36l47nzf"},{"post_id":"clzy4tsmu00130p4k6fjxdg86","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsne008v0p4k11fmde3l"},{"post_id":"clzy4tsmu00130p4k6fjxdg86","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsne008y0p4k8pb6fwkw"},{"post_id":"clzy4tsmj00030p4ka4mbe3g6","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsne00900p4ka13k5e8c"},{"post_id":"clzy4tsmj00030p4ka4mbe3g6","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsne00930p4kc8tz4xze"},{"post_id":"clzy4tsmj00030p4ka4mbe3g6","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnf00950p4k4zb5hsak"},{"post_id":"clzy4tsmu00160p4k621p61y5","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnf00970p4k6glu7v1u"},{"post_id":"clzy4tsmu00160p4k621p61y5","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnf00990p4kghesehzh"},{"post_id":"clzy4tsmu00160p4k621p61y5","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnf009a0p4k7fsr84or"},{"post_id":"clzy4tsmv00180p4kgx3k0lrt","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnf009c0p4k1cnf1882"},{"post_id":"clzy4tsmv00180p4kgx3k0lrt","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnf009e0p4k5e8ab70u"},{"post_id":"clzy4tsmv00180p4kgx3k0lrt","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnf009h0p4k9y5tgto8"},{"post_id":"clzy4tsmp000d0p4k8ivqhhwc","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnf009j0p4k6s6fhlny"},{"post_id":"clzy4tsmp000d0p4k8ivqhhwc","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnf009m0p4k22c95val"},{"post_id":"clzy4tsmp000d0p4k8ivqhhwc","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnf009o0p4kdks12g4y"},{"post_id":"clzy4tsmv001a0p4kfweq0629","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsng009r0p4k93mj1tpe"},{"post_id":"clzy4tsmv001a0p4kfweq0629","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsng009t0p4k2fvb4ged"},{"post_id":"clzy4tsmv001a0p4kfweq0629","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsng009w0p4keysn95m3"},{"post_id":"clzy4tsmv001c0p4ketw1b4vf","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsng009y0p4k527lhzi5"},{"post_id":"clzy4tsmv001c0p4ketw1b4vf","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsng00a10p4k39bhc4pn"},{"post_id":"clzy4tsmv001c0p4ketw1b4vf","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsng00a30p4k4xap31ju"},{"post_id":"clzy4tsmq000g0p4kf42o7x9n","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsng00a60p4k3sgd5brh"},{"post_id":"clzy4tsmq000g0p4kf42o7x9n","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsng00a80p4k02mb7nis"},{"post_id":"clzy4tsmq000g0p4kf42o7x9n","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsng00ab0p4kc9xr4qo1"},{"post_id":"clzy4tsmw001e0p4k5v58h187","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsng00ad0p4k2jz8350o"},{"post_id":"clzy4tsmw001e0p4k5v58h187","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsng00ag0p4k9jxqfdcb"},{"post_id":"clzy4tsmw001e0p4k5v58h187","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsng00ai0p4kbcgt9c17"},{"post_id":"clzy4tsmw001g0p4k0k4vhutb","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsng00al0p4k4ol0gfoy"},{"post_id":"clzy4tsmw001g0p4k0k4vhutb","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnh00an0p4khzd47zhh"},{"post_id":"clzy4tsmw001g0p4k0k4vhutb","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnh00aq0p4kbdwl185t"},{"post_id":"clzy4tsmm00070p4k05cz5zt9","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnh00as0p4k54xb5kv6"},{"post_id":"clzy4tsmm00070p4k05cz5zt9","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnh00av0p4kdkwldnl4"},{"post_id":"clzy4tsmm00070p4k05cz5zt9","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnh00ax0p4k5dt5dsmi"},{"post_id":"clzy4tsmw001i0p4k2olbb79a","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnh00b00p4kg8jee7vi"},{"post_id":"clzy4tsmw001i0p4k2olbb79a","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnh00b20p4kgb44ajas"},{"post_id":"clzy4tsmw001i0p4k2olbb79a","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnh00b50p4k5t3lbwir"},{"post_id":"clzy4tsmx001l0p4k0q3edf3p","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnh00b70p4kej0z8i4z"},{"post_id":"clzy4tsmx001l0p4k0q3edf3p","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnh00ba0p4k9ug7ax3q"},{"post_id":"clzy4tsmx001l0p4k0q3edf3p","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsni00bc0p4k06a8a48w"},{"post_id":"clzy4tsmq000h0p4kf1bk55ty","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsni00bf0p4k2om3byie"},{"post_id":"clzy4tsmq000h0p4kf1bk55ty","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsni00bh0p4kc84sfn4d"},{"post_id":"clzy4tsmq000h0p4kf1bk55ty","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsni00bk0p4k61d62r64"},{"post_id":"clzy4tsmx001o0p4kcld16xa6","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsni00bm0p4kc3wq4i3b"},{"post_id":"clzy4tsmx001o0p4kcld16xa6","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsni00bp0p4khplhgjya"},{"post_id":"clzy4tsmx001o0p4kcld16xa6","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsni00br0p4kfqxj6hha"},{"post_id":"clzy4tsns00gh0p4k0itoh1bc","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnt00gn0p4k2js9da5y"},{"post_id":"clzy4tsns00gh0p4k0itoh1bc","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnu00gr0p4kdxc0h4wz"},{"post_id":"clzy4tsns00gh0p4k0itoh1bc","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnu00gu0p4k8j8v1j3u"},{"post_id":"clzy4tsns00gi0p4k5muigr4a","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnv00gx0p4k1fdk9sbc"},{"post_id":"clzy4tsns00gi0p4k5muigr4a","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnv00gz0p4kh57vc3o8"},{"post_id":"clzy4tsns00gi0p4k5muigr4a","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnv00h10p4k0fve88zt"},{"post_id":"clzy4tsnt00gk0p4kbe1j6qt9","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnv00h40p4kdny70lhj"},{"post_id":"clzy4tsnt00gk0p4kbe1j6qt9","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnv00h60p4kdedlgkt0"},{"post_id":"clzy4tsnt00gk0p4kbe1j6qt9","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnv00h80p4kfkj6coqq"},{"post_id":"clzy4tsnt00gm0p4k5nza3nwg","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnv00ha0p4kcpt308d4"},{"post_id":"clzy4tsnt00gm0p4k5nza3nwg","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnv00hc0p4kdw3reaib"},{"post_id":"clzy4tsnt00gm0p4k5nza3nwg","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnv00he0p4k4enhfl3n"},{"post_id":"clzy4tsnt00gp0p4k1f5c31jk","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnv00hg0p4kejap9zia"},{"post_id":"clzy4tsnt00gp0p4k1f5c31jk","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnv00hi0p4kdku900b4"},{"post_id":"clzy4tsnt00gp0p4k1f5c31jk","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnv00hk0p4khcoz1nnq"},{"post_id":"clzy4tsnu00gt0p4k1o8y5ky2","category_id":"clzy4tsmk00040p4k7t9c6gj9","_id":"clzy4tsnw00hm0p4kdk34gvo6"},{"post_id":"clzy4tsnu00gt0p4k1o8y5ky2","category_id":"clzy4tsmr000i0p4k2xfhcwq7","_id":"clzy4tsnw00ho0p4k3ljy2xi2"},{"post_id":"clzy4tsnu00gt0p4k1o8y5ky2","category_id":"clzy4tsmx001p0p4k0s1z5pt2","_id":"clzy4tsnw00hq0p4k56wl8ern"}],"PostTag":[{"post_id":"clzy4tsmh00010p4k4cqqejre","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsmt000t0p4k9yy62vjg"},{"post_id":"clzy4tsmh00010p4k4cqqejre","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsmt000w0p4k72fyaxxn"},{"post_id":"clzy4tsmh00010p4k4cqqejre","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsmt000z0p4k2c6qeuml"},{"post_id":"clzy4tsmh00010p4k4cqqejre","tag_id":"clzy4tsmr000j0p4kc56d3t8g","_id":"clzy4tsmu00120p4khdpk5km1"},{"post_id":"clzy4tsmh00010p4k4cqqejre","tag_id":"clzy4tsms000n0p4k8q8rauyp","_id":"clzy4tsmu00150p4k5b911kwc"},{"post_id":"clzy4tsmj00030p4ka4mbe3g6","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsmx001k0p4khv8igw4u"},{"post_id":"clzy4tsmj00030p4ka4mbe3g6","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsmx001m0p4k5ib376bp"},{"post_id":"clzy4tsmj00030p4ka4mbe3g6","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsmx001q0p4kbn1k52yk"},{"post_id":"clzy4tsmj00030p4ka4mbe3g6","tag_id":"clzy4tsmv00190p4k06w695nk","_id":"clzy4tsmy001s0p4kb2x1g289"},{"post_id":"clzy4tsmj00030p4ka4mbe3g6","tag_id":"clzy4tsmw001d0p4k0xfg962g","_id":"clzy4tsmy001v0p4kb7fq9jhh"},{"post_id":"clzy4tsn1002m0p4k0ski0myy","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsn2002s0p4ke1ux3f66"},{"post_id":"clzy4tsn1002m0p4k0ski0myy","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsn2002w0p4k36qhd5fv"},{"post_id":"clzy4tsn1002m0p4k0ski0myy","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsn200300p4kdu5m73pe"},{"post_id":"clzy4tsn1002m0p4k0ski0myy","tag_id":"clzy4tsmy001y0p4kehblfgwa","_id":"clzy4tsn300340p4kcyuigq0n"},{"post_id":"clzy4tsmm00070p4k05cz5zt9","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsn300380p4kg7f68g0z"},{"post_id":"clzy4tsmm00070p4k05cz5zt9","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsn3003c0p4k438o5sly"},{"post_id":"clzy4tsmm00070p4k05cz5zt9","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsn4003f0p4kex0z1x3z"},{"post_id":"clzy4tsmm00070p4k05cz5zt9","tag_id":"clzy4tsmy001y0p4kehblfgwa","_id":"clzy4tsn4003i0p4k16r5a75c"},{"post_id":"clzy4tsmm00070p4k05cz5zt9","tag_id":"clzy4tsmz00240p4k7a2w22no","_id":"clzy4tsn4003l0p4k1zqbejn1"},{"post_id":"clzy4tsmm00070p4k05cz5zt9","tag_id":"clzy4tsn0002b0p4k81ka8sny","_id":"clzy4tsn4003o0p4kgofx2gs7"},{"post_id":"clzy4tsmm00070p4k05cz5zt9","tag_id":"clzy4tsn0002h0p4kgaw2b8aj","_id":"clzy4tsn4003r0p4k7rnu01o3"},{"post_id":"clzy4tsmm00070p4k05cz5zt9","tag_id":"clzy4tsn1002p0p4k95il2t3w","_id":"clzy4tsn4003u0p4k2d382xd4"},{"post_id":"clzy4tsmm00080p4k7vhx37f3","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsn5003x0p4k9wlyesu2"},{"post_id":"clzy4tsmm00080p4k7vhx37f3","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsn500400p4k9t65dx7x"},{"post_id":"clzy4tsmm00080p4k7vhx37f3","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsn500430p4k2vb3bajh"},{"post_id":"clzy4tsmm00080p4k7vhx37f3","tag_id":"clzy4tsmy001y0p4kehblfgwa","_id":"clzy4tsn500460p4kbr6fhor0"},{"post_id":"clzy4tsmm00080p4k7vhx37f3","tag_id":"clzy4tsn4003j0p4k0w04dy1y","_id":"clzy4tsn500490p4kdmp125no"},{"post_id":"clzy4tsmm00080p4k7vhx37f3","tag_id":"clzy4tsmz00240p4k7a2w22no","_id":"clzy4tsn5004c0p4k05hcc9uc"},{"post_id":"clzy4tsmn00090p4k39vfat2y","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsn6004j0p4kf7rf195o"},{"post_id":"clzy4tsmn00090p4k39vfat2y","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsn6004m0p4k63ty41ca"},{"post_id":"clzy4tsmn00090p4k39vfat2y","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsn6004p0p4kbf0p8zuj"},{"post_id":"clzy4tsmn00090p4k39vfat2y","tag_id":"clzy4tsmr000j0p4kc56d3t8g","_id":"clzy4tsn6004s0p4k2x61dwsf"},{"post_id":"clzy4tsmn00090p4k39vfat2y","tag_id":"clzy4tsms000n0p4k8q8rauyp","_id":"clzy4tsn6004v0p4k83lyfio6"},{"post_id":"clzy4tsmp000c0p4k9p4u62qk","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsn6004y0p4k02gjg5zw"},{"post_id":"clzy4tsmp000c0p4k9p4u62qk","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsn600510p4kcudn13bj"},{"post_id":"clzy4tsmp000c0p4k9p4u62qk","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsn600540p4k4swt9we2"},{"post_id":"clzy4tsmp000c0p4k9p4u62qk","tag_id":"clzy4tsn6004n0p4kalg4cfov","_id":"clzy4tsn700570p4kbhkzhzc8"},{"post_id":"clzy4tsmp000d0p4k8ivqhhwc","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsn7005e0p4k6yjb0c7x"},{"post_id":"clzy4tsmp000d0p4k8ivqhhwc","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsn7005g0p4k618dhiyc"},{"post_id":"clzy4tsmp000d0p4k8ivqhhwc","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsn7005k0p4keajf2432"},{"post_id":"clzy4tsmp000d0p4k8ivqhhwc","tag_id":"clzy4tsn6004z0p4k4lfq3b8a","_id":"clzy4tsn7005m0p4kcnkvdx9u"},{"post_id":"clzy4tsmp000d0p4k8ivqhhwc","tag_id":"clzy4tsn600560p4k3vuw8tdx","_id":"clzy4tsn7005q0p4k064n67bg"},{"post_id":"clzy4tsmq000g0p4kf42o7x9n","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsn8005z0p4k5pg642k0"},{"post_id":"clzy4tsmq000g0p4kf42o7x9n","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsn800610p4keiwo0u6a"},{"post_id":"clzy4tsmq000g0p4kf42o7x9n","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsn800640p4k4d8n6y6a"},{"post_id":"clzy4tsmq000g0p4kf42o7x9n","tag_id":"clzy4tsn0002h0p4kgaw2b8aj","_id":"clzy4tsn800660p4khgbkg6c5"},{"post_id":"clzy4tsmq000g0p4kf42o7x9n","tag_id":"clzy4tsn0002b0p4k81ka8sny","_id":"clzy4tsnb00690p4k0mft20n2"},{"post_id":"clzy4tsmq000g0p4kf42o7x9n","tag_id":"clzy4tsn7005n0p4k143sf6k4","_id":"clzy4tsnb006b0p4k10i4dkvx"},{"post_id":"clzy4tsmq000g0p4kf42o7x9n","tag_id":"clzy4tsn8005s0p4k20na25iu","_id":"clzy4tsnb006e0p4kclo08qjy"},{"post_id":"clzy4tsmq000h0p4kf1bk55ty","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnb006g0p4k5tnl77mq"},{"post_id":"clzy4tsmq000h0p4kf1bk55ty","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnb006j0p4k4206g1iv"},{"post_id":"clzy4tsmq000h0p4kf1bk55ty","tag_id":"clzy4tsn8005w0p4kgkil00ow","_id":"clzy4tsnc006l0p4ke07t3y2h"},{"post_id":"clzy4tsmr000k0p4k1ioxckic","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnc006n0p4k34ui0qxp"},{"post_id":"clzy4tsmr000k0p4k1ioxckic","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnc006q0p4k7449fh04"},{"post_id":"clzy4tsmr000k0p4k1ioxckic","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnc006s0p4k7kcz68ek"},{"post_id":"clzy4tsmr000k0p4k1ioxckic","tag_id":"clzy4tsn800630p4kf2gmcx95","_id":"clzy4tsnc006v0p4kd2pc1l12"},{"post_id":"clzy4tsmr000l0p4k37uv5eye","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnc006x0p4kfv1e24o9"},{"post_id":"clzy4tsmr000l0p4k37uv5eye","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnc00700p4k3emv479g"},{"post_id":"clzy4tsmr000l0p4k37uv5eye","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnc00720p4k32oif606"},{"post_id":"clzy4tsmr000l0p4k37uv5eye","tag_id":"clzy4tsn800680p4k6es3enfp","_id":"clzy4tsnc00750p4k031k8fe1"},{"post_id":"clzy4tsms000o0p4kcl5g8m2j","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnc00770p4ke0jph6u1"},{"post_id":"clzy4tsms000o0p4kcl5g8m2j","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnc00790p4k9hzbcozv"},{"post_id":"clzy4tsms000o0p4kcl5g8m2j","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnc007c0p4k50kxdkda"},{"post_id":"clzy4tsms000o0p4kcl5g8m2j","tag_id":"clzy4tsn800680p4k6es3enfp","_id":"clzy4tsnc007e0p4kdkdw6zbn"},{"post_id":"clzy4tsms000p0p4kcps7453c","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnd007h0p4kcf8cb9q0"},{"post_id":"clzy4tsms000p0p4kcps7453c","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnd007j0p4ka556cmsl"},{"post_id":"clzy4tsms000p0p4kcps7453c","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnd007m0p4kcb2nbgnn"},{"post_id":"clzy4tsms000p0p4kcps7453c","tag_id":"clzy4tsnb006i0p4kay1h7uoq","_id":"clzy4tsnd007o0p4k7rq43tn9"},{"post_id":"clzy4tsms000p0p4kcps7453c","tag_id":"clzy4tsnc006o0p4kcv6f5ohq","_id":"clzy4tsnd007r0p4k36x7dh1s"},{"post_id":"clzy4tsms000p0p4kcps7453c","tag_id":"clzy4tsnc006u0p4kakph8ut2","_id":"clzy4tsnd007t0p4kgygvg4yn"},{"post_id":"clzy4tsms000r0p4k1fhha0b5","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnd007w0p4k5dnggphx"},{"post_id":"clzy4tsms000r0p4k1fhha0b5","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnd007y0p4kgckja51z"},{"post_id":"clzy4tsms000r0p4k1fhha0b5","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnd00810p4k5m9ahyvr"},{"post_id":"clzy4tsms000r0p4k1fhha0b5","tag_id":"clzy4tsnc006z0p4k92a58cu0","_id":"clzy4tsnd00830p4kc8tj8kiq"},{"post_id":"clzy4tsms000r0p4k1fhha0b5","tag_id":"clzy4tsnc00740p4kbcxo36mz","_id":"clzy4tsnd00850p4k15acckev"},{"post_id":"clzy4tsms000r0p4k1fhha0b5","tag_id":"clzy4tsnc007a0p4k6wy44kwo","_id":"clzy4tsnd00880p4k9alof10s"},{"post_id":"clzy4tsms000r0p4k1fhha0b5","tag_id":"clzy4tsnd007g0p4kfef1fc9x","_id":"clzy4tsne008a0p4ka82qfeeb"},{"post_id":"clzy4tsms000r0p4k1fhha0b5","tag_id":"clzy4tsnd007l0p4kgb945tnw","_id":"clzy4tsne008d0p4ka56qfkay"},{"post_id":"clzy4tsmt000u0p4kenajh0xm","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsne008f0p4kg0uc042y"},{"post_id":"clzy4tsmt000u0p4kenajh0xm","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsne008i0p4kd45men1c"},{"post_id":"clzy4tsmt000u0p4kenajh0xm","tag_id":"clzy4tsn8005w0p4kgkil00ow","_id":"clzy4tsne008k0p4khk23cr8d"},{"post_id":"clzy4tsmt000x0p4k0a8r1fh9","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsne008n0p4k59msh9u0"},{"post_id":"clzy4tsmt000x0p4k0a8r1fh9","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsne008p0p4k15xbdaba"},{"post_id":"clzy4tsmt000x0p4k0a8r1fh9","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsne008s0p4kfaia6r6s"},{"post_id":"clzy4tsmt000x0p4k0a8r1fh9","tag_id":"clzy4tsmy001y0p4kehblfgwa","_id":"clzy4tsne008u0p4k97mz0kpv"},{"post_id":"clzy4tsmt000x0p4k0a8r1fh9","tag_id":"clzy4tsnd00800p4kdxmmgspw","_id":"clzy4tsne008x0p4k5or894ur"},{"post_id":"clzy4tsmt000x0p4k0a8r1fh9","tag_id":"clzy4tsmv00190p4k06w695nk","_id":"clzy4tsne008z0p4k64bzdnsr"},{"post_id":"clzy4tsmt00100p4ketyogyon","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsne00910p4kd8zb2bhk"},{"post_id":"clzy4tsmt00100p4ketyogyon","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnf00940p4khr4q5s23"},{"post_id":"clzy4tsmt00100p4ketyogyon","tag_id":"clzy4tsn8005w0p4kgkil00ow","_id":"clzy4tsnf00960p4k4wej2wgt"},{"post_id":"clzy4tsmu00130p4k6fjxdg86","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnf009d0p4kbmny7b96"},{"post_id":"clzy4tsmu00130p4k6fjxdg86","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnf009f0p4k7t4y3bdd"},{"post_id":"clzy4tsmu00130p4k6fjxdg86","tag_id":"clzy4tsne008g0p4kcgyjdv7z","_id":"clzy4tsnf009i0p4kda5oezl7"},{"post_id":"clzy4tsmu00130p4k6fjxdg86","tag_id":"clzy4tsne008m0p4k8k9s2nxx","_id":"clzy4tsnf009k0p4kgnbc3vf5"},{"post_id":"clzy4tsmu00130p4k6fjxdg86","tag_id":"clzy4tsne008r0p4k16otcni1","_id":"clzy4tsnf009n0p4k5x0od2sg"},{"post_id":"clzy4tsmu00130p4k6fjxdg86","tag_id":"clzy4tsne008w0p4k51oo4kqk","_id":"clzy4tsnf009p0p4kgqbv16oz"},{"post_id":"clzy4tsmu00130p4k6fjxdg86","tag_id":"clzy4tsne00920p4kdoateh4i","_id":"clzy4tsng009s0p4kdebz7bm8"},{"post_id":"clzy4tsmu00130p4k6fjxdg86","tag_id":"clzy4tsnf00980p4k7h4w9l9q","_id":"clzy4tsng009u0p4k0npj6hqd"},{"post_id":"clzy4tsmu00160p4k621p61y5","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsng009x0p4kdn8z6imr"},{"post_id":"clzy4tsmu00160p4k621p61y5","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsng009z0p4kckiqhpbn"},{"post_id":"clzy4tsmu00160p4k621p61y5","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsng00a20p4k5xwt1ouq"},{"post_id":"clzy4tsmu00160p4k621p61y5","tag_id":"clzy4tsn800680p4k6es3enfp","_id":"clzy4tsng00a40p4kdbd41v0i"},{"post_id":"clzy4tsmv00180p4kgx3k0lrt","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsng00a70p4kfmmc74bg"},{"post_id":"clzy4tsmv00180p4kgx3k0lrt","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsng00a90p4k7rc1hyq6"},{"post_id":"clzy4tsmv00180p4kgx3k0lrt","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsng00ac0p4kfeoh3am0"},{"post_id":"clzy4tsmv00180p4kgx3k0lrt","tag_id":"clzy4tsnb006i0p4kay1h7uoq","_id":"clzy4tsng00ae0p4k49cracw9"},{"post_id":"clzy4tsmv00180p4kgx3k0lrt","tag_id":"clzy4tsnf009l0p4k8j0v7jdq","_id":"clzy4tsng00ah0p4k3ojaaezp"},{"post_id":"clzy4tsmv001a0p4kfweq0629","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsng00aj0p4ke7g5gdjt"},{"post_id":"clzy4tsmv001a0p4kfweq0629","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsng00am0p4kdd1ihojj"},{"post_id":"clzy4tsmv001a0p4kfweq0629","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnh00ao0p4k8f1xcir0"},{"post_id":"clzy4tsmv001a0p4kfweq0629","tag_id":"clzy4tsn0002h0p4kgaw2b8aj","_id":"clzy4tsnh00ar0p4k2xal9uw3"},{"post_id":"clzy4tsmv001a0p4kfweq0629","tag_id":"clzy4tsn0002b0p4k81ka8sny","_id":"clzy4tsnh00at0p4k74reh02h"},{"post_id":"clzy4tsmv001a0p4kfweq0629","tag_id":"clzy4tsn7005n0p4k143sf6k4","_id":"clzy4tsnh00aw0p4k90sx3xow"},{"post_id":"clzy4tsmv001a0p4kfweq0629","tag_id":"clzy4tsn8005s0p4k20na25iu","_id":"clzy4tsnh00ay0p4k4hjogme9"},{"post_id":"clzy4tsmv001c0p4ketw1b4vf","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnh00b10p4kcfz141wn"},{"post_id":"clzy4tsmv001c0p4ketw1b4vf","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnh00b30p4kbqq7amvu"},{"post_id":"clzy4tsmv001c0p4ketw1b4vf","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnh00b60p4k0sf02ue3"},{"post_id":"clzy4tsmv001c0p4ketw1b4vf","tag_id":"clzy4tsn0002h0p4kgaw2b8aj","_id":"clzy4tsnh00b80p4k4w0lerku"},{"post_id":"clzy4tsmv001c0p4ketw1b4vf","tag_id":"clzy4tsn0002b0p4k81ka8sny","_id":"clzy4tsni00bb0p4k3pgu7m3p"},{"post_id":"clzy4tsmv001c0p4ketw1b4vf","tag_id":"clzy4tsn7005n0p4k143sf6k4","_id":"clzy4tsni00bd0p4k7gdye0nt"},{"post_id":"clzy4tsmv001c0p4ketw1b4vf","tag_id":"clzy4tsn8005s0p4k20na25iu","_id":"clzy4tsni00bg0p4k5m5n9jrq"},{"post_id":"clzy4tsmw001e0p4k5v58h187","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsni00bi0p4k6p2u64a4"},{"post_id":"clzy4tsmw001e0p4k5v58h187","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsni00bl0p4kag0odf0k"},{"post_id":"clzy4tsmw001e0p4k5v58h187","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsni00bn0p4k5lcd2v48"},{"post_id":"clzy4tsmw001e0p4k5v58h187","tag_id":"clzy4tsn800630p4kf2gmcx95","_id":"clzy4tsni00bq0p4kdtcdgkmg"},{"post_id":"clzy4tsmw001g0p4k0k4vhutb","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsni00bs0p4kehh77tlp"},{"post_id":"clzy4tsmw001g0p4k0k4vhutb","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsni00bu0p4k7ijia93o"},{"post_id":"clzy4tsmw001g0p4k0k4vhutb","tag_id":"clzy4tsn8005w0p4kgkil00ow","_id":"clzy4tsni00bv0p4k4xt588pn"},{"post_id":"clzy4tsmw001i0p4k2olbb79a","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsni00bx0p4kf5ptbeno"},{"post_id":"clzy4tsmw001i0p4k2olbb79a","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsni00by0p4k92qwa36g"},{"post_id":"clzy4tsmw001i0p4k2olbb79a","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsni00c00p4kfaj11731"},{"post_id":"clzy4tsmw001i0p4k2olbb79a","tag_id":"clzy4tsnh00b40p4k6eny10c6","_id":"clzy4tsni00c10p4k2dkk6n0t"},{"post_id":"clzy4tsmw001i0p4k2olbb79a","tag_id":"clzy4tsmv00190p4k06w695nk","_id":"clzy4tsnj00c30p4kgo34hn11"},{"post_id":"clzy4tsmw001i0p4k2olbb79a","tag_id":"clzy4tsmz00240p4k7a2w22no","_id":"clzy4tsnj00c40p4k3j2uc6i8"},{"post_id":"clzy4tsmw001i0p4k2olbb79a","tag_id":"clzy4tsn0002b0p4k81ka8sny","_id":"clzy4tsnj00c60p4k2g9c3g8n"},{"post_id":"clzy4tsmw001i0p4k2olbb79a","tag_id":"clzy4tsnb006i0p4kay1h7uoq","_id":"clzy4tsnj00c70p4kck7ec9um"},{"post_id":"clzy4tsmx001l0p4k0q3edf3p","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnj00c90p4k4hmeb47m"},{"post_id":"clzy4tsmx001l0p4k0q3edf3p","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnj00ca0p4khch5dbij"},{"post_id":"clzy4tsmx001l0p4k0q3edf3p","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnj00cc0p4kapf80c10"},{"post_id":"clzy4tsmx001l0p4k0q3edf3p","tag_id":"clzy4tsn0002h0p4kgaw2b8aj","_id":"clzy4tsnj00cd0p4kaec3bsxj"},{"post_id":"clzy4tsmx001l0p4k0q3edf3p","tag_id":"clzy4tsn0002b0p4k81ka8sny","_id":"clzy4tsnj00cf0p4k7xf98tr5"},{"post_id":"clzy4tsmx001l0p4k0q3edf3p","tag_id":"clzy4tsn7005n0p4k143sf6k4","_id":"clzy4tsnj00cg0p4khozc8qc3"},{"post_id":"clzy4tsmx001l0p4k0q3edf3p","tag_id":"clzy4tsn8005s0p4k20na25iu","_id":"clzy4tsnj00ci0p4kfga5gikg"},{"post_id":"clzy4tsmx001o0p4kcld16xa6","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnj00cj0p4k80e944jb"},{"post_id":"clzy4tsmx001o0p4kcld16xa6","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnj00cl0p4kcgwvenp2"},{"post_id":"clzy4tsmx001o0p4kcld16xa6","tag_id":"clzy4tsn8005w0p4kgkil00ow","_id":"clzy4tsnj00cm0p4k1zbzh4ft"},{"post_id":"clzy4tsmx001r0p4kfkhz57lw","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnk00co0p4k5cj19q88"},{"post_id":"clzy4tsmx001r0p4kfkhz57lw","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnk00cp0p4khpuk5q9a"},{"post_id":"clzy4tsmx001r0p4kfkhz57lw","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnk00cr0p4kh5zof6yz"},{"post_id":"clzy4tsmx001r0p4kfkhz57lw","tag_id":"clzy4tsmv00190p4k06w695nk","_id":"clzy4tsnk00cs0p4k8ebiamnr"},{"post_id":"clzy4tsmx001r0p4kfkhz57lw","tag_id":"clzy4tsmz00240p4k7a2w22no","_id":"clzy4tsnk00cu0p4kd4vbecav"},{"post_id":"clzy4tsmx001r0p4kfkhz57lw","tag_id":"clzy4tsn0002b0p4k81ka8sny","_id":"clzy4tsnk00cv0p4k4s50autl"},{"post_id":"clzy4tsmx001r0p4kfkhz57lw","tag_id":"clzy4tsnb006i0p4kay1h7uoq","_id":"clzy4tsnk00cx0p4k7iwn4puh"},{"post_id":"clzy4tsmy001u0p4kbhyla7a5","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnk00cy0p4kc70zb86d"},{"post_id":"clzy4tsmy001u0p4kbhyla7a5","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnk00cz0p4k4rmn5gmk"},{"post_id":"clzy4tsmy001u0p4kbhyla7a5","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnk00d10p4kce3u7nqo"},{"post_id":"clzy4tsmy001u0p4kbhyla7a5","tag_id":"clzy4tsnc006z0p4k92a58cu0","_id":"clzy4tsnk00d20p4k3urde2o8"},{"post_id":"clzy4tsmy001u0p4kbhyla7a5","tag_id":"clzy4tsnd007g0p4kfef1fc9x","_id":"clzy4tsnk00d40p4kf4l1awf0"},{"post_id":"clzy4tsmy001u0p4kbhyla7a5","tag_id":"clzy4tsnd007l0p4kgb945tnw","_id":"clzy4tsnk00d50p4k6sxef7go"},{"post_id":"clzy4tsmy001x0p4ka1x236kq","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnk00d70p4kep815p83"},{"post_id":"clzy4tsmy001x0p4ka1x236kq","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnl00d80p4kg0rv3252"},{"post_id":"clzy4tsmy001x0p4ka1x236kq","tag_id":"clzy4tsn8005w0p4kgkil00ow","_id":"clzy4tsnl00da0p4kdosg4q5n"},{"post_id":"clzy4tsmy001z0p4kavkaedcm","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnl00db0p4kh8ne105r"},{"post_id":"clzy4tsmy001z0p4kavkaedcm","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnl00dd0p4k9b07btdn"},{"post_id":"clzy4tsmy001z0p4kavkaedcm","tag_id":"clzy4tsn8005w0p4kgkil00ow","_id":"clzy4tsnl00de0p4kcxy5flea"},{"post_id":"clzy4tsmz00220p4k0zzh79l1","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnl00df0p4k4xr7g2lk"},{"post_id":"clzy4tsmz00220p4k0zzh79l1","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnl00dh0p4k0ptz3iy7"},{"post_id":"clzy4tsmz00220p4k0zzh79l1","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnl00di0p4kb5i5c5on"},{"post_id":"clzy4tsmz00220p4k0zzh79l1","tag_id":"clzy4tsmy001y0p4kehblfgwa","_id":"clzy4tsnl00dk0p4kaevrd3i0"},{"post_id":"clzy4tsmz00220p4k0zzh79l1","tag_id":"clzy4tsnk00ct0p4kaxt9hff3","_id":"clzy4tsnl00dl0p4k2om7gchj"},{"post_id":"clzy4tsmz00220p4k0zzh79l1","tag_id":"clzy4tsnk00cw0p4k3i9j30us","_id":"clzy4tsnl00dn0p4kf7rxe1v7"},{"post_id":"clzy4tsmz00220p4k0zzh79l1","tag_id":"clzy4tsnk00d00p4k2o0t8fxy","_id":"clzy4tsnl00do0p4kfgxfffdc"},{"post_id":"clzy4tsmz00220p4k0zzh79l1","tag_id":"clzy4tsn800680p4k6es3enfp","_id":"clzy4tsnm00dq0p4k5it76tkf"},{"post_id":"clzy4tsmz00250p4k2ui96aqa","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnm00dr0p4k8niogv8u"},{"post_id":"clzy4tsmz00250p4k2ui96aqa","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnm00dt0p4k3joq6y4m"},{"post_id":"clzy4tsmz00250p4k2ui96aqa","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnm00du0p4k9hx1hbve"},{"post_id":"clzy4tsmz00250p4k2ui96aqa","tag_id":"clzy4tsmy001y0p4kehblfgwa","_id":"clzy4tsnm00dw0p4kd4g3fbgq"},{"post_id":"clzy4tsmz00250p4k2ui96aqa","tag_id":"clzy4tsnk00d60p4kbmdb75fy","_id":"clzy4tsnm00dx0p4khjkkb5bj"},{"post_id":"clzy4tsmz00280p4k2nfvggv5","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnm00dz0p4k3on277tj"},{"post_id":"clzy4tsmz00280p4k2nfvggv5","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnm00e00p4k7qbw3lw9"},{"post_id":"clzy4tsmz00280p4k2nfvggv5","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnm00e10p4k45j9hgsz"},{"post_id":"clzy4tsmz00280p4k2nfvggv5","tag_id":"clzy4tsmy001y0p4kehblfgwa","_id":"clzy4tsnm00e30p4k0ijhb3h1"},{"post_id":"clzy4tsmz00280p4k2nfvggv5","tag_id":"clzy4tsnl00d90p4keyqsdupb","_id":"clzy4tsnm00e40p4kain9hwjl"},{"post_id":"clzy4tsmz00280p4k2nfvggv5","tag_id":"clzy4tsnl00dc0p4kakdihwe9","_id":"clzy4tsnm00e60p4k4ctn5vqi"},{"post_id":"clzy4tsmz00280p4k2nfvggv5","tag_id":"clzy4tsmz00240p4k7a2w22no","_id":"clzy4tsnm00e70p4kdsvtcica"},{"post_id":"clzy4tsn0002a0p4kh8jtd4dw","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnn00e90p4k6d9r1570"},{"post_id":"clzy4tsn0002a0p4kh8jtd4dw","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnn00ea0p4k27xm364s"},{"post_id":"clzy4tsn0002a0p4kh8jtd4dw","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnn00ec0p4kfzemhq3o"},{"post_id":"clzy4tsn0002a0p4kh8jtd4dw","tag_id":"clzy4tsmy001y0p4kehblfgwa","_id":"clzy4tsnn00ed0p4k2b9abp4f"},{"post_id":"clzy4tsn0002a0p4kh8jtd4dw","tag_id":"clzy4tsnl00d90p4keyqsdupb","_id":"clzy4tsnn00ef0p4k050zcacg"},{"post_id":"clzy4tsn0002a0p4kh8jtd4dw","tag_id":"clzy4tsnl00dc0p4kakdihwe9","_id":"clzy4tsnn00eg0p4ke2bz3e9j"},{"post_id":"clzy4tsn0002a0p4kh8jtd4dw","tag_id":"clzy4tsnl00dm0p4k4ohf6e9a","_id":"clzy4tsnn00ei0p4kg9bn51u5"},{"post_id":"clzy4tsn0002a0p4kh8jtd4dw","tag_id":"clzy4tsn7005n0p4k143sf6k4","_id":"clzy4tsnn00ej0p4kbqbn8kze"},{"post_id":"clzy4tsn0002a0p4kh8jtd4dw","tag_id":"clzy4tsnm00ds0p4khteuhe5l","_id":"clzy4tsnn00el0p4k6ko3fl39"},{"post_id":"clzy4tsn0002a0p4kh8jtd4dw","tag_id":"clzy4tsnm00dv0p4kfmdd7hhs","_id":"clzy4tsnn00em0p4kd01wc06y"},{"post_id":"clzy4tsn0002a0p4kh8jtd4dw","tag_id":"clzy4tsnm00dy0p4k84oaftq5","_id":"clzy4tsnn00eo0p4k9yvqbnpz"},{"post_id":"clzy4tsn0002e0p4k3m414qtc","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsno00ep0p4khn7s705x"},{"post_id":"clzy4tsn0002e0p4k3m414qtc","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsno00er0p4k37q42gn2"},{"post_id":"clzy4tsn0002e0p4k3m414qtc","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsno00es0p4k13u61xh4"},{"post_id":"clzy4tsn0002e0p4k3m414qtc","tag_id":"clzy4tsn800680p4k6es3enfp","_id":"clzy4tsno00et0p4katkygbz2"},{"post_id":"clzy4tsn0002e0p4k3m414qtc","tag_id":"clzy4tsnm00e50p4kasifdvgo","_id":"clzy4tsno00ev0p4kf8723f12"},{"post_id":"clzy4tsn0002g0p4kcz8k0zgz","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsno00ew0p4k23ju6mkm"},{"post_id":"clzy4tsn0002g0p4kcz8k0zgz","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsno00ey0p4kcaunhtme"},{"post_id":"clzy4tsn0002g0p4kcz8k0zgz","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsno00ez0p4k6nmd4heu"},{"post_id":"clzy4tsn0002g0p4kcz8k0zgz","tag_id":"clzy4tsmy001y0p4kehblfgwa","_id":"clzy4tsno00f10p4kg0ovfdbo"},{"post_id":"clzy4tsn0002g0p4kcz8k0zgz","tag_id":"clzy4tsnm00e80p4k3qf7c98n","_id":"clzy4tsno00f20p4kd6g26mwx"},{"post_id":"clzy4tsn0002g0p4kcz8k0zgz","tag_id":"clzy4tsn800680p4k6es3enfp","_id":"clzy4tsnp00f40p4k7iu2aqlh"},{"post_id":"clzy4tsn0002g0p4kcz8k0zgz","tag_id":"clzy4tsmz00240p4k7a2w22no","_id":"clzy4tsnp00f50p4k6zi22f3y"},{"post_id":"clzy4tsn0002g0p4kcz8k0zgz","tag_id":"clzy4tsnn00ee0p4kdm969vf3","_id":"clzy4tsnp00f70p4k94rlbeos"},{"post_id":"clzy4tsn1002k0p4k3atkhqd7","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnp00f80p4k2y2eg1ti"},{"post_id":"clzy4tsn1002k0p4k3atkhqd7","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnp00fa0p4k0flw37zd"},{"post_id":"clzy4tsn1002k0p4k3atkhqd7","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnp00fb0p4khvlygtqn"},{"post_id":"clzy4tsn1002k0p4k3atkhqd7","tag_id":"clzy4tsmy001y0p4kehblfgwa","_id":"clzy4tsnp00fd0p4k9pph6l3d"},{"post_id":"clzy4tsn1002k0p4k3atkhqd7","tag_id":"clzy4tsn800680p4k6es3enfp","_id":"clzy4tsnp00fe0p4kbu9w1baa"},{"post_id":"clzy4tsn1002q0p4k23kd4w80","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnp00fg0p4k2bhcgmct"},{"post_id":"clzy4tsn1002q0p4k23kd4w80","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnp00fh0p4k1cab4gz9"},{"post_id":"clzy4tsn1002q0p4k23kd4w80","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnp00fj0p4kapordixs"},{"post_id":"clzy4tsn1002q0p4k23kd4w80","tag_id":"clzy4tsn800680p4k6es3enfp","_id":"clzy4tsnp00fk0p4k9kqidf9r"},{"post_id":"clzy4tsn1002q0p4k23kd4w80","tag_id":"clzy4tsnm00e50p4kasifdvgo","_id":"clzy4tsnp00fm0p4k663n5mm5"},{"post_id":"clzy4tsn2002t0p4k0mm281q5","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnp00fn0p4kh4rj4lsc"},{"post_id":"clzy4tsn2002t0p4k0mm281q5","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnq00fp0p4kbldw7s79"},{"post_id":"clzy4tsn2002t0p4k0mm281q5","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnq00fq0p4kcjdn7cxs"},{"post_id":"clzy4tsn2002t0p4k0mm281q5","tag_id":"clzy4tsn0002b0p4k81ka8sny","_id":"clzy4tsnq00fr0p4k07a4azy1"},{"post_id":"clzy4tsn2002t0p4k0mm281q5","tag_id":"clzy4tsno00eq0p4kcn2ahy6s","_id":"clzy4tsnq00fs0p4k7h3dd49t"},{"post_id":"clzy4tsn2002y0p4k0s0s5f0u","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnq00ft0p4k9cawao9n"},{"post_id":"clzy4tsn2002y0p4k0s0s5f0u","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnq00fu0p4kcb0v79id"},{"post_id":"clzy4tsn2002y0p4k0s0s5f0u","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnq00fv0p4k37mi78j0"},{"post_id":"clzy4tsn2002y0p4k0s0s5f0u","tag_id":"clzy4tsn800680p4k6es3enfp","_id":"clzy4tsnq00fw0p4kdxbw7oio"},{"post_id":"clzy4tsn2002y0p4k0s0s5f0u","tag_id":"clzy4tsmz00240p4k7a2w22no","_id":"clzy4tsnq00fx0p4kfjwib4jp"},{"post_id":"clzy4tsn200310p4k2z6uej8j","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnq00fy0p4kdma7cfa8"},{"post_id":"clzy4tsn200310p4k2z6uej8j","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnq00fz0p4kcr687n2e"},{"post_id":"clzy4tsn200310p4k2z6uej8j","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnq00g00p4kblmo3s07"},{"post_id":"clzy4tsn200310p4k2z6uej8j","tag_id":"clzy4tsn800680p4k6es3enfp","_id":"clzy4tsnq00g10p4k0cqj4itz"},{"post_id":"clzy4tsn300360p4k1je1669v","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnq00g20p4k0efc8o21"},{"post_id":"clzy4tsn300360p4k1je1669v","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnq00g30p4kg4rz6gmp"},{"post_id":"clzy4tsn300360p4k1je1669v","tag_id":"clzy4tsn8005w0p4kgkil00ow","_id":"clzy4tsnq00g40p4ke9wf9iot"},{"post_id":"clzy4tsn300390p4k1bq9epn3","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnq00g50p4kdc2s8bry"},{"post_id":"clzy4tsn300390p4k1bq9epn3","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnq00g60p4kbqt137wl"},{"post_id":"clzy4tsn300390p4k1bq9epn3","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnq00g70p4ka4vz49z6"},{"post_id":"clzy4tsn300390p4k1bq9epn3","tag_id":"clzy4tsmy001y0p4kehblfgwa","_id":"clzy4tsnq00g80p4kbxdp2v0v"},{"post_id":"clzy4tsn300390p4k1bq9epn3","tag_id":"clzy4tsno00f30p4k8gvg5qrv","_id":"clzy4tsnq00g90p4k9rfo17m9"},{"post_id":"clzy4tsn300390p4k1bq9epn3","tag_id":"clzy4tsnl00dm0p4k4ohf6e9a","_id":"clzy4tsnq00ga0p4kdiax9eff"},{"post_id":"clzy4tsn300390p4k1bq9epn3","tag_id":"clzy4tsn7005n0p4k143sf6k4","_id":"clzy4tsnq00gb0p4k2bdi4qtr"},{"post_id":"clzy4tsn300390p4k1bq9epn3","tag_id":"clzy4tsnm00ds0p4khteuhe5l","_id":"clzy4tsnq00gc0p4k4ty69hfh"},{"post_id":"clzy4tsn300390p4k1bq9epn3","tag_id":"clzy4tsnm00dv0p4kfmdd7hhs","_id":"clzy4tsnq00gd0p4kht68aowu"},{"post_id":"clzy4tsn300390p4k1bq9epn3","tag_id":"clzy4tsnm00dy0p4k84oaftq5","_id":"clzy4tsnq00ge0p4khik8e0jg"},{"post_id":"clzy4tsn300390p4k1bq9epn3","tag_id":"clzy4tsnp00fl0p4khix47c99","_id":"clzy4tsnq00gf0p4kf2ei8znr"},{"post_id":"clzy4tsn300390p4k1bq9epn3","tag_id":"clzy4tsnp00fo0p4k6xl0e9am","_id":"clzy4tsnq00gg0p4k3obraxk4"},{"post_id":"clzy4tsns00gi0p4k5muigr4a","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnt00gl0p4k1b044tll"},{"post_id":"clzy4tsns00gi0p4k5muigr4a","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnt00go0p4k2nayazhp"},{"post_id":"clzy4tsns00gi0p4k5muigr4a","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnu00gs0p4k584petwn"},{"post_id":"clzy4tsns00gi0p4k5muigr4a","tag_id":"clzy4tsmy001y0p4kehblfgwa","_id":"clzy4tsnu00gv0p4k0ehse8gz"},{"post_id":"clzy4tsns00gi0p4k5muigr4a","tag_id":"clzy4tsno00f30p4k8gvg5qrv","_id":"clzy4tsnv00gy0p4k15ug7pf5"},{"post_id":"clzy4tsnt00gm0p4k5nza3nwg","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnv00h00p4k3lllarsq"},{"post_id":"clzy4tsnt00gm0p4k5nza3nwg","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnv00h30p4k7mqldhga"},{"post_id":"clzy4tsnt00gm0p4k5nza3nwg","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnv00h50p4k91ck1jnv"},{"post_id":"clzy4tsnt00gm0p4k5nza3nwg","tag_id":"clzy4tsnl00d90p4keyqsdupb","_id":"clzy4tsnv00h70p4k93q15rwd"},{"post_id":"clzy4tsnt00gm0p4k5nza3nwg","tag_id":"clzy4tsnp00fl0p4khix47c99","_id":"clzy4tsnv00h90p4khx3d75sw"},{"post_id":"clzy4tsns00gh0p4k0itoh1bc","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnv00hb0p4khgqnam65"},{"post_id":"clzy4tsns00gh0p4k0itoh1bc","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnv00hd0p4kdpoffvfw"},{"post_id":"clzy4tsns00gh0p4k0itoh1bc","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnv00hf0p4k5gbc85uu"},{"post_id":"clzy4tsns00gh0p4k0itoh1bc","tag_id":"clzy4tsns00gj0p4kekmi6hjk","_id":"clzy4tsnv00hh0p4k1cy9fk62"},{"post_id":"clzy4tsns00gh0p4k0itoh1bc","tag_id":"clzy4tsmz00240p4k7a2w22no","_id":"clzy4tsnv00hj0p4k51zqdc84"},{"post_id":"clzy4tsnu00gt0p4k1o8y5ky2","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnw00hl0p4kaso19fck"},{"post_id":"clzy4tsnu00gt0p4k1o8y5ky2","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnw00hn0p4kgiy2g6te"},{"post_id":"clzy4tsnu00gt0p4k1o8y5ky2","tag_id":"clzy4tsn8005w0p4kgkil00ow","_id":"clzy4tsnw00hp0p4kfh0fcd65"},{"post_id":"clzy4tsnt00gk0p4kbe1j6qt9","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnw00hr0p4k0g8ico3h"},{"post_id":"clzy4tsnt00gk0p4kbe1j6qt9","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnw00hs0p4k4augh4yi"},{"post_id":"clzy4tsnt00gk0p4kbe1j6qt9","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnw00ht0p4khghm2uba"},{"post_id":"clzy4tsnt00gk0p4kbe1j6qt9","tag_id":"clzy4tsnu00gq0p4k9mby8pml","_id":"clzy4tsnw00hu0p4k88sk7rns"},{"post_id":"clzy4tsnt00gk0p4kbe1j6qt9","tag_id":"clzy4tsnv00gw0p4kepq6h6nc","_id":"clzy4tsnw00hv0p4k3cxjbetp"},{"post_id":"clzy4tsnt00gp0p4k1f5c31jk","tag_id":"clzy4tsml00050p4k2y6gd39e","_id":"clzy4tsnw00hw0p4khy75h329"},{"post_id":"clzy4tsnt00gp0p4k1f5c31jk","tag_id":"clzy4tsmn000b0p4k3kn02xm9","_id":"clzy4tsnw00hx0p4k0hntben0"},{"post_id":"clzy4tsnt00gp0p4k1f5c31jk","tag_id":"clzy4tsmq000f0p4k3o4g6ax0","_id":"clzy4tsnw00hy0p4kh3u1c7vu"},{"post_id":"clzy4tsnt00gp0p4k1f5c31jk","tag_id":"clzy4tsnv00h20p4kcyyk1jlg","_id":"clzy4tsnw00hz0p4kf3fedmq4"},{"post_id":"clzy4tsnt00gp0p4k1f5c31jk","tag_id":"clzy4tsnp00fl0p4khix47c99","_id":"clzy4tsnw00i00p4k694h24ws"}],"Tag":[{"name":"NLP","_id":"clzy4tsml00050p4k2y6gd39e"},{"name":"LLM","_id":"clzy4tsmn000b0p4k3kn02xm9"},{"name":"transformer","_id":"clzy4tsmq000f0p4k3o4g6ax0"},{"name":"positional encoding","_id":"clzy4tsmr000j0p4kc56d3t8g"},{"name":"RoPE","_id":"clzy4tsms000n0p4k8q8rauyp"},{"name":"","_id":"clzy4tsmv00190p4k06w695nk"},{"name":"","_id":"clzy4tsmw001d0p4k0xfg962g"},{"name":"","_id":"clzy4tsmy001y0p4kehblfgwa"},{"name":"","_id":"clzy4tsmz00240p4k7a2w22no"},{"name":"","_id":"clzy4tsn0002b0p4k81ka8sny"},{"name":"","_id":"clzy4tsn0002h0p4kgaw2b8aj"},{"name":"agent","_id":"clzy4tsn1002p0p4k95il2t3w"},{"name":"","_id":"clzy4tsn4003j0p4k0w04dy1y"},{"name":"","_id":"clzy4tsn6004n0p4kalg4cfov"},{"name":"","_id":"clzy4tsn6004z0p4k4lfq3b8a"},{"name":"","_id":"clzy4tsn600560p4k3vuw8tdx"},{"name":"SFT","_id":"clzy4tsn7005n0p4k143sf6k4"},{"name":"","_id":"clzy4tsn8005s0p4k20na25iu"},{"name":"","_id":"clzy4tsn8005w0p4kgkil00ow"},{"name":"","_id":"clzy4tsn800630p4kf2gmcx95"},{"name":"MoE","_id":"clzy4tsn800680p4k6es3enfp"},{"name":"attention","_id":"clzy4tsnb006i0p4kay1h7uoq"},{"name":"sliding window attention","_id":"clzy4tsnc006o0p4kcv6f5ohq"},{"name":"sparse attention","_id":"clzy4tsnc006u0p4kakph8ut2"},{"name":"layernorm","_id":"clzy4tsnc006z0p4k92a58cu0"},{"name":"post-norm","_id":"clzy4tsnc00740p4kbcxo36mz"},{"name":"pre-norm","_id":"clzy4tsnc007a0p4k6wy44kwo"},{"name":"normalization","_id":"clzy4tsnd007g0p4kfef1fc9x"},{"name":"batchnorm","_id":"clzy4tsnd007l0p4kgb945tnw"},{"name":"","_id":"clzy4tsnd00800p4kdxmmgspw"},{"name":"ChatGPT","_id":"clzy4tsne008g0p4kcgyjdv7z"},{"name":"Sparrow","_id":"clzy4tsne008m0p4k8k9s2nxx"},{"name":"LaMDA","_id":"clzy4tsne008r0p4k16otcni1"},{"name":"GopherCite","_id":"clzy4tsne008w0p4k51oo4kqk"},{"name":"WebGPT","_id":"clzy4tsne00920p4kdoateh4i"},{"name":"InstructGPT","_id":"clzy4tsnf00980p4k7h4w9l9q"},{"name":"KV Cache","_id":"clzy4tsnf009l0p4k8j0v7jdq"},{"name":"","_id":"clzy4tsnh00b40p4k6eny10c6"},{"name":"DeepSeek","_id":"clzy4tsnk00ct0p4kaxt9hff3"},{"name":"MLA","_id":"clzy4tsnk00cw0p4k3i9j30us"},{"name":"GQA","_id":"clzy4tsnk00d00p4k2o0t8fxy"},{"name":"Gemma2","_id":"clzy4tsnk00d60p4kbmdb75fy"},{"name":"Meta","_id":"clzy4tsnl00d90p4keyqsdupb"},{"name":"Llama","_id":"clzy4tsnl00dc0p4kakdihwe9"},{"name":"post-training","_id":"clzy4tsnl00dm0p4k4ohf6e9a"},{"name":"DPO","_id":"clzy4tsnm00ds0p4khteuhe5l"},{"name":"RM","_id":"clzy4tsnm00dv0p4kfmdd7hhs"},{"name":"RS","_id":"clzy4tsnm00dy0p4k84oaftq5"},{"name":"routing","_id":"clzy4tsnm00e50p4kasifdvgo"},{"name":"Qwen","_id":"clzy4tsnm00e80p4k3qf7c98n"},{"name":"","_id":"clzy4tsnn00ee0p4kdm969vf3"},{"name":"","_id":"clzy4tsno00eq0p4kcn2ahy6s"},{"name":"","_id":"clzy4tsno00f30p4k8gvg5qrv"},{"name":"","_id":"clzy4tsnp00fl0p4khix47c99"},{"name":"","_id":"clzy4tsnp00fo0p4k6xl0e9am"},{"name":"","_id":"clzy4tsns00gj0p4kekmi6hjk"},{"name":"Bert","_id":"clzy4tsnu00gq0p4k9mby8pml"},{"name":"","_id":"clzy4tsnv00gw0p4kepq6h6nc"},{"name":"","_id":"clzy4tsnv00h20p4kcyyk1jlg"}]}}