<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>理解LLM位置编码:RoPE</title>
    <url>/a051710f.html</url>
    <content><![CDATA[<p>最近在做LLM窗口外推的相关工作，因此刚好也回顾一下目前最流行的位置编码RoPE。</p>
<h1 id="关于rope">关于RoPE</h1>
<p>RoPE（Rotary Position
Embedding），是苏剑林大神在2021年就提出的一种Transformer模型的位置编码。RoPE是一种可以<big><u><strong>以绝对位置编码形式实现的相对位置编码</strong></u></big>，兼顾了模型性能和效率。</p>
<p>2023年上半年的时候，大模型位置编码尚有Alibi和RoPE在相互比拼，而到了2023年下半年，及今2024年，新开源出来的模型，大部分都是使用RoPE了。当然Alibi也有其优势，这个在讲Alibi的时候来说。</p>
<p>苏神在他的个人网站科学空间中对RoPE有相关文章进行了介绍，本篇是在这个基础上，对RoPE进行理解（公式和符号上也会沿用苏神的写法）。</p>
<h1 id="以绝对位置编码的方式实现相对位置编码">以绝对位置编码的方式实现相对位置编码</h1>
<p>前面提到，RoPE是一种一绝对位置编码的方式实现的相对位置编码，那么这么做能带来什么收益？</p>
<p>先说原因：</p>
<p>在文本长度不长的情况下（比如Bert时代基本都是256/512token的长度），相对位置编码和绝对位置编码在使用效果上可以说没有显著差别。<br>
如果要处理更大长度的输入输出，使用绝对位置编码就需要把训练数据也加长到推理所需长度，否则对于没训练过的长度（训练时没见过的位置编码），效果多少会打些折扣。<br>
而使用相对位置编码则<u><strong>更容易外推</strong></u>，毕竟token-2和token-1的距离，与token-10002和token-10001的距离是一样的，也因此可以缓解对巨量长文本数据的需求。<br>
但是传统相对位置编码的实现相对<u><strong>复杂</strong></u>，有些也会有<u><strong>计算效率低</strong></u>的问题。由于修改了self-attention的计算方式，也比较难推广到<u><strong>线性注意力</strong></u>计算法模型中。<br>
总结来说，就是绝对位置编码<u><strong>好实现</strong></u>，<u><strong>效率高</strong></u>，<u><strong>适用线性注意力</strong></u>，而相对位置编码<u><strong>易外推</strong></u>，因此就有了对“绝对位置编码的方式实现相对位置编码”的追求，去把二者的优点结合起来。</p>
<p>下面简单回顾一下绝对位置编码和相对位置编码。</p>
<p>（对位置编码比较熟悉的朋友可以直接跳到第3节。）</p>
<h2 id="绝对位置编码">绝对位置编码</h2>
<p>先回顾一下带绝对位置编码的self-attention。</p>
<p><span class="math display">\[\left.\left\{\begin{array}{l}q_\mathrm{i}=(x_\mathrm{i}+p_\mathrm{i})W_\mathrm{Q}\\k_\mathrm{j}=(x_\mathrm{j}+p_\mathrm{j})W_\mathrm{K}\\\nu_\mathrm{j}=(x_\mathrm{j}+p_\mathrm{j})W_\mathrm{V}\\\mathrm{a_\mathrm{i,j}}=\mathrm{softmax}\left(q_\mathrm{i}k_\mathrm{j}^\top\right)\\o_\mathrm{i}=\sum_\mathrm{j}a_\mathrm{i,j}\nu_\mathrm{j}\end{array}\right.\right.\tag{1}\]</span></p>
<p><span class="math inline">\(x_i\)</span> 和 <span class="math inline">\(x_j\)</span> 分别是位置 <span class="math inline">\(i\)</span> 和 <span class="math inline">\(j\)</span> 的输入，<span class="math inline">\(p\)</span> 是对应位置的位置编码向量。</p>
<p>这里的位置编码<span class="math inline">\(p\)</span>可以是三角函数式，或者直接训练式。但是无论是哪种，其实现方式都很简单，就是在输入端把词向量
<span class="math inline">\(x\)</span> 和位置向量 <span class="math inline">\(p\)</span>
相加即可，相比attention中的softmax计算，element-wise
addition操作的计算量非常小，是可以忽略不计的。</p>
<p>大部分绝对位置编码使用的是这样向量相加的形式，即加性编码，也有一些用乘性编码的工作，把
<span class="math inline">\(x + p\)</span> 变成 <span class="math inline">\(x * p\)</span> 这样，效果上也是大差不差。</p>
<h2 id="相对位置编码">相对位置编码</h2>
<p>在绝对位置编码中，可以在输入阶段就把 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(p\)</span>
直接相加，是因为这里把位置信息当做是这个位置的词的固有特征。</p>
<p>比如“我”这个词放在位置1时，形成一个 <span class="math inline">\(e_1 =
x_我 + p_1\)</span>
这么一个向量来代表【“我”在位置1】这一个情况；而当同样的词“我”放在位置8时，形成了另一个向量
<span class="math inline">\(e_8 = x_我 + p_8\)</span> 。两个向量 <span class="math inline">\(e_1\)</span> 和 <span class="math inline">\(e_8\)</span>
虽然包含同一个词，但是对于模型来说，这两个输入是不同的（因为每个数值都包含了位置向量），词向量和位置向量<u><strong>耦合</strong></u>在一起共同构成了一个完整的输入。</p>
<p>直观来说，比如词表大小是1万，模型训练窗口最大长度是512，那么对于模型来说，实际上要区分的输入是1万×512=512万个。看起来虽然不少，但是在海量的数据和训练量下，这也不算什么事儿，模型确实能handle。</p>
<p>扯远了，现在回来看一下相对位置编码。把公式（1）中的 <span class="math inline">\(q_{i}k_{j}^{T}\)</span>展开来</p>
<p><span class="math display">\[\begin{align*}q_ik_j^\top&amp;=\left(x_i+p_i\right)W_\mathbb{Q}W_K^\top\left(x_j+p_j\right)^\top\\&amp;=\left(x_iW_\mathbb{Q}+{\color{red}p_iW_\mathbb{Q}}\right)\left(W_K^\top
x_j^\top+{\color{red}W_K^\top
p_j^\top}\right)\end{align*}\tag{2}\]</span></p>
<p>和位置相关的有 <span class="math inline">\(p_iW_\mathbb{Q}\)</span>
和 <span class="math inline">\(W_K^\top p_j^\top\)</span> 两项。</p>
<h3 id="google式">Google式</h3>
<p>在最早引入相对位置编码的Google的论文《Self-Attention with Relative
Position Representations》中，把第一项 <span class="math inline">\(p_iW_\mathbb{Q}\)</span>
去掉了（因为要搞相对位置编码，只要能把相对位置信息加到其中一项输入就可以了，这里加在了位置
<span class="math inline">\(j\)</span>），把第二项 <span class="math inline">\(W_K^\top p_j^\top\)</span> 改成和位置 <span class="math inline">\(i\)</span>、<span class="math inline">\(j\)</span>
都相关的位置向量 <span class="math inline">\(R_{ij}^K\)</span>，于是在这个使用相对位置编码的attention计算中，<u><strong>不再是直接计算input
projection的内积来获取权重</strong></u>，而变成</p>
<p><span class="math display">\[
\mathrm{a_{ij}=softmax}\left(x_{i}W_{\mathbb{Q}}\left(x_{j}W_{\mathbb{K}}+R_{\mathbf{i,j}}^{\mathbf{K}}\right)^{\top}\right)\tag{3}
\]</span></p>
<p><span class="math inline">\(R_{ij}^K\)</span>
是什么呢？可以是可训练式的向量，也可以是类似三角函数式的，在这个基础上增加了一个clip操作。</p>
<p><span class="math display">\[
R_{\mathrm{i,j}}^\mathrm{K}=p_\mathrm{K}\left[\mathrm{clip(i-j,p_{min},p_{max})}\right]
\]</span></p>
<p>其中 <span class="math inline">\(p_\mathrm{K}\)</span>
就是可训练的向量或者三角函数向量。</p>
<p>为什么要增加一个clip操作？因为直观上，一个词对其左右附近的其他词的位置关系<strong>理应</strong>更加敏感，比如“我请你吃饭”中，“吃饭”这个词需要以高分辨率明确区分出前面三个词“我”、“请”、“你”的位置，以免理解成了“你请我吃饭”；而随着距离越来越远，这种高分辨率的需求也就越来越低，十万个token之前的内容顺序对于当前token来说，影响比较小了，在位置向量上可以一视同仁。另外这也是方便了位置信息的外推，比如我们可以只训练256个相对位置编码信息，而在应用是可以外推到&gt;256的长度。</p>
<p>本来到这里就可以了，相对位置信息已经加入了，但是Google除了在input端增加了相对位置信息，在输出端也增加了相对位置信息。本来输出端的计算是</p>
<p><span class="math display">\[\begin{align*}
o_\mathrm{i}&amp;=\sum_\mathrm{j}a_\mathrm{i,j}\nu_\mathrm{j}\\
&amp;=\sum_{\mathrm{j}}\mathrm{a_{i,j}}(x_{j} + p_{j})W_{\mathrm{V}}\\
&amp;=\sum_{\mathrm{j}}\mathrm{a_{i,j}}(x_{j}W_{\mathrm{V}} +
{\color{red}p_{j}W_{\mathrm{V}}})\\
\end{align*}\tag{4}\]</span></p>
<p>Google的方法把 <span class="math inline">\(p_{j}W_{\mathrm{V}}\)</span>
也改成了包含相对位置信息的向量</p>
<p><span class="math display">\[\begin{align*}
o_{\mathrm{i}}=\sum_{\mathrm{j}}\mathrm{a_{i,j}}\left(x_{j}W_{\mathrm{V}}+R_{\mathrm{i,j}}^{\mathrm{V}}\right)\tag{5}
\end{align*}\]</span></p>
<p><span class="math inline">\(R_{\mathrm{i,j}}^{\mathrm{V}}\)</span> 和
<span class="math inline">\(R_{ij}^K\)</span> 相似，都是一个相对位置向量
+ clip操作。</p>
<h3 id="xlnet式">XLNET式</h3>
<p>XLNET也使用了相对位置编码，思路类似Google，只是具体的操作不同。</p>
<p>在公式（2）的基础上继续展开</p>
<p><span class="math display">\[\begin{align*}
q_ik_j^T
&amp;= \left(x_iW_\mathbb{Q}+{p_iW_\mathbb{Q}}\right)\left(W_K^\top
x_j^\top+{W_K^\top p_j^\top}\right)\\
&amp;=
x_iW_\mathbb{Q}W_\mathbb{K}^Tx_j^T
+x_iW_\mathbb{Q}W_\mathbb{K}^T{\color{red}p_j^T}
+{\color{red}p_i}W_\mathbb{Q}W_\mathbb{K}^T{x_j^T}
+{\color{red}p_i}W_\mathbb{Q}W_\mathbb{K}^T{\color{red}p_j^T}\\
\end{align*}\tag{6}
\]</span></p>
<p>把绝对位置相关的几个参数改成相对位置相关的参数，变成：</p>
<p><span class="math display">\[
\mathrm{a_{ij}=softmax}\left
(x_iW_\mathrm{Q}W_\mathrm{K}^\top x_\mathrm{j}^\top
+x_iW_\mathrm{Q}W_\mathrm{K}^\top {\color{red}R_\mathrm{i-j}^\top}
+{\color{red}u}W_\mathrm{Q}W_\mathrm{K}^\top x_\mathrm{j}^\top
+{\color{red}\nu}
W_\mathrm{Q}W_\mathrm{K}^\top{\color{red}R_\mathrm{i-j}^\top}\right)
\tag{7}
\]</span></p>
<p>把 <span class="math inline">\(p_i\)</span> 变成了两个可训练的向量
<span class="math inline">\(u\)</span> 和 <span class="math inline">\(\nu\)</span> ，把 <span class="math inline">\(p_j\)</span> 变成相对位置向量 <span class="math inline">\(R_{i-j}^\top\)</span> 。</p>
<p>实际实现上可以把 <span class="math inline">\(u\)</span> 和 <span class="math inline">\(\nu\)</span>
后面跟着的矩阵省掉了，去掉这个线性变化不影响 <span class="math inline">\(u\)</span> 和 <span class="math inline">\(\nu\)</span> 的训练，变成</p>
<p><span class="math display">\[
x_iW_\mathrm{Q}W_\mathrm{K}^\top x_\mathrm{j}^\top
+x_iW_\mathrm{Q}W_\mathrm{K}^\top {\color{red}R_\mathrm{i-j}^\top}
+{\color{red}u}W_\mathrm{K}^\top x_\mathrm{j}^\top
+{\color{red}\nu} W_\mathrm{K}^\top{\color{red}R_\mathrm{i-j}^\top}
\tag{8}
\]</span></p>
<p>此外，XLNET只对输入端做了处理，输出端则直接把位置相关的计算去掉了，即</p>
<p><span class="math display">\[\begin{align*}
o_\mathrm{i}
&amp;=\sum_{\mathrm{j}}\mathrm{a_{i,j}}x_{j}W_{\mathrm{V}}\\
\end{align*}\tag{9}\]</span></p>
<p>可以看到，Google式和XLNET式的相对位置编码在权重 <span class="math inline">\(\mathrm{a_{i,j}}\)</span>
的计算上都变得比较复杂了（相对绝对位置编码而言），并且到这里可以看到，获取相对位置信息的思路其实就是想办法把原来公式（2）中的绝对位置向量替换成和位置
<span class="math inline">\(i\)</span> 、 <span class="math inline">\(j\)</span>
都相关的向量。很多其他变体其实都大差不差，基本就是在怎么加入相对位置向量、怎么clip上下功夫。</p>
<p>当然，也有简单一点的实现，比如T5的方法。</p>
<h3 id="t5式">T5式</h3>
<p>公式（6）中展开了内积计算，一共有四项，第一项完全没有位置信息，只和词向量本身有关，第二三项分别包含了位置
<span class="math inline">\(i\)</span> 和位置 <span class="math inline">\(j\)</span>
的信息，而第四项只和位置相关，和词向量本身是什么内容无关。也就是说，位置相关的信息都是在后面三项引入的，那简单点，直接把后面三项替换成一个位置向量：</p>
<p><span class="math display">\[
\mathrm{a_{ij}=softmax}\left
(x_iW_\mathrm{Q}W_\mathrm{K}^\top x_\mathrm{j}^\top
+ \beta_{i,j}\right)
\tag{10}
\]</span></p>
<p>（从最早提出，到XLNET，以及DeBerta，T5等，可以看到相对位置编码的实现有一个简化的趋势，而效果也越来越好，正所谓大道至简，有时候有用的东西未必需要很复杂）</p>
<h2 id="对比">对比</h2>
<p>看来相对位置编码确实比较复杂，说个大概需要这么多篇幅；并且相对绝对位置编码，也没有那么直接明了，需要对attention计算做一些改造。</p>
<p>公式（1）的绝对位置编码中，可以看到在进softmax操作前需要做3次矩阵加法，3次矩阵乘法</p>
<p>从公式（8）可以看到，共有4组矩阵计算要做，每组要做3次矩阵乘法，相对会比较复杂。公式（3）也有类似的情况。当然同时也有一些针对相对位置编码的高效计算被提出，这些就需要针对不同的计算方案来优化了。</p>
<p>总之在实现方式上和计算效率上，绝对位置编码具有一些优势。</p>
<p>而在输入输出窗口外推方面，相对位置编码有着天然的优势。</p>
<p>另外，绝对位置编码保持self-attention的经典形式，使得应用面更广，如可以使用到linear
attention方案中去，这个以后再展开讲（又挖了个坑）。</p>
<h1 id="rope的设计思路">RoPE的设计思路</h1>
<h2 id="保持attention计算形式">保持attention计算形式</h2>
<p>回顾完经典的绝对位置编码和相对位置编码，回到RoPE上来。</p>
<p>先说设计思路：</p>
<p>首先我们想要保持经典self-attention的计算方式，即公式（1）中的形式，输入端
= 内积 +
softmax，至于输出端则保持完全不变。softmax我们不去动，那这里留给我们操作的就是内积。</p>
<p>也就说，现在问题是，我们怎么在只做内积的情况下，把内积结果变成只和相对位置有关，而和绝对位置无关的结果。写成公式就是</p>
<p><span class="math display">\[
\langle
f_q(\boldsymbol{q}_m,m),f_k(\boldsymbol{k}_n,n)\rangle=g(\boldsymbol{q}_m,\boldsymbol{k}_n,m-n)
\tag{11}
\]</span></p>
<p>其中 <span class="math inline">\(q_m\)</span> 是在位置 <span class="math inline">\(m\)</span> 的query向量，<span class="math inline">\(k_n\)</span> 是在位置 <span class="math inline">\(n\)</span> 的key向量，<span class="math inline">\(f_q\)</span> 和 <span class="math inline">\(f_k\)</span>
是分别针对这query和key向量的操作函数。</p>
<p>我们的任务就是要找到一组 <span class="math inline">\(f_q\)</span> 、
<span class="math inline">\(f_k\)</span> 和 <span class="math inline">\(g\)</span> ，使得公式（11）恒成立。</p>
<p>当然理论上这里是存在无数多组答案的，那么RoPE怎么找到一组好实现的组合呢？</p>
<h2 id="借用复数寻找组合">借用复数寻找组合</h2>
<p>式（11）中， <span class="math inline">\(g\)</span>
的结果是一个标量，我们需要一个能连接向量内积和标量的桥梁，这个桥梁就是复数。</p>
<p>这里先回顾一下复数的知识。任意复数都可以表示成复平面的一个2维向量</p>
<img src="/a051710f/complex_number.png" class width="282" height="401" title="复数平面">
<p>现在考虑query和key向量都是2维的情况，那么可以代入复数的操作<br>
（先把 hidden size = 2 的情况推理清楚，后续再推广到更高维的情况）</p>
<p>那么在2维复数平面上有什么操作可以满足公式（11）的要求呢？Roformer论文中提出的是这组：</p>
<p><span class="math display">\[
\begin{aligned}
f_q(\boldsymbol{q}_m,m)&amp;=\boldsymbol{q}_me^{im\theta}=\left(\boldsymbol{W}_q\boldsymbol{x}_m\right)e^{im\theta}
\\
f_k(\boldsymbol{k}_n,n)&amp;=\boldsymbol{k}_ne^{in\theta}=(\boldsymbol{W}_k\boldsymbol{x}_n)e^{in\theta}
\\
g(\boldsymbol{q}_m,\boldsymbol{k}_n,m-n)&amp;=\mathrm{Re}\left[\boldsymbol{q}_m\boldsymbol{k}_n^*e^{i(m-n)\theta}\right]
=\mathrm{Re}\left[(\boldsymbol{W}_q\boldsymbol{x}_m)(\boldsymbol{W}_k\boldsymbol{x}_n)^*e^{i(m-n)\theta}\right]\\
\end{aligned} \\
\tag{12}
\]</span></p>
<p>其中 <span class="math inline">\(\boldsymbol{k}_n^*\)</span> 是 <span class="math inline">\(\boldsymbol{k}_n\)</span> 的共轭复数。</p>
<p>（如果暂时理解不了是怎么想出这个组合来满足要求的的，先把它放一边，毕竟数学就是这么神奇）</p>
<p>共轭复数是这样的关系</p>
<p><span class="math display">\[
\begin{gathered}
z=a+ib \\
z^*=a-ib
\end{gathered}
\tag{13}
\]</span></p>
<p>先证明一下这个组合的正确性，是不是真的满足公式（11）。</p>
<p>（也可以先跳过证明，选择先相信这个组合）</p>
<p>回顾一下欧拉公式</p>
<p><span class="math display">\[
e^{ix}=\cos x+i\sin x
\tag{14}
\]</span></p>
<p>因为现在我们讨论的是2维的情况，那2维向量 <span class="math inline">\(q_m\)</span> 可以用一个复数来表示</p>
<p><span class="math display">\[
q_m = q_m^{(1)} + iq_m^{(2)}
\tag{15}
\]</span></p>
<p>那从复数角度来看，就有</p>
<p><span class="math display">\[
\begin{aligned}
f_q(\boldsymbol{q}_m,m)
&amp;= \boldsymbol{q}_me^{im\theta} \\
&amp;= (q_m^{(1)} + iq_m^{(2)})(\cos (m\theta)+i\sin (m\theta)) \\
&amp;=
(q_m^{(1)}cos(m\theta)-q_m^{(2)}\sin(m\theta))+i(q_m^{(1)}\sin(m\theta)
+ q_m^{(2)}\cos(m\theta))
\end{aligned}
\tag{16}
\]</span></p>
<p>式（16）的结果也是一个复数，那也可以用复平面上的一个向量来表示：</p>
<p><span class="math display">\[
f_q(\boldsymbol{q}_m,m) =
\left.\left[\begin{matrix}{q_m^{(1)}cos(m\theta)-q_m^{(2)}\sin(m\theta)}\\{q_m^{(1)}\sin(m\theta)
+ q_m^{(2)}\cos(m\theta)}\end{matrix}\right.\right]^\top
\tag{17}
\]</span></p>
<p>（这里沿用式（1）中，默认向量为行向量的设定，所有有个transpose，实际上是行向量还是列向量都没关系，只是推算的时候写法问题）</p>
<p>类似地，有</p>
<p><span class="math display">\[
\begin{aligned}
f_k(\boldsymbol{k}_n,n)
&amp;=
(k_n^{(1)}cos(n\theta)-k_n^{(2)}\sin(n\theta))+i(k_n^{(1)}\sin(n\theta)
+ k_n^{(2)}\cos(n\theta))
\end{aligned}
\tag{18}
\]</span></p>
<p>和</p>
<p><span class="math display">\[
f_k(\boldsymbol{k}_n,n) =
\left.\left[\begin{matrix}{k_n^{(1)}cos(n\theta)-k_n^{(2)}\sin(n\theta)}\\{k_n^{(1)}\sin(n\theta)
+ k_n^{(2)}\cos(n\theta)}\end{matrix}\right.\right]^\top
\tag{19}
\]</span></p>
<p>则有<br>
<span class="math display">\[
\begin{aligned}
&amp;\langle
f_q(\boldsymbol{q}_m,m),f_k(\boldsymbol{k}_n,n)\rangle\\=&amp;(q_m^{(1)}cos(m\theta)-q_m^{(2)}\sin(m\theta))(k_n^{(1)}cos(n\theta)-k_n^{(2)}\sin(n\theta))
\\&amp;+ (q_m^{(1)}\sin(m\theta) +
q_m^{(2)}\cos(m\theta))(k_n^{(1)}\sin(n\theta) +
k_n^{(2)}\cos(n\theta))\\
=&amp;q_m^{(1)}k_n^{(1)}\left(\cos(m\theta)\cos(n\theta)+\sin(m\theta)\sin(n\theta)\right)
\\
&amp;+q_m^{(1)}k_n^{(2)}\left(-\cos(m\theta)\sin(n\theta)+\sin(m\theta)\cos(n\theta)\right)
\\
&amp;+q_m^{(2)}k_n^{(1)}(-\sin(m\theta)\cos(n\theta)+\cos(m\theta)\sin(n\theta))
\\
&amp;+q_m^{(2)}k_n^{(2)}(\sin(m\theta)\sin(n\theta)+\cos(m\theta)\cos(n\theta))
\\
=&amp;q_m^{(1)}k_n^{(1)}\cos((m-n)\theta)+q_m^{(1)}k_n^{(2)}\sin((m-n)\theta)
\\
&amp;-\left.q_m^{(2)}k_n^{(1)}\right.\sin((m-n)\theta)
+q_m^{(2)}k_n^{(2)}\cos((m-n)\theta)\\
= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\cos((m - n)\theta) +
(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})\sin((m-n)\theta)
\end{aligned}
\tag{20}
\]</span></p>
<p>用了三角函数和差公式 <span class="math display">\[
\sin(\alpha\pm\beta)=\sin\alpha\cos\beta\pm\cos\alpha\sin\beta\\
{\cos(\alpha\pm\beta)=\cos\alpha\cos\beta\mp\sin\alpha\sin\beta}
\]</span></p>
<p>再看 <span class="math inline">\(g\)</span></p>
<p><span class="math display">\[
\begin{aligned}
&amp;g(\boldsymbol{q}_m,\boldsymbol{k}_n,m-n)\\
=
&amp;\mathrm{Re}\left[\boldsymbol{q}_m\boldsymbol{k}_n^*e^{i(m-n)\theta}\right]
\\
= &amp;\mathrm{Re}\left[[(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) -
i(q_m^{(1)}k_n^2 - q_m^{(2)}k_n^{(1)})](\cos((m -
n)\theta) + i\sin((m-n)\theta))\right] \\
= &amp;(q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\cos((m - n)\theta) +
(q_m^{(1)}k_n^2 -
q_m^{(2)}k_n^{(1)})\sin((m-n)\theta)\\
= &amp;\langle f_q(\boldsymbol{q}_m,m),f_k(\boldsymbol{k}_n,n)\rangle
\end{aligned}
\tag{21}
\]</span></p>
<p>证毕。</p>
<h2 id="旋转位置编码">“旋转”位置编码</h2>
<p>发现式（17）可以写成这样</p>
<p><span class="math display">\[
f_q(\boldsymbol{q}_m,m)^\top =
\left.\left[\begin{matrix}{\cos(m\theta)}&amp;{-\sin(m\theta)}\\{\sin(m\theta)}&amp;{\cos(m\theta)}\end{matrix}\right.\right]
{\left.\left[\begin{matrix}{q_m^{(1)}}\\{q_m^{(2)}}\end{matrix}\right.\right]}
\tag{22}
\]</span></p>
<p>同样地</p>
<p><span class="math display">\[
f_k(\boldsymbol{k}_n,n)^\top =
\left.\left[\begin{matrix}{\cos(n\theta)}&amp;{-\sin(n\theta)}\\{\sin(n\theta)}&amp;{\cos(n\theta)}\end{matrix}\right.\right]
{\left.\left[\begin{matrix}{k_n^{(1)}}\\{k_n^{(2)}}\end{matrix}\right.\right]}
\tag{23}
\]</span></p>
<p>如果从向量视角来看，则有</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\langle f_q(\boldsymbol{q}_m,m),f_k(\boldsymbol{k}_n,n)\rangle\\
=&amp;{\left.\left[\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\end{matrix}\right.\right]}
\left.\left[\begin{matrix}{\cos(m\theta)}&amp;{\sin(m\theta)}\\{-\sin(m\theta)}&amp;{\cos(m\theta)}\end{matrix}\right.\right]
\left.\left[\begin{matrix}{\cos(n\theta)}&amp;{-\sin(n\theta)}\\{\sin(n\theta)}&amp;{\cos(n\theta)}\end{matrix}\right.\right]
{\left.\left[\begin{matrix}{k_n^{(1)}}\\{k_n^{(2)}}\end{matrix}\right.\right]}\\
=&amp;{\left.\left[\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\end{matrix}\right.\right]}\left.\left[\begin{matrix}{\cos(m\theta)\cos(n\theta)
+ \sin(m\theta)\sin(n\theta)}&amp;
{-\cos(m\theta)\sin(n\theta) + \sin(m\theta)\cos(n\theta)}\\
{-\cos(n\theta)\sin(m\theta) + \cos(m\theta)\sin(n\theta)}&amp;
{\sin(m\theta)\sin(n\theta) + \cos(m\theta)\cos(n\theta)}
\end{matrix}\right.\right]
{\left.\left[\begin{matrix}{k_n^{(1)}}\\{k_n^{(2)}}\end{matrix}\right.\right]}\\
=&amp;{\left.\left[\begin{matrix}{q_m^{(1)}}&amp;{q_m^{(2)}}\end{matrix}\right.\right]}
\left.\left[\begin{matrix}{\cos((m-n)\theta)}&amp;{\sin((m-n)\theta)}\\{-\sin((m-n)\theta)}&amp;{\cos((m-n)\theta)}\end{matrix}\right.\right]
{\left.\left[\begin{matrix}{k_n^{(1)}}\\{k_n^{(2)}}\end{matrix}\right.\right]}
\end{aligned}
\tag{24}
\]</span></p>
<p>看式（22）和（23），可以看到等号右边都有</p>
<p><span class="math display">\[
\left.\left[\begin{matrix}{\cos(n\theta)}&amp;{-\sin(n\theta)}\\{\sin(n\theta)}&amp;{\cos(n\theta)}\end{matrix}\right.\right]
\]</span></p>
<p>这正是一个二维平面的旋转矩阵。 <span class="math inline">\(f_q\)</span> 、 <span class="math inline">\(f_k\)</span>
的操作相当于对输入向量进行了一次不改变大小，只改变方向的旋转。</p>
<p>这也是为什么叫做“旋转”位置编码。</p>
<h2 id="从2维推广到高维">从2维推广到高维</h2>
<p>我们现在已经确认，对于2维的情况，经过 <span class="math inline">\(f_q\)</span> 、 <span class="math inline">\(f_k\)</span> 和 <span class="math inline">\(g\)</span>
这么一波操作，能够满足式（11）的要求，但是实际上怎么在高维模型里实现呢？</p>
<p>答案是把高维输入拆分成两个两个一组（这要求输入是偶数维，目前的模型也都是偶数维，所以没问题），则高维的“旋转”矩阵有多个小旋转矩阵组成</p>
<p><span class="math display">\[
\boldsymbol{R}_{\Theta,m}^d=\begin{pmatrix}\cos m\theta_0&amp;-\sin
m\theta_0&amp;0&amp;0&amp;\cdots&amp;0&amp;0\\\sin m\theta_0&amp;\cos
m\theta_0&amp;0&amp;0&amp;\cdots&amp;0&amp;0\\0&amp;0&amp;\cos
m\theta_1&amp;-\sin m\theta_1&amp;\cdots&amp;0&amp;0\\0&amp;0&amp;\sin
m\theta_1&amp;\cos
m\theta_1&amp;\cdots&amp;0&amp;0\\\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots&amp;\vdots\\0&amp;0&amp;0&amp;0&amp;\cdots&amp;\cos
m\theta_{d/2-1}&amp;-\sin
m\theta_{d/2-1}\\0&amp;0&amp;0&amp;0&amp;\cdots&amp;\sin
m\theta_{d/2-1}&amp;\cos n\theta_{d/2-1}\end{pmatrix}
\tag{25}
\]</span></p>
<p><span class="math inline">\(d\)</span>
是的输入向量的维度，由于是两个两个一组，所以一共有 <span class="math inline">\(d/2\)</span> 组小旋转矩阵，这 <span class="math inline">\(d/2\)</span> 组矩阵为了区分，设计使用了不同的
<span class="math inline">\(\theta\)</span></p>
<p><span class="math display">\[
\Theta=\left\{\theta_i=10000^{-2(i-1)/d},i\in[1,2,\ldots,d/2]\right\}
\tag{26}
\]</span></p>
<p>那么在实际操作的时候，给位置 <span class="math inline">\(m\)</span>
和位置 <span class="math inline">\(n\)</span> 的输入向量分别乘以 <span class="math inline">\(R_m\)</span> 和 <span class="math inline">\(R_n\)</span>，再进行self-attention，就能获得仅使用相对位置信息编码的效果。</p>
<p>另外 <span class="math inline">\(\theta\)</span>
是怎么来的呢？这里是参考了Google最初在《Attention is All You
Need》中提出的，这里就先不展开了，可以看看论文原文。</p>
<h2 id="高效率实现">高效率实现</h2>
<p>式（25）中的矩阵在高维的情况下很稀疏，直接使用这么个矩阵来计算效率并不高，可以使用一个这样的高效率实现方式</p>
<p><span class="math display">\[
\boldsymbol{R}_{
m}\boldsymbol{q}=\begin{pmatrix}q_0\\q_1\\q_2\\q_3\\q_4\\\vdots\\q_{d-2}\\q_{d-1}\end{pmatrix}\otimes\begin{pmatrix}\cos
m\theta_0\\\cos m\theta_0\\\cos m\theta_1\\\cos m\theta_1\\\cos
m\theta_1\\\vdots\\\cos m\theta_{d/2-1}\\\cos
m\theta_{d/2-1}\end{pmatrix}
+\begin{pmatrix}-q_1\\q_0\\-q_3\\\vdots\\-q_{d-1}\\q_{d-2}\end{pmatrix}\otimes\begin{pmatrix}\sin
m\theta_0\\\sin m\theta_0\\\sin m\theta_1\\\sin m\theta_1\\\sin
m\theta_1\\\vdots\\\sin m\theta_{d/2-1}\\\sin
m\theta_{d/2-1}\end{pmatrix}
\tag{27}
\]</span></p>
<p>只需进行两组element-wise乘法即可。形式上看起来是类似乘性绝对位置编码的做法。</p>
<p>另外，看LLAMA中的实现，可以看到旋转位置编码是在每一个decoder层的输入都加了的。每次都强化一次位置信息，也有助于模型更好识别不同距离的内容。</p>
<h2 id="远程衰减的特性">远程衰减的特性</h2>
<p>至此，旋转位置编码已经完备，具备了计算高效，实现容易，便于外推，适用于线性注意力的特性。实际上它还具备另一项优点：有远程衰减的特性。</p>
<p>直观看起来远程衰减很符合直觉，毕竟注意力机制随着距离的衰减而降低，这个机制和人类也很像。</p>
<p>回顾训练式的绝对位置编码，由于每个位置的位置向量是模型在训练中自我学习的，所以并不保证能具备这样的特性。而这个
<span class="math inline">\(\theta\)</span>
的选择沿用了三角函数式编码的做法，就使得整体具有远程衰减的特性。</p>
<p>证明过程这里就偷偷懒略过了，具体可以看<a href="https://arxiv.org/abs/2104.09864">Roformer的论文</a>或者<a href="https://spaces.ac.cn/archives/8265">苏神的博客</a>。</p>
<p>当 <span class="math inline">\(d = 128\)</span>
时，画出来的图像如下</p>
<img src="/a051710f/remote_attenuation.png" class width="775" height="457" title="远程衰减">
<h1 id="小结">小结</h1>
<p>总之，RoPE在设计和实现上还是挺巧妙的，性质上也很有很多优势，所以被广泛应用到transformer模型中去了。</p>
<h1 id="reference">Reference</h1>
<p>【1】让研究人员绞尽脑汁的Transformer位置编码，https://spaces.ac.cn/archives/8130<br>
【2】Transformer升级之路：2、博采众长的旋转式位置编码，https://spaces.ac.cn/archives/8265<br>
【3】RoFormer: Enhanced Transformer with Rotary Position Embedding
https://arxiv.org/abs/2104.09864<br>
【4】十分钟读懂旋转编码（RoPE）
https://zhuanlan.zhihu.com/p/647109286</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>positional encoding</tag>
        <tag>RoPE</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM长上下文的问题</title>
    <url>/c4da56c0.html</url>
    <content><![CDATA[<p>最近长上下文的业务需求越来越多，刚好把这个能力现状和主流方案的基础内容简单梳理一下。</p>
<p>跟长文本最相关的自然就是位置编码，现在很多模型都使用了RoPE这种位置编码，之前已经把RoPE的基础内容梳理了一遍：<a href="http://www.linsight.cn/a051710f.html">博客</a> <a href="https://zhuanlan.zhihu.com/p/684072868">知乎</a> <a href="https://mp.weixin.qq.com/s?__biz=MzkyODY1MTA3Ng==&amp;mid=2247483759&amp;idx=1&amp;sn=f7b59b879476b8687a340606b5568eae&amp;chksm=c214c344f5634a52e299108c3deddfd2a0eccbf14d9392c205410723956c477925e89e791b9b&amp;token=88551061&amp;lang=zh_CN#rd">微信公众号</a></p>
<h1 id="关于长上下文">关于长上下文</h1>
<p>2023年中开始，各大LLM厂商开始关注到长上下文的问题。2023年5月，Claude把长度支持到100k
tokens；6、7月的时候，ChatGPT3.5也已经支持16k，而ChatGLM2-B最大长度已经可以到32k。</p>
<p>（插一句，ChatGLM系列做得一直很不错，从基础模型、长窗口、工具调用、Agent都一直保持在比较前沿的水平，个人最近用ChatGLM3、ChatGLM4体验还是很不错的）</p>
<p>差不多同时间还有LM-SYS的LongChat，MosaicLM的MPT也支持16k以及更长的上下文。</p>
<p>今年过年前刚出来的Qwen-1.5系列全家桶也都是32k起步了。还有一些支持超长窗口的模型</p>
<center>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">模型</th>
<th style="text-align: center;">支持长度</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Baichuan2</td>
<td style="text-align: center;">192k</td>
</tr>
<tr class="even">
<td style="text-align: center;">GPT4-turbo</td>
<td style="text-align: center;">128k</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Yi</td>
<td style="text-align: center;">200k</td>
</tr>
<tr class="even">
<td style="text-align: center;">Kimi Chat</td>
<td style="text-align: center;">128k(20万汉字)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Claude2</td>
<td style="text-align: center;">200k</td>
</tr>
</tbody>
</table>
</center>
<p>大厂商们卷完基础模型效果，把能刷的榜刷完，又盯上了长上下文能力（当然现在长上下文也有榜了）。</p>
<p>为什么要那么长？</p>
<h1 id="长上下文的需求">长上下文的需求</h1>
<p>取决于语言和所使用的tokenizer，每个token对应编码的文本有所不同。以中文为例，大部分模型每个token对应的中文字数都&gt;1.5个字（部分高效的tokenizer可以做到2个字以上）。那么200k的token就能对应处理30w字的上下文了。</p>
<p>最近刚看了刘震云的长篇小说《一句顶一万句》，全书差不多27万字，也就是说现在这些长上下文大模型可以秒读完一部长篇小说，然后和我交流心得，或者告诉我全书的概要，又或者帮我找到一些文中的细节描写。</p>
<p>上面这个场景对应的是大模型的<big><u><strong>工具化</strong></u></big>场景。我们可以借助大模型的能力，来阅读论文，总结研报或者阅读代码，这些场景都需要比较长的上下文输入。</p>
<p>另外还有一个也比较火的大模型应用场景，RAG（Retrieval-augmented
generation），也对长上下文输入有要求，只是在RAG中，大部分输入文本并不是直接来自于用户输入，而是通过检索得来的。</p>
<p>除了工具化的应用场景，还有一些<big><u><strong>个性化</strong></u></big>的场景也会对长上下文有需求。举例来说，就是一些智能助手需要对用户的偏好和设置做长期记忆，这些偏好和设置可以以prompt或者对话的形式持久化存储下来，在进行新的对话的时候就把这些内容连同用户新的输入一起给到模型进行处理。</p>
<p>实际上，哪怕是单次的聊天，也很有可能需要模型处理比较长的上下文。比如我们可能会让模型扮演一个特定的影视角色或者游戏角色和我们进行对话。这时通常会给模型一些设定，比如这是一个什么样的任务，故事背景世界观都是什么样的，以及现在要进行哪些方面的交流等。这些设定都会以prompt的形式在最开始输入给模型。而随着对话的进行，模型如果长文本能力比较差，就有可能忘记了我们之前给的设定，这样体验上就有问题了。</p>
<p>上面这个例子实际引出了对长文本需求更具体的内容：（1）在文本比较长的时候，还能说人话，ppl要低（2）说人话之余，还要能attention到前面提过的细节，不能出现自我矛盾。</p>
<h1 id="模型怎么支持长上下文">模型怎么支持长上下文</h1>
<p>看来目前的很多应用场景确实对长上下文有需求，那怎么实现呢？</p>
<p>如果我们直接训练2k/4k长度的模型，然后在推理的时候设定8k或者16k窗口，那么PPL会急剧上升，导致模型直接讲不了人话，原因之一在之前讲RoPE的时候也有提到，对于没有训练过的<u><strong>位置编码</strong></u>，模型不能很好地处理。</p>
<h2 id="直接训练">直接训练</h2>
<p>既然训练的时候用2k/4k不能很好地在8k/16k/32k+的上下文长度下推理，那直接在训练的时候用更长的数据进行训练不就可以了？</p>
<p>这个思路理论上可行，只是实操的时候会遇到一些问题（壕可能觉得不是问题）。</p>
<p>1.训练数据</p>
<p>直观上来说，要训练长上下文的模型，就需要长文本。要达到32k或者更大的长度，基本都只能是书籍。</p>
<p>当然，我们也可以通过把多个中等长度的文本进行拼接，再用来训练。比如筛选4k长度数据，那8条拼在一起也够长了。然后通过attention
mask来限制各段文本之间注意力，让它们可以在各自的位置上各训各的，互不干扰。甚至实际上即使不做attention
mask，效果也挺好。</p>
<p>总的来说，就是【连续长文本】&gt;【多个中等文本拼接】（也可用）</p>
<p>2.资源消耗</p>
<p>来简单看一下transformer在训练中所消耗的资源。</p>
<p>假设模型有 <span class="math inline">\(l\)</span> 层，词表大小为
<span class="math inline">\(V\)</span> ，hidden size为 <span class="math inline">\(h\)</span> ，batch size为 <span class="math inline">\(b\)</span> ，训练窗口长度为 <span class="math inline">\(s\)</span>
，使用Adam优化器训练（需要存一阶和二阶动量），为简化估算，可以假设注意力头数为1。</p>
<ol type="1">
<li>参数量</li>
</ol>
<p>模型总参数量 <span class="math inline">\(\Phi\)</span> = 词向量参数量
+ <span class="math inline">\(l\)</span> * decoder层参数量 = <span class="math inline">\(Vh + l(12h^2 + 13h)\)</span></p>
<p>可以看到参数量和窗口长度 <span class="math inline">\(s\)</span>
无关，模型确定了就是一个固定值。</p>
<ol start="2" type="1">
<li>计算量</li>
</ol>
<p>一次前向计算量 = 输出分类头logits计算 + <span class="math inline">\(l\)</span> * 每层计算量 <span class="math inline">\(\approx2bshV + l*(24bsh^2+4bs^2h)\)</span></p>
<p>（这里的计算忽略了softmax，实际上softmax计算量也是和长度 <span class="math inline">\(s\)</span> 成平方关系）</p>
<p>看一下计算量和参数量的关系。忽略参数量和计算量中的低次项，则有</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
\frac{计算量}{参数量}
&amp;=\frac{2bshV + l*(24bsh^2+4bs^2h)}{Vh + l(12h^2 + 13h)}\\
&amp;\rightarrow bs\frac{6h+s}{3h}
\end{aligned}
\end{equation}
\]</span></p>
<p>可以看到，总计算量随着输入长度的增长是平方的。在 <span class="math inline">\(s &lt;&lt; h\)</span>
的时候，基本还可以认为是线性的。目前大部分模型的 <span class="math inline">\(h\)</span> 是在1k到1w这个范围，基本上可以认为
<span class="math inline">\(s\)</span> 和 <span class="math inline">\(sh\)</span>
在不是超级长的情况下，还是可比较的。计算量算是长度的“弱”二次方关系</p>
<ol start="3" type="1">
<li>显存</li>
</ol>
<p>训练过程中，显存主要有模型参数、梯度、optimizer状态值和中间激活值。</p>
<p>训练中，每个参数（<span class="math inline">\(\Phi\)</span>）有一个对应梯度（<span class="math inline">\(\Phi\)</span>），每个参数又对应优化器一个一阶动量和二阶动量（<span class="math inline">\(2\Phi\)</span>）。在混合精度训练中，使用半精度进行前向计算和梯度计算，同时优化器备份一份单精度的优化器状态、梯度和参数用于更新参数，因此共有
<span class="math inline">\((\Phi + \Phi) \times 2 + (\Phi + \Phi +
2\Phi) \times 4 = 20\Phi = 20[Vh + l(12h^2 + 13h)]\)</span>
的参数占用。</p>
<img src="/c4da56c0/mix_precision_fp16.png" class title="混合精度训练">
<p>这部分跟输入长度没有直接关系。</p>
<p>另外一个需要占用显存的部分是中间激活值。</p>
<p>保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。</p>
<p>对于attention层，输入时先要对 <span class="math inline">\(x\)</span>
做 <span class="math inline">\(Q、K、V\)</span> 投影，需要保存 <span class="math inline">\(x\)</span> 的中间值；计算权重的时候有 <span class="math inline">\(Q、K\)</span> 矩阵的相乘，需要保存 <span class="math inline">\(Q、K\)</span> 矩阵的值；做softmax的时候输入有
<span class="math inline">\(QK^T\)</span>
要保存；以此类推，则需要保存的所有中间激活值为 <span class="math inline">\(11bsh+5bs^2+19sh+4bsh=34bsh+5bs^2\)</span> 。对于
<span class="math inline">\(l\)</span> 层的模型，就再乘以 <span class="math inline">\(l\)</span> 。</p>
<p>可以看到中间激活值随着 <span class="math inline">\(s\)</span>
增大，是以平方关系在增长。训练4k长度的模型和32k长度的模型，激活值所需的显存增长到了64倍。这种情况下，要么扩大集群，加入更多的GPU，要么减小batch
size，或者提升gradient
accumulation的值，无论如何，都会增加<big><u><strong>训练成本</strong></u></big>。</p>
<p>小模型（比如2B、7B）可以硬刚，支持到16k或者32k长度，但是对于更大的长度（200k），或者更大的模型（34B、70B+），这么做就性价比就比较低了。</p>
<p>现在一般的做法是分两阶段，第一阶段用2k或者4k训练一个基础模型，等到模型把文本内容和短位置关系都学好之后，再来用相比第一阶段小的数据量优化在长上下文情况下的效果。</p>
<p>而第二阶段在如何用更少的训练量达到更好的效果这件事上，又有很多工作。</p>
<h2 id="线性插值-position-interpolation">线性插值 Position
Interpolation</h2>
<p>23年6月，Meta在<a href="https://arxiv.org/pdf/2306.15595.pdf">《EXTENDING CONTEXT WINDOW
OF LARGE LANGUAGE MODELS VIA POSITION
INTERPOLATION》</a>中就提出了针对RoPE的线性插值方法PI（Position
Interpolation），可以把2k的基础模型扩展到32k，并在1k个step的训练下就达到很好的效果。</p>
<img src="/c4da56c0/meta_pi.png" class title="PI效果">

<blockquote>
<p>In contrast, LLaMA models that are extended via direct fine-tuning
only saw a minimal increase of the effective context window size kmax
from 2048 to 2560, even after fine-tuning for more than 10000 steps,
with no clear indication of an acceleration in the increase of window
size.</p>
</blockquote>
<p>相比之下，直接基于基础模型进行长文本微调的效率就比较低，训练1w步后，有效长度只是从2048提升到2560。</p>
<p>看来RoPE虽然拥有诸多优点，长上下文外推这个事情却不在其中。</p>
<p>论文中对RoPE外推性能也进行了一些分析。本来RoPE是相对位置编码，而且具有远程衰减的特性，理论上应该具备一定的外推能力，但实际上却不是这样。简单地说，论文发现，在相对位置差
<span class="math inline">\(\left|m-n \right|\)</span>
不太大的时候（&lt;2048），确实能保持远程衰减且attention值保持在较小的区间，但是一旦
<span class="math inline">\(\left|m-n \right|\)</span>
超过这个区间，还是有可能出现很大的值。</p>
<img src="/c4da56c0/meta_rope_ext.png" class title="RoPE外推">
<p>看上图中间的这个图，在位置超过3000的时候，突然出现很大的attention
score。而右边的图使用了插值的方式，就相对稳定。</p>
<p>（远程衰减上界问题具体推导的过程就不展开了，感兴趣的朋友可以看下论文原文）</p>
<p>而另一方面，PI甚至可以在只使用插值，而没有训练的情况下，就拥有一定的长窗口能力。</p>
<img src="/c4da56c0/meta_pi_nosft.png" class title="PI效果">
<p>插值的思路是这样的：如下图所示，左上部分表示预训练过的2k长度的位置编码，右上部分表示在这个基础上直接外推，这样就会出现很多之前没有训练过的值，模型的学习成本会比较高；下半部分表示在已经训练好的2k模型基础上进行插值，类似于在每两个位置编码之间，插入一个位置点，这样总的位置表示就从2k增加到4k。在这个基础上再进行少量的微调，模型就可以很快学到新的位置表示。</p>
<img src="/c4da56c0/meta_pi_explanation.png" class title="PI效果">
<p>这个思路也很符合直觉，比如原来模型针对位置1，位置2，位置3...学到了一定的规律，现在告诉模型，位置不一定是整数，变成位置1，位置1.5，位置2，位置2.5...。虽然值变了，但是相对关系还在，因此模型也能借助原来学到的关系，快速推广到“0.5”的位置中。</p>
<p>由于三角函数光滑的特性，我们可以重新定义attention
score的计算，使得结果不要出现异常大的值，也就是 <span class="math inline">\(\tilde{a}(s)=a(Ls/L^{\prime})\)</span> ，<span class="math inline">\(L\)</span> 是原长度（也就是2048），<span class="math inline">\(L^{\prime}\)</span>
是我们想要增大的长度（8k/16k/32k等）。</p>
<p>更具体来说，就是对RoPE做一点修改</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
\mathbf{f}&#39;(\mathbf{x},m)=\mathbf{f}\left(\mathbf{x},\frac{mL}{L&#39;}\right)
\end{aligned}
\end{equation}
\]</span></p>
<p>相当于位置 <span class="math inline">\(m\)</span> 的分辨率从1下降成了
<span class="math inline">\({L}/{L&#39;}\)</span>。</p>
<p>（一通分析，实现却很简单，工程师的福音，真希望以后论文都能给出这样的结果）</p>
<p>然后使用几万到几十万条样本进行预训练，就可以了。</p>
<p>（文章里还埋有一个后续的关注点，在对长上下文进行训练后，在短上下文的场景下，效果略略有所下降）</p>
<h2 id="ntk-aware-interpolation">NTK-Aware Interpolation</h2>
<p>线性插值相当于直接线性拉伸了cos函数，减小了不同位置之间的差异，使得模型对于相近位置的区分度有所下降，多少有点简单粗暴，而NTK-Aware
Interpolation就提出了理论上看起来更为“精致”方法，一种在RoPE上做<u><strong>非线性插值</strong></u>的方法，NTK-Aware
Scaled RoPE。CodeLlama就是用这种方法把长度推广到1M。</p>
<p>NTK，就是Neural Tangent
Kernel，神经正切核。具体是什么，让GLM4帮忙解答一下</p>
<blockquote>
<p>Neural Tangent Kernel (NTK)
是深度学习理论中的一个概念，它是在训练无限宽的神经网络时出现的一个理论工具。在深度学习领域，宽度通常指的是网络中每一层的神经元数量。NTK
的提出，为理解深度学习提供了一个新的视角，特别是在网络宽度趋于无穷大时，网络的行为可以用线性模型来近似。<br>
在理想情况下，当网络宽度趋于无穷大时，网络的训练过程可以用一个固定的核函数来描述，这个核函数就是
Neural Tangent
Kernel。这个核函数描述了网络输出与输入之间关系的性质，使得我们可以用核方法来分析这类网络的训练和泛化能力。<br>
具体来说，NTK
使得我们能够预测在无限宽的网络中，随着训练的进行，网络权重的演化情况，以及网络的损失函数如何随时间变化。这些预测在数学上更加清晰和可分析，有助于我们深入理解深度学习的工作原理，特别是在泛化和优化方面。<br>
NTK
的研究对设计新的网络架构、训练算法和理论分析提供了重要的理论基础。它也表明，在一定的条件下，深度学习可以简化为传统的统计学习理论，如核方法，这为将深度学习与经典机器学习理论联系起来提供了桥梁。</p>
</blockquote>
<p>这个了解各大概就行。那么具体在长上下文扩展这个事上，NTK有什么说法呢？</p>
<p>它认为，线性插值把所有方向上的分量一视同仁，把旋转速度统一变慢，这里有问题。</p>
<p>回顾一下在RoPE中，对位置 <span class="math inline">\(m\)</span>
的输入向量进行“旋转”的矩阵长这样</p>
<img src="/c4da56c0/rope_matrix.png" class title="RoPE旋转矩阵">
<p>它把输入向量的元素划分成2个2个一组，共有 <span class="math inline">\(d/2\)</span>
组，每组有两个元素，不同组分别旋转。这里可以发现每组的旋转速度并不相同，由于
<span class="math inline">\(\theta_j=10000^{-2j/d}\)</span> ，可以看到，
<span class="math inline">\(j\)</span> 越小越靠前的组旋转越快，<span class="math inline">\(j\)</span> 越大的旋转越慢。这里 <span class="math inline">\(base=10000\)</span> ， <span class="math inline">\(base\)</span>
越大，整体的旋转速度越慢，反之越快。同一个位置下，由于旋转速度不同，位置向量的信号频率有高低，前面的部分是高频，越往后越低频。</p>
<p>不加区分地对高低频信息进行拉伸，会丢失很多重要的高频信息，这样不是很好。高频信号应该外推，以防止分辨率太低，都挤在一起；而低频信号就适合插值。</p>
<p>怎么实现“高频外推，低频内插”？</p>
<p>先看回讲<a href="https://www.zhihu.com/people/us4ever">RoPE</a>的时候，对于2维情况，有</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
&amp;\langle f_q(\boldsymbol{q}_m,m),f_k(\boldsymbol{k}_n,n)\rangle=
\mathrm{Re}\left[\boldsymbol{q}_m\boldsymbol{k}_n^*e^{i(m-n)\theta}\right]
\end{aligned}
\end{equation}
\]</span></p>
<p>推广到高维的情况，则有</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
\langle
f_q(\boldsymbol{q}_m,m),f_k(\boldsymbol{k}_n,n)\rangle=&amp;\mathrm{Re}[\sum_j^{d/2}h_je^{is\theta_j}]\\
\end{aligned}
\end{equation}
\]</span></p>
<p>其中 <span class="math inline">\(h_j=\boldsymbol{q}_m\boldsymbol{k}_n^*\)</span>
，<span class="math inline">\(s=m-n\)</span> 。</p>
<p>在这个公式下，线性插值相当于把</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
\mathrm{Re}[\sum_j^{d/2}h_je^{is\theta_j}]\\
\end{aligned}
\end{equation}
\]</span></p>
<p>变成了</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
\mathrm{Re}[\sum_j^{d/2}h_je^{i\frac{s}{\alpha}\theta_j}]\\
\end{aligned}
\end{equation}
\]</span></p>
<p>其中 <span class="math inline">\(\alpha=L&#39;/L&gt;1\)</span>
，相当于把 <span class="math inline">\(s\)</span> 压缩了。</p>
<p>而NTK-Aware Scaled RoPE则是对 <span class="math inline">\(\theta_j\)</span>
进行了改动，具体来说，是修改了其中的base值（RoPE中原来是10000）</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
\hat{base}=base\times\alpha^{\frac{d}{d-2}}
\end{aligned}
\end{equation}
\]</span></p>
<p>则有</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
\hat{\theta_j}=\hat{base}^{-2j/d}=base^{-2j/d}\times\alpha^{\frac{-2j}{d-2}}
\end{aligned}
\end{equation}
\]</span></p>
<p>相当于 <span class="math inline">\(\theta\)</span> 乘了一个系数 <span class="math inline">\(\alpha^{\frac{-2j}{d-2}}\)</span> ，当 <span class="math inline">\(j\)</span> 比较小的时候， <span class="math inline">\(\alpha^{\frac{-2j}{d-2}}\)</span>
接近1，相当于直接进行了外推，而当 <span class="math inline">\(j\)</span>
比较大的时候（注意 <span class="math inline">\(j\)</span> 的取值是从0到
<span class="math inline">\(d/2 - 1\)</span>），<span class="math inline">\(\alpha^{\frac{-2j}{d-2}}\)</span> 就接近 <span class="math inline">\(\alpha^{-1}\)</span> ，这就和线性插值趋近了。</p>
<p>引用来自<a href="https://zhuanlan.zhihu.com/p/645770522">知乎一篇文章</a>的一个视角来理解NTK-Aware
Interpolation</p>
<blockquote>
<p>有意思的解释一下，RoPE
的行为就像一个时钟。12小时时钟基本上是一个维度为 3、底数为 60 的
RoPE。因此，每秒钟，分针转动 1/60 分钟，每分钟，时针转动
1/60。现在，如果将时间减慢 4 倍，那就是二使用的线性RoPE
缩放。不幸的是，现在区分每一秒，因为现在秒针几乎每秒都不会移动。因此，如果有人给你两个不同的时间，仅相差一秒，你将无法从远处区分它们。NTK-Aware
RoPE 扩展不会减慢时间。一秒仍然是一秒，但它会使分钟减慢 1.5
倍，将小时减慢 2 倍。这样，您可以将 90 分钟容纳在一个小时中，将 24
小时容纳在半天中。所以现在你基本上有了一个可以测量 129.6k 秒而不是 43.2k
秒的时钟。由于在查看时间时不需要精确测量时针，因此与秒相比，更大程度地缩放小时至关重要。不想失去秒针的精度，但可以承受分针甚至时针的精度损失。</p>
</blockquote>
<p>另外苏剑林从“进制”角度对RoPE作了分析，感兴趣的朋友可以看下<a href="https://kexue.fm/archives/9675">原文</a>，也很巧妙。</p>
<p>在YaRN的<a href="https://arxiv.org/pdf/2309.00071.pdf">论文</a>中，对NTK的优缺点作了点评</p>
<blockquote>
<p>Given the results from [6], this method performs much better at
extending the context size of non-finetuned models compared to PI [9].
However, one major disadvantage of this method is that given it is not
just an interpolation scheme, some dimensions are slightly extrapolated
to "out-of-bound" values, thus fine-tuning with "NTK-aware"
interpolation [6] yields inferior results to PI [9]. Furthermore, due to
the "out-of-bound" values, the theoretical scale factor s does not
accurately describe the true context extension scale. In practice, the
scale value s has to be set higher than the expected scale for a given
context length extension.</p>
</blockquote>
<p>NTK的优点是不用微调的情况下，能比线性插值做得好。但是由于低频部分还是会有部分被外推到超出范围的值，因此在设定系数的时候，要比需要的设得更大才行。比如想4k模型要在32k的时候取得比较好的效果，那
<span class="math inline">\(\alpha=L&#39;/L\)</span>
就要选得比8更大一些，比如16。</p>
<h2 id="ntk-by-parts">NTK-by-parts</h2>
<p>NTK-by-parts的方法在NTK插值的基础上又多想了一层。它认为无论是线性插值还是NTK-aware插值，认为RoPE的所有分量都对网络有同样的重要性。而NTK-by-parts的思路认为，应该区别对待不同分量，他们对网络的影响有所不同。</p>
<p>对于分量 <span class="math inline">\(j\)</span> ，RoPE嵌入的波长</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
\lambda_j=\frac{2\pi}{\theta_j}=2\pi\cdot base^{\frac{2j}{d}}
\end{aligned}
\end{equation}
\]</span></p>
<p><span class="math inline">\(\lambda_j\)</span>
代表旋转一周所需的长度。当 <span class="math inline">\(j\)</span>
比较小时，波长短，反之波长长，这也对应我们前面说的，前面的分量高频，后面的分量低频。</p>
<p>这里观察到，当 <span class="math inline">\(j\)</span>
比较大时，波长就可能比 <span class="math inline">\(L\)</span>
要大，这种情况下RoPE一圈都没有转完，会导致这个分量的分布不均匀（比如
<span class="math inline">\(sin\)</span>
只转了1/4圈，那值全都集中在0<sub>1之间，-1</sub>0的就没有值）。这种情况下，这个维度的编码相当于是绝对位置编码了，因为几乎每个位置都有自己独特的一个值。反之当
<span class="math inline">\(j\)</span>
比较小时，模型只能访问到相对位置信息。</p>
<p>此外，插值会导致相邻或相近位置的关系更近（因为旋转量小，点积更大），文章认为这样会损害模型理解局部关系的能力，因此选择不对高频部分进行插值。NTK-by-parts的思路是</p>
<ul>
<li>如果维度 <span class="math inline">\(j\)</span> 的波长 <span class="math inline">\(\lambda_j\)</span> 远小于上下文长度
，就不插值只外推<br>
</li>
<li>如果波长 <span class="math inline">\(\lambda_j\geq\)</span>
上下文长度，就只插值不外推<br>
</li>
<li>中间的部分就同时存在两种，类似NTK-aware interpolation</li>
</ul>
<p>引入一个比例 <span class="math inline">\(r(j)=\frac{L}{\lambda_j}\)</span>
来表示波长和上下文长度的关系。另外还需要两个阈值 <span class="math inline">\(\beta_1、\beta_2\)</span> 来区分以上三种情况。如果
<span class="math inline">\(r(j)&lt;\beta_1\)</span>
，就认为波长大，如果 <span class="math inline">\(r(j)\geq
\beta_2\)</span> ，就认为波长小。方便起见，定义一个斜坡函数</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
\left.\gamma(r)=\left\{\begin{matrix}0&amp;if&amp;r(j)&lt;\beta_1\\1&amp;if&amp;r(j)\geq\beta_2\\\frac{r-\beta_1}{\beta_2-\beta_1}&amp;otherwise\end{matrix}\right.\right.
\end{aligned}
\end{equation}
\]</span></p>
<p>NTK-by-parts插值可以定义为对 <span class="math inline">\(\theta_j\)</span> 的一个操作</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
\hat{\theta_j}=\left(1-\gamma(r(j))\right)\frac{\theta_j}s+\gamma(r(j))\theta_j
\end{aligned}
\end{equation}
\]</span></p>
<p>这里有两个超参 <span class="math inline">\(\beta_1、\beta_2\)</span>
要定，文中根据实验给出的推荐值是 <span class="math inline">\(\beta_1=1，\beta_2=32\)</span>
，也就是当波长和上下文长度一样长的时候，认为波长大，就只插值，当波长小于上下文长度1/32时，认为波长远小于上下文，就只外推。</p>
<h2 id="dynamically-ntk-scaled-rope">Dynamically NTK Scaled RoPE</h2>
<p>无论是线性插值还是NTK-Aware
Interpolation，都是通过使用一个固定的系数，对原RoPE做了一个缩放，这样就会有一些局限。一方面，这种情况下，模型能支持的最大上下文就由使用的这个缩放系数来决定了，超出这个范围的，依然会出现attention
score暴增的风险。另一方面，在解码过程中，当已解码的长度 <span class="math inline">\(l\)</span> 还没有达到训练长度 <span class="math inline">\(L\)</span> 时，就使用 <span class="math inline">\(\alpha\)</span>
来修改base，也可能带来一些损失。Dynamically NTK Scaled
RoPE是在NTK插值的基础上，把固定的系数改成动态的系数。</p>
<p>具体来说，就是</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
\hat{\alpha}=max(1,\frac{l}{L})
\end{aligned}
\end{equation}
\]</span></p>
<p>这样随着解码长度 <span class="math inline">\(l\)</span> 的增长，当
<span class="math inline">\(l&gt;L\)</span> 之后 <span class="math inline">\(\alpha\)</span> 从1逐渐增大， <span class="math inline">\(l\leq L\)</span> 时则不需要改动。</p>
<p>有一点要注意的是，使用动态的系数时要注意kv-cache的缓存机制是否正确，记得要缓存使用应用RoPE之前的值。</p>
<h2 id="yarn">YaRN</h2>
<p>上面的方法都是使用插值，研究者发现，随着插值，token之间的距离变得更近（因为现在旋转角度变小了），平均最小距离在减小，这样注意力softmax的分布会变得更尖（也就是都集中在某个区间）。换句话说，就是RoPE原本远距离衰减的特性变弱了，衰减得更不明显，就会导致模型更平均地关注到更多的token，这样就削弱了注意力机制，导致输出质量下降。</p>
<p>当将RoPE插值到更长的上下文时，注意力softmax分布中的熵会减少，因此研究者的目标是逆转这种熵减（即增加注意力logit的“温度”）。这可以通过在softmax之前，将中间注意力矩阵乘以温度
<span class="math inline">\(t&gt;1\)</span>
来完成，但由于RoPE被编码为一个旋转矩阵，就可以简单地按常数因子 <span class="math inline">\(\sqrt{t}\)</span>
来扩展RoPE的长度。这样可以不必修改注意力的代码。</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
\text{softmax}\left(\frac{\mathbf{q}_m^T\mathbf{k}_n}{t\sqrt{d}}\right)
\end{aligned}
\end{equation}
\]</span></p>
<p>通过对Llama 1和Llama 2的实验，文章提出了建议值<span class="math inline">\(\begin{aligned}\sqrt{\frac1t}&amp;=0.1\ln(\alpha)+1.\end{aligned}\)</span>。这个值的效果再Llama各个版本和规模的模型都能有比较好的效果，这样说明这样的熵变在长文本中是常见的。</p>
<p>YaRN最终的方法就是结合NTK-by-parts，以及使用这个温度值对attention
score进行调整。</p>
<p>YaRN在微调以及无微调的情况下，效果都比上面的几种都要好。</p>
<h2 id="logn">logn</h2>
<p>logn指的是对attention计算中的缩放因子 <span class="math inline">\(\sqrt{d}\)</span>
进行通过logn进行改进的一个方法，苏剑林在<a href="https://zhuanlan.zhihu.com/p/678755776">博客</a>中进行了分析。大致的思路和YaRN中的缩放颇有些相似。</p>
<p>简单来说，依然是希望在长上下文的时候，引入了更多token的情况下，已有的token还能保持聚焦在原来哪些token上，而不要被过分分散了注意力。因此提出了一个新的attention
score公式</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
\text{Attention}_E(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})=\text{softmax}\left(\frac{\log_{L}{L&#39;}}{\sqrt{d}}\boldsymbol{Q}\boldsymbol{K}^\mathrm{T}\right)\boldsymbol{V}
\end{aligned}
\end{equation}
\]</span></p>
<p>可以看到，当 <span class="math inline">\(L&#39;&gt;L\)</span>
时，其效果和YaRN中的放缩是类似的。</p>
<h2 id="其他">其他</h2>
<p>在扩展推理长度上，还有很多其他有效的工作，比如各种window
attention，streaming LLM，LongLoRA，Focus
Transformer等，还有数据、评测等更方面的分析，待逐个梳理。</p>
<h1 id="小结">小结</h1>
<p>较短的预训练模型（2k、4k）应用在长上下文会因为训练和推理的两个不一致导致效果下降</p>
<ul>
<li>推理时用到了没训练过的位置编码<br>
</li>
<li>推理时注意力机制所处理的token数量远超训练时的数量，导致注意力机制的崩坏</li>
</ul>
<p>这两个问题分别可以从位置编码和attention score的放缩来缓解。</p>
<p>线性插值PI、NTK插值、分部NTK插值都可以缓解第一个问题，logn和YaRN则把第二个问题纳入的考虑。目前这些方法在实际应用中也有很多变体，包括超参的修改，函数的重定义等。</p>
<h1 id="reference">Reference</h1>
<p>【1】分析transformer模型的参数量、计算量、中间激活、KV cache
https://zhuanlan.zhihu.com/p/624740065<br>
【2】EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION
INTERPOLATION https://arxiv.org/pdf/2306.15595.pdf<br>
【3】Transformer升级之路：10、RoPE是一种β进制编码
https://kexue.fm/archives/9675<br>
【4】YaRN: Efficient Context Window Extension of Large Language Models
https://arxiv.org/pdf/2309.00071.pdf<br>
【5】详解基于调整RoPE旋转角度的大模型长度外推方法
https://mp.weixin.qq.com/s/RtI95hu-ZLxGkdGuNIkERQ<br>
【6】浅谈LLM的长度外推 https://zhuanlan.zhihu.com/p/645770522
【7】想让大模型在prompt中学习更多示例，这种方法能让你输入更多字符
https://cloud.tencent.com/developer/article/2330611<br>
【8】Transformer升级之路：8、长度外推性与位置鲁棒性
https://spaces.ac.cn/archives/9444<br>
【9】RoPE外推优化——支持192K上下文长度
https://zhuanlan.zhihu.com/p/678755776</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>长上下文</tag>
        <tag>窗口外推</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型偏好对齐-simPO</title>
    <url>/280fa97a.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>前面我们对DPO和ODPO的思路做了整理：<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a>，<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a>。</p>
<p>最近新出的simPO受到了很多关注。相比DPO，simPO不需要reference
model，并且有更好的效果。simPO的另一个好处是，能够保持生成结果在较短长度下的质量。</p>
<img src="/280fa97a/intro.png" class title="simPO">
<h1 id="dpo的局限">DPO的局限</h1>
<p>回顾一下DPO。DPO的reward function有一个closed-form expression</p>
<p><span class="math display">\[\begin{aligned}r(x,y)=\beta\log\frac{\pi_\theta(y\mid
x)}{\pi_\text{ref}(y\mid x)}+\beta\log Z(x)\end{aligned}\]</span></p>
<p>基于此，通过Bradley-Terry model进行建模，得到损失函数</p>
<p><span class="math display">\[\mathcal{L}_{\text{DPO}}(\pi_\theta;\pi_{\text{ref}})=-\mathbb{E}_{(x,y_w,y_l)\thicksim\mathcal{D}}\left[\log\sigma\left(\beta\log\frac{\pi_\theta(y_w\mid
x)}{\pi_{\text{ref}}(y_w\mid x)}-\beta\log\frac{\pi_\theta(y_l\mid
x)}{\pi_{\text{ref}}(y_l\mid x)}\right)\right]\]</span></p>
<p>理论上，DPO的优化目标和RLHF是一致的，但是DPO有两个缺陷：<br>
- 仍然需要一个reference model，这样依然有比较大的内存和计算开销<br>
-
训练过程中优化的reward和推理时的生成指标存在差异，也就是训练和推理的目标不完全对齐</p>
<p>第二点怎么理解呢？模型在自回归生成response时，理论上是寻找最大化所有token平均log
likelihood的组合，即</p>
<p><span class="math display">\[\begin{aligned}p_\theta(y\mid
x)=\frac{1}{|y|}\log\pi_\theta(y\mid
x)=\frac{1}{|y|}\sum_{i=1}^{|y|}\log\pi_\theta(y_i\mid
x,y_{&lt;i})\end{aligned}\]</span></p>
<p>当然实际上这个组合空间太大了，没法直接遍历寻找，因此会使用一些解码策略来寻找局部最优解，比如greedy
decoding、beam search或者top-k
sampling等，不过我们还是可以按这个公式近似计算。另外这个公式还是可用在多个response/多选题的排序上的。</p>
<p>可以看到推理时的这个目标和DPO的reward差了个referenc
model。那么在DPO里，满足 <span class="math inline">\(r(x,y_w)&gt;r(x,y_l)\)</span>
的偏好数据并不一定意味着 <span class="math inline">\(p_\theta(y_w\mid
x)&gt;p_\theta(y_l\mid x)\)</span>。</p>
<p>论文做了一个统计，对于DPO，满足 <span class="math inline">\(r(x,y_w)&gt;r(x,y_l)\)</span> 和 <span class="math inline">\(p_\theta(y_w\mid x)&gt;p_\theta(y_l\mid
x)\)</span> 两个结果对齐的比例大概只有50%，如下图所示</p>
<img src="/280fa97a/contingency_table.png" class title="contingency table">
<p>这就是训练和推理目标没有完全对齐。</p>
<p>而simPO则可以完全对齐</p>
<img src="/280fa97a/simpo_contingency.png" class title="simPO contingency table">
<h1 id="simpo">simPO</h1>
<h2 id="损失函数">损失函数</h2>
<p>从上面这个分析，我们自然就想到要把训练的目标往推理目标上靠拢对齐。那么最直接的做法，就是把reward从</p>
<p><span class="math display">\[\begin{aligned}r^*(x,y)=\beta\log\frac{\pi_\theta(y\mid
x)}{\pi_\text{ref}(y\mid x)}\end{aligned}\]</span></p>
<p>（这里省略了配分函数Z）</p>
<p>变成</p>
<p><span class="math display">\[\begin{aligned}r_{\text{SimPO}}(x,y)=\frac{\beta}{|y|}\log\pi_\theta(y\mid
x)=\frac{\beta}{|y|}\sum_{i=1}^{|y|}\log\pi_\theta(y_i\mid
x,y_{&lt;i})\end{aligned}\]</span></p>
<p>注意这里有个长度归一化项，这个很重要，没有这一项的话，模型会倾向于生成长度更长但是低质量的内容。</p>
<p>除了修改reward的计算，simPO和IPO、ODPO一样，引入了一个reward
margin，这是一个固定的超参，要求winning response和losing
response的reward差值要大于reward margin</p>
<p><span class="math display">\[p(y_w\succ y_l\mid
x)=\sigma\left(r(x,y_w)-r(x,y_l)-\gamma\right)\]</span></p>
<p>按已有的经验，增大这个margin有助于提高模型泛化能力，但是太大的margin也会导致模型的退化。</p>
<p>至此我们得到了simPO的损失函数</p>
<p><span class="math display">\[\mathcal{L}_{\text{SimPO}}(\pi_\theta)=-\mathbb{E}_{(x,y_w,y_l)\thicksim\mathcal{D}}\left[\log\sigma\left(\frac{\beta}{|y_w|}\log\pi_\theta(y_w|x)-\frac{\beta}{|y_l|}\log\pi_\theta(y_l|x)-\gamma\right)\right]\]</span></p>
<h2 id="simpo梯度更新的直观理解">simPO梯度更新的直观理解</h2>
<p>DPO和simPO的梯度如下</p>
<img src="/280fa97a/gradient.png" class title="梯度">
<p>DPO和simPO的梯度有两个主要区别：<br>
- 梯度权重：simPO的梯度权重没有包含reference model，这样当policy
model给dispreferred
response更高的reward的时候，权重就会变大，加强对这个错误case的修正力度。<br>
- simPO的梯度更新带有length-normalized；而如《Disentangling length from
quality in direct preference
optimization》所发现，DPO里更长的token会有更大的梯度值从而主导了梯度更新的过程，这导致训练出来的模型倾向于生成更长的模型。</p>
<h1 id="实验">实验</h1>
<h2 id="设置">设置</h2>
<p>论文使用了Llama3-8B和Mistral-7B的base和instruct模型进行实验。</p>
<p>对于base模型，就先在UltraChat-200k数据集上训练一个对应的SFT模型，之后在
UltraFeedback数据集上进行preference optimization。</p>
<p>对于instruct模型，参照《Iterative DPO
alignment》的做法，先用这些SFT模型生成preference数据集。具体来说，使用UltraFeedback的prompt，用temperature=0.8的配置，从SFT模型生成5个response，并用PairRM（《LLM-Blender:
Ensembling large language models with pairwise ranking and generative
fusion》）对这5个response进行打分，选择最高分作为preferred
response，最低分的座位dispreferred response。</p>
<p>这样就得到了四组实验组合：Llama3-Base, Llama3-Instruct,
Mistral-Base和Mistral-Instruct。</p>
<p>此外，论文发现超参对preference
optimization的影响很大，因此对不同的方法进行了超参搜索，范围如下</p>
<img src="/280fa97a/hyperparameters.png" class title="超参搜索">
<img src="/280fa97a/simpo_hyperparameters.png" class title="超参搜索">
<p>此外对batch size、解码温度等参数也进行搜索。</p>
<p>所用的数据集如下</p>
<img src="/280fa97a/benchmark.png" class title="benchmark">
<h2 id="对比">对比</h2>
<p>在各个数据集上，不同的优化方法结果对比如下</p>
<img src="/280fa97a/main_results.png" class title="对比结果">
<p>其中LC表示length-controlled，即在限制长度条件下的win rate。</p>
<p>有几个发现：<br>
-
在MT-Bench上，各个方法的差异不大，那些微小的波动可能更多来自于随机性。究其原因可能是因为这个数据集的量比较少，且评价的方案也比较单一，这个发现和《From
live data to high-quality benchmarks: The Arena-Hard
pipeline》的发现是一致的。<br>
-
instruct模型的表现比base要好，这可能是因为这些精心微调过甚至强化学习过的模型本身质量更高。<br>
- 在AlpacaEval 2和Arena-Hard上，simPO在raw win rate和length-controlled
win rate相比其他方案都有明显优势。</p>
<h2 id="消融实验">消融实验</h2>
<p>simPO两个主要的部分就是length
normalization和margin。分别去掉这两个部分之后的结果如下表</p>
<img src="/280fa97a/ablation.png" class title="消融实验">
<p>结果上看，length normalization的影响很大，margin也有一定的影响。</p>
<p>下面具体分析一下。</p>
<p>首先是关于长度归一化。从上表的结果上看，对于simPO，使用长度归一化会让模型生成更短且质量更高的结果。</p>
<p>对比其他训练方法，simPO在长度控制下的win
rate有明显优势，这说明simPO实现了对生成长度的最小利用，即不通过长篇大论来提高得分。</p>
<p>而通用来说，生成结果的长度和质量之间并没有什么强联系。如下表所示，各个训练方法的生成长度和wr并没有什么明显规律，这表明，生成结果的长度并不是衡量生成质量的一个可靠指标。</p>
<img src="/280fa97a/ln.png" class title="长度归一化">
<p>此外，长度归一化会增大偏好数据对之间的reward差。这个很好理解，在有长度归一化的损失函数下，想要达到相同的reward差，模型需要给出y倍的数值才能比margin大。</p>
<p>论文把在不同的长度差异下的reward差画出来，如下图所示</p>
<img src="/280fa97a/ln_effect.png" class title="长度归一化">
<p>可以发现带有长度归一化的simPO无论数据的长度差如何，都能给出positive
reward margin，而没有带长度归一化的模型在winning
response的长度更短的情况下，会给出negative reward
difference，这表明模型对这些样本的学习效果很差。</p>
<p>而从上图b和c子图可以看出，移除长度归一化会使得reward和response
length呈现强烈的正相关关系，而这显然不是我们想要的。</p>
<p>接下来看下reward margin的影响。</p>
<p>把reward accuracy定义为policy model对winning
response的reward高于losing
response的比例。那么如下图所示，随着margin的增大，reward
accuracy也在提升</p>
<img src="/280fa97a/reward_accuracy.png" class title="reward accuracy">
<p>另外实验还发现，增大reward margin，会使得reward difference和winning
response的平均对数似然的分布变得扁平，且winning
response的平均对数似然会减小，如下图所示</p>
<img src="/280fa97a/margin_dist.png" class title="影响分布">
<p>这说明太大的margin设置对模型会有负面影响，因此需要寻找一个中间值使得模型效果最好。</p>
<h2 id="dpo和simpo的对比">DPO和simPO的对比</h2>
<ol type="1">
<li>虽然DPO的reward表达式里没有显式涵盖长度归一化的信息，但是由于使用了reference
model进行对比，在一定程度上可以对抗length
bias。如下图所示，DPO在一定程度上可以打破长度和reward之间的正相关关系，但是没有simPO的效果那么好</li>
</ol>
<img src="/280fa97a/dpo_correlation.png" class title="correlation">
<ol start="2" type="1">
<li>simPO比DPO有更高的reward
accuracy，这表明simPO的reward设计有更强的泛化能力，可以提供更高质量的生成能力</li>
</ol>
<img src="/280fa97a/reward_accuracy_compare.png" class title="reward accuracy对比">
<h1 id="小结">小结</h1>
<p>simPO对损失函数做了一些改变，对齐了训练和推理的目标，使得policy
model能够在提升效果的同时，不过分影响生成结果的长度。并且simPO不再需要reference
model，这也使得训练的空间成本更加节省。</p>
<p>论文在LLAMA和Mistral两个热门的模型上进行了比较多的实验，比较有说服力。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】SimPO: Simple Preference Optimization with a Reference-Free
Reward https://arxiv.org/abs/2405.14734</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>强化学习</tag>
        <tag>微调</tag>
        <tag>SFT</tag>
        <tag>偏好对齐</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型偏好对齐-DPO</title>
    <url>/473f2b43.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>要对齐大模型偏好并不容易，从预训练的数据内容、模型的结构到SFT数据配比甚至数据格式等都会影响最终结果。</p>
<p>按ChatGPT的技术路线，用SFT+RLHF
PPO强化学习确实可以获得一定的提升，但是PPO比较复杂，训练过程不稳定，对微调后的模型、PPO的超参、reward模型的质量等都很敏感，且数据收集和训练的成本都较高，跑通大规模PPO有一定的成本门槛，因此PPO并没有被很广泛地应用。</p>
<p>而DPO，Direct Preference
Optimization，就是PPO的一个简化替代方案。DPO不需要训练reward模型，把PPO的两阶段训练变成一阶段训练，让模型可以直接从偏好数据里学习。</p>
<p>DPO公式有点多，但是并不算太复杂，一步一步理解即可。</p>
<h1 id="对齐">对齐</h1>
<p>大模型在预训练中学到很多知识和技能，但是并不是所有知识和技能都是我们想要的。</p>
<p>比如有一个常见的错误知识，有超过80%的人会有这样的错误认知，那么这个错误知识在预训练数据里也会经常出现。虽然数据集里也会有关于这个知识的正确认知，但是比例相对会比较低。</p>
<p>如果让模型直接用在预训练中学到的知识进行回答，那么模型就有可能给出错误的知识。</p>
<p>这不是我们所希望的。因此需要通过一些方法，让模型给出的结果能对齐人类的偏好，比如最基础的偏好，正确性。</p>
<p>从模型非常广泛的知识和技能中选出我们所需的response和action是构建安全、高效、可控的AI系统的关键。</p>
<p>SFT是最直接的偏好学习方法，而RLHF/RLAIF是上限更高的偏好对齐方案。但RLHF比较复杂，训练不稳定，成本也高。</p>
<p>而DPO的优化目标和RLHF一样，但是实现更简单。</p>
<h1 id="rlhf">RLHF</h1>
<p>先回顾下RLHF的三个阶段。</p>
<ol type="1">
<li>SFT Phase</li>
</ol>
<p>基于预训练模型，在高质量的下游任务数据上训练，获得 <span class="math inline">\(\pi^{\mathrm{SFT}}\)</span>。</p>
<ol start="2" type="1">
<li>Reward Modelling Phase</li>
</ol>
<p>首先给定prompt <span class="math inline">\(x\)</span>，生成两个答案
<span class="math inline">\((y_1,y_2)\sim\pi^\text{SFT}(y|x)\)</span>，并通过人工标注对比
<span class="math inline">\(y_1,y_2\)</span>，获得偏好结果(preference)
<span class="math inline">\(y_w\succ y_l\mid
x\)</span>，其中w和l表示win和lose。</p>
<p>假设在这些偏好结果中，有一个我们无法直接访问的latent reward model
<span class="math inline">\(r^*(y,x)\)</span>，对每对 <span class="math inline">\((x,y)\)</span> 进行打分，这个 <span class="math inline">\(r^*(y,x)\)</span> 就是RLHF里reward
model的拟合目标。</p>
<p>基于 <span class="math inline">\(r^*(y,x)\)</span>，有很多方法对preference进行建模，Bradley-Terry
model就是一个常用的选择。（当然在多个ranked
answers的情况下，可以使用Plackett-Luce ranking models）</p>
<p>基于Bradley-Terry model，人类偏好的分布 <span class="math inline">\(p^{*}\)</span> 写作</p>
<p><span class="math display">\[\begin{aligned}p^*(y_1\succ y_2\mid
x)=\frac{\exp\left(r^*(x,y_1)\right)}{\exp\left(r^*(x,y_1)\right)+\exp\left(r^*(x,y_2)\right)}\end{aligned}\]</span></p>
<p>看起来不复杂，就是把两个答案的reward通过softmax归一化成概率。</p>
<p>假设我们从 <span class="math inline">\(p^{*}\)</span>
采样到一个静态的偏好对比数据集 <span class="math inline">\(\mathcal{D}=\left\{x^{(i)},y_w^{(i)},y_l^{(i)}\right\}_{i=1}^N\)</span>
，那我们就可以用基于 <span class="math inline">\(\pi^{\mathrm{SFT}}\)</span> 初始化得到的reward模型
<span class="math inline">\(r_\phi(x,y)\)</span>，通过maximum
likelihood来拟合 <span class="math inline">\(r^*(y,x)\)</span>。将这个问题表述为二元分类问题，我们就得到negative
log-likelihood loss：</p>
<p><span class="math display">\[\mathcal{L}_R(r_\phi,\mathcal{D})=-\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}\begin{bmatrix}\log\sigma(r_\phi(x,y_w)-r_\phi(x,y_l))\end{bmatrix}\]</span></p>
<p>为了确保reward
function有较低的方差，一般会对reward进行归一化，使得对于所有的 <span class="math inline">\(x\)</span>，有 <span class="math inline">\(\mathbb{E}_{x,y\thicksim\mathcal{D}}\left[r_\phi(x,y)\right]=0\)</span>。</p>
<ol start="3" type="1">
<li>RL Fine-Tuning Phase</li>
</ol>
<p>在强化学习阶段，我们用上一步中得到的reward给目标模型提供反馈，优化如下目标</p>
<p><span class="math display">\[\max_{\pi_\theta}\mathbb{E}_{x\sim\mathcal{D},y\sim\pi_\theta(y|x)}\begin{bmatrix}r_\phi(x,y)\end{bmatrix}-\beta\mathbb{D}_{\mathrm{KL}}\begin{bmatrix}\pi_\theta(y\mid
x)\mid\mid\pi_{\mathrm{ref}}(y\mid x)\end{bmatrix}\]</span></p>
<p>上式中第一项是reward模型对目标模型（即RLHF中的actor
model）给出的答案的reward打分，这一项是越高越好。</p>
<p>而第二项是目标模型和参考模型之间的KL散度，用来限制经过训练后的目标模型，不要偏离参考模型（即
<span class="math inline">\(\pi^{\mathrm{SFT}}\)</span>）太多。这样可以保证reward模型能在经过充分训练的区间工作，同时避免目标模型因过分向高reward分数优化而出现mode-collapse，失去回复的多样性。<span class="math inline">\(\beta\)</span> 用来控制这个限制项的比重。</p>
<p>由于语言生成是离散的，因此上面这个优化目标是不可导的，需要通过RL优化。</p>
<p>标准的RL把reward fucntion构建成</p>
<p><span class="math display">\[r(x,y)=r_\phi(x,y)-\beta(\log\pi_\theta(y\mid
x)-\log\pi_\text{ref}(y\mid x))\]</span></p>
<p>并通过PPO优化。</p>
<h1 id="direct-preference-optimization">Direct Preference
Optimization</h1>
<p>DPO的目标是推导出一种简单的方法，直接使用偏好来进行policy
optimization，而省去训练reward模型的训练。</p>
<img src="/473f2b43/intro.png" class title="DPO">
<h2 id="dpo优化目标的推导">DPO优化目标的推导</h2>
<p>首先，DPO起始的优化目标和RL是相同的：对于任意的reward function <span class="math inline">\(r(x,y)\)</span>，reference model <span class="math inline">\(\pi_{\mathrm{ref}}\)</span></p>
<p><span class="math display">\[\max_\pi\mathbb{E}_{x\thicksim\mathcal{D},y\thicksim\pi}\begin{bmatrix}r(x,y)\end{bmatrix}-\beta\mathbb{D}_{\mathrm{KL}}\begin{bmatrix}\pi(y|x)||\pi_{\mathrm{ref}}(y|x)\end{bmatrix}\]</span></p>
<p>由KL散度的定义，把上式中的第二项展开</p>
<p><span class="math display">\[\beta\mathbb{D}_{\mathrm{KL}}(\pi\|\pi_{\mathrm{ref}})=\beta\sum_y\pi(y|x)\log\frac{\pi(y|x)}{\pi_{\mathrm{ref}}(y|x)}\]</span></p>
<p>这里的条件概率求和其实就是期望值，因此有</p>
<p><span class="math display">\[\max_\pi\mathbb{E}_{x\thicksim\mathcal{D},y\thicksim\pi}\begin{bmatrix}r(x,y)\end{bmatrix}-\beta\mathbb{D}_{\mathbf{KL}}\begin{bmatrix}\pi(y|x)&amp;\mid\mid\pi_{\mathrm{ref}}(y|x)\end{bmatrix}\]</span></p>
<p><span class="math display">\[\begin{aligned}&amp;=\max_\pi\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\left[r(x,y)-\beta\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}\right]\end{aligned}\]</span></p>
<p>然后我们把最大化问题转化成最小化问题</p>
<p><span class="math display">\[\begin{aligned}\max_\pi\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\left[r(x,y)-\beta\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}\right]\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}&amp;=\min_\pi\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\left[\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}-\frac{1}{\beta}r(x,y)\right]\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}&amp;=\min_\pi\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\left[\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)\exp{\left(\frac{1}{\beta}r(x,y)\right)}}\right]\end{aligned}\]</span></p>
<p>在这里我们用配分函数，归一一下分母。令</p>
<p><span class="math display">\[Z(x)=\sum_y\pi_\text{ref}(y|x)\exp\left(\frac1\beta
r(x,y)\right)\]</span></p>
<p>那我们就得到了一个新的有效的概率分布</p>
<p><span class="math display">\[\begin{aligned}\pi^*(y|x)=\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)\end{aligned}\]</span></p>
<p>那么就有</p>
<p><span class="math display">\[\begin{aligned}\min_\pi\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\left[\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)\exp{\left(\frac{1}{\beta}r(x,y)\right)}}\right]\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}&amp;=\min_\pi\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\left[\log\frac{\pi(y|x)}{\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)}-\log
Z(x)\right]\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}&amp;=\min_\pi\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\left[\log\frac{\pi(y|x)}{\pi^*(y|x)}-\log
Z(x)\right]\end{aligned}\]</span></p>
<p>由于 <span class="math inline">\(Z(x)\)</span> 不是 <span class="math inline">\(y\)</span> 的函数，我们可以把它拿出来</p>
<p><span class="math display">\[\begin{aligned}\min_\pi\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\left[\log\frac{\pi(y|x)}{\pi^*(y|x)}-\log
Z(x)\right]\end{aligned}\]</span></p>
<p><span class="math display">\[=\min_\pi\mathbb{E}_{x\thicksim\mathcal{D}}\left[\mathbb{E}_{y\thicksim\pi(y|x)}\left[\log\frac{\pi(y|x)}{\pi^*(y|x)}\right]-\log
Z(x)\right]\]</span></p>
<p><span class="math display">\[=\min_\pi\mathbb{E}_{x\thicksim\mathcal{D}}\left[\mathbb{D}_{\text{KL}}(\pi(y|x)\mid\mid\pi^*(y|x))-\log
Z(x)\right]\]</span></p>
<p><span class="math inline">\(Z(x)\)</span> 和 <span class="math inline">\(\pi\)</span>
无关，因此最小化这个式子只要最小化第一项KL散度。而当且仅当两个分布完全相同的时候，KL散度取得最小值0，因此有</p>
<p><span class="math display">\[\begin{aligned}\pi(y|x)=\pi^*(y|x)=\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)\end{aligned}\]</span></p>
<p>虽然得到了显示解，但是这里的 <span class="math inline">\(Z(x)\)</span>
没法求解，因为排列组合数太多，我们不可能去遍历。</p>
<p>继续对这个式子做一些变换</p>
<p><span class="math display">\[\begin{aligned}\pi_r(y|x)=\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
\log Z(x)+\log \pi_r(y|x)=\log \pi_{\text{ref}}(y|x)
+\frac{1}{\beta}r(x,y)
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}r(x,y)=\beta\log\frac{\pi_r(y\mid
x)}{\pi_\text{ref}(y\mid x)}+\beta\log Z(x)\end{aligned}\]</span></p>
<p>这里我们开始用上Bradley-Terry model了。前面我们提到了Bradley-Terry
model是如下形式</p>
<p><span class="math display">\[\begin{aligned}p^*(y_1\succ y_2\mid
x)=\frac{\exp\left(r^*(x,y_1)\right)}{\exp\left(r^*(x,y_1)\right)+\exp\left(r^*(x,y_2)\right)}\end{aligned}\]</span></p>
<p>在这个基础上做一点变换</p>
<p><span class="math display">\[\begin{aligned}
p^*(y_1\succ y_2\mid
x)&amp;=\frac{\exp\left(r^*(x,y_1)\right)}{\exp\left(r^*(x,y_1)\right)+\exp\left(r^*(x,y_2)\right)}\\
&amp;=\frac1{1+\frac{\exp(r^*(x,y_2))}{\exp(r^*(x,y_1))}}\\
&amp;=\frac1{1+\exp(r^*(x,y_2)-r^*(x,y_1))}
\end{aligned}\]</span></p>
<p>然后我们把 <span class="math inline">\(r\)</span>
代入进去，就得到</p>
<p><span class="math display">\[p^*(y_1\succ y_2\mid
x)=\frac{1}{1+\exp\left(\beta\log\frac{\pi^*(y_2|x)}{\pi_{\text{ref}}(y_2|x)}-\beta\log\frac{\pi^*(y_1|x)}{\pi_{\text{ref}}(y_1|x)}\right)}\]</span></p>
<p>到这里，我们就有了关于optimal
policy的人类偏好数据的概率，而无需经过reward模型。我们可以用MLE直接在这个概率模型上优化目标模型</p>
<p><span class="math display">\[\mathcal{L}_{\text{DPO}}(\pi_\theta;\pi_{\text{ref}})=-\mathbb{E}_{(x,y_w,y_l)\thicksim\mathcal{D}}\left[\log\sigma\left(\beta\log\frac{\pi_\theta(y_w\mid
x)}{\pi_{\text{ref}}(y_w\mid x)}-\beta\log\frac{\pi_\theta(y_l\mid
x)}{\pi_{\text{ref}}(y_l\mid x)}\right)\right]\]</span></p>
<p>DPO loss的实现如下</p>
<img src="/473f2b43/dpo_loss_code.png" class title="DPO实现">
<h2 id="理解dpo损失函数">理解DPO损失函数</h2>
<p>首先我们了解一下DPO的loss在做什么，对DPO的损失函数求个导。</p>
<p>方便起见，令</p>
<p><span class="math display">\[u=\beta\log\frac{\pi_{\theta}(y_{w}|x)}{\pi_{\mathrm{ref}}(y_{w}|x)}-\beta\log\frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{ref}}(y_{l}|x)}\]</span></p>
<p>那么原损失函数可以写成</p>
<p><span class="math display">\[L_{DPO}(\pi_{\theta};\pi_{\mathrm{ref}})=-\min_{\pi_{0}}E_{(x,y_{u},y_{t})\sim
D}[\log\sigma(u)]\]</span></p>
<p>对sigmoid求导，有</p>
<p><span class="math display">\[\frac\partial{\partial
u}\log\sigma(u)=\frac1{\sigma(u)}\cdot\sigma(u)(1-\sigma(u))=1-\sigma(u)\]</span></p>
<p>由sigmoid函数性质，有</p>
<p><span class="math display">\[1-\sigma(u)=\sigma(-u)\]</span></p>
<p>对 <span class="math inline">\(u\)</span> 求导</p>
<p><span class="math display">\[\frac{\partial
u}{\partial\theta}=\beta\left(\frac{\partial}{\partial\theta}\log\frac{\pi_\theta(y_w|x)}{\pi_{\mathrm{ref}}(y_w|x)}-\frac{\partial}{\partial\theta}\log\frac{\pi_\theta(y_l|x)}{\pi_{\mathrm{ref}}(y_l|x)}\right)\]</span></p>
<p>第一项对数求导，由于 <span class="math inline">\(\pi_{\mathrm{ref}}\)</span> 不依赖 <span class="math inline">\(\theta\)</span>，可以视作常数，因此有</p>
<p><span class="math display">\[\begin{aligned}
\frac\partial{\partial\theta}\log\frac{\pi_\theta(y_w|x)}{\pi_\mathrm{ref}(y_w|x)}=&amp;\frac{1}{\frac{\pi_{\theta}(y_{w}|x)}{\pi_{\mathrm{ref}}(y_{w}|x)}}\cdot\frac{\partial}{\partial\theta}\frac{\pi_{\theta}(y_{w}|x)}{\pi_{\mathrm{ref}}(y_{w}|x)}\\
=&amp;\frac{1}{\pi_{\theta}(y_{w}|x)}\cdot\frac{\partial}{\partial\theta}\pi_{\theta}(y_{w}|x)\\
=&amp;\begin{aligned}\nabla_\theta\log\pi(y_w\mid x)\end{aligned}
\end{aligned}\]</span></p>
<p>类似地，第二项求导</p>
<p><span class="math display">\[\frac{\partial}{\partial\theta}\log\frac{\pi_\theta(y_l|x)}{\pi_{\mathrm{ref}}(y_l|x)}=\nabla_\theta\log\pi(y_l\mid
x)\]</span></p>
<p>因此，DPO损失的导数是</p>
<p><span class="math display">\[\begin{aligned}
&amp;\nabla_\theta\mathcal{L}_{\text{DPO}}(\pi_\theta;\pi_{\text{ref}})\\&amp;=-\mathbb{E}_{(x,y_w,y_l)\thicksim\mathcal{D}}\left[\beta\sigma\left(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)}-\beta\log\frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right)\left[\nabla_\theta\log\pi(y_w\mid
x)–\nabla_\theta\log\pi(y_l\mid x)\right]\right]
\end{aligned}\]</span></p>
<p>再令</p>
<p><span class="math display">\[\hat{r}_\theta(x,y)=\beta\log\frac{\pi_\theta(y|x)}{\pi_\text{ref}(y|x)}\]</span></p>
<p>那么DPO损失的梯度可以写作</p>
<p><span class="math display">\[\begin{aligned}
&amp;\nabla_\theta\mathcal{L}_{\text{DPO}}(\pi_\theta;\pi_{\text{ref}})\\&amp;=-\beta\mathbb{E}_{(x,y_w,y_l)\thicksim\mathcal{D}}\left[\sigma\left(\hat{r}_\theta(x,y_l)-\hat{r}_\theta(x,y_w)\right)\left[\nabla_\theta\log\pi(y_w\mid
x)–\nabla_\theta\log\pi(y_l\mid x)\right]\right]
\end{aligned}\]</span></p>
<p>梯度各项的意义如下</p>
<img src="/473f2b43/gradient.png" class title="DPO梯度">
<p><span class="math inline">\(\hat{r}_\theta(x,y)\)</span> 相当于 <span class="math inline">\(\pi_{\theta}\)</span> 和 <span class="math inline">\(\pi_{\mathrm{ref}}\)</span>
共同确定的隐式reward。</p>
<h2 id="dpo流程">DPO流程</h2>
<p>DPO的一般流程是：<br>
- 对于每个prompt <span class="math inline">\(x\)</span>，采样 <span class="math inline">\(y_1,y_2\sim\pi_{\text{ref}}(\cdot\mid
x)\)</span>，然后进行人工标注构建偏好数据集 <span class="math inline">\(\mathcal{D}=\{x^{(i)},y_w^{(i)},y_l)^{(i)}\}_{i=1}^N\)</span><br>
- 基于 <span class="math inline">\(\mathcal{L}_{\mathrm{DPO}}\)</span>，在已有的
<span class="math inline">\(\pi_{\mathrm{ref}}\)</span>、<span class="math inline">\(\mathcal{D}\)</span> 和 <span class="math inline">\(\beta\)</span> 上优化 $$</p>
<p>但是收集偏好数据的成本还是比较高的，因此实际使用中，人们更愿意使用开源的偏好数据集。</p>
<p>当我们的偏好数据是来自 <span class="math inline">\(\pi^{\mathrm{SFT}}\)</span> 的时候，我们直接让
<span class="math inline">\(\pi_{\mathrm{ref}}=\pi^{\mathrm{SFT}}\)</span>。如果我们使用开源偏好数据集的话，就可能没法直接使用生成这些数据的模型，这时可以用偏好数据集里
<span class="math inline">\((x,y_w)\)</span> 数据对 <span class="math inline">\(\pi_{\mathrm{ref}}\)</span> 进行微调，即</p>
<p><span class="math display">\[\pi_{\text{ref}}=\arg\max_\pi\mathbb{E}_{x,y_w\thicksim\mathcal{D}}\left[\log\pi(y_w\mid
x)\right]\]</span></p>
<p>这个微调步骤有助于缓解 <span class="math inline">\(\pi_{\mathrm{ref}}\)</span> 和真实 reference
distribution 之间的distribution shift。</p>
<h2 id="your-language-model-is-secretly-a-reward-model">Your Language
Model Is Secretly a Reward Model</h2>
<p>在前面推导DPO的loss函数的时候，我们把reward的公式显示表达成</p>
<p><span class="math display">\[\begin{aligned}r(x,y)=\beta\log\frac{\pi_r(y\mid
x)}{\pi_\text{ref}(y\mid x)}+\beta\log Z(x)\end{aligned}\]</span></p>
<p>但是这里 <span class="math inline">\(Z(x)\)</span>
的组合空间太大，实际上没法求解。</p>
<p>好在"在Plackett-Luce/Bradley-Terry模型框架下，同一等价类中的两个reward
function有相同的preference distribution"</p>
<blockquote>
<p>Under the Plackett-Luce preference framework, and in particular the
BradleyTerry framework, two reward functions from the same equivalence
class induce the same preference distribution</p>
</blockquote>
<p>如果两个reward function <span class="math inline">\(r(x,y)\)</span>
和 <span class="math inline">\(r^{\prime}(x,y)\)</span> 可以写成</p>
<p><span class="math display">\[r&#39;(x,y)=r(x,y)+f(x)\]</span></p>
<p>即表示这两个reward function来自同一等价类(equivalence class)。</p>
<p>对于prompt <span class="math inline">\(x\)</span> 和 answer <span class="math inline">\(y_1,\ldots,y_K\)</span>，以及对应的ranking <span class="math inline">\(\tau\)</span>，在Plackett-Luce
framework（Bradley–Terry也是其中一个特例）下的证明如下</p>
<p><span class="math display">\[\begin{aligned}
p_{r&#39;}(\tau|y_1,\ldots,y_K,x)&amp;
=\prod_{k=1}^K\frac{\exp(r&#39;(x,y_{\tau(k)}))}{\sum_{j=k}^K\exp(r&#39;(x,y_{\tau(j)}))}  \\
&amp;=\prod_{k=1}^K\frac{\exp(r(x,y_{\tau(k)})+f(x))}{\sum_{j=k}^K\exp(r(x,y_{\tau(j)})+f(x))}
\\
&amp;=\prod_{k=1}^K\frac{\exp(f(x))\exp(r(x,y_{\tau(k)}))}{\exp(f(x))\sum_{j=k}^K\exp(r(x,y_{\tau(j)}))}
\\
&amp;=\prod_{k=1}^K\frac{\exp(r(x,y_{\tau(k)}))}{\sum_{j=k}^K\exp(r(x,y_{\tau(j)}))}
\\
&amp;=p_r(\tau|y_1,\ldots,y_K,x)
\end{aligned}\]</span></p>
<p>基于此，我们可以把上面的 <span class="math inline">\(\beta\log
Z(x)\)</span> 项忽略掉，也就是说下面两个reward
function是具有相同的preference distribution的</p>
<p><span class="math display">\[\begin{aligned}r(x,y)=\beta\log\frac{\pi_r(y\mid
x)}{\pi_\text{ref}(y\mid x)}+\beta\log Z(x)\end{aligned}\]</span></p>
<p><span class="math display">\[\hat{r}_\theta(x,y)=\beta\log\frac{\pi_\theta(y|x)}{\pi_\text{ref}(y|x)}\]</span></p>
<p>更进一步地，两个来自同一等价类的reward
function在相同的RL问题下会导向相同的optimal policy。</p>
<p>在推导DPO的loss的部分中，我们得到了optimal policy的显式解</p>
<p><span class="math display">\[\begin{aligned}\pi(y|x)=\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)\end{aligned}\]</span></p>
<p>这里证明一下两个reward function可以导向相同的optimal
policy。假设<span class="math inline">\(r&#39;(x,y)=r(x,y)+f(x)\)</span>，<span class="math inline">\(\pi_r\)</span> 和 <span class="math inline">\(\pi_{r&#39;}\)</span> 分别是它们对应的optimal
policy，有</p>
<p><span class="math display">\[\begin{aligned}
\pi_{r^{\prime}}(y|x)&amp;
\begin{aligned}&amp;=\frac{1}{\sum_y\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r&#39;(x,y)\right)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r&#39;(x,y)\right)\end{aligned}  \\
&amp;=\frac{1}{\sum_y\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}(r(x,y)+f(x))\right)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}(r(x,y)+f(x))\right)
\\
&amp;\begin{aligned}=\frac{1}{\exp\left(\frac{1}{\beta}f(x)\right)\sum_y\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)\exp\left(\frac{1}{\beta}f(x)\right)\end{aligned}
\\
&amp;\begin{aligned}&amp;=\frac{1}{\sum_y\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)\end{aligned}
\\
&amp;=\pi_r(y|x)
\end{aligned}\]</span></p>
<p>那么，与Plackett-Luce（特别是Bradley-Terry）模型一致的所有reward类别，都可以被某个模型
<span class="math inline">\(\pi(y\mid x)\)</span> 和 一个给定的reference
model <span class="math inline">\(\pi_{ref}(y\mid x)\)</span>
所表示：</p>
<p><span class="math display">\[r(x,y)=\beta\log\frac{\pi(y|x)}{\pi_{ref}(y|x)}\]</span></p>
<p>也就是我们的语言模型都天然具有reward model的功能。</p>
<h2 id="实验">实验</h2>
<p>实际训练中，论文中所使用的超参和设置：<br>
- <span class="math inline">\(\beta=0.1\)</span>（对于TL;DR
summarization，设为0.5）<br>
- batch size = 64<br>
- RMSprop optimizer<br>
- learning rate = 1e-6<br>
- linearly warmup 0 to 1e-6 over 150 steps</p>
<p>论文在对话、摘要等任务进行的效果评测，主要对比了PPO、SFT和DPO的效果。</p>
<p>DPO即使在没有精细调参的情况下，也有比价好的效果</p>
<img src="/473f2b43/result_1.png" class title="对比1">
<img src="/473f2b43/result_2.png" class title="对比2">
<img src="/473f2b43/result_3.png" class title="对比3">
<img src="/473f2b43/result_4.png" class title="对比4">
<h1 id="小结">小结</h1>
<ul>
<li>DPO在RLHF
PPO相同的优化问题下，推导出了新的优化形式，省去了reward模型的部分，从而可以直接用偏好数据优化模型<br>
</li>
<li>DPO在效果和效率上相比PPO都有优势</li>
</ul>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】Direct Preference Optimization: Your Language Model is Secretly
a Reward Model https://arxiv.org/abs/2305.18290v2</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>强化学习</tag>
        <tag>微调</tag>
        <tag>SFT</tag>
        <tag>偏好对齐</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型算法题(5)</title>
    <url>/336f2f3e.html</url>
    <content><![CDATA[<p><img src="/images/cover.png"></p>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a></p>
<hr>
<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<p>本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~</p>
<p>如有错漏，欢迎指正~</p>
<hr>
<h1 id="使用半精度训练时bf16和fp16格式有什么异同">1.使用半精度训练时，bf16和fp16格式有什么异同？</h1>
<p>二者都是占用16bit空间。</p>
<p>fp16由1个符号位、5个指数位和10个尾数位组成。fp16在表达小数时具有较高的精度，但表示的最大范围相对bf16比较小。相比bf16，在表达较大的数时更容易出现上溢的情况。</p>
<p>bf16由1个符号位、8个指数位和7个尾数位组成。相比于fp16，bf16牺牲了一些尾数位以增加指数位，扩大了表达的范围，但是精度降低了，因此对于对精度需求比较高的模型，模型可能效果不如fp16。</p>
<p>模型训练时使用bf16和fp16都可以降低内存使用和传输量，提高训练效率。</p>
<img src="/336f2f3e/bfloat16.jpeg" class title="bf16">
<h1 id="支持模型长上下文的方案ntk-aware-interpolation的思路是什么">2.支持模型长上下文的方案「NTK-aware
interpolation」的思路是什么？</h1>
<p>1.在NTK插值之前，线性插值通过在原模型训练的两个位置编码中间，插入新的位置编码，使得同样的取值范围可以容纳更多位置。</p>
<p>2.而NTK插值则是一种非线性插值的方法。它通过仅改变RoPE的base，使得位置编码中不同频率的信号有不同的表现，具体来说就是“高频外推，低频内插”。高频信号使用外推，防止分辨率太低，而低频信号沿用插值的方式，实现方便。</p>
<h1 id="llm长度外推方案ntk-by-parts的思路是什么">3.LLM长度外推方案NTK-by-parts的思路是什么？</h1>
<p>NTK-by-parts的方法在NTK插值的基础上又多想了一层。它认为无论是线性插值还是NTK-aware插值，都认为RoPE的所有分量都对网络有同样的重要性。而NTK-by-parts的思路认为，应该区别对待不同分量，他们对网络的影响有所不同。对于波长远小于上下文长度的分量（如波长&lt;=1/32上下文），就不插值只外推；而对于波长大于等于上下文长度的分量，就只外推不插值；对于介于两者之间的分量，就使用外推和插值的加权和。</p>
<p>使用一个斜坡函数来定义NTK-by-parts的分段插值方法，如下所示</p>
<img src="/336f2f3e/ntk_by_parts.png" class title="NTK-by-parts">
<h1 id="llm长度外推方案yarn是怎做的">4.LLM长度外推方案YaRN是怎做的？</h1>
<p>PI/NTK/NTK-by-parts主要的做法都是使用插值，而随着插值进行，token之间的距离变得更近（因为现在每一个位置旋转角度变小了），平均最小距离在减小，这样注意力softmax的分布会变得更尖，也就是都集中在某个区间。</p>
<p>换句话说，就是RoPE原本远距离衰减的特性变弱了，衰减得更不明显，就会导致模型更平均地关注到更多的token，这样就削弱了注意力机制，导致输出质量下降。</p>
<p>可以通过在softmax之前，将中间注意力矩阵乘以温度
t&gt;1来缓解这个问题。由于RoPE被编码为一个旋转矩阵，就可以简单地给旋转矩阵乘以一个系数根号t来实现，这样可以不必修改注意力的代码。</p>
<p>YaRN结合NTK-by-parts和这个温度系数，对attention score进行调整。</p>
<img src="/336f2f3e/yarn.png" class title="YaRN">
<h1 id="对于使用group-query-attention的模型假设hidden-sizedq的注意力头数量为h每个头维度为d假设有ddhkv组数为n输入上下文长度为sbatch-sizeb模型层数为l计算推理时kv-cache所需的空间">5.对于使用Group-Query
Attention的模型，假设hidden
size=D，Q的注意力头数量为h，每个头维度为d（假设有D=d×h），kv组数为n，输入上下文长度为s，batch
size=b，模型层数为L，计算推理时kv cache所需的空间。</h1>
<p>kv cache缓存的是经过投影变换之后的K和V矩阵。</p>
<p>对于GQA，每层有n组K和V，每组的特征维度和Q的每个头的特征维度相同，为D/h。则每层每组K和V数据量为sD/h，整个模型共有2LnsD/h个数据，因此整个batch需要缓存2bLnsD/h个数据。
如果使用的是半精度浮点数，每个浮点需要两个字节，因此共需要4bLnsD/h字节的空间。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a></p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>算法题</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型偏好对齐-ODPO</title>
    <url>/da871ebe.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>前面对DPO的思路做了整理：<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a>。</p>
<p>DPO把RLHF的两阶段训练，变成了一阶段训练，降低了训练成本。而ODPO（DPO
with an
offset）在DPO的基础上做了一点改进，在几个下游任务的实验中，获得了比DPO更好的效果。</p>
<h1 id="背景">背景</h1>
<p>直接使用指令微调，是让模型学会处理下游任务的一个快速有效的方法。</p>
<p>但是指令微调的优化目标是maximize the response
log-likelihood，这和“生成人类所偏好的高质量内容”的目标之间存在gap，不完全对齐。</p>
<p>这个misalignment部分是因为maximum
likelihood的目标无法区分数据里“大错”（比如幻觉）和“小错”（比如标点符号不恰当）。</p>
<blockquote>
<p>Training with the maximum likelihood objective makes the model assign
nonzero probability mass to all responses in SFT dataset, even those of
lower quality.</p>
</blockquote>
<p>因此有RLHF的方法来解决这个问题。RL通过人类偏好数据训练一个reward模型，并用reward模型来指导策略模型。</p>
<p>而reward的modeling有两种，pointwise reward和pairwise preference。</p>
<p>pointwise
reward一般用于reward有比较确定定义且简单的场景，比如情感分类，我们可以定义positive的情感的reward为1，negative的reward为0。类似的还有toxicity等。这些类别一般也有很多现成的打分模型/classifier可以使用。</p>
<p>pairwise
preference一般用于比较复杂的任务，比如文本摘要和对话生成。这类任务难以直接基于单个答案来打分，而需要通过对比才能知道哪个更好。</p>
<p>但RLHF成本比较高，因此DPO对训练过程进行了简化。</p>
<h1 id="bradleyterry-model的局限">Bradley–Terry model的局限</h1>
<p>DPO的损失如下</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{L}^{\mathrm{DPO}}(\boldsymbol{\theta})&amp;
=-\mathbb{E}_{(\boldsymbol{x},\boldsymbol{y}_w,\boldsymbol{y}_l)\sim\mathcal{D}_{\text{HF}}}\left[\log\sigma\Big(\beta\log\frac{\pi_{\boldsymbol{\theta}}(\boldsymbol{y}_w\mid\boldsymbol{x})}{\pi_{\text{SFT}}(\boldsymbol{y}_w\mid\boldsymbol{x})}-\beta\log\frac{\pi_{\boldsymbol{\theta}}(\boldsymbol{y}_l\mid\boldsymbol{x})}{\pi_{\text{SFT}}(\boldsymbol{y}_l\mid\boldsymbol{x})}\Big)\right]  \\
&amp;=-\underset{(\boldsymbol{x},\boldsymbol{y}_w,\boldsymbol{y}_l)\thicksim\mathcal{D}_{\mathrm{HF}}}{\operatorname*{\mathbb{E}}}\left[\log\sigma\left(\hat{r}_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{y}_w)-\hat{r}_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{y}_l)\right)\right]
\end{aligned}\]</span></p>
<p>其中</p>
<p><span class="math display">\[\hat{r}_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{y})=\beta\log\frac{\pi_{\boldsymbol{\theta}}(\boldsymbol{y}|\boldsymbol{x})}{\pi_{\mathrm{SFT}}(\boldsymbol{y}|\boldsymbol{x})}\]</span></p>
<p>是estimated reward。</p>
<p>这个DPO损失的形式背后用到了Bradley–Terry
model对偏好进行建模。而Bradley–Terry
model只给出了一个response比另一个response好的概率，而没有告诉我们好的程度。</p>
<p>而实际上我们很多偏好对比数据都提供了具体的分数，而不仅仅是排序信息。有这些具体分数我们就可以知道两条response之间是差一点点，还是差很多。</p>
<p>那么把这个差距的信息引入到偏好的建模里，应该能带来收益，这也是ODPO的思路，而两个response之间的差距就是offset。</p>
<h1 id="dpo-with-an-offset">DPO with an Offset</h1>
<p>给 <span class="math inline">\(\hat{r}_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{y}_w),\hat{r}_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{y}_l)\)</span>
分别加上Gumbel noise，即得到</p>
<p><span class="math display">\[\tilde{r}_w\sim\operatorname{Gumbel}(\hat{r}_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{y}_w),1)\]</span></p>
<p><span class="math display">\[\tilde{r}_l\sim\operatorname{Gumbel}(\hat{r}_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{y}_l),1)\]</span></p>
<p>论文中证明了</p>
<p><span class="math display">\[p\big(\tilde{r}_w-\tilde{r}_l&gt;\Delta_r\big)=\sigma(\Delta_{\hat{r}_\theta}-\Delta_r)\]</span></p>
<p>基于此，ODPO的损失函数表达成</p>
<p><span class="math display">\[\mathcal{L}^{\mathrm{ODPO}}(\boldsymbol{\theta})=-\underset{(\boldsymbol{x},\boldsymbol{y}_w,\boldsymbol{y}_l)\sim\mathcal{D}_{\mathrm{HF}}}{\operatorname*{\mathbb{E}}}\left[\log\sigma{\left(\hat{r}_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{y}_w)-\hat{r}_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{y}_l)-\Delta_r\right)}\right]\]</span></p>
<p>这相当于要求preferred response的estimated reward要比dispreferred
response的estimated reward大，且要大offset值这么多。</p>
<p>当offset=0的时候，ODPO的损失等价于DPO的损失。</p>
<p>ODPO的这个做法和softmax margin loss/marginal
loss有些相似，都是在原来loss的基础上，加上一个margin，加大对靠得比较近的数据对的penalization的力度。</p>
<p>ODPO里，offset是两个response之间的actual reward的increasing scaling
function。</p>
<p><span class="math display">\[\Delta_r=\alpha\mathbf{f}\big(\mathrm{score}(\boldsymbol{x},\boldsymbol{y}_w)-\mathrm{score}(\boldsymbol{x},\boldsymbol{y}_l)\big)\]</span></p>
<p>其中 <span class="math inline">\(\alpha\)</span> 是超参。</p>
<img src="/da871ebe/odpo_intro.png" class title="intro">
<h1 id="实验">实验</h1>
<p>论文在几个下游任务上做了实验。</p>
<h2 id="sentiment-control">sentiment control</h2>
<p>首先是sentiment control的任务，即要求模型输出positive的response。</p>
<p>先用GPT2-Large在IMDB
dataset做了finetune，获得SFT模型。论文用一个现成的sentiment
classifier作为reward的打分模型，给response分别打分，分数如下计算</p>
<p><span class="math display">\[r_{negative}(\boldsymbol{x},\boldsymbol{y}) =
1-p(\text{negative}\mid\cdot)\]</span></p>
<p><span class="math display">\[r_{positive}(\boldsymbol{x},\boldsymbol{y}) =
1+p(\text{positive}\mid\cdot)\]</span></p>
<p>有了reward打分数据之后，还要构造偏好数据对。这里把同一个prompt下生成的所有reward分数不同的response进行排列组合，获得偏好数据对。</p>
<p>对于DPO，有这些偏好数据对就够了。而ODPO还需要一个offset，按如下方式计算：</p>
<p><span class="math display">\[\Delta_r=\log\left(r(\boldsymbol{y}_w)-r(\boldsymbol{y}_l)\right)\]</span></p>
<p>实验里把 <span class="math inline">\(\alpha\)</span> 设为1。</p>
<p>实验中使用两个不同的random
seed，从SFT模型里进行采样，从而得到了2份不同的偏好数据。</p>
<p>而 <span class="math inline">\(\beta\)</span> 使用了14个不同的取值
<span class="math inline">\(\{0.1,0.2,\ldots,1\}\cup\{1,2,3,4,5\}\)</span>
进行实验。</p>
<p>论文在2份数据集下分别使用不同的数据量进行训练（5000，7500,10000），这样DPO和ODPO分别有2×3×14=84个实验。</p>
<p>每个实验计算模型生成结果的sentiment打分，以及和SFT模型的KL
divergence。结果如下图</p>
<img src="/da871ebe/sentiment_control.png" class title="sentiment control">
<p>我们希望模型在sentiment的打分上越高越好，同时不要和SFT模型有太大的差距，因此越靠近左上角的点越符合我们的要求。从结果上看，ODPO比DPO更好一些。</p>
<h2 id="toxicity-control">toxicity control</h2>
<p>toxicity control任务和sentiment
control类似，要求模型的response的毒性尽量低。</p>
<p>这次使用GPT-neo-2.7b模型，<span class="math inline">\(\beta\)</span>
的取值范围为 <span class="math inline">\(\{0.05,0.1,0.2,0.3,0.4,0.5\}\)</span>，使用从REALTOXICITYPROMPTS数据集里抽样的10000个毒性评分大于0.3的prompt。</p>
<p>结果如下</p>
<img src="/da871ebe/toxicity_control.png" class title="toxicity control">
<p>在数据量较少的情况下（8000 &amp; 9000），ODPO效果更明显好。</p>
<h2 id="summarization">summarization</h2>
<p>摘要任务使用REDDIT TL;DR数据集，使用的模型是GPTJ-6B。</p>
<p>DPO和ODPO训练后的评分：抽了100条测试prompt，用不同的temperature生成结果，并用GPT-4进行评分对比。结果如下</p>
<img src="/da871ebe/summarization.png" class title="summarization">
<p>DPO和ODPO都比SFT好，并且在temperature比较低的设置下，DPO和ODPO都比human-written的结果好。</p>
<h2 id="消融实验scaling-function">消融实验：scaling function</h2>
<p>前面实验的offset都是用reward差值的log值，这里使用其他两种计算方式进行对比</p>
<p><span class="math display">\[\Delta_r=\log r(\boldsymbol{y}_w)-\log
r(\boldsymbol{y}_l)\]</span></p>
<p><span class="math display">\[\begin{array}{rcl}\Delta_r=r(\boldsymbol{y}_w)-r(\boldsymbol{y}_l)\end{array}\]</span></p>
<p>使用5000对sentiment control的数据，<span class="math inline">\(\beta
\in \{0.1,0.2,\ldots,0.9\}\cup\{1,2,3,4,5\}\)</span>。</p>
<p>对比结果如下</p>
<img src="/da871ebe/scaling_function.png" class title="scaling function">
<p>使用log scaling的ODPO在KL
divergence更小的时候（0.4）可以达到0.8的reward，而没有使用log
scaling的模型需要再更大的KL divergence下才能达到通用的reward。</p>
<h2 id="消融实验α">消融实验：α</h2>
<p>同样使用7500对sentiment control的数据，<span class="math inline">\(\beta=0.5\)</span>，改变<span class="math inline">\(\alpha\in\{0.0,0.1,0.2,0.3,0.5,0.8,1.\}\)</span>。</p>
<img src="/da871ebe/alpha.png" class title="alpha">
<p>发现更高的 <span class="math inline">\(\alpha\)</span>
会使得模型更多偏离SFT模型，并带来更高的reward值。</p>
<h1 id="小结">小结</h1>
<p>ODPO在DPO的基础上加入了offset，在实现上并不复杂，而且能带来一些收益。</p>
<p>略有瑕疵的是ODPO的实验覆盖面并不太全，也没有使用LLAMA等更强大的模型进行实验。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】Direct Preference Optimization with an Offset
https://arxiv.org/pdf/2402.10571</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>强化学习</tag>
        <tag>微调</tag>
        <tag>SFT</tag>
        <tag>偏好对齐</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型推理窗口-从有限到无限大</title>
    <url>/45ee1a6d.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>步入2024年Q2，大模型在RAG、文档对话、大模型Agent能力等方向的发展持续升温。在平时的日常生活和工作中，大模型工具提供的文档总结、文本润色、代码生成等能力已经是提高效率的必备帮手，甚至在一些复杂或者不熟悉的场景上，大模型也已经能提供一些比较专业的帮助。</p>
<p>在这些方向上，大模型(超)长上下文的能力都是基础。目前不少模型已经在128k+的长度上展示出比较强的能力，这些模型有的收集了超长训练数据，有的优化位置编码，有的则暴力训练，以提升模型的长上下文能力。</p>
<p>另外一些工作则另辟蹊径，直接让模型具备无限大窗口的能力。本篇将梳理几个理论上支持无限大上下文长度的工作。</p>
<h1 id="streamingllm">StreamingLLM</h1>
<p>论文：Efficient Streaming Language Models with Attention Sinks</p>
<p>时间：2023年9月</p>
<p>长度：测试4M，理论∞</p>
<p>阶段：无需训练，直接应用在推理阶段</p>
<p>这是一篇由MIT，CMU，NVIDIA，META联合发布的论文。论文提出了一个在流式输出场景下支持无限大上下文长度的方法StreamingLLM，并且不需要进行任何形式的训练或微调
-- 在有限长度上训练，在无限长度上推理。论文用LLAMA2, MPT,
Falcon和Pythia作为基线模型进行验证，在4M的上下文长度，StreamingLLM仍然能够保持较低的PPL。</p>
<p>需要注意的是，StreamingLLM主要关注在超长上下文的情况下，让模型能够生成低PPL的内容，但是在内容的准确性上并没有保证。</p>
<h2 id="背景">背景</h2>
<p>大模型流式应用，比如现在大部分对话机器人，在部署推理的时候，在长上下文的情况下通常会面临两个问题：<br>
-
缓存不足：流式应用下，为了提升响应速度，KV缓存目前是不可缺少的。而随着对话交互的进行，KV缓存所需的空间也以线性速度在增长。鉴于目前模型的规模动辄百亿千亿的参数量，即使较强的GPU
H100、A100拥有40G/80G的显存，推理时也捉襟见肘。在显存不足的情况下，我们就不得不抛弃部分旧的KV缓存值，这也导致了模型效果的下降。<br>
-
训练长度：主流模型大部分使用旋转位置编码RoPE或其变体，而RoPE的外推能力一般，因此当推理输入的上下文长度远超训练的长度时，模型效果也会迅速下降。</p>
<p>现有一些工作针对其中的部分问题进行优化：<br>
- 类似Longformer那样的window
attention通过只保存最近的KV值来缓解缓存不足的问题，同时保持一定的窗口扩展能力。但是论文观察到，一旦上下文长度超过缓存大小，window
attention的机制会把最初的部分token移出缓存，这时模型的效果依然会迅速下滑。<br>
- sliding window with
recomputation（https://github.com/mit-han-lab/streaming-llm/issues/51），可以节省KV所需的空间，但是recomputation计算量和长度是平方关系。这个方案也是论文基线方案里效果最接近可用的。<br>
-
针对位置编码的工作，比如线性插值、NTK插值、YaRN，已经被证明是有效的，但是优化的窗口依然是有限的，大概能在原来的训练长度的基础上，提升一倍或者几倍的推理窗口。论文还提到长窗口的一个实践工作，一个关注在NSFW内容(手动滑稽)的模型：https://kaiokendev.github.io/til#extending-context-to-8k<br>
- 针对推理效率，也有一些工作：《Efficiently scaling transformer
inference，SmoothQuant: Accurate and efficient post-training
quantization for large language models》、《Dynamic context pruning for
efficient and interpretable autoregressive transformers》、《Spatten:
Efficient sparse attention architecture with cascade token and head
pruning》、《H2o: Heavyhitter oracle for efficient generative inference
of large language models》，但是在支持的最大长度依然有限制。<br>
-
无损的FlashAttention已经被广泛采用，无论是训练还是推理都很有帮助。<br>
- 其他一些有损的扩展方案，比如Big Bird、Linformer等。</p>
<p>StreamingLLM这篇论文发现在长上下文的情况下，存在attention
sink的现象，这也是window
attention在上下文超过缓存大小之后效果迅速变差的原因。通过利用attention
sink，让模型在超长上下文的情况还可以保持较低的PPL。并且和基线里唯一效果比较好的sliding
window with
recomputation方案相比，StreamingLLM在速度上有22+倍的提升。</p>
<p>StreamingLLM和其他方案在长上下文上的PPL对比如下图所示，StreamingLLM在各个模型上都能保持稳定较低的PPL。</p>
<img src="/45ee1a6d/streamingllm_model_ppl.png" class title="PPL对比">
<h2 id="attention-sink">attention sink</h2>
<p>从window
attention在上下文超过缓存大小之后的失效，论文发现自回归LLM存在的一个有趣现象：对于输入文本最靠前的少量几个token，无论它们在语义上与语言建模任务的相关性如何，大量的注意力分数都会分配给他们，如下图所示</p>
<img src="/45ee1a6d/stremingllm_attention_sink.png" class title="attention sink">
<p>模型的前两层还能保持attention
score更多分配给当前token附近位置的特性，而在其他层，靠前的几个token都会接受到大量的注意力。</p>
<p>论文里把这些token称为attention
sink。尽管这些token在语义上很可能并没有什么重要性，但它们却聚集了大量的注意力分数。</p>
<p>出现这个现象的原因就是softmax操作。softmax要求所有上下文token的注意力分数加起来等于1，因此，即使当前token跟前面的其他token都没有匹配的需要，模型仍然需要将多余的注意力值分配到前面的某些token，以使得总和为1。</p>
<p>那么文本最开头的几个初始token就会承担“接收多余的、不需要的注意力”的任务。为什么是初始token来承担这个任务，最简单的原因就是，对于自回归语言建模，初始token对所有后续token都是可见的，这使得它们更容易被训练成attention
sink。</p>
<p>上面这个解释还只是猜想，于是论文做了一个实验来验证这个猜想：把初始的4个token都换成没有重要实际语义的换行符号，结果发现模型依然会把大量的注意力分配给这些token，这就说明attention
sink这个现象和内容语义无关，而只和这些token所在的位置相关。</p>
<p>这个现象在一些quant的工作里也有发现，比如《SmoothQuant: Accurate and
efficient post-training quantization for large language
models》和《Quantizable transformers: Removing outliers by helping
attention heads do nothing》等。</p>
<h2 id="推理到无限长">推理到无限长</h2>
<p>基于以上的发现，论文提出了StreamingLLM。StreamingLLM利用了attention
sink具有高注意力值的事实，认为保留它们可以保持注意力分数的分布接近正常。</p>
<p>因此，StreamingLLM的方案就是，在window
attention的基础上，增加一个策略，保留了attention sink
token的KV值，这些token的KV值会和滑动窗口内的token一起构成完整的KV
cache，用于进行新token的推理计算，StreamingLLM和其他方案的attention计算如下图所示。</p>
<img src="/45ee1a6d/streamingllm_compare.png" class title="StreamingLLM对比">
<p>这里有一个问题，要保留多少个初始token才足够。论文在几个不同的模型上做了实验，结果如下表所示。</p>
<img src="/45ee1a6d/stremingllm_init_token_num.png" class title="attention sink number">
<p>实验发现，使用4个初始token就可以基本把PPL降下来，而只使用一个或者两个初始token则仍有很大损失。</p>
<p>最终方案是，在window attention的基础上，StreamingLLM把KV
cache分成两部分：<br>
- 包含4个初始token的attention sink<br>
- 正常滚动的KV cache</p>
<p>StreamingLLM的KV缓存如下所示</p>
<img src="/45ee1a6d/stremingllm_kv_cache.png" class title="cache">
<p>一个重要的点是，StreamingLLM在实际应用上，位置编码中的相对位置，不再直接使用原文中的distance，而是改成使用这些token在cache中的distance。对于像RoPE这样的位置编码，需要缓存引入旋转变换之前token的KV值，解码时再把这些token在缓存内的相对位置加上。这样做使得模型不用处理大于预训练窗口大小的位置编码，而保证了效果。（关于这一点，在下面一篇论文《LM-Infinite》有相关实验）</p>
<p>当前的大模型基本都没有针对attention
sink的现象而做针对性的设计，论文提出可以在训练的时候增加一个特殊的token作为attention
sink token使用，方便模型把多余的注意力值放在这个特殊token上。</p>
<p>如果不增加一个特殊的token，那另外一个方法就是使用softmax的变体，softmax-off-by-one，替换attention中的softmax。</p>
<p>softmax-off-by-one的公式如下所示</p>
<p><span class="math display">\[\text{SoftMax}_1(x)_i=\frac{e^{x_i}}{1+\sum_{j=1}^Ne^{x_j}}\]</span></p>
<p>softmax-off-by-one在分母增加了一个常数1，这样就不要求所有attention
score的和为1。这相当于有一个K和V全都是0的虚拟token，固定和所有其他token的注意力得分为1，这个虚拟token在一定程度上也起到了attention
sink的作用。</p>
<p>为了验证增加特殊token作为attention
sink，以及使用softmax-off-by-one的方案效果，论文在相同设置下训练了3个160M参数的模型，效果如下</p>
<img src="/45ee1a6d/stremingllm_exp.png" class title="实验">
<p>从结果上看，即使使用softmax-off-by-one（zero
sink），模型还是会依赖前几个token作为attention sink。</p>
<p>最后，论文在LLAMA2, MPT,
Falcon和Pythia模型验证StreamingLLM在4M长度上的效果。MPT使用的位置编码是ALIBI，其他模型则是RoPE。效果如下图</p>
<img src="/45ee1a6d/stremingllm_perf_4m.png" class title="效果">
<p>在4M的长度上，StreamingLLM仍然能保持PPL较为稳定。</p>
<p>理论上StreamingLLM可以把窗口推到无限长，但是由于本质上还是window
attention，因此超过cache大小的部分还是会被丢弃。因此StreamingLLM虽然能保持PPL较低，但是对于需要高精度阅读理解和推理的任务，StreamingLLM还是有局限。</p>
<h1 id="lm-infinite">LM-Infinite</h1>
<p>论文：LM-Infinite: Zero-Shot Extreme Length Generalization for Large
Language Models</p>
<p>时间：2023年8月</p>
<p>长度：在2k或者4k长度上训练，在200M长度上推理保持较低PPL</p>
<p>阶段：无需训练，直接应用在推理阶段</p>
<p>LM-Infinite可以和现有的LLM结合使用，在Passkey
Retrieval和Qasper任务上有比较好的效果，且和原始模型相比，在解码速度上有2.7倍提升，显存上节省7.5倍。</p>
<h2 id="三个挑战">三个挑战</h2>
<p>LM-Infinite认为，Transformer
LLM无法有效地泛化到长上下文的情况，主要是因为会面临3个挑战。</p>
<ul>
<li>挑战1：challenges in handling unseen distances among tokens</li>
</ul>
<p>对于使用相对位置编码的模型，两个token的位置对注意力值的影响取决于两个token之间的距离。当推理的长度越来越长，就会出现在训练时没有见过的相对距离，此时attention
logits就会倾向于爆炸增长到无穷大，以区分在训练时从未见过的距离。论文对此给出了数学证明。</p>
<p>而从实践上来看，把LLAMA2所有注意力头在ArXiv数据集上8k以内的attention
logits抽取出来，其均值和方差如下图所示。</p>
<img src="/45ee1a6d/lm_infinite_attention_logits_explode.png" class title="logits爆炸">
<p>LLAMA2的预训练窗口大小为4k，可以看到4k之后，attention
logits的均值和方差开始有明显的上升。</p>
<p>那么为了缓解这个问题，一个自然的想法是将token之间的相对距离值限制在模型预训练期间看到过的最大值，即设置一个距离上限。这样可以解决logits爆炸，但是会导致下面这个问题。</p>
<ul>
<li>挑战2：attending to unseen numbers of tokens</li>
</ul>
<p>对于较长的上下文，位置靠后的token需要在更大的上下文长度范围内，分配它的注意力权重。论文提出，如果注意力logits有界，那么随着上下文长度变长，注意力熵(attention
entropy)将增长至无穷大。这里原文也给出了数学证明。</p>
<p>直观来说，在注意力权重分配为平均分配的时候，注意力熵最大。而注意力熵增大，表示当前的token不能确定应该将主要注意力放在上文的哪些token，即无法集中注意力。</p>
<p>同样地，我们把模型在8k内的注意力熵画出来，结果如下图所示，随着长度增大，注意力熵也逐渐增大。</p>
<img src="/45ee1a6d/lm_infinite_attention_entropy.png" class title="注意力熵爆炸">
<p>这表明我们应该限制attention context
size，以确保注意力熵保持在预训练见过的范围内。</p>
<p>如果简单地使用一个window
attention，使得每个token只能关注到一定距离内的token，在一定程度上可以一定程度上handle挑战1和挑战2。这样的思路和XPos、Longformer类似。</p>
<p>但是使用window attention，又会引入新的问题。</p>
<ul>
<li>挑战3：starting tokens occupy a distinct feature space</li>
</ul>
<p>即使没有显式的绝对位置编码，位置最靠前的几个初始token的注意力输出和其他位置相比，也会占据一个独特的表示空间。</p>
<p>这一结论来自于《The impact of positional encoding on length
generalization in
transformers》中的定理1，该定理证明了即使没有使用位置编码，单向注意力使得单个注意力层中的token输出也可以隐式地编码绝对位置，而初始token的信号是最强的，且很容易和其他token区分开来。</p>
<p>这个观察和StreamingLLM中指出的attention
sink很相似，总之就是最靠前的几个初始token有很大的影响，因此使用window
attention把初始token忽略会带来很大的效果损失。</p>
<p>论文把LLAMA2中各层的hidden state
output做了PCA，降到2维之后画出来，如下图所示</p>
<img src="/45ee1a6d/lm_infinite_starting_tokens.png" class title="初始token">
<p>蓝色的点表示初始token，红色的点表示靠后的token。上图显示了初始token的特征在特征空间中占据了和其他token很不同的区域。这就解释了为什么简单地将注意力窗口限制在最近的token上会效果不好：由于注意力本质上是加权平均，丢弃初始的几个token影响了注意力输出到达它们应占据的空间。</p>
<p>因此，我们需要保留初始token。</p>
<h2 id="解决方案">解决方案</h2>
<p>基于上面的这些观察和发现，论文提出LM-Infinite，给LLM做zero-shot的长度泛化。LM-Infinite设计了两个主要机制，Λ-shaped
attention mask和Distance ceiling。</p>
<ul>
<li>Λ-shaped attention mask</li>
</ul>
<p>Λ-shaped attention mask在概念上和Longformer、LongNet、Big
Bird的思路类似。</p>
<p>Λ-shaped attention mask在window
attention的基础上，保持了初始token对后续所有token可见。具体来说，每个token能关注到两部分：（1）<span class="math inline">\(n_{\mathrm{starting}}\)</span>
个初始token（2）<span class="math inline">\(L_{\mathrm{pretrain}}\)</span>
个最近的token。其中<span class="math inline">\(n_{\mathrm{starting}}\)</span> 为超参，<span class="math inline">\(L_{\mathrm{pretrain}}\)</span>
是预训练时的最大窗口大小。除了这两个部分的token之外，其他token都会被直接忽略。</p>
<p>论文对 <span class="math inline">\(n_{\mathrm{starting}}\)</span>
的数量进行了实验，发现 <span class="math inline">\(n_{\mathrm{starting}}\in[5,100]\)</span>
时的效果比较好，如下图所示。</p>
<img src="/45ee1a6d/lm_infinite_starting_tokens_num.png" class title="初始token数量实验">
<p>注意这里不需要对模型进行任何训练和微调，只需要修改推理的方式即可。</p>
<p>Λ-shaped attention mask可以解决挑战2和挑战3。</p>
<ul>
<li>Distance ceiling</li>
</ul>
<p>LM-Infinite把最大距离限制在 <span class="math inline">\(L_{\mathrm{pretrain}}\)</span>，这样做基本只影响初始token的位置编码。</p>
<p>具体来说，对于相对位置编码，假设原来的attention logit是 <span class="math inline">\(w(\mathbf{q},\mathbf{k},d)\)</span>，其中 <span class="math inline">\(d\)</span> 是两个token之间的距离；那么Distance
ceiling就是把attention logit的计算变成：</p>
<p><span class="math display">\[\text{attention
logits}=w(\mathbf{q},\mathbf{k},d&#39;)\]</span></p>
<p><span class="math display">\[d&#39;=\min(d,L_\text{pretrain})\]</span></p>
<p>Distance ceiling限制了最大距离，可以解决挑战1。</p>
<ul>
<li>Optionally attending to top-k tokens in the middle</li>
</ul>
<p>除了Λ-shaped attention mask和Distance
ceiling以外，论文还设计了一个可选的机制，用于把“除了当前窗口内的最近token和初始token”以外的中间部分token也加入到attention计算中，这部分token本来是会被直接丢弃的。</p>
<p>具体来说，会从原本被抛弃的中间token里，选择 <span class="math inline">\(k\)</span> 个attention
logits最大的token加入到attention 计算中来。这 <span class="math inline">\(k\)</span> 个token的距离都会被统一设置为 <span class="math inline">\(d=\frac12L_\text{pre-train}\)</span>。</p>
<p>对 <span class="math inline">\(k\)</span> 的取值，论文在Passkey
Retrieval的validation set上做了实验，结果如下</p>
<img src="/45ee1a6d/lm_infinite_middle_k.png" class title="k的数量">
<p>最终取了 <span class="math inline">\(k=5\)</span>，并且只在模型&gt;5的层中加入。</p>
<p>加入middle
token主要用于优化需要精准获取中间信息的任务，对于没有这个需要的任务，加入过多的middle
token不仅无益，甚至有害，原因如前面挑战2所分析的，会增大注意力熵。</p>
<p>最终，LM-Infinite的整体方案如下图所示</p>
<img src="/45ee1a6d/lm_infinite_design.png" class title="设计">
<h2 id="实验">实验</h2>
<p>论文在LLaMA-7B、LLaMA2-7B、MPT-7B和GPT-J-6B这几个模型验证LM-Infinite的效果。除了MPT-7B使用Alibi之外，其他模型都是用RoPE。</p>
<ul>
<li>Language Modeling</li>
</ul>
<p>首先是语言建模的评测，使用的数据集是ArXiv和OpenWebText2。</p>
<p>选择的几个基线模型，以及这些模型使用LM-Infinite的推理策略之后，在0-12k长度上的PPL如下图所示</p>
<img src="/45ee1a6d/lm_infinite_ppl_figure.png" class title="PPL">
<p>LLAMA2在略长于10K的长度上就输出了NaN，因此其曲线较短。所有原始模型在约32K的时候就出现OOM。</p>
<p>当测试长度超过训练长度时，基线模型的PPL会迅速爆炸。而使用LM-Infinite之后，所有模型都可以泛化到比训练长度长得多，而仍然保持较低PPL。</p>
<p>为了验证LM-Infinite在更大长度上的效果，用LM-Infinite +
Llama2，在由ArXiv数据重复抽样拼接构成的200M
token长度上的评估PPL。结果如下图所示，在200M长度下，依然可以保持稳定较低的PPL。</p>
<img src="/45ee1a6d/lm_infinite_ppl_200m.png" class title="PPL 200M">
<ul>
<li>Passkey Retrieval and Qapser</li>
</ul>
<p>Passkey
Retrieval和Qapser就是需要对整个上下文有精确理解的任务，因此对于这两个任务，加入了top-5个middle
token到attention计算中。</p>
<p>结果如下表。原始模型在长度超出训练长度之后，几乎无法输出有效内容，因此Passkey
Retrieval上全得0分。而LM-Infinite相比简单的长度裁剪方案，在两个任务上都有更好效果。</p>
<img src="/45ee1a6d/lm_infinite_downstream.png" class title="下游任务">
<ul>
<li>Ablation study</li>
</ul>
<p>对于LM-Infinite的两个主要设计，Λ-shaped attention mask和Distance
ceiling，论文做了消融实验，如下图所示。</p>
<img src="/45ee1a6d/lm_infinite_ablation.png" class title="消融实验">
<p>只使用Λ-shaped attention mask或者distance
ceiling单一策略的情况下，PPL仍然会出现较明显的上升，说明这两个组件都是必须的。</p>
<h1 id="transformer-xl">Transformer-XL</h1>
<p>讲下一篇Infini-Transformer之前，先简单回顾下Transformer-XL的思路。</p>
<p>Transformer-XL是2019年6月，由CMU和Google
Brain发布的一个工作，目的是赋予在固定长度上训练的模型，在更大长度上的推理能力。</p>
<p>一般来说，transformer的训练和推理如下图所示，训练的时候模型在固定的长度上训练，如果输入文本超过了模型的训练窗口大小，那就把输入切分成多个segment，模型在各个segment分别独立地进行训练；而推理的时候，如果文本超过了模型的推理窗口大小，模型可以使用滑动窗口的方式，逐步进行推理。</p>
<img src="/45ee1a6d/xl_vanilla_sw.png" class title="vanilla transformer">
<p>在窗口滑动的过程中，模型始终只能看到窗口内的内容，对于超出的部分则无法把信息纳入计算中，这显然对有长距离依赖的任务有损害。</p>
<p>为了让模型看得更远，Transformer-XL把attention计算修改成下面的样子：把输入切分成多个segment，每个的长度为L，对于两个相邻的segment
<span class="math inline">\([\mathbf{s}_{\tau}=x_{\tau,1},\cdots,x_{\tau,L}]\)</span>
和 <span class="math inline">\(\mathbf{s}_{\tau+1}=[x_{\tau+1,1},\cdots,x_{\tau+1,L}]\)</span>，计算如下</p>
<p><span class="math display">\[\begin{aligned}&amp;\widetilde{\mathbf{h}}_{\tau+1}^{n-1}=\left[\mathrm{SG}(\mathbf{h}_\tau^{n-1})\circ\mathbf{h}_{\tau+1}^{n-1}\right]\\&amp;\mathbf{q}_{\tau+1}^n,\mathbf{k}_{\tau+1}^n,\mathbf{v}_{\tau+1}^n=\mathbf{h}_{\tau+1}^{n-1}\mathbf{W}_q^\top,\widetilde{\mathbf{h}}_{\tau+1}^{n-1}\mathbf{W}_k^\top,\widetilde{\mathbf{h}}_{\tau+1}^{n-1}\mathbf{W}_v^\top\\&amp;\mathbf{h}_{\tau+1}^n=\text{Transformer-Layer}\left(\mathbf{q}_{\tau+1}^n,\mathbf{k}_{\tau+1}^n,\mathbf{v}_{\tau+1}^n\right)\end{aligned}\]</span></p>
<p>其中SG是stop
gradient的操作，这样在训练的时候能保持仍然只有固定窗口大小的参数进行梯度计算和更新，而不会扩大显存需求。<span class="math inline">\(\begin{bmatrix}\mathrm{SG}(\mathbf{h}_\tau^{n-1})\circ\mathbf{h}_{\tau+1}^{n-1}\end{bmatrix}\)</span>
表示在序列长度维度进行拼接。h、q、k、v的上标 <span class="math inline">\(n\)</span> 和 <span class="math inline">\(n-1\)</span> 是模型的层序号。</p>
<p>对于当前segment的每一层attention计算，Transformer-XL把上一个segment的信息加入到当前的K和V中再进行注意力计算，这样相当于每层相比上一层，都能往前多关注到一个segment长度的信息，模型的层数越多，模型最终能关注的长度越长，最后一个segment的最终输出依赖第一个segment的第一层输出，如下图所示。</p>
<img src="/45ee1a6d/xl_attention.png" class title="Transformer-XL">
<h1 id="infini-transformer">Infini-Transformer</h1>
<p>论文：Leave No Context Behind:Efficient Infinite Context Transformers
with Infini-attention</p>
<p>时间：2024年4月</p>
<p>长度：实验500k/1M，理论∞</p>
<p>阶段：继续预训练/微调</p>
<p>Google提出的一种叫Infini-attention的注意力计算方式，可以使用有限的显存计算无限长的上下文。</p>
<h2 id="背景-1">背景</h2>
<p>记忆能力是智能的基石，因为它使得针对特定上下文的高效计算成为可能。</p>
<p>但是transformer的注意力计算在时间和空间上都跟输入长度是二次的关系，这就使得在长上下文的场景下很难高效、准确地处理信息。</p>
<p>使用记忆模块可以在大大减少计算量和显存需求的前提下，依然保持长上下文完整信息对模型的可见性。理论上所有输入信息都可以保存在记忆模块中，因此模型具备处理“有长距离依赖，且需要对输入信息进行准确检索”的任务。从这点上看，和StreamingLLM相比，记忆模块的效果上限更高。</p>
<p>针对高效的记忆，目前也有一些相关工作，比如《Metalearned neural
memory》/《Enhancing the transformer with explicit relational encoding
for math problem solving》所使用的compressive
memory，可以用固定大小的记忆模块来编码context，但是现有的记忆模块在效果和设计的simplicity上仍然存在tradeoff，比较难平衡。</p>
<h2 id="infini-attention">Infini-attention</h2>
<p>基于此，Infini-Transformer主要的改进点就在记忆模块的设计上。Infini-attention的结构设计如下图所示，每个Transformer
block内包括了常规的mask local attention，和储存长期记忆的long term
linear attention。</p>
<img src="/45ee1a6d/infini_attention_structure.png" class title="infini-attention结构">
<p>Infini-attention机制复用了标准注意力计算中的所有QKV值，构建了固定大小的注意力模块，用于长期记忆。在处理后续序列时，根据当前的Q从记忆模块中检索相关内容。Infini-attention将长期记忆检索到的值与局部注意力上下文通过concat进行聚合，共同计算最终的输出。</p>
<p>这种对Transformer注意力层的微小但关键的修改，使得现有的LLM能够通过继续预训练和微调自然地扩展到无限长的上下文。</p>
<p>Infini-attention和Transformer-XL的对比如下图所示</p>
<img src="/45ee1a6d/infini_attention_process.png" class title="infini-attention计算方式">
<p>和Transformer-XL类似，Infini-Transformer也把输入分成多个segment来处理。不同的是，Transformer-XL每层只能比前一层多看到一个segment的信息，因此需要通过堆叠模型的层数来提升模型处理窗口的大小；而由于层数是有限的，最终窗口的大小还是有限的。</p>
<p>Infini-Transformer则是每层都有对之前所有segment的长期记忆，因此不受模型层数的影响，可以扩展到无限大的窗口。</p>
<p>compressive memory的具体设计，出于simplicity和computational
efficiency的考虑，论文参考《Learning associative inference using fast
weight memory》，把memory设计成一个associative matrix。</p>
<p>对于记忆模块，最重要的就是记忆的更新和检索。</p>
<ul>
<li>记忆检索</li>
</ul>
<p>先看下记忆的检索。假设每个segment的长度为 <span class="math inline">\(N\)</span>，我们使用当前的 <span class="math inline">\(Q\in\mathbf{R}^{N\times d_{key}}\)</span> 从memory
<span class="math inline">\(M_{s-1}\in\mathbf{R}^{d_{key}\times
d_{value}}\)</span> 中检索，计算如下</p>
<p><span class="math display">\[A_{\text{mеm}}=\frac{\sigma(Q)M_{s-1}}{\sigma(Q)z_{s-1}}\]</span></p>
<p>其中<span class="math inline">\(\sigma\)</span>是激活函数，使用的是element-wise
ELU + 1。</p>
<p><span class="math inline">\(z_{s-1}\in\mathbf{R}^{d_{key}}\)</span>是normalization
term。这里normalization term参考《Transformers are rnns: Fast
autoregressive transformers with linear
attention》的做法，使用所有K值的求和值。</p>
<ul>
<li>记忆更新</li>
</ul>
<p>完成检索之后，就要更新记忆。需要更新的有normalization
term和memory两项，更新方式如下。</p>
<p><span class="math display">\[M_s\leftarrow
M_{s-1}+\sigma(K)^TV\]</span></p>
<p><span class="math display">\[z_s\leftarrow
z_{s-1}+\sum_{t=1}^N\sigma(K_t)\]</span></p>
<p>memory的更新，参考《Metalearned neural memory》和《Learning
associative inference using fast weight memory》所使用的delta
rule，进一步优化成如下形式</p>
<p><span class="math display">\[M_s\leftarrow
M_{s-1}+\sigma(K)^T(V-\frac{\sigma(K)M_{s-1}}{\sigma(K)z_{s-1}})\]</span></p>
<ul>
<li>长期记忆和local attention结合</li>
</ul>
<p>回到当前segment的注意力计算中，需要把长期记忆检索项 <span class="math inline">\(A_{mem}\)</span> 和local attention state <span class="math inline">\(A_{dot}\)</span> 结合</p>
<p><span class="math display">\[A=sigmoid(\beta)\odot
A_{mem}+(1-sigmoid(\beta))\odot A_{dot}\]</span></p>
<p>其中 <span class="math inline">\(\beta\)</span>
是一个可学习的标量参数。</p>
<p>在训练中， <span class="math inline">\(\beta\)</span>
所学习到的值如下，0则表示这个头只关注当前segment，1则表示值关注长期记忆的内容，介于0和1之间的则是两种的混合，可以看到大部分的注意力头会使用二者的混合结果</p>
<img src="/45ee1a6d/infini_attention_gating.png" class title="gating分布">
<p>整体策略上，和其他包含segment-level
memory的模型在方案设计上的比较如下</p>
<img src="/45ee1a6d/infini_attention_compare.png" class title="方案比较">
<h2 id="实验-1">实验</h2>
<p>实验上，每个segment的长度 <span class="math inline">\(N\)</span>
设为2048，模型的训练窗口大小为32768，即总共有16个segment。</p>
<ul>
<li>语言建模能力</li>
</ul>
<p>Infini-Transformer和Transformer-XL/Memorzing
Transformer/RMT在PG-19和Arxiv数据集上的language modeling能力对比如下</p>
<img src="/45ee1a6d/infini_attention_language_modeling.png" class title="语言建模能力对比">
<p>Infini-Transformer的效果显著更好，而且所需的记忆空间只有Memorizing
Transformers的不到1%。</p>
<p>原文指出，把训练的数据长度提升到100k，Infini-Transformer的PPL还能进一步降低。</p>
<ul>
<li>下游任务</li>
</ul>
<p>用1B的模型进行轻量级的继续预训练，训练配置如下：<br>
- batch size = 64 - step = 30k - 训练数据长度 &gt; 4k - segment length =
2k</p>
<p>之后在1M长度的passkey
retrieval任务上评测，zero-shot和fine-tune模型在各个长度的效果如下表</p>
<img src="/45ee1a6d/infini_attention_passkey.png" class title="passkey效果">
<p>Infini-Transformer（linear + delta）在1M长度的passkey
retrieval任务上做到完全正确。</p>
<p>另外在8B模型上，用8k长度训练30k步，在500k的BookSum任务上评测，结果如下</p>
<img src="/45ee1a6d/infini_attention_booksum.png" class title="booksum效果">
<p>Infini-Transformer（linear + delta）相比其他模型有一定优势。</p>
<h1 id="小结">小结</h1>
<ol type="1">
<li>Streaming和LM-Infinite思路有些相似，都观察到attention
sink对模型效果的影响，并设计了相关的注意力计算机制保留初始token以稳定PPL，而LM-Infinite在这个基础上使用distance
ceiling，使得模型在推理时不会使用超出训练所用的token相对距离。虽然这两个工作使得模型可以在M级别的长度仍然保持较低PPL，但是实际大量中间token被丢弃，导致对需要在上下文精准检索的任务效果有损害。<br>
</li>
<li>Infini-Transformer使用记忆模块，把输入进行分段，并把靠前的内容通过固定大小的记忆矩阵进行压缩。理论上记忆模块可以提供所有上文的信息，能够应用在更精细和困难的任务。记忆模块的检索和更新的设计是方案的核心，需要考虑复杂度和效果的平衡。</li>
</ol>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】Efficient Streaming Language Models with Attention Sinks
https://arxiv.org/abs/2309.17453<br>
【2】Transformer-XL: Attentive Language Models Beyond a Fixed-Length
Context https://arxiv.org/abs/1901.02860<br>
【3】Leave No Context Behind: Efficient Infinite Context Transformers
with Infini-attention https://arxiv.org/abs/2404.07143<br>
【4】LM-Infinite: Zero-Shot Extreme Length Generalization for Large
Language Models https://arxiv.org/abs/2308.16137</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>长上下文</tag>
        <tag>微调</tag>
        <tag>无限大</tag>
        <tag>预训练</tag>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型算法题(6)</title>
    <url>/7c04944d.html</url>
    <content><![CDATA[<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a></p>
<hr>
<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<p>本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~</p>
<p>如有错漏，欢迎指正~</p>
<hr>
<h1 id="xavier初始化思路是什么是怎么做的">1.Xavier初始化思路是什么，是怎么做的？</h1>
<p>2010年的《Understanding the difficulty of training deep feedforward
neural
networks》提出Xavier初始化，目的是为了保持模型中每一层的输出方差大致相等，这样做的原因主要有：</p>
<p>（1）避免梯度消失或梯度爆炸：在深度神经网络中，如果层与层之间的输出方差相差很大，就会有很大的值出现，那么在反向传播过程中，梯度可能会变得非常小（梯度消失）或者非常大（梯度爆炸）导致梯度更新缓慢，或者训练过程不稳定。</p>
<p>（2）保持信号的传播：通过保持每一层的输出方差大致相等，可以确保网络中的信号在前向传播和反向传播时不会因为方差的变化而减弱或增强，从而有助于网络更好地学习和传递信息。</p>
<p>（3）提高训练的稳定性和效率：当每一层的输出方差保持一致时，训练过程会更加稳定，因为权重更新的magnitude更加可控。这样可以提高训练的效率。</p>
<p>（4）避免过拟合或欠拟合：适当的方差控制有助于网络在训练过程中保持适当的泛化能力。过大的方差可能导致过拟合，而过小的方差可能导致欠拟合。</p>
<p>Xavier初始化方法通过考虑前一层的节点数（fan-in）和后一层的节点数（fan-out）来设置初始权重的分布范围。有三种方案：</p>
<p>（1）只考虑输入，设置参数的方差为1/fan-in</p>
<p>（2）只考虑输出，设置参数的方差为1/fan-out</p>
<p>（3）同时考虑输入和输出，设置参数的方差为2/（fan-in +
fan-out）。实际使用中1/fan-in效果就较好。</p>
<h1 id="rlhf中reward模型和critic模型的作用分别是什么">2.RLHF中，Reward模型和Critic模型的作用分别是什么？</h1>
<p>Critic模型主要负责评估当前策略下的行为，并预测未来的回报。在PPO中，Critic模型的输出用于计算优势函数（advantage
function），这是一个衡量实际回报与预期回报之间差异的指标。优势函数是策略梯度方法中的关键组成部分，它帮助actor模型了解哪些行为比预期要好，哪些行为比预期要差。因此，Critic模型的评分直接影响actor模型的优化过程，指导其调整行为以提高整体性能。</p>
<p>Reward模型是基于人类反馈训练的，用于评估和打分模型生成的文本或行为的质量。在RLHF中，Reward模型的评分通常被用作奖励信号，直接反馈给Actor模型。这些奖励信号反映了人类对生成内容的偏好和评价标准，Actor模型会根据这些信号调整其行为，以生成更符合人类期望的输出。
RLHF中，Actor模型的优化目标是最大化期望回报。这个期望回报可以由Critic模型的预测和Reward模型的评分共同决定。具体来说，Actor模型的优化目标（objective）通常包括（1）策略损失：这部分损失来自于策略模型尝试最大化奖励信号（由Reward模型提供）和/或优势函数（由Critic模型提供）。（2）价值损失：在Actor-Critic架构中，Actor模型也负责优化Critic模型，那么价值损失会尝试最小化Critic模型预测的价值与实际回报之间的差异。（3）其他正则化项：可能还包括一些正则化项。</p>
<p>因此，Critic模型的评分用于计算优势函数，而Reward模型的评分直接作为奖励信号。这两个模型共同作用于Actor模型，帮助其学习如何生成更符合人类偏好和期望的行为。</p>
<h1 id="kaiming初始化是怎么做的">3.Kaiming初始化是怎么做的？</h1>
<p>Kaiming初始化在Xavier初始化的基础上做了调整。Xavier初始化的推导是基于线性函数的，但实际上今天所有模型都会大量使用非线性函数，比如ReLU，这样就导致了Xavier初始化的失效。
Kaiming初始化针对使用ReLU的模型提出。因为ReLU会抛弃掉小于0的值，对于一个均值为0的输入数据，这就相当于砍掉了一半的值，这样输出的方差和均值就变了。因此把Xavier初始化中使用的方差sqrt(1/N)改为sqrt(2/N)，这样数据不会因为ReLU而变得越来越小。</p>
<h1 id="使用bert中的cls作为输出进行分类和相似度计算可能会有什么问题">4.使用Bert中的[CLS]作为输出进行分类和相似度计算，可能会有什么问题？</h1>
<p>self-attention中，每个token都天然倾向于关注自己和附近的token，而对更远的内容分配较少的注意力。在Bert中，[CLS]一般是第一个token，放在最前面。这就导致[CLS]更容易关注到靠近它的输入，也就是文本的靠前部分。如果文本较长，且关键内容出现在靠后的位置，就有可能出现[CLS]中关注不够的情况，而导致效果下降。可以通过强制注意力分配，或者使用所有位置的输出而不单是[CLS]token的方法来缓解这个问题。</p>
<h1 id="pytorch中的register_buffer是什么有什么用">5.pytorch中的register_buffer是什么，有什么用？</h1>
<p>register_buffer是nn.Module类中的一个方法，它用于注册一个不需要梯度的缓冲区。一般会把模型中需要持续使用或者跟踪，但是不需要通过梯度来更新的参数使用register_buffer注册。使用register_buffer注册的参数可以和其他训练的参数一样，保存在state_dict中，比如旋转位置编码的旋转矩阵，batchnorm中的全局均值和方差，一般都会用register_buffer注册。如果不使用register_buffer注册，普通的常量或者参数保存的时候不会被state_dict跟踪到。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a></p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>算法题</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型推理加速-投机解码</title>
    <url>/f5c015c.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>大语言模型虽然效果很好，但是推理时，朴素的自回归解码策略需要逐个串行解码，耗时较长，这给用户的耐心带来了很大挑战。如今各家大模型提供商基本都有对外提供大模型的体验平台，而模型的推理效率自然也成了一个重要的竞争点。</p>
<p>speculative
decoding，译作投机解码，就是推理加速的一个比较巧妙的方案。本篇将介绍投机解码的基础思路。</p>
<h1 id="背景">背景</h1>
<p>2022年11月，Google在《Fast Inference from Transformers via
Speculative
Decoding》里提出投机解码的策略；DeepMind稍晚一点，在2023年初的《Accelerating
Large Language Model Decoding with Speculative
Sampling》也提出了一样的解码策略。（以这两家的关系，很可能私底下就沟通过这个idea了）Google的论文相比DeepMind的，做了更多的实验和分析，更为详尽一些。</p>
<p>在speculative
decoding之前，研究人员已经在模型推理加速这个方向做了不少工作：<br>
- 模型蒸馏：以Hinton的《Distilling the Knowledge in a Neural
Network》为代表，以及后面衍生出的各种蒸馏方法（参考《Knowledge
Distillation: A
Survey》），可以把规模更大的、性能更强的模型的能力，部分迁移到规模较小的模型上，在效果上相比直接训练小模型有一定的提升。transformer上蒸馏相关的经典工作有《TinyBERT:
Distilling BERT for Natural Language Understanding》和《DistilBERT, a
distilled version of BERT: smaller, faster, cheaper and
lighter》等。<br>
- 模型量化：如《Quantized Neural Networks: Training Neural Networks with
Low Precision Weights and Activations》、《LLM.int8(): 8-bit Matrix
Multiplication for Transformers at Scale》、《Zeroquant: Efficient and
affordable post-training quantization for large-scale
transformers》等，把模型参数量化到int8、int4以及更低的精度，在减少空间需求的同时，最大化地保持模型的推理效果。<br>
- 高效模型结构设计：如使用稀疏层的《Sparse is Enough in Scaling
Transformers》，减少KV缓存需求的MQA《Fast Transformer Decoding: One
Write-Head is All You Need》、GQA《《GQA: Training Generalized
Multi-Query Transformer Models from Multi-Head
Checkpoints》》以及最近DeepSeek-V2中的MLA等，还有通过进化算法进行高效架构搜索的工作《Primer:
Searching for Efficient Transformers for Language Modeling》。</p>
<p>以上这些做法对不同的输入一视同仁，采用一个全局来看有收益的方案来统一处理，达到推理加速的目的。</p>
<p>相对地，也有一些其他的方案，认为不是每一步推理都适合一样处理：某些推理step需要大模型，而另一些step只需要高效的小模型，从而根据输入，动态地决定模型参与计算的参数，相关工作有：<br>
- 《Dynamic Neural Networks: A Survey》<br>
- 《Adaptive Attention Span in Transformers》<br>
- 《Consistent Accelerated Inference via Confident Adaptive
Transformers》<br>
- 《Why should we add early exits to neural networks?》<br>
- 《Controlling Computation versus Quality for Neural Sequence
Models》<br>
- 《The Right Tool for the Job: Matching Model and Instance
Complexities》<br>
- 《Depth-Adaptive Transformer》<br>
- 等</p>
<p>MoE也属于动态激活的方案之一。</p>
<p>而《Training compute-optimal large language models》的scaling
law则指出模型规模没有原先预想的影响那么大，可以通过增加训练数据等方法让小模型逼近大模型的效果。</p>
<p>以上这些方案虽然可以在一定程度上提升推理效率，但是要么需要重新训练模型，要么对模型的效果有损害。</p>
<p>也有一些方案在解码的方法上进行优化，比如《Blockwise Parallel Decoding
for Deep Autoregressive Models》和《Lossless Acceleration for Seq2seq
Generation with Aggressive Decoding》。</p>
<p>speculative
decoding也是一个在解码策略上进行优化的方法。投机解码可以在不用训练原模型的基础上，提升2x-3x的推理速度，并且保证结果和原模型完全一致，没有任何效果损失。</p>
<h1 id="speculative-decoding算法">speculative decoding算法</h1>
<p>回想一下，自回归语言模型在训练的时候，在每一个位置，会根据当前及前面所有的token，预测下一个token。由于强制学习的特性，所有token可以一起训练。在某种特别的情况下，模型对当前的输入拟合得特别好，就有可能出现每个token的预测，都完美命中下一个输入token的情况。举个例子：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">位置：一  二  三  四</span><br><span class="line">输入：我  爱  中  国</span><br><span class="line">输出：爱  中  国  EOS</span><br></pre></td></tr></table></figure>
<p>而在推理的时候，这种依赖前面所有token的特性，使得自回归模型只能一个一个串行地解码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">step1：输入“我”，输出“爱”；</span><br><span class="line">step2：输入“我爱”，输出“中”；</span><br><span class="line">step3：输入“我爱中”，输出“国”；</span><br><span class="line">step4：输入“我爱中国”，输出“EOS”；</span><br></pre></td></tr></table></figure>
<p>现在，假设我们有一个神奇海螺，你只要输入“我”，就会输出“爱 中 国
EOS”四个token作为草稿，我们就可以拿着这四个draft
token一起放到原来的模型，跑一下各个位置的输出，进行验证，跟训练时的前向推理一样：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">位置：一  二  三  四</span><br><span class="line">输入：我  爱  中  国</span><br><span class="line">输出：爱  中  国  EOS</span><br></pre></td></tr></table></figure>
<p>然后就会发现模型的输出和神奇海螺给出的草稿完全一致，那就相当于我们只进行了一次模型推理，就解码了四个token，并且和原模型的效果完全一致。并且一般情况下，模型对一个位置进行预测和对四个位置进行预测的耗时基本没有太大的差异，也就是说在这个例子下，模型解码速度提升到了将近4倍。</p>
<p>当然，神奇海螺不会总是能够给出和模型一模一样的结果，除非它就是模型本身。因此，在上面这个例子中，输入“我”之后，神奇海螺有可能给出的是“爱
中 华 EOS”这四个draft
token。这种情况下，我们把这些token一起输入到模型进行验证</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">位置：一  二  三  四</span><br><span class="line">输入：我  爱  中  华</span><br><span class="line">输出：爱  中  国  EOS</span><br></pre></td></tr></table></figure>
<p>会发现神奇海螺给出的“爱”和“中”命中了模型的结果，但是“华”没对上。不过这种情况下，跑一次模型推理也能解码出两个token，推理效率依然有提升。</p>
<p>部分情况下，神奇海螺给出的结果也可能完全跑偏，比如给它输入“我”，它有可能输出“叫
小
明”，这就和原模型一个都没对上。但是只要统计上，神奇海螺给出的草稿平均命中token数
&gt; 0，我们就有机会获得推理加速。</p>
<p>使用神奇海螺的这个思路其实就是speculative
decoding的主要思路，而你肯定也已经猜到了，神奇海螺其实就是一个规模比较小的模型，论文中把它称为approximation
model或者draft model，而我们想要加速的原模型则叫target model。</p>
<p>论文给出的一个例子如下</p>
<img src="/f5c015c/fi_example.png" class title="例子">
<p>绿色的就是approximation model给出并命中target
model验证结果的token，红色的是错误的token，蓝色则是修正后的token。</p>
<p>在这个例子中，target模型只推理了9次，就解码出了38个token，推理速度获得了较大提升。</p>
<p>看完了例子，现在对投机解码算法给出正式的描述。</p>
<p><span class="math inline">\(M_p\)</span> 是target model， <span class="math inline">\(M_q\)</span> 是approximation
model，prefix是当前的输入。</p>
<p>首先 <span class="math inline">\(M_q\)</span> 给出 <span class="math inline">\(\gamma\)</span> 个draft token，然后 <span class="math inline">\(M_p\)</span> 并行地对这 <span class="math inline">\(\gamma\)</span> 个draft
token进行验证，根据验证结果，按顺序把通过验证的token加入到当前序列中；如果出现被
<span class="math inline">\(M_p\)</span>
拒绝的token，这些token则按规则重新抽样。</p>
<p>Google论文给出的投机解码算法描述如下图。</p>
<img src="/f5c015c/fi_sd_algo.png" class title="投机解码算法">
<p>（DeepMind版本的算法描述在下面）</p>
<p>这里注意，投机解码单次运行能解码的token数量，除了这 <span class="math inline">\(n\)</span> 个被接受的draft token，还有 <span class="math inline">\(M_p\)</span>
对这些草稿进行验证时顺便推理出来的一个额外token，因此最终可以得到 <span class="math inline">\(n+1\)</span> 个token。因此如果approximation
model每次给出 <span class="math inline">\(\gamma\)</span> 个draft
token，理论上最多可以获得 <span class="math inline">\(\gamma+1\)</span>
新解码token，而最少也能有1个（来自target模型）。</p>
<p>投机解码的原理大致就是这样，思路还是很巧妙的，但是要实际应用还有几个问题需要解决，比如：<br>
- 关于投机采样speculative sampling：target model怎么对approximation
model给出的token进行验证？在一个draft
token被拒绝之后，怎么重新采样？<br>
- 怎么选择 <span class="math inline">\(\gamma\)</span> 才合理？<br>
- 怎么选择approximation model，用什么指标表征approximation
model的质量？</p>
<p>另外，DeepMind论文的给出投机解码算法如下，可以对照Google的算法，方便理解。（DeepMind所用的符号有所不同，本篇采用Google论文的符号描述。）</p>
<img src="/f5c015c/acce_alog.png" class title="DeepMind投机解码算法">
<p>里面的 <span class="math inline">\((.)_+\)</span> 操作表示 <span class="math inline">\((f(x))_+=\frac{\max(0,f(x))}{\sum_x\max(0,f(x))}\)</span>
。</p>
<h1 id="speculative-sampling的正确性">speculative sampling的正确性</h1>
<p>我们希望投机解码的最终结果，和target
model自回归解码的结果一致，即完全无损，因此需要对投机采样做一些设计和分析。</p>
<p>首先，当前在transformer的解码上已经有很多策略，包括但不限于argmax、top-k采样、使用温度等。而大部分操作都是在logits上进行操作，这相当于改变了模型的输出分布。而在最终分布上的采样操作，都是相同的。因此我们可以只在朴素的标准采样上进行分析，而结果可以推广到其他的解码策略上。</p>
<p>假设 <span class="math inline">\(p(x)\)</span> 是target model <span class="math inline">\(M_p\)</span> 在当前输入下的分布， <span class="math inline">\(q(x)\)</span> 是approximation model <span class="math inline">\(M_q\)</span> 在当前输入下的分布。</p>
<p>投机解码的做法是，先采样 <span class="math inline">\(x\sim
q(x)\)</span>，如果 <span class="math inline">\(q(x)\leq
p(x)\)</span>，就保留 <span class="math inline">\(x\)</span>，否则就以
<span class="math inline">\(1-\frac{p(x)}{q(x)}\)</span> 的概率拒绝
<span class="math inline">\(x\)</span>，并在分布 <span class="math inline">\(p&#39;(x)=norm(max(0,p(x)-q(x)))\)</span>
对被拒绝的 <span class="math inline">\(x\)</span>
重新采样，并结束当前的投机解码。</p>
<p>其中 <span class="math inline">\(norm(max(0,p(x)-q(x)))=\frac{\max(0,p(x)-q(x))}{\sum_x\max(0,p(x)-q(x))}\)</span>
。</p>
<p>看起来并不复杂。一个问题是，为什么这样从 <span class="math inline">\(q(x)\)</span> 采样之后，我们得到的结果符合分布
<span class="math inline">\(p(x)\)</span>？即按这样的概率进行拒绝之后，结果和target
model自己解码一样？</p>
<p>从公式上来说，approximation model的抽样有 <span class="math inline">\(\tilde{x}\sim q\)</span>。假设 <span class="math inline">\(X\)</span> 是最终结果，我们的目标就是证明 <span class="math inline">\(\mathbb{P}(X=x)=p(x)\)</span>。</p>
<p>而要使得 <span class="math inline">\(X=x\)</span>，只有 <span class="math inline">\(\tilde{x}=x\)</span> 且 <span class="math inline">\(\tilde{x}\)</span> 被接受，或者在 <span class="math inline">\(\tilde{x}\)</span> 被拒绝之后重新采样到 <span class="math inline">\(\tilde{x}=x\)</span> 两种情况，即有</p>
<p><span class="math display">\[\mathbb{P}(X=x)\\=\mathbb{P}(\tilde{x}=x)\mathbb{P}(\tilde{x}\textit{
accepted}|\tilde{x}=x)\\+\mathbb{P}(\tilde{x}\textit{
rejected})\mathbb{P}(X=x|\tilde{x}\textit{ rejected})\]</span></p>
<p>对于第一项，有</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathbb{P}(\tilde{x}=x)\mathbb{P}(\tilde{x}\text{
ассерґе}d|\tilde{x}=x)\\=&amp;q(x)\min\left(1,\frac{p(x)}{q(x)}\right)\\=&amp;\min\left(q(x),p(x)\right)
\end{aligned}
\]</span></p>
<p>而第二项里</p>
<p><span class="math display">\[\begin{gathered}
\mathbb{P}(\tilde{x}\textit{ rejected})=1-\mathbb{P}(\tilde{x}\textit{
accepted}) \\
=1-\sum_{x^{\prime}}\mathbb{P}(X=x^{\prime},\tilde{x}\text{ ассерґе}d)
\\
=1-\sum_{x&#39;}\min(q(x&#39;),p(x&#39;)) \\
=\sum_{x&#39;}\max(0,p(x&#39;)-q(x&#39;)) \\
\end{gathered}\]</span></p>
<p>上式第三行到第四行的解释：第三行相当于计算1减区域b的面积，而区域a+区域b的面积和为1，因此第三行相当于区域a的面积，即
<span class="math inline">\(\sum_{x&#39;}\max(0,p(x&#39;)-q(x&#39;))\)</span>。</p>
<img src="/f5c015c/formula.png" class title="图解">
<p>从采样规则，有</p>
<p><span class="math display">\[\mathbb{P}(X=x|\tilde{x}\text{
rejected})=\frac{\max(0,p(x)-q(x))}{\sum_x\max(0,p(x)-q(x))}\]</span></p>
<p>因此</p>
<p><span class="math display">\[\mathbb{P}(\tilde{x}\text{
rejected})\mathbb{P}(X=x|\tilde{x}\text{
rejected})=\max(0,p(x)-q(x))\]</span></p>
<p>最终有</p>
<p><span class="math display">\[\mathbb{P}(X=x)\\=\min(q(x),p(x))+\max(0,p(x)-q(x))\\=p(x)\]</span></p>
<p>因此按照前面设计的规则进行采样，就能保证结果和target
model自己解码出来的一样。</p>
<h1 id="approximation-model的评估">approximation model的评估</h1>
<p>approximation model的一个采样 <span class="math inline">\(x\sim
q(x)\)</span> 被target model接受的概率为 <span class="math inline">\(\beta\)</span>，我们把这个概率叫acceptance
rate接受率。</p>
<p>那么其期望值 <span class="math inline">\(E(\beta)\)</span>
就是approximation model对target model拟合质量一个很好的评估指标。</p>
<p><span class="math inline">\(E(\beta)\)</span>
越大，每个token被接受的概率越大，那么每次投机解码能获得的输出token越多。</p>
<p>我们令 <span class="math inline">\(\alpha=E(\beta)\)</span>，并且为简化起见，假设
<span class="math inline">\(\beta\)</span>
的分布是i.i.d.的，那么跑一次投机解码能够获得的token数量是一个capped
geometric variable，其期望值如下式</p>
<p><span class="math display">\[E(\#\textit{ generated
tokens})=\frac{1-\alpha^{\gamma+1}}{1-\alpha}\]</span></p>
<p>不同 <span class="math inline">\(\gamma\)</span> 下的图像如下</p>
<img src="/f5c015c/fi_expected_token_num.png" class title="解码数量期望值">
<p>而 <span class="math inline">\(\alpha\)</span> 是可以推算的。</p>
<p>首先定义一个 <span class="math inline">\(M_p\)</span> 和 <span class="math inline">\(M_q\)</span> 之间的divergence <span class="math inline">\(D_{LK}\)</span></p>
<p><span class="math display">\[\begin{aligned}D_{LK}(p,q)=\sum_x|p(x)-M(x)|=\sum_x|q(x)-M(x)|\end{aligned}\]</span></p>
<p>其中 <span class="math inline">\(M(x)=\frac{p(x)+q(x)}2\)</span>。</p>
<p>而</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\sum_x|p(x)-M(x)|\\=&amp;\sum_x\frac{|p-q|}{2}\\=&amp;1-\sum_x\frac{p+q-|p-q|}2\\=&amp;1-\sum_x\min(p(x),q(x))
\end{aligned}
\]</span></p>
<p>因此有</p>
<p><span class="math display">\[D_{LK}(p,q)=1-\sum_x\min(p(x),q(x))\]</span></p>
<p><span class="math inline">\(D_{LK}(p,q)\)</span>越小，则 <span class="math inline">\(M_p\)</span> 和 <span class="math inline">\(M_q\)</span> 越相近。如果 <span class="math inline">\(D_{LK}(p,q)=0\)</span>，说明 <span class="math inline">\(p=q\)</span>；如果 <span class="math inline">\(D_{LK}(p,q)=1\)</span>，说明 <span class="math inline">\(p\)</span> 和 <span class="math inline">\(q\)</span> 两个分布完全没有交叉的部分。</p>
<p>根据 <span class="math inline">\(\beta\)</span> 的定义，有</p>
<p><span class="math display">\[
\begin{aligned}
\beta=&amp;E_{x\sim q(x)}\begin{cases}1&amp;q(x)\leq
p(x)\\\frac{p(x)}{q(x)}&amp;q(x)&gt;p(x)\end{cases}\\
=&amp;E_{x\thicksim q(x)}\min(1,\frac{p(x)}{q(x)})\\
=&amp;\sum_x\min(p(x),q(x))\\
=&amp;1-D_{LK}(p,q)
\end{aligned}
\]</span></p>
<p>最终得到</p>
<p><span class="math display">\[\alpha=E(\beta)=1-E(D_{LK}(p,q))=E(\min(p,q))\]</span></p>
<p>实验中，不同approximation model和target model之间测得的 <span class="math inline">\(\alpha\)</span> 值如下表所示</p>
<img src="/f5c015c/fi_alpha.png" class title="各种模型的alpha">
<h1 id="耗时优化的分析">耗时优化的分析</h1>
<p>定义cost coefficient <span class="math inline">\(c\)</span>，表示
<span class="math inline">\(M_q\)</span> 单次推理 和 <span class="math inline">\(M_p\)</span> 单次推理的比值。</p>
<p>和仅与模型相关的 <span class="math inline">\(\alpha\)</span>
不同，<span class="math inline">\(c\)</span>
的具体值会受到硬件、推理框架等影响。在论文的实验中 <span class="math inline">\(c\)</span> 的值大部分小于0.05。</p>
<p>假设 <span class="math inline">\(M_p\)</span> 每次推理所需的时间为
<span class="math inline">\(T\)</span>，则一次投机解码所需的时间为 <span class="math inline">\(Tc\gamma+T\)</span>。</p>
<p>根据前面的推算，投机解码每次能获得的token数为 <span class="math inline">\(E(\#\textit{ generated
tokens})=\frac{1-\alpha^{\gamma+1}}{1-\alpha}\)</span>
个，因此每个token所需的时间为 <span class="math inline">\(\frac{(c\gamma+1)(1-\alpha)}{1-\alpha^{\gamma+1}}T\)</span>。综上，使用投机解码在推理时间上的improvement
factor为</p>
<p><span class="math display">\[\frac{1-\alpha^{\gamma+1}}{(1-\alpha)(\gamma
c+1)}\]</span></p>
<p>只要 <span class="math inline">\(\alpha&gt;c\)</span>，就一定存在能提升解码效率的
<span class="math inline">\(\gamma\)</span>，并且improvement
factor至少为 <span class="math inline">\(\frac{1+\alpha}{1+c}\)</span>（<span class="math inline">\(\gamma=1\)</span>时）。</p>
<h1 id="计算成本的分析">计算成本的分析</h1>
<p><span class="math inline">\(M_p\)</span> 同时对 <span class="math inline">\(\gamma+1\)</span>
个token进行验证。如果一个token被接受了，那么推理效率就获得了提升；如果token被拒绝了，那么相关的计算就没有实际收益，就会有计算的“浪费”。</p>
<p>假设 <span class="math inline">\(\hat{c}\)</span> 是 <span class="math inline">\(M_q\)</span> 和 <span class="math inline">\(M_p\)</span> 计算一个token的arithmetic
operations的比例，<span class="math inline">\(\hat{T}\)</span> 是 <span class="math inline">\(M_p\)</span> 解码一个token所需的arithmetic
operations。</p>
<p>那么一次投机解码的计算量就是 <span class="math inline">\(\hat{T}\hat{c}\gamma+\hat{T}(\gamma+1)\)</span>，这个计算量除以投机解码每次获得的token数
<span class="math inline">\(\frac{1-\alpha^{\gamma+1}}{1-\alpha}\)</span>
就得到平均每个token的计算量为 <span class="math inline">\(\hat{T}\frac{(1-\alpha)(\gamma\hat{c}+\gamma+1)}{1-\alpha^{\gamma+1}}\)</span>。</p>
<p><span class="math inline">\(\alpha\)</span> 越大，<span class="math inline">\(\frac{(1-\alpha)(\gamma\hat{c}+\gamma+1)}{1-\alpha^{\gamma+1}}\)</span>
这个比值越小，平均计算成本越低。</p>
<p>另外，使用投机解码减少了KV cache和显存的读写。</p>
<h1 id="gamma-的选择"><span class="math inline">\(\gamma\)</span>
的选择</h1>
<p>给定 <span class="math inline">\(\alpha\)</span> 和 <span class="math inline">\(c\)</span>，最佳的 <span class="math inline">\(\gamma\)</span> 应该最大化walltime improvement
factor <span class="math inline">\(\frac{1-\alpha^{\gamma+1}}{(1-\alpha)(\gamma
c+1)}\)</span>。</p>
<p>下图给出不同 <span class="math inline">\(\alpha\)</span> 和 <span class="math inline">\(c\)</span> 下，最佳的 <span class="math inline">\(\gamma\)</span> 值</p>
<img src="/f5c015c/fi_choose_gamma.png" class title="gamma的选择">
<p>推理速度和总计算量之间有tradeoff，即增大 <span class="math inline">\(\gamma\)</span>
会提升推理速度，同时也会带来更多的计算成本，如下所示</p>
<img src="/f5c015c/fi_speed_and_op_table.png" class title="计算量和加速">
<img src="/f5c015c/fi_speed_and_op.png" class title="计算量和加速">
<img src="/f5c015c/fi_walltime.png" class title="walltime">
<p>实际上，<span class="math inline">\(\beta\)</span>
并不是固定的常数，因此实际上我们可以通过在投机解码的过程中预测 <span class="math inline">\(\beta\)</span> 值来选择 <span class="math inline">\(\gamma\)</span>，这是未来的一个改进方向。</p>
<h1 id="approximation-model的选择">approximation model的选择</h1>
<p>论文的实验中，一部分使用现成的模型作为approximation
model。这种情况下，让approximation model的参数规模比target
model小两个数量级是比较好的选择，能够平衡推理加速和计算量。</p>
<p>有趣的是，即使使用很简单的模型，比如n-gram模型作为approximation
model，也能获得不错的 <span class="math inline">\(\alpha\)</span>
值。</p>
<p>另外，在一些特殊的任务，比如摘要任务，由于生成结果往往会从输入的原文里摘取内容，因此使用一个会从输入里copy
token的approximation model可能会得到较高的 <span class="math inline">\(\alpha\)</span> 值。</p>
<p>approximation model的另一个选择是如《Blockwise parallel decoding for
deep autoregressive models》使用的非自回归模型。</p>
<h1 id="实验">实验</h1>
<p>论文在翻译任务和摘要任务上测试了投机解码的效果。使用了T5的较小规模模型作为approximation
model，来加速T5-XXL的推理，效果如下表，最高能达到3倍+的推理加速。</p>
<img src="/f5c015c/fi_t5_result.png" class title="T5系列加速效果">
<p>此外，论文对更多样的模型组合测试了 <span class="math inline">\(\alpha\)</span> 值，如下表所示</p>
<img src="/f5c015c/fi_alpha.png" class title="各种模型的alpha">
<p>可以观察到，比target model小几个数量级的approximation
model倾向于产生介于0.5和0.9之间的 <span class="math inline">\(\alpha\)</span>
值。还注意到，对于所有模型，用于采样的分布越尖（即T比较小，如argmax），
<span class="math inline">\(\alpha\)</span> 值越高。</p>
<h1 id="小结">小结</h1>
<ul>
<li>投机解码可以在完全无损的情况下，把推理速度提升2~3倍<br>
</li>
<li>即使使用最简单的n-gram模型，也能在投机解码的策略下获得推理速度提升<br>
</li>
<li>正常来说，使用比target model小两个数量级的approximation
model就有较好的效果</li>
</ul>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】Fast Inference from Transformers via Speculative Decoding
https://arxiv.org/abs/2211.17192<br>
【2】Accelerating Large Language Model Decoding with Speculative
Sampling https://arxiv.org/abs/2302.01318</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>推理加速</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepSeek-V2和MLA</title>
    <url>/83c49df0.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>DeepSeek-V2发布之后，其低价策略在国产大模型界掀起一阵降价风。</p>
<p>DeepSeek-V2能做到低成本推理的一个原因就是使用了MLA，使得推理时缓存量大大减小。</p>
<p>本篇来看下MLA以及DeepSeek-V2一些其他细节。</p>
<p>DeepSeek-V2除了一个总参数量为236B的主模型外，还有一个方便开源研究的DeepSeek-V2-Lite，总参数量为15.7B，这个在最后介绍。</p>
<h1 id="模型">模型</h1>
<p>DeepSeek-V2介绍：<br>
- 总参数量为236B参数，激活21B<br>
- 支持128k长度<br>
- 相比DeepSeek-67B，DeepSeek-V2节省42.5%的训练成本和93.3%的推理KV
cache需求，而最大throughput则是前者的5.76倍</p>
<p>DeepSeek-V2和其他一些大模型在MMLU上的效果以及激活参数量的对比如下图</p>
<img src="/83c49df0/intro.png" class title="DeepSeek-V2">
<p>可以看到DeepSeek-V2以更少的激活参数量达到了接近70B
dense模型水平的效果。</p>
<p>DeepSeek-V2模型结构如下图</p>
<img src="/83c49df0/model.png" class title="模型">
<p>同V1版本一样，V2在MoE层使用了fine-grained expert和shared
expert（或者叫DeepSeekMoE结构）（可参考《<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a>》）。而V2在结构上最重要的变动就是在注意力层使用了Multi-Head
Latent Attention（MLA）。</p>
<h2 id="mla">MLA</h2>
<p>MLA是DeepSeek-V2提升推理效率，减低KV cache需求的关键。</p>
<p>（关于KV cache和MHA/GQA/MQA的对比，可参考<a href="https://zhuanlan.zhihu.com/p/686149289">《理解Attention:从起源到MHA,MQA和GQA》</a>）</p>
<p>1、从MHA出发</p>
<p>先回顾下标准的MHA。假设 <span class="math inline">\(n_h\)</span>
是注意力头的数量，<span class="math inline">\(d_h\)</span>
是每个注意力头的大小，<span class="math inline">\(\mathbf{h}_{t}\in\mathbb{R}^{d}\)</span>
是第t个输入token。</p>
<p>MHA首先通过三个投影矩阵 <span class="math inline">\(W^{Q},W^{K},W^{V}\in\mathbb{R}^{d_{h}n_{h}\times
d}\)</span> 获得<span class="math inline">\(\mathbf{q}_t,\mathbf{k}_t,\mathbf{v}_t\in\mathbb{R}^{d_hn_h}\)</span>：</p>
<p><span class="math display">\[\mathbf{q}_t=W^Q\mathbf{h}_t\]</span></p>
<p><span class="math display">\[\mathbf{k}_t=W^K\mathbf{h}_t\]</span></p>
<p><span class="math display">\[\mathbf{v}_t=W^V\mathbf{h}_t\]</span></p>
<p>之后 <span class="math inline">\(\mathbf{q}_t,\mathbf{k}_t,\mathbf{v}_t\)</span>
就会被切成 <span class="math inline">\(n_h\)</span>
份，分别进行注意力计算：</p>
<p><span class="math display">\[[\mathbf{q}_{t,1};\mathbf{q}_{t,2};...;\mathbf{q}_{t,n_{h}}]=\mathbf{q}_{t}\]</span></p>
<p><span class="math display">\[[\mathbf{k}_{t,1};\mathbf{k}_{t,2};...;\mathbf{k}_{t,n_{h}}]=\mathbf{k}_{t}\]</span></p>
<p><span class="math display">\[[\mathbf{v}_{t,1};\mathbf{v}_{t,2};...;\mathbf{v}_{t,n_{h}}]=\mathbf{v}_{t}\]</span></p>
<p><span class="math display">\[\mathbf{o}_{t,i}=\sum_{j=1}^t\mathrm{Softmax}_j(\frac{\mathbf{q}_{t,i}^T\mathbf{k}_{j,i}}{\sqrt{d_h}})\mathbf{v}_{j,i}\]</span></p>
<p><span class="math display">\[\mathbf{u}_t=W^O[\mathbf{o}_{t,1};\mathbf{o}_{t,2};...;\mathbf{o}_{t,n_h}]\]</span></p>
<p>其中 <span class="math inline">\(\mathbf{q}_{t,i},\mathbf{k}_{t,i},\mathbf{v}_{t,i}\in\mathbb{R}^{d_{h}}\)</span>，<span class="math inline">\(W^O\in\mathbb{R}^{d\times d_hn_h}\)</span>。</p>
<p>在推理的时候，为了加速会对已经计算过的K、V值进行缓存，那么每个token每层就要保存
<span class="math inline">\(2{n}_{h}{d}_{h}\)</span> 个数值。</p>
<p>而GQA/MQA通过减少K、V头的数量并重复使用，减少了需要缓存的KV的量。</p>
<img src="/83c49df0/GQA.png" class title="GQA">
<p>MQA相当于组数为1的GQA，它在推理时，每层每个token所需要缓存的量为
<span class="math inline">\(2{d}_{h}\)</span>，相比MHA有了1~2两个数量级的减少。可以说这是这种减少KV组数的思路的极限了。但是GQA/MQA毕竟相当于减少了注意力头的数量，在效果上就会有一定的损失。</p>
<p>DeepSeek-V2报告里也对此进行了验证：用1.33T
token的数据分别训练了MHA、GQA、MQA的7B模型，在4个benchmark的对比如下</p>
<img src="/83c49df0/GQA_compare_MHA.png" class title="MHA&#x2F;GQA&#x2F;MQA效果对比">
<p>相比MHA，MQA效果损失最大，GQA次之。</p>
<p>2、MLA</p>
<p>MLA通过对K和V做low-rank joint compression来压缩KV
cache，理论上可以更有效地压缩KV缓存值。</p>
<img src="/83c49df0/MLA.png" class title="MLA">
<p>下面看下MLA具体是怎么做的。</p>
<p>在MHA中，K和V是对 <span class="math inline">\(h_t\)</span>
分别用投影矩阵进行变化得到的，而MLA把KV的变换改成使用一个共用的down-projection
matrix和两个up-projection matrices进行操作：</p>
<p><span class="math display">\[\mathbf{c}_t^{KV}=W^{DKV}\mathbf{h}_t\]</span></p>
<p><span class="math display">\[\mathbf{k}_t^C=W^{UK}\mathbf{c}_t^{KV}\]</span></p>
<p><span class="math display">\[\mathbf{v}_t^C=W^{UV}\mathbf{c}_t^{KV}\]</span></p>
<p><span class="math inline">\(\mathfrak{c}_t^{KV}\in\mathbb{R}^{d_c}\)</span>
就是K和V的compressed latent vector，这也是推理时要缓存的部分。</p>
<p>这里相当于把MHA中的 <span class="math inline">\(W^{K},W^{V}\)</span>
拆成两个矩阵：</p>
<p><span class="math display">\[\mathbf{k}_t=W^K\mathbf{h}_t\rightarrow\mathbf{k}_tW^{UK}W^{DKV}\mathbf{h}_t\]</span></p>
<p><span class="math display">\[\mathbf{v}_t=W^V\mathbf{h}_t\rightarrow\mathbf{k}_tW^{UV}W^{DKV}\mathbf{h}_t\]</span></p>
<p><span class="math inline">\(d_c\)</span> 是KV的压缩维度，让 <span class="math inline">\(d_c\ll
d_hn_h\)</span>，就可以大大减少需要推理时需要缓存的数据量。</p>
<p>看回attention计算，在得到q、k、v之后，会计算权重矩阵并获得最终注意力输出结果：</p>
<p><span class="math display">\[\operatorname{Attention}(Q,K,V)=\operatorname{softmax}(\frac{Q^TK}{\sqrt{d}})V\]</span></p>
<p>而 <span class="math inline">\(Q^TK=H^T(W^Q)^TW^{UK}C\)</span>
中，因此 <span class="math inline">\(W^{UK}\)</span> 可以被吸收进 <span class="math inline">\(W^{Q}\)</span>
中，而不用在计算时显式算出K，只需调整 <span class="math inline">\(W^Q\)</span> 的shape后直接输入C即可。同理 <span class="math inline">\(W^{UV}\)</span> 可以被吸收进 <span class="math inline">\(W^{O}\)</span>。实操上，这样的矩阵合并可能会带来一些精度损失，这是一个值得注意的问题。</p>
<p>此外，DeepSeek-V2还对Q也做了low-rank
compression，跟对K、V的操作类似：</p>
<p><span class="math display">\[\mathbf{c}_t^Q=W^{DQ}\mathbf{h}_t,\\\mathbf{q}_t^C=W^{UQ}\mathbf{c}_t^Q,\]</span></p>
<p>关于对Q进行压缩的原因，这里原文说的是为了减少训练时的activation。但是两个矩阵所得的activation按道理应该比直接使用单个投影矩阵还要多一些，因此此处有点疑问。苏神在<a href="https://kexue.fm/archives/10091">《缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA》</a>中也认为Q的压缩更多是减少了参数量和梯度，而非激活值。</p>
<p>3、兼容RoPE</p>
<p>到这里似乎MLA已经完成了，即减少了缓存的量，也不用引入其他overhead（两个up-projection
matrices都不用算了）。</p>
<p>但是实际上还有一个问题没有解决。同大部分其他大模型一样，DeepSeek-V2使用的位置编码是RoPE，而RoPE是通过在Q、K上乘一个旋转矩阵来编码位置的。相关内容可参考<a href="https://zhuanlan.zhihu.com/p/684072868">《理解LLM位置编码:RoPE》</a>。</p>
<p>而在上面MLA的设计中，已经没有显式计算K了，而RoPE也不能加在latent
vector上。一个方法是重新把K和V显式计算出来，但是这样计算量就会增加，MLA的推理加速效果就会打折扣。</p>
<p>针对这个问题，DeepSeek-V2提出decoupled
RoPE的解决方案，使用额外的multi-head queries <span class="math inline">\(\mathbf{q}_{t,i}^R\in\mathbb{R}^{d_h^R}\)</span>
和一个shared key <span class="math inline">\(\mathbf{k}_t^R\in\mathbb{R}^{d_h^R}\)</span>
来携带RoPE的位置信息，<span class="math inline">\(d_h^R\)</span>
是decoupled queries的维度。</p>
<p>新增的q和k维度使用常规的RoPE计算，用于携带位置信息；而原来的维度依然使用低秩分解的方式计算，最后再计算attention的时候两个部分拼接起来。</p>
<p>最终完整的MLA计算如下</p>
<img src="/83c49df0/MLA_formula.png" class title="MLA公式">
<p>蓝框中的部分就是推理时需要缓存的内容。</p>
<p>MLA所需的缓存量约等于组数为2.5的GQA：</p>
<img src="/83c49df0/MLA_cache.png" class title="MLA缓存量">
<p>在效果上，DeepSeek-V2分别对比了MLA和MHA的16B模型（训练1.33T
token）和250B模型（训练420B token）：</p>
<img src="/83c49df0/MLA_perf.png" class title="MLA效果">
<p>在4个benchmark上看，MLA基本都比要比MHA要好。这个结果还是有些出乎意料的，这妥妥就是免费的午餐，在节省KV
cache的同时还能获得效果提升。感觉MLA效果还有待进一步验证。</p>
<h2 id="负载均衡">负载均衡</h2>
<p>负载均衡策略是MoE永远要考虑的问题，对效果和效率都有很大的影响。</p>
<p>1、Device-Limited Routing</p>
<p>在使用专家并行的情况下，每个token所需的通讯量取决于它的target
expert所在的device数。而由于使用了fine-grained
expert，这个device数量可能会比较大，就会导致通讯成为瓶颈。</p>
<p>因此DeepSeek-V2会基于target
expert的得分，限制最多所能发送的device数量M。实践中，发现M≥3就能达到和不限制相同的效果了。</p>
<p>2、Expert-Level Balance Loss</p>
<p>和DeepSeekMoE V1一样，专家级的负载均衡如下：</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{L}_{\mathrm{ExpBal}}&amp; =\alpha_1\sum_{i=1}^{N_r}f_iP_i
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
f_{i}&amp; =\frac{N_{r}}{K_{r}T}\sum_{t=1}^T\mathbb{1}(\text{Token
}t\text{ selects Expert }i)
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
P_{i}&amp; =\frac1T\sum_{t=1}^Ts_{i,t}
\end{aligned}\]</span></p>
<p><span class="math inline">\(\alpha_1\)</span> 是expert-level balance
factor，T为token数。</p>
<p>3、Device-Level Balance Loss</p>
<p>在使用专家并行的情况下，专家被分成D个组<span class="math inline">\(\{\mathcal{E}_1,\mathcal{E}_2,...,\mathcal{E}_D\}\)</span>，各个组之间的负载均衡损失：</p>
<p><span class="math display">\[\mathcal{L}_\mathrm{DevBal}=\alpha_2\sum_{i=1}^Df_i^{\prime}P_i^{\prime}\]</span></p>
<p><span class="math display">\[f_i&#39;=\frac1{|\mathcal{E}_i|}\sum_{j\in\mathcal{E}_i}f_j\]</span></p>
<p><span class="math display">\[P_i&#39;=\sum_{j\in\mathcal{E}_i}P_j\]</span></p>
<p><span class="math inline">\(\alpha_2\)</span> 是device-level balance
factor。</p>
<p>4、Communication Balance Loss</p>
<p>前面对token发送target专家的总device数做了限制，但是依然有可能出现某些device【接收】的token数量不平衡的情况，这同样会影响通讯效率。</p>
<p>因此这里还加了一个communication balance loss：</p>
<p><span class="math display">\[\mathcal{L}_{\mathrm{CommBal}}=\alpha_3\sum_{i=1}^Df_i^{\prime\prime}P_i^{\prime\prime}\]</span></p>
<p><span class="math display">\[f_i^{\prime\prime}=\frac
D{MT}\sum_{t=1}^T1(\text{Token t is sent to Device i})\]</span></p>
<p><span class="math display">\[P_i&#39;&#39;=\sum_{j\in\mathcal{E}_i}P_j\]</span></p>
<p><span class="math inline">\(\alpha_3\)</span> 是communication balance
factor。</p>
<p>5、Token-Dropping Strategy</p>
<p>前面虽然加了各种负载均衡loss，但是实际上还是没有办法保证能够得到严格的负载均衡，因此在训练时还引入了一个device-level
token-dropping
strategy，对每个device设定一个capacity，如果在一个batch中，某个device所处理的token达到了容量，那么后面再分配到这个device的token就都会被drop。</p>
<p>另外为了保证模型能够处理到完整的sequence，训练时有10%的sequence保证永远不drop任何token。</p>
<p>注意这个策略只在训练时时候，推理时不会给device设置容量限制。</p>
<h1 id="训练">训练</h1>
<p>DeepSeek-V2使用和DeepSeek
67B一样的tokenizer，BBPE训练出来的100k词表。</p>
<p>模型的所有预训练数据约有8.1T，其中12%是中文。</p>
<h2 id="超参">超参</h2>
<p>1、模型超参</p>
<ul>
<li>layer num = 60<br>
</li>
<li>hidden size = 5120<br>
</li>
<li>initialization standard deviation = 0.006<br>
</li>
<li>attention head数量 = 128，每个attention head size = 128<br>
</li>
<li>KV压缩维度 <span class="math inline">\(d_c=512\)</span><br>
</li>
<li>Q压缩维度 <span class="math inline">\(d_c&#39;=1536\)</span><br>
</li>
<li>decoupled queries and key per head dimension = 64<br>
</li>
<li>2个共享专家 + 6/160路由专家<br>
</li>
<li>专家大小 = 1536<br>
</li>
<li>总参数236B，激活参数21B</li>
</ul>
<p>2、预训练超参</p>
<ul>
<li>AdamW：beta_1 = 0.9，beta_2 = 0.95，weight_decay = 0.1<br>
</li>
<li>lr scheduler：warmup-and-step-decay，warmup = 2k step，最大lr =
2.4E-4；在训练进度60%和90%的时候lr乘以0.316<br>
</li>
<li>gradient clipping norm = 1.0<br>
</li>
<li>batch size scheduling strategy：在训练的前225B，batch
size逐渐从2304增大到9216，之后保持不变<br>
</li>
<li>maximum sequence length = 4k<br>
</li>
<li>负载均衡权重：<span class="math inline">\(\alpha_1=0.003\)</span>，<span class="math inline">\(\alpha_2=0.05\)</span>，<span class="math inline">\(\alpha_3=0.02\)</span></li>
</ul>
<h2 id="长窗口">长窗口</h2>
<p>在完成基础预训练后，通过在 <span class="math inline">\(k_t^R\)</span>
上使用YaRN把模型窗口从4k推广到128k。YaRN的参数设置如下：<br>
- s = 40<br>
- α = 1<br>
- β = 32<br>
- target maximum context length = 160k</p>
<p>和原始的YaRN有所不同，由于注意力机制有所改动，所以把length scaling
factor改成 <span class="math inline">\(\sqrt{t}=0.0707\ln
s+1\)</span>，以更好调控注意力熵。</p>
<p>整个长文本训练在32k长度，batch size =
576的数据上训练了1000步，最终在大海捞针评测上的结果如下</p>
<img src="/83c49df0/needle.png" class title="大海捞针">
<h2 id="评测">评测</h2>
<p>DeepSeek-V2的base模型和其他较大规模模型的效果对比如下</p>
<img src="/83c49df0/pt_eval.png" class title="评测">
<p>DeepSeek-V2看起来基本达到了和70B规模dense模型竞争的水平。</p>
<h2 id="对齐">对齐</h2>
<p>SFT共使用了1.5M条数据，其中1.2M条以helpfulness为主，0.3M条以safety为主。</p>
<p>训练设置：<br>
- epoch = 2<br>
- lr = 5e-6</p>
<p>在SFT基础上，DeepSeek-V2通过GRPO进行了强化学习训练。</p>
<p>最终对齐模型的评测如下</p>
<img src="/83c49df0/align_eval.png" class title="评测">
<h1 id="deepseek-v2-lite">DeepSeek-V2-Lite</h1>
<p>为方便开源研究，研究人员还提供一个稍小一点规模的DeepSeek-V2-Lite。</p>
<p>模型超参：<br>
- layer num = 27<br>
- hidden size = 2048<br>
- initialization standard deviation = 0.006<br>
- attention head数量 = 16，每个attention head size = 128<br>
- KV压缩维度 <span class="math inline">\(d_c=512\)</span><br>
- Q不进行压缩<br>
- decoupled queries and key per head dimension = 64<br>
- 2个共享专家 + 6/64路由专家<br>
- 第一层不使用MoE<br>
- 专家大小 = 1408<br>
- 总参数15.7B，激活参数2.4B</p>
<p>预训练超参：<br>
- AdamW：beta_1 = 0.9，beta_2 = 0.95，weight_decay = 0.1<br>
- lr scheduler：warmup-and-step-decay，warmup = 2k step，最大lr =
4.2E-4；在训练进度60%和90%的时候lr乘以0.316<br>
- gradient clipping norm = 1.0<br>
- constant batch size = 4608<br>
- maximum sequence length = 4k<br>
- 负载均衡权重：<span class="math inline">\(\alpha_1=0.003\)</span>，没有使用其他负载均衡loss<br>
- 总训练量 = 5.7T</p>
<img src="/83c49df0/lite_eval_1.png" class title="评测">
<img src="/83c49df0/lite_eval_2.png" class title="评测">
<h1 id="小结">小结</h1>
<ul>
<li>MLA是DeepSeek-V2很重要一个模块，在提升推理效率上有很大帮助，这个方向后续应该会有更多工作。<br>
</li>
<li>MoE受到越来越多的关注，几乎有一半的popular的模型是MoE结构了。</li>
</ul>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a><br>
<a href="https://www.linsight.cn/dd614e12.html">大模型算法题(7)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】DeepSeek-V2: A Strong, Economical, and Efficient
Mixture-of-Experts Language Model https://arxiv.org/abs/2405.04434<br>
【2】缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA
https://kexue.fm/archives/10091</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>技术报告</tag>
        <tag>DeepSeek</tag>
        <tag>MLA</tag>
        <tag>GQA</tag>
        <tag>MoE</tag>
      </tags>
  </entry>
  <entry>
    <title>解锁大模型长上下文能力</title>
    <url>/cc852861.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>步入2024年Q2，大模型在RAG、文档对话、大模型Agent能力等方向的发展持续升温。在平时的日常生活和工作中，大模型工具提供的文档总结、文本润色、代码生成等能力已经是提高效率的必备帮手，甚至在一些复杂或者不熟悉的场景上，大模型也已经能提供一些比较专业的帮助。</p>
<p>在这些方向上，大模型(超)长上下文的能力都是基础。无论是使用详细的CoT/ToT，还是通过多篇检索文档提供专业知识，抑或是使用相关样例提升回复质量，都需要模型具备处理很长的输入输出信息的能力。这不仅要求模型在较长的位置编码下依然具有良好的语言建模能力，而且还需要模型能够进行长距离的、细致的阅、准确的阅读和理解。</p>
<p>本篇将梳理几个通过轻量级训练解锁大模型长上下文能力的工作。</p>
<h1 id="支持128k上下文的数据工程">支持128k上下文的数据工程</h1>
<p>论文：Data Engineering for Scaling Language Models to 128K
Context</p>
<p>时间：2024年2月</p>
<p>阶段：预训练</p>
<p>长度：128k</p>
<h2 id="评测指标">评测指标</h2>
<p>模型的长上下文能力不仅体现在文本较长的时候，模型的PPL依然能保持在较低的水平，还体现在对于长上下文输入，模型依然能够进行准确的阅读理解和推理。</p>
<p>以往一些工作仅使用validation
dataset上的PPL作为评测指标，并不能很好地表征模型的真实长上下文能力。而目前被广泛使用的
Needle in a
Haystack，或者叫大海捞针任务，是对长上下文能力的一个比较好的评测。这篇论文主要就以大海捞针任务为标准，对不同的模型和方案进行对比。</p>
<p>两个PPL几乎相同的模型，在大海捞针任务上的差距可以很大，如下图所示，颜色越绿代表正确率越高</p>
<img src="/cc852861/eng_ppl.png" class title="PPL和大海捞针">
<p>目前已有的一些扩展大模型上下文窗口的方法，比如LongLoRA和Mistral所采用的YaRN，虽然理论上来说，能够支持&gt;100k的上下文长度，但是实际上在大海捞针任务的表现却不太好。相关模型在大海捞针任务上的效果对比如下所示，只有GPT-4的效果比较好。</p>
<img src="/cc852861/eng_needle_comp.png" class title="大海捞针任务对比">
<h2 id="数据分布">数据分布</h2>
<p>这篇论文认为，在&lt;=4k窗口长度完成预训练的模型，其实就已经基本具备在128k或者更大的上下文窗口进行推理的能力，只需要进行轻量级的继续预训练（e.g.
&lt;5B token），就能够解锁这种能力。</p>
<p>（而一些其他的工作在这方面则有着相反的观点，比如在32k窗口训练了400B
token的《Effective long-context scaling of foundation
models》，以及Xverse）</p>
<p>要做继续预训练，最重要的一点就是要决定使用什么样的数据。</p>
<p>这篇论文里的实验是基于LLAMA的，因此使用了和LLAMA预训练数据具有相近领域分布的SlimPajama数据集作为基础。</p>
<p>对于长上下文的继续预训练数据，需要仔细考虑数据长度和领域分布的影响。通常来说，某些领域天然会有更高比例的长文本数据，比如书籍、论文和github，而一些其他领域的长数据就较少，比如新闻。如果直接从整体数据中挑选长数据而忽略领域分布，就可能造成训练数据在领域分布上的偏移。</p>
<p>论文使用了几种不同的数据处理策略，用于后面的实验对比：<br>
- Cut at
4K：把所有的数据按4k长度进行分块，这样不会影响领域分布。这也是很多4k预训练模型所采样的方案，比如LLAMA。<br>
- Cut at
128K：截断长度提升到128k，可以保留长文本内部信息的依赖关系。LongLoRA就是这么做的。<br>
- Per-source
Upsampling：在保持各个领域的比例不变的前提下，对长文本进行上采样，提高长文本的比例。这是这篇论文所推荐的方法，实验效果最好。<br>
- Global Upsampling：不管领域，直接对长文本进行上采样。<br>
- Upsample Arxiv/ Book/
Github：提高特定领域的数据比例，对长文本进行上采样。</p>
<p>这些策略基本涵盖了大部分长文本相关工作在数据上的处理策略。</p>
<p>不同数据处理策略下，SlimPajama数据内各领域的分布如下图所示</p>
<img src="/cc852861/eng_data_dist.png" class title="数据分布">
<p>Per-source
Upsampling是效果最好的，也是这篇论文所推荐的数据工程策略。</p>
<h2 id="实验配置">实验配置</h2>
<p>实验上，用80k的窗口长度训练LLAMA2-7B模型，用64k的窗口训练LLAMA2-13B模型。</p>
<p>虽然理论上，计算复杂度度和模型训练窗口长度是平方关系，但是实际实现上，由于有FlashAttention等方案，可以把Attention的计算通过设备间通讯，在多个设备间并行起来。而设备间的通讯（包括GPU和CPU，GPU和GPU之间）成本都是constant或者linear，因此实际上80k窗口的的训练耗时只是4k长度的训练的3倍，而不是理论上的400倍。</p>
<p>当然，实际所需的计算量并没有减少，但是至少时间成本从平方变成了线性。剩下的，只要堆jia卡qian就可以提速。</p>
<img src="/cc852861/add_money.jpg" class title="加钱就行">
<p>Per-source Upsampling和其他工作的数据处理策略的对比如下</p>
<img src="/cc852861/eng_data.png" class title="模型策略">
<p>训练的配置和耗时如下所示</p>
<img src="/cc852861/eng_config.png" class title="训练配置">
<p>实验的其他配置：<br>
- lr = 2e-5<br>
- RoPE base从1,0000改为500,000<br>
- batch size = 4M token</p>
<h2 id="训练量">训练量</h2>
<p>前面提到，论文认为只需要轻量级的继续预训练就可以解锁长上下文能力，那么到底需要训练多少token呢？</p>
<p>论文分别取了训练了100M、300M、500M、1B、5B、10B
token的中间checkpoint进行PPL和海底捞针任务评测，结果如下</p>
<img src="/cc852861/eng_tokens.png" class title="训练量">
<p>结论是，在训练了500M
token的时候，模型基本解锁了长上下文的能力；在训练了5B
token的时候，模型已经收敛，而且继续训练到10B
token也没有进一步收益了。</p>
<h2 id="数据策略对比">数据策略对比</h2>
<p>使用前面提到的不同数据策略在LLAMA2-7B模型用5B
token进行训练，并对比效果。</p>
<p>LLAMA2的预训练长度为4k，因此对比的时候分成了0-4k和4k-128k两段，分别评测模型经过长文本训练后，在短文本上的效果是否有变差，以及在长文本上是否有提升。</p>
<p>各个数据策略在不同领域的效果变化如下</p>
<img src="/cc852861/eng_sample.png" class title="采样的影响">
<p>可以得到几个结论：<br>
- 在0-4k长度上，除了Per-source
Upsampling以外，各种数据策略都会对模型效果有损害<br>
-
在一些领域上的提升，并不能很好地迁移到其他领域，比如Book和Github之间就有点跷跷板效应，其中一个效果好了，另一个可能就有损失<br>
- 在4k-128k，Per-source
Upsampling在各个领域的效果相对较为平衡（绿色的数量最多）</p>
<p>此外，length upsampling很重要。Per-source
Upsampling的策略在领域上可以和源数据保持一致，而提升长文本的比例。</p>
<p>用同样80k的训练窗口在LLAMA2-7B进行实验，一个使用原数据进行拼接，另一个使用Per-source
Upsampling，结果如下。在PPL基本相同的情况下，Per-source
Upsampling在大海捞针的效果远超原数据。这说明提高长文本的比例，能极大优化模型远距离建模的能力。</p>
<img src="/cc852861/eng_ppl.png" class title="PPL和大海捞针">
<h2 id="结论">结论</h2>
<p>通过实验，论文提出提升模型长上下文能力的数据工程实践的几个关键点：<br>
-
在长窗口上进行轻量级训练，可以提升模型实际的远距离建模能力，而不仅仅是保持PPL较低<br>
- 领域之间有竞争关系，最好和原预训练模型所用的分布保持一致<br>
- 长度上采样对最终效果有很大影响，要提高各领域内长文本的比例</p>
<h1 id="paraphrasing">Paraphrasing</h1>
<p>论文：Training With "Paraphrasing the Original Text" Improves
Long-Context Performance</p>
<p>时间：2023年12月</p>
<p>阶段：微调</p>
<p>长度：在50k长度依然能有较好的效果，如下所示。</p>
<img src="/cc852861/paraphrasing_intro.png" class title="paraphrasing">
<h2 id="检索能力">检索能力</h2>
<p>对于长上下文的任务，有用的信息通常是稀疏的，一般只有少量的句子或者段落包含了可以用于回答问题的有用信息。可以隐式地将这样长上下文的任务拆分成两个子任务，即相关信息的检索，和根据相关信息回答问题两个任务。</p>
<p>目前一些支持长上下文的方法，比如位置编码相关的线性插值、NTK插值、YaRN等，虽然使得模型在形式上支持了长上下文的任务，但是在任务的准确性上效果却不佳。</p>
<p>使用这些优化方案的模型依然会遇到lost in the
middle的问题，即模型天然更容易关注到输入文本的开头和结尾部分的信息，而更容易忽略中间部分的信息，注意力迷失在大量无关内容上，而无法集中到少数相关的位置上。而对于长上下文的任务，大量的信息是处于middle的位置的，如果忽略这些信息自然会使得任务效果不好。而效果不好的原因就是模型在长上下文的情况下，retrieval能力偏弱，不能找到有用的信息。</p>
<h2 id="相关工作">相关工作</h2>
<p>一些工作直接把模型在长窗口下进行训练，比如：<br>
-
Together的LLaMA-2-7B-32K（https://huggingface.co/datasets/togethercomputer/Long-Data-Collections）；Together开源了Multipassage-QA-from-Natural-Questions和BookSum微调数据集。<br>
- LongAlpaca（《LongLoRA: Efficient Fine-tuning of Long-Context Large
Language Models》）<br>
- Ziya-Reader（《Never Lost in the Middle:Improving Large Language
Models via Attention Strengthening Question Answering》）</p>
<p>直接在长窗口训练有一定的效果，但是依然有几个问题：<br>
- 模型推理窗口越来越大，所需的训练数据集长度也要不断更新。<br>
- 随着长度增大，训练成本变高。<br>
-
构建长上下文数据集的成本比价高，高质量的数据并不容易获得。虽然有一些开源的数据集，但是在实际场景上可能还需要做领域适配，分布调整等工作。</p>
<p>一个更简单一点的方法是优化prompt的设计，比如CoT。</p>
<p>在长上下文的场景下，可以通过prompt让模型显式地先找到原文的相关信息再进行回答。比如Claude-2.1就通过在prompt增加“Here
is the most relevant sentence in the
context”让长文本问答的准确率从27%提升到98%（https://www.anthropic.com/news/claude-2-1-prompting）。</p>
<p>也可以对输入内容进行重新的编排：<br>
- LongLLMLingua（《LongLLMLingua: Accelerating and Enhancing LLMs in
Long Context Scenarios via Prompt
Compression》）对输入文本进行了压缩。<br>
- Attention Sorting（《Attention Sorting Combats Recency Bias In Long
Context Language
Models》）在decode过程中根据各个文档被分配到的注意力值，对文档进行重新排序。</p>
<h2 id="提高检索能力">提高检索能力</h2>
<p>这篇论文提出了一个叫检索相关度（retrieval
relevance）的指标，一个token（或者n-gram） <span class="math inline">\(x\)</span> 的检索相关度 <span class="math inline">\(R(x)\)</span> 定义如下。</p>
<p><span class="math display">\[R(x)=\frac{n^\prime}n\log\frac
N{N^\prime+1}\]</span></p>
<p>这个指标和TF-IDF很像。其中，<span class="math inline">\(n^\prime\)</span> 表示 <span class="math inline">\(x\)</span> 在gold-chunk中的频率，而 <span class="math inline">\(n\)</span> 是gold-chunk中的总token数；<span class="math inline">\(N\)</span> 表示整个上下文中总chunk数，<span class="math inline">\(N^\prime\)</span> 是包含x的chunk的数量。</p>
<p>基于token <span class="math inline">\(x\)</span> 的检索相关度 <span class="math inline">\(R(x)\)</span> ，定义训练样本 <span class="math inline">\(S\)</span> 的检索相关度如下</p>
<p><span class="math display">\[\mathcal{R}(S)=\frac{1}{|S_a|}\sum_{x\in\mathcal{S}_a}R(x)\]</span></p>
<p>其中 <span class="math inline">\(S_a\)</span> 表示 <span class="math inline">\(S\)</span> 的答案部分。</p>
<p>通过 <span class="math inline">\(\mathcal{R}(S)\)</span>
这个指标可以反映出一个训练样本对模型提高检索能力的贡献。<span class="math inline">\(\mathcal{R}(S)\)</span>
越高，这个样本对提高模型检索能力的贡献越大。</p>
<p>那么一个简单有效提升训练样本检索相关度的做法，就是把gold-chunk放到答案中，即paraphrasing
the original text。</p>
<p>一个paraphrasing和其他答案设计方案对比的例子如下</p>
<img src="/cc852861/paraphrasing_example.png" class title="paraphrasing例子">
<p>其中高亮部分的token是高检索相关度的token，明显paraphrasing拥有更高的比例。</p>
<p>论文使用GPT-4来构建包含paraphrasing的问答对，流程实际如下</p>
<img src="/cc852861/paraphrasing_dataset.png" class title="构建数据集">
<p>这种方式收集了一批单文档问答和多文档问答的数据，再加上一些传统文本摘要数据（摘要不好用这种方式构建，因此直接使用）等，构成一个包含10,825条英文数据，8,454条中文数据，长度在8k和32k之间的数据集。数据集详细的领域分布如下所示</p>
<img src="/cc852861/paraphrasing_dataset_dist.png" class title="数据集分布">
<p>论文构建的数据集和Multi-passage-QA-from-NQ的检索相关性指标对比如下</p>
<img src="/cc852861/paraphrasing_quality.png" class title="数据集检索相关性对比">
<p>使用这个数据集微调的模型，和其他模型在LongBench上的效果对比如下</p>
<img src="/cc852861/paraphrasing_perf.png" class title="效果对比">
<p>另外，在这个数据集上微调之后，模型对于lost in the
middle的问题也有一定的缓解，如下所示</p>
<img src="/cc852861/paraphrasing_lost.png" class title="缓解lost in the middle">
<h1 id="pose">PoSE</h1>
<p>论文：PoSE: Efficient Context Window Extension of LLMs via Positional
Skip-wise Training</p>
<p>时间：2023年9月</p>
<p>阶段：微调</p>
<p>长度：128k</p>
<h2 id="背景">背景</h2>
<p>目前大部分流行的大模型使用旋转位置编码RoPE。在短文本上训练的模型，在长输入上效果不好的原因之一，就是长文本有很多模型没有见过没有训练过的位置编码。</p>
<p>基于位置编码的长上下文优化，比如线性插值、NTK插值和YaRN等，依然需要进行目标长度的训练才能有比价好的效果。而随着目标长度越来越长（8k，32k，128k...），这样的训练成本也越来越高，逐渐变得不容易进行。</p>
<p>这篇论文提出<strong>Po</strong>sitional
<strong>S</strong>kip-wis<strong>E</strong>，PoSE，通过在短的训练窗口模拟长距离的位置编码，提升模型处理长上下文的能力。模型可以在2k的训练窗口进行训练，而在128k的长度进行推理。相比直接训练128k模型效率更高。</p>
<p>也有一些工作的思路和这篇文章有相近之处，比如RandPos（《Randomized
positional encodings boost length generalization of
transformers》），但是RandPos主要用于预训练阶段，并且相邻token之间的位置是不连续的，而PoSE主要用于微调阶段，相邻token之间的位置是连续的。</p>
<h2 id="位置模拟">位置模拟</h2>
<p>PoSE提出两个设计原则：<br>
-
模拟所用的位置编码index要覆盖目标长度的范围。如果我们想在128k的窗口进行推理，那就要保证训练的时候，模型从1-128k的位置编码都见过。<br>
-
为了不损害原模型的能力，位置编码应该尽量保持原来预训练的结构，即尽量连续，和保持顺序关系。</p>
<p>假设我们的训练窗口长度为 <span class="math inline">\(L_c\)</span>，首先我们随机把它切成 <span class="math inline">\(N\)</span> 个chunk， <span class="math inline">\(c_0,c_1,\ldots,c_{N-1}\)</span>，长度分别为 <span class="math inline">\(l_0,l_1,\ldots,l_{N-1}\)</span>。对于chunk <span class="math inline">\(i\)</span>，其中token的位置编码下标如下</p>
<p><span class="math display">\[\mathrm{Pos}(c_i)=\{st_i,st_i+1,\ldots,st_i+l_i-1\},\quad
st_i=\sum_{j=0}^{i-1}l_j\]</span></p>
<p>然后我们给每个chunk，从uniform distribution <span class="math inline">\(\mathcal{U}(S)\)</span> 中随机采样一个skipping
bias <span class="math inline">\(u_i\)</span>，把这个bias加到这个对应chunk的token的位置编码下标中，就有</p>
<p><span class="math display">\[\mathrm{PoSE}(c_i)=\{u_i+st_i,u_i+st_i+1,\ldots,u_i+st_i+l_i-1\}\]</span></p>
<p>这里要注意，处理后各个chunk的位置编码下标不能有overlap，所以要求
<span class="math inline">\(u_i\geq u_{i-1}\)</span>。</p>
<p>直观地说，引入skipping
bias使模型能接触到更大范围的位置编码。为了全面覆盖目标上下文窗口，我们为每个训练sample单独采样每个chunk的长度和skipping
bias。</p>
<p>此外，位置编码index在每个chunk内的连续性，与原模型预训练期间所采用的结构非常相似。因此，在这些新的index上进行微调，不会损害模型原有的能力。</p>
<p>现在，位置编码的下标决定好了，我们还需要决定每个chunk的token使用哪些。</p>
<p>token的采样和位置编码下标的采样类似，具体来说，我们采样<span class="math inline">\(v_i\sim\mathcal{U}(\{v_{i-1},\ldots,L_x-L_c\})\)</span>，那么
<span class="math inline">\(c_i\)</span> 的token如下</p>
<p><span class="math display">\[c_i=\boldsymbol{x}[v_i+st_i:v_i+st_i+l_i]\]</span></p>
<p>论文对一些采样变体，比如 <span class="math inline">\(v_i=u_i\)</span>，<span class="math inline">\(v_i=0\)</span>
等进行了探索，发现基本没有什么影响，因此 <span class="math inline">\(v_i\)</span> 保持原来的采样方案即可。</p>
<p>在实际训练中，<span class="math inline">\(N\)</span>
设置为2，因为如果太大可能对原模型的能力造成损害。而 <span class="math inline">\(u_0\)</span> 和 <span class="math inline">\(v_0\)</span> 设为了0。</p>
<p>PoSE方案如下图所示</p>
<img src="/cc852861/pose_method.png" class title="PoSE">
<p>实验上，使用了LLAMA-7B模型，在2k的窗口上进行了1,000步的训练，batch
size为64。使用lr=2e-5，warmup step=10。</p>
<p>PoSE和其他模型在PPL上的对比如下，基本能达到和Full-length训练相近的水平。</p>
<img src="/cc852861/pose_ppl.png" class title="PPL">
<p>而在passkey retrieval任务上，也有不错的效果，如下图所示</p>
<img src="/cc852861/pose_passkey.png" class title="passkey">
<p>相比其他方案，PoSE的一个优势是可以在没有任何成本增加的情况下，支持更长的推理长度。比如可以通过简单修改采样策略的参数，PoSE就可以支持到1M，甚至更大的窗口长度，这是其他方法难以做到的。</p>
<h1 id="小结">小结</h1>
<ol type="1">
<li>有了FlashAttention等方案之后，在128k这个长度，我们也有能力在合理的成本下，进行继续预训练，使用5B左右的token解锁模型的长上下文能力。<br>
</li>
<li>预训练中，长文本对模型的远距离建模能力很重要，要提高长文本的比例才有更好的效果。此外，领域的分布也是一个需要关注的点。<br>
</li>
<li>在长窗口的微调上，精心设计输入输出形式能带来一些收益。</li>
<li>对于更长的窗口，比如M级别这种几乎无法直接训练/微调的长度，PoSE这种模拟的方案能够在不增加成本的情况下，在效果上达到接近直接训练/微调的表现。</li>
</ol>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】Data Engineering for Scaling Language Models to 128K Context
https://arxiv.org/abs/2402.10171<br>
【2】Training With "Paraphrasing the Original Text" Improves
Long-Context Performance https://arxiv.org/abs/2312.11193<br>
【3】PoSE: Efficient Context Window Extension of LLMs via Positional
Skip-wise Training https://arxiv.org/abs/2309.10400</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>长上下文</tag>
        <tag>微调</tag>
        <tag>预训练</tag>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title>Gemma2</title>
    <url>/cf3f1f81.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>Google发布Gemma2了，包括2B、9B和27B三个规模。</p>
<p>其中9B和27B的模型在huggingface上已经可下载，包括base模型和fine-tuned模型，2B模型晚点也会放出来。</p>
<img src="/cf3f1f81/intro.png" class title="Gemma2">
<p>来看看有啥可关注的技术点。</p>
<h1 id="结构设计">结构设计</h1>
<p>3个规模的模型结构设计如下</p>
<img src="/cf3f1f81/model.png" class title="模型">
<p>一些设计点和Gemma1一样：<br>
- decocer-only<br>
- RoPE<br>
- context length = 8192<br>
- GeGLU</p>
<p>除此之外相比一代也有一些变化点，下面一一看下。</p>
<h2 id="sliding-window-attention">sliding window attention</h2>
<p>Gemma2每两层使用一个sliding window attention层，sliding
window的大小为4096。</p>
<p>关于sliding window的内容，可参考<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a>。</p>
<p>理论上，这样的设计可以在减少计算资源需求的情况下，保持一定的长文本能力（得益于这部分没有使用sliding
window的层）。</p>
<p>Mistral的早期版本也用了sliding window
attention，但后来又去掉了。感觉是否使用sliding window
attention还得看下游场景的需求。</p>
<h2 id="logit-soft-capping">logit soft-capping</h2>
<p>参考Gemini 1.5，Gemma2使用了logit soft-capping。</p>
<p>soft-capping是一种在不进行truncation的情况下，防止logits过度增长的方法。</p>
<p>具体来说，就是对logits进行如下的操作：</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> soft_cap ∗ tanh(logits <span class="op">/</span> soft_cap)  </span></code></pre></div>
<p>这样logits的最终值就可以保持在(-soft_cap,
+soft_cap)区间上，就能够在不损失太多信息的情况下稳定训练。</p>
<p>soft-capping应用在模型的final layer和每个attention
layer。对于9B和27B模型，final layer和attention
layer的soft_cap值分别是30.0和50.0。</p>
<p>这里有个问题是Flash Attention /
SDPA不支持soft-capping，因此微调训练的时候推荐使用eager
attention而非SDPA。</p>
<p>至于推理，研究人员发现去掉soft-capping对结果影响不大，因此推理的时候可以去掉然后用原先的加速方案加速。当然，这样依然有小概率出现结果被改变的情况，所以推理的时候是否移除soft-capping，可能需要根据下游任务来定。</p>
<h2 id="其他">其他</h2>
<p>Gemma2报告还提到：</p>
<p>（1）post-norm 和 pre-norm 都使用 RMSNorm</p>
<p>（2）使用group num = 2的GQA</p>
<h1 id="训练">训练</h1>
<h2 id="预训练数据">预训练数据</h2>
<p>2B模型总共训练了2B token，9B模型训练了8T
token，而27B模型训练了13T，是第一代的两倍。data
mixture通过和Gemma1类似的消融方法确定，这里没有给出具体的数据。</p>
<p>Gemma2所用的tokenizer和Gemma1、Gemini一样，基于BPE，大小为256k。</p>
<h2 id="knowledge-distillation">knowledge distillation</h2>
<p>Gemma2 27B模型是直接进行预训练的，而2B和9B模型没有通过next token
prediction的任务训练，而是使用了知识蒸馏的方法：</p>
<p><span class="math display">\[\min_{P_S}\sum_x-P_T(x\mid x_c)\log
P_S(x\mid x_c)\]</span></p>
<p>实操时，teacher
model先离线跑出每个token的概率保存下来。由于vocabulary太大了，所以保存的时候只保存一个subset。（长尾部分置零，头部重新归一化概率？这里报告没有细说）</p>
<p>而在SFT的时候，通常的做法是把synthetic
data和真实prompt数据喂给teacher模型，获取对应的response，然后用常规的distillation的方式进行训练。Zephyr和OpenHermes就是这样的做法。</p>
<p>这样的训练方式虽然有效，但是有可能出现train-inference
mismatch的问题，即student model在推理的时候出现和训练时不同的分布。</p>
<p>为了解决这个mismatch的问题，这里Gemma2参考《On-policy distillation of
language models: Learning from self-generated mistakes》，使用on-policy
distillation的方法。</p>
<p>具体来说，就是由student对prompt生成response，然后最小化teacher和student在这个response上的KL
divergence。这样就不会出现train-inference mismatch的问题了。</p>
<p>得到SFT模型之后，这里还进行了RLHF进一步提升模型效果。</p>
<p>post-training所用的特殊token和格式样例如下</p>
<img src="/cf3f1f81/format.png" class title="formatting">
<img src="/cf3f1f81/example.png" class title="formatting">
<p>Gemma2报告中还提到了参考《Warp: On the benefits of weight averaged
rewarded policies》进行了model merging。</p>
<p>以前训练Bert的时候就用了Exponential Moving
Average对多个checkpoint进行平均，整体来说确实是略有提升。</p>
<h1 id="消融实验">消融实验</h1>
<p>Gemma2还做了一些消融实验。</p>
<p>（1）distillation versus from scratch</p>
<p>相比直接从零训练，蒸馏的效果略好一些，如下所示</p>
<img src="/cf3f1f81/ablation_1.png" class title="消融实验">
<p>（2）impact of distillation w.r.t. model size</p>
<p>使用相同的7B模型作为teacher
model，不同规模的student模型都可以有相对稳定的收益，没有明显衰减</p>
<img src="/cf3f1f81/ablation_2.png" class title="消融实验">
<p>（3）GQA versus MHA</p>
<p>在9B模型上对比GQA和MHA的效果，GQA要略好一些（这就有点反直觉了）</p>
<img src="/cf3f1f81/ablation_3.png" class title="消融实验">
<p>（4）wide versus deep</p>
<p>在相同参数量下，更深的9B模型比更宽的9B模型更好，这个和以往的认知的相同的：模型深度对效果影响更大</p>
<img src="/cf3f1f81/ablation_4.png" class title="消融实验">
<p>（5）changing sliding window size</p>
<p>使用不同大小的sliding window，在评测集上的ppl差别并不大</p>
<img src="/cf3f1f81/ablation_5.png" class title="消融实验">
<p>（6）impact of formatting</p>
<p>相对Mistral，Gemma2的得分方差相对更小一些</p>
<img src="/cf3f1f81/ablation_6.png" class title="消融实验">
<h1 id="评测">评测</h1>
<p>在各个benchmark的效果：</p>
<img src="/cf3f1f81/eval1.png" class title="eval">
<img src="/cf3f1f81/eval2.png" class title="eval">
<h1 id="小结">小结</h1>
<p>Gemma2集合了一些模型、训练上的改进，最大的点应该就是知识蒸馏，而结构上的soft-cappint看来也有一些效果。另外巨大的数据量再次证明了中/小模型还能吸收更多的数据。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a><br>
<a href="https://www.linsight.cn/dd614e12.html">大模型算法题(7)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】Gemma 2: Improving Open Language Models at a Practical Size
https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf<br>
【2】https://huggingface.co/blog/gemma2<br>
【3】稀疏注意力计算:sliding window attention
http://www.linsight.cn/c61d17e3.html</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>技术报告</tag>
        <tag>Gemma2</tag>
      </tags>
  </entry>
  <entry>
    <title>Llama3.1--预训练要点一览</title>
    <url>/7d7294cb.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>最近Llama-3.1-405B模型放出，从官方的评测结果看，已经超越了GPT-4-0125，基本达到顶尖闭源模型Claude-3.5-Sonnet和GPT-4-OMNI的水平；而更小规模的8B和70B模型相比其他同规模模型优势更加明显：</p>
<img src="/7d7294cb/eval.png" class title="评测">
<p>Meta还放出了将近100页的Llama-3技术报告，披露了一些方案的细节，从中还是能得到很多有用的信息的。本篇先梳理一下预训练相关的内容。</p>
<p>（最近这两周真是大新闻一个接一个啊，这不Mistral Large
V2又出来了；另外Llama-3仍然不支持中文，某种角度上算是利好中文大模型开发者吧）</p>
<h1 id="llama-3家族">Llama-3家族</h1>
<p>Llama-3家族包括早先发布的8B/70B模型，以及最近刚放出来的Llama-3.1系列的8B/70/405B模型：</p>
<img src="/7d7294cb/model.png" class title="模型">
<p>在这些模型里，Llama-3.1-405B作为最强的旗舰模型，也是大家最关注的，报告里基本也是以405B模型为主介绍的。</p>
<h2 id="关于405b模型">关于405B模型</h2>
<p>405B模型总共在15.6T
token上进行预训练，并且支持128k的窗口长度。这个数据量是Llama-2的将近9倍（15.6T
vs 1.8T），而总训练计算量也达到了Llama-2-70B训练的50+倍。</p>
<p>405B × 15.6T这个模型规模和数据量是根据Meta实验出来的scaling
law计算出来的（后面会讲到）。对于较小的模型，Meta进行了“比scaling
law建议的compute-optimal
token数”更多的训练，而从结果上看，效果确实也有进一步的提升。</p>
<p>405B这个规模的dense模型在如今MoE的潮流中显得有些“复古”。Meta对此给出了解释：不做成MoE模型是因为要追求能力的最大化（通常来说，相同总参数量下dense模型还是比MoE要强一些的），同时使用标准的Transformer模型可以让训练更加稳定，毕竟这样大规模的训练成本巨大，如果中间训炸了还是比较麻烦的。包括在post-training中使用的supervised
finetuning（SFT），rejection sampling（RS），and direct preference
optimization（DPO）都是经受住了许多考验，证明有效的方案。看起来这里在路线的选择上，Meta倾向于保守一些。</p>
<p>405B模型的训练用到了16k个 H100，并行方案结合了tensor
parallelism、pipeline parallelism、context parallelism和data
parallelism，整个集群的搭建也是花了很多精力。</p>
<h2 id="模型结构">模型结构</h2>
<p>Llama-3和之前的两个版本在模型结构上没有做太多变化。少数的几个改动也是在其他工作都已经广泛应用的了：<br>
- 使用GQA，降低推理时KV cache的需求。<br>
- 训练时使用document
mask，防止各个文档关注到序列中拼接的其他无关文档；这个改动对预训练后期的长窗口训练比较重要，而对短文本的常规预训练没太大影响。<br>
- 把RoPE的base frequency增大到500,000，按《Effective long-context
scaling of foundation
models》的结果，这个数值足够支持32,768长度的窗口了。<br>
-
使用了128k大小的词表，其中100k是从tiktoken来的，其它28k用于支持非英文内容；更大的词表提供了更高的压缩率，平均每token字符数3.17--&gt;3.94。</p>
<p>具体的模型参数如下表</p>
<img src="/7d7294cb/llama3.png" class title="模型">
<h2 id="scaling-laws">Scaling Laws</h2>
<p>LLM的scaling
law可以告诉我们在给定的预算下，应该用多少的数据和训多大的模型来获得效果最佳的模型。不过在用scaling
law来预测最佳模型规模的时候，会遇到两个问题：<br>
- 现有的scaling law主要用next-token
prediction的loss来预测，但这个loss未必和下游任务的效果单调相关<br>
- 用于拟合scaling law的实验如果使用的compute
budget比较少，可能会因为随机性等引入了一些噪音，导致scaling
law拟合的结果失真</p>
<p>针对这个两个问题，Meta用一个two-stage的方法来建立downstream benchmark
performace和模型规模+数据量的关系：<br>
- 首先建立“compute-optimal model在downstream task上的negative
log-likelihood”和训练FLOPs的关系<br>
- 然后建立negative log-likelihood与task
accuracy之间的关联，这里除了scaling law
models，还用上了Llama-2中有更高训练FLOPs的模型</p>
<p>类似的方法也应用到选择pre-training data mix中。</p>
<p>具体来说，对从40M到16B的模型进行不同FLOPs的训练，得到各个compute预算下的最佳规模：</p>
<img src="/7d7294cb/scaling_law_exp.png" class title="实验">
<p>这里训练的时候根据模型大小使用了不同的lr，同时在不同的compute
budget下使用了从250k到4M不等的batch size。</p>
<p>基于这些实验结果，对给定compute budget C下的optimal number of
training token <span class="math inline">\(N^{\star}(C)\)</span>
进行拟合：</p>
<p><span class="math display">\[N^\star(C)=AC^\alpha \]</span></p>
<p>得到 <span class="math inline">\((\alpha,A)=(0.53,0.29)\)</span>，从这里推算出
<span class="math inline">\(3.8\times10^{25}\)</span> FLOPs的计compute
budget对应的最佳规模和数据量是402B和16.55T token。</p>
<p>从这些实验结果还另外得到一个发现：随着compute
budget的增加，IsoFLOPs的曲线逐渐变得平缓，这说明大规模的模型对规模和训练数据量的少量波动会更加robust，少量的波动不会对最终结果造成很大影响。</p>
<p>在这个基础上，先拟合“各个compute
budget下最佳模型在下游benchmark的正确答案上的Normalized NLL per
Char”和FLOPs之间的线性关系，再拟合Normalized NLL per
Char和下游任务accuracy的sigmoid关系。这样就建立了FLOPs和下游benchmark上accuracy的关系。在ARC
Challenge任务上的拟合情况如下</p>
<img src="/7d7294cb/scaling_law.png" class title="实验">
<p>从结果上看，这个方法预测的405B效果基本准确，偏差很小。</p>
<h1 id="pre-training">Pre-Training</h1>
<h2 id="数据">数据</h2>
<p>Llama-3的训练数据更新到2023年底。</p>
<p>1、数据获取</p>
<p>大部分数据是从网页爬取的，要经过一系列的清洗处理才可用。</p>
<p>（1）personally identifiable information（PII）and safety
filtering</p>
<p>首先就是要清洗掉和个人信息相关，以及包含成人内容的数据。</p>
<p>（2）text extraction and cleaning</p>
<p>为了提取网页数据，Meta构建了效果更好的HTML
parser，并用人工检验了效果。</p>
<p>对于数学相关的页面，特意保留了图片，因为很多公式都被渲染成了图片。</p>
<p>此外，经过实验还发现markdown格式的数据对效果有损害，因此把所有markdown
marker都干掉了。</p>
<p>（3）去重</p>
<ul>
<li>URL-level：对于同一个页面，只保留最新的版本。<br>
</li>
<li>Document-level：用MinHash做了文档级别的近似去重。<br>
</li>
<li>Line-level：和ccNet的做法相似，对于一个包含30M文档的bucket，如果某行数据重复出现超过6次就会被删除。人工检查发现这样做能够删掉一些如网页导航、cookie
warnings这样的没太大价值的数据，但是也会删掉一些高频的高质量数据，不过从结果上来看总体的正收益是比较大的。</li>
</ul>
<p>（4）Heuristic filtering</p>
<p>数据质量清洗：<br>
- 参考《Scaling language models: Methods, analysis &amp; insights from
training gopher》，用n-gram coverage
ratio过滤掉包含大量重复信息的内容（比如logging和error
messages）；这些内容在大量重复的同时又不完全相同，所以可能在去重中会被漏掉。<br>
- 参考《Exploring the limits of transfer learning with a unified
text-to-text transformer》，用dirty word counting过滤成人内容。<br>
- 通过token分布的KL散度过滤掉包含过量outlier token的内容。</p>
<p>（5）Model-based quality filtering</p>
<p>用Llama-2对数据质量做分类，然后用fasttext和DistilRoberta学习Llama-2给出的数据，用于对数据是否符合质量要求进行分类。</p>
<p>（6）Code and reasoning data</p>
<p>在代码和推理数据上，使用类似DeepSeek-Coder-V2的做法。针对包含数学推理、STEM领域推理以及与自然语言交织的代码网页，调整了HTML的提取规则、质量分类的prompt等。</p>
<p>（7）Multilingual data</p>
<p>对于多语言数据，在移除可能包含PII和成人内容的数据之后：<br>
- 用fasttext把数据进行176种语言的分类。<br>
- 进行document-level和line-level的去重。<br>
- 用每种语言各自的质量分类器过滤低质量数据。</p>
<p>并通过实验确定最终各种语言的占比，平衡英文和多语言的应答质量。</p>
<p>2、Data Mix</p>
<p>不同来源和领域的数据配比会极大影响各个下游任务效果。这里主要用到knowledge
classification和scaling law experiments来决定数据配比。</p>
<ul>
<li>Knowledge
classification：给数据进行领域的分类，并减少训练数据中某些种类的数据，比如arts和entertainment数据。<br>
</li>
<li>Scaling laws for data mix：通过在规模较小的模型对不同的data
mix分别跑scaling law的实验，来获取最佳的data mix。<br>
</li>
<li>Data mix summary：最终的数据中，约50%属于general
knowledge，25%属于数学和推理，17%的代码以及8%的多语言数据。</li>
</ul>
<p>3、Annealing Data</p>
<p>在learning
rate的退火阶段使用高质量的代码和数学数据可以提升在关键benchmark上的效果。参考《Datacomp-lm:
In search of the next generation of training sets for language
models》的做法，在退火阶段对高质量数据进行了upsampled。</p>
<p>按这个做法，在GSM8k和MATH数据集上检测了8B模型，发现都有比较大的提升，但是405B模型的提升则不大，猜测可能是因为405B模型的in-context
learning能力和推理能力本身已经比较强了，因此即使不在退火阶段使用相关高质量数据集，也已经效果比较好。</p>
<p>另外，既然annealing加入对应数据可以提升下游任务的效果，那么就可以用annealing来检测数据质量了。通过在退火阶段加入不同的数据，观察对下游任务的影响，来判断所加数据是否是高质量数据，这和《Does
your data spark joy?performance gains from domain upsampling at the end
of training》的思路类似。</p>
<h2 id="训练方案">训练方案</h2>
<p>405B模型的预训练分为3个阶段：<br>
- initial pre-training<br>
- long-context pre-training<br>
- annealing</p>
<p>（1）initial pre-training</p>
<p>一些训练设置：<br>
- cosine learning rate schedule<br>
- peark lr = 8e-5<br>
- batch size schedule：最开始用长度4k的窗口训练，batch
size为4M；训练到252M（个人觉得这里可能是写错了，应该是252B）
token之后，把窗口长度提升到8k，batch size也增大到8M
token；在训练了2.87T之后，再次把长度double，batch
size变成16M；这样的batch size schedule更加稳定，突刺更少出现</p>
<p>在训练的后期还加入了更多时间上更新的网络数据，把模型的知识截止点往后推进。</p>
<p>（2）long-context pre-training</p>
<p>Llama-3最终支持128k的窗口，但是模型并不是从8k或者16k一下子提升到128k，而是从8k开始，分6次增大窗口到128k，并且之后当模型适应了当前阶段的长度变化之后，才会继续提升到下一阶段的长度。</p>
<p>判断模型是否已经适应当前长度有两个标准：<br>
- 在短文本评测上的表现完全恢复<br>
- 对当前长度下的大海捞针任务做到100%召回</p>
<p>整个长文本训练总共训练了800B数据。</p>
<p>（3）annealing</p>
<p>在最后的40M（这里可能是写错了，应该是40B，毕竟一个step都128M了）
token数据，lr线性衰减到0，同时提高高质量数据的比例。最后，对annealing阶段的多个model
checkpoint进行平均，获得最终模型。</p>
<h1 id="小结">小结</h1>
<p>从个人角度觉得有几个点可以参考：<br>
- 使用annealing来发现有价值的预训练数据<br>
- 长文本的curriculum learning，逐步扩展<br>
- 通过scaling
law把FLOPs和下游任务效果关联起来，但是这个成本比较高，一般机构直接用结果就行了<br>
- 基于和下游任务效果关联的scaling law选择data mix，同样是大力出奇迹，all
you need is money<br>
- checkpoint average，和苹果用到的model soup类似，是个值得关注的技巧</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】The Llama 3 Herd of Models
https://ai.meta.com/research/publications/the-llama-3-herd-of-models/<br>
【2】https://ai.meta.com/blog/meta-llama-3-1/</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>预训练</tag>
        <tag>技术报告</tag>
        <tag>Meta</tag>
        <tag>Llama</tag>
      </tags>
  </entry>
  <entry>
    <title>Qwen2技术报告</title>
    <url>/a8f8b641.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>不久前Qwen2发布了4个dense模型和1个MoE模型，模型规模从0.5B到57B，实用效果都还不错。现在技术报告终于来了，来看下技术报告里披露了那些有用的信息。</p>
<h1 id="模型">模型</h1>
<p>Qwen2的5个模型结构和训练token数如下表</p>
<img src="/a8f8b641/model.png" class title="Qwen2">
<h2 id="tokenizer">tokenizer</h2>
<p>Qwen2使用和Qwen1一样的tokenizer，压缩率比较好，也支持多语言。</p>
<p>词表包含151,643个常规token和3个control
token。而在训练的时候，为了方便分布式训练，实际的vocab
size设到了151936，多出来的部分实际是没有用到的。</p>
<h2 id="dense-model">dense model</h2>
<ul>
<li>和之前的版本不同，Qwen2都使用了GQA而不是MHA。<br>
</li>
<li>SwiGLU、RoPE、RMSNorm + pre-norm和之前一样，属于常规设置。<br>
</li>
<li>参考了苏神在《Bias项的神奇作用：RoPE + Bias =
更好的长度外推性》里的做法，在QKV加上bias以提升RoPE长度外推的能力。<br>
</li>
<li>参考《Training-free long-context scaling of large language
models》，实现了Dual Chunk Attention（DCA），并使用YaRN对attention
weights进行rescale以获取更好的长度外推效果。</li>
</ul>
<h2 id="moe-model">MoE model</h2>
<p>Qwen2-57B-A14B使用了fine-grained expert和shared
expert，都是已经证明效果比较好的做法。</p>
<p>Qwen2-57B-A14B是从Qwen2-7B初始化的，类似《Sparse upcycling: Training
mixture-ofexperts from dense
checkpoints》的做法，但是更加强调了在细粒度专家之间实现多样化。</p>
<p>假设专家大小为h_E,
专家数量为n，用于初始化MoE模型的原始FFN层大小为h_FFN，那么FFN层将被复制
⌈n × h_E / h_FFN⌉ 次。这样可以确保和任意专家大小和专家数量兼容。</p>
<p>为了促进每个FFN
copy内部的多样性，参数在intermeidate维度会做shuffle。这样使得后面得到的每个细粒度专家都能从不同特征初始化。</p>
<p>在这个基础上，每个细粒度专家内部有50%的参数会用随机初始化覆盖，只保留50%训练过的参数。这样可以增强模型在训练中探索的能力。</p>
<h1 id="预训练">预训练</h1>
<h2 id="预训练数据">预训练数据</h2>
<p>Qwen2预训练语料的处理包含了几个关键领域：<br>
- Quality
Enhancement：包括使用之前版本的Qwen模型来过滤掉低质量数据，和合成高质量的预训练数据。<br>
- Data
Expansion：相比Qwen1.5，Qwen2多收集很多代码数据、数学数据和囊括30种语言的多语言数据。<br>
- Distribution
Improvement：在小规模的数据上做了数据配比的实验，优化不同来源和领域的数据混合。</p>
<p>基于以上的工作，Qwen2最终得到了7T高质量数据。除了0.5B模型，其他dense模型都是在7T数据上训练，Qwen2-57B-A14B则是在4.5T数据上训练的。而在0.5B模型上，尝试使用了放松清洗阈值而得到的12T数据进行了训练，但是相比7T高质量数据，12T数据的训练并没有带来进一步的提升。</p>
<h2 id="长上下文训练">长上下文训练</h2>
<p>在预训练的最后阶段，把训练窗口从4096提升到32,768以提升模型的长上下文能力。这个阶段使用了长度更长的文本。</p>
<p>除了数据的变化，RoPE的base也从10,000提升到1,000,000。再加上YaRN和Dual
Chunk
Attention，Qwen2模型可以在131,072长度的窗口上保持比较好的效果。</p>
<h1 id="post-training">POST-TRAINING</h1>
<p>Qwen2的对齐阶段包括SFT和RLHF。主要针对coding，mathematics，logical
reasoning，instruction following 和 multilingual comprehension
提升效果。</p>
<h2 id="数据">数据</h2>
<p>在数据的获取上，Qwen2的方法更多关注在“scalable alignment with minimal
human annotation”（《Towards scalable automated alignment of LLMs: A
survey》）。</p>
<p>构建数据的过程主要包括两个步骤，collaborative data annotation 和
automated data synthesis。</p>
<p>1、collaborative data annotation</p>
<ul>
<li>本体提取：借助InsTag（《#InsTag: Instruction tagging for analyzing
supervised fine-tuning of large language
models》）的tagger，再加上人工精炼保证本体提取的准确。<br>
</li>
<li>指令选择：每条带有tag的指令都根据tag多样性、语义丰富性、复杂性和意图完整性进行了评估。基于这些标准选择有代表性的指令（《How
abilities in large language models are affected by supervised
fine-tuning data composition》）。<br>
</li>
<li>指令进化：为了丰富指令数据集，采用了self-evolution策略（《Tree-Instruct:
A preliminary study of the intrinsic relationship between complexity and
alignment》），用Qwen模型对现有指令添加约束或要求，增加其复杂性，并确保数据集中难度级别的多样性。<br>
</li>
<li>人类打标：使用不同的生成策略和不同规模的Qwen模型获取一条指令的多个response。标注者这些response进行排序，确保最佳response符合标准，最终得到demonstration和preference数据。</li>
</ul>
<p>2、automated data synthesis</p>
<ul>
<li>拒绝采样：对于数学或类似的有明确最终答案的任务，应用了拒绝采样（《Scaling
relationship on learning mathematical reasoning with large language
models》）来提高solution的质量。LLM被用来为每条指令生成多个response。那些准确且被模型认为是合理的response就保留下来。通过对比正确和错误的response还可以获得偏好数据。<br>
</li>
<li>执行反馈：对于编程任务，LLM被用来生成solution和相关测试用例。这些solution的有效性通过执行测试用例来评估。这种方法也适用于评估指令遵循情况（《Self-play
with execution feedback: Improving instruction-following capabilities of
large language
models》）。比如对有长度限制的指令，LLM的任务是生成一个Python验证函数，以确保response的长度要求。<br>
</li>
<li>数据再利用：对于没有专门训练的标注者来说，在文学写作任务中给出好的答案是很困难的。为了解决这个问题，收集了高质量的文学作品，并使用LLM开发不同详细程度的指令。这些指令与原作品配对，作为训练数据。比如为了获取角色扮演数据，先从知识库（如维基百科）中获取详细的角色资料，并指导LLM生成相应的指令和response（《Large
language models are superpositions of all characters: Attaining
arbitrary role-play via
self-alignment》）。这个过程类似于阅读理解任务，确保了角色资料的完整性。<br>
</li>
<li>Constitutional Feedback：参考《Constitutional AI: Harmlessness from
AI
feedback》的做法，制定了response要遵循的规则和原则，用于知道模型生成合情合理合法合规的response。</li>
</ul>
<h2 id="sft">SFT</h2>
<ul>
<li>数量&gt;500,000条样本上训练<br>
</li>
<li>训练2个epoch<br>
</li>
<li>lr = 7e-6，最终decay到7e-7<br>
</li>
<li>weight decay = 0.1<br>
</li>
<li>gradient clip = 1.0<br>
</li>
<li>seq length = 32,768</li>
</ul>
<h2 id="rlhf">RLHF</h2>
<p>强化学习使用DPO，并参照《Online merging optimizers for boosting
rewards and mitigating tax in alignment》，用Online Merging
Optimizer以缓解alignment tax的影响。</p>
<h1 id="评测">评测</h1>
<h2 id="base模型">base模型</h2>
<p>各个规模的base模型评测结果如下。</p>
<p>1、0.5B模型和1.5B模型</p>
<img src="/a8f8b641/eval_base_small.png" class title="评测">
<p>2、7B模型</p>
<img src="/a8f8b641/eval_base_7B.png" class title="评测">
<p>3、32B模型和57B-A14B模型</p>
<img src="/a8f8b641/eval_base_large.png" class title="评测">
<h2 id="instruction-tuned模型">INSTRUCTION-TUNED模型</h2>
<p>各个规模的it模型评测结果如下。</p>
<p>1、0.5B模型和1.5B模型</p>
<img src="/a8f8b641/eval_chat_small.png" class title="评测">
<p>2、7B模型</p>
<img src="/a8f8b641/eval_chat_7B.png" class title="评测">
<p>3、32B模型和57B-A14B模型</p>
<img src="/a8f8b641/eval_chat_large.png" class title="评测">
<h2 id="长窗口">长窗口</h2>
<p>Qwen2模型的长窗口能力在3个评测集上进行了评估。</p>
<p>1、the Needle in a Haystack</p>
<img src="/a8f8b641/eval_needle.png" class title="评测">
<p>2、NeedleBench（OpenCompass）</p>
<p>（见下图）</p>
<p>3、LV-Eval</p>
<img src="/a8f8b641/eval_long.png" class title="评测">
<h1 id="小结">小结</h1>
<ul>
<li>Qwen2
MoE模型的初始化思路可以作为从dense模型upcycling的一个参考。<br>
</li>
<li>预训练数据量来到10T
token，这里12T训练数据没有更大收益的原因，除了数据质量外，猜测可能是0.5B模型本身容量有限导致。<br>
</li>
<li>在数据配比上，报告没有给出太多信息，但这块很重要，各家应该有些压箱底信息没有舍得给出来。</li>
</ul>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a><br>
<a href="https://www.linsight.cn/dd614e12.html">大模型算法题(7)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】QWEN2 TECHNICAL REPORT https://arxiv.org/abs/2407.10671</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>预训练</tag>
        <tag>技术报告</tag>
        <tag>MoE</tag>
        <tag>Qwen</tag>
        <tag>对齐</tag>
      </tags>
  </entry>
  <entry>
    <title>Llama3.1--post-training要点一览</title>
    <url>/93328a2a.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>书接上回：<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a>，继续整理一下Llama-3.1中post-training的内容。</p>
<p>在Llama-3的报告中，任何在pre-training之后发生的训练都属于post-training，包括SFT、DPO等。</p>
<p>Llama-3的post-training不是一次完成的，而是多个round迭代进行，整个post-training包含6轮的SFT和DPO。</p>
<h1 id="modeling">Modeling</h1>
<p>post-training的流程如下图</p>
<img src="/93328a2a/post_training.png" class title="post-training">
<h2 id="chat-dialog-format">Chat Dialog Format</h2>
<p>Llama-3相比之前的版本多了一些能力，比如tool
use。在这些场景下，模型可能生成多个结果，并发送给不同的接收方，之后再由模型汇总各方结果。为了支持这些能力，Meta设计了multi-message
chat protocol。</p>
<h2 id="reward-modeling">Reward Modeling</h2>
<p>reward model（RM）是post-training中的一个重要部分。</p>
<p>和Llama-2相比，这次RM的一个变化是移除了训练时加入的margin
term（用于把chosen和rejected
response区分得更开），因为随着模型规模的增大，加入margin
term收益越来越小了。</p>
<p>另一方面，同Llama-2一样，preference
data中只有区分度比较大的数据对用于训练RM。</p>
<p>数据上，除了常规的chosen和rejected response之外，还引入了第三种 --
“edited
response”，即在chosen的基础上通过（人工）编辑，进一步提升这条response的质量。这样每条ranking
sample就可能有3条response（edited &gt; chosen &gt; rejected）。</p>
<p>训练的时候，prompt和对应的多条随机打乱的response拼接在一起训练（prompt
+ resp_1 + resp_2 +
resp_3），这和通常的做法，即每个response都拼接prompt有些不同（prompt +
resp_1, prompt + resp_2, prompt +
resp_3）。从结果上来看，都拼接到一起在accuracy上没有什么损失，而训练效率更高。（个人理解这里可能是通过乐死document
mask来实现的）</p>
<h2 id="sft">SFT</h2>
<p>训练好的RM模型会用于rejection sampling，对human annotation
prompt的不同生成结果进行过滤。得到的高质量数据会和其他来源的SFT数据一起用来微调模型。</p>
<p>SFT训练的时候使用lr=1e-5，步数为8.5k~9k步。实践上这样的参数设置在多轮的post-training中都能保持较好的效果。</p>
<h2 id="dpo">DPO</h2>
<p>在DPO阶段，会用在上一轮post-training得到的最佳模型收集偏好数据对，这样能使得偏好数据的分布和强化学习时的policy
model更一致。</p>
<p>除了DPO以外，Meta也尝试了一些on-policy的方案，如PPO。但是相对来说，DPO消耗更少的计算资源，并且效果也更好，特别是在instruction
following的能力上，所以还是选择在post-training使用DPO。</p>
<p>DPO训练中，使用lr=1e-5，beta=0.1。</p>
<p>此外，训练中还做了一些不同于标准做法的改动：</p>
<p>1、Masking out formatting tokens in DPO loss</p>
<p>把特殊token比如header和termination
token屏蔽，不用于计算训练loss。因为使用这些token计算loss会使得模型在生成时，出现如复读机或者在不合适的地方截断的情况。这可能就是因为chosen
repsponse和rejected
response同时包含的这些特殊token，让模型在训练时要同时增大和较小它们的likelihood，导致冲突。</p>
<p>2、Regularization with NLL loss</p>
<p>除了DPO的常规loss，Meta额外加入了NLL损失项，这和《Iterative reasoning
preference optimization》的做法类似。这也有点像PPO里加入next token
prediction
loss，能使训练更加稳定，并能保持SFT学到的生成格式，并保持chosen
response的log probability不下降（《Smaug: Fixing failure modes of
preference optimisation with dpo-positive》）。</p>
<h2 id="model-averaging">Model Averaging</h2>
<p>参考《Averaging weights leads to wider optima and better
generalization》《Model soups: averaging weights of multiple fine-tuned
models improves accuracy without increasing inference
time》和《Branch-train-merge: Embarrassingly parallel training of expert
language
models》，在RM、SFT和DPO阶段，分别把“用不同版本的数据和超参训练得到模型”进行平均，以获得最终模型。</p>
<h1 id="数据">数据</h1>
<h2 id="偏好数据">偏好数据</h2>
<p>偏好数据的处理过程和Llama-2类似。</p>
<p>首先，在每轮训练完后部署一批“在不同数据、超参、训练策略上训练”得到的模型，这些模型有各自的特点，比如有些擅长写代码，有些擅长数学推理。</p>
<p>对于每个user
prompt，从这些模型里采样两个response。之后标注人员给每对chosen和rejected
response分成4类：<br>
- significantly better - better<br>
- slightly better<br>
- marginally better</p>
<p>过程中标注人员也可以对chosen
response进一步编辑，获得更好的response。</p>
<p>下表给出了偏好数据的的统计：</p>
<img src="/93328a2a/preference_data.png" class title="preference data">
<p>相比Llama-2的数据，Llama-3所用的prompt和response的长度都有所增加，这说明Llama-3的任务复杂度提升了。</p>
<p>在每一轮的post-training之后，都会分析当前版本模型效果不好的领域，并针对这些领域提升prompt的复杂度。</p>
<p>每轮post-training中，训练RM的时候，会使用所有来自不同轮所收集到的偏好数据。而DPO训练则只会用到最新的偏好数据。</p>
<p>对于RM和DPO，都只使用分类为significantly better 和
better的数据进行训练，而另外两类质量相近的偏好数据对则被丢弃。</p>
<h2 id="sft-data">SFT Data</h2>
<p>SFT数据主要有这几个来源：<br>
- 人工收集的prompt，以及对应的通过拒绝采样得到的response<br>
- 特定领域的合成数据（后面capacities部分会讲到）<br>
- 少量人类真实数据</p>
<p>1、拒绝采样（RS）</p>
<p>在RS阶段，每个prompt会从“最新的/领域最佳的chat模型”采样K个回复（一般10~30个），然后用RM选出最佳回复（《Constitutional
AI: harmlessness from AI feedback》）。</p>
<p>在靠后轮次的post-training里，RS引入了控制风格、格式、语气等特性的system
prompt以更精细地控制数据质量。不同的领域（如代码、推理、工具使用等）可能会采用不同的prompt。</p>
<p>2、数据组成</p>
<p>下表给出了helpful数据中每个大类别的数据统计：</p>
<img src="/93328a2a/sft_data.png" class title="sft data">
<h2 id="数据处理-质量控制">数据处理 &amp; 质量控制</h2>
<p>由于大部分数据都是模型生成的，所以需要仔细地清洗。</p>
<p>1、数据清洗</p>
<p>在post-training的前几轮中，研究人员发现数据中混入了一些包含过量emoji或者感叹号之类的数据，因此用专门的规则对发现的低质量pattern进行了清洗。此外有些数据还有overly-apologetic（比如模型经常回复“我很抱歉”）的问题，也会有规则识别如“I‘m
sorry”这样的内容，并降低这类数据的比例。</p>
<p>2、Data pruning</p>
<p>一系列model-based的方法用来过滤低质量数据：<br>
- 话题分类：用Llama-3-8B来做粗粒度 &amp; 细粒度的领域分类。<br>
-
质量打分：一方面，RM模型会用于识别高质量回复，只有RM得分在前四分之一的数据被认为是高质量的；另一方面，基于Llama-3
checkpoint，使用特定的prompt（不同领域prompt可能不同）进行多个方面的打分，只有得分最高的数据会被认为是高质量的。RM和Llama-3二者的高质量数据识别一致性并不高，实践上，取二者结果的并集对高质量数据的召回效果最好。<br>
- 难度打分：用了Instag和Llama模型打分两种方式来衡量数据的难度。<br>
- 语义去重：用Roberta对对话进行聚类，在每个类别中按quality score ×
difficulty
score进行排序，然后只保留“和已选高质量数据相似度小于阈值”的样本（《Semdedup:
Data-efficient learning at web-scale through semantic
deduplication》、《What makes good data for alignment? a comprehensive
study of automatic data selection in instruction tuning》）</p>
<h1 id="capabilities">Capabilities</h1>
<p>在不同的具体领域上，Meta分别有一套方法，来提升对应的能力。</p>
<h2 id="code">Code</h2>
<p>代码上，要提升的目标语言包括：Python, Java, Javascript, C/C++,
Typescript, Rust, PHP, HTML/CSS, SQL, bash/shell。</p>
<p>代码能力提升的方法包括训练code
expert、生成数据用于SFT训练、通过system prompt调整格式以及使用quality
filter过滤低质量数据。</p>
<h3 id="expert-training">Expert training</h3>
<p>首先，在主预训练模型的基础上，增加1T的代码继续预训练，其中&gt;85%的样本是代码数据。然后采用和CodeLlama类似的方法训练code
expert。</p>
<p>在训练的最后几千个step，会加入repo-level的长代码数据，以提升code
expert的长窗口能力。</p>
<p>继续预训练之后会采用前面提到的方法进行post-training，只是所用数据主要是代码数据。</p>
<p>得到的code expert用于：<br>
- 在主模型的post-training中获取高质量的代码数据<br>
- code prompt的rejection sampling</p>
<h3 id="合成数据">合成数据</h3>
<p>生成的代码会存在一些问题，包括难以遵循指令、语法错误、生成错误代码和难以修复错误等。</p>
<p>虽然人工标注理论上可以解决这些问题，但合成数据的成本更低、更方便扩展到更大规模，因此还是使用Llama
3和code expert生成大量SFT合成数据。</p>
<h3 id="代码生成的方法">代码生成的方法</h3>
<p>基于以下三个方法，一共生成了超过2.7M的代码SFT数据。</p>
<p>1、执行反馈</p>
<p>Llama-3的8B和70B模型在用更大的模型（比如405B）所生成的数据训练时，获得了明显的收益。但是405B模型在用自己生成的数据训练之后（毕竟这个规模下很难有更大的模型了），不仅没有提升，甚至还有些退化。</p>
<p>为了解决这个问题，Meta引入了execution
feedback，来对代码进行正确性校验，并让模型从错误中学习。</p>
<p>具体来说，用以下的过程获得了1M左右的训练数据：</p>
<p>（1）生成问题描述</p>
<p>这一步生成大量涵盖广泛主题的编程问题描述。为了增加多样性，从不同的来源随机抽取代码片段，然后根据代码片对生成对应的问题描述。（《Magicoder:
Empowering code generation with oss-instruct》）</p>
<p>（2）Solution生成</p>
<p>这一步用Llama-3生成代码问题的答案。</p>
<p>这个过程中，会在prompt里加入优质代码的general
rule，并要求模型在注释里给出思路。这两个做法能有效促进代码质量的提升。</p>
<p>（3）正确性分析</p>
<p>检查生成的solution正确性包括两个方面。</p>
<p>一是静态分析，即通过parser和linter保证基础的语法正确性。</p>
<p>另一个是动态检查，通过让模型给代码生成单元测试并执行来判断代码的正确性。</p>
<p>（4）错误反馈 &amp; 迭代修正</p>
<p>对于有问题的代码，并不是直接舍弃，而是让模型修改优化。</p>
<p>通过prompt把错误信息给到模型，不断迭代修改，直到代码通过所有单元测试。</p>
<p>原数据里大概有20%的样本通过这样的修改才通过测试，说明如果不对正确性进行校验的话，会在训练数据里引入大量的错误信息。</p>
<p>（5）微调 &amp; 迭代优化</p>
<p>微调迭代了多个round，每个round产生的模型都用来生成新的数据给下一次迭代训练。</p>
<p>2、programming language translation</p>
<p>不同语言的代码数据量有不平衡的情况，因此Meta基于Llama-3把高频语言的代码“翻译”成低频语言的数据，并通过syntax
parsing, compilation, execution等来保证翻译数据的质量。（类似《Breaking
language barriers in multilingual mathematical reasoning: Insights and
observations》的思路）</p>
<p>3、backtranslation</p>
<p>在代码相关的能力如documentation、debugging和explanation上，执行+反馈的做法并不适用。</p>
<p>因而采用一个多步方法backtranslation，从代码片段开始：<br>
- Generate：让模型先生成，比如文档，或者代码功能解释<br>
- Backtranslate：再要求用生成的文档或者功能说明生成代码<br>
-
Filter：如果第二步生成的代码和原代码一致性够高，则说明生成的文档/代码解释好用，可作为训练数据</p>
<p>通过backtranslation，大约获得了1.2M的documentation、debugging和explanation等数据。</p>
<h3 id="其他">其他</h3>
<p>1、system prompt</p>
<p>使用代码专用的system
prompt可以提高生成数据的质量，下图是一个样例，右边多了comment，变量名更为合理，还更省空间。</p>
<img src="/93328a2a/code_sample.png" class title="样例">
<p>2、Filtering training data with execution and model-as-judge
signals</p>
<p>rejection
sampling的过程会遇到有问题的代码，但是检验这些代码并不是想象中的那么straightforward，比如生成的内容可能包含了不能执行的内容（如伪代码），或者用户要求生成的是完整代码的一个小片段（无法单独执行），这些都无法直接通过单元测试来检验。</p>
<p>因此使用“model-as-judge”的方法，即通过Llama-3对生成内容做正确性和风格好坏的二分类，只有当二者都被分为好，对应的代码数据才会被使用。</p>
<p>但是这种方法会倾向于保留简单任务（因为复杂的任务更容易出现问题），导致模型在复杂问题上的能力受损。因此研究人员还专门人为地修改了困难任务上的response，直到这些response符合Llama-3的要求。</p>
<h2 id="多语言">多语言</h2>
<p>Llama-3支持8种语言：German, French, Italian, Portuguese, Hindi,
Spanish, Thai。</p>
<h3 id="expert-training-1">Expert training</h3>
<p>用包含超过90%的多语言（即除英语以外的语言）的data
mix，对主预训练模型做继续预训练，之后再进行同code
expert类似的post-training。得到的多语言expert
model用于收集高质量的非英文数据。</p>
<h3 id="多语言数据收集">多语言数据收集</h3>
<p>多语言的SFT数据中，包含：<br>
- 2.4%的人类数据<br>
- 44.2%的NLP task数据<br>
- 18.8%来自rejection sampling<br>
- 34.6%来自translated reasoning data</p>
<p>1、人类数据</p>
<p>这部分都是从native
speaker收集的，大部分包含开放的多轮对话，代表了真实世界的数据。</p>
<p>2、NLP task</p>
<ul>
<li>把常规NLP任务改写成对话格式。<br>
</li>
<li>为了提升语言的alignment，使用了来自《Parallel global voices: a
collection of multilingual corpora with citizen media
stories》和Wikimedia的parallel text。<br>
</li>
<li>用LID based filtering和Blaser2.0 （《Seamlessm4t—massively
multilingual &amp; multimodal machine
translation》）清洗掉低质量数据。</li>
</ul>
<p>3、拒绝采样数据</p>
<p>相比英文数据，多语言数据的RS做了几点改动：<br>
-
Generation：在post-training的前几轮中，使用0.2~1.0的随机温度来生成回复，以提升多样性。而在最后一轮中，则使用0.6的温度，以保持生成结果中创新性和流畅性的平衡。<br>
-
Selection：在RM模型之前，对prompt和response做了语言检查，保证语言的匹配性（比如不会出现一种语言问，另一种语言回答，除非明确要求）。</p>
<p>4、翻译数据</p>
<p>大部分数据都没有做翻译，以避免引入翻译腔等问题，除了一个例外：synthetic
quantitative reasoning data。</p>
<p>这类数据的语言描述通常比较简单，所以翻译之后没有什么质量问题，而推理数据可以帮助改善多语言的定量推理能力。</p>
<h2 id="math-and-reasoning">Math and Reasoning</h2>
<p>reasoning被定义为“执行多步计算并得出最终正确答案”的能力。</p>
<p>reasoning能力的训练有几个挑战：<br>
- 缺少prompt：这种高难度的任务数据相对较少<br>
-
缺少正确的CoT：reasoning任务一般有多步，包含这些多步CoT的正确答案的数据也不多<br>
- 错误的中间步骤：基于模型生成的CoT很容易有错误的中间步骤<br>
- 使用外部工具：教会模型使用外部工具能极大提升效果，但这并不容易<br>
-
训练与推理的差异：推理的时候可能需要在中间和用户进行交互获取反馈，这可能和训练数据不完全一致</p>
<p>针对这些问题，Meta给出以下解决方案。</p>
<p>1、解决缺少prompt的问题</p>
<p>为了解决缺少prompt的问题，研究人员从数学相关的context抽取数据片段并转换为对话形式。</p>
<p>对于模型表现不好的数学领域，专门收集了人类的prompt。为此构建了数学相关的分类体系（《Metacognitive
capabilities of llms: An exploration in mathematical problem
solving》），并让人类专家提供相应的prompt和问题。</p>
<p>2、Augmenting training data with step-wise reasoning traces</p>
<p>就是用Llama-3为一系列的prompt生成step-by-step的解决方案。</p>
<p>对于每个prompt，模型会生成不同数量的结果。这些生成结果随后根据正确答案进行筛选（《Common
7b language models already possess strong math capabilities》）。</p>
<p>此外还进行了自我验证，即使用Llama-3来验证给定的步骤解决方案对于特定问题是否有效。</p>
<p>3、Filtering incorrect reasoning trace</p>
<p>训练outcome RM和stepwise RM来把中间过程错误的数据清洗掉（《Let’s
verify step by step》，《Math-shepherd:Verify and reinforce llms
step-by-step without human annotations》）。</p>
<p>对于更难的prompt，使用Monte Carlo Tree Search (MCTS)来处理（《Monte
carlo tree search boosts reasoning via iterative preference
learning》）。</p>
<p>4、Interleaving code and text reasoning</p>
<p>在文本推理之外，加上python
code的执行反馈来对结果正确性做进一步确认（《Tora: A tool-integrated
reasoning agent for mathematical problem solving》）。</p>
<p>5、Learning from feedback and mistakes</p>
<p>为了模仿人类的反馈，使用包含错误的生成结果，并要求模型给出修正（《Learning
from mistakes makes llm better reasoner》，《Generating sequences by
learning to self-correct》，《Self-refine: Iterative refinement with
self-feedback》）。</p>
<h2 id="long-context">Long Context</h2>
<p>在预训练的最后阶段，训练窗口从8k扩展到128k。</p>
<p>而和预训练相似，在post-training阶段也需要仔细平衡模型的短文本能力和长文本能力。</p>
<p>1、SFT</p>
<p>如果直接把常规的、较短的SFT数据应用在预训练模型上做SFT，会使得预训练阶段得到的长文本能力退化，因此SFT阶段必须加上长数据。</p>
<p>由于让人类来给出超长（128k）的SFT数据，难度太大耗时太长，并不现实，所以主要还是依赖合成数据。</p>
<p>用早期的Llama-3版本来生成长文本关键场景的数据，比如多轮问答、长文本摘要和代码仓库级别的reasoning。</p>
<p>（1）Question answering</p>
<p>从预训练数据里筛选一些长文档，并把它们切分为8k的片段，之后让（短窗口）模型对随机选择的片段生成QA数据。长文本训练时则是把完整的文档和相关的QA作为输入。</p>
<p>（2）Summarization</p>
<p>摘要采用层次化的方式，即先用8k的模型对长文档的每个8k片段进行摘要，多个片段摘要合在一起再进行二次摘要，获得最终结果。</p>
<p>此外，Meta还基于文档摘要生成QA对，要求模型回答那些需要对文档做全面理解的问题。</p>
<p>（3）Long context code reasoning</p>
<p>首先解析Python文件，识别导入语句并确定它们的依赖关系。</p>
<p>接下来，对那些被至少五个其他文件使用的文件，随机删除一个，训练时要求模型识别哪些文件依赖于被删除的文件，并生成所需的缺失代码。</p>
<p>以上这些数据都被分成16K, 32K,
64K和128K的长度，方便进行细粒度的微调。</p>
<p>另外，消融实验发现，在原SFT数据中混入0.1%的这些合成的长文本，对模型的短文本和长文本能力都有提升。</p>
<p>2、DPO</p>
<p>实验发现DPO阶段仅使用短文本并不会对模型长文本能力造成明显影响，可能是因为DPO的更新步数比较少，因此DPO没有特意增加长文本数据。</p>
<h2 id="tool-use">Tool Use</h2>
<p>使用工具的能力可以拓展模型的能力边界，让模型从单纯的聊天机器人变成有用的智能助手。Llama-3被训练使用以下core
tools：<br>
- 搜索引擎：Brave Search<br>
- Python interpreter：用于执行生成的代码<br>
- Mathematical computational engine：Wolfram Alpha API</p>
<p>当用户的query需要用到多个工具时，Llama-3可以给出plan，对工具进行串行调用，并在每次调用之后进行推理整合。</p>
<p>除了core
tool之外，Llama-3还有zero-shot的工具调用能力，能根据query调用此前没见过的用户定义的工具。</p>
<p>1、Implementation</p>
<p>Meta将core tools实现为具有不同方法的Python对象。</p>
<p>而zero-shot
tool可以作为带有描述、文档（使用示例）的Python函数来实现，模型只需要函数的签名和文档字符串作为上下文来生成适当的调用。</p>
<p>函数的定义和调用都转换为json格式，例如用于Web API调用。</p>
<p>所有工具调用都由Python解释器执行，且需要在Llama-3的system
prompt中启用（即告诉模型可以使用哪些工具能力）。core tool可以在system
prompt中单独启用或禁用。</p>
<p>2、Data collection</p>
<p>与ToolFormer不同，Llama-3主要依赖人类的标注数据和偏好数据来训练。</p>
<p>人类标注员对模型给出的多个message进行排序，如果两个都不好，就手动编辑一个好的，并让对话继续。</p>
<p>工具使用的训练没有使用rejection
sampling，因为实践上来看这样做没有效果。</p>
<p>为了减少标注的人力投入，会先进行基本的finetune让模型具备基本的工具使用能力，并且会先从单轮对话开始，慢慢迭代到多轮对话。</p>
<p>3、Tool datasets</p>
<p>通过以下方法来获取数据。</p>
<p>（1）Single-step tool use</p>
<p>先用few-shot prompt让模型生成core
tools的调用，之后要求模型基于用户query和调用结果回答问题。</p>
<p>顺序如下：system prompt, user prompt, tool call, tool output, final
answer。</p>
<p>生成的数据里有30%的数据有诸如无法执行，或者有格式问题，就被清除掉了。</p>
<p>（2）Multi-step tool use</p>
<p>先让Llama-3生成至少需要调用2次core
tool（可以相同也可以不同）的prompt，然后再用few shot
prompt让Llama-3生成一个由交错推理步骤和工具调用组成的解决方案，和ReAct类似。下图是一个多步工具调用的例子：</p>
<img src="/93328a2a/multi_step_tool.png" class title="多步工具调用">
<p>（3）File uploads</p>
<p>使用这些格式的文件：.txt, .docx, .pdf, .pptx, .xlsx, .csv, .tsv, .py,
.json, .jsonl, .html, .xml。</p>
<p>基于上传的文件，要求模型进行摘要生成、查找并修复错误、优化代码片段、执行数据分析和可视化等任务。下图是一个示例</p>
<img src="/93328a2a/file_upload.png" class title="文件上传样例">
<p>在使用这些合成数据进行了微调之后，Meta进一步收集了多样化且具有挑战性的任务数据，包括多轮交互、三个以上步骤的工具使用，以及工具调用未能得到满意答案的case。</p>
<p>为了让模型避免对简单的query调用工具，使用了简单数学或问答数据集的query，及其不使用工具的response，但在system
prompt中激活了工具。这样模型就能学到，即使工具时available的，但是对于简单问题可以不调用工具，避免了工具滥用。</p>
<p>4、Zero-shot tool use data</p>
<p>通过在一个大型的多样化（合成）数据集上微调，提高了Llama-3的zero-shot工具使用能力（函数调用）。</p>
<p>数据包括函数定义、用户query和相应的调用。然后另一批从未见过的工具上进行评测。</p>
<p>（1）Single, nested, and parallel function calling</p>
<p>函数的调用情况有多重，可以是简单的单次调用，也可以是嵌套的（即将一个函数调用作为另一个函数的参数），或者是并行的（即模型返回一个独立的函数调用列表）。</p>
<p>要生成多样化的工具调用数据并不容易（《Toolverifier: Generalization to
new tools via self-verification》），因此通过在Stack里（《The stack: 3
tb of permissively licensed source
code》）进行挖掘，确保函数调用和定义是真实的。即从里面提取出真实的函数调用和定义，过滤掉如文档有问题或者无法执行的函数，之后用Llama-3生成函数调用的query。</p>
<p>（2）Multi-turn function calling</p>
<p>参照《Api-bank: A comprehensive benchmark for tool-augmented
llms》的做法，为带有函数调用的多轮对话生成了合成数据。</p>
<p>通过使用不同的prompt，让Llama-3扮演不同的agent，分别用于生成domains,
APIs, user queries, API calls, 和 responses。</p>
<h2 id="factuality">Factuality</h2>
<p>Hallucination依然是大模型的一个问题。即使在模型不怎么了解的领域，模型也会给出很自信的回答，这就会给大模型的使用带来风险。</p>
<p>Meta遵循的原则是，post-training应该使模型 “know what it knows”
，而不是增加知识（《Does fine-tuning llms on new knowledge encourage
hallucinations?》，《Linguistic calibration through metacognition:
aligning dialogue agent responses with expected correctness》）。</p>
<p>主要方法是生成数据 --
生成与预训练数据中存在的实际数据保持一致的微调数据。</p>
<p>为了实现这一点，Meta开发了一种基于Llama-3的in-context能力的knowledge
probing技术。</p>
<p>这个数据生成过程包括以下步骤：</p>
<ul>
<li>从预训练数据抽取一个片段<br>
</li>
<li>用Llama-3对这个片段生成一个事实性问题<br>
</li>
<li>用Llama-3采样这个问题的答案<br>
</li>
<li>用原片段的context对生成答案的正确性进行打分<br>
</li>
<li>对生成结果的informativeness进行打分<br>
</li>
<li>用Llama-3生成对“信息丰富但错误的response”的refusal</li>
</ul>
<p>Meta使用knowledge
probing生成的数据，来鼓励模型只回答它有知识的问题，并拒绝回答它不确定的问题。</p>
<p>此外，预训练数据并不总是一致或正确的。因此还专门收集了一个数据集，处理那些事实矛盾或不正确陈述普遍存在的敏感话题。</p>
<h2 id="steerability">Steerability</h2>
<p>可操控性是指引导模型的行为和结果以满足开发者和用户需求的能力。</p>
<p>由于Llama-3是一个通用的基础模型，它应该具备在不同使用场景下的可操控性。</p>
<p>Meta主要通过system
prompt来增强Llama-3的可操控性，特别是在response长度、格式、语气等方面。</p>
<p>数据收集上，首先要求annotator为Llama-3设计不同的system
prompt，然后，annotator与模型进行对话，评估模型在对话过程中遵循system
prompt中定义指令的一致性，并收集偏好数据。</p>
<p>以下是一个增强可操控性的system prompt例子：</p>
<img src="/93328a2a/steerability.png" class title="steerability">
<h1 id="小结">小结</h1>
<ul>
<li>Llama-3不仅仅是一个模型，而且是一个巨大的工程<br>
</li>
<li>大量的工作仍然是在数据上，而且post-training的权重提高了许多<br>
</li>
<li>对各个领域数据的细致整理，也提醒开发者们，目前阶段的“通用能力”说到底还是多任务训练，而多任务，就需要一个领域一个领域踏实优化</li>
</ul>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】The Llama 3 Herd of Models
https://ai.meta.com/research/publications/the-llama-3-herd-of-models/</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>SFT</tag>
        <tag>技术报告</tag>
        <tag>Meta</tag>
        <tag>Llama</tag>
        <tag>post-training</tag>
        <tag>DPO</tag>
        <tag>RM</tag>
        <tag>RS</tag>
      </tags>
  </entry>
  <entry>
    <title>MoE路由--expert choice routing</title>
    <url>/2c8bbc7.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>MoE模型两大主要组件就是gating network和expert network。</p>
<p>gating决定了选择专家的方式、各个专家权重和专家数量。</p>
<p>目前大部分主流的MoE模型都是token choice routing（或者直接叫top-k
routing），即根据输入token和所有专家的匹配得分，选择匹配度最高的k个专家进行处理，以加权和作为对应token的输出。</p>
<p>那么也有反过来，根据专家和所有token的匹配度，选择每个专家处理的token的做法，就是expert
choice routing（EC）。</p>
<p>两种routing的示意图如下</p>
<img src="/2c8bbc7/intro.png" class title="routing">
<p>token choice
routing有点像大学课堂，老师就是专家，每个学生就是token，每个学生选择最适合自己的老师。而expert
choice有点像中小学课堂，由每个老师选择上课的班级。</p>
<h1 id="token-choice-routing的弊端">token choice routing的弊端</h1>
<p>虽然目前主流的MoE都是使用token choice
routing，但是它也还存在一些问题。</p>
<p>1、Load Imbalance</p>
<p>各个专家间的负载均衡问题从2017年的《Outrageously Large Neural
Networks: The Sparsely-Gated Mixture-of-Experts
Layer》里就有专门拉出来讨论。</p>
<p>负载不平衡主要是因为token
choice是独立为每个token选择k个专家，而没有考虑所选择的这k个专家是否被其他token选择。</p>
<p>如果有几个专家训练得特别好，那么它们就会很容易被各个输入token选中，从而又使得这几个专家训练得更充分，和其他没有训练足够的专家的得分差距继续拉大，出现马太效应，造成恶性训练。</p>
<p>如果负载出现不平衡的情况，会削弱MoE模型的推理效率，因为每层都要等所有专家处理完，而这又取决于负载最大的专家的耗时。</p>
<p>后续的Gshard、Switch
Transformer和ST-MoE，到现在的Qwen2-MoE和DeepSeek-MoE等，基本固定了使用多个level的负载均衡损失来缓解这个问题。</p>
<p>2、Under Specialization</p>
<p>如很多MoE模型提到的，加入负载均衡损失并不能完全解决负载问题，而如果过于强调负载均衡，使用比较大的权重系数，模型的效果也会有一定的损失。因为这样gating可能会被迫把一些token分配给没有充分训练的专家。从效果上考虑，增大负载均衡损失的权重显然不是最好的方案。</p>
<p>3、Same Compute for Every Token</p>
<p>token choice
routing有一个隐含的假设是每个输入token都需要由相同数量的expert来处理，但经验来说这并不是最合理的：一般来说，更难的token可能需要更多专家，而相对简单的token可能只需要一个专家就能解决，而不需要k个专家。</p>
<p>直接对所有token使用固定k个专家，可能限制了模型计算资源的更合理分配。</p>
<h1 id="expert-choice-routing">expert choice routing</h1>
<h2 id="方法">方法</h2>
<p>expert choice
routing的思路是让每个expert选择当前所有输入token（比如一个batch）中和自身匹配度最高的k个token来处理。</p>
<p>假设共有n个输入token，专家的数量为e，那么k的值为：</p>
<p><span class="math display">\[k=\frac{n\times c}e\]</span></p>
<p>c是超参capacity
factor，代表每个token平均会有多少个expert来处理，这和token choice
routing一样。</p>
<p>对于输入 <span class="math inline">\(X\in\mathbb{R}^{n\times
d}\)</span> （d是hidden size），expert choice
routing用到3个矩阵I、G、P来操作。</p>
<p><span class="math display">\[S=\mathrm{Softmax}(X\cdot W_g),\quad
S\in\mathbb{R}^{n\times
e}\\G,I=\mathrm{TopK}(S^\top,k),P=\mathrm{Onehot}(I)\]</span></p>
<p><span class="math inline">\(W_g\in\mathbb{R}^{d\times e}\)</span>
表示expert embedding。S表示所有专家和所有输入token之间的匹配程度。</p>
<p>I是index matrix，<span class="math inline">\(I[i,j]\)</span>
表示第i个expert选择的第j个token（按得分排序）。</p>
<p><span class="math inline">\(G\in\mathbb{R}^{e\times k}\)</span>
是gating matrix，表示各个expert所选token的权重。</p>
<p>P是permutation matrix，是I的one-hot版本，把token分配给各个专家：</p>
<p><span class="math display">\[X_{in}=P\cdot X\]</span></p>
<p><span class="math inline">\(X_{\mathrm{in}}\in\mathbb{R}^{e\times
k\times d}\)</span> 是发个各个专家的输入。<span class="math inline">\(X_\text{in}[i]\in\mathbb{R}^{k\times d}\)</span>
表示给第i个专家的输入。</p>
<p>每个专家的输出 <span class="math inline">\({X}_e[i]\)</span>
如下计算：</p>
<p><span class="math display">\[X_e[i]=\mathrm{GeLU}(X_{in}[i]\cdot
W_1[i])\cdot W_2[i]^\top \]</span></p>
<p>最终MoE层的输出 <span class="math inline">\(X_{\mathrm{out}}\in\mathbb{R}^{n\times d}\)</span>
可由P和G得到：</p>
<p><span class="math display">\[X_\mathrm{out}[l,d]=\sum_{i,j}P[i,j,l]
G[i,j] X_e[i,j,d]\]</span></p>
<h2 id="加上constraint">加上constraint</h2>
<p>上面这样实施的expert choice
routing存在一个问题，那就是可能大部分expert甚至所有expert都选中了同一个token，相当于这个token会被分配到所有token来处理。这样在通讯上可能会成为一个瓶颈。</p>
<p>针对这个问题，论文提出一个约束条件，给每个token所能分配到的最大expert数作了限制。</p>
<p>让 <span class="math inline">\(A\in\mathbb{R}^{e\times n}\)</span>
表示 <span class="math inline">\(A[i,j]\)</span>
表示第i个专家是否选择了第j个token。</p>
<p>通过以下约束优化问题，获得A，用 <span class="math inline">\(TopK(A,k)\)</span> 代替I。</p>
<p><span class="math display">\[\max_A\left\langle
S^\top,A\right\rangle+\lambda H(A)\]</span></p>
<p><span class="math display">\[\begin{aligned}H(A)=\sum_{ij}-A[i,j]\log
A[i,j]\end{aligned}\]</span></p>
<p><span class="math display">\[\mathrm{s.t.}\quad\forall
i:\sum_{j^{\prime}}A[i,j^{\prime}]=k; \forall
j:\sum_{i^{\prime}}A[i^{\prime},j]\leq b; \forall i,j: 0\leq
A[i,j]\leq1\]</span></p>
<p>b是每个token所能选择的最大专家数。H(A)是sum of element-wise
entropy。加入H(A)项，文中给的理由是</p>
<blockquote>
<p>Adding a small entropy term gives a near-integer solution while
enabling a fast iterative solver we can run on TPUs.</p>
</blockquote>
<p>实践中 λ = 0.001。</p>
<h1 id="实验">实验</h1>
<p>实验中，每两层替换一层为MoE网络，所实验的各个模型参数如下</p>
<img src="/2c8bbc7/intro.png" class title="routing">
<h2 id="效果">效果</h2>
<p>1、Training Efficiency</p>
<p>从step数上看，相比GShard top-2 gating，EC-CF2在训练中的收敛速度 &gt;
2x。</p>
<img src="/2c8bbc7/efficiency.png" class title="efficiency">
<p>此外，EC-CF2每个step都比GShard top-2
gating快20%，也就是说从时间上看效率更高。</p>
<p>2、Scaling the Number of Experts</p>
<p>改变专家的数量，可以看到expert choice routing相比top-2
routing都有稳定的提升。</p>
<img src="/2c8bbc7/expert_num.png" class title="expert num">
<p>3、Capped Expert Choice</p>
<p>对每个token所能发送的最大专家数作了限制之后，效果对比如下：</p>
<img src="/2c8bbc7/capped.png" class title="Capped Expert Choice">
<p>当限制专家数量为2时，效果有所下降，而限制专家数为3时，基本达到了和不加限制相同的效果。这说明允许每个token使用不同的专家数进行处理，确实是有效果的。</p>
<p>4、Variable Experts per Token</p>
<p>下图给出了token所用专家数量的分布。</p>
<img src="/2c8bbc7/dist.png" class title="Variable Experts per Token">
<p>大多数token使用了一到两个专家，之后大约3%的token使用了四个以上的专家。</p>
<p>这里可以发现，还有少量的token没有专家处理，这是EC存在的一个问题。</p>
<h2 id="消融实验">消融实验</h2>
<p>1、Capacity Factor</p>
<p>使用不同的CF，模型的效果对比如下。</p>
<img src="/2c8bbc7/cf.png" class title="cf">
<p>随着CF的增大，模型效果逐步提升。神奇的是，即使CF=0.5，即每个token平均只有0.5个专家处理，效果依然不错，甚至比switch
transformer（top-1）高。</p>
<p>2、Comparison with Dense Models on Pre-training</p>
<p>EC在ppl和收敛时间上始终优于dense：</p>
<img src="/2c8bbc7/dense.png" class title="Comparison with Dense Models on Pre-training">
<h1 id="小结">小结</h1>
<ul>
<li>EC从思路上来看，相比token
choice确实有些优势，但是EC本身也存在一些问题，比如可能存在没有被任何专家选中的token。<br>
</li>
<li>另外在推理时如何结合cache等应该也是一个问题。</li>
</ul>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a> <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a> <a href="http://www.linsight.cn/1736008.html">(3)</a> <a href="http://www.linsight.cn/1736008.html">(4)</a> <a href="http://www.linsight.cn/336f2f3e.html">(5)</a> <a href="http://www.linsight.cn/7c04944d.html">(6)</a> <a href="https://www.linsight.cn/dd614e12.html">(7)</a> <a href="https://www.linsight.cn/e287b9c3.html">(8)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】Mixture-of-Experts with Expert Choice Routing
https://arxiv.org/abs/2202.09368</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>MoE</tag>
        <tag>routing</tag>
      </tags>
  </entry>
  <entry>
    <title>从Yuan2.0到Yuan2.0-M32</title>
    <url>/3df0cd42.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>整理下Yuan2.0和Yuan2.0-M32技术报告的一些内容。</p>
<h1 id="yuan2.0">Yuan2.0</h1>
<p>Yuan2.0发布于23年11月，系列模型有3个规模：</p>
<img src="/3df0cd42/yuan2_intro.png" class title="Yuan2.0">
<h2 id="模型">模型</h2>
<p>通常的self-attention会计算token两两之间的关联性，但是没有显式加入“更近的token有更高重要性”这样的local
dependency机制，按原文说法，有“short of neighbouring local associations
of tokens”的问题。</p>
<p>于是Yuan2.0把attention修改成Localized Filtering-based
Attention（LFA），通过增加两个一维的convolution来增强相邻token之间的关联性，结构如下图</p>
<img src="/3df0cd42/lfa.png" class title="LFA">
<p>为了防止训练时卷积泄露未来的信息，这里的convolution是单向的，只能往前看：</p>
<img src="/3df0cd42/lfa_conv.png" class title="LFA Conv">
<p>和经典的Attention以及Attention with
EMA对比，LFA在效果上更好，在模型参数的增加和计算性能上的损耗也相比EMA更小。具体的对比数据如下</p>
<img src="/3df0cd42/lfa_result.png" class title="LFA result">
<p>EMA是《Mega: moving average equipped gated
attention》所使用的方法，目前使用EMA的模型似乎不多。</p>
<h2 id="数据">数据</h2>
<p>Yuan2.0的预训练数据分布如下</p>
<img src="/3df0cd42/yuan2_pretrain_data.png" class title="pretrain data">
<p>主要是书籍、百科、专业知识、代码和数学相关的内容。</p>
<p>一些数据的细节：<br>
- Baike和BOOK数据移除了小说数据<br>
- Code Instruct data：用生成的4M instruction获取大模型生成的 Python
solution<br>
- StarCoder中的header如&lt;reponame&gt;, &lt;filename&gt;,
&lt;gh_stars&gt;都移除了，一些code里的特殊token加到了tokenizer里</p>
<p>微调数据集包括：<br>
- Code Instruction Dataset：专注在python上，其他语言去掉了<br>
- Math Instruction Dataset<br>
- Chat Instruction Dataset：数据分布如下表</p>
<img src="/3df0cd42/yuan2_chat_data.png" class title="chat数据">
<p>从数据上看，Yuan2.0主要是往代码和数学能力方向进行了提升。</p>
<p>SFT的训练超参如下</p>
<img src="/3df0cd42/yuan2_sft_hp.png" class title="sft超参">
<h2 id="tokenizer">Tokenizer</h2>
<p>Yuan2.0使用SentencePiece，训练基于Unigram的tokenizer。</p>
<p>由于训练数据量比较大，所以这里使用了paralle的训练方法：1.6T的中文数据切分为135个文件，每个文件各自训练一个vocab
size为30000的tokenizer。</p>
<p>获得135个tokenizer之后，每个tokenizer在各自训练数据上统计vocab中每个token占训练数据的byte
size的比例。</p>
<p>之后把各个tokenizer统计的token占比合并起来，只保留占比最高的50000个token。</p>
<p>合并的过程中还会删掉包括数字、字母、特殊符号和长度&gt;7个字的中文词。</p>
<p>在这个基础上，再加入人工挑选的9000个低频中文字和30000个低频中文词，和前面的50000个token合并去重后得到了73417个token。</p>
<p>最后，再把arxiv（上训练的） tokenizer、StarCoder（上训练的）
tokenizer 和 LLaMA
tokenizer和获得的词表进行合并，最终得到了词表大小为134953的tokenizer。</p>
<h2 id="训练">训练</h2>
<p>Yuan2.0预训练的loss曲线走势如下</p>
<img src="/3df0cd42/yuan2_train_curve.png" class title="训练">
<h1 id="yuan2.0-m32">Yuan2.0-M32</h1>
<p>Yuan2.0-M32是基于Yuan2.0-2B结构扩展的MoE模型（包括LFA），每层激活32个专家的其中2个，总参数量为40B，激活参数量为3.7B。</p>
<img src="/3df0cd42/m32_intro.png" class title="模型">
<h2 id="模型-1">模型</h2>
<p>Yuan2.0-M32在结构上的主要改进是在router上使用了注意力机制。</p>
<p>一般来说，router就是给每个专家赋一个可学习的向量，每次通过这个可学习的向量和输入token的向量的内积来决定这个token分配给哪个专家，如下图a。</p>
<p>这种做法一个问题是没有考虑到分配的多个专家之间的关联性，而简单地把它们看作是独立的。</p>
<p>考虑路由到的专家之间的关联性应该是对提升效果有帮助的。</p>
<p>基于此Yuan2.0-M32提出attention router，如下图b。</p>
<img src="/3df0cd42/router.png" class title="attention router">
<p>对于输入token向量I（维度=d，在Yuan2.0-M32里d=2048），以及N个候选专家，计算如下：</p>
<p><span class="math display">\[Q=WI,\quad W\in\mathbb{R}^{N\times
d}\]</span></p>
<p><span class="math display">\[K=W^{\prime}I,\quad
W^{\prime}\in\mathbb{R}^{N\times d}\]</span></p>
<p><span class="math display">\[V=W^{^{\prime\prime}}I,\quad
W^{^{\prime\prime}}\in\mathbb{R}^{N\times d}\]</span></p>
<p><span class="math display">\[P=\mathrm{Softmax}(QK^T)\mathrm{V},\quad
P\in R^N\]</span></p>
<p>然后从P中选出top M个专家。</p>
<p>不同router在相同的30B数据上进行训练，然后在另外10B数据进行评测，效果对比如下</p>
<img src="/3df0cd42/router_eval.png" class title="attention router">
<p>其中attention router和classical router都是8个专家，而shared expert
router总共16个专家，其中2个共享专家，再从另外14个里选择两个路由专家激活。</p>
<p>另外还通过增加总的专家数来测试这个模型结构的scalability。在50B数据上训练，在另外10B数据上评测，总专家数为8/16/32时效果如下</p>
<img src="/3df0cd42/scalability.png" class title="scalability">
<p>Yuan2.0-M32使用了和Yuan2.0一样的tokenizer。</p>
<h2 id="训练-1">训练</h2>
<p>预训练和微调的超参如下</p>
<img src="/3df0cd42/train_hp.png" class title="train hp">
<p>Yuan2.0-M32总共在2T
token的数据上训练，loss变化如下，最终的loss下降到了1.22。</p>
<img src="/3df0cd42/pretrain.png" class title="pretrain">
<p>预训练的时候窗口长度为4k，微调的时候为16k。参考CodeLLama的做法，这里要增大RoPE的base。这里不是简单地把10000扩展到500k或者1M，而是根据NTK-aware的公式计算：</p>
<p><span class="math display">\[b^{\prime}=b\cdot
s^{\frac{|D|}{|D|-2}}\]</span></p>
<p>这里的D是head size，Yuan2.0-M32中head
size为128。把4k扩展到16k，则s=4，计算得到新的base=40890。</p>
<p>这里还拿base=40890和其他base（40000, 80000, 160000, 320000, 640000,
1280000, 2560000, 5120000,
10240000）进行效果对比，确认确实是40890的效果最好。</p>
<h2 id="评测">评测</h2>
<p>Yuan2.0-M32在code generation、math、MMLU、AI2 Reasoning Challenge
(ARC) benchmark上的评测效果如下。</p>
<img src="/3df0cd42/eval1.png" class title="评测">
<img src="/3df0cd42/eval2.png" class title="评测">
<img src="/3df0cd42/eval3.png" class title="评测">
<img src="/3df0cd42/eval4.png" class title="评测">
<h1 id="小结">小结</h1>
<ul>
<li>Yuan2.0、Yuan2.0-M32使用了一些人造数据，在数学的代码上看起来有一定收益。<br>
</li>
<li>结构上的改进感觉需要更多的实验来验证，另外这些改进在推理加速缺乏支持可能也是个问题。</li>
</ul>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a><br>
<a href="https://www.linsight.cn/dd614e12.html">大模型算法题(7)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】Yuan 2.0-M32: Mixture of Experts with Attention Router
https://arxiv.org/abs/2405.17976<br>
【2】YUAN 2.0: A Large Language Model with Localized Filtering-based
Attention https://arxiv.org/ftp/arxiv/papers/2311/2311.15786.pdf</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>技术报告</tag>
        <tag>MoE</tag>
      </tags>
  </entry>
  <entry>
    <title>MoE的top-p routing</title>
    <url>/224c42da.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>北大、快手和AGIBang共同提出MoE模型的dynamic
routing机制，把gating的top-k routing改成top-p
routing，在减少平均激活参数量的同时效果还略有提升。</p>
<p>MoE相关基础可参考<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a>。</p>
<h1 id="routing">routing</h1>
<h2 id="top-k-routing">top-k routing</h2>
<p>目前大部分的MoE模型采用的routing策略是top-k routing。比如当 k =
2，则每个输入token在每个MoE层会激活2个专家（忽略token drop等机制）。</p>
<p>假设每个MoE层有N个expert，这些expert的集合记为 <span class="math inline">\(E=\{e_{1},e_{2},..,e_{N}\}\)</span>，那么输入token
x在MoE层的计算如下：</p>
<p><span class="math display">\[MoE(\mathbf{x})=\sum_{i=1}^Ng_i(\mathbf{x})*e_i(\mathbf{x})\]</span></p>
<p><span class="math display">\[g_i(\mathbf{x})=\begin{cases}\frac{P_i}{\sum_{j\in
TopK(\mathbf{P})}P_j},&amp;i\in TopK(\mathbf{P})\\0,&amp;i\notin
TopK(\mathbf{P})\end{cases}\]</span></p>
<p><span class="math display">\[\mathbf{P}=Softmax(\mathbf{W_r}\cdot\mathbf{x}^T)\]</span></p>
<p>top-k routing由Google在《Outrageously large neural networks: The
sparsely-gated mixture-of-experts
layer》中提出，应用在LSTM模型上，之后的一些工作比如《Gshard》、《Switch
Transformer》、《ST-MoE》和《Taming sparsely activated transformer with
stochastic
experts》等则引入了相关constraint来确保多个专家间的负载均衡，以保障模型的效果和效率。</p>
<h2 id="top-p-routing">top-p routing</h2>
<p>虽然top-k
routing的效果不错，但是每个token都激活相同数量的专家这个假设粗暴地忽略了不同输入token之间的难度区别，并且在不同MoE层也都激活相同数量的专家这样的策略也没有考虑到模型不同层间的表达能力差异。</p>
<p>针对这个情况，就有了top-p
routing的策略：不直接限制每个token激活的专家数量，而是根据设定的阈值p（超参），一个一个把候选专家中gating得分最高的加入到激活专家集合里，直到激活专家集合的accumulative
confidence超过p。写成公式如下：</p>
<p><span class="math display">\[t=\underset{k\in\{1...,N\}}{argmin}\sum_{j&lt;=k}P_{i,j}\geq
p\]</span></p>
<p><span class="math display">\[g_i(\mathbf{x})=\begin{cases}P_i&amp;e_i\in
S\\0,&amp;e_i\notin S\end{cases}\]</span></p>
<p><span class="math display">\[S=\{e_{I_1},e_{I_2}...e_{I_t}\}\]</span></p>
<p>top-k routing和top-p routing的示意图如下：</p>
<img src="/224c42da/top-p.png" class title="top-p">
<h1 id="loss">Loss</h1>
<h2 id="dynamic-loss">Dynamic Loss</h2>
<p>使用top-p
routing会有一个风险：模型可能会学到把gating的权重在所有专家间进行均匀分配的策略，因为这样可以使得激活的专家数最大。</p>
<p>比如阈值p设置为0.5，那么在所有专家的权重均匀分配的情况下，激活专家数为总专家数的一半，这远多于正常MoE机制下的激活比例。这样由于激活参数量较大，最终模型的效果就会更好。</p>
<p>但这样的均匀分配策略显然是违背了MoE设计的初衷的。</p>
<p>为了避免这个问题，避免出现均匀分布的情况，可以增加一个dynamic
loss，要求模型最小化权重分布P的熵，让不同专家可以专注在特定的领域，提高专家化的程度：</p>
<p><span class="math display">\[Loss_d=-\sum_{i=1}^NP_i*log(P_i)\]</span></p>
<h2 id="load-balance-loss">Load Balance Loss</h2>
<p>这里负载均衡损失的设计就比较常规，和很多其他MoE模型所用的一致：</p>
<p><span class="math display">\[Loss_b=N*\sum_{i=1}^Nf_i*Q_i\]</span></p>
<p><span class="math display">\[f_i=\frac{1}{M}\sum_{j=1}^M1\{e_i\in
S^j\}\]</span></p>
<p><span class="math display">\[Q_i=\frac{1}{M}\sum_{j=1}^nP_i^j\]</span></p>
<p><span class="math inline">\(S^{j}\)</span>
是第j个token激活的专家集合。</p>
<h2 id="final-loss">Final Loss</h2>
<p>最后完整的训练loss计算如下：</p>
<p><span class="math display">\[Loss=Loss_{lm}+\alpha Loss_b+\beta
Loss_d\]</span></p>
<p>训练中，使用了 <span class="math inline">\(\alpha=1e-2\)</span>，<span class="math inline">\(\beta=1e-4\)</span>。</p>
<h1 id="实验">实验</h1>
<h2 id="数据">数据</h2>
<p>从RedPajama抽了100B数据，包括common crawl (CC), C4, github,
Wikipedia, books, arxiv 和 Stackexchange。</p>
<h2 id="模型">模型</h2>
<p>模型采用LLaMA的结构：<br>
- vocab size = 32000<br>
- layer num = 24<br>
- 初始化standard deviation = 0.006<br>
- MHA，head num = 16，head size = 64</p>
<p>共设计了5个模型：<br>
- dense模型1：hidden size = 1024，总参数量 = 374M<br>
- dense模型2：hidden size = 1280，总参数量 = 570M<br>
- top-1 MoE模型：hidden size = 1024，专家数 = 16，总参数量 =
3.5B，激活参数量 = 374M<br>
- top-2 MoE模型：hidden size = 1024，专家数 = 16，总参数量 =
3.5B，激活参数量 = 581M<br>
- top-p MoE模型：hidden size = 1024，专家数 = 16，总参数量 = 3.5B，阈值p
= 0.4</p>
<p>训练设置如下：<br>
- AdamW，beta_1 = 0.9, beta_2 = 0.95<br>
- weight decay = 0.1<br>
- cosine schedule<br>
- max lr = 3e-4，final lr = 3e-5<br>
- warmup = 2000 step<br>
- context length = 2048<br>
- batch size = 2048</p>
<p>上面5个模型的在下游任务的对比如下</p>
<img src="/224c42da/perf.png" class title="performance">
<p>top-p MoE在下游任务上的平均激活专家数为1.76。</p>
<p>top-p MoE以≤top-2 MoE模型90%的激活参数量，获得了比top-2
MoE提升0.7%的效果。</p>
<h1 id="分析">分析</h1>
<h2 id="p的影响">p的影响</h2>
<p>不同的阈值p（0.1~0.7）下的模型效果</p>
<img src="/224c42da/diff_p.png" class title="p">
<p>当p值特别低比如0.1或者0.2时，效果比较差，而p≥0.3之后基本就效果保持在比较好的水平了。</p>
<h2 id="激活专家收敛">激活专家收敛</h2>
<p>top-p
MoE在训练一开始激活的专家数会比较多，而随着训练进行，激活专家数逐渐下降：</p>
<img src="/224c42da/active_num.png" class title="训练过程激活专家数">
<p>可以看到在60B以后就逐渐低于2了，并且从图上看还有下降趋势。这里实验只做了100B，如果训了1T或者10T，应该会有更大的收益。</p>
<h2 id="top-p-moe适合更难的任务">top-p MoE适合更难的任务</h2>
<p>BBH（BIG-Bench Hard），包括了23个比较有挑战性的BIG-Bench任务。</p>
<p>从下图可以看到，相比其他任务，模型在BBH任务会激活更多的专家</p>
<img src="/224c42da/task_expert.png" class title="激活专家数">
<p>并且相对于其他下游任务，top-p MoE在BBH上的提升也是最多的。</p>
<p>这说明top-p
MoE允许模型激活更多的专家，以获得足够的能力和信息，从而能在更难的任务上进一步提升效果。</p>
<h1 id="底层需要激活更多专家">底层需要激活更多专家</h1>
<p>top-p
MoE以更少的激活参数量在下游任务取得更好的效果，这归功于专家在不同层间的合理分配。</p>
<p>下图给出模型不同MoE层的平均激活专家数量</p>
<img src="/224c42da/diff_layer.png" class title="不同层激活专家数">
<p>这样的现象和overthinking有些相似。</p>
<p>按《Shallow-deep networks: Understanding and mitigating network
overthinking》说法，overthinking指相对于最终层的复杂表示，更早期层中输入样本的更简单表示就足以做出正确的预测。</p>
<p>随着层数增多，激活的专家数量逐渐下降。模型能够把更多的计算budget用在收益更大的浅层表征，提升最终效果。</p>
<h1 id="小结">小结</h1>
<ul>
<li>解除MoE模型的专家激活数限制，可以让模型自由选择需要的专家，以应对更难的任务，应该是个不错的思路。</li>
</ul>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a><br>
<a href="https://www.linsight.cn/dd614e12.html">大模型算法题(7)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】Harder Tasks Need More Experts: Dynamic Routing in MoE Models
https://arxiv.org/abs/2403.07652</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>MoE</tag>
        <tag>routing</tag>
      </tags>
  </entry>
  <entry>
    <title>bilibili的index-1.9B</title>
    <url>/770b63e1.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>bilibili发布的Index-1.9B包含：<br>
- Index-1.9B base：在2.8T语料训练的基模型<br>
- Index-1.9B pure：部分数据和base不同，其他相同<br>
- Index-1.9B chat：基于base进行SFT和DPO后的模型<br>
- Index-1.9B
character：在chat的基础上用RAG+fewshots提供角色扮演的能力</p>
<p>下面看下细节。</p>
<h1 id="模型">模型</h1>
<p>（1）模型深度</p>
<p>目前业界普遍的认知是模型深度比宽度对效果的影响更大（相同参数下）。对比层数分别为36层和9层，总参数都为1.01B的模型，结果如下图，36层模型（base）效果确实更好。</p>
<img src="/770b63e1/layer_num.png" class title="层数">
<p>需要注意的是，相同模型参数量下，层数更多的模型会有更多的activation（与L
* hidden size成正比），因此会需要更多显存。</p>
<p>（2）Norm-Head</p>
<blockquote>
<p>模型不同层的梯度尺度分布非常不同，最后一层 LM-Head
的梯度，占据了绝大部分的梯度大小。而词表的稀疏性让 LM-Head
层稳定性较低，影 响模型训练稳定性，进而影响模型性能表现，所以稳定的
LM-Head 层对于训练非常重要。</p>
</blockquote>
<p>参考Baichuan2使用Norm-Head，即对LM-Head进行Norm，能让训练更稳定。</p>
<p>对比有无Norm-Head的模型，效果和Gradient Norm具体情况如下图</p>
<img src="/770b63e1/norm_head.png" class title="norm head">
<p>Norm-Head版本的效果更好，Gradient
Norm整体变化也比较小，相对稳定（除了开头有个突升）。</p>
<h1 id="预训练">预训练</h1>
<h2 id="数据">数据</h2>
<p>数据一些细节：<br>
- 总量2.8T<br>
- 中文：英文 = 4：5，代码占比 = 6%<br>
- 书籍、百科、论文、STEM相关精选数据占比10%，这部分很重要</p>
<p>具体占比饼图如下</p>
<img src="/770b63e1/pt_data.png" class title="预训练数据">
<p>数据去重上做了一个事情：如果预先分局分段就有可能有一些重复没有被发现，因此这里采用支持任意长度、任意位置的文档内字符串去重。实现上基于<a href="https://github.com/google-research/deduplicate-text-datasets">https://github.com/google-research/deduplicate-text-datasets</a>进行了优化。</p>
<p>支持任意位置所能找到的难以发现的重复片段例子如下图。月份下拉框文字，重复了
15.6w 次，只能通过精准字符串去重识别</p>
<img src="/770b63e1/dedup.png" class title="去重">
<p>训练数据在packing的时候，重置了attention-mask和position-id。</p>
<h2 id="训练">训练</h2>
<p>（1）训练设置</p>
<ul>
<li>AdamW，beta_1=0.9，beta_2=0.95，eps=1e-8<br>
</li>
<li>gradient clip=1.0<br>
</li>
<li>weight decay=0.1</li>
</ul>
<p>（2）lr和scheduler</p>
<p>训练的时候使用WSD的learning rate
scheduler，分为了两阶段的训练：<br>
- warmup 100步 + 全局混合数据<br>
- decay阶段，增大精选数据比例</p>
<p>这里把decay阶段设得比较长，最大lr为5e-4，decay结束时减小到1%即5e-6，整个decay过程训练了400B数据。</p>
<p>这里还用0.1B的模型，分别用Cosin、Linear和WSD训练1T的数据，训练过程的loss和最终模型的效果对比如下</p>
<img src="/770b63e1/scheduler.png" class title="scheduler">
<p>几个发现：<br>
- 最终三种scheduler的valid loss都收敛到同一个水平<br>
- WSD在S阶段的valid loss比较高，而在decay阶段会快速下降<br>
- 评测指标整体没有差太多</p>
<p>（3）decay阶段数据的配合</p>
<p>从上面的实验发现WSD在decay阶段的loss会快速下降，那么在这个阶段加入更多的高质量精选数据应该有更好的效果。</p>
<p>为了验证这个想法，用以下四种配置做实验：<br>
- Cosine<br>
- WSD<br>
- Cosine + 后10%加入更多精选数据<br>
- WSD + 后10%进入decay阶段，并加入更多精选数据</p>
<p>评测结果如下，确实是在decay阶段增加高质量数据的WSD效果最好。</p>
<img src="/770b63e1/wsd_quality.png" class title="wsd quallity">
<p>（4）预训练阶段加入指令数据</p>
<p>在decay阶段还做了一个实验，验证加入指令数据对最终预训练模型效果的影响。</p>
<p>为了验证效果，训了两个版本的模型：<br>
- index-1.9b-ablation-pure：Decay
阶段自然文本数据，精选数据做重新放入增加浓度(书籍、论文、百科、专业等类别)<br>
- index-1.9b-ablation-boost：在pure基础上，额外加入占比7%的指令数据</p>
<p>在MMLU的效果对比如下</p>
<img src="/770b63e1/add_instruct_data.png" class title="指令数据">
<p>在decay阶段增加指令数据能明显提升下游任务指标。</p>
<h1 id="对齐">对齐</h1>
<h2 id="sft">SFT</h2>
<p>（1）数据</p>
<ul>
<li>使用10M中英数据<br>
</li>
<li>使用奖励模型打分进行筛选清洗<br>
</li>
<li>参照了主流的聚类增强多样性<br>
</li>
<li>对缺少指令数据的任务，构造/标注进行补充</li>
</ul>
<p>（2）训练</p>
<ul>
<li>lr=1e-5<br>
</li>
<li>采用 system-query-response 格式</li>
</ul>
<p>实验还发现，SFT时，加载预训练优化器的参数，并加入部分预训练语效果最好。预训练语料和SFT数据response的token数占比为4:6。</p>
<img src="/770b63e1/sft.png" class title="SFT对比">
<h2 id="dpo">DPO</h2>
<p>这里开发人员认为评价标准偏判别而非枚举的任务通过偏好学习可以取得最大的收益，因此DPO主要针对写作类、指令遵循和安全进一步对齐。</p>
<p>此外在构造安全类数据的时候，发现通过prompt让模型生成拒答回复，比人写好，原因是人工构造的拒答样本在
SFT 模型中的 ppl
过高，如果强行对齐又会导致拒答率较高，带来灾难性遗忘。</p>
<p>lr = 1e-6 scheduler = cosine epoch = 1 dpo 超参beta = 1</p>
<p>评测结果如下</p>
<img src="/770b63e1/dpo.png" class title="DPO">
<h1 id="小结">小结</h1>
<p>Index-1.9B验证了几个事情：<br>
-
在WSD的decay阶段加入高质量预训练数据，以及指令数据，对下游任务效果有明显提升。<br>
- SFT时加载预训练优化器参数有收益。<br>
- Norm-Head能稳定预训练过程。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a><br>
<a href="https://www.linsight.cn/dd614e12.html">大模型算法题(7)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】https://github.com/bilibili/Index-1.9B/blob/main/Index-1.9B%20%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A.pdf</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>技术报告</tag>
      </tags>
  </entry>
  <entry>
    <title>模型平均 -- model soup</title>
    <url>/bb8fcf21.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>最近苹果的DCLM和Llama-3.1技术报告都提到了model soup：《Model soups:
averaging weights of multiple fine-tuned models improves accuracy
without increasing inference time》。</p>
<p>拿model soup出来和SWA已经EMA一起看下。</p>
<h1 id="背景">背景</h1>
<p>一般来说，模型微调的过程是这样的：<br>
- 1、用不同的超参训练多个模型，每个配置下得到一系列模型checkpoint<br>
- 2、选择在验证集上最佳的checkpoint，其他的就丢弃掉了</p>
<p>这样的常规做法方便易操作，但是有几个缺点：<br>
-
多个微调得到的模型如果进行合适的ensemble应该能有超过单个模型的效果，直接选择一个”最佳模型“浪费了一部分算力<br>
- 微调之后对于out-of-distribution
data的效果可能变差，而这在验证集是看不出来的；而模型在实际使用中，很容易遇到有分布偏移的数据</p>
<h1 id="swa-ema">SWA &amp; EMA</h1>
<p>针对单次训练的模型平均方法主要有SWA和EMA。</p>
<p>Stochastic Weight
Averaging（SWA）算是模型微调里比较常见、普遍有效的方法了。</p>
<p>SWA一般取训练后期的等间隔checkpoint，然后计算他们的参数平均。</p>
<img src="/bb8fcf21/swa_1.png" class title="swa">
<img src="/bb8fcf21/swa_2.png" class title="swa">
<p>SWA为什么有效呢？</p>
<p>一般SGD会让模型收敛到loss平面的一个wide flat
region。这个空间的维度很高，所以wide flat
region的大部分volume都集中在边界附近（类似碗口附近），所以SGD得到的解更容易出现在边界附近。</p>
<p>另外，train loss和test error的曲面并非完全对齐。位于wide flat
region中心的解不像边界附近的解那样容易受到训练和测试曲面之间的偏移影响，也就是靠近中间的解有更好的泛化性。</p>
<p>SWA对多个解进行平均，能使其能够朝着区域的中心移动，因此得到的模型有更好的效果。</p>
<p>下图是SWA和SGD解的train loss和test
error曲面，虽然SWA得到的位置，train
loss较大，但是它在收敛区域的中心，有更好的泛化性，在test
error上更好。</p>
<img src="/bb8fcf21/swa_3.png" class title="swa">
<p>EMA和SWA类似，只是对模型进行平均的方法不一样，细节可以参照《【炼丹技巧】指数移动平均（EMA）的原理及PyTorch实现》(<a href="https://zhuanlan.zhihu.com/p/68748778">https://zhuanlan.zhihu.com/p/68748778</a>)。</p>
<h1 id="model-soup方法">model soup方法</h1>
<p>关于model average的一些工作：<br>
- 《What is being transferred in transfer
learning?》里观察到，从同一个预训练模型进行微调的下游模型，会收敛到同一个error
landscape basin。<br>
- 《Rethinking the inception architecture for computer
vision》和《Averaging weights leads to wider optima and better
generalization》（SWA）的结果显示在单个微调训练路径上进行weight
average有效果。<br>
- 《No one representation to rule them all: Overlapping features of
training
methods》中观察到，把使用不同超参微调出来的模型进行ensemble有效果提升。</p>
<p>受上面这些方法和观察的启发，model soup把model
average扩展到使用多个超参的independent
run，而不仅是如EMA/SWA那样的单次训练。</p>
<p>假设使用多套超参 <span class="math inline">\([h_1,...h_k]\)</span>
对预训练模型（<span class="math inline">\(\theta_0\)</span>）进行微调，得到 <span class="math inline">\([\theta_1,...,\theta_k]\)</span>
共k个模型checkpoint，分别是各自超参下，在验证集上取得最佳结果的checkpoint。通过对这k个checkpoint的模型参数进行平均，获得比单次微调的模型更好的效果，这就是model
soup。</p>
<p>文中提出了3种具体model soup方法：uniform soup、greedy soup和learned
soup：</p>
<img src="/bb8fcf21/method_soup.png" class title="model soup">
<p>其中uniform soup把所有模型都用起来，计算均值。</p>
<p>而greedy
soup的做法则是把k个checkpoint按在验证集上的效果排序，按从高到低的顺序逐个验证checkpoint，只有当前checkpoint的加入对最终效果有提升时，才会保留它，否则就丢弃。算法如下：</p>
<img src="/bb8fcf21/algo.png" class title="model soup">
<p>uniform soup和greedy soup都比较直接，learned
soup方法则需要额外训练。假设 <span class="math inline">\(\alpha\in\mathbb{R}^k\)</span> 是mixing
coefficients，<span class="math inline">\(\beta\)</span> 是temperature
scaling parameter，learned soup基于以下目标解出 <span class="math inline">\(\alpha\)</span> 和 <span class="math inline">\(\beta\)</span>：</p>
<p><span class="math display">\[\arg\min_{\alpha\in\mathbb{R}^k,\beta\in\mathbb{R}}\sum_{j=1}^n\ell\Bigg(\beta\cdot
f\Bigg(x_j,\sum_{i=1}^k\alpha_i\theta_i\Bigg),y_j\Bigg)\]</span></p>
<p>当k比较大时，learned soup对显存的需要会很大。</p>
<p>综合来看，greedy soup应该是比较方便有效，性价比高的做法。</p>
<h1 id="model-soup实验">model soup实验</h1>
<p>图像上，用CLIP、ALIGN和BASIC模型做了验证，而文本则是用文本分类transformer模型。</p>
<p>1、Error landscape visualizations</p>
<p>用CLIP在ImageNet上使用不同超参进行多次微调，training loss和test
error的可视化如下：</p>
<img src="/bb8fcf21/angle.png" class title="model soup">
<p>x和y轴是二维化的模型参数空间。多次的的微调模型本身并不在error
landscape的最低点，而是分布在边缘上。</p>
<p>这结果说明：<br>
- 对多个finetuned solution取平均能获得超过单个模型的效果<br>
- 越不相关的solution --
参数空间上和initialization模型构成的连线之间的夹角越大 --
的平均效果可能更好（个人这点感觉不是很直观）</p>
<p>为了验证solution相关性对model
average效果的影响，分别改变随机数种子、学习率和图像数据增强，得到多对结果。model
soup的准确性增益随着solution之间的差异增大而增加，如下图：</p>
<img src="/bb8fcf21/angle_2.png" class title="model soup">
<p>2、Ensemble comparison</p>
<p>model soup和ensemble方法，在不同learning rate下的对比如下：</p>
<img src="/bb8fcf21/compare.png" class title="model soup">
<p>观察到：<br>
- 当lr较小时，ensemble和model soup的效果同样，都比较差<br>
- 当lr适中时，ensemble和model soup的效果都较好<br>
- 当lr较大时，ensemble比model soup好，但都比适中lr差<br>
- 整体上，在in-distribution的数据上，ensemble效果更好，而在distribution
shift数据上，则model soup更好</p>
<p>3、One dimensional hyperparameter grids</p>
<p>仅改变一个超参，获得的多个模型进行平均，效果是否有提升？</p>
<p>针对这个问题，文章在optimizer、augmentation和lr上分别做了实验，结果是除了太大或者太小的lr，其他都有正收益。</p>
<p>4、效果</p>
<p>图像和文本模型在下游任务上使用model soup的效果如下：</p>
<img src="/bb8fcf21/result.png" class title="model soup">
<p>从结果上来看，都有比较稳定的收益，但是文本任务的收益没有图像那么明显。</p>
<h1 id="小结">小结</h1>
<ul>
<li>model soup中性价比比较高的就是greedy model
soup，操作简单，不影响推理成本，大部分任务都能获得提升<br>
</li>
<li>model
soup的方法可以和adapter比如LoRA结合起来使用，还是比较有可扩展性的<br>
</li>
<li>是和对抗训练、r-drop之类的方式一样，涨点好用，但是水文不多的方案</li>
</ul>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】Averaging Weights Leads to Wider Optima and Better
Generalization https://arxiv.org/abs/1803.05407<br>
【2】Model soups: averaging weights of multiple fine-tuned models
improves accuracy without increasing inference time
https://arxiv.org/abs/2203.05482<br>
【3】Stochastic Weight Averaging in PyTorch
https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/<br>
【4】【炼丹技巧】指数移动平均（EMA）的原理及PyTorch实现
https://zhuanlan.zhihu.com/p/68748778</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>微调</tag>
        <tag>模型融合</tag>
      </tags>
  </entry>
  <entry>
    <title>对MoE模型的一些观察</title>
    <url>/5e1d14b3.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>包括清华和港科大的五所高校对几个MoE模型进行一些研究，并给出一些相应的模型设计建议。</p>
<h1 id="moe">MoE</h1>
<p>当前主流的Sparse Mixture-of-Experts模型在N个专家中激活k个，k &lt;
N，具体建模如下</p>
<p><span class="math display">\[\mathbf{y}=\sum_{n\in
N}g_n(\mathbf{x};\mathbf{G},\mathbf{k})E_n(\mathbf{x})\]</span></p>
<p><span class="math display">\[\mathrm{Expert}(x)=W_\text{down}(W_\text{up}x\odot\mathrm{Act}(W_\text{gate}x))\]</span></p>
<p><span class="math display">\[W_{\mathrm{up}},W_{\mathrm{gate}}\in\mathbb{R}^{d_{\mathrm{mid}}\times
d_{\mathrm{hid}}}\]</span></p>
<p><span class="math display">\[W_{\mathrm{down}}\in\mathbb{R}^{d_{\mathrm{hid}}\times
d_{\mathrm{mid}}}\]</span></p>
<p>这里把 <span class="math inline">\(W_{\mathrm{up}}[i,:]\)</span> 和
<span class="math inline">\(W_{\mathrm{gate}}[i,:]\)</span>
这两个行向量以及 <span class="math inline">\(W_{\mathrm{down}}[:,i]\)</span>
这个列向量定义为一个neuron，这样每个专家就包含d_mid个专家，这些neuron后面会有分析。</p>
<h1 id="研究对象">研究对象</h1>
<p>文章中选择了Mixtral 8x7B，DeepSeekMoE 和
Grok-1三个MoE模型作为研究对象，另外还加上了Mistral
7B这个dense模型作为对比。</p>
<p>各个模型设置对比如下</p>
<img src="/5e1d14b3/models.png" class title="模型">
<p>后续研究使用的相似度如无说明都是指cosine similarity。</p>
<h1 id="analysis-of-static-parameters">Analysis of Static
Parameters</h1>
<p>对这些模型的静态参数研究主要是（1）MoE
experts和（2）gating，这两个也是MoE最重要的部分。</p>
<h2 id="moe-experts">MoE experts</h2>
<p>参照《Transformer feed-forward layers are keyvalue
memories》和《Empirical study on updating key-value memories in
transformer feed-forward layers》的说法，expert的projection
matrices可以看做是keys和values：<br>
- W_down的列向量表示possible outputs<br>
- W_up的行向量决定各个possible outputs的权重<br>
- W_gate决定是否激活对应的neuron</p>
<p>对experts的研究又分为matrix level和neuron level。</p>
<p>1、matrix level</p>
<p>各个模型不同层下，所有专家三个投影矩阵对应的相似度如下图（DeepSeekMoE忽略了shared
expert）。计算相似度的时候把矩阵进行了一维展开，之后通过PCA把维度转换到2维。</p>
<img src="/5e1d14b3/matrix_level.png" class title="matrix level">
<p>一些发现：<br>
-
DeepSeekMoE和Grok-1的专家相似度比Mixtral低，而DeepSeekMoE和Grok-1是从零训练的，这表明Mixtral可能不是从零初始化的。<br>
-
Mixtral中有一些专家和其他所有专家的相似度都极低，表明这些专家可能学到了一些特殊的内容。<br>
-
深层的专家相似度相比浅层更低，这说明深层专家可能有更高的专业化程度。</p>
<p>2、neuron level</p>
<p>matrix
level的计算没有考虑到这样的情况：两个专家有相似的neuron，但是这些neuron的位置不同，这样也会导致相似度不高。因此这里通过取平均和重排序的方式来研究neuron
level的相关性。重排序使用了Jonker-Volgenant算法。</p>
<p>重排序后的相似度增长和Kendall’s coefficient如下表所示。Kendall’s
coefficient是一种用于衡量多个评分者或多个方法对同一组对象进行评分或排名的一致性的统计量：1表示完全正相关，即两个变量的排名完全一致，-1表示完全负相关，即一个变量的排名与另一个变量的排名完全相反，0表示没有相关性，即两个变量的排名之间没有一致的模式。</p>
<img src="/5e1d14b3/t2.png" class title="neuron level">
<p>可以看到Mixtral的相关性依然是显著高于其他模型，说明其各个专家之间的初始化可能有关联。</p>
<h2 id="gate-embedding">Gate Embedding</h2>
<p>对于gating，研究人员首先计算了gate
embedding向量之间的相似度，发现gate
embedding的相似度和matrices之间的相似度结果相似。</p>
<p>而后又研究了gate
embedding（X）和W_up、W_gate、W_down（Y）的相似关系，并做了linear
regression。下表是各个模型所有层平均之后的square of Pearson correlation
coefficients。</p>
<img src="/5e1d14b3/gating_1.png" class title="gating">
<p>具体各层的数据</p>
<img src="/5e1d14b3/gating_2.png" class title="gating">
<p>一些发现：<br>
- gate embedding和W_gate的相似度最高。<br>
-
Mixtral和DeepSeekMoE的（X，Y_gate）保持正相关，而Grok-1在&gt;25层后出现了负相关。<br>
- gate
embedding和W_gate的功能有些类似：前者决定专家的选择，后者则决定要激活的neuron，这两个部分有可能学习到了相近的知识。</p>
<h2 id="summary">Summary</h2>
<ul>
<li>深层的专家间的相似度更低，可能是专门化的程度更高。<br>
</li>
<li>专家的W_up、W_gate、W_down的相似度关系相近。</li>
</ul>
<h1 id="analysis-of-dynamic-behaviours">Analysis of Dynamic
Behaviours</h1>
<p>前面研究的是模型的静态参数，这里通过使用一个6个token的短文本，和一个1100token的长文本对各个模型的动态特性进行探索。（emmm只用一两条数据是不是有点少）</p>
<h2 id="outputs-of-experts">Outputs of Experts</h2>
<p>对于MoE模型，一个自然的问题是，选中专家和未选中专家的输出之间有哪些相似性和差异性。</p>
<p>短文本和长文本的各个专家（包含没有被选中的专家）的输出的相似度如下。</p>
<img src="/5e1d14b3/dynamic.png" class title="相似度">
<p>这里长文本使用的是angular similarity：</p>
<p><span class="math display">\[\text{angular
sim}=1-\frac{\arccos{(\text{cosine sim})}}{\pi}\]</span></p>
<p>Mixtral：被选中的专家间的相似度更大，这可能是因为它们的norm更大。随着深度增加，在比较深的层中，整体的相似度较低，但是最后一两层却又突然变得特别相似。</p>
<p>DeepSeek：和Mixtral相似，在最后一层也出现了相似度增大的情况。</p>
<p>Grok：可能是因为Grok的expert
size比较大，导致各个专家都能学到比较全面的内容，因此所有专家之间的输出相似度显著高于其他两个模型。</p>
<h2 id="norms-of-expert-outputs-and-gate-scores">Norms of Expert Outputs
and Gate Scores</h2>
<p>在上面这个实验发现被选中的专家的相似度会比较高，为了探索可能的原因，这里对experts的L2
norm和gating decision的关系进行了研究。</p>
<p>使用了短文本作为输入，gate score和对应专家的norm如下</p>
<img src="/5e1d14b3/norm.png" class title="norm">
<p>Mixtral：发现被门控网络选中的两个expert通常都是feature vector
norm最高的那两个。这个发现和《Competesmoe–effective training of sparse
mixture of experts via
competition》一致。另外层数越深，norm的值也越大，这和《Improved
transformer pretraining with extra normalization》中的增长相似。</p>
<p>DeepSeek：和Mixtral不同，DeepSeek的gating选择对norm的依赖看上去相对较低，但是top-1专家的得分更加突出，并且但是同样有随着层数增长，norm增大的特性。</p>
<p>Grok：Grok的gating和norm没有明显的相关关系。可能的原因之一是GeLU相对较低的激活比率导致gating对norm的依赖性较弱。此外，与Mixtral和DeepSeek不同，Grok专家的norm在模型不同深度内几乎不变，而且其中一些norm值可能小于1，这是其他两个模型没有的现象。</p>
<h2 id="summary-1">Summary</h2>
<ul>
<li>在Mixtral和DeepSeek中，各个专家的输出相似度随着深度加深而变弱，而在最后一两层相似度又会突然提高。<br>
</li>
<li>expert output的heat map和neuron-level相似度的heat
map相似，这说明这两个测量某种程度上可以等价。<br>
</li>
<li>对于Mixtral和DeepSeek，具有large norm
output的expert更容易被选中。</li>
</ul>
<h1 id="suggestions">Suggestions</h1>
<p>基于上面的一些观察，文章提出了一些建议：<br>
- Neuron-level experts：直观上gate
embedding决定了专家的选择，而W_gate负责激活特定的neuron。而gate
embedding和W_gate之间的相似性又存在关联。这意味着neuron可能是更细粒度的专家。应该进一步在微观层面上研究对专家的操作。<br>
- Model
architecture：由于专家之间的相似性在深层/最后一层倾向于相对较低/高，可以考虑在深层增加专家的数量，而在最后一层减少专家数量。此外，gating机制通常选择输出norm较大的专家，因此使用norm作为路由机制是合理的。<br>
- Correlation measurement：在分析专家之间的相关性时，测量它们的weight
matrices之间的相似性与测量它们在token上的输出特征向量之间的相似性在某种程度上是等价的。因此，测量weight
matrices可以获取overview。<br>
- Training
scheme：从零训练的DeepSeek和Grok比（可能）从dense模型初始化的Mixtra的专家相关性更低，说明从零训练能促进专家多样性。</p>
<h1 id="小结">小结</h1>
<ul>
<li>文章基于几个比较知名的MoE模型做了一些研究，但是研究样本量感觉应该再增大一些。<br>
</li>
<li>基于观察结果的一些建议和一些其他工作结论不同，最好能一起对比下。<br>
</li>
<li>说明MoE确实还有很多内容没有搞清楚。</li>
</ul>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a><br>
<a href="https://www.linsight.cn/dd614e12.html">大模型算法题(7)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】A Closer Look into Mixture-of-Experts in Large Language Models
https://arxiv.org/abs/2406.18219</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>MoE</tag>
      </tags>
  </entry>
  <entry>
    <title>从dense到MoE -- sparse upcycling</title>
    <url>/a0824e29.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>目前已经有很多优秀的dense大模型，那么要通过MoE获得更强的模型，用已有的dense模型进行初始化是一个自然的想法。Google的sparse
upcycling对此做了一些实验，由于实验是在2022年做的，模型用的是T5系列语言模型和Vision
Transformer系列视觉模型。</p>
<p>文中给出两个适合使用sparse upcycling的场景：<br>
- 已有dense模型，想在有限的计算资源下提升模型效果。<br>
-
要训一个模型，不知道dense模型和MoE哪个会有更好的效果（虽然通常来说MoE更好，但是训练难度和结果不确定也更大），那么就可以先训练一个dense模型保底，然后再在dense模型的基础上扩展成MoE结构继续优化。</p>
<p>下面具体看下一些实验细节。</p>
<h1 id="设置">设置</h1>
<p>对于transformer模型，sparse upcycling的操作如下图</p>
<img src="/a0824e29/intro.png" class title="upcycling">
<p>除了原模型的MLP层替换成MoE层外，其他组件包括layernorm、attention都直接从原dense模型copy到MoE模型。</p>
<p>实验上，一些具体的基础设置如下：<br>
- 在原模型基础上，每2层替换一个MoE层，从第二层开始替换<br>
- MoE模型的总层数的dense模型层数相同<br>
-
每个MoE层专家数为32个；虽然使用更多的专家不会明显增大训练的FLOPS，但是更多的专家会带来larger
initial quality drop relative to baseline dense
model，而需要更多的计算资源来恢复这个quality
drop；后续会有实验探索expert数量的影响<br>
- 每个expert都用原模型的MLP层参数初始化<br>
- router使用standard deviation=0.02的zero-mean normal
distribution随机初始化<br>
- 在encoder使用expert choice routing，基础的设置是capacity factor C =
2，后面也做了关于capacity factor的消融实验<br>
- 在decoder使用token choice routing（top-k
routing），k=2，并加上auxiliary
loss帮助负载均衡，权重为0.01；在decoder使用top-k routing的原因是"to
avoid train time (full batch teacher forcing) versus inference time
(single token auto-regressive decoding) discrepancies"（和expert choice
routing的设计相关）</p>
<p>MoE模型训练时使用和dense模型一致的batch size、learning rate
schedule和weight decay等。</p>
<p>其中learning rate schedule用的是inverse square root learning rate
schedule，因此MoE的训练可以接着dense模型的schedule接着进行。</p>
<p>实验中所用的一些模型参数如下表</p>
<img src="/a0824e29/models.png" class title="模型">
<h1 id="实验">实验</h1>
<h2 id="core-results">CORE RESULTS</h2>
<p>1、dense模型继续训练 vs upcycling</p>
<p>随着训练量的增加，upcycling相比dense模型继续预训练的优势逐渐扩大，如下图所示</p>
<img src="/a0824e29/1.png" class title="实验">
<p>2、下游任务模型效果</p>
<p>上面对比的是预训练模型。这些预训练模型经过微调后的效果对比如下。</p>
<img src="/a0824e29/2.png" class title="实验">
<p>相比预训练模型，微调模型表现出相对更大的震荡，不过大致趋势还是可以看出MoE模型更有优势。</p>
<p>3、MoE from scratch vs upcycling</p>
<p>从零开始训练的MoE和upcycling方法的对比如下</p>
<img src="/a0824e29/3.png" class title="实验">
<ul>
<li>从零开始预训练的MoE模型效果提升得更快，这可能得益于多样化的专家初始化和更大的lr。<br>
</li>
<li>只要给的计算资源足够多，从零开始训练的模型最终会赶上甚至超过upcycling的模型。<br>
</li>
<li>在有限的训练资源下，upcycling的训练效率更高，从零开始训练的模型大约需要相当于原dense模型1.2倍的训练资源才能达到upcycling模型的效果。如果现在的训练资源&lt;=训练dense模型的资源，那么sparse
upcycling是更划算的。</li>
</ul>
<p>4、sparse upcycling vs dense upcycling</p>
<p>对比《Scaling language models: Methods, analysis &amp; insights from
training gopher》中的depth tiling（dense upcycling） 和 sparse
upcycling的预训练效果，结果当然是sparse
upcycling效率更高点，如下图所示</p>
<img src="/a0824e29/4.png" class title="实验">
<p>（不过这里没有提及depth tiling之后的模型规模）</p>
<h2 id="消融实验">消融实验</h2>
<p>1、Amount of dense pretraining</p>
<p>upcycling的效果可能受用于初始化的dense模型的收敛情况影响，因此取了不同step的dense模型checkpoint作为upcycling的初始化，并且都继续训练了200k个step，结果如下图</p>
<img src="/a0824e29/a1.png" class title="实验">
<p>结论是基本上无论从哪个checkpoint初始化MoE模型，收益都比较稳定。</p>
<p>2、Router type</p>
<p>使用不同的router（expert choice和token choice）对比结果如下</p>
<img src="/a0824e29/a2.png" class title="实验">
<p>结论是，在相同的step下，expert choice和token
choice的效果基本一样，但是如果从时间上来看，使用expert choice
routing的模型训练更快。</p>
<p>3、Expert capacity factor</p>
<p>每个专家处理的token越多，计算量就越大，理论上效果也越好。</p>
<p>使用不同的capacity factor，模型效果对比如下</p>
<img src="/a0824e29/a3.png" class title="实验">
<p>结论是，虽然理论上增加专家容量可以提升效果，但时间上，通常C =
2的效率比较好，即一定的时间内提升的效果最多（注意计算资源是有限的）。</p>
<p>4、Number of MoE layers</p>
<p>在视觉模型上对MoE层数的效果进行了式样。</p>
<p>如下图右边两个小图，是使用不同的MoE层的效果，比如1表示只把最后一层MLP层替换为MoE层，以此类推</p>
<img src="/a0824e29/a4.png" class title="实验">
<p>结论是，更多的MoE层并不总是更好，大概是把5<sub>6层替换成MoE层的时候效果最好（40%</sub>50%的层数）。</p>
<p>5、Initialization of experts</p>
<p>对比了使用dense模型的MLP层初始化专家，和随机初始化专家，结果如下</p>
<img src="/a0824e29/a5.png" class title="实验">
<p>结果上看，使用dense模型的参数初始化专家效果更好。</p>
<p>6、Number of experts</p>
<p>如前面提到的，增加专家数并不会增大计算量，下图实验了2~128个专家下的效果</p>
<img src="/a0824e29/a6.png" class title="实验">
<p>结果上来看，效果是随着专家的增加而提升的，虽然最后表现出了收益递减的情况。</p>
<h2 id="其他">其他</h2>
<p>1、optimizer</p>
<p>在vision模型上，还尝试了使用dense模型的optimizer状态来训练MoE模型，但是并没有带来任何收益。</p>
<p>2、router normalization</p>
<p>另外，为了减少从dense到MoE初始化的performace
drop，尝试了对router的输出进行normalization，以保持每个token得到的weight总和是1。</p>
<p>这个做法直觉上应该是有益的，不过会有一个小问题，那就是对于只被一个expert选中的token，会有vanishing
routing gradients。</p>
<p>实践上，router
normalization在视觉模型上基本和不进行normalization的效果差不多，但是在语言模型上，会使得MoE模型效果变差。这二者的表现差异可能是因为语言模型上部分router使用了token
choice routing。</p>
<p>实际上目前大部分最新的MoE模型都没有开router
normalization，但这里的原因感觉还有待深入验证。</p>
<h1 id="小结">小结</h1>
<ul>
<li>如果资源无限，那么从零初始化MoE模型更好，这点和天工MoE的实验是相同的；<br>
</li>
<li>从结果上看，MoE层和专家数不是越多越好，但是多少是好，感觉至今其实没有很明确；<br>
</li>
<li>目前大部分的MoE都使用token choice
routing了，这里可能需要重新实验一下效果。</li>
</ul>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a><br>
<a href="https://www.linsight.cn/dd614e12.html">大模型算法题(7)</a><br>
<a href="https://www.linsight.cn/e287b9c3.html">大模型算法题(8)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】SPARSE UPCYCLING: TRAINING MIXTURE-OF-EXPERTS FROM DENSE
CHECKPOINTS https://arxiv.org/abs/2212.05055</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>预训练</tag>
        <tag>MoE</tag>
      </tags>
  </entry>
  <entry>
    <title>苹果的OpenELM</title>
    <url>/f845f3e4.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>苹果开源的OpenELM系列大模型包括整套规模（0.27B、0.45B、1.08B、3.04B）的最终模型、一些模型的中间checkpoint，training
log以及训练框架和具体配置等，这些资源都在https://github.com/apple/corenet可以找到，训练信息算是给得比较全面的了。</p>
<p>在苹果给出的效果对比中，OpenELM-1B算是同规模模型效果比较好的，比训练数据量多一倍的OLMo也要好一些，具体如下表</p>
<img src="/f845f3e4/intro.png" class title="OpenELM">
<p>这里整理一下OpenELM的方案细节。</p>
<h1 id="模型设计">模型设计</h1>
<p>OpenELM模型设计：<br>
- 所有linear层都没有使用bias<br>
- pre-norm + RMSNorm<br>
- RoPE<br>
- GQA<br>
- SwiGLU FFN<br>
- 和LLAMA相同的tokenizer</p>
<p>这些都是比较常规的设计。和目前其他主流模型比较不同的是，苹果参考《Delight:
Deep and light-weight transformer》，在OpenELM采用了layer-wise
scaling的设计。</p>
<p>通常来说，主流模型的设计是每层都使用一样的超参，比如注意力头的数量，和hidden
size的大小等。但是他们认为这样的设计在参数量的分配上不够高效，不能很好发挥这么多参数的效果，因此对每层的超参进行scaling。</p>
<p>假设模型共有N层，每层的宽度为 <span class="math inline">\(d_{model}\)</span>，MHA有 <span class="math inline">\(n_{h}\)</span> 个头，每个头的大小为 <span class="math inline">\(d_h=\frac{d_{model}}{n_h}\)</span>，FFN层的大小为
<span class="math inline">\(d_{\mathrm{FFN}}=m\cdot
d_{model}\)</span>，其中m是FFN multiplier。</p>
<p>layer-wise scaling使用 <span class="math inline">\(\alpha\)</span> 和
<span class="math inline">\(\beta\)</span>
两个参数来调整模型每层的attention head数量 <span class="math inline">\(n_{h}\)</span> 和FFN multiplier m。</p>
<p>具体调整如下：对于第i层，有</p>
<img src="/f845f3e4/formula.png" class title="公式">
<p>其中 <span class="math inline">\(\alpha_{min}\)</span> 和 <span class="math inline">\(\alpha_{max}\)</span>
是调整注意力头数量的超参，<span class="math inline">\(\beta_{min}\)</span> 和 <span class="math inline">\(\beta_{max}\)</span>
是调整宽度的超参。实践中，使用了 <span class="math inline">\(\alpha_{min}=0.5\)</span>，<span class="math inline">\(\alpha_{max}=1.0\)</span>，<span class="math inline">\(\beta_{min}=0.5\)</span>，<span class="math inline">\(\beta_{max}=4.0\)</span>。</p>
<p>（后面有整理各个模型结构和训练超参的表）</p>
<h1 id="数据">数据</h1>
<p>预训练数据来源包括：<br>
- RefinedWeb<br>
- deduplicated PILE<br>
- a subset of RedPajama<br>
- a subset of Dolma v1.6</p>
<p>总共1.8T token，各个来源具体token数量如下表</p>
<img src="/f845f3e4/data.png" class title="数据">
<p>数据中小于200 character或者小于256
token的数据都会被筛出来不使用。</p>
<h1 id="训练">训练</h1>
<p>训练超参：<br>
- 总共约350k step<br>
- AdamW optimizer<br>
- cosine learning rate schedule，warmup=5k<br>
- weight decay = 0.1<br>
- gradient clipping = 1.0</p>
<p>各个规模模型的结构超参、训练设置和资源消耗如下表</p>
<img src="/f845f3e4/pretrain_hp.png" class title="超参">
<p>微调超参如下</p>
<img src="/f845f3e4/sft_hp.png" class title="超参">
<h1 id="evaluation">evaluation</h1>
<p>下游在三大类型任务上评测了OpenELM：<br>
- Standard zero-shot tasks<br>
- OpenLLM leaderboard tasks<br>
- LLM360 leaderboard tasks</p>
<img src="/f845f3e4/eval_1.png" class title="评测">
<p>pretrained model在Standard zero-shot
tasks的7个任务上，不同checkpoint的效果如下图</p>
<img src="/f845f3e4/eval_2.png" class title="评测">
<p>可以看到随着训练步数增加，效果有上升趋势。</p>
<p>此外，研究人员观察到使用最后5个相隔5000
step的模型checkpoint进行滑动平均获得的模型参数，比单纯使用最后一个checkpoint要好，这可能是因为多个checkpoint平均可以去掉一些noise的影响。</p>
<p>后续评测都是使用平均后的模型。</p>
<p>OpenELM预训练模型在各个类型的任务上和其他模型对比如下</p>
<img src="/f845f3e4/eval_3.png" class title="评测">
<p>OpenELM微调前后的效果如下</p>
<img src="/f845f3e4/sft_result.png" class title="评测">
<p>对OpenELM分别使用LoRA和DoRA微调的效果对比如下</p>
<img src="/f845f3e4/peft_eval.png" class title="评测">
<h1 id="小结">小结</h1>
<p>OpenELM相比其他模型，最主要的变化是使用了layer-wise
scaling，看起来效果不错，但是苹果并没有在消融实验整个各个变化的有效性。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a><br>
<a href="https://www.linsight.cn/dd614e12.html">大模型算法题(7)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】OpenELM: An Efficient Language Model Family with Open Training
and Inference Framework https://arxiv.org/abs/2404.14619</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>技术报告</tag>
        <tag>苹果</tag>
      </tags>
  </entry>
  <entry>
    <title>苹果智能系统模型--AFM</title>
    <url>/1e34e252.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>之前苹果在WWDC24发布了包含多个强大模型的Apple
Intelligence系统，苹果刚刚最新发出来的技术报告《Apple Intelligence
Foundation Language Models》介绍了关于其中两个模型的一些细节 --
端侧使用的，大小约3B的AFM-on-device，和云侧使用的更大模型AFM-server（AFM=Apple
Foundation Model）。报告里没有给出AFM-server的规模。</p>
<h1 id="模型">模型</h1>
<p>模型的设计比较常规（没有和OpenELM一样玩底大头尖的设计）：</p>
<img src="/1e34e252/afm.png" class title="afm">
<p>几个细节：<br>
- 共享了输入输入的embedding，减少参数量<br>
- 参考《Small-scale proxies for large-scale transformer training
instabilities》，使用Query/key normalization，提升训练稳定性<br>
- RoPE的base frequency为500k</p>
<p>tokenizer是基于SentencePiece用BPE训的，所有数字都切分为单个数字。AFM-server模型的词表大小为100k，AFM-on-device则小一些，只有49k。</p>
<h1 id="预训练">预训练</h1>
<h2 id="数据">数据</h2>
<ul>
<li>数据的来源主要包括：开源数据，从出版商获得使用许可的数据，已经通过苹果的爬虫Applebot爬取的数据<br>
</li>
<li>苹果认为相比数量，预训练数据的质量对模型在下游任务的影响更大</li>
</ul>
<p>另外苹果（自称）特别看重隐私和安全性，因此所有数据的几乎全部流程都有大量移除有害数据、personally
identifiable information（PII）、成人内容的处理工作。</p>
<p>下面罗列一些预训练数据的处理细节。</p>
<p>1、网页数据</p>
<p>处理pipeline包括：<br>
- 结合Safari的reader mode和Boilerpipe算法提取网页的主体内容<br>
- 规则+model based的安全过滤<br>
- 基于locality-sensitive n-gram hashing的模糊去重<br>
- 质量过滤（《Large language model-guided document
selection》，《Datacomp-lm: In search of the next generation of training
sets for language models》）<br>
-
Decontamination：从预训练数据按n-gram删除和811个benchmark过度相关的数据，避免测试集污染</p>
<p>2、授权数据</p>
<p>从出版社获取的高质量长文本数据，主要用在二阶段和三阶段的预训练（各阶段方案在后面）。同样做了避免测试集污染的操作。</p>
<p>3、代码</p>
<p>来自github的开源仓库，包含14种语言，经过去重、PII过滤、质量过滤和Decontamination处理。</p>
<p>4、数学</p>
<p>包括3B数学QA内容，和14B数学相关的文档，来自数学论坛、博客、tutorial和seminar等。为了提取这些数据，苹果专门开发了对应的模板、数学符号filter、数学相关的quality
filter以及领域filter。</p>
<p>5、公开数据</p>
<p>从公开数据里挑了一部分高质量数据。</p>
<h2 id="训练">训练</h2>
<p>AFM的预训练分为3个stage：<br>
- core：大部分训练预算都在这一个阶段消耗<br>
- continued：上采样高质量数据，更多的code、math等内容<br>
-
context-lengthening：和continued类似，使用更大的训练窗口和长文本数据</p>
<p>三个stage在调参的时候，用了和《Small-scale proxies for large-scale
transformer training instabilities》中的“μParam
(simple)”类似的方法。</p>
<h3 id="core-pre-training">Core pre-training</h3>
<p>规模较大的AFM-server模型是从0开始训的，而较小的AFM-on-device则是从更大的模型蒸馏+剪枝来的。</p>
<p>1、AFM-server</p>
<ul>
<li>使用6.3T数据<br>
</li>
<li>sequence length = 4096<br>
</li>
<li>batch size = 4096<br>
</li>
<li>weight decay = 3.16e-4<br>
</li>
<li>cosine lr schedule, max lr = 0.01, min lr = 0.5% max lr<br>
</li>
<li>warmup step = 5000</li>
</ul>
<p>batch size是通过scaling
law的实验决定的，不过实践中发现，下游任务的效果对预训练的batch
size并不敏感：batch
size增大一倍或者缩小一半下游任务效果没有影响，因此虽然scaling
law给出的预测最佳batch
size是3072，实际训练的时候，为了效率还是使用了4096。</p>
<p>通过proxy
model的lr扫描，定了最佳lr在0.01~0.02，最终选择了0.01。（这里使用类似μParam的方法，各参数初始化和前向计算的时候应该都有缩放，所以这个lr会相对大一些）</p>
<p>苹果训练的时候选择的优化器是RMSProp with
momentum，而其他大部分大模型基本都是使用AdamW。</p>
<p>对于训练设置的问题，苹果做了消融实验，把上面的core
training和以下配置的训练（baseline）进行对比：<br>
- 使用AdamW，beta_1 = 0.9，beta_2 = 0.95<br>
- weight decay = 1e-4<br>
- lr 最小decay到0.0001<br>
- batch size = 1024</p>
<p>其他设置保持一致，用AFM-on-device模型结构训练3.1T数据。</p>
<p>二者的对比如下：</p>
<img src="/1e34e252/core_ablation.png" class title="afm">
<p>整体上AFM的core
training比baseline略略好一点，基本上可以认为是持平。</p>
<p>2、AFM-on-device</p>
<p>AFM-on-device模型不是从零训练的，而是基于一个6.4B的模型（使用和AFM-server一样的训练方法得到的），使用了structural
pruning和distillation得到的。</p>
<p>所用的structural pruning和《Structured pruning of large language
models》、《Sheared llama: Accelerating language model pre-training via
structured pruning》相似，除了几点变化：<br>
- 只对FFN层做prune<br>
- 使用Soft-Top-K masking（《Conditional adapters: Parameter-efficient
transfer learning with fast inference》）<br>
- 用了和core training一样的data mix训练了188B得到pruning mask</p>
<p>以得到的模型的为初始化，进行知识蒸馏：把原来core训练的target
label替换成：0.9 * teacher top-1 prediction + 0.1 * true label。</p>
<p>同样进行了6.3T的蒸馏训练。</p>
<p>相比直接从零训练，pruning和distillation在数据效率和最终结果上都有收益。使用不同方法训练出来的模型效果对比如下：</p>
<img src="/1e34e252/distill.png" class title="afm">
<p>整体来看，prune + distill能比多5倍training
cost的从零训练baseline更好一点，训练效率更高。</p>
<h3 id="continued-pre-training">Continued pre-training</h3>
<p>这一stage提高了math和code的比例，而降低了低质量的爬虫数据比例，进行了1T
token的训练。</p>
<p>训练设置：<br>
- sequence length = 8192<br>
- max lr = 3e-4，min lr = 0.1% max lr<br>
- weight decay = 1e-5<br>
- warmup step = 1000</p>
<p>其他和core training保持一致。</p>
<p>这一阶段数据蒸馏没有什么收益，所以AFM-on-device和AFM-server一样，采用直接训练的方式。</p>
<p>3、Context lengthening</p>
<p>最后这一阶段使用100B的长窗口训练来提升模型的长文本能力：<br>
- sequence length = 32768<br>
- RoPE base frequency 500k --&gt; 6315089（《Scaling laws of rope-based
extrapolation》）<br>
- 在二阶段数据的基础上，增加长的QA合成数据</p>
<h3 id="评测">评测</h3>
<p>三个阶段后，AFM-on-device和AFM-server的评测效果如下（报告提到，使用了internal的formulation，所以没法和其他模型直接比较）</p>
<img src="/1e34e252/pretrain_1.png" class title="afm">
<img src="/1e34e252/pretrain_2.png" class title="afm">
<p>continued
pre-training和预期的一样，对math和code的能力有比较大的提升。</p>
<h1 id="post-training">Post-Training</h1>
<p>AFM的post-training包括SFT和RLHF两个阶段，并使用了两个新方法iTec和MDLOO。</p>
<h2 id="数据-1">数据</h2>
<p>post-training的数据包括人类真实数据和合成数据。</p>
<h3 id="合成数据">合成数据</h3>
<p>一个好的reward model是合成高质量数据的关键，同时扩展prompt
set提高多样化和覆盖范围也很重要。</p>
<p>苹果介绍了数学、工具使用和代码这3个领域的数据合成。</p>
<p>1、Mathematics</p>
<p>数学数据的合成包括两个stage：<br>
- 生成数学问题<br>
- 生成对应答案</p>
<p>基于一些种子prompt，通过以下方法获取数量更大、更多样化的prompt：<br>
- Problem rephrase and reversion：参考《Metamath: Bootstrap your own
mathematical questions for large language models》，进行问题重述<br>
- Problem evolution：和指令进化类似（《WizardLM: Empowering large
language models to follow complex
instructions》），深度进化提升指令的复杂度，而广度进化提升话题的覆盖范围</p>
<p>2、Tool use</p>
<p>先从简单的single-tool数据开始，训练模型。然后逐步包含multi-tool和multi-step的问题，提升模型能力。此外，还会在数据里混入oracle
tool和其他相似同居，增加工具选择的难度。</p>
<p>另外还增加了tool intent detection数据，以减少过度使用工具的问题。</p>
<p>3、Coding</p>
<p>从71个话题的种子数据开始，通过self-instruct和rejection
sampling让模型自动化学习。</p>
<p>对于每个问题，模型会生成单元测试和多个solution，通过执行这些solution能够检验结果的正确性，组中选择通过测试最多的solution。</p>
<p>另外还会给通过的单元测试设定一个阈值，只要高于这个阈值才会被使用。最终得到了12k的高质量代码数据。</p>
<h2 id="sft">SFT</h2>
<p>1、数据选择</p>
<p>在质量过滤、去重之外，通过数据合成 + rejection
sampling来提供大量合成数据，提升SFT训练数据规模。</p>
<p>2、比例调整</p>
<p>对不同数据组成部分的权重进行训练，然后调整比例。对此进行了大量实验，移除掉一些影响较小的数据。</p>
<p>3、训练超参</p>
<p>模型使用constant
lr训练，AFM-server和AFM-on-device的lr分别为5e−6和2e−5。</p>
<p>和其他家做法比较不同的，苹果使用的0.1的dropout rate。</p>
<p>由于不同checkpoint的eval指标会有波动，因此使用RM选择best-of-N的方式来挑选最佳checkpoint。</p>
<h2 id="rlhf">RLHF</h2>
<p>苹果的RLHF有多轮，迭代提升模型。</p>
<h3 id="rm">RM</h3>
<p>用前面收集的偏好数据训练RM：<br>
- 每条prompt有两个response（对比一下，Llama-3可能有3条）<br>
- 偏好数据分为significantly better, better, slightly better, negligibly
better四个等级<br>
-
除了综合的对比之外，每条response还有细粒度的打分，维度包括指令跟随、真实性、有害性、简明程度，每个维度的打分有3个等级</p>
<p>RM取最后一层的最后一个non-padding
token的embedding，再加上一个linear层和4个MLPhead来输出打分。linear层输出偏好奖励，而4个MLP层分别输出4个细粒度打分的分类结果，四个分类头的输出分别是
<span class="math inline">\(u_\phi^\mathrm{if},u_\phi^\mathrm{verb},u_\phi^\mathrm{truth},u_\phi^\mathrm{harm}\)</span>。</p>
<p>RM训练时，使用soft label loss
function，这样可以把偏好的程度也纳入考虑。同时细粒度的打分也作为regularization
term加入训练，实验发现这些细粒度打分能提升RM的准确性。</p>
<p>1、Soft label loss</p>
<p>基于Bradley-Terry
model，y_c（c=chosen）比y_r（r=rejected）的概率是</p>
<p><span class="math display">\[\sigma(r_\phi(x,y_c)-r_\phi(x,y_r))\]</span></p>
<p>直观来说，如果两条response的质量差距越大，这个值应该越大。</p>
<p>因此对于不同的偏好程度l，设计了一个超参，target preference
probability p_l，并构造一个soft label loss：</p>
<p><span class="math display">\[\begin{aligned}
L_{\mathrm{ranking}}(\phi)=&amp;
-p_\ell\log(\sigma(r_\phi(x,y_c)-r_\phi(x,y_r))  \\
&amp;-\left(1-p_\ell\right)\log(\sigma(r_\phi(x,y_r)-r_\phi(x,y_c))
\end{aligned}\]</span></p>
<p>如果偏好程度比较高，那么p_l的值应该更大。实践中，p_l使用了0.95、0.85、0.75、0.65这四个值。</p>
<p>2、Single-sided grading as regularization</p>
<p>regularization loss如下：</p>
<p><span class="math display">\[\begin{aligned}L_{\mathrm{regu}}(\phi)&amp;=\sum_{\text{grade}\in\text{if,verb,truth,harm}}\left(\text{cross}_\text{entropy}(u_\phi^\mathrm{grade}(x,y_c),z_c^\mathrm{grade})\right.\\&amp;+\text{cross}_\text{entropy}(u_\phi^\mathrm{grade}(x,y_r),z_r^\mathrm{grade})\Big)\end{aligned}\]</span></p>
<p>其中z是各个细粒度维度的打分。</p>
<p>最终RM的训练loss为：</p>
<p><span class="math display">\[L_\text{ranking}(\phi)+\lambda
L_\text{regu}(\phi)\]</span></p>
<h3 id="iterative-teaching-committeeitec">Iterative teaching
committee（iTeC）</h3>
<p>苹果提出一个iterative RLHF框架来优化模型。</p>
<p>苹果在AFM的RLHF中，学到的最重要的事情之一就是“refresh online human
preference data collection using a diverse set of the best performing
models”。</p>
<p>具体来说，构建一个由SFT、拒绝采样、DPO/IPO和RL训练出来的最佳模型，以及前几轮的最佳模型组成的集合，称之为“model
committee”，并从这个model committee收集最新的偏好数据。</p>
<p>在获取最新的偏好数据之后，会更新RM，让后训练一组新的最佳模型，这些新的模型会加入model
comittee，继续下一轮迭代。</p>
<p>不同的优化算法训练出来的模型有不同的特点，比如使用负例的算法，online
RLHF、DPO、IPO等，在数学推理方面的能力较好，而rejection
sampling在指令遵循和写作方面更有效。通过在model
comittee进行采样，并用最新的RM进行排序，可以结合多个模型的强项。</p>
<h3 id="online-rlhf-algorithm-mdloo">Online RLHF algorithm: MDLOO</h3>
<p>从经典的RLHF优化目标出发：</p>
<p><span class="math display">\[\max_\theta\mathbb{E}_{x\sim\mathcal{D},y\sim\pi_\theta(\cdot|x)}\left[r_\phi(x,y)-\beta
D_{\mathrm{KL}}\left(\pi_\theta(\cdot|x)\|\pi_{\mathrm{ref}}(\cdot|x)\right)\right]\]</span></p>
<p>苹果选用的reward function是</p>
<p><span class="math display">\[R(x,y)=r_\phi(x,y)-\beta\log\frac{\pi_\theta(y|x)}{\pi_\text{ref}(y|x)}\]</span></p>
<p>这个reward function的前面的expectation是等价的。</p>
<p>和经典RLHF有所不同的是，这里把一整个response视为一个action，因此就不再需要critic模型来对每个token的reward进行打分了。</p>
<p>此外，苹果还做了几个改动。</p>
<p>1、Leave-One-Out (LOO) estimator of the advantage</p>
<p>对于底k个iteration的policy
model，每次输入n条prompt，每个prompt采样K条数据。</p>
<p>那么按照定义，advantage就是：</p>
<p><span class="math display">\[A_k(x,y_i)=R(x,y_i)-\mathbb{E}_{y\sim\pi_{\theta_k}(\cdot|x)}[R(x,y)]\]</span></p>
<p>苹果使用leave-one-out (LOO)方法来估计 <span class="math inline">\(A_k(x,y_i)\)</span>，即通过prompt
x和其他K-1个response：</p>
<p><span class="math display">\[\widehat{A}_k(x,y_i)=R(x,y_i)-\frac{1}{K-1}\sum_{j\neq
i}R(x,y_j)\]</span></p>
<p>同《Back to basics: Revisiting reinforce style optimization for
learning from human feedback in
LLMs》发现的一样，这样的advantage估计在RLHF有一些收益。另外，实践上发现这样做能让训练更加稳定。</p>
<p>2、Mirror descent policy optimization (MDPO)</p>
<p>和常用的clipping-based PPO不同，使用KL
divergence作为regularization。</p>
<h1 id="赋能apple-intelligence">赋能Apple Intelligence</h1>
<p>AFM是给Apple Intelligence使用的，而Apple
Intelligence主要是支持iPhone、iPad和Mac等设备的，因此「计算效率」和针对这些设备场景下的「专用能力」是重点。</p>
<p>虽然经过post-training之后，模型的通用能力已经不错，但是针对设别上的任务进行专门的微调，还能获得进一步的提升。苹果通过使用多个任务相关的adapter，在提升多个任务效果的同时，保持了参数和计算的高效。这些adapter很小，运行时可以在内存中随意切换。</p>
<p>整体的框架如下图所示</p>
<img src="/1e34e252/intelligence.png" class title="afm">
<h2 id="accuracy-recovery-adapter">accuracy-recovery adapter</h2>
<p>1、效果恢复</p>
<p>端侧设备的空间比较小，所以量化是必须要做的。首先，post-training后的模型会用4-bit的精度进行量化。</p>
<p>但是由于量化模型会带来一定的效果损失，所以这个量化模型并不是直接使用，而是会在固定量化模型的基础上，用16-bit的LoRA进行训练，以尽量恢复因为量化带来的效果损失。这个LoRA就叫accuracy-recovery
adapter。</p>
<p>accuracy-recovery
adapter的训练过程和主干模型的训练保持一致，也进行了pre-training和post-training的训练。不过由于参数量很小（只有几十MB），所以整个预训练大概只用了10B的数据，并且基本可以恢复大部分由于量化带来的效果损失。</p>
<p>实践上，rank
16基本上可以获得比较好的效果，不过出于灵活性的考虑，还是提供了不同rank的LoRA参数给下游使用：8、16、32。</p>
<p>模型量化前后，以及使用accuracy-recovery
adapter之后的效果对比如下：</p>
<img src="/1e34e252/recover.png" class title="afm">
<p>rank
16的adapter基本可以恢复大部分量化带来的效果损失，并且量化的损失越多，adapter恢复的比例越大。也就是使用了accuracy-recovery
adapter之后，基本可以不用太在意量化的损失，可以进一步提高模型压缩的程度。</p>
<p>2、Quantization schemes</p>
<p>以往量化的时候，因为要兼顾效率和效果损失，一般把block
size设成32或者64这样比较小的规模。现在有了accuracy-recovery
adapter，反正损失掉的基本都可以恢复，那block
size就可以设得更大了，甚至可以达到100k。</p>
<p>另外，由于AFM的输入输出embedding是shared的，为了有更好的效率，embedding部分使用8-bit的per-channel
quantization。</p>
<p>3、混合精度量化</p>
<p>模型中明显每层对效果的影响是不同的，对于对最终效果影响较小的层，苹果进一步用2-bit的量化精度，最终整体可以达到3.5~3.7的bpw。</p>
<h2 id="task-specific-adapter">task-specific adapter</h2>
<p>针对不同的下游任务，可以在accuracy-recovery
adapter的基础上再进一步微调。这样在保持主干网络为4-bit模型的情况下，下游任务就能有很好的效果。</p>
<p>以summarization为例，具体任务是对设备上的email、message和notification进行摘要。使用AFM-server模型，用设备上真实信息的格式构造训练数据，然后用这些训练数据训练adapter。</p>
<h1 id="小结">小结</h1>
<ul>
<li>模型设计、pre-training和post-training大部分使用的都是比较常规有效的做法，但是在训练上使用了RMSProp
with momentum，和其他大部分模型的做法不太一样。<br>
</li>
<li>accuracy-recovery
adapter看起来是比较合理有效的，看后面其他手机厂商怎么follow。</li>
</ul>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【推荐文章】</p>
<ul>
<li>MoE：<br>

<p style="line-height: 1.2;">
<small> <a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
</small>
</p></li>
<li>预训练：<br>

<p style="line-height: 1.2;">
<small> <a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
</small>
</p></li>
<li>数据：<br>

<p style="line-height: 1.2;">
<small> <a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
</small>
</p></li>
<li>长上下文：<br>

<p style="line-height: 1.2;">
<small> <a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
</small>
</p></li>
<li>推理加速：<br>

<p style="line-height: 1.2;">
<small> <a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
</small>
</p></li>
<li>对齐：<br>

<p style="line-height: 1.2;">
<small> <a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
</small>
</p></li>
<li>Transformer：<br>

<p style="line-height: 1.2;">
<small> <a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
</small>
</p></li>
<li>大模型算法题：<br>

<p style="line-height: 1.2;">
<small> <a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a><br>
</small>
</p></li>
</ul>
<h1 id="reference">Reference</h1>
<p>【1】Apple Intelligence Foundation Language Models
https://machinelearning.apple.com/papers/apple_intelligence_foundation_language_models.pdf</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>SFT</tag>
        <tag>技术报告</tag>
        <tag>post-training</tag>
        <tag>DPO</tag>
        <tag>RM</tag>
        <tag>RS</tag>
        <tag>苹果</tag>
        <tag>端侧模型</tag>
        <tag>蒸馏</tag>
      </tags>
  </entry>
  <entry>
    <title>预训练数据处理--长度分解</title>
    <url>/210dbccd.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>LLM预训练最重要的工作就是数据的准备，可以说90%的时间都在处理数据。</p>
<p>苹果提出Dataset
Decomposition，对数据按长度进行分桶，提升预训练的效率。</p>
<h1 id="预训练数据准备">预训练数据准备</h1>
<h2 id="concat-and-chunk">concat-and-chunk</h2>
<p>目前一般的做法是准备好文档数据后，获取对应的token，再通过特殊的分隔token把数据拼接起来，最后按训练窗口大小，切分成相应的长度，这就是concat-and-chunk。</p>
<p>这样的做法存在一些问题：<br>
-
不同的文档拼接在一起，如果在训练的时候没有对注意力进行特殊处理，那就会出现模型在对当前文档数据进行next
token prediction的时候，关注到前面拼接的不相关内容。<br>
-
不同文档的拼接训练效果上未必有很大帮助，但是却带来了额外的计算成本，因为attention的复杂度和输入长度的平方相关。<br>
-
多个文档拼接后切分，这样就有很多处于窗口边界的文档被切开，导致模型所见到的实际长度小于平均文档长度，影响了模型的长文本能力。</p>
<p>也有一些方法在concat-and-chunk基础上进行了改进，比如 document masking
(DM)、 best-fit sequence packing和in-context
pretraining等，后面会一起比较效果。</p>
<h2 id="variable-sequence-length">Variable Sequence Length</h2>
<p>针对以上问题，一个自然的想法就是做Dataset Decomposition（DD）。</p>
<p>数据集分解的规则有：<br>
-
把原文档数据集重构成多个bucket，每个bucket内的sequence有不同的长度<br>
- 每个bucket内的sequence都是一个完整文档的subsequence<br>
- 所有bucket内的token没有重复</p>
<p>符合这样规则的分解方法并不唯一，苹果提出一个具体的做法：对于长度为l的文档，按
<span class="math inline">\(l=2^{i_1}+2^{i_2}+\ldots+2^{i_k}\)</span>
进行长度分解，k个subsequence的长度都是2的幂次方。这就是Variable Sequence
Length（VSL）。</p>
<p>比如一个文档的长度为200，那么就分解成128 + 64 +
8三个长度的子序列。不同长度的子序列分别集中到对应长度的bucket中（D_i），如下图</p>
<img src="/210dbccd/buckets.png" class title="分解">
<p>最终各个bucket所包含的token数量分布如下图（b）。可以看到长度为512的序列总token数是最多的。此外长度为2^i的bucket主要也都是来源于长度为<span class="math inline">\(2^i\leq l&lt;2^{i+1}\)</span> 的文档。</p>
<img src="/210dbccd/dist.png" class title="分布">
<p>这样的切分方法能够保持切分后的序列长度分布基本和原数据一致。此外，上图（c）给出了concat-and-chunk和VSL的context
length分布对比。</p>
<h1 id="实验">实验</h1>
<p>基于VSL训练的时候，保证了每个batch中的token数b总不变的。训练时会先从多个bucket中进行采样（假设选中的bucket
i），选择当前要训练的长度，然后根据当前选择的长度，计算sample数量 = b /
2^i。</p>
<p>这样保持不同长度下都能有相同token数的做法，能够使得训练时不需要改变超参，比如learning
rate等。</p>
<p>使用所用模型基于OpenLM，具体参数如下</p>
<img src="/210dbccd/model.png" class title="模型">
<p>另外把RoPE的base frequency从10,000增加到100,000。不同base
frequency下的下游任务评测效果如下，使用100k的效果更好。</p>
<img src="/210dbccd/base_freq.png" class title="base frequence">
<p>评测任务分成两部分，regular language modeling benchmarks和long
context task。</p>
<p>regular包括：<br>
- Commonsense Reasoning (CSR):
PIQA-0-shot，COPA-0-shot，OpenBookQA-10-shots<br>
- Language Understanding (LU):
Lambada-OpenAI，Hellaswag-0-shot，Winograd-3-shots，WinoGrande-5-shots<br>
- Reading Comprehension (RC):
SQuAD-3-shots，BoolQ-0-shot，CoQA-0-shot<br>
- World Knowledge (WK):
Jeopardy-3-shots，ArcEasy-3-shots，ArcChallenge-3-shots，WikiDataQA-3-shots</p>
<p>long context包括：<br>
- Multi-Document Question Answering
(MDQA)：从NaturalQuestions-Open里抽的一些问题，多个从Wikipedia抽的文档<br>
- TOEFL：多项选择题<br>
- QuALITY：多项选择题</p>
<h2 id="训练效率">训练效率</h2>
<p>保持batch size = 8 * 8192，用OpenLM-1B/3B/7B分别训练 <span class="math inline">\(2^6\)</span> 到 <span class="math inline">\(2^13\)</span> 的窗口长度。</p>
<p>各个模型在不同窗口下的单步训练时间如下图</p>
<img src="/210dbccd/efficiency.png" class title="训练效率">
<p>随着窗口长度增大，每步的training overhead增长的速度也在加快。</p>
<p>对于concat-and-chunk方案，每步所需平均时间是固定的，而对于VSL，每步训练所需的平均时间是不同长度下的加权平均。</p>
<p>按前面分桶获得的长度分布，同样训练8k长度的情况下，VSL可以节省20%的训练时间。</p>
<h2 id="sequence-length-bias">Sequence length bias</h2>
<p>这个实验是为了了解使用不同长度的预训练数据对模型的效果有什么影响。</p>
<p>首先是在OpenLM-1B上，固定用不同长度的窗口进行训练，各个模型在评测任务上的标下如下图（a）</p>
<img src="/210dbccd/bias.png" class title="Sequence length bias">
<p>结果上，reasoning, language understanding和world
knowledge的准确性随着预训练长度的增加呈现出倒U型行为，而阅读理解则从随训练长度增加而变好。这样的结果可能和“与任务相关的数据在预训练语料中的长度分布”相关，比如和阅读理解任务相关的预训练数据，长度往往较长，这也就使得在更大窗口长度上训练出来的模型有更好的阅读理解能力。</p>
<p>上图（b）给出了各个下游任务的长度分布，结果也符合上面的分析。</p>
<p>但是还有一个问题，比如在适中长度数据上训练有收益的regular任务，其收益是来自于这些有合适长度的“数据”内容，还是只要窗口长度合适就行，而不论是什么数据？</p>
<p>对此另外训了两个模型：<span class="math inline">\(\mathcal{D}_{13\to10}\)</span> 和 <span class="math inline">\(\mathcal{D}_{7\to10}\)</span>，分别把8k数据拆成8个1k长度，和把8个128个数据拼接成1k长度窗口。结果如上图（c）。</p>
<p>首先 <span class="math inline">\(\mathcal{D}_{13\to10}\)</span> 比
<span class="math inline">\(\mathcal{D}_{13}\)</span>
有2.6分的提升，这证明了窗口长度的影响。</p>
<p>而 <span class="math inline">\(\mathcal{D}_{13\to10}\)</span> 又比
<span class="math inline">\(\mathcal{D}_{10}\)</span>
的得分低一些，这说明长的文档其内容和段文档与特定任务的惯性性确实有所不同。</p>
<p>此外，<span class="math inline">\(\mathcal{D}_{7\to10}\)</span> 相比
<span class="math inline">\(\mathcal{D}_{7}\)</span>
没有提升，这说明拼接并不能缓解长度相关性的问题。</p>
<h2 id="data-mixture">Data mixture</h2>
<p>训练数据集中，不同长度的比例如何影响结果呢？为了研究这个问题，这里用了7种不同的数据混合比例做实验（保持总token数相同）。</p>
<img src="/210dbccd/mixture.png" class title="Data mixture">
<p>首先，观察到较小上下文长度的mixture在MDQA上的表现较差，因为MDQA需要长上下文理解的能力。而较大的平均上下文长度也与阅读理解任务的性能呈正相关，但是训练的代价更大。此外，1k-only在regular任务的效果不错，但在长上下文任务则表现较差。</p>
<p>最后Natural分布在regular任务和MDQA任务上的效果都接近最佳，这证明了VSL方法在更大数据集上的可扩展性。</p>
<h2 id="length-based-curriculum">Length-based curriculum</h2>
<p>经验上，短文本的训练难度应该更低，而长文本则难度更大。</p>
<p>这个实验使用不同的采样比例，探索长度的curriculum
learning。各个采样比例和模型效果如下</p>
<img src="/210dbccd/curriculum.png" class title="Length-based curriculum">
<p>由于lr会衰减，这样可能导致采样比例较低的数据学习不足，因此使用了多个epoch来缓解这样的差异影响。</p>
<p>结果显示，“Grow-P2” curriculum在不同的指标上接近最优。</p>
<p>curriculum learning另一个好处是训练的稳定性。《The
stability-efficiency dilemma: Investigating sequence length warmup for
training gpt
models》中就提到长序列在训练的开始阶段会导致比较极端的梯度方差，从而造成训练的不稳定。</p>
<h2 id="scaling">Scaling</h2>
<p>下图（a）显示，VSL方法在不同的总训练量下，在regular任务上的效果都比更高，而训练效率也大概提升了一倍。</p>
<img src="/210dbccd/scaling.png" class title="Scaling">
<p>（但是这里没有报告长文本的效果？）</p>
<p>而图（b）则显示在不同规模的模型上，VSL也有稳定收益。</p>
<h2 id="和sota的对比">和sota的对比</h2>
<p>VSL和document masking (DM)、 best-fit sequence packing、in-context
pretraining的效果对比如下</p>
<img src="/210dbccd/sota.png" class title="和sota的对比">
<p>DM相比baseline-8k在regular的效果有提升，但是长文本的能力下降了。如《Effective
long-context scaling of foundation
models》指出的，即使的不想管的文档拼接，在长文本上也有一定的收益，因此DM直接屏蔽不相关文档的做法对长文本效果有损害。</p>
<p>best-fit sequence
packing通过更好地组合文档，在regular任务和长上下文任务都有所提升。</p>
<p>in-context
pretraining则是拼接相似文档构建长数据序列，因此长文本效果有提升，不过相似性搜索对资源的消耗比较大。</p>
<p>和以上的方法相比，VSL方法在效果和成本上算是有比较好的平衡。</p>
<h1 id="小结">小结</h1>
<ul>
<li>这些实验探索了窗口长度、文档长度和下游各个任务效果的关系，能给我们做特定任务提升提供一些参考。<br>
</li>
<li>通过长度对数据集进行分解的方案在效果上看起来略有提升，主要是能在资源消耗模型训练稳定性上有帮助，在大规模模型上，这应该更为重要。<br>
</li>
<li>DeepSeek-V2、LLAMA3.1和苹果的DCLM也都提到了关于长度的curriculum
learning，这还是一个值得尝试的方法。</li>
</ul>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a> <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a> <a href="http://www.linsight.cn/1736008.html">(3)</a> <a href="http://www.linsight.cn/1736008.html">(4)</a> <a href="http://www.linsight.cn/336f2f3e.html">(5)</a> <a href="http://www.linsight.cn/7c04944d.html">(6)</a> <a href="https://www.linsight.cn/dd614e12.html">(7)</a> <a href="https://www.linsight.cn/e287b9c3.html">(8)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】Dataset Decomposition: Faster LLM Training with Variable
Sequence Length Curriculum https://arxiv.org/abs/2405.13226</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>预训练</tag>
        <tag>数据</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型算法题(8)</title>
    <url>/e287b9c3.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<p>本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~</p>
<p>如有错漏，欢迎指正~</p>
<hr>
<h1 id="激活函数gelu是怎么设计的有什么优点">1.激活函数GeLU是怎么设计的，有什么优点？</h1>
<p>GeLU函数在2016年在论文《Gaussian Error Linear Units
(GELUs)》中提出。</p>
<p>之前比较常用的激活函数ReLU具有计算快，有激活值的时候不容易出现梯度消失等优点，但是ReLU是个分段函数，存在不可导的断点，可能会对模型效果有所影响。此外，ReLU是确定性的激活函数，一般会加入随机正则项（如dropout，随机将部分输出置0）以提高模型的泛化能力。</p>
<p>而GeLU被设计为一种自带随机正则属性的激活函数，即输入是否置0，取决于当前的输入和其他输入的对比：</p>
<p>GeLU(x) = x<em>P(X≤x)=x</em>Φ(x)</p>
<p>其中Φ(x)是标准正态分布的累积分布函数。GeLU精确值的计算比较复杂，因此论文给出了近似解，图像如图。</p>
<img src="/e287b9c3/1.png" class title="图">
<p>GeLU相比ReLU更为光滑，在处理负数的时候也不会直接输出0，减少了梯度消失和神经元死亡的情况，实际使用中效果也较好。</p>
<h1 id="gshard和switch-transformer对moe模型都提出expert-capacity的概念即每个expert在一个batch中最多能处理的token数设定不同的expert-capacity对模型会有什么影响">2.Gshard和Switch
Transformer对MoE模型都提出expert
capacity的概念，即每个expert在一个batch中最多能处理的token数。设定不同的expert
capacity，对模型会有什么影响？</h1>
<p>expert
capacity模型容量限定了一个batch中最多能处理的token数。如果一个token被分配到已经满载的expert，则这个token不会被处理，而是通过残差链接透传给下一层，这种情况称为overflow。overflow会对最终效果有损害，因为有些token没有被完整计算。</p>
<p>比如一个batch里有N个token，每层有E个expert，每个token被分配到2个expert，假设我们设置了expert
capacity =
2N/E。只有当所有token完美平衡分配才不会出现overflow。显然，expert
capacity设置得越大，越能兼容分配不平衡的情况，overflow的情况越少；但是分配不平衡的情况会对模型的计算效率有损害，因为部分expert被频繁使用，而另外一部分可能使用很少，造成计算资源浪费。</p>
<p>下图展示了不同expert capacity下的情况</p>
<img src="/e287b9c3/2.png" class title="图">
<h1 id="重计算recomputation是什么有什么用">3.重计算recomputation是什么，有什么用？</h1>
<p>通常来说，使用更大的模型或者更大的batch
size可以获得更好的训练效果。但是更多的激活值也带来了显存不足的问题。正常来说，为了进行反向传播，我们需要保存模型中每个节点的激活值，比如使用一个4层的模型，每层的和输出大小都为8，那么一共就要保存32个激活值。如果设备的显存只够储存24个值，这时候就会出现OOM。一个解决方法就是，我们只保存第1和第3层的激活值，那么就只需要保存16个值。在计算第4层的反向传播时，先拿第3层的激活值重新计算第4层的前向结果，再进行正常反向传播；在计算第2层的的反向传播时，先拿第1层的激活值重新计算第2层的前向结果，以此类推。这里重新计算第4层、第2层的前向结果的做法，就是重计算recomputation。</p>
<p>重计算可以节省显存，使得同样的设备可以训练更大的模型，但是由于需要多次计算前向结果，训练速度就会比较慢。因此重计算是用时间换空间的方法。</p>
<h1 id="moe模型中使用更小的expert有什么优势">4.MoE模型中，使用更小的expert有什么优势？</h1>
<p>更小的expert在同样总参数量和激活参数的情况下，可以提供更多的排列组合，使得每个expert在训练过程中能达到更高程度的专家化。比如原来每层有16个专家，每个专家大小为128，每次激活两个，那么组合情况总共有120种不同的组合；如果使用64个专家，每个专家大小为32，每次激活8个，那么就有4,426,165,368种不同的组合。</p>
<p>另外，专家数量不够多的情况下，一个专家就可能要负责学习多个领域的内容。以学习高中知识为例，在只有两个专家的时候，只能一个专家学习理科知识，另一个学习文科知识；当我们有8个专家的时候，不同专家就可以分别学习语文、英语、历史、地理、物理、生物、化学、数学知识。显然后者所学知识的专业化程度更高。</p>
<h1 id="moe模型中使用的expert数量多少对模型有什么影响">5.MoE模型中，使用的expert数量多少对模型有什么影响？</h1>
<p>（1）专家的数量越多，同样的输入，给每个专家分配的平均token越少。这在训练时可能导致batch
size过小，影响训练效果。</p>
<p>（2）专家的数量越多，训练和推理时所需的设备越多，多个设备进行通讯的成本会变高。</p>
<p>（3）专家的数量如果比较少，提供的组合数就比较少；专家数量多的话可以提供更多可能的组合。</p>
<p>（4）专家的数量多，模型的总参数量更大，理论上模型能具有更大的容量，上限更高。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a><br>
<a href="https://www.linsight.cn/dd614e12.html">大模型算法题(7)</a></p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>算法题</tag>
      </tags>
  </entry>
  <entry>
    <title>稀疏注意力计算:sliding window attention</title>
    <url>/c61d17e3.html</url>
    <content><![CDATA[<p>【本文已在同名微信公众号/知乎/个人博客同步上线】</p>
<p>LLM的长文本能力现在已经是各个大模型巨头的必争之地。</p>
<p>我们之前在<a href="http://www.linsight.cn/c4da56c0.html">《LLM长上下文的问题》</a>简单介绍了目前把大模型理解和生成能力推广到32k+/128k+的主流方法，在<a href="http://www.linsight.cn/3dc22f96.html">《理解Attention:从起源到MHA,MQA和GQA》</a>一文中也解析了MQA和GQA通过节省KV缓存的方式，支持模型在长上下文情况下推理加速的方案。</p>
<p>在这讲一下另一种（理论有损）提升注意力计算效率的方法：SWA（sliding
window attention）。</p>
<p>一些效果受到广泛关注的模型，如Qwen系列和Mistral就使用了SWA。</p>
<p>关于Mistral：</p>
<p>Mistral
AI是法国一家AI独角兽公司，2023年5月才成立，但是在2023年9月和12月就分别推出了Mistral
7B和MoE模型Mistral 8x7B并开源。</p>
<p>2024年2月，微软也投资了它。</p>
<img src="/c61d17e3/ms_invest_mistral.png" class title="MS">
<p>它在2024年2月发布的Mistral Large，支持多语言 &amp;
32k的上下文长度，在MMLU上也是获得了直逼GPT4的效果</p>
<img src="/c61d17e3/mistral_large_performance.jpeg" class title="Mistral Large MMLU Performance">
<p>（大家也因此对Mistral寄予了厚望，希望它能成为大模型行业的鲶鱼，激活一下OPENAI和META加速一下开源。）</p>
<h1 id="swa">SWA</h1>
<p>虽然SWA的思路最早不是Mistral提出的，我们还是先以Mistral
7B为例来看下SWA的具体做法。</p>
<h2 id="mistral-7b">Mistral 7B</h2>
<p>2023年10月，Mistral发布了Mistral 7B的<a href="https://arxiv.org/pdf/2310.06825.pdf">技术报告</a>。其中开篇就说到，相比Llama，Mistral在结构上做了一些改动，除了GQA，另一个用于支持长文本下高效推理的改动就是SWA。</p>
<p>来看下Mistral 7B的模型结构参数</p>
<img src="/c61d17e3/mistral_architechture.png" class title="Mistral Architechture">
<p>Mistral使用了kv组数=8的GQA，intermediate
size相比Llama2（11008）大一些，其他基本没有太大变化。</p>
<h2 id="计算量和缓存">计算量和缓存</h2>
<p>对于原始的causal
attention，其注意力矩阵是一个下三角矩阵，这样每个token都能看到自己和在自己前面的token。</p>
<p>这样随着输入长度 <span class="math inline">\(s\)</span>
增大，这个下三角矩阵中1的元素数量以 <span class="math inline">\(s^2\)</span> 的速度增长，带来的是计算量和所需的KV
Cache以平方的速度增长。</p>
<p>（我们知道计算量/缓存和长度 <span class="math inline">\(s\)</span>
成平方关系，这里放一些更具体的推算细节，已经熟悉的朋友可以跳过）</p>
<p>（1）计算量</p>
<p>对于两个这样大小的矩阵相乘： <span class="math inline">\([m,n]\times[n,p]\)</span> ，输出矩阵大小为 <span class="math inline">\([m,p]\)</span>，共有 <span class="math inline">\(m\times p\)</span> 个元素，每个元素需要 <span class="math inline">\(n\)</span> 次乘法和 <span class="math inline">\(n\)</span> 次加法，因此一次矩阵乘法有 <span class="math inline">\(2mpn\)</span> 个floating point
operations（FLOPs）。</p>
<p>计算量上，按<a href="https://arxiv.org/pdf/2203.15556.pdf">《Training
Compute-Optimal Large Language Models》</a>的算法来。</p>
<p>对于一般MHA，输入长度为 <span class="math inline">\(s\)</span>
，层数为 <span class="math inline">\(L\)</span> ，模型hidden size为
<span class="math inline">\(d_{model}\)</span> ，每个头的维度为 <span class="math inline">\(d_{q}\)</span> ， 头的数量为 <span class="math inline">\(n_{q}\)</span>（这里假设有 <span class="math inline">\(d_{model} = n_{q}\times d_{q}\)</span>
），各个operation的FLOPs如下</p>
<center>
<table>
<colgroup>
<col style="width: 45%">
<col style="width: 54%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Operation</th>
<th style="text-align: center;">FLOPs（MHA）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Attention: QKV</td>
<td style="text-align: center;"><span class="math inline">\(6\times
s\times h_{model}^{2}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Attention: QK logits ( <span class="math inline">\(QK^T\)</span> )</td>
<td style="text-align: center;"><span class="math inline">\(n_{q}\times
2\times s^2\times h_{q}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Attention: Softmax</td>
<td style="text-align: center;"><span class="math inline">\(n_{q}\times
3\times s^2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Attention: Reduction (apply to <span class="math inline">\(V\)</span>)</td>
<td style="text-align: center;"><span class="math inline">\(n_{q}\times
2\times s^2\times h_{q}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Attention: Outupt Linear Project</td>
<td style="text-align: center;"><span class="math inline">\(2\times
s\times h_{model}^{2}\)</span></td>
</tr>
</tbody>
</table>
</center>
<p>Softmax项中，对一个 <span class="math inline">\([1,s]\)</span>
的向量做softmax，计算量为 <span class="math inline">\(3s\)</span> （一个
<span class="math inline">\(s\)</span> 是算每个元素的exp，一个 <span class="math inline">\(s\)</span> 是求和算分母，一个 <span class="math inline">\(s\)</span> 是算除法），而对 <span class="math inline">\([s,s]\)</span> 的矩阵做softmax，则计算量为 <span class="math inline">\(3s^2\)</span> ，每个头都要计算一遍，因此再乘以
<span class="math inline">\(n_{q}\)</span> 。</p>
<p>（这里忽略了其他一些operation，比如scaling，dropout等，有兴趣的朋友可以自己推算一下）</p>
<p>顺便算下对于Mistral 7B这样使用了GQA的情况。</p>
<p>其实只有第一项的KV有变化，其他都没变。假设kv头的数量为 <span class="math inline">\(n_{kv}\)</span>，则有</p>
<center>
<table>
<colgroup>
<col style="width: 45%">
<col style="width: 54%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Operation</th>
<th style="text-align: center;">FLOPs（GQA）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Attention: QKV</td>
<td style="text-align: center;"><span class="math inline">\(2\times
s\times h_{model}^{2}\\+4\times s\times h_{model}\times (h_{q}\times
n_{kv})\)</span></td>
</tr>
</tbody>
</table>
</center>
<p>从上面的推算可以看到QK logits、Softmax和Reduction三项是和长度 <span class="math inline">\(s\)</span> 成平方关系的，其他则是线性关系。</p>
<p>（2）缓存</p>
<p>KV Cache需要缓存的参数量为</p>
<p><span class="math display">\[
2\times L\times s\times d_{q}\times n_{kv}
\]</span></p>
<p>如果使用的是半精度浮点数，那么总共所需的空间就是</p>
<p><span class="math display">\[
2\times 2\times L\times s\times d_{q}\times n_{kv}
\]</span></p>
<p>对于Mistral 7B，在输入长度为16k的情况下，所需的KV_Cache约为2G。</p>
<p>看来虽然用了GQA，但是在长文本（16k+）的情况下计算量和显存还是颇有压力。</p>
<h2 id="swa思路">SWA思路</h2>
<p>看来要提升attention计算效率，需要想办法减小上面推算中的 <span class="math inline">\(s\)</span> ，但是怎么在减小 <span class="math inline">\(s\)</span>
的同时，还能保持模型长上下文的理解和生成能力呢？</p>
<p>来看一下，CNN中的感受野</p>
<img src="/c61d17e3/receptive_field_cnn.png" class title="CNN Receptive Field">
<p>如上图，假设模型有3层，每层卷积核大小为 <span class="math inline">\(3\times 3\)</span>
（实际上CNN里卷积操作就是一个sliding window）。</p>
<p>那对于layer 3，每一个像素能看到layer 2中的一个 <span class="math inline">\(3\times 3\)</span> 的区域，layer
2中其他较远的像素就看到不了。</p>
<p>但我们再往前推，layer 2里的每个像素也可以看到layer 1中的一个 <span class="math inline">\(3\times 3\)</span> 区域，那么layer 2中的 <span class="math inline">\(3\times 3\)</span> 区域就可以看到layer 1中一个
<span class="math inline">\(5\times 5\)</span> 的区域，相当于layer
3中一个像素可以<u><strong>间接</strong></u>看到一个 <span class="math inline">\(5\times 5\)</span> 的输入。</p>
<p>以此类推，如果我们再增加一层layer 4，那么layer
4中一个像素就能获取输入层（layer 1） 一个 <span class="math inline">\(7\times 7\)</span> 区域的信息。</p>
<p>虽然每层只能多看周围一格的信息，但是只要我们层数够多，理论上靠近输出端的层想看多远就能看多远。</p>
<p>值得注意的一点是，我们一般认为模型低层部分提取比较基础的特征，而高层会提取高级的语义特征。</p>
<p>在CNN里，前几层提取的可能更多是关于简单的边界、颜色、形状等基础特征，而后面的层则提取较复杂的语义特征，比如在分类任务中会是和分类类别相关的花纹、物体大小、风格等特征。</p>
<p>如果我们把模型设计成，最后一层的一个像素刚好要到第一层才能接收到全局信息（在其它层都只能看到局部），那对于图像边缘的语义特征识别能力可能会受到一些限制。</p>
<p>具体来说，假设我们做猫和狗的图像分类任务，如果这个时候决定性的特征出现在图像最边缘几个像素里，那这种情况下的错误率会比特征出现在图像中间时要高。</p>
<p>而对于语言模型，一般情况下，越远距离的信息，对当前位置的重要性越低，因此只要我们的窗口大小不要太过极限小，问题应该都还不大。</p>
<p>看下Mistral的SWA具体是怎么做的</p>
<img src="/c61d17e3/mistral_swa.png" class title="Mistral SWA">
<p>左边是正常的causal
attention，每个位置能看到自己和前面的位置，attention
mask是个下三角矩阵。</p>
<p>中间则是SWA的attention
mask，这里的窗口大小为3。包括自己在内，每个位置只能往前看3个输入。</p>
<p>同CNN的感受野一样，随着层数的堆叠，模型理论上能处理的最远距离也逐层线性递增。只是LLM里递增的方向是单向的，只能往前。</p>
<p>Mistral 7B使用了4096的窗口大小，模型层数为32，则最终输出的”感受野“为
<span class="math inline">\(4096\times 32=131,072\)</span>
达到131k的长度。</p>
<p>前面我们推算了attention的计算量，其中QK
logits、Softmax和Reduction三项是和长度 <span class="math inline">\(s\)</span>
成平方关系。在使用了SWA之后，理论上，这几个operation仅使用4k的计算量，就能获得131k的上下文效果。当输入长度为131k时，除去已经缓存部分的数值，新的输入计算量相差
<span class="math inline">\(32\times 32=1024\)</span> 倍。</p>
<p>而缓存和上下文长度 <span class="math inline">\(s\)</span>
成线性关系，当上下文长度为131k时，最大也能节省 <span class="math inline">\(31/32\)</span> 的显存。</p>
<p>即SWA在上下文长度在4k以下时，和普通causal
attention一样；当上下文长度超过4k时，则相对节省资源，长度越大，节省的比例越高。</p>
<blockquote>
<p>In practice, for a sequence length of 16K and W = 4096, changes made
to FlashAttention [11] and xFormers [18] yield a 2x speed improvement
over a vanilla attention baseline.</p>
</blockquote>
<p>实际使用中，Mistral通过把SWA实现在FlashAttention和xFormers中，对于16k的上下文长度，获得了2倍的速度提升。</p>
<h2 id="和kv-cache的配合实现">和KV Cache的配合实现</h2>
<p>在不使用sliding window的情况下，随着自回归推理的进行，KV
Cache是只增不减的。</p>
<p>而在使用SWA的情况下，超出窗口长度的kv就可以不用再缓存了，因此使用一个轮转替换的策略。</p>
<p>比如窗口大小 <span class="math inline">\(W=4\)</span>
，则当第5个token需要缓存是，直接替换掉第1个token，这样就可以保持kv缓存有一个最大值（为窗口大小），而不会无限增长。</p>
<img src="/c61d17e3/rolling_buffer.png" class title="swa rolling buffer">
<p>这样便于我们估计硬件设备所能支持的throughput，也不会因为少量超长的case而造成堵塞，在工程上有利于提高硬件利用率，降低成本。</p>
<h2 id="长prompt的分块">长Prompt的分块</h2>
<p>更近一步，考虑到我们使用RAG或者funciton
call的时候，都会使用比较长的，固定的prompt来知道模型的行为。</p>
<p>比如GPT4就被诱导说出它接收到的长system
prompt（当然未必真的就是OPENAI用的）</p>
<blockquote>
<p>Your user's user agent is "Mozilla/5.0 (Windows NT 10.0; Win64; x64)
AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
and the user's locale is "en-US" Your knowledge cutoff date is 2023-04.
The current date is 2024-02-07. Image input capabilities: Enabled</p>
<p>Tools</p>
<p>python</p>
<p>When you send a message containing Python code to python, it will be
executed in a stateful Jupyter notebook environment. python will respond
with the output of the execution or time out after 60.0 seconds. The
drive at '/mnt/data' can be used to save and persist user files.
Internet access for this session is disabled. Do not make external web
requests or API calls as they will fail.</p>
<p>dalle</p>
<p>Whenever a description of an image is given, create a prompt that
dalle can use to generate the image and abide to the following policy:
1. The prompt must be in English. Translate to English if needed. 2. DO
NOT ask for permission to generate the image, just do it! 3. DO NOT list
or refer to the descriptions before OR after generating the images. 4.
Do not create more than 1 image, even if the user requests more. 5. Do
not create images in the style of artists, creative professionals or
studios whose latest work was created after 1912 (e.g. Picasso, Kahlo).
- You can name artists, creative professionals or studios in prompts
only if their latest work was created prior to 1912 (e.g. Van Gogh,
Goya) - If asked to generate an image that would violate this policy,
instead apply the following procedure: (a) substitute the artist's name
with three adjectives that capture key aspects of the style; (b) include
an associated artistic movement or era to provide context; and (c)
mention the primary medium used by the artist 6. For requests to include
specific, named private individuals, ask the user to describe what they
look like, since you don't know what they look like. 7. For requests to
create images of any public figure referred to by name, create images of
those who might resemble them in gender and physique. But they shouldn't
look like them. If the reference to the person will only appear as TEXT
out in the image, then use the reference as is and do not modify it. 8.
Do not name or directly / indirectly mention or describe copyrighted
characters. Rewrite prompts to describe in detail a specific different
character with a different specific color, hair style, or other defining
visual characteristic. Do not discuss copyright policies in responses.
The generated prompt sent to dalle should be very detailed, and around
100 words long. Example dalle invocation: { "prompt":
"<insert prompt here>" } namespace dalle {</insert></p>
<p>Create images from a text-only prompt. type text2im = (_: { The size
of the requested image. Use 1024x1024 (square) as the default, 1792x1024
if the user requests a wide image, and 1024x1792 for full-body
portraits. Always include this parameter in the request. n?: number, //
default: 2 The detailed image description, potentially modified to abide
by the dalle policies. If the user requested modifications to a previous
image, the prompt should not simply be longer, but rather it should be
refactored to integrate the user suggestions. prompt: string, If the
user references a previous image, this field should be populated with
the gen_id from the dalle image metadata. referenced_image_ids?:
string[], }) =&gt; any; } // namespace dalle</p>
<p>voice_mode Voice mode functions are not available in text
conversations. namespace voice_mode { } // namespace voice_mode</p>
<p>browser</p>
<p>You have the tool <code>browser</code>. Use <code>browser</code> in
the following circumstances: - User is asking about current events or
something that requires real-time information (weather, sports scores,
etc.) - User is asking about some term you are totally unfamiliar with
(it might be new) - User explicitly asks you to browse or provide links
to references</p>
<p>Given a query that requires retrieval, your turn will consist of
three steps: 1. Call the search function to get a list of results. 2.
Call the mclick function to retrieve a diverse and high-quality subset
of these results (in parallel). Remember to SELECT AT LEAST 3 sources
when using <code>mclick</code>. 3. Write a response to the user based on
these results. In your response, cite sources using the citation format
below.</p>
<p>In some cases, you should repeat step 1 twice, if the initial results
are unsatisfactory, and you believe that you can refine the query to get
better results.</p>
<p>You can also open a url directly if one is provided by the user. Only
use the <code>open_url</code> command for this purpose; do not open urls
returned by the search function or found on webpages.</p>
<p>The <code>browser</code> tool has the following commands:
<code>search(query: str, recency_days: int)</code> Issues a query to a
search engine and displays the results.
<code>mclick(ids: list[str])</code>. Retrieves the contents of the
webpages with provided IDs (indices). You should ALWAYS SELECT AT LEAST
3 and at most 10 pages. Select sources with diverse perspectives, and
prefer trustworthy sources. Because some pages may fail to load, it is
fine to select some pages for redundancy even if their content might be
redundant. <code>open_url(url: str)</code> Opens the given URL and
displays it.</p>
<p>For citing quotes from the 'browser' tool: please render in this
format: 【{message idx}†{link text}】. For long citations: please render
in this format: <a href="message%20idx">link text</a>. Otherwise do not
render links.</p>
</blockquote>
<p>除了预先计算好system
prompt的kv值，并保存在缓存中方便每次用户输入使用外，如果system
prompt很长（比sliding window大），还可以通过对system
prompt的kv值进行切分来进一步优化计算。</p>
<p>比如窗口大小 <span class="math inline">\(W=4\)</span>，system
prompt大小为9时，就可以把system prompt的kv缓存切成 [4,4,1] 三块。</p>
<p>第一块由于和当前的输入距离超过了一个window的大小，所以是完全看不见的，对应的attention
mask全为0，因此可以完全忽略。</p>
<p>第二块的attention
mask则是一个上三角矩阵，当前的输入需要用到这部分信息。</p>
<p>第三块是一个下三角矩阵（的左边部分），包含了当前的输入在内。</p>
<p>在推理的时候，我们只需要用到第二块和第三块的内容，这就节省了缓存的操作。</p>
<p>而且无论prompt有多长，只要我们按窗口大小分块，一定只会用到最后两块。</p>
<img src="/c61d17e3/prefill_and_chunking.png" class title="prefill and chunking">
<p>（实际上现在推理框架基本上都有FlashAttention/PagedAttention等技术加持，能够进一步节省资源，提高效率，这个后续再开一篇讲）</p>
<p>Mistral
7B整体的效果上的效果相比Llama是有优势的，部分任务甚至超过了Llama
34B。</p>
<img src="/c61d17e3/mistral_perf.png" class title="mistral performance">
<p>Mistral认为大语言模型压缩知识的能力实际超过我们的认知，7B这个规模的效果还有提升空间。</p>
<h1 id="sparse-attention">Sparse Attention</h1>
<p>SWA实际上是一种sparse attention，而sparse
attention也有许多工作做了深入探索。</p>
<p>这里简单说一小部分，有机会再完整梳理一遍sparse
attention的理论和实践。</p>
<h2 id="longformer">Longformer</h2>
<p>前面提到，Mistral并不是第一个使用SWA的。</p>
<p>2020年，<a href="https://arxiv.org/pdf/2004.05150.pdf">《Longformer:
The Long-Document Transformer》</a>就提出包含SWA在内的一系列sparse
attention的做法。</p>
<p>从文章名字就看到出来，Longformer主要目的也是为了解决长上下文的问题。</p>
<img src="/c61d17e3/longformer_attention.png" class title="longformer">
<p>上图中的（b）就是SWA，只是用在Bert中的时候它是双向的。</p>
<p>在SWA的基础上，还可以进行空洞滑窗（dilated sliding
window），在不增加计算量的情况下，提升感受野。这也是从空洞卷积（下图）来的灵感了。</p>
<img src="/c61d17e3/dilated_conv.png" class title="dilated convolution">
<p>还可以更进一步优化attention。无论是SWA还是dilated sliding
window，每个位置都只能看到局部的信息。</p>
<p>但是实际上有些位置就是对全局信息有很高的需求。</p>
<p>在Bert中，[CLS]
token就常常作为分类token或者相似度向量使用，这种情况下就需要它能获取整个上下文的完整信息。</p>
<p>而在GPT中，instruction，或者说prompt的部分也对全局信息有更高要求，因为我们希望在整个对话过程中，模型都能遵循我们给出的规则。</p>
<p>对于这些token，我们让它可以看到其他所有位置，使用完整的global
attention，而其他位置则使用sliding window，如（d）中所示。</p>
<h2 id="big-bird">Big Bird</h2>
<p>无独有偶，同样在2020年，和Longformer差不多在同一时期，也有另外一个通过sparse
attention来优化长文本效果的工作，<a href="https://arxiv.org/abs/2007.14062">《Big Bird: Transformers for
Longer Sequences》</a>。</p>
<p>其中sliding window和global attention结合的思路和Longformer相似。Big
Bird还额外加入了一个random attention的做法。</p>
<img src="/c61d17e3/big_bird_attention.png" class title="big bird attention">
<p>上图中 <span class="math inline">\(r=2\)</span>
即每个位置使用2个随机注意力。</p>
<h1 id="小结">小结</h1>
<p>SWA在优化长上下文的计算效率上有明显的收益。而在模型效果上，目前基本没有看到不可接受的损失。对长上下文有需求的业务，值得探索。</p>
<p>除了SWA，sparse
attention还有许多其他探索。目前来看，这些做法都有一定的理论基础，效果也不错。但是阻碍这些方案大规模使用的一个原因就是<big><strong>工程实现</strong></big>，比如如何高效计算global
+ local attention，在flash attention中能够支持random
attention，这都是要考虑的内容。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<h1 id="reference">Reference</h1>
<p>【1】Mistral 7B https://arxiv.org/pdf/2310.06825.pdf<br>
【2】Longformer: The Long-Document Transformer
https://arxiv.org/pdf/2004.05150.pdf<br>
【3】Training Compute-Optimal Large Language Models
https://arxiv.org/pdf/2203.15556.pdf<br>
【4】GPT-4 System Prompt Revealed
https://patmcguinness.substack.com/p/gpt-4-system-prompt-revealed<br>
【5】Big Bird: Transformers for Longer Sequences
https://arxiv.org/abs/2007.14062</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>attention</tag>
        <tag>sliding window attention</tag>
        <tag>sparse attention</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型算法题(1)</title>
    <url>/3345028a.html</url>
    <content><![CDATA[<p><img src="/images/cover.png"></p>
<p>往期回顾</p>
<p><a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a></p>
<p><a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a></p>
<p><a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a></p>
<p><a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a></p>
<hr>
<p>【本文已在同名微信公众号/知乎/个人博客同步上线】</p>
<p>本系列将整理一些LLM中关键的、细节的、值得关注的内容，持续更新<sub>如有错误，欢迎指正</sub></p>
<h1 id="在transformer模型中为什么scaled-dot-product-attention在计算qk内积之后要除以根号d">1、在Transformer模型中，为什么scaled
dot-product attention在计算QK内积之后要除以根号d？</h1>
<p>简单来说，就是需要压缩softmax输入值，以免输入值过大，进入了softmax的饱和区，导致梯度值太小而难以训练。如果不对attention值进行scaling，也可以通过在参数初始化时将方差除以根号d
，同样可以起到预防softmax饱和的效果。</p>
<h1 id="transformer自注意力计算中为什么q和k要使用不同的权重矩阵进行线性变换投影为什么不使用同一个变换矩阵或者不进行变换">2、Transformer自注意力计算中，为什么Q和K要使用不同的权重矩阵进行线性变换投影，为什么不使用同一个变换矩阵，或者不进行变换？</h1>
<p>1、如果Q和K一样，则矩阵乘积的结果是一个对称矩阵，这样减弱了模型的表达能力。</p>
<p>2、如果Q和K一样，乘积结果的对称矩阵中，对角线的值会比较大，导致每个位置过分关注自己。</p>
<p>3、使用不同的投影矩阵，参数增多，可以增强模型表达能力。</p>
<h1 id="transformer模型中注意力计算后面使用了两个ffn层为什么第一个ffn层先把维度提升第二个ffn层再把维度降回原大小">3、Transformer模型中，注意力计算后面使用了两个FFN层，为什么第一个FFN层先把维度提升，第二个FFN层再把维度降回原大小？</h1>
<p>1、提升维度：类似SVM
kernel，通过提升维度可以识别一些在低维无法识别的特征。</p>
<p>2、提升维度：更大的可训练参数，提升模型的容量。</p>
<p>3、降回原维度：方便多层注意力层和残差模块进行拼接，而无需进行额外的处理。</p>
<h1 id="mqamulti-query-attention和gqagrouped-query-attention相比mhamulti-head-attention计算量变化如何主要带来了什么优化">4、MQA(Multi-Query
Attention)和GQA(Grouped-Query Attention)相比MHA(Multi-Head
Attention)，计算量变化如何，主要带来了什么优化？</h1>
<p>1、MQA和GQA虽然可训练参数量比MHA少，但是计算量和MHA相比变化不大，主要在生成KV时有少量降低。</p>
<p>2、Decoder-only的大模型由于causal
attention的存在，使用了KV缓存加速推理。MQA和GQA能减少KV头的数量，节省了缓存，使得在输入长度较长时也能把KV放进缓存。</p>
<h1 id="为什么现在主流的llm模型基本都是decoder-only的结构单向注意力模型为什么效果比双向注意力效果好">5、为什么现在主流的LLM模型基本都是Decoder-only的结构？单向注意力模型为什么效果比双向注意力效果好？</h1>
<p>1、双向Attention在多层模型训练中容易退化成低秩矩阵，限制了模型容量；而Decoder-only模型使用了下三角注意力矩阵，使得训练过程中矩阵是满秩，建模能力更强。</p>
<p>2、单向注意力模型相比双向注意力模型在训练的时候难度更大，能迫使模型学到更多信息。</p>
<p>3、Causal
Attention天然具有位置编码的功能，而双向Attention即使交换两个token的位置也基本不影响表示，对语序区分能力较弱。</p>
<p>4、工程上，单向模型支持KV Cache等，对于对话场景效率友好。</p>
<p>5、轨迹依赖，基模型训练成本高，业界倾向于沿着已经成功的模型继续开发。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>往期回顾</p>
<p><a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a></p>
<p><a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a></p>
<p><a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a></p>
<p><a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a></p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>算法题</tag>
      </tags>
  </entry>
  <entry>
    <title>大规模对话模型：ChatGPT下的银牌选手们</title>
    <url>/14e576c.html</url>
    <content><![CDATA[<img src="/14e576c/1.png" class title="page_1">
<img src="/14e576c/2.png" class title="page_2">
<img src="/14e576c/3.png" class title="page_3">
<img src="/14e576c/4.png" class title="page_4">
<img src="/14e576c/5.png" class title="page_5">
<img src="/14e576c/6.png" class title="page_6">
<img src="/14e576c/7.png" class title="page_7">
<img src="/14e576c/8.png" class title="page_8">
<img src="/14e576c/9.png" class title="page_9">
<img src="/14e576c/10.png" class title="page_10">
<img src="/14e576c/11.png" class title="page_11">
<img src="/14e576c/12.png" class title="page_12">
<img src="/14e576c/13.png" class title="page_13">
<img src="/14e576c/14.png" class title="page_14">
<img src="/14e576c/15.png" class title="page_15">
<img src="/14e576c/16.png" class title="page_16">
<img src="/14e576c/17.png" class title="page_17">
<img src="/14e576c/18.png" class title="page_18">
<img src="/14e576c/19.png" class title="page_19">
<img src="/14e576c/20.png" class title="page_20">
<img src="/14e576c/21.png" class title="page_21">
<img src="/14e576c/22.png" class title="page_22">
<img src="/14e576c/23.png" class title="page_23">
<img src="/14e576c/24.png" class title="page_24">
<img src="/14e576c/25.png" class title="page_25">
<img src="/14e576c/26.png" class title="page_26">
<img src="/14e576c/27.png" class title="page_27">
<img src="/14e576c/28.png" class title="page_28">
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>ChatGPT</tag>
        <tag>Sparrow</tag>
        <tag>LaMDA</tag>
        <tag>GopherCite</tag>
        <tag>WebGPT</tag>
        <tag>InstructGPT</tag>
      </tags>
  </entry>
  <entry>
    <title>Yi技术报告-划重点看细节</title>
    <url>/41b6a819.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>01.AI（零一万物），是李开复带队孵化的AI公司。2023年11月初，01.AI发布并开源了Yi-6B、Yi-34B
base模型，同一周内，又开源了Yi-6B-200K和Yi-34B-200K
base模型。Yi号称是从零预训练的双语模型。接下来的几个月，01.AI陆续推出了chat模型、多模态能力，Yi-9B、长上下文的记忆和检索能力等优化。</p>
<p>从2023年11发布起，个人就有测试和使用Yi的模型。在SuperCLUE/CMMLU等一些榜单数据的实测上，发现Yi的效果确实不错。实际工作使用里，Yi的效果基本也都能排在同时期中文（开源）大模型里的第一梯队。</p>
<p>2024年3月，Yi终于发布了技术报告，在此来梳理一下报告中的重点内容和值得关注的细节信息。</p>
<h1 id="tldr">TL;DR</h1>
<p>先给出核心内容的总结：</p>
<ul>
<li>Yi-34B模型int4量化之后，相比float16损失&lt;1%，可以跑在RTX4090上（24G显存）</li>
<li>模型结构不需要太多变化，LLAMA2标准结构已经足够训出很好的效果</li>
<li>3.1T的预训练数据远比scaling
law建议的1T大，但是效果更好，并且模型还没饱和，继续增大数据量还能提升</li>
<li>微调数据质量很重要，由算法人员直接标注，只要&lt;10k的数据量就足够了</li>
<li>4k长度的基础预训练模型已经具备长文本能力，只需用长文本数据继续预训练，更新百步就有很好效果</li>
<li>总之，数据要精心设计，数据质量要高，数据量要大</li>
</ul>
<h1 id="模型">模型</h1>
<h2 id="规模选择">规模选择</h2>
<p>Yi目前有6B、9B、34B三个规模，其中34B是主力模型。</p>
<p>选择34B，而不是更大规模的原因，是这个规模能在24G显存的消费级显卡（如RTX4090）上运行。</p>
<p>使用int4量化之后的34B模型可以运行在24G显存的GPU上。</p>
<p>参考<a href="https://arxiv.org/abs/2301.12017">《Understanding INT4
Quantization for Language Models: Latency Speedup, Composability, and
Failure
Cases》</a>的量化方法，Yi-34B的int8量化模型相比bf16模型，几乎可以做到效果无损（差距&lt;1%），而int4量化模型在大部分任务的损失也完全可以接受，具体效果如下表。</p>
<img src="/41b6a819/eval.png" class title="Yi效果">
<p>训练数据总共是3.1T token，这比DeepMind的scaling
law所建议的1TB要大不少。目前能接触到的这个规模的模型，数据量基本都&lt;2T。</p>
<p>（即提出Chinchilla模型的<a href="https://arxiv.org/abs/2203.15556">《Training Compute-Optimal Large
Language Models》</a>的scaling law）</p>
<p>也就是从scaling law的角度来说，Yi是overtrain的。</p>
<p><big><strong>但是Yi实践结果证明，较小模型+更大规模的高质量数据，是可以获得进一步效果提升的，这也就让我们获得了高性价比的推理模型--34B推理成本+大训练投入，就能得到接近普通70B规模的推理效果。</strong></big></p>
<h2 id="模型结构">模型结构</h2>
<p>结构上，基于标准LLAMA2模型，做了一些变化</p>
<ul>
<li>注意力机制：LLAMA2只在70B用了GQA，Yi全系列都用了GQA，具体参数如下表<br>
</li>
<li>位置编码：RoPE，参考RoPE ABF（《Effective long-context scaling of
foundation models》），base扩大到10M，用于支持长上下文。<br>
</li>
<li>激活函数：使用SwiGLU，参考《GLU Variants Improve Transformer》</li>
</ul>
<p>并且把activation
size从4h降为8/3h，这里的说法是补偿了GQA带来的参数下降</p>
<blockquote>
<p>We use SwiGLU as Yi’s post-attention layer, reducing its activation
size from 4h to 8/3h (h denotes hidden size) to be consistent with the
normal post-attention layer. This adjustment also compensates for the
reduction in parameter resulted from GQA, making the overall parameter
count comparible of existing 7B and 34B models.</p>
</blockquote>
<img src="/41b6a819/model.png" class title="Yi模型结构">
<p>关于模型结构，一个结论就是<big><strong>“虽然做了很多模型结构上的实验，但是最终发现，标准的结构就足以训出足够好的模型”</strong></big></p>
<h2 id="tokenizer">tokenizer</h2>
<ul>
<li>用BPE，词表大小为64000，这个大小平衡了计算效率和表达能力；<br>
</li>
<li>其中数字全是单个的digit，让模型能更好地理解数字数据；<br>
</li>
<li>对于OOV的词，会降级用unicode编码 ；</li>
<li>保留全角标点符号，不转为半角；</li>
</ul>
<p>另外，优先考虑英语的LLM在tokenizer会使用虚拟前缀（文本开头的空格）来泛化句子不同位置相同的单词。Yi不这么做，因为即使是在英语语境中，这种假设并不总是成立，比如对于以引号开头的句子，而且在中文语境中，这么做没有明显效果。</p>
<h1 id="数据">数据</h1>
<p>数据，是LLM最核心的部分，没有之一。Yi最核心的工作就是提升数据数量和质量。</p>
<img src="/41b6a819/cover.png" class title="数据">
<h2 id="预训练数据">预训练数据</h2>
<p>预训练数据整体处理流程如下</p>
<img src="/41b6a819/pretrain_data_pipeline.png" class title="预训练数据处理流程">
<ol type="1">
<li>语料获取 &amp; 语言分类</li>
</ol>
<p>从网络爬虫开始，爬取中英文这两种语言的网站，对网站内容进行解析。</p>
<p>并参考CCNeT（《CCNet: Extracting High Quality Monolingual Datasets
from Web Crawl Data》）的做法，进行语言识别。</p>
<ol start="2" type="1">
<li>规则过滤器 Heuristic Rule Filters</li>
</ol>
<p>目的是快速过滤掉明显的低质量数据。基于这些规则来过滤掉：</p>
<ul>
<li>URL、域名、单词黑名单和乱码文本；<br>
</li>
<li>文档长度、特殊符号的比例，以及短行、连续行或不完整行的比例；<br>
</li>
<li>重复的单词模式、n-gram或段落，参考《Scaling Language Models:
Methods, Analysis &amp; Insights from Training
Gopher》的做法，阈值则是来参考《CulturaX: A Cleaned, Enormous, and
Multilingual Dataset for Large Language Models in 167 Languages》；</li>
<li>数据脱敏：识别并匿名化个人信息（Personal Identifiable
Information，PII），如电子邮件地址和电话号码。</li>
</ul>
<ol start="3" type="1">
<li>可训练过滤器 Learned Filters</li>
</ol>
<p>对于不好用规则处理的，就用模型来学习模式，并进行清洗。共有4个scorer：</p>
<ul>
<li>Perplexity Scorer：参照《CCNet: Extracting High Quality Monolingual
Datasets from Web Crawl
Data》，用kenlm库，把高于平均perplexity的内容丢弃；</li>
<li>Quality
Scorer：识别如维基百科这样的高质量内容，丢弃低质量内容；</li>
<li>Document Coherence
Scorer：用于发现句子、段落零散不连贯的文本，要么分割，要么直接丢弃；</li>
<li>Safety Scorer：识别并删除暴力、色情、涉政内容</li>
</ul>
<ol start="4" type="1">
<li>基于聚类的过滤 Cluster-based Filters</li>
</ol>
<p>用聚类的方法，把所有文档进行分类。一方面用于给数据混合策略做参考，一方面如果整个类别的质量太差，就直接抛弃类别内的所有数据。</p>
<ol start="5" type="1">
<li>去重</li>
</ol>
<p>参考《The RefinedWeb Dataset for Falcon LLM: Outperforming Curated
Corpora with Web Data, and Web Data
Only》，做文档级的minhash去重，以及子文档级的完全匹配去重。</p>
<p>最终获得的数据分布如下</p>
<img src="/41b6a819/pretrain_data_dist.png" class title="预训练数据分布">
<p>虽然数据规模一定要够，但是也不能因此就放弃数据质量，否则只能是garbage
in，garbage out</p>
<blockquote>
<p>we prefer 3T tokens over sophasticated engineering over 10T tokens
without extensive filtering</p>
</blockquote>
<p>这句话大概表示清洗前的数据有10T，这也是一个信息，符合质量的数据可能只有3成</p>
<h2 id="微调数据">微调数据</h2>
<p>对于微调数据，一句话：Quality is All You Need。</p>
<p>一共只有&lt;10k条SFT数据，每条数据都通过人工多次打磨，这比大数量但质量一般的数据的效果好。</p>
<p>这思路和《Gemini: A family of highly capable multimodal
models.》、《Llama 2: Open Foundation and Fine-Tuned Chat
Models》、《Lima: Less is more for alignment》一致，而和FLAN（《Scaling
instruction-finetuned language models》）以及UltraChat（《Enhancing chat
language models by scaling high-quality instructional
conversations》）这样更关注数据量的做法不同。</p>
<p>具体做法上有：</p>
<ul>
<li>对于<big><strong>prompt distribution
selection</strong></big>：参考《Wizardlm: Empowering large language
models to follow complex
instructions》，开发复合指令，并通过指令进化，逐步增加指令的复杂度。这种做法显著减少了SFT数据量。<br>
</li>
<li>对于<big><strong>CoT data formatting</strong></big>：参考《Take a
step back: Evoking reasoning via abstraction in large language
models》，采用了“Step-Back”的模式。即通过抽象化处理，让模型学习在深入探讨原始、具体的问题之前，制定更高层次的解决方案。<br>
</li>
<li>对于<big><strong>response formatting</strong></big>：使用从《Lima:
Less is more for
alignment》扩展的默认样式。总体而言，response的结构为introduction-body-conclusion的格式，“where
the body is usually a list of bullet point”。<br>
</li>
<li>在缓解<big><strong>幻觉</strong></big>问题上，思路是确保response中的知识不由模型内部产生，对应的做法是把会导致模型进行记忆的response删掉。（但是这个具体标准是什么，有没有了解的朋友说下看法？）<br>
</li>
<li>在缓解<big><strong>生成重复</strong></big>的问题上，则是直接把response中包含重复的部分都重写了。（核心还是洗数据，一条条打磨）<br>
</li>
<li>数据<big><strong>多样性</strong></big>很重要，因此参考《#instag:
Instruction tagging for analyzing supervised fine-tuning of large
language
models》建立了一个打标系统，并设计一个注重多样性的采样算法，平衡了各个领域数据的分布。<br>
</li>
<li>为了找到最佳的数据<big><strong>配比</strong></big>，参考《How
abilities in large language models are affected by supervised
fine-tuning data composition》，使用近似网络搜索（approximate grid
search），对每个领域以{1, 1/2, 1/4, 1/8, 1/16, 1/32,
1/64}的比例进行实验和人工测评，找到最佳的组合方式。<br>
</li>
<li>除了内容，<big><strong>数据格式</strong></big>对效果也有很大影响。参OPENAI的ChatML格式（<a href="https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md">https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md</a>），这种结构化的格式使模型能够区分各种信息类型，如system
prompt、user input和bot response。</li>
</ul>
<p>SFT数据质量能极大影响模型的效果，随着数据量的增加，高质量数据能带来更多提升，如下图</p>
<img src="/41b6a819/sft.png" class title="SFT">
<h1 id="训练">训练</h1>
<h2 id="infra">infra</h2>
<p>从数据处理到模型训练都需要大集群大算力的支持。Yi构建了支持全栈数据处理、预训练、微调和服务的基础设施。包括：</p>
<ol type="1">
<li>自动管理和监控计算资源的能力；</li>
<li>通过优化并行策略、内核效率和长上下文支持提高训练速度；</li>
<li>统一微调框架，支持异构分布式训练后端，例如在DPO中同时使用Megatron和DeepSpeed进行多个模型的训练；</li>
<li>通过各种LLM服务加速技术（如量化、continuous batching 和 paged
attention）降低部署成本。</li>
</ol>
<p>总之这部分工作还是很多的，比如由于经常有硬件坏，坏的硬件会被自动从资源池移除；任务失败时，会自动跟踪重启。给算法人员考法UI等。</p>
<h2 id="预训练">预训练</h2>
<p>训了4k基础模型。（暂时没有给出更多细节）</p>
<h2 id="微调">微调</h2>
<p>超参如下</p>
<ul>
<li>AdamW：beta=[0.9,0.999]，epsilon = 1e-8<br>
</li>
<li>seq_len = 4096<br>
</li>
<li>batch size = 64<br>
</li>
<li>constant lr = 1e-5，weight decay = 0.1<br>
</li>
<li>gradient clip = 1.0<br>
</li>
<li>max step = 300</li>
<li>参考《Neftune: Noisy embeddings improve instruction
finetuning》，对于6B模型 noise scale = 5，对于34B模型 noise scale =
45</li>
</ul>
<h1 id="模型评测">模型评测</h1>
<h2 id="基模型评测">基模型评测</h2>
<ol type="1">
<li>基础能力评测</li>
</ol>
<p>对其他开源模型，保持和公开的设置相同做法获取结果。Yi使用贪婪解码，没有进行任何后处理，结果如下表</p>
<img src="/41b6a819/base_model_eval.png" class title="Base模型效果">
<p>在数学和代码能力上，和GPT3.5、GPT4还存在一些差距，而这些能力是可以通过继续预训练和微调来持续提升的。Yi最初的设计并没有针对这些能力，因此没有在预训练数据中包含特别多相关数据，后续会有计划增加这部分能力的提升。</p>
<p>而和其他开源模型相比，在代码和数学以外的任务，Yi基本上做到了跟大一倍模型的效果相近，甚至更好的水平。</p>
<ol start="2" type="1">
<li>观察</li>
</ol>
<ul>
<li>模型规模带来的增益：尽管Yi-34B和Yi-6B使用了相同的预训练语料，但Yi-34B的性能相比Yi-6B有了质的提升。更大的模型尺寸在代码和数学基准测试上带来了明显的增益。<br>
</li>
<li>数据质量：高质量预训练数据的小型模型，如Yi-34B或Qwen-14B，通常表现优于尺寸更大但（可能）数据质量较低的模型，例如Falcon-180B。</li>
<li>GPT-4与开源LLM之间的差距：开源LLM在多种基准测试上的性能仍然落后于GPT-4和GPT-3.5。然而，具有代表性的双语LLM，例如Qwen-14B和Yi-34B，可以在包括C-Eval、CMMLU和Gaokao在内的中文知识相关基准测试上匹配甚至超过GPT-4的性能。然而，在BBH、代码（HumanEval）和数学（MATH）等推理相关基准测试上，仍然存在巨大的差距。</li>
</ul>
<ol start="3" type="1">
<li>In-Context Learning能力的测试</li>
</ol>
<p>Yi进一步研究了in-context
learning的能力，即根据少数展示的输入-输出示例，推断underlying
function的能力。</p>
<p>考虑的任务是推断加权和的线性系数。具体来说，定义 y = w1x1 + w2x2 +
... + wnxn。</p>
<p>少量示例展示是 x1, x2, ..., xn, y，要求模型预测给定一组新输入 x 的
y。</p>
<p>这就要求模型隐式地推断出 w1, w2, ..., wn。</p>
<p>评测上，使用（a）模型预测的 y 与真实值 y∗ 之间的绝对差，即 |y − y∗|
作为连续度量，以及使用（b）精确匹配 y == y∗ 作为不连续度量。</p>
<p>模型在算术上的效果正常，因此可以认为这样的测试不受算术能力的影响，而能直接看模型是否具备根据给定的实例进行underlying
function推理的能力。</p>
<p>实验发现，当问题比较简单时（系数是[1,-1]），Yi-34B和LLAMA-70B效果比较好（看下图）。</p>
<p>当问题更复杂点（系数是[1，1，1，1，1]），只有LLAMA-70B和Mistral
8*7B这样的大模型表现出了涌现的能力。</p>
<img src="/41b6a819/ict.png" class title="ICT">
<h2 id="chat模型评测">Chat模型评测</h2>
<ol type="1">
<li>自动评测</li>
</ol>
<p>评测的任务和base模型相同，分别采用zero-shot和few-shot，效果依然不错，具体结果如下</p>
<img src="/41b6a819/eval.png" class title="Yi效果">
<p>报告强调，如Goodhart’s
principle所说，当一个指标变成目标，就不再是一个好指标。因此这里的测试只是为了确认微调没有使得模型的知识能力下降，而不会专门去针对任务做优化。</p>
<p>结果上，Yi-34B-Chat数学能力不错，而Yi-6B-Chat并没有展现出强大的数学能力。推测较小的模型可能需要更多的数据在SFT阶段激活其相应的能力。</p>
<ol start="2" type="1">
<li>人工评测</li>
</ol>
<img src="/41b6a819/third_party.png" class title="三方评测">
<h1 id="能力扩展">能力扩展</h1>
<p>base模型的基础上，做了3个能力扩展：长上下文、多模态、深度扩展。</p>
<h2 id="长上下文能力">长上下文能力</h2>
<p>报告中认为，4k的base模型已经具备了长文本（200k）的能力。只要用少量数据，进行继续预训练来释放这个能力，再用轻量级的SFT来调整格式，就能获得足够好的长文本能力。</p>
<p>长文本的继续预训练中，依然使用完整的attention，而不是线性attention或者sparse
attention。</p>
<p>继续预训练的数据，混合了（1）原始预训练数据（2）length-upsampled
long-context
data长文本数据，长文本数据主要来自书籍（3）多文档问答的人造数据。</p>
<p>多文档问答数据的应答中，在最终答案之前会对和答案相关的段落进行复述（recitation），以此来提升模型长文本关联的能力。</p>
<p>这部分的数据工作主要参考《Data engineering for scaling language
models to 128k context》和《Paraphrasing the original text makes high
accuracy long-context qa》。</p>
<p>最终用了5B token的长文本数据，batch
size=4M（token），只更新了100个step（这里没明白100步是怎么来，不应该是5B/4M=1250？有没有明白的朋友指点一下）。</p>
<blockquote>
<p>We continue pretrain the model on 5B tokens with 4M batch size, which
translate to 100 optimization steps. Aligning with the concurrent work
from Fu et al. [22], we observe that such light-weight continue
pretraining is already able to enable a strong performance on
Needle-in-a-Haystack test, as we will show in Figure 6.</p>
</blockquote>
<p>这个做法与《Data engineering for scaling language models to 128k
context》一致，这样轻量级的微调已经足够在“大海捞针”任务做得很好。</p>
<p>而微调的数据，也混合了短的SFT数据，以及长的文本问答数据。</p>
<p>这些文本问答数据都是人工用模型造出来的。</p>
<p>具体的做法是，随机抽一些文档，然后从中随机选择一个或者多个段落，让一个训练好的模型根据这些段落造出问题和答案。</p>
<p>一个重要的细节是复述和改写：在给出答案之前，我们要求模型复述或改写原文段落。这种数据格式鼓励模型的检索行为，从而抑制其虚构行为：面对一个问题，模型更倾向于使用输入中的信息来构建答案，而不是使用其内部知识，后者可能与问题相关但不准确。</p>
<p>使用以上所述的轻量级训练，已经可以在“大海捞针”任务做得很好，几乎能够做到全绿。</p>
<img src="/41b6a819/long_context_result.png" class title="大海捞针效果">
<h2 id="多模态">多模态</h2>
<p>ViT部分由CLIP ViT-H/14
model初始化，后面的transformer由Yi-Chat初始化</p>
<img src="/41b6a819/multimodal.png" class title="多模态">
<p>3步训练：</p>
<p>（1）使用224^2的图像来训练ViT和projection模块的参数。这一训练利用了包含1亿个图像-文本对的数据集，这些数据来自LAION-400M。主要目标是增强ViT在架构中的知识获取能力，并实现ViT与LLM之间更好的对齐。</p>
<p>（2）将ViT的图像分辨率提升到448^2，目的是进一步推动模型识别复杂视觉细节的能力。在这个阶段使用的数据集包括从LAION-400M中提取的2000万个图像-文本对。此外，还融入了来自不同来源的大约480万个图像-文本对，例如CLLaVA、LLaVAR、Flickr、VQAv2、RefCOCO、Visual7w等。</p>
<p>（3）整个模型的参数一起训练。主要目标是提高模型在多模态聊天交互方面的熟练度，从而赋予它能够无缝融合和解释视觉与语言输入的能力。为此，训练数据集涵盖了多种来源，总共大约有100万张图像-文本对，包括GQA、VizWiz
VQA、TextCaps、OCR-VQA、Visual
Genome、ShareGPT4V等等。为了确保数据平衡，对任何单一来源的最大数据量设定了上限，将其限制在不超过50,000对。</p>
<p>使用128张A100，6B训了3天，34B训10天。</p>
<h2 id="depth-upscaling-深度扩展">Depth Upscaling 深度扩展</h2>
<p>目标是把32层的6B扩展到48层的9B模型。</p>
<p>参考《Scaling large language models with simple yet effective depth
up-scaling》，通过复制中间的12-28层共16层，把层数扩展为48层。</p>
<p>实验表明，要确定复制哪些层，可以通过测量输入和每层输出的cosine
similarity来衡量。</p>
<p>这种方法使得模型能在不额外训练的情况下，和原模型性能最接近，损失最少。</p>
<img src="/41b6a819/9B.png" class title="9B模型">
<p>这说明复制的这些层并不会很大地改变原模型的激活值。</p>
<p>除了层数增加，Depth Upscaling还要做继续预训练，才能提升效果。</p>
<p>继续预训练使用约800B token，训练过程分为两个阶段。</p>
<p>其中约70%的数据是最近收集并精心挑选的。在最后阶段增强了代码的比例以提高代码性能。</p>
<p>训练保持constant lr = 3e-5，并在模型损失达到平台期时，从4M
token开始逐渐增加batch size大小。</p>
<p>这种增加batch
size的方法，以及保持所有其他参数与Yi-6B基础模型配置一致，继续预训练很重要。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>往期文章</p>
<p><a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a></p>
<p><a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a></p>
<p><a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a></p>
<p><a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a></p>
<p><a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a></p>
<p><a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a></p>
<p><a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a></p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>长上下文</tag>
        <tag>技术报告</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>理解Attention:从起源到MHA,MQA和GQA</title>
    <url>/3dc22f96.html</url>
    <content><![CDATA[<p>【本文已在同名微信公众号/知乎/个人博客同步上线】</p>
<p>Attention模块是现在几乎所有大模型的核心模块，因此也有很多工作致力于提升注意力计算的性能和效果。其中MHA（Multi-Head
Attention）、MQA（Multi-Query Attention）和GQA（Grouped-Query
Attention）这一路线的思路和做法被很多主流模型所采用，因此简单地梳理一些这几个变体的思路和做法，以及会涉及到的KV
Cache相关内容。思路比较直白，但也有一些细节和原理值得思考。</p>
<p>当然针对Attention优化，也有很多其他优秀的方案和思路，如线性注意力、FlashAttention和Sliding
Window Attention等，这些在后续再开篇梳理。</p>
<p>（应一些朋友的要求，会增加一些直观基础的内容，以及LLM应用的案例。也欢迎大家提出更多建议。）</p>
<h1 id="关于attention从rnn到attention">关于Attention：从RNN到Attention</h1>
<p>了解一个概念的诞生和演进，有助于我们更深入去理解它。我们先简单回顾下attention从起源到最初的实现。</p>
<p>（熟悉attention的朋友可以跳过这一节）</p>
<h2 id="从rnn说起">从RNN说起</h2>
<blockquote>
<p>Memory is attention through time. ~ Alex Graves 2020</p>
</blockquote>
<p>注意力机制最初起源是为了解决序列问题。回想在还没有Transformer的上一世代，使用RNN的Seq2Seq是这样的</p>
<img src="/3dc22f96/seq2seq.png" class title="seq2seq">
<img src="/3dc22f96/encoder.png" class title="encoder">
<img src="/3dc22f96/decoder.png" class title="decoder">
<p>（图来自<a href="https://theaisummer.com/attention/">AI
Summer</a>）</p>
<p>每个RNN cell接收两个输入，输出一个hidden state。比如在翻译任务中，RNN
encoder把所有输入迭代地编码成context向量 <span class="math inline">\(z\)</span> ，然后由RNN decoder基于 <span class="math inline">\(z\)</span>
迭代地解码。一般来说，这里decoder的第一个输入是一个特殊token，如[start]，表示解码开始。</p>
<p>这样会有一个问题， <span class="math inline">\(z\)</span>
能编码的长度显然有限，而且由于模型结构问题，会更加关注靠近尾部的输入。这样如果关键信息出现在开头，就容易被忽略。</p>
<p>并且时间步骤上的传播由于有多次迭代相乘，梯度很容易就过小，导致梯度消失问题。</p>
<p>当然我们有LSMT和GRU等变体来增强长距离记忆的能力，也缓解了梯度问题，但这些方案还是没有产生质变的能力。</p>
<p>回到问题的核心，我们想要 <span class="math inline">\(z\)</span>
能够编码所有前面的内容，但是显然， <span class="math inline">\(z\)</span>
的生成方式天然会让它更容易注意到靠后的内容，而容易忽略靠前的输入。</p>
<p>一个直觉的想法就是，我们需要想个办法跳过 <span class="math inline">\(z\)</span>
，和前面的每个输入建立直接的联系。我们希望模型能够有机会学习到去“注意”关键的输入，不管这个输入是在前面还是后面。</p>
<p>实际上神经网络天生就具有“注意力”的天赋。</p>
<p>比如在CNN分类中，如果我们画出分类层前的heatmap，会是如下图这个样子</p>
<img src="/3dc22f96/cnn_heatmap.png" class title="heatmap">
<p>可以看到，值比较高的地方是在猫的鼻子胡子嘴巴区域，次之是身上和头上的花纹。直观来说，就是模型主要通过脸部的特征和身上的花纹，来识别出这是一只猫。这就是CNN学习到的注意力，这样的特征是神经网络implicitly学到的。</p>
<p>回归到Seq2Seq，我们怎么来实现注意力，并且让这种implicit的机制变得explicit：单独抽离出来并具备一定可控制性？</p>
<p>回想翻译场景，在RNN中，每一个时间步骤 <span class="math inline">\(i\)</span> 都会产生一个隐向量，<span class="math inline">\(h_i\)</span> 向量，我们把这些 <span class="math inline">\(h_i\)</span>
保存起来，在最后要生成新的输出的时候，我们让模型回头看一下之前的这每一个
<span class="math inline">\(h_i\)</span>
，再决定要生成什么内容。相比原来只利用最后一个hidden
state，现在我们可以访问之前所有的中间状态，如果发现前面有关键信息，就可以直接用上了，而不用担心输入太长而被覆盖了。</p>
<p>那么问题又来了，我们怎么知道前面某一个中间状态对于当前的生成来说是否重要？如果我们不知道怎么定义是否重要，那我们就把这个问题交给模型自己解决好了
--
通过网络参数来学习识别某个输入状态是否重要，学习是否要“注意”到它，要给予多少的“注意力”。</p>
<p>具体来说，我们定义在解码第 <span class="math inline">\(i\)</span>
个输出是，decoder当前隐状态 <span class="math inline">\(y_{i-1}\)</span>
和encoder的所有隐状态 <span class="math inline">\(\mathbf{h}\)</span>
之间的一个score计算</p>
<p><span class="math display">\[\mathbf{e}_i=\text{attention}_{\mathrm{net}}\left(y_{i-1},\mathbf{h}\right)\in
R^n\]</span></p>
<p>其中</p>
<p><span class="math display">\[e_{ij}=\text{attentiom}_{\text{net
}(\mathbf{y}_{i-1},h_j)}\]</span></p>
<p>注意力网络通过 <span class="math inline">\(\mathbf{y}_{i-1}\)</span>
和 <span class="math inline">\(h_j\)</span> 来计算一个值 <span class="math inline">\(e_{ij}\)</span>，这里的注意力网络可以设计各种操作，比如对输入进行拼接再通过fc层进行计算等。</p>
<p>这里 <span class="math inline">\(e_{ij}\)</span>
是一个标量，但它还不是一个可用的权重值，还需要通过一个函数，把attention
net对各个encoder hidden state的输出值转成一个分布：softmax。</p>
<p><span class="math display">\[\alpha_{ij}=\frac{\exp\left(e_{ij}\right)}{\sum_{k=1}^{T_x}\exp\left(e_{ik}\right)}\]</span></p>
<p>最后通过加权计算，获得最终输入给decoder的隐变量。</p>
<p><span class="math display">\[z_i=\sum_{j=1}^T\alpha_{ij}\mathbf{h}_j\]</span></p>
<img src="/3dc22f96/seq2seq_attention.png" class title="seq2seq attention">
<p>可以看到，这里的attention net的任务就是找到decoder上一个hidden
state和encoder hidden
state之间的“相关”关系，使得模型能够将更多的注意力放在对应的输入信息上。</p>
<p>实际上，上面这种attention的计算方式并不是唯一的，attention的计算方式有许多种</p>
<img src="/3dc22f96/attention_calculation.png" class title="attention calculation">
<p>这些attention的一般形式可以写作 <span class="math inline">\(\mathrm{Attention}(s, h)=\mathrm{Score}(s,h)\cdot
h\)</span> 。这里的 <span class="math inline">\(s\)</span>
就是decoder的hidden state（也就是前文的 <span class="math inline">\(y\)</span> ），<span class="math inline">\(h\)</span> 就是encoder的hidden state。</p>
<p>（当然从结果上看，是scaled dot-product
attention经受住了历史的考验，成为了主流。）</p>
<h2 id="transformer的attention">Transformer的attention</h2>
<p>从RNN attention到transformer
attention，所做的事情就如论文题目所说：《Attention Is All You
Need》，彻底抛弃RNN的在time
step上的迭代计算，完全拥抱attention机制，只用最简单粗暴的方式同步计算出每个输入的hidden
state，其他的就交给attention来解决。</p>
<img src="/3dc22f96/transformer_structure.png" class title="transformer structure.png">
<p>这里还是保留有encoder和decoder的结构，encoder中的attention都是self-attention，decoder则除了self-attention还有cross-attention。</p>
<p>transformer结构下，attention的一般形式可以写作 <span class="math inline">\(\mathrm{Attention}(Q,K,V)=\mathrm{Score}(Q,K)V\)</span>，这里有
<span class="math inline">\(Q=W_{Q}Y，K=W_{K}X，V=W_{V}X\)</span>
。对于cross-attention， <span class="math inline">\(X\)</span>
是encoder的hidden states，<span class="math inline">\(Y\)</span>
是decoder的hidden states，而对于self-attention，则有 <span class="math inline">\(X=Y\)</span>。</p>
<p>具体到我们熟悉的scaled dot-product attention，使用softmax计算，有</p>
<p><span class="math display">\[\operatorname{Attention}(Q,K,V)=\operatorname{softmax}(\frac{QK^T}{\sqrt{d}})V\]</span></p>
<p>到这里，终于见到我们熟悉的attention计算。</p>
<p>用一张很直观的图来展示整个计算</p>
<img src="/3dc22f96/Scaled-dot-product-self-attention.pbm" class title="self-attention">
<p>这里的「query」，「key」和「value」的名称也暗示了整个attention计算的思路。</p>
<p>类比到一个数据库查询+预测的例子。</p>
<p>假设我们现在有一个“文章-阅读量”数据库，记录了每篇文章在发布30天内的阅读量。每篇文章就是一个key，对应的阅读量就是value。</p>
<p>我们现在有一篇将要发布的文章，想要预测这篇文章在30天内的阅读量，那我们就把这篇新的文章，作为query，去和数据库里的文章（key）做一个相关性计算，取最相关的5篇文章。</p>
<p>假设top5篇文章的相关性分别是 <span class="math inline">\([8,4,4,2,2]\)</span> ，对应阅读量是 <span class="math inline">\([5\text{w},2\text{w},8\text{w},3\text{w},6\text{w}]\)</span>
。</p>
<p>那我们把相关性得分归一化成和为1的概率值 <span class="math inline">\([0.4,0.2,0.2,0.1,0.1]\)</span>
，那我们就可以预测新文章30天内的阅读量是 <span class="math inline">\(0.4\times5+0.2\times2+0.2\times8+0.1\times3+0.1\times6=4.9\text{w}\)</span>
。</p>
<p>这个例子中，我们计算相关性就相当于transformer attention中的 <span class="math inline">\(QK^T\)</span>
，归一化就是softmax，然后通过加权求和取得最后的阅读量/特征向量。</p>
<p>对于self-attention， <span class="math inline">\(Q、K、V\)</span>
都来自输入 <span class="math inline">\(X\)</span>，sequence自己计算自己每个token的之间的相关性。而对于cross-attention，decoder中的输出sequence就是上面这个例子中的“将要发布的文章”，通过把这篇新的文章和数据库中的文章做相关计算，我们得到了新的预测结果。</p>
<p>对于self-attention，由于 <span class="math inline">\(Q、K、V\)</span>
都来自输入 <span class="math inline">\(X\)</span> ，在计算 <span class="math inline">\(QK^T\)</span>
时，模型很容易关注到自身的位置上，也就是 <span class="math inline">\(QK^T\)</span>
对角线上的激活值会明显比较大。这样的情况其实不是很好，因为这会削弱模型关注其他高价值位置的能力，也就限制模型的理解和表达能力。后面讲的MHA对这个问题会有一些缓解作用。</p>
<p>顺着这样的思路梳理下来，会发现attention的大思路还是很好理解的。而计算上，怎么去获得更好的效果，就是接下来要分析的几个内容，MHA，MQA和GQA所关注的。</p>
<p>代码上，实现也很容易，直接看<a href="https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention">pytorch
forcasting</a>的代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScaledDotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dropout: <span class="built_in">float</span> = <span class="literal">None</span>, scale: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.dropout = dropout</span><br><span class="line">        self.softmax = nn.Softmax(dim=<span class="number">2</span>)</span><br><span class="line">        self.scale = scale</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span><br><span class="line">        attn = torch.bmm(q, k.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>))  <span class="comment"># query-key overlap</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.scale:</span><br><span class="line">            dimension = torch.as_tensor(k.size(-<span class="number">1</span>), dtype=attn.dtype, device=attn.device).sqrt()</span><br><span class="line">            attn = attn / dimension</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attn = attn.masked_fill(mask, -<span class="number">1e9</span>)</span><br><span class="line">        attn = self.softmax(attn)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attn = self.dropout(attn)</span><br><span class="line">        output = torch.bmm(attn, v)</span><br><span class="line">        <span class="keyword">return</span> output, attn</span><br></pre></td></tr></table></figure>
<h2 id="关于scaling">关于scaling</h2>
<p>BTW，为什么计算中 <span class="math inline">\(QK^T\)</span>
之后还要除以 <span class="math inline">\(\sqrt{d}\)</span> ？</p>
<p>简单来说，就是需要压缩softmax输入值，以免输入值过大，进入了softmax的饱和区，导致梯度值太小而难以训练。</p>
<img src="/3dc22f96/softmax.png" class title="softmax">
<p>苏剑林的<a href="https://spaces.ac.cn/archives/8620">博客</a>中也有详细分析，并提到如果不对attention值进行scaling，也可以通过在参数初始化是将方差除以一个
<span class="math inline">\(\sqrt{d}\)</span>
，同样可以起到预防softmax饱和的效果。类似地，通过normalization也可以做到类似的效果。不过实现上在attention里做scaling还是比较稳定高效的。</p>
<h1 id="mha">MHA</h1>
<p>只要理解了attention计算的细节，MHA（multi-head
attention）其实就很好明白。</p>
<p>MHA在2017年就随着《Attention Is All You
Need》一起提出，主要干的就是一个事：把原来一个attention计算，拆成多个小份的attention，并行计算，分别得出结果，最后再合回原来的维度。</p>
<p><span class="math display">\[\mathrm{MultiHeadAttention}(Q,K,V)=\mathrm{Concat}(head_1,\ldots,head_h)\]</span></p>
<p><span class="math display">\[head_i=\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)\]</span></p>
<p>假设原来模型的hidden size是 <span class="math inline">\(d\)</span>
，在MHA中，会把投影后的 <span class="math inline">\(Q、K、V\)</span>
在hidden state的维度上切成 <span class="math inline">\(head_{num}\)</span> 份，每个头的维度是 <span class="math inline">\(d_{head}\)</span> 。这 <span class="math inline">\(head_{num}\)</span> 组小 <span class="math inline">\(Q、K、V\)</span>
分别独立地进行attention计算，之后把得到的 <span class="math inline">\(head_{num}\)</span> 份维度 <span class="math inline">\(d_{head}\)</span> 的输出concat起来。</p>
<p>直接看这个amazing的图，很直观</p>
<img src="/3dc22f96/multihead_attention.png" class title="MHA">
<p>操作是这么个操作，多头注意力相比单头有什么好处呢？</p>
<p>《Attention Is All You Need》文章中给出的说法是</p>
<blockquote>
<p>Multi-head attention allows the model to jointly attend to
information from different representation subspaces at different
positions.</p>
</blockquote>
<p>我们希望多个头能够在训练中学会注意到不同的内容。例如在翻译任务里，一些attention
head可以关注语法特征，另一些attention
head可以关注单词特性。这样模型就可以从不同角度来分析和理解输入信息，获得更好的效果了。</p>
<p>这有点类似CNN中，不同的卷积核来学习不同的信息。比如一个 <span class="math inline">\(3\times3\times128\)</span> 的卷积，有128个 <span class="math inline">\(3\times3\)</span>
参数组，假设我们的输入是一个灰度图，其中一组 <span class="math inline">\(3\times3\)</span> 的参数是这样的</p>
<p><span class="math display">\[\left.\left[\begin{matrix}1&amp;0&amp;-1\\1&amp;0&amp;-1\\1&amp;0&amp;-1\end{matrix}\right.\right]\]</span></p>
<p>那么这是一个检测纵向边界的卷积，而另外一组参数长这样</p>
<p><span class="math display">\[\left.\left[\begin{matrix}1&amp;1&amp;1\\0&amp;0&amp;0\\-1&amp;-1&amp;-1\end{matrix}\right.\right]\]</span></p>
<p>这是一个检测横向边界的卷积。</p>
<p>这128组 <span class="math inline">\(3\times3\)</span>
就是128个不同特征的检测器，就同MHA中多个头一样，从不同的子空间学到不同的内容，最后再放到一起融合使用。</p>
<p>但是这是我们expect模型能做到的事情，实际情况是否真的是这样？</p>
<p>知乎上这篇<a href="https://zhuanlan.zhihu.com/p/626820422">文章</a>里对此做了一些实验和分析。简单来说就是（1）每个头确实学到东西有所不同，但大部分头之间的差异没有我们想的那么大（比如一个学句法，一个学词义这样明显的区分）（2）多个头的情况下，确实有少部分头可以比较好地捕捉到各种文本信息，而不会过分关注自身位置，一定程度缓解了上文提到的计算
<span class="math inline">\(QK^T\)</span> 之后对角线元素过大的问题。</p>
<p>我们可以把MHA的多个attention计算视为多个独立的小模型，那么最终整体的attention计算相当于把来自多个小模型的结果进行了融合，这样效果比较好也是比较符合直觉的。</p>
<p>另外还有一个问题是，使用几个头比较好呢？</p>
<p>实际上这个问题比较难有确定性的答案，首先可以确定的是头的数量不是越多约好（毕竟头的数量多了，各个子空间小了，子空间能表达的内容就少了），具体多少要视模型规模，任务而定。另外<a href="https://arxiv.org/pdf/1905.10650.pdf">《Are Sixteen Heads Really
Better than One?》</a>中也指出MHA并不总是优于单头的情况。</p>
<p>目前可以看到的趋势是，模型越大（也就是hidden
size越大），头数的增多越能带来平均效果上的收益（或者说允许注意力头增大而不影响子空间的学习能力）。目前LLM主流的头数视乎模型结构和规模，大致有12、16、24、48、96这样一些主流设置。这里面又有比较多的方向和工作，在此暂时不展开，挖个坑，以后专门开一篇讲。</p>
<p>最后看一下<a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The
Annotated Transformer</a>中的MHA代码实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) \</span><br><span class="line">             / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim = -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        h: head number</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d</span></span><br><span class="line">        self.d = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d </span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, -<span class="number">1</span>, self.h, self.d).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(nbatches, -<span class="number">1</span>, self.h * self.d)</span><br><span class="line">        <span class="keyword">return</span> self.linears[-<span class="number">1</span>](x)</span><br></pre></td></tr></table></figure>
<p>（<a href="https://github.com/huggingface/transformers">transformers</a>中的写法就更为成熟一点，不过里面兼容了比较多的功能，代码太长就不放上来了）</p>
<h1 id="解码中的kv-cache">解码中的KV Cache</h1>
<p>在讲MQA和GQA之前，还需要了解一点背景，那就是解码的计算问题，以及KV
Cache的方案。</p>
<p>无论是encoder-decoder结构，还是现在我们最接近AGI的decoder-only的LLM，解码生成时都是自回归auto-regressive的方式。</p>
<p>也就是，解码的时候，先根据当前输入 <span class="math inline">\(\text{input}_{i-1}\)</span> ，生成下一个 <span class="math inline">\(\text{token}_{i}\)</span> ，然后把新生成的 <span class="math inline">\(\text{token}_{i}\)</span> 拼接在 <span class="math inline">\(\text{input}_{i-1}\)</span> 后面，获得新的输入
<span class="math inline">\(\text{input}_{i}\)</span> ，再用 <span class="math inline">\(\text{input}_{i}\)</span> 生成 <span class="math inline">\(\text{token}_{i+1}\)</span>
，依此迭代，直到生成结束。</p>
<p>比如我们输入“窗前明月光下一句是”，那么模型每次生成一个token，输入输出会是这样（方便起见，默认每个token都是一个字符）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">step0: 输入=[BOS]窗前明月光下一句是；输出=疑</span><br><span class="line">step1: 输入=[BOS]窗前明月光下一句是疑；输出=是</span><br><span class="line">step2: 输入=[BOS]窗前明月光下一句是疑是；输出=地</span><br><span class="line">step3: 输入=[BOS]窗前明月光下一句是疑是地；输出=上</span><br><span class="line">step4: 输入=[BOS]窗前明月光下一句是疑是地上；输出=霜</span><br><span class="line">step5: 输入=[BOS]窗前明月光下一句是疑是地上霜；输出=[EOS]</span><br></pre></td></tr></table></figure>
<p>（其中[BOS]和[EOS]分别是起始符号和终止符号）</p>
<p>仔细想一下，我们在生成“疑”字的时候，用的是输入序列中“是”字的最后一层hidden
state，通过最后的分类头预测出来的。以此类推，后面每生成一个字，使用的都是输入序列中最后一个字的输出。</p>
<p>我们可以注意到，下一个step的输入其实包含了上一个step的内容，而且只在最后面多了一点点（一个token）。那么下一个step的计算应该也包含了上一个step的计算。</p>
<p>从公式上来看是这样的：</p>
<p>回想一下我们attention的计算</p>
<p><span class="math display">\[
\alpha_{i,j}=\text{softmax}(q_{i}k_{j}^\top)\\
o_{i}=\sum_{j=0}^{i}{\alpha_{i,j}v_{j}}
\]</span></p>
<p>注意对于decoder的时候，由于mask
attention的存在，每个输入只能看到自己和前面的内容，而看不到后面的内容</p>
<p>假设我们当前输入的长度是3，预测第4个字，那每层attention所做的计算有</p>
<p><span class="math display">\[
\begin{aligned}
o_{0}&amp;=\alpha_{0,0}v_{0}\\
o_{1}&amp;=\alpha_{1,0}v_{0}+\alpha_{1,1}v_{1}\\
o_{2}&amp;=\alpha_{2,0}v_{0}+\alpha_{2,1}v_{1}+\alpha_{2,2}v_{2}\\
\end{aligned}
\]</span></p>
<p>预测完第4个字，放到输入里，继续预测第5个字，每层attention所做的计算有</p>
<p><span class="math display">\[
\begin{aligned}
o_{0}&amp;=\alpha_{0,0}v_{0}\\
o_{1}&amp;=\alpha_{1,0}v_{0}+\alpha_{1,1}v_{1}\\
o_{2}&amp;=\alpha_{2,0}v_{0}+\alpha_{2,1}v_{1}+\alpha_{2,2}v_{2}\\
o_{3}&amp;=\alpha_{3,0}v_{0}+\alpha_{3,1}v_{1}+\alpha_{3,2}v_{2}+\alpha_{3,3}v_{3}\\
\end{aligned}
\]</span></p>
<p>可以看到，在预测第5个字时，只有最后一步引入了新的计算，而 <span class="math inline">\(o_{0}\)</span> 到 <span class="math inline">\(o_{2}\)</span> 的计算和前面是完全重复的。</p>
<p>但是模型在推理的时候可不管这些，无论你是不是只要最后一个字的输出，它都把所有输入计算一遍，给出所有输出结果。</p>
<p>也就是说中间有很多我们用不到的计算，这样就造成了浪费。</p>
<p>而且随着生成的结果越来越多，输入的长度也越来越长。上面这个例子里，输入长度就从step0的10个，每步增长1，直到step5的15个。如果输入的instruction是让模型写高考作文，那可能就有800个step或者更多。这个情况下，step0被算了800次，step1被算了799次...</p>
<p>有没有什么办法可以重复利用上一个step里已经计算过的结果，减少浪费呢？</p>
<p>答案就是KV
Cache。利用缓存空间，把需要重复利用的中间计算结果存下来，减少重复计算。</p>
<p>而 <span class="math inline">\(k\)</span> 和 <span class="math inline">\(v\)</span> 就是要缓存的对象。</p>
<p>想象一下，在上面的例子中，假设我们一开始的输入就是3个字，我们第一次预测就是预测第4个字，那么由于一开始没有任何缓存，所有我们每一层还是要踏踏实实地计算一遍。然后把
<span class="math inline">\(k\)</span> 、 <span class="math inline">\(v\)</span> 值缓存起来。</p>
<p>则有</p>
<p><span class="math display">\[
\text{cache}_l=\text{None}\\
\]</span></p>
<center>
↓
</center>
<p><span class="math display">\[
\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},
v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]
\]</span></p>
<p>kv_cache的下标 <span class="math inline">\(l\)</span>
表示模型层数。</p>
<p>在进行第二次预测，也就是预测第5个字的时候，在第 <span class="math inline">\(l\)</span>
层的时候，由于前面我们缓存了<u><strong>每层</strong></u>的 <span class="math inline">\(k\)</span> 、 <span class="math inline">\(v\)</span> 值，那本层就只需要算新的 <span class="math inline">\(o_{3}\)</span> ，而不用算 <span class="math inline">\(o_{0}、o_{1}、o_{2}\)</span> 。</p>
<p>因为第 <span class="math inline">\(l\)</span> 层的 <span class="math inline">\(o_{0}、o_{1}、o_{2}\)</span>
本来会经过FNN层之后进到 <span class="math inline">\(l+1\)</span>
层，再经过新的投影变换，成为 <span class="math inline">\(l+1\)</span>
层的 <span class="math inline">\(k\)</span> 、 <span class="math inline">\(v\)</span> 值，但是 <span class="math inline">\(l+1\)</span> 层的 <span class="math inline">\(k\)</span> 、 <span class="math inline">\(v\)</span> 值我们已经缓存过了！</p>
<p>然后我们把本次新增算出来的 <span class="math inline">\(k\)</span> 、
<span class="math inline">\(v\)</span> 值也存入缓存。</p>
<p><span class="math display">\[
\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},
v_{1}^{l}),(k_{2}^{l}, v_{2}^{l})]
\]</span></p>
<center>
↓
</center>
<p><span class="math display">\[
\text{cache}_l=[(k_{0}^{l}, v_{0}^{l}),(k_{1}^{l},
v_{1}^{l}),(k_{2}^{l}, v_{2}^{l}),(k_{3}^{l}, v_{3}^{l})]
\]</span></p>
<p>这样就节省了attention和FFN的很多重复计算。</p>
<p>transformers中，生成的时候传入use_cache=True就会开启KV Cache。</p>
<p>也可以简单看下GPT2中的实现，中文注释的部分就是使用缓存结果和更新缓存结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Class GPT2Attention(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        hidden_states: <span class="type">Optional</span>[<span class="type">Tuple</span>[torch.FloatTensor]],</span></span><br><span class="line"><span class="params">        layer_past: <span class="type">Optional</span>[<span class="type">Tuple</span>[torch.Tensor]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        head_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        encoder_hidden_states: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        encoder_attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        use_cache: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        output_attentions: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">Tuple</span>[<span class="type">Union</span>[torch.Tensor, <span class="type">Tuple</span>[torch.Tensor]], ...]:</span><br><span class="line">        <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(self, <span class="string">&quot;q_attn&quot;</span>):</span><br><span class="line">                <span class="keyword">raise</span> ValueError(</span><br><span class="line">                    <span class="string">&quot;If class is used as cross attention, the weights `q_attn` have to be defined. &quot;</span></span><br><span class="line">                    <span class="string">&quot;Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.&quot;</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">            query = self.q_attn(hidden_states)</span><br><span class="line">            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=<span class="number">2</span>)</span><br><span class="line">            attention_mask = encoder_attention_mask</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        query = self._split_heads(query, self.num_heads, self.head_dim)</span><br><span class="line">        key = self._split_heads(key, self.num_heads, self.head_dim)</span><br><span class="line">        value = self._split_heads(value, self.num_heads, self.head_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 过去所存的值</span></span><br><span class="line">        <span class="keyword">if</span> layer_past <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            past_key, past_value = layer_past</span><br><span class="line">            key = torch.cat((past_key, key), dim=-<span class="number">2</span>)  <span class="comment"># 把当前新的key加入</span></span><br><span class="line">            value = torch.cat((past_value, value), dim=-<span class="number">2</span>)  <span class="comment"># 把当前新的value加入</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_cache <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">            present = (key, value)  <span class="comment"># 输出用于保存</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            present = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.reorder_and_upcast_attn:</span><br><span class="line">            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)</span><br><span class="line"></span><br><span class="line">        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)</span><br><span class="line">        attn_output = self.c_proj(attn_output)</span><br><span class="line">        attn_output = self.resid_dropout(attn_output)</span><br><span class="line"></span><br><span class="line">        outputs = (attn_output, present)</span><br><span class="line">        <span class="keyword">if</span> output_attentions:</span><br><span class="line">            outputs += (attn_weights,)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs  <span class="comment"># a, present, (attentions)</span></span><br></pre></td></tr></table></figure>
<p>总的来说，KV
Cache是以空间换时间的做法，通过使用快速的缓存存取，减少了重复计算。（注意，只有decoder结构的模型可用，因为有mask
attention的存在，使得前面的token可以不用关注后面的token）</p>
<p>但是，用了KV Cache之后也不是立刻万事大吉。</p>
<p>我们简单算一下，对于输入长度为 <span class="math inline">\(s\)</span>
，层数为 <span class="math inline">\(L\)</span> ，hidden size为 <span class="math inline">\(d\)</span> 的模型，需要缓存的参数量为</p>
<p><span class="math display">\[
2\times L\times s\times d
\]</span></p>
<p>如果使用的是半精度浮点数，那么总共所需的空间就是</p>
<p><span class="math display">\[
2\times 2\times L\times s\times d
\]</span></p>
<p>以Llama2 7B为例，有 <span class="math inline">\(L=32\)</span> ，
<span class="math inline">\(L=4096\)</span>
，那么每个token所需的缓存空间就是524,288 bytes，约52K，当 <span class="math inline">\(s=1024\)</span> 时，则需要536,870,912
bytes，超过500M的空间。</p>
<p>这里考虑的还只是batch size=1的情况，如果batch
size增大，这个值更是很容易就超过1G。</p>
<p>（MHA相比单头的情况，相当于只是把 <span class="math inline">\(q、k、v\)</span>
切成多份并行计算了，对于实际需要缓存的大小没有影响）</p>
<p>看下现在主流的科学计算卡配置</p>
<img src="/3dc22f96/gpu_cache.png" class title="gpu cache">
<p>强如H100也只有50M的L2 Cache（L1
Cache的大小更是可以忽略不计），大概只能支持Llama2
7B总共100个token左右的输入。</p>
<p>想想我们现在用的LLM动辄34B/70B的规模，长度更是以千为基础单位，这样明显是不够用的。</p>
<p>那么超出L2 Cache的部分只能走到显存中去了，但是HBM速度比L2
Cache慢多了。</p>
<img src="/3dc22f96/sram_dram.png" class title="储存空间与速度">
<p>看来还需要进一步优化。</p>
<p>要保证模型的推理加速，要么增大Cache的大小，而且是需要一到两个数量级的增强，那这个只能靠黄老板了。</p>
<p>要么就是减少需要缓存的量。</p>
<h1 id="mqa">MQA</h1>
<p>MQA就是来减少缓存所需要的量的。</p>
<p>Google在2019年就在《Fast Transformer Decoding: One Write-Head is All
You
Need》提出了MQA，不过那时候主要到的人不多，那是大家主要还是关注在用Bert把榜刷出新高上。</p>
<p>MQA的做法其实很简单。在MHA中，输入分别经过 <span class="math inline">\(W_{Q}、W_{K}、W_{V}\)</span>
的变换之后，都切成了n份（n=头数），维度也从 <span class="math inline">\(d_{model}\)</span> 降到了 <span class="math inline">\(d_{head}\)</span>
，分别进行attention计算再拼接。而MQA这里，在线性变换之后，只对 <span class="math inline">\(Q\)</span> 进行切分（和MHA一样），而 <span class="math inline">\(K、V\)</span> 则直接在线性变换的时候把维度降到了
<span class="math inline">\(d_{head}\)</span>
（而不是切分变小），然后这n个Query头分别和同一份 <span class="math inline">\(K、V\)</span>
进行attention计算，之后把结果拼接起来。</p>
<p>简单来说，就是MHA中，每个注意力头的 <span class="math inline">\(K、V\)</span>
是不一样的，而MQA这里，每个注意力头的 <span class="math inline">\(K、V\)</span>
是一样的，值是共享的。而其他步骤都和MHA一样。</p>
<img src="/3dc22f96/MQA.webp" class title="MQA">
<p>这样一来，需要缓存的 <span class="math inline">\(K、V\)</span>
值一下就从所有头变成一个头的量。</p>
<p>比如在Llama2
7B中用的是32个头，那用MQA后，1024个token需要缓存的量就变成1/32，536,870,912
bytes / 32 = 16,777,216 bytes，差不多是16M，这就能全塞进缓存中了。</p>
<p>（实现上，就是改一下线性变换矩阵，然后把 <span class="math inline">\(K、V\)</span>
的处理从切分变成复制，就不再赘述。）</p>
<p>当然，由于共享了多个头的参数，限制了模型的表达能力，MQA虽然能好地支持推理加速，但是在效果上略略比MHA差一点，但是并不多，且相比其他修改hidden
size或者head num的做法效果都好。</p>
<img src="/3dc22f96/mqa_result_1.png" class title="MQA results 1">
<img src="/3dc22f96/mqa_result_3.png" class title="MQA results 3">
<h1 id="gqa">GQA</h1>
<p>既然MQA对效果有点影响，MHA缓存又存不下，那GQA（Grouped-Query
Attention）就提出了一个折中的办法，既能减少MQA效果的损失，又相比MHA需要更少的缓存。</p>
<p>（文章：《GQA: Training Generalized Multi-Query Transformer Models
from Multi-Head Checkpoints》，2023年）</p>
<p>GQA里， <span class="math inline">\(Q\)</span>
还是按原来MHA/MQA的做法不变。只使用一套共享的 <span class="math inline">\(K、V\)</span>
不是效果不好吗，那就还是多弄几套。但是不要太多，数量还是比 <span class="math inline">\(Q\)</span> 的头数少一些。这样相当于把 <span class="math inline">\(Q\)</span> 的多个头给分了group，同一个group内的
<span class="math inline">\(Q\)</span> 共享同一套 <span class="math inline">\(K、V\)</span> ，不同group的 <span class="math inline">\(Q\)</span> 所用的 <span class="math inline">\(K、V\)</span> 不同。</p>
<p>MHA可以认为是 <span class="math inline">\(K、V\)</span>
头数最大时的GQA，而MQA可以任务是 <span class="math inline">\(K、V\)</span> 头数最少时的GQA。</p>
<p>看论文里的图就很直观</p>
<img src="/3dc22f96/GQA.png" class title="GQA">
<p>效果怎么样呢？</p>
<img src="/3dc22f96/GQA_result_1.png" class title="GQA result">
<p>看表中2/3/4行对比，GQA的速度相比MHA有明显提升，而效果上比MQA也好一些，能做到和MHA基本没差距。文中提到，这里的MQA和GQA都是通过average
pooling从MHA初始化而来，然后进行了少量的训练得到的。如果我们想要把之前用MHA训练的模型改造成GQA，也可以通过这样的方法，增加少量训练来实现。当然如果从一开始就加上，从零开始训练，也是没有问题的。</p>
<p>Llama2用的就是GQA，在tech
report中也做了MHA、MQA、GQA的效果对比，可以看到效果确实很不错。</p>
<img src="/3dc22f96/llama2_qga.png" class title="llama2 GQA result">
<h1 id="小结">小结</h1>
<p>MHA、MQA、GQA的实现其实并不复杂，效果也很好，理解上并没有太多困难。但是想要真正理解它们的出发点，还是需要深入每一个细节，去了解当时要解决的事什么问题。</p>
<p>目前来看GQA是LLM比较好的方案，但未来肯定还会有针对不同方向的进一步优化方案，计算效率、推理速度、显存消耗这些方向都值得我们继续去探索优化。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<h1 id="reference">Reference</h1>
<p>【1】The Annotated Transformer
https://nlp.seas.harvard.edu/2018/04/03/attention.html<br>
【2】Attention Is All You Need
https://arxiv.org/pdf/1706.03762.pdf<br>
【3】Fast Transformer Decoding: One Write-Head is All You Need
https://arxiv.org/pdf/1911.02150.pdf<br>
【4】https://www.researchgate.net/figure/Scaled-dot-product-self-attention-mechanism_fig1_363923096<br>
【5】GQA: Training Generalized Multi-Query Transformer Models from
Multi-Head Checkpoints https://arxiv.org/pdf/2305.13245.pdf<br>
【6】How Attention works in Deep Learning: understanding the attention
mechanism in sequence models https://theaisummer.com/attention/<br>
【7】A simple overview of RNN, LSTM and Attention Mechanism
https://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b<br>
【8】https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.html#ScaledDotProductAttention<br>
【9】浅谈Transformer的初始化、参数化与标准化
https://spaces.ac.cn/archives/8620<br>
【10】https://theaisummer.com/self-attention/
https://theaisummer.com/self-attention/<br>
【11】https://zhuanlan.zhihu.com/p/626820422
https://zhuanlan.zhihu.com/p/626820422<br>
【12】Are Sixteen Heads Really Better than One?
https://arxiv.org/pdf/1905.10650.pdf<br>
【13】This post is all you need（上卷）——层层剥开Transformer
https://zhuanlan.zhihu.com/p/420820453<br>
【14】The Illustrated Transformer
https://jalammar.github.io/illustrated-transformer/<br>
【15】Multi-Query Attention is All You Need
https://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>attention</tag>
        <tag>KV Cache</tag>
      </tags>
  </entry>
  <entry>
    <title>transformer中normalization的二三事</title>
    <url>/6a40bfa5.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<p>Normalization在模型中，相对于attention这种经常被魔改的结构，受到的关注度似乎没那么高，但它对模型能否顺利训练，却有很关键的作用。</p>
<p>在此简单梳理下normalization相关的背景和内容，也分析一下在transformer发展上的相关内容。</p>
<p>这部分内容感觉目前还有些存在争议的地方，如果有不同意见欢迎讨论。</p>
<h1 id="why-normalization">why normalization</h1>
<p>normalization，也叫「归一化」、「正则化」、「规范化」、「标准化」等，可以说已经是神经网络不可以或缺的一环。</p>
<p>使用的话，现在基本只需几行代码就能实现。但要用得好，还是需要了解一下它作用的机制。</p>
<h2 id="从输入数据看normalization">从输入数据看normalization</h2>
<p>假设我们有一个二元损失函数 <span class="math inline">\(Loss(x_1,x_2)=x_1^2+x_2^2+b\)</span>
，那在三维空间画出来的损失平面大概是这样</p>
<img src="/6a40bfa5/lossfunc_surface.jpeg" class title="loss function surface">
<p>在这样一个平面上，使用梯度下降法，梯度方向是垂直于当前位置等高线的切线方向的。</p>
<p>如果这个损失函数的等高线是一系列完美的同心圆，那么无论我们起点在哪里，梯度下降的时候都会以垂直切线方向，沿着圆心一路奔去。</p>
<p>这种情况下优化很快，控制好学习率不要跳过minimum就可以（也可以用自适应优化器来控制速度）。</p>
<p>但是实际上我们的损失平面很难那么完美。损失函数的等高线更可能是个椭圆（或者更复杂的形状）。</p>
<img src="/6a40bfa5/ellipse_1.png" class title="ellipse">
<p>这样我们梯度下降是方向就要经常修正，训练效率就会受影响。</p>
<p>如果这个椭圆很扁或者我们的训练参数不太好，可能会出现反复震荡收敛缓慢的情况。</p>
<img src="/6a40bfa5/ellipse_2.png" class title="ellipse">
<p>损失在这个狭窄的山谷中反复横跳。</p>
<p>那损失函数等高线什么时候会是椭圆形？</p>
<p>假设我们现在有两个输入变量，以米为单位的身高 <span class="math inline">\(x_{1}\)</span>，和以元为单位的月工资收入 <span class="math inline">\(x_{2}\)</span>。（这里对量纲的使用也会改变数值，如米-&gt;厘米）</p>
<p>如果我们用这两个自变量训练模型，我们会发现，身高取值范围基本是在0.x米~2.x米，而工资的取值范围是0到几百几千几万或者几十万以及更多。</p>
<p>而模型的一个主要操作就是对输入特征进行线性组合。</p>
<p>这时模型的输出值会更大地受到 <span class="math inline">\(x_{2}\)</span>
的影响，因为它的变化更大，取值范围也更大。</p>
<p>这时损失函数在不同变量维度的变化速度相差很多，损失函数就会出现椭圆形等高线的情况。</p>
<p>既然由于量纲和取值范围的问题，会导致训练困难，那最直接方法就是规定一个标准范围，所有输入变量，不管原来是什么范围，现在都归一化到标准范围里来。</p>
<p>这就是最朴素的输入normalization的思想。</p>
<p>输入的normalization有很多种做法<br>
<span class="math display">\[x^{\prime}=\frac{x-\min(\mathrm{x})}{\max(\mathrm{x})-\min(\mathrm{x})}\]</span></p>
<p><span class="math display">\[x^{\prime}=\frac{x-\mu}{\max(\mathrm{x})-\min(\mathrm{x})}\]</span></p>
<p><span class="math display">\[x^{\prime}=\frac{x-\mu}\sigma\]</span></p>
<p>其中 <span class="math inline">\(\mu\)</span> 为均值，<span class="math inline">\(\sigma\)</span> 为方差。</p>
<p>第三种，均值方差归一化，也叫Z-score
normalization，应该是我们用得比较多的。</p>
<p>这样我们通过对输入进行一些操作，把「椭圆」拉成了「圆」，解决输入参数范围带来的一些训练问题。</p>
<p>除了针对均值、方差、最大值、最小值的归一化，对输入还有一些其他的处理，如PCA等，就暂不展开。</p>
<h2 id="缓解ics...吗">缓解ICS...吗？</h2>
<p>机器学习里有一个叫i.i.d.（independent and identical
distribution，独立同分布）的假设：独立，每次抽样之间是没有关系的，不会相互影响；同分布，即每次抽样，样本都服从同样的一个分布。</p>
<p>为什么需要i.i.d.？</p>
<p>由于机器学习依赖于使用现有数据来训练模型，进而对未来的数据做出预测和模拟，因此这一过程本质上是在历史数据的基础上，通过模型来推测未来的数据走向。</p>
<p>这就要求我们使用的历史数据必须具备整体的代表性。以便从现有数据（经验）中提炼出规律，对未知数据进行决策。</p>
<p>如果用于训练的数据缺乏总体代表性，即仅代表特殊情况，那么得出的规律可能不准确或错误，因为这些规律是基于个别案例推测出来的，就不具备泛化性。</p>
<p>当然并不是所有机器学习都需要i.i.d.，但是有i.i.d.的话，可以简化很多事情，让模型学习起来更容易快速。</p>
<p>对于输入，通过合理的抽样和处理（前面提到的PCA就可以用来解耦特征间的关联，达到“独立”的效果），我们可以得到输入的i.i.d.的条件，但这只是针对输入。</p>
<p>在多层的神经网络中，上一层的输出会作为下一层的输入。</p>
<p>而在训练过程中，由于上层的模型参数在不断学习变化，则上层输出的分布也在不断变化，靠后的层实际上要学习不断的变化的分布，这就很不i.i.d.，那靠后面的层的学习速度和效果就会收到影响，调参也变得困难，模型也难以加深。</p>
<p>这个问题就是ICS，internal covariate shift。</p>
<p>那有没有办法保证上一层的分布不要变化呢？</p>
<p>一个「可能」的方案就是normalization。我们通过把上一层的输出映射到一个固定的分布上，来稳定给下一层的输入，这样就降低了学习难度。</p>
<p>但也有一些工作表明normalization（batchnorm）的作用机制和ICS的关系并不大，这个观点下面在batchnorm部分说。</p>
<p>当然ICS的问题也可以通过改变初始化策略、调控训练超参如学习率等方法来优化，但是这样做的效率并不是很高。</p>
<h2 id="远离激活函数饱和区">远离激活函数饱和区</h2>
<p>神经网络中还有一个重要组件，非线性激活函数，比如常用的sigmoid。</p>
<img src="/6a40bfa5/sigmoid.png" class title="sigmoid">
<p>当输入 &gt; 6 或者 &lt; -6
的时候，sigmoid函数的梯度已经变得非常小，也就是进入了饱和区。</p>
<p>这种情况下训练就变得困难。</p>
<p>ICS就会加剧梯度消失的情况。在没有normalization的情况下，分布不断变化，后面层的参数变化激烈，导致输出值更容易进入到左右两端，更容易进入到激活函数的饱和区。</p>
<p>而normalization能把部分输出值拉回到梯度正常的范围内，一定程度缓解了梯度消失的问题，使训练可以正常进行下去。</p>
<h1 id="batchnorm">batchnorm</h1>
<p>神经网络中使用的normalization有很多种，这里不一一展开，只梳理一下最重要的batchnorm和layernorm两类。</p>
<h2 id="batchnorm算法">batchnorm算法</h2>
<p>假设输入数据的形状是 <span class="math inline">\([B,C]\)</span>
，其中 <span class="math inline">\(B\)</span> 是batch size，<span class="math inline">\(C\)</span> 是特征向量维度。</p>
<p>这 <span class="math inline">\(C\)</span>
个输入特征每个都有不同的含义，如我们前面的例子，第一个元素可能是身高，第二个元素可能是月收入，因此做normalization的时候这
<span class="math inline">\(C\)</span> 个特征分别来做。</p>
<p>具体来说，对于第 <span class="math inline">\(i\)</span>
个特征维度，首先计算整个batch内的均值</p>
<p><span class="math display">\[
\mu_{i}=\frac{1}{B}\sum_{j=1}^{B}x_{i,j}
\]</span></p>
<p>再计算这个维度上的方差</p>
<p><span class="math display">\[
\sigma_{i}^{2}=\frac{1}{B}\sum_{j=1}^{B}(x_{i,j}-\mu_{i})^2
\]</span></p>
<p>得到均值和方差之后，对batch内维度上的所有值进行Z-score
normalization</p>
<p><span class="math display">\[
x_{i,j}&#39;=\frac{x_{i,j}-\mu_{i}}{\sqrt{\sigma_{i}^{2}+\epsilon}}
\]</span></p>
<p>其中 <span class="math inline">\(\epsilon\)</span>
是为了防止分母为0。这个在实际代码中挺重要的，忘记加可能会出问题。</p>
<p>经过这样的变换之后，在 <span class="math inline">\(C\)</span>
个特征维度上就是均值为0，方差为1的分布了。</p>
<p>但是到这还没结束。</p>
<p>每个维度的数值全部归一化之后，对于激活函数来说，更集中在中间的部分，而这部分的非线性特征并不强（比如上面的sigmoid），这样非线性激活层近似了一个线性变换，这样就降低了模型的学习能力。</p>
<p>且无论输入是什么，最终输出都会被强行拉到这样一个“平均”的值，也极大抑制了模型的表达能力。</p>
<p>所以为了保证模型的能力，也保证非线性能力的获得，对每个特征，又增加两个可学习的参数，
缩放参数 <span class="math inline">\(\gamma\)</span> 和位移参数 <span class="math inline">\(\beta\)</span> 。</p>
<p><span class="math display">\[
y_{i,j} = \gamma_{i} x_{i,j}&#39; + \beta_{i}
\]</span></p>
<p>这样每个特征值就有机会从“线性区”移动到“非线性区”，把被归一化削弱的非线性能力找了回来。</p>
<p>并且通过这样一个归一化再重新缩放移动的操作，解耦了上层输出分布和下层输入，本来下层参数要去适应上层分布变化，现在只需要通过每个batchnorm层中的
<span class="math inline">\(\gamma\)</span> 和 <span class="math inline">\(\beta\)</span>
直接学习就行了，训练变得简单了。</p>
<p><a href="https://zhuanlan.zhihu.com/p/340856414">《Batch
Normalization: Accelerating Deep Network》</a>给出的算法如下</p>
<img src="/6a40bfa5/bn_algo.png" class title="batch norm">
<h2 id="cnn中的batchnorm">CNN中的batchnorm</h2>
<p>batchnorm最主要的应用还是在CNN模型中。</p>
<p>假设CNN中feature map的size是 <span class="math inline">\([B,C,H,W]\)</span> ，其中 <span class="math inline">\(B\)</span> 是batch size，<span class="math inline">\(C\)</span> 是channel数（也是卷积核数量），<span class="math inline">\(H\)</span> 和 <span class="math inline">\(W\)</span> 分别是特征图的高和宽。</p>
<p>如果按照前面batchnorm的算法，那应该有 <span class="math inline">\(C\times H\times W\)</span> 组特征，每组特征有
<span class="math inline">\(B\)</span> 个，对每组内的 <span class="math inline">\(B\)</span> 进行归一化，再进行放缩和平移。</p>
<p>但是实际上，CNN中卷积是一个滑动窗口，对于同一个channel下的 <span class="math inline">\(H\times W\)</span>
个特征值其实都来自于同一个卷积核的计算，这 <span class="math inline">\(H\times W\)</span>
也属于一个“batch”，它们要放在一起进行归一化。</p>
<p>也就是对于卷积核来说，真正的batch数是 <span class="math inline">\(B\times H\times W\)</span> ，而只有 <span class="math inline">\(C\)</span> 组特征值，因此也只有 <span class="math inline">\(C\)</span> 个 <span class="math inline">\(\gamma\)</span> 和 <span class="math inline">\(\beta\)</span> 。</p>
<p>batchnorm原文中，batchnorm放在了relu后面，作者认为这样使得进入激活函数的分布会更加稳定，顺便对于fc层，由于batchnorm和fc都有bias项，还可以省略掉其中一个而不影响效果。</p>
<p>btw，一般来说，batchnorm初始化的时候，把 <span class="math inline">\(\gamma\)</span> 设为1（不缩放），把 <span class="math inline">\(\beta\)</span>
设为0（不平移），在训练中让模型从相当于没有batchnorm开始慢慢学习这两个参数。</p>
<h2 id="训练和推理">训练和推理</h2>
<p>现在我们知道在训练时，batchnorm对一个mini-batch计算均值和方差来进行归一化，再进行缩放和移动。</p>
<p><span class="math inline">\(\gamma\)</span> 和 <span class="math inline">\(\beta\)</span>
属于模型学出来的参数，只要训练结束这两个向量就固定了，在推理的时候直接使用即可。</p>
<p>但是推理时，均值和方差怎么计算呢。推理的时候可能是一个sample，也可能是任意个sample作为一个batch，和训练的时候一样计算肯定不合适。</p>
<p>我们需要在训练的时候就为推理做准备：训练的时候，模型会遍历整个训练集，因此理论上可以统计出整个训练集的均值和方差，然后把这个大量样本统计出来的均值和方差当做真实分布的均值和方差，在推理的时候使用。（回想i.i.d.）</p>
<p>当时又有一个问题，训练集可能会很大，有百万甚至千万的数据，在训练的数据记录下所有层所有特征来计算均值和方差显然效率不高，因此用一个近似的方法：</p>
<p>moving_mean = momentum × moving_mean + (1.0 − momentum) × mean</p>
<p>moving_var = momentum × moving_var + (1.0 − momentum) × var</p>
<p>通过把多个batch的均值和方差进行移动平均的方式来逼近整个训练集的均值和方差。</p>
<p>momentum为动量参数，在 TF/Keras 中，该值为0.99，在 Pytorch
中，这个值为0.9。</p>
<p>小的momentum值对应快的更新速度，能够更快地向真实分布靠近，但是同时也会导致更大的波动。</p>
<p>大的momentum值对应慢的更新速度，如果更新过慢，则可能导致训练结束时还没有统计到真实的分布，是欠拟合的状态。</p>
<p>如果batch size比较小，每个mini
batch和全局差异较大，就不应该用太大的momentum。</p>
<p>理论上，训练步数越长是会越靠近真实分布的，实际上，因为每个batch并不能代表整个训练集的分布，所以最后的值是在真实分布附近波动。</p>
<p>这里还引入另外一个问题，如果batch size太小，每个mini
batch统计的均值和方差和全局的值偏差相对会比较大，对模型收敛的稳定性有影响，因此一般来说，使用batchnorm的话，batch
size不能太小，如下图</p>
<img src="/6a40bfa5/bs_bn.png" class title="batch size的影响">
<p>小结一下，batchnorm的优点是解耦了上层输出和下层输入的分布，既缓解了进入激活函数饱和区带来的梯度消失的情况，又保留了模型的表达能力。每一层的输入尺度相对固定，提供了更好的尺度不变形，使模型训练更稳定。</p>
<p>同时每个batch分别进行归一化，相当于引入了一些随机噪音，使得模型不容易过拟合到某些微小的特征上，相当于进行了一定的正则化，将损失平面变得相对平滑。</p>
<p>但是同时也引入了新的超参（如momentum），另外也依赖batch
size的大小，过小的batch size可能会带来问题。</p>
<h2 id="batchnorm起作用的真正原因">batchnorm起作用的真正原因？</h2>
<p>虽然batchnorm原文认为batchnorm在一定程度上是缓解了ICS，但是2018年的《How
Does Batch Normalization Help Optimization?》提出了不同观点。</p>
<p>为了探究batchnorm的效果，是否是因为优化了ICS（或者说和优化了ICS有多大关系），做了一个这样的实验：在batchnorm后面又通过加入随机噪音来引入“covariate
shift”，并和没有加噪音，以及没有加batchnorm的模型效果进行对比，如下图</p>
<img src="/6a40bfa5/bn_ics.png" class title="ICS">
<p>结果发现，即使人工加强了ICS的情况，但是只要用了batchnorm，效果依然比不用好；而人工引入ICS的模型，在效果上并没有多大影响。</p>
<p>这就说明缓解ICS并不是batchnorm有效的真正原因。</p>
<p>那batchnorm到底有没有缓解到ICS呢？</p>
<p>要测量ICS的变化，就要先定义ICS。</p>
<p>对于网络中的每一层，ICS被定义为在前一层参数更新后，当前层输入分布的变化。这种变化可以通过比较更新前后的梯度来量化。</p>
<img src="/6a40bfa5/ics_define.png" class title="ICS 定义">
<p>具体来说，对于每一层i，作者们计算了以下两个梯度之间的L2范数差异：</p>
<p><span class="math inline">\(G_{t,i}\)</span>
，在时间t，使用当前所有层的参数（包括前一层的参数）计算的梯度。</p>
<p><span class="math inline">\(G_{t,i}&#39;\)</span>
，在时间t，使用更新后的前一层参数计算的梯度，而其他层的参数保持不变。</p>
<p>这个差异直观上表明了「上一层参数变化，下一层需要在多大程度上来变化，以适应新的分布」。</p>
<p>理想来说，ICS越小，上一层参数更新对当前层的分布影响越小，梯度变化程度应该越小。</p>
<img src="/6a40bfa5/ics_measure.png" class title="ICS measure">
<p>但是从结果上来看，使用了batchnorm并不能有效减少这个变化，甚至还有所增加。</p>
<p>这也说明batchnorm实际上并不能真正缓解ICS的情况。</p>
<p>那batchnorm起效果的真正原因是什么？</p>
<p>作者认为主要是batchnorm使得损失函数更加平滑，直观上来说就是减少了很多坑坑洼洼的位置，使得训练更不容易陷入到局部最小值中去。</p>
<h1 id="layernorm">layernorm</h1>
<p>看完batchnorm，再来看layernorm。</p>
<h2 id="理解layernorm">理解layernorm</h2>
<p>layernorm，不要被名字骗了，这里的layer指的不是模型的层，而是数值的layer。</p>
<p>对于二维的输入，batchnorm实在batch维度上做归一化，而layernorm是在特征维度做归一化</p>
<img src="/6a40bfa5/bn_and_ln.png" class title="bn和ln">
<p>对于非NLP数据而言，相比batchnorm，layernorm归一化的维度似乎解释性没那么强。batchnorm对同一个特征，比如身高计算均值是有意义的，而layernorm在不同的特征，比如身高、工资、温度做归一化，好像并没有可靠的物理意义。</p>
<p>layernorm最主要的应用就是NLP的模型，包括RNN和transfomrer模型。</p>
<p>在transformer中，一般输入的形状是 <span class="math inline">\([B,S,H]\)</span> ，<span class="math inline">\(S\)</span>
是序列长度，每个样本的长度可能不同，因此在这个维度需要使用padding（一般是zero-padding）来把batch内的数据处理成一样长。</p>
<p>比如这样一批文本输入</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">我  爱  中  国</span><br><span class="line">你  好</span><br><span class="line">谢  谢  你</span><br></pre></td></tr></table></figure>
<p>为了使模型能够统一处理，会pad成</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">我  爱  中  国</span><br><span class="line">你  好  [P] [P]</span><br><span class="line">谢  谢  你  [P]</span><br></pre></td></tr></table></figure>
<p>一般来说，我们认为由于有padding的存在，做batchnorm并不合适。</p>
<p>比如上面的例子，对“中”，“[P]”，“你”做归一化，由于 [P]
的存在，实际的batch size只有2，并且和 [P]
做normalization也对训练没什么帮助。</p>
<p>而且对于文本数据，batch内同一个位置上的字是不同的，对完全没有关系的字进行归一化也并没有什么意义。</p>
<p>也就是说，哪怕没有 [P]
的存在，比如对第一个token“我”，“你”，“谢”做归一化，直觉上也不太有物理意义。</p>
<p>因此使用layernorm，在 <span class="math inline">\(H\)</span>
维度上进行normalization，同时有 <span class="math inline">\(H\)</span>
个<span class="math inline">\(\gamma\)</span> 和 <span class="math inline">\(\beta\)</span> 需要学习。</p>
<p>相当于计算每一句输入内，每个token所有特征之间的均值和方差来进行归一化。</p>
<h2 id="为什么transformer用layernorm">为什么transformer用layernorm</h2>
<p>和batchnorm不同的是，由于layernorm不需要再batch维度上计算均值和方差，所以不存在训练和推理的时候不一样的地方，不用保存一个全局的均值和方差供推理的时候使用。</p>
<p>而由于layernorm和batch无关，也就不会受到batch size大小的影响。</p>
<p>除了以上的原因，也有一些工作深入探究了在nlp任务上layernorm和batchnorm的区别。</p>
<p>如《PowerNorm: Rethinking Batch Normalization in
Transformers》就研究了transformer中BN为啥表现不太好。</p>
<p>研究了训练中的四个统计量：batch的均值和方差，以及他们的梯度的均值和方差。对于batch的均值和方差，计算了他们和running
statistics（就是用移动平均法累积的均值和方差，见前面的文章）的欧氏距离。发现NLP任务上（IWSLT14）batch的均值和方差一直震荡，偏离全局的running
statistics，而CV任务也相对稳定。</p>
<p>对于他们梯度的均值和方差，研究了其magnitude（绝对值），在CV任务上震荡更小，且训练完成后，也没有离群点。</p>
<p>总结来说，transformer中BN表现不太好的原因可能在于CV和NLP数据特性的不同，对于NLP数据，前向和反向传播中，batch统计量及其梯度都不太稳定。</p>
<p>更重要的是，实际效果就是layernorm在NLP的效果比batchnorm好，效果好，这是最重要的原因。</p>
<h2 id="rmsnorm">RMSnorm</h2>
<p>19年《Root Mean Square Layer
Normalization》提出了normalization变体RMSnorm，主要针对layernorm来改进。</p>
<p>简单地说，RMSnorm就是在标准layernorm的基础上，省略了平移，只进行缩放。</p>
<img src="/6a40bfa5/rmsnorm.png" class title="RMSnorm">
<p>作者认为标准layernorm计算效率并不高</p>
<img src="/6a40bfa5/rmsnorm_eff.png" class title="RMSnorm效率">
<p>作者用一个GRU模型做实验，对比是否添加layernorm的结果，发现在相同时间和相同步骤下，有layernorm的模型，都没有无layernorm的模型收敛得快。</p>
<p>并且layernorm的平移对梯度方差的减小没有贡献，因此作者直接舍弃了中心化和平移两步，只对数据进行方差归一化和缩放。</p>
<p>更近一步，作者提出pRMSnorm，只对数据中前p%的数值进行处理，这样就能进一步加速训练，而效果也基本不太受影响。</p>
<img src="/6a40bfa5/prmsnorm.png" class title="prmsnorm">
<p>RMSnorm现在被很多主流的大模型所采样了。</p>
<h1 id="post-norm-pre-norm">post-norm &amp; pre-norm</h1>
<h2 id="二者对比">二者对比</h2>
<p>layernorm在模型里放哪也有讲究。</p>
<p>原始的transformer模型使用的post-norm，而《On Layer Normalization in
the Transformer Architecture》则认为pre-norm更好。</p>
<p>post-norm和pre-norm分别是下面这样</p>
<img src="/6a40bfa5/postnorm_prenorm.png" class title="postnorm and prenorm">
<p>post-norm是在残差和主干相加之后进行归一化，而pre-norm则是在主干先归一化再和残差相加。</p>
<p>post-norm和pre-norm对比，目前大家比较接受的结论是，pre-norm更容易训练，因此可以叠加更多的层，但是在层数不是特别多的情况下，post-norm最终的收敛效果会比pre-norm要好。</p>
<p>模型中，第 <span class="math inline">\(l\)</span> 层的输出是第 <span class="math inline">\(l+1\)</span> 层的输入，对于post-norm有</p>
<p><span class="math display">\[
x_{l+1}=\mathrm{Norm}(x_l+\mathrm{F}_t(x_l))
\]</span></p>
<p>而对于pre-norm则是</p>
<p><span class="math display">\[
x_{l+1}=x_l+\mathrm{F}_l(\mathrm{Norm}(x_l))
\]</span></p>
<p>参考苏剑林在《为什么Pre Norm的效果不如Post Norm？》中的分析，认为
<span class="math inline">\(\mathrm{F}_l(\mathrm{Norm}(x_l))\)</span>
的方差，由于有norm的存在，是不随层数变化的。</p>
<p>当 <span class="math inline">\(l\)</span> 比较大时，<span class="math inline">\(x_{l}、x_{l+1}\)</span> 的差距较小，因此 <span class="math inline">\(\mathrm{F}_l(\mathrm{Norm}(x_l))\)</span> 和 <span class="math inline">\(\mathrm{F}_{l+1}(\mathrm{Norm}(x_{l+1}))\)</span>
的差距也很小，这时有</p>
<p><span class="math display">\[\begin{aligned}
&amp;\mathrm{F}_l(\operatorname{Norm}(x_l))+\operatorname{F}_{l+1}(\operatorname{Norm}(x_{l+1}))
\\
&amp;{\approx}\mathrm{F}_l(\mathrm{Norm}(x_l))+\mathrm{F}_{l+1}\left(\mathrm{Norm}(x_l)\right)
\\
&amp;=(\mathrm{F}_l\oplus\mathrm{F}_{l+1})(\mathrm{Norm}(\pmb{x_l}))
\end{aligned}\]</span></p>
<p>相当于 <span class="math inline">\(l\)</span> 层和 <span class="math inline">\(l+1\)</span> 层的效果接近于一个更宽的 <span class="math inline">\(l\)</span> 层的效果。</p>
<p>也就是使用pre-norm的时候，模型的深度有水分，表面看起来有 <span class="math inline">\(l\)</span>
层，实际在效果上，等效于post-norm的浅层模型。</p>
<p>从模型结构上看，恒等分支永远有一部分不用经过normalization，这部分能够直接把梯度回传到最前面，这也是pre-norm能够训练“层数更多”的模型的原因--缓解了梯度消失。</p>
<p>正常来说，模型深度对最终效果的影响，是大于模型宽度的。</p>
<p>而post-norm，在残差分支之后做归一化，对参数正则化的效果更好（loss平面更平滑），且它每norm一次就削弱一次恒等分支的权重，所以post-norm相对pre-norm，是更突出残差分支的，因此它的层数更加“足秤”，训练好之后效果更优。</p>
<h2 id="和warmup的关系">和warmup的关系</h2>
<p>《On Layer Normalization in the Transformer
Architecture》（认为pre-norm更好）还分析指出，使用post-norm的transformer，在初始化时候，靠近输出层的部分梯度期望很大，所以模型在开始训练的时候很依赖warmup的策略，通过缓慢提升学习率来稳定训练过程。</p>
<p>使用warmup引入了新的超参，调参更为麻烦点。</p>
<p>而实验表明，使用pre-norm的transformer在不需要warmup的情况下，也能收敛到post-norm+warmup的相同水平，而post-norm不加warmup效果就差点了。</p>
<img src="/6a40bfa5/warmup_effect.png" class title="warmup影响">
<h2 id="deepnorm">Deepnorm</h2>
<p>2022年，《DeepNet: Scaling Transformers to 1,000
Layers》对transformer训练不稳定的原因进行了深入分析，发现模型更新过大是导致不稳定的主要原因。</p>
<p>为了解决这个问题，他们提出了Deepnorm，可以限制模型更新的大小。</p>
<img src="/6a40bfa5/deepnorm.png" class title="deepnorm">
<p>其中 <span class="math inline">\(\alpha&gt;1\)</span>
是根据模型参数定的常数。这里相比post-norm提升了恒等分支的权重，使训练更容易进行。</p>
<p>另外，还用了一个 <span class="math inline">\(\beta\)</span> 参数，把
<span class="math inline">\(G_{l}\)</span>
中的模型参数进行了缩小，以此来稳定模型的训练。</p>
<p>实验结果上，deepnorm结合了pre-norm的容易训练，和post-norm的收敛效果好的特点，能够把百层、浅层的模型训到比较好的效果。</p>
<img src="/6a40bfa5/deepnorm_result.png" class title="deepnorm result">
<p>参数过程相比post-norm稳定了很多。</p>
<h2 id="realformer--residual-attention">Realformer--residual
attention</h2>
<p>post-norm和pre-norm实际上改变的是模型残差分支和恒等分支怎么排布的问题，而《RealFormer:
Transformer Likes Residual Attention》则提出了另外一种做法</p>
<img src="/6a40bfa5/realformer.png" class title="realformer">
<p>RealFormer的核心是在其标准Transformer编码器的每一层中引入了残差连接。这些残差连接将前一层的原始注意力分数（即在应用Softmax之前的分数）与当前层计算出的注意力分数相结合。这样做的结果是，当前层的注意力分数在计算时会考虑到前一层的信息。</p>
<p>每个多头注意力模块都会接收来自前一层的残差注意力分数作为额外输入。这意味着每个注意力头不仅考虑了当前层内的输入序列，而且还直接利用了前一层的注意力信息。</p>
<img src="/6a40bfa5/realformer_attention.png" class title="realformer attention">
<p>其中 <span class="math inline">\(Prev&#39;\)</span>
是来自上一层softmax之前的权重矩阵（多头注意力的话，则是对应的头的值），而
<span class="math inline">\(\frac{Q^{\prime}K^{\prime
T}}{\sqrt{d_k}}+Prev&#39;\)</span> 则是传给下一层的attention的。</p>
<h1 id="小结">小结</h1>
<p>本篇粗略梳理了一下关于normalization，batchnorm，以及layernorm在transformer的一些使用情况。</p>
<p>目前主流的大模型使用的是rmsnorm + prenorm，也有使用其他变体的。</p>
<p>关于normalization，依然留有一些探索空间。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>往期文章</p>
<p><a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a></p>
<p><a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a></p>
<p><a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a></p>
<p><a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a></p>
<p><a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】https://www.zhihu.com/question/487766088<br>
【2】Towards Stabilizing Batch Statistics in Backward Propagation of
Batch Normalization https://arxiv.org/abs/2001.06838<br>
【3】Transformer中的归一化(一)：什么是归一化&amp;为什么要归一化
https://zhuanlan.zhihu.com/p/476102712<br>
【4】Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift https://arxiv.org/abs/1502.03167<br>
【5】How Does Batch Normalization Help Optimization?
https://arxiv.org/abs/1805.11604<br>
【6】Batch Normalization: Accelerating Deep Network
https://zhuanlan.zhihu.com/p/340856414<br>
【7】Layer Normalization https://arxiv.org/abs/1607.06450<br>
【8】详解深度学习中的Normalization，BN/LN/WN
https://zhuanlan.zhihu.com/p/33173246<br>
【9】Transformer中的归一化(四)：BatchNormalization的原理、作用和实现
https://zhuanlan.zhihu.com/p/481277619<br>
【10】Layer Normalization https://arxiv.org/abs/1607.06450<br>
【11】PowerNorm: Rethinking Batch Normalization in Transformers
https://arxiv.org/abs/2003.07845<br>
【12】Root Mean Square Layer Normalization
https://arxiv.org/abs/1910.07467<br>
【13】On Layer Normalization in the Transformer Architecture
https://arxiv.org/abs/2002.04745<br>
【14】为什么Pre Norm的效果不如Post Norm？
https://spaces.ac.cn/archives/9009<br>
【15】Understanding the Difficulty of Training Transformers
https://arxiv.org/abs/2004.08249<br>
【16】RealFormer: Transformer Likes Residual Attention
https://arxiv.org/abs/2012.11747<br>
【17】DeepNet: Scaling Transformers to 1,000 Layers
https://arxiv.org/abs/2203.00555</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>layernorm</tag>
        <tag>post-norm</tag>
        <tag>pre-norm</tag>
        <tag>normalization</tag>
        <tag>batchnorm</tag>
      </tags>
  </entry>
  <entry>
    <title>GLM4报告的一些技术点</title>
    <url>/a5206abd.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>智谱的GLM系列模型在中文领域一直是比较受关注的，特别是最新的GLM-4，个人在使用体验上，感觉已经可以满足大部分日常需求。</p>
<p>最近智谱开源了GLM-4-9B，也发了相关的技术报告，总结了整个系列从预训练到agent的各个方案。</p>
<img src="/a5206abd/glm.png" class title="GLM">
<p>这里简单梳理一下一些技术相关的点。</p>
<h1 id="数据">数据</h1>
<p>报告中提到的关于预训练数据的几个内容：<br>
- 数据的处理分为deduplication, filtering和tokenization。<br>
- deduplication包括exact deduplication和fuzzy deduplication。<br>
- 去重提升了数据分布的多样性，这对模型训练结果有很大的影响。<br>
- tokenization采用byte-level
BPE，基于tiktoken的cl100k_base进行训练，获得150k的词表。<br>
-
最后的训练数据中re-weight了各个来源的数据，提高了高质量数据如wiki、books的比例。<br>
- 最终获得10T的token。</p>
<h1 id="模型结构">模型结构</h1>
<p>模型结构上的改动和发现：<br>
- No Bias Except
QKV：除了QKV之后都不使用bias，这样可以提升训练速度。此外还发现不使用bias在长度外推的能力有微微提升。<br>
- 使用RMSNorm和SwiGLU。<br>
- 使用2D的RoPE。<br>
- 使用GQA，以减少推理的KV
cache需求。由于GQA相比MHA有更少的参数，把FFN的大小增加到10/3的hidden
size来保持模型总参数基本不变。</p>
<h1 id="alignment">Alignment</h1>
<p>SFT中，发现真实的人类prompt和交互比template-based的人造数据和模型生成的答案要好得多。</p>
<h1 id="chatglm-techniques">ChatGLM Techniques</h1>
<p>在训练ChatGLM的路上，智谱总结了不少经验：<br>
- LongAlign：《Longalign: A recipe for long context alignment of large
language models》能把GLM-4的推理长途提升到128k，并且效果达到Claude
2和GPT-4 Turbo (1106)的水平。<br>
- ChatGLM-Math：《Chatglm-math: Improving math problem-solving in large
language models with a self-critique
pipeline》给出了一套通过self-critique提升数学能力的方法。<br>
- ChatGLM-RLHF：《Chatglm-rlhf: Practices of aligning large language
models with human feedback》总结了PPO和DPO的应用。<br>
- Self-Contrast：《Extensive self-contrast enables feedback-free
language model
alignment》给出了Self-Contrast的策略，用于让模型自动生成大量负样本用于RLHF，避免了投入大量的人力。<br>
- AgentTuning：《Agenttuning: Enabling generalized agent abilities for
llms》包括agent的训练框架和AgentInstruct
instruction-tuning数据集。<br>
- APAR：《Apar: Llms can do auto-parallel auto-regressive
decoding》总结了auto-parallel auto-regressive的并行解码生成策略。</p>
<h1 id="glm-4-all-tools">GLM-4 All Tools</h1>
<p>GLM-4 All Tools可以说是前面这些技术探索的集大成者，效果确实不错。</p>
<img src="/a5206abd/all_tools.png" class title="All Tools">
<h1 id="评测">评测</h1>
<p>GLM在agent、function call、all
tools的评测整理了一些数据集和方案，后续可以参考使用。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a><br>
<a href="https://www.linsight.cn/dd614e12.html">大模型算法题(7)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】ChatGLM: A Family of Large Language Models from GLM-130B to
GLM-4 All Tools https://arxiv.org/abs/2406.12793</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>强化学习</tag>
        <tag>微调</tag>
        <tag>预训练</tag>
        <tag>技术报告</tag>
        <tag>agent</tag>
      </tags>
  </entry>
  <entry>
    <title>MoE模型的前世今生</title>
    <url>/44e38c1b.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>2024年3、4月这段时间，很多MoE模型扎堆发布，包括Qwen1.5-MoE、DBRX、Jamba和Mistral等。</p>
<p>下面这个表格列出了部分近期发布的MoE工作</p>
<center>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">模型</th>
<th style="text-align: center;">发布时间</th>
<th style="text-align: center;">备注</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">GPT4</td>
<td style="text-align: center;">2023年3月</td>
<td style="text-align: center;">23年6月George
Hotz爆料GPT4是8×220B模型</td>
</tr>
<tr class="even">
<td style="text-align: center;">Mistral-8×7B</td>
<td style="text-align: center;">2023年12月</td>
<td style="text-align: center;">Mistral AI，开源</td>
</tr>
<tr class="odd">
<td style="text-align: center;">LLAMA-MoE</td>
<td style="text-align: center;">2023年12月</td>
<td style="text-align: center;">github开源项目</td>
</tr>
<tr class="even">
<td style="text-align: center;">DeepSeek-MoE</td>
<td style="text-align: center;">2024年1月</td>
<td style="text-align: center;">幻方量化，国内首个开源MoE模型，有技术报告</td>
</tr>
<tr class="odd">
<td style="text-align: center;">abab6</td>
<td style="text-align: center;">2024年1月</td>
<td style="text-align: center;">MiniMax，号称千亿MoE，无开源，无细节发布</td>
</tr>
<tr class="even">
<td style="text-align: center;">天工2.0</td>
<td style="text-align: center;">2024年2月</td>
<td style="text-align: center;">昆仑万维，无开源，无细节发布</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Step-2</td>
<td style="text-align: center;">2024年3月</td>
<td style="text-align: center;">阶跃星辰，无开源，无细节发布</td>
</tr>
<tr class="even">
<td style="text-align: center;">MM1</td>
<td style="text-align: center;">2024年3月</td>
<td style="text-align: center;">苹果，多模态MoE，无开源，有技术报告</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Grok-1</td>
<td style="text-align: center;">2024年3月</td>
<td style="text-align: center;">X，开源</td>
</tr>
<tr class="even">
<td style="text-align: center;">Qwen1.5-MoE-A2.7B</td>
<td style="text-align: center;">2024年3月</td>
<td style="text-align: center;">阿里巴巴，开源</td>
</tr>
<tr class="odd">
<td style="text-align: center;">DBRX</td>
<td style="text-align: center;">2024年3月</td>
<td style="text-align: center;">Databricks，开源</td>
</tr>
<tr class="even">
<td style="text-align: center;">Jamba</td>
<td style="text-align: center;">2024年3月</td>
<td style="text-align: center;">AI21，开源</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Mistral-8×22B</td>
<td style="text-align: center;">2024年4月</td>
<td style="text-align: center;">Mistral AI，开源</td>
</tr>
<tr class="even">
<td style="text-align: center;">WizardLM-2-8×22B</td>
<td style="text-align: center;">2024年4月</td>
<td style="text-align: center;">微软，开源</td>
</tr>
<tr class="odd">
<td style="text-align: center;">天工3.0</td>
<td style="text-align: center;">2024年4月</td>
<td style="text-align: center;">昆仑万维，400BMoE</td>
</tr>
<tr class="even">
<td style="text-align: center;">Arctic</td>
<td style="text-align: center;">2024年4月</td>
<td style="text-align: center;">Snowflake，480B，Dense-MoE
Hybrid，开源</td>
</tr>
</tbody>
</table>
</center>
<p>MoE模型目前风头正劲，就连前不久小米汽车发布会上，雷总也弄了个多模态MoE大模型做汽车智能中控</p>
<img src="/44e38c1b/xiaomi_moe.jpg" class title="小米汽车多模态MoE模型">
<p>相信今年接下来的这段时间，MoE还会给我们带来更多的大新闻。</p>
<p>本篇将初步梳理MoE相关的一些经典工作和几个近期发布的中文MoE模型，从背景、思路和效果来了解MoE模型。</p>
<p>到文章发出的2024年4月为止，个人认为DeepSeek-MoE和Qwen1.5-MoE是中文领域做得比较好的两个工作，赶时间的朋友可以优先关注这两个工作。</p>
<h1 id="时间线">时间线</h1>
<p>这里先对后面会涉及的MoE相关工作，大致按时间线梳理一下，也列出一些关键信息包括模型结构、模型规模等。</p>
<p>（很多经典的MoE工作都出自Google）</p>
<h2 id="上古时代">上古时代</h2>
<p>首先是很多MoE相关论文都会引用的，发表在1991年的论文<a href="https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf">《Adaptive
Mixtures of Local Experts》</a>，这篇文章出自Geoffrey Hinton和Michael I.
Jordan两位大神之手。虽然在更早的时候就有MoE相关概念的工作，如原文所提到的，1988年这个概念就有了</p>
<blockquote>
<p>This idea was first presented by Jacobs and Hinton at the
Connectionist Summer School in Pittsburg in 1988.</p>
</blockquote>
<p>但是大部分MoE文章还是认为是这个工作奠定了MoE的基础。</p>
<h2 id="rnn时代">RNN时代</h2>
<p>时隔二十多年，Google在2017年1月发布了<a href="https://arxiv.org/abs/1701.06538">《Outrageously Large Neural
Networks: The Sparsely-Gated Mixture-of-Experts
Layer》</a>，把MoE带进了LSTM，训出了最大137B参数，专家数达到128k的LSTM模型。</p>
<h2 id="transformer时代">Transformer时代</h2>
<ol type="1">
<li><p>2020年6月，Google发布<a href="https://arxiv.org/abs/2006.16668">《GShard: Scaling Giant Models
with Conditional Computation and Automatic
Sharding》</a>，把MoE应用在encoder-decoder结构的transformer模型上，每两层将一个FFN层替换成一个MoE层，训出了模型参数量从12.5B到600B的一系列MoE模型，每层最大专家数也达到2048个。</p></li>
<li><p>2021年1月，Google发布<a href="https://arxiv.org/abs/2101.03961">《Switch Transformers: Scaling
to Trillion Parameter Models with Simple and Efficient Sparsity》</a>
，在T5（encoder-decoder结构）的基础上，把FFN层替换成MoE层，并简化了routing策略，训出了最大1.6T参数量的switch
transformer。Switch
Transformers对scaling、蒸馏等做了很多详细的探索，影响深远，是很重要的一个工作。</p></li>
<li><p>2022年2月，Google发布<a href="https://arxiv.org/abs/2202.08906">《ST-MoE: Designing Stable and
Transferable Sparse Expert
Models》</a>，也是一个基于encoder-decoder结构的MoE模型，最大模型有269B的总参数，32B的激活参数。ST-MoE可以说不仅仅是一个MoE工作，对于模型结构、工程实现、训练策略等都做了很多分析，个人认为其重要程度相比Switch
Transformer都有过之而无不及。</p></li>
</ol>
<h2 id="gpt时代">GPT时代</h2>
<ol type="1">
<li><p>2021年12月，Google发布了GLaM，<a href="https://arxiv.org/abs/2112.06905">《GLaM: Efficient Scaling of
Language Models with
Mixture-of-Experts》</a>，训出了最大为1.2T参数量的decoder-only模型。（从encoder-decoder到decoder-only，可以看到Google内部在模型结构方向上也有很多不同的尝试）</p></li>
<li><p>2024年1月，幻方量化发布<a href="https://arxiv.org/abs/2401.06066">《DeepSeekMoE: Towards Ultimate
Expert Specialization in Mixture-of-Experts Language
Models》</a>，对在23年12月开源的DeepSeekMoE，给出了一些细节。</p></li>
<li><p>2024年，Databricks的DBRX、阿里的Qwen1.5-MoE-A2.7B、Mistral
AI的Mistral-8x22B等陆续发布。</p></li>
</ol>
<h1 id="奠基工作">奠基工作</h1>
<p>Geoffrey Hinton和Michael I. Jordan的<a href="https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf">《Adaptive
Mixtures of Local Experts》</a>是大多数MoE论文都会引用的最早工作。</p>
<ol type="1">
<li>思路</li>
</ol>
<p>这篇文章大致的思路是这样的：对于比较复杂的任务，一般可以拆分为多个子任务。比如要求计算输入文本中有多少个动词和名词，那就可以拆分为“数动词”和“数名词”这两个子任务。</p>
<p>而一个模型如果要同时学习多个子任务，多个子任务相互之间就会互相影响，模型的学习就会比较缓慢、困难，最终的学习效果也不好。</p>
<p>因此这篇文章提出了一种由多个分开的子网络组成的监督学习方法。这些分开的网络，在训练过程中，分别学习处理整个训练数据集中的一个子集，也就是一个子任务。这个思路就是现代MoE的思路，每个子网络（也就是一个expert）学习处理一部分内容。</p>
<p>文章里把这个MoE的方法应用于vowel discrimination
task，即元音辨别任务，验证了MoE设计的有效性。元音辨别指的是语音学中区分不同元音的能力，在语音学中，模型需要学习辨别不同的元音因素，以便准确地理解和识别语音输入。通过让多个子模型分别学习分别学习不同元音（a、e、i、o、u）辨别的子任务，最终效果得到了提升。</p>
<ol start="2" type="1">
<li>模型设计</li>
</ol>
<p>下图展示的就是这个MoE的思路：各个expert network和gating
network接收同样的输入，每个expert给出各自的处理结果；而gating
network输出每个expert的权重，就像一个开关一样，控制着每个expert对当前输入的打开程度，只是这个开关不是离散的，而是stochastic的，给出的不是true和false，而是权重。</p>
<img src="/44e38c1b/vanilla_moe.png" class title="Vanilla MoE">
<ol start="3" type="1">
<li>损失函数优化</li>
</ol>
<p>实际上，MoE这个idea在这篇文章之前就有了。如论文中所提，Jacobs和Hinton在1988就讨论过。但是之前的工作在loss的设计上，和ensemble更相近，多个expert之间更倾向于合作，每个expert会学习其他expert的residual部分。</p>
<p>具体来说，对于case <span class="math inline">\(c\)</span>，假设第
<span class="math inline">\(d^c\)</span> 是对应的ground truth，第 <span class="math inline">\(i\)</span> 个expert的输出是 <span class="math inline">\(o_{i}^c\)</span>，<span class="math inline">\(p_{i}^c\)</span> 是gating network给第 <span class="math inline">\(i\)</span>
个expert分配的权重，那么以前的工作所使用的损失函数 <span class="math inline">\(E^{c}\)</span> 计算如下</p>
<p><span class="math display">\[E^{c}=\left|\left|d^{c}-\sum_{i}p_{i}^{c}o_{i}^{c}\right|\right|^{2}\]</span></p>
<p>这样的损失计算方式，是把期望输出和所有expert输出的混合结果进行比较。</p>
<p>这样做的结果是，在训练过程中，每个expert学习的其实是其他expert的组合结果所剩下的残差。这样的学习目标并不能很好迫使每个expert单独输出好的结果，因此不能得到稀疏的模型。</p>
<p>从另一个角度来看，这个损失计算把所有专家耦合在了一起。即当一个expert的输出发生了变化，所有expert的组合结果也会变化，其他所有的expert也需要做相应的改动来适应这个变化。因此各个expert之间更加倾向于合作，而不是相互竞争并单独给出好的结果，让gating
network输出稀疏的结果。</p>
<p>虽然可以使用如增加辅助损失函数的做法，迫使模型给出稀疏激活的结果，但是这样相当于增加了很强的先验正则化，对模型最终效果也是有损害的。</p>
<p>而Hinton和Jordan在这个工作里，提出更简单的做法是对loss计算进行修改，使得各个expert之间的关系从合作变成竞争。</p>
<p>假设gating network每次随机选择一个expert，损失计算如下</p>
<p><span class="math display">\[E^{c}=\langle\|\mathbf{d}^c-\mathbf{o}_i^c\|^2\rangle=\sum_{i}p_{i}^{c}\left|\left|d^{c}-o_{i}^{c}\right|\right|^{2}\]</span></p>
<p>在这个损失函数中，每个expert的输出结果会单独和期望结果进行对比，这就要求每个expert单独给出完整的结果，而不是仅学习其他expert的残差。</p>
<p>这样的loss计算具有localization的特性，即如果一个训练case错了，那么会被修改的主要是被gating
network选中且出错的expert，以及负责分配权重的gating
network，而不会很大地影响其他expert。</p>
<p>此外，localization还体现在，每个expert只会负责处理输入空间中某个特定子空间的向量，而不是完整的输入空间。</p>
<p>这样一来，不同的expert之间不会直接相互影响，虽然还是有间接的影响，比如某个expert的输出变了，gating
network可能会分配新的权重，但是至少不会改变其他expert
error的符号（+，-），即优化的方向。</p>
<p>最终的结果是，对于给定的输入，这样的系统会倾向于以高权重分配单一一个expert来预测结果（但其他权重还不是真正的0，不是真正的稀疏）。</p>
<ol start="4" type="1">
<li>实操技巧</li>
</ol>
<p>上面提出的这个loss计算，理论上没有问题，实际上也能训练，但是为了得到更好的效果，作者把原loss计算作了如下变化：先指数化再求和，最后再取对数，得到了优化loss。看下变化前后的对比</p>
<p><span class="math display">\[\text{原loss：}E^{c}=\sum_{i}p_{i}^{c}\left|\left|d^{c}-o_{i}^{c}\right|\right|^{2}\]</span></p>
<p><span class="math display">\[\text{优化loss：}E^c=-log\sum_ip_i^ce^{-\frac12\|\mathbf{d}^c-\mathbf{o}_i^c\|^2}\]</span></p>
<p>这样做有什么好处呢？来对比一下原loss函数和优化后的loss函数的求导结果</p>
<p><span class="math display">\[\text{原loss导数：}\frac{\partial
E^c}{\partial\mathbf{o}_i^c}=-2p_i^c(\mathbf{d}^c-\mathbf{o}_i^c)\]</span></p>
<p><span class="math display">\[\text{优化loss导数：}\frac{\partial
E^c}{\partial\mathbf{o}_i^c}=-\left[\frac{p_i^ce^{-\frac{1}{2}\|\mathbf{d}^c-\mathbf{o}_i^c\|^2}}{\sum_jp_j^ce^{-\frac{1}{2}\|\mathbf{d}^c-\mathbf{o}_j^c\|^2}}\right](\mathbf{d}^c-\mathbf{o}_i^c)\]</span></p>
<p>相比原loss函数的导数，优化后的loss函数的导数，把当前第 <span class="math inline">\(i\)</span>
个expert的表现，和其他expert联系起来了。这样能够更好地衡量expert <span class="math inline">\(i\)</span>
对当前case的处理结果好坏。特别是在训练初期，gating
network的权重是近似平均分配的，那么使用原loss函数的结果是，对当前case效果最好的expert，学习速度是最慢的（因为loss最小）；而优化的loss函数则可以让当前最好的expert的学习速度最快。相当于让“有天赋”的专家在对应的子任务上尽快提高水平。这样就强化了localization的特征，使得各个expert更快拟合到自己擅长的部分，加速训练。</p>
<p>（BTW，优化后的这个loss导数，和现在的对比学习形式上看起来也很相似）</p>
<p>这个工作在今天看来不很复杂，但是思路还是很踏实有效的，给MoE奠定了基础。</p>
<h1 id="lstm-moe">LSTM MoE</h1>
<p>Google在2017年1月发布了 <a href="https://arxiv.org/abs/1701.06538">《OUTRAGEOUSLY LARGE NEURAL
NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS
LAYER》</a>，把MoE应用到了LSTM上，训出了最大137B的LSTM模型。这样规模的模型哪怕放在7年后的今天，也是巨无霸的存在，需要解决很多工程问题。</p>
<p>相比1991年的工作，这里做到了真正的稀疏激活，从而可以在实际计算量较少的情况下，训练巨大的模型。</p>
<h2 id="背景">背景</h2>
<p>虽然当时Transformer还没出来，大规模模型的竞赛也还不像今天这么激烈，但是在多个领域中（文本、图像、音频），已经有不少工作反复证实了一件事：模型容量越大，能训出来的效果越好，上限越高。但是模型越大，需要的训练数据也就越多，二者共同作用下，就造成了训练开销基本是随着模型增大，以平方关系在增长。</p>
<p>在这个背景下就出现一些conditional
computation，条件计算的工作来解决这个问题。conditional
computation就是根据输入，有选择地只激活部分网络模块。那么MoE其实就是一种条件计算的实现。由于不用激活全部参数，训练所需的计算量就大大减小，整体计算成本就不用以平方速度增长。</p>
<p>虽然理论上计算量的成本下来了，不过实操起来还是会遇到几个问题：</p>
<ul>
<li>训练的时候，在MoE结构下，每个expert的batch size比整个模型的batch
size小了。<br>
比如模型的batch
size是32，一共有16个expert，那实际上一次迭代平均每个expert只能分到2个训练样本。而batch
size对训练效率影响是很大的，大的batch
size摊小了参数传输和更新的成本。如果直接增大模型的batch
size，又会受显存和通讯效率的限制。<br>
</li>
<li>训练数据量不足。<br>
要训大模型就需要大量的数据，让模型参数充分学习。在当时的背景下，大规模的NLP数据是比较缺的。当然如今数据集多了很多，特别是预训练数据，这个问题现在来看没有那么突出了。<br>
</li>
<li>损失函数的设计。<br>
如何使用合适的损失函数来训练模型，提升效果，并且使得模型的负载比较均衡，这是一个不容易解决的问题。<br>
</li>
<li>集群通讯问题。<br>
一个GPU集群的计算能力可能比设备间网络带宽的总和高出数千倍，因此设备间的通讯很可能成为训练效率的瓶颈。为了计算效率，就要使得设备内计算量和所需的通讯量的比值，达到相应的比例。<br>
</li>
<li>GPU计算特点。<br>
GPU做数学计算很快，但是并不擅长做branching（if/else），因此MoE的工作基本上都是用gating
network来控制参数的激活。这个严格来说不算是新的挑战了，应该说是根据计算设备沿用下来的设计。</li>
</ul>
<p>要解决好这些问题，才能训出比较好的模型来。</p>
<h2 id="模型设计">模型设计</h2>
<ol type="1">
<li>整体结构</li>
</ol>
<p>先看下模型结构的设计。</p>
<p>论文里使用的是两个LSTM层，中间夹着一个MoE层，最上面和最下面分别还有一个embedding层和一个任务输出层，结构如下图所示</p>
<img src="/44e38c1b/rnn_moe.png" class title="LSTM MoE">
<p>每个expert是一个简单的feed-forward neural
network。一共有n个expert，gating network输出是一个稀疏的n维向量</p>
<p><span class="math display">\[\begin{aligned}y=\sum_{i=1}^nG(x)_iE_i(x)\end{aligned}\]</span></p>
<p><span class="math inline">\(E_{i}(x)\)</span> 是第 <span class="math inline">\(i\)</span> 个expert的输出，<span class="math inline">\(G(x)_{i}\)</span> 是gating network给出的第 <span class="math inline">\(i\)</span> 个expert的权重。</p>
<p>如果 <span class="math inline">\(G(x)_{i}\)</span>
为0，就不用计算对应的那个expert了，节省了计算。</p>
<p>如果expert的数量特别多，可以用two-level hierarchical
MoE，即使用两层gating network，第一层的gating
network先选择一个包含一批expert的分支，每个分支又有一个单独的gating
network来选择具体的expert。类似word2vec训练所用的hierarchical
softmax。这样做可以节省一些计算。</p>
<ol start="2" type="1">
<li>gating network</li>
</ol>
<p>那具体gating network怎么设计呢？</p>
<p>如果对输入进行线性变换，再简单加上一个softmax，那得到的是一个非稀疏的gating
function</p>
<p><span class="math display">\[\begin{aligned}G_\sigma(x)=Softmax(x\cdot
W_g)\end{aligned}\]</span></p>
<p>在这个基础上，使用一个topk函数，只保留最大的k个值，其他都设为﹣∞（softmax之后变成0），这样就能只选择部分expert，得到了稀疏性。</p>
<p>论文提到，虽然理论上这个形式的sparsity（topk）会造成gating
function的不连续，不过在实操中暂时没有遇到相关问题。</p>
<p>在这个基础上，在输入再加上一个Gaussian
noise，这个noise的大小由另外一个可学习的参数来控制。整体的计算公式如下</p>
<p><span class="math display">\[\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\end{aligned}\]</span></p>
<p><span class="math display">\[KeepTopK(v,k)_i=\begin{cases}v_i&amp;\text{if
}v_i\text{ is in the top }k\text{ elements of
}v.\\-\infty&amp;\text{otherwise.}\end{cases}\]</span></p>
<p><span class="math display">\[\begin{aligned}H(x)_i=(x\cdot
W_g)_i+StandardNormal()\cdot Softplus((x\cdot
W_{noise})_i)\end{aligned}\]</span></p>
<p>其中用来调整noise的非线性函数softplus是个类似ReLU的激活函数，但是更为光滑，函数图像如下</p>
<img src="/44e38c1b/softplus.png" class title="softplus">
<p>这里添加噪声的原因和负载均衡有关，下面来分析下负载均衡。</p>
<h2 id="负载均衡">负载均衡</h2>
<p>在MoE模型训练的实验中观察到，如果不对gating
network进行干预，任由模型自由学习，那么最终模型会倾向于收敛到“总是选那几个固定的expert”的状态，而其他expert几乎不会被使用。这就是负载不均衡的状态，如果这些专家分布在不同的计算设备上，结果就是有些设备输入排队特别长，而有些设备基本处于闲置状态，这明显不是我们想要的。</p>
<p>这种负载不均衡的状态有自我加强的属性，因为一旦开始出现部分专家被较多选中激活，这些专家就会得到更充分的训练，从而获得更好的效果，进而又提升被选中激活的概率。</p>
<p>针对这种情况，之前有一些工作使用hard
constraint来缓解，比如当某个expert激活次数达到上限，就把它从候选集合中移除。hard
constraint明显会对模型效果有影响。而这篇论文使用的是一种soft
constraint。</p>
<p>具体来说，对于每个expert，定义了一个它在当前这批输入数据里的重要性指标，如以下公式所示</p>
<p><span class="math display">\[Importance(X)=\sum_{x\in
X}G(x)\]</span></p>
<p><span class="math inline">\(G(x)\)</span> 是gating
network给出的权重，是一个维度等于expert数量的向量。</p>
<p>基于这个重要性指标，论文定义了一个辅助损失 <span class="math inline">\(L_{importance}\)</span>，训练时和模型的交叉熵损失加到一起。<span class="math inline">\(L_{importance}\)</span> 的计算方式如下</p>
<p><span class="math display">\[L_{importance}(X)=w_{importance}\cdot
CV(Importance(X))^2\]</span></p>
<p>其中权重 <span class="math inline">\(w_{importance}\)</span>
是手动设置的超参，实验的推荐值是0.1，CV是coefficient of variation。</p>
<p>coefficient of
variation离散系数，是概率分布离散程度的一个归一化量度，定义为标准差
<span class="math inline">\(\sigma\)</span> 和 均值 <span class="math inline">\(\mu\)</span> 的比值。</p>
<p>对于MoE来说，确定激活的expert数之后，均值是固定的。如果expert的gating很不平衡，标准差就会很大，离散系数也会很大，使得
<span class="math inline">\(L_{importance}\)</span> 变大。</p>
<p>但是这里还是有问题，虽然均衡的负载可以推导出 <span class="math inline">\(L_{importance}\)</span> 较小的结论，但是 <span class="math inline">\(L_{importance}\)</span>
较小却不能保证负载均衡。也就是说 <span class="math inline">\(L_{importance}\)</span>
较小只是负载均衡一个必要不充分条件。</p>
<p>比如一个expert可能以很高的权重被分配到一个样本，而另一个expert可能以不太高的权重被分配到好几个样本。这种情况下对所有输入数据的gating权重进行求和，仍然可能呈现出均匀的表象（离散系数比较小），但这并不符合我们的要求。</p>
<p>为了解决这个问题，需要额外再加上一个损失 <span class="math inline">\(L_{load}\)</span>
。这里就要用到添加在每个expert输出上的随机噪音了。</p>
<p>我们想要各个expert的负载均衡，也就是每个专家需要处理的样本数基本一致，但是分配到各个专家的样本数是个离散值，因此没有办法直接用于back
propagation，而 <span class="math inline">\(L_{load}\)</span>
就是对各个expert负载的一个平滑评估。</p>
<p>回想一下前面在设计MoE的时候，定义了 <span class="math inline">\(H(x)\)</span> 为KeepTopK函数的输入</p>
<p><span class="math display">\[\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}H(x)_i=(x\cdot
W_g)_i+StandardNormal()\cdot Softplus((x\cdot
W_{noise})_i)\end{aligned}\]</span></p>
<p>那么这里先定义一个 <span class="math inline">\(kth\_excluding(H(x),k,i)\)</span>，表示在除去
<span class="math inline">\(H(x)\)</span> 中的第 <span class="math inline">\(i\)</span> 个分量之后，排在第 <span class="math inline">\(k\)</span> 大的值。基于这个，再定义 <span class="math inline">\(P(x,i)\)</span>
为：固定其他分量已经选取好的noise，重新给第 <span class="math inline">\(i\)</span> 个分量再添加一次noise，结果比 <span class="math inline">\(kth\_excluding(H(x),k,i)\)</span>
大的概率，公式如下</p>
<p><span class="math display">\[\begin{aligned}P(x,i)=Pr\Big((x\cdot
W_g)_i+StandardNormal()\cdot Softplus((x\cdot
W_{noise})_i)\\&gt;kth\_excluding(H(x),k,i)\Big)\end{aligned}\]</span></p>
<p>通过这个noise，我们把“第 <span class="math inline">\(i\)</span>
个专家是否处理这个输入”的离散值，变成“第 <span class="math inline">\(i\)</span>
个专家处理这个输入的概率”这样一个平滑的估计，<span class="math inline">\(P(x,i)\)</span>
就表示这个概率。这个概率可以简化写成</p>
<p><span class="math display">\[\begin{aligned}P(x,i)&amp;=\Phi\Big(\frac{(x\cdot
W_g)_i-kth\_excluding(H(x),k,i)}{Softplus((x\cdot
W_{noise})_i)}\Big)\end{aligned}\]</span></p>
<p>其中 <span class="math inline">\(\Phi\)</span>
是标准正态分布的CDF。</p>
<p>接下来就可以把第 <span class="math inline">\(i\)</span>
个expert的负载定义为</p>
<p><span class="math display">\[\begin{aligned}Load(X)_i=\sum_{x\in
X}P(x,i)\end{aligned}\]</span></p>
<p>有了每个expert的负载衡量，就可以和前面第一个负载均衡损失一样，计算新的负载均衡损失了</p>
<p><span class="math display">\[L_{load}(X)=w_{load}\cdot
CV(Load(X))^2\]</span></p>
<p><span class="math inline">\(w_{load}\)</span>
是手动设置的超参，实验的推荐值是0.1。</p>
<p>相比前面的 <span class="math inline">\(L_{importance}(X)\)</span>，<span class="math inline">\(Load(X)\)</span>
是对负载是否均衡更细粒度的评估。</p>
<p>论文中提到一个细节，在刚开始训练的时候，希望模型分配的expert尽量均衡，因此把
<span class="math inline">\(W_g\)</span> 和 <span class="math inline">\(W_{noise}\)</span>
都设为0，这样相当于没有信号，也没有噪音。</p>
<p>最终使用负载均衡之后的效果如下</p>
<img src="/44e38c1b/rnn_moe_load_function.png" class title="负载平衡效果">
<p>使用这两个负载均衡损失之后，能达到接近完全平均分配的效果。</p>
<h2 id="实验">实验</h2>
<ol type="1">
<li>解决工程问题</li>
</ol>
<p>针对前面提出的一些工程问题，论文给出一些方案</p>
<p>（1）batch size减小</p>
<p>由于稀疏激活的原因，每个expert的batch
size会变小。假设每次在n个expert中选择k个，模型训练的batch
size为b，那么每个expert的batch
size就是kb/n。论文通过以下这几种方法来提升每个expert的batch size：<br>
-
混合使用数据并行和模型并行。本来在使用数据并行的情况下，每个模型副本是异步处理各自的数据的。而这里做了优化，各个副本的batch是同步处理的，这样就可以把多个模型副本的batch组合起来。对于非MoE部分的参数，依然使用标准的数据并行机制；而对于每个expert，则在整个集群中只保留一个副本。如果模型分布在d个设备上，那每个expert就能得到一个kbd/n的batch
size。 - 对于LSTM模型，在时间步上展开，就能把batch
size提升相应的倍数。</p>
<p>（2）集群通讯问题</p>
<p>另一个挑战就是平衡集群计算量和通讯量的关系。</p>
<p>对于每个expert来说，主要的通讯就是input和output的传输。而每个专家的主要计算量就是两个全连接层，大小分别为[input_size,
hidden_size]和[hidden_size,
output_size]。对于GPU来说，计算速度可能是通讯速度的1000倍，那我们就需要把计算量设计得足够大。最简单的做法就是把hidden_size提高，使得每个expert的内部计算量比通讯量大1000倍，以保证通讯不会成为训练的瓶颈。</p>
<ol start="2" type="1">
<li>模型容量 &amp; 参数效率</li>
</ol>
<p>为了验证模型容量提升带来的收益，以及MoE模型的参数效率（即和dense模型同样推理计算量下能达到的效果），训练了包含4/32/256个expert的flat
MoE模型，和包含256/1024/4096个expert的hierarchical
MoE模型。每个expert大约是1M参数量，对于所有flat模型都是激活4个expert，而对于hierarchical
MoE是每层gating激活2个。</p>
<p>效果如下图。左边的图显示，随着模型容量提升，测试的ppl有明显下降。右边的图将相近模型容量的dense模型和MoE模型的效果放在一起对比，可以看到MoE模型在相同模型容量下，效果更好</p>
<img src="/44e38c1b/rnn_moe_perf.png" class title="效果">
<ol start="3" type="1">
<li>更大的模型</li>
</ol>
<p>前面几个模型训练用的数据量不是很大，模型最大也只有4B左右，训练不久就出现diminishing
returns。</p>
<p>为了验证更大数据集 + 更大模型的收益，在100B
token的语料上，分别训了包含32, 256, 1024，4096, 16384, 65536,
和131072个expert的MoE模型，最大的模型达到了137B的参数量。</p>
<p>各个模型对比如下表。整体来看，增加数据和模型容量，是可以继续获得提升的。</p>
<img src="/44e38c1b/rnn_moe_137b.png" class title="137模型效果">
<p>从这里还可以看出，在专家数量不太多时，提升专家数量效果有提升，但是收益会慢慢减小，甚至会出现专家数量太多，效果反而下降的情况。</p>
<ol start="4" type="1">
<li>Expert Specialization</li>
</ol>
<p>按照MoE的设计思路，不同的专家应该学习到不同的子任务，但是实际上是否是这样呢？</p>
<p>论文里把模型中不同的专家分配到token拿出看，发现确实有比较强的specialization效果，不同的专家处理不同的内容，如下所示</p>
<img src="/44e38c1b/rnn_moe_specilized.png" class title="RNN MoE 专门化">
<h1 id="gshard">GShard</h1>
<ol type="1">
<li>简介</li>
</ol>
<p>2018年，随着Bert的发布，transformer结构彻底火了起来。2020年6月，Google发布《GShard:
Scaling Giant Models with Conditional Computation and Automatic
Sharding》，把MoE用到了encoder-decoder结构的transformer模型上。MoE开始变成我们现在熟悉的样子了。</p>
<p>GShard这个工作做了很多的实验，训了很多规模巨大的MoE模型，最大的达到了600B。训练的一系列模型的参数如下表</p>
<img src="/44e38c1b/gshard_moe_family.png" class title="GShard MoE family">
<p>在expert数量的设计上，延续上面LSMT MoE工作的思路 --
expert越多，效果越好。（站在24年这个时间节点来看，太多的expert未必适合；但是也不能说这个思路一定错误，毕竟事物的发展是螺旋式的，就像ChatGPT出来之前大多数人都在魔改各种Bert，而GPT已经坐了几年冷板凳了。）</p>
<p>GShard论文中很大的篇幅在介绍工程实现和优化，这也是MoE模型训练最大的痛点。关于工程框架的内容比较硬核，因此这里不会展开讲太多，而是关注在模型算法层面上。</p>
<ol start="2" type="1">
<li>模型设计</li>
</ol>
<p>先来看下模型设计。</p>
<p>Google在那段时间走的是encoder-decoder
transfomer的技术路线，因此GShard也是基于encoder-decoder
transfomer的模型结构。</p>
<p>GShard的模型设计是，在encoder和decoder中，每两层把其中一个FFN层替换成MoE层。对于总共有N层的模型，则有N/2个MoE层，如下图</p>
<img src="/44e38c1b/gshard_model.png" class title="GShard模型结构">
<p>每层会选择最多top-2 expert来激活。为什么是最多，后面解释。</p>
<p>GShard在上面这篇LSTM MoE论文的基础上，改进了gating
function和auxiliary loss function。</p>
<p>从公式来看，MoE层的具体计算如下</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{G}_{s,E}&amp; =\mathrm{GATE}(x_s)  \\
\mathrm{FFN}_e(x_s)&amp; =wo_e\cdot\text{ReLU}(wi_e\cdot x_s)  \\
y_{s}&amp; =\sum_{e=1}^E\mathcal{G}_{s,e}\cdot\mathrm{FFN}_e(x_s)
\end{aligned}\]</span></p>
<p>其中 <span class="math inline">\(x_s\)</span> 是MoE的输入token，<span class="math inline">\(w_i\)</span> 和 <span class="math inline">\(w_o\)</span>
分别是输入输出的线性变换矩阵。向量<span class="math inline">\(\mathcal{G}_{s}\)</span> 就是gating
function的输出。</p>
<p>GShard在gating
function的设计上提出了两个要求：（1）负载均衡（2）高效扩展。</p>
<p>负载均衡和前面讲的一样，很好理解。而为什么要高效扩展，因为如果要对N个token分别进行E个expert的分配，在N能达到百万甚至千万级别，而E也有几百上千的情况下，就需要一个高效的分布式实现，以免其他计算资源等待gating
function。</p>
<p>为了满足这些要求，gating function提出了以下机制</p>
<p>（1）专家容量 expert capacity</p>
<p>为了确保负载平衡，我们不希望有少量expert需要处理很多token，因此强制规定了每一个expert所负责处理的token数量有一个最大值，这个最大值就叫专家容量，在这里设置为2N/E，相当于平均分配的量。</p>
<p>这个expert capacity通过GATE(·)给每个expert维护一个计数器 <span class="math inline">\(c_e\)</span>
来监控。如果一个token所选的两个专家当前处理量都已经超过设定的专家容量，那么这个token就不会被当前层的任何expert处理，而是直接通过残差链接透传到下一层。</p>
<p>（2）分组分配 Local group dispatching</p>
<p>给所有输入token分成了G组，不同的组并行处理，每个组相应地也把组内专家容量变成2N/EG。</p>
<p>这样做相当于在前向推理时，把大的batch拆分成小的batch，每个小的batch就是一个group。这样做的好处是通讯的时候（特别是all2all）只需要在每个group内进行就可以了，减少了通讯量。</p>
<p>而进行反向计算的时候这些group可以合起来一起用，相当于进行了gradient
accumulation。</p>
<p>（3）辅助损失函数 Auxiliary loss</p>
<p>光设置专家容量并不能使得gating负载均衡，而且会导致大量溢出。参考前面LSTM
MoE的工作，这里也定义了一个辅助损失函数，来帮助负载均衡。辅助损失函数设计如下</p>
<p><span class="math display">\[\ell_{aux}=\frac1E\sum_{e=1}^E\frac{c_e}S\cdot
m_e\]</span></p>
<p><span class="math inline">\(S\)</span> 是token数，<span class="math inline">\(E\)</span> 是专家数，<span class="math inline">\(c_e\)</span> 是分配给第 <span class="math inline">\(e\)</span> 个专家的token数，<span class="math inline">\(m_e\)</span> 是第 <span class="math inline">\(e\)</span> 个expert在 <span class="math inline">\(S\)</span> 个token中获得的平均权重。</p>
<p>思路是，本来是要算 <span class="math inline">\(\frac{c_e}S\)</span>
的平方的，但这是离散值不可导，因此把平方中的一个 <span class="math inline">\(\frac{c_e}S\)</span> 换成了 <span class="math inline">\(m_e\)</span> ， <span class="math inline">\(m_e\)</span> 是第 <span class="math inline">\(e\)</span> 个expert在 <span class="math inline">\(S\)</span>
个token中获得的平均权重。在平均分配的情况下，这个loss达到最小。</p>
<p>相比前面的负载均衡损失，这个loss的设计就简单许多。</p>
<p>gating的整个算法如下</p>
<img src="/44e38c1b/gshard_algo_1.png" class title="GShard gating 算法">
<p>（4）随机路由 Random routing</p>
<p>前面提到，每层会选择最多top-2
expert来激活，就是因为有随机路由的机制。直观来说，就是认为如果top-1专家的权重很高，而第二个专家的权重如果较小，那很有可能只用第一个专家就足够解决问题了。</p>
<p>随机路由的机制是top-1的专家永远会被激活，而第二个专家如果权重很小，就认为它可以被忽略。具体来说，会以与第二个专家的权重g2成比例的概率激活第二个专家。</p>
<ol start="3" type="1">
<li>效果</li>
</ol>
<p>最后看一下模型在翻译任务上的效果</p>
<img src="/44e38c1b/gshard_perf.png" class title="GShard效果">
<h1 id="switch-transformer">Switch Transformer</h1>
<p>2022年4月，距离ChatGPT发布还有半年，Google发布了《Switch
Transformers: Scaling to Trillion Parameter Models with Simple and
Efficient Sparsity》（实际上2021年Google就提出Switch
Transformer了）。</p>
<p>Switch
Transformer和GShard一样，是encoder-decoder结构，基于T5开发的，具有1.6T的参数，2048个expert。</p>
<p>和前面的很多工作一样，Switch
Transformer有一个出发点，那就是参数量越大，模型效果越好，并且可以通过稀疏激活来减少总计算量。</p>
<p>但是相比其他工作，Switch
Transformer给出了一个更为具体的描述，那就是模型参数量可以是一个独立于总计算量的，单独的缩放轴。也就是说，在改变参数量的同时，（几乎）不改变训练和推理的计算量，就可以带来效果的提升。因此Switch
Transformer关注在“同样的FLOPS/token的计算量”下，如何扩大模型，提升效果。</p>
<p>Switch Transformer所做的工作还是比较多的，包括：</p>
<p>（1）模型结构简化：简化了Transformer上的MoE架构，提出Switch
Transformer架构。</p>
<p>（2）MoE to
dense：把训出来的效果较好的MoE模型蒸馏到dense模型，在压缩MoE模型99%的参数的情况下，效果还是比直接训练dense模型好。</p>
<p>（3）训练和微调技术：<br>
- 首次使用bf16成功训练MoE模型<br>
- 更适合MoE结构的模型初始化<br>
- 增加的专家正则化，改善了稀疏模型的微调和多任务训练</p>
<p>（4）训练框架：结合数据、模型和专家并行性，训练了超过1T参数的MoE模型。</p>
<p>（5）多语言：在多语言数据集上训练，发现101种语言效果普遍有提升。</p>
<p>（6）训练效率：在同样的FLOPS/token的计算量下，Switch
Transformer模型收敛速度有数倍的提升。</p>
<h2 id="模型设计-1">模型设计</h2>
<p>Switch
Transformer的模型结构如下图，类似GShard，把transformer每层的FFN替换成MoE层</p>
<img src="/44e38c1b/switch_transformer_structure.png" class title="Switch Transformer 模型结构">
<p>Switch Transformer一个重要的改进点就是简化了gating
function的做法（Switch Transformer论文里叫routing）。</p>
<p>之前的工作大多探索了选择k个expert的做法，而Switch
Transformer则直接把gating简化为只选择1个expert，即k=1。这样的MoE层叫做Switch
layer。</p>
<p>这样简化之后，routing的实现更简单，router的计算量小了，也减少了通讯量。</p>
<h2 id="负载均衡-1">负载均衡</h2>
<p>同GShard一样，Switch Transformer规定了一个专家容量expert
capacity，来限制每个expert在一个batch里能处理的最大token数。</p>
<p>如果一个token被分配到了一个已经满载的expert，就会出现overflow，那这个token在本层就不会被处理，而是直接通过残差链接，透传给下一层。这点也同GShard一样。</p>
<p>在Switch Transformer，专家容量通过容量系数capacity factor来控制。</p>
<p><span class="math display">\[\text{expert
capacity}=\left(\frac{\text{tokens per batch}}{\text{number of
experts}}\right)\times\text{capacity factor}.\]</span></p>
<p>一个大的capacity
factor意味着每个expert能够处理更多的token，从而减少overflow情况的发生，但是计算量和通讯量的压力也会增大，所以这是一个需要权衡的参数。</p>
<p>下图给出了一个不同capacity factor下的例子</p>
<img src="/44e38c1b/switch_transformer_diff_expert_capacity.png" class title="不同的expert capacity">
<p>那么如何设定expert capacity呢？</p>
<p>如果capacity
factor为1的话，只有在完全平均分配的时候，才不会出现overflow的情况。而太大的capacity
factor则可能造成算力和存储的浪费。</p>
<p>首先，实验中发现expert的数量和overflow的数量之间没有什么关系，所以在所有实验中，所有MoE和Switch
Transformer模型都用128个专家。</p>
<p>不同的capacity
factor对模型影响如下表。可以看到，大的容量系数相对来说能取得更好的效果（因为更少的overflow），但是相应地，大容量系数的模型处理速度就会慢一些。</p>
<img src="/44e38c1b/switch_transformer_capacity_effect.png" class title="expert capacity的效果">
<p>经验上，低的token丢弃率对模型的scaling很重要，想要训练超大规模的模型，就要解决这个问题。而通过负载均衡损失就可以确保良好的平衡，使得在使用较小容量系数的情况下，overflow尽量少，从而兼顾效果和计算速度。</p>
<p>关键问题来到负载均衡损失怎么设计。</p>
<p>给定 <span class="math inline">\(N\)</span> 个expert，和包含 <span class="math inline">\(T\)</span> 个token的batch <span class="math inline">\(\mathcal{B}\)</span>，负载均衡损失是这么计算的</p>
<p><span class="math display">\[\begin{aligned}\text{loss}&amp;=\alpha\cdot
N\cdot\sum_{i=1}^Nf_i\cdot P_i\end{aligned}\]</span></p>
<p><span class="math inline">\(f_{i}\)</span> 表示被分配到第 <span class="math inline">\(i\)</span> 个expert的token数，这个不可导</p>
<p><span class="math display">\[\begin{aligned}f_i=\frac{1}{T}\sum_{x\in\mathcal{B}}\mathbb{1}\{\text{argmax
}p(x)=i\}\end{aligned}\]</span></p>
<p><span class="math inline">\(P_i\)</span>
表示整个batch每个token分配给第<span class="math inline">\(i\)</span>
个expert的概率的总和，这个可导</p>
<p><span class="math display">\[\begin{aligned}P_i=\frac{1}{T}\sum_{x\in\mathcal{B}}p_i(x).\end{aligned}\]</span></p>
<p>这个损失的设计其实和GShard中的也是一样的。</p>
<p>在完美平均分配的情况下，<span class="math inline">\(f\)</span> 和
<span class="math inline">\(P\)</span> 这两个向量都是 <span class="math inline">\(1/N\)</span>，这个时候负载均衡损失是最小的。</p>
<p><span class="math inline">\(\alpha\)</span>
扫描了1e-5到1e-1，发现设为1e-2，已经足够大保持负载平衡，同时不过分影响模型收敛。</p>
<p>观察到 <span class="math inline">\(\sum_{i=1}^N(f_i\cdot
P_i)=\sum_{i=1}^N(\frac1N\cdot\frac1N)=\frac1N\)</span>，所以负载均衡loss还乘了个
<span class="math inline">\(N\)</span>，这样可以保持无论使用多少个expert，在平均分配的情况下，loss都能保持相同的常数。</p>
<h2 id="实验-1">实验</h2>
<ol type="1">
<li>一些训练的trick</li>
</ol>
<p>（1）选择性地使用bf16</p>
<p>半精度训练会带来一些训练的不稳定。因此选择性地使用bf16，具体来说，routing
function内部使用单精度，其他部分使用半精度，这样既不影响通讯，也能提高效果。</p>
<p>为什么选择在routing提高精度？因为softmax对误差特别敏感，exponential计算会极大放大输入中的rounding
error，因此高精度对routing很重要。</p>
<p>（2）较小的参数初始化</p>
<p>从截断正态分布中抽取元素来初始化的模型参数，平均值 <span class="math inline">\(\mu=0\)</span>，标准差<span class="math inline">\(\sigma=\sqrt{s}/n\)</span>，其中s是超参，n是权重张量中的输入单元数量（e.g.
fan-in）。</p>
<p>论文建议将默认的Transformer初始化尺度s=1.0减少10倍。这个方案在实验中既提高了质量又降低了训练不稳定性的可能性。初始化实验对比如下表</p>
<img src="/44e38c1b/switch_transformer_init.png" class title="初始化对比">
<p>（3）增大dropout</p>
<p>由于Switch
Transformer参数量很大，在微调的时候更容易过拟合，因此一个简单的方法就是增大dropout，效果如下</p>
<img src="/44e38c1b/switch_transformer_dropout.png" class title="dropout效果">
<p>可以看到大的dropout有效果，并且dense层保持0.1，只有expert层增大dropout效果更好。</p>
<ol start="2" type="1">
<li>scaling</li>
</ol>
<p>对Switch Transformer结构预训练的scaling做了一些实验。</p>
<p>（1）Step-Basis</p>
<p>首先是验证在固定训练step的条件下，增大expert数量带来的提升，如下图所示。</p>
<p>左边是不同规模的模型在相同step下收敛的结果，可以看到在保持相同计算量的条件下，只通过增大专家数量来提升规模，就有明显的收益。右边则展示训练过程中，不同规模的模型在各个step下的效果。</p>
<img src="/44e38c1b/switch_transformer_scaling_step.png" class title="step scaling">
<p>（2）Time-Basis</p>
<p>虽然Switch
Transformer可以保持计算量不变的情况下提升模型规模，但是专家数量的增多会带来额外的通讯成本，所以即使训练的step数相同，实际的训练时间也不同。因此这里要回答的问题是，给定一个固定的训练时长，Switch
Transformer是否相比dense模型仍有收益。</p>
<p>答案是肯定的。下图展示以训练时长为横轴，Switch
Transformer和dense模型的效果对比。Switch
Transformer收敛到dense模型最终效果的时间只有dense模型的1/7。</p>
<img src="/44e38c1b/switch_transformer_scaling_time.png" class title="time scaling">
<p>（3）和更大的dense模型对比</p>
<p>前面Switch
Transformer和dense模型的比较，是基于相同计算量的前提。那么Switch
Transformer是否具备超越更大规模dense模型的能力？</p>
<p>下图在Step-Basis和Time-Basis对比了64个专家的Switch
Transformer和T5-Large。无论是相同step还是相同时间下，Switch
Transformer都有明显优势。</p>
<img src="/44e38c1b/switch_transformer_scaling_dense.png" class title="dense对比">
<ol start="3" type="1">
<li>SFT效果对比</li>
</ol>
<p>在GLUE和SuperGLUE等下游任务上微调，和dense模型对比。</p>
<p>对于各个模型，每两百步进行一次eval，选最好的效果，尽量保证公平。结果如下表，大部分任务都有明显的提升。</p>
<img src="/44e38c1b/switch_transformer_sft_result.png" class title="sft对比">
<ol start="4" type="1">
<li>模型蒸馏</li>
</ol>
<p>虽然Switch
Transformer在相同计算量下效果更好，但是部署几百B甚至T级别的模型，还是不太方便，因此考虑把稀疏模型蒸馏到dense模型上来进行推理。</p>
<p>论文中给出了几个蒸馏的技巧：<br>
- 初始化的时候，把Switch
Transformer模型中的非稀疏部分用于初始化dense模型<br>
- 蒸馏所用的label，25%来自教师模型，75%来自ground truth，加权求和</p>
<p>预训练模型的蒸馏效果如下，相比无蒸馏训练的dense模型，把同样计算量的稀疏模型蒸馏到dense模型，dense模型大约能获得Switch
Transformer提升部分30%的增益。</p>
<img src="/44e38c1b/switch_transformer_distill.png" class title="蒸馏">
<p>更进一步，用不同规模的稀疏模型下进行蒸馏，结果如下表，可以实现高达99%的压缩率</p>
<img src="/44e38c1b/switch_transformer_distill_diff_model.png" class title="蒸馏">
<p>除了预训练模型，微调模型也可以蒸馏，效果如下，在SuperGLUE也有一定的提升</p>
<img src="/44e38c1b/switch_transformer_distill_sft.png" class title="sft蒸馏">
<h1 id="glam">GLaM</h1>
<ol type="1">
<li>简介</li>
</ol>
<p>2021年12月Google发表了《GLaM: Efficient Scaling of Language Models
with
Mixture-of-Experts》，训练出最大参数量为1.2T，每层包含64个专家，每个token激活参数量为96.6B的MoE模型。</p>
<p>相比Switch Transformer，GLaM的训练数据量要大得多，达到了1.6T
token。</p>
<p>下表是论文中给出的，当时一些大规模模型的对比</p>
<img src="/44e38c1b/glam_related_model.png" class title="glam和相关模型">
<p>虽然模型总参数量比GPT-3（175B）大很多，但是训练成本却比GPT-3低很多，推理速度也更快，而且在多个NLP任务上的效果都超越了GPT-3，如下所示。</p>
<img src="/44e38c1b/glam_compare_gpt3.png" class title="glam和gpt3对比">
<img src="/44e38c1b/glam_compare_gpt3_2.png" class title="glam和gpt3对比">
<ol start="2" type="1">
<li>模型设计</li>
</ol>
<p>模型设计上，和Switch
Transformer一样，每两层把一个FFN替换成MoE层。但是和Switch
Transformer不同，GLaM用回了每次激活两个expert的方案，模型结构如下图。</p>
<img src="/44e38c1b/glam_model.png" class title="glam模型">
<p>除此之外，模型在结构上海做了一些其他改动：</p>
<p>（1）位置编码</p>
<p>使用XLNET的相对位置编码。</p>
<p>（2）激活函数</p>
<blockquote>
<p>In the non-MoE Transformer feed-forward sub-layers, we replace the
first linear projection and the activation function with the Gated
Linear Unit，which computes the component-wise product of two linear
transformation of the input, followed by a Gaussian Error Linear
Unit.</p>
</blockquote>
<ol start="3" type="1">
<li>实验</li>
</ol>
<p>训练中的一些trick：</p>
<p>（1）参考《Lingvo: a modular and scalable framework for
sequence-to-sequence
modeling》，在梯度出现NaN或者Inf的时候就跳过那一步更新。</p>
<p>（2）如果在BP更新的时候遇到NaN或者Inf，则重新加载更早的checkpoint并跳过有问题的数据来避免NaN或者Inf。</p>
<p>论文训了一系列模型来探索MoE，这些模型的设置如下表</p>
<img src="/44e38c1b/glam_family.png" class title="glam模型系列">
<p>GLaM和dense模型的评测结果如下</p>
<img src="/44e38c1b/glam_perf.png" class title="glam模型效果">
<p>可以看到GLaM MoE的有效参数效率一致高于dense模型。</p>
<h1 id="st-moe">ST-MoE</h1>
<p>2022年2月，Google发表了《ST-MOE: DESIGNING STABLE AND TRANSFERABLE
SPARSE EXPERT
MODELS》。ST-MoE可以说不仅仅是一个MoE工作，对于模型结构、工程实现、训练策略等都做了很多分析，可以说是MoE的必读论文。</p>
<p>ST-MoE最大模型包含269B总参数量，和与32B
dense模型相当的激活计算量。论文中把模型称为称为Stable Transferable
Mixture-of-Experts，或者ST-MoE-32B。</p>
<p>在MoE层的使用上，ST-MoE比Switch
Transformer更“节省”一点，每四层才替换1个MoE层。</p>
<p>论文中主要训了两个规模的ST-MoE模型，分别有4B和269B的总参数量。ST-MoE以及其他用于对比的模型参数如下表</p>
<img src="/44e38c1b/st_moe_models.png" class title="ST-MoE模型及对比模型的参数">
<h2 id="稳定性与效果分析">稳定性与效果分析</h2>
<p>论文通过对乘性操作、噪音和裁剪这几个内容进行探索，来指导模型的设计。</p>
<ol type="1">
<li>乘性操作对模型稳定性和效果的影响</li>
</ol>
<p>论文首先研究了乘性操作对模型的训练稳定性和最终效果的影响。</p>
<p>之前已经有一些工作表明更多的乘法对模型效果有收益。</p>
<blockquote>
<p>Some architectural improvements involve more multiplications than
additions or do not sum many items at once</p>
</blockquote>
<p>（1）GELU Gated Linear Units (GEGLU)</p>
<p>第一个例子是关于激活函数的。GLU是一个对两个输入向量进行component-wise相乘的操作，之后被扩展成GELU-Linear
FFN变体，用于替换transformer中的ReLU FFN变体，其计算如下</p>
<p><span class="math display">\[\begin{aligned}FFN_{GEGLU}(x,W,V,b,c)=GELU(xW+b)\odot(xV+c)\end{aligned}\]</span></p>
<p>这样在一些其他工作里已经被证明了对模型效果有提升。</p>
<p>（2）RMSNorm</p>
<p>第二个例子是RMSNorm中的缩放参数，也就是下面公式的 <span class="math inline">\(g\)</span>。</p>
<p><span class="math display">\[y_i=\frac{x_i}{\sqrt{\frac1d\sum_{i=1}^dx_i^2}}\cdot
g_i\]</span></p>
<p>ST-MoE针对GEGLU和RMSNorm这两个乘性操作，做了实验，结果如下表。</p>
<img src="/44e38c1b/st_moe_remove_multiplications.png" class title="移除乘法操作的影响">
<p>发现移除乘性操作可以使模型稳定性更好（训练中发散的情况减少），但是最终效果变差了。</p>
<p>（3）增加dense层</p>
<p>ST-MoE还验证了在expert层增加更多dense层的效果。结果发现增加更多的乘法交互（增加dense层），可以在带来效果收益的同时，基本不影响推理速度，如下表所示。</p>
<img src="/44e38c1b/st_moe_more_dense_layer.png" class title="更多的dense层">
<p>（4）增加一个bias</p>
<p>在FFN层的第一个矩阵乘法后面增加一个可学习的bias
B，分别通过加法和乘法加入</p>
<p><span class="math display">\[\text{FFN}_{\text{GEGLU}}+\text{Add
Bias}(x)=[(\text{GELU}(xW_{11})\odot xW_{12})+B]W_2\]</span></p>
<p><span class="math display">\[\mathrm{FFN}_{\mathrm{GEGLU}}+\mathrm{Mult~Bias}(x)=[(\mathrm{GELU}(xW_{11})\odot
xW_{12})\odot B]W_2\]</span></p>
<p>乘法的收敛速度更快，效果也更好。</p>
<p>上面这些实验显示，后续在模型效果的探索方向可以往多使用乘性操作去考虑。</p>
<ol start="2" type="1">
<li>noise对模型稳定性和效果的影响</li>
</ol>
<p>接下来ST-MoE探索了“噪音可以提升模型稳定性”的假设。</p>
<p>通过input-jitter，给router的输入logits乘以一个在[1e-2,
1e2]之间的均匀随机变量来添加噪音。</p>
<img src="/44e38c1b/st_moe_more_add_noise.png" class title="增加noise">
<p>结果是增加noise之后，有助于让模型的收敛更加稳定，但是对模型最终效果有负面影响。</p>
<p>这里论文还提到，小模型上的结果不一定能直接推广到更大的模型上，比如在小模型上稳定的配置，在大模型就可能就不稳定了。因此还是需要在大模型上也进行充分实验。</p>
<ol start="3" type="1">
<li>限制激活值和梯度值对模型稳定性和效果的影响</li>
</ol>
<p>对activation和gradient进行限制是目前广泛应用的提升模型训练稳定性的手段。在反向传播过程中，通过裁剪梯度的范数来缓解梯度爆炸，就是一种常用的限制手段。</p>
<p>但是在ST-MoE训练269B的大规模模型时，发现裁剪会使得模型收敛的效果很差。</p>
<p>为了解决这个问题，ST-MoE在训练中引入了router z-loss，形式如下。</p>
<p><span class="math display">\[L_z(x)=\frac{1}{B}\sum_{i=1}^B\left(\log\sum_{j=1}^Ne^{x_j^{(i)}}\right)^2\]</span></p>
<p><span class="math inline">\(B\)</span> 是token的数量，<span class="math inline">\(N\)</span> 是专家数，<span class="math inline">\(x\in\mathcal{R}^{B\times N}\)</span>
是router的输入。</p>
<p>z-loss会对进入router的较大的logits值进行惩罚，以达到尽量减少进入指数函数的较大误差的目的。什么意思呢？后面来解释，先看下使用z-loss的效果。</p>
<img src="/44e38c1b/st_moe_z_loss_result.png" class title="z-loss效果">
<p>ST-MoE认为，在模型训练过程中，由于精度不足或者其他问题，会产生很大的值，从而引入误差。而对梯度进行裁剪是在误差发生之后，并且裁剪本身也造成了数据的不连续性，某种程度上，裁剪本身也是一种误差。相反地，z-loss自然地鼓励模型产生较小的对数值，因此可以更精确地建模。</p>
<p>z-loss乘以一个权重超参 <span class="math inline">\(c_z\)</span>
加入到模型训练的总损失中，如下式所示。</p>
<p><span class="math display">\[L_{tot}=L_{CE}+c_BL_B+c_zL_Z\]</span></p>
<p>ST-MoE经过实验，选择了<span class="math inline">\(c_z=0.001\)</span>。</p>
<p><span class="math inline">\(L_B\)</span> 是 auxiliary load balance
loss负载均衡损失，ST-MoE这里使用了和GShard/Switch
Transformer所用的相同的损失计算，这里回顾一下：</p>
<p><span class="math display">\[\begin{aligned}\text{loss}&amp;=\alpha\cdot
N\cdot\sum_{i=1}^Nf_i\cdot P_i\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}f_i=\frac{1}{T}\sum_{x\in\mathcal{B}}\mathbb{1}\{\text{argmax
}p(x)=i\}\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}P_i=\frac{1}{T}\sum_{x\in\mathcal{B}}p_i(x).\end{aligned}\]</span></p>
<p><span class="math inline">\(N\)</span> 是专家数， <span class="math inline">\(\mathcal{B}\)</span>是包含 <span class="math inline">\(T\)</span> 个token的batch。<span class="math inline">\(f_{i}\)</span> 表示被分配到第 <span class="math inline">\(i\)</span> 个expert的token数，这个不可导；<span class="math inline">\(P_i\)</span> 表示整个batch每个token分配给第<span class="math inline">\(i\)</span> 个expert的概率的总和，这个可导。</p>
<ol start="4" type="1">
<li>数据精度对训练效率和训练效果的影响</li>
</ol>
<p>目前大部分的大模型训练都使用混合精度训练：模型权重以float32格式存储以进行梯度更新，然后在正向和反向传播的矩阵乘法中转换为bfloat16；此外，所有激活值都以bfloat16存储和操作，而allreduce通信可以在bfloat16或float32数值精度中进行。</p>
<p>对于ST-MoE-32B的训练，allreduce的数值使用半精度可以加速训练，然而这也会使训练变得不稳定，因此ST-MoE保持allreduce的数值精度为float32。</p>
<p>bfloat16和float32在不同范围的舍入误差如下表所示</p>
<img src="/44e38c1b/st_moe_round_error.png" class title="bf16精度损失">
<p>可以看到，表达的数值越大，舍入误差越大。而z-loss限制了数值大小，也就将误差值限制在比较小的范围。</p>
<p>MoE模型天生对舍入误差敏感，因为它们由于router的使用而有更多的指数函数，而指数函数会将小的输入误差放大很多，这就加剧舍入误差所导致的训练不稳定。</p>
<p>另外，ST-MoE有一个策略：只有当排第二的专家的权重大于等于第一的专家的1/5时，token才会被路由到其第二位专家，否则第二个专家就会被忽略。</p>
<p>因此虽然舍入误差不会改变softmax运算中各个概率的排序，但它确实会影响MoE中第二个专家的激活。</p>
<h2 id="模型设计-2">模型设计</h2>
<p>dense模型的设计有scaling
law进行指导，但是MoE模型的设计比dense模型多出几个要考虑的点：</p>
<p>（1）使用多少个expert</p>
<p>（2）怎么routing</p>
<p>（3）专家容量系数怎么定</p>
<p>（4）硬件的影响</p>
<p>（这里提到MoE模型的scaling law工作：《Unified scaling laws for routed
language models》，可以了解一下）</p>
<ol type="1">
<li>使用多少个expert</li>
</ol>
<p>ST-MoE认为，从以往的经验来看，在总专家数量较少的情况下（如8/16/32），提升专家数量，能有收益。但是在特别稀疏的情况下（如激活专家数量&lt;1%），或者总专家数较大（比如&gt;256）之后，提升专家数量收益就很小了。</p>
<p>从另一个角度来看，如果一个计算核心使用&gt;1个专家，那么就会出现比较大的加载参数张量的成本，因此建议每个计算核心使用&lt;=1个专家。</p>
<ol start="2" type="1">
<li>routing和capacity factor</li>
</ol>
<p>论文做了一系列实验来探索capacity factor的选择，如下表所示</p>
<img src="/44e38c1b/st_moe_capacity_factor.png" class title="capacity factor">
<p>从这些实验中得到几个结论：</p>
<p>（1）训练和推理的capacity factor增大都会有收益</p>
<p>（2）如果硬件资源足够，推理的capacity
facotr可以设得比训练的时候大，会有进一步提升</p>
<p>（3）激活的expert数量提升会有收益，但是收益随着capacity
factor提升而越来越小</p>
<p>当然，选择capacity
factor还要看硬件的特性，如果通讯很快，可以适当增大capacity
factor，否则就不能选择太大的。</p>
<p>下表展示了不同capacity factor对推理速度的影响</p>
<img src="/44e38c1b/st_moe_capacity_factor_speed.png" class title="不同capacity factor推理速度">
<h2 id="实验-2">实验</h2>
<ol type="1">
<li>ST-MoE效果</li>
</ol>
<p>ST-MoE-32B在下游任务上和以往最佳结果对比如下表，ST-MoE-32B刷新了超过一半任务的最佳效果</p>
<img src="/44e38c1b/st_moe_perf.png" class title="不同capacity ST-MoE-32B效果">
<ol start="2" type="1">
<li>Expert Specialization</li>
</ol>
<p>论文还对各个专家的专业化进行了追踪，发现decoder中几乎没有专业化的迹象，各种类型的token近乎随机分配给不同的专家。而在encoder中则表现出了高度专业化的特征，如下表</p>
<img src="/44e38c1b/st_moe_encoder_specialization.png" class title="encoder专业化">
<p>此外，还发现在多语言的模型的encoder中，专业化的情况并不想原先预想那样，按不同语言划分，而是每个专家都会处理一种语言的一部分token，如下表</p>
<img src="/44e38c1b/st_moe_multiling_specialization.png" class title="多语言专业化">
<h1 id="deepseekmoe">DeepseekMoE</h1>
<p>2024年1月，幻方量化开源了DeepseekMoE，是国内首个开源的MoE大模型。幻方还发布了论文《DeepSeekMoE:
Towards Ultimate Expert Specialization in Mixture-of-Experts Language
Models》，给出了一些DeepSeekMoE的细节内容，颇为实在了。</p>
<p>DeepSeekMoE在其他MoE工作的基础上，进一步给出了2个模型设计的主要思路：</p>
<p>（1）对expert的粒度进行细分，以提供更多样的expert激活组合；</p>
<p>（2）对expert的类型进行区分，从所有expert中保留一部分作为shared
expert共享专家，这部分专家对所有输入都保持激活。</p>
<p>这样的做法可以帮助每个expert达到更高程度的专业化(specialization)的水平，更好地学习不同的专业知识。</p>
<p>DeepSeekMoE先在2B的较小MoE模型上进行了充分的实验，然后把方案应用到16B参数的MoE模型上，并获得了较好的效果。其中DeepSeekMoE-16B不需要量化就可以在40GB显存的设备上运行。</p>
<p>DeepSeekMoE-2B模型具有和稠密2B模型相当的性能，而DeepSeekMoE-16B则具有和7B稠密模型相当的性能，且计算量仅为稠密模型的40%。</p>
<p>DeepSeekMoE-16B的参数效率相比稠密模型有明显的优势，如下图所示</p>
<img src="/44e38c1b/ds_moe_perf.png" class title="deepseek moe">
<p>并且DeepSeekMoE-2B和16B模型都开源了。</p>
<p>在前面实验的基础上，幻方还训练了DeepSeekMoE-145B的超大MoE模型，具有和稠密的DeepSeek-67B模型相当的表现，但计算量更小。这个后续也有机会放出来。</p>
<h2 id="模型设计-3">模型设计</h2>
<p>MoE，mixture of
expert，顾名思义，一个最初始的motivation就是让不同expert学习不同的内容，然后再混合起来。</p>
<p>比如最上面提到的1991年的工作里，就是让不同的expert学习不同的元音特征，以此提升特征提取的准确率。</p>
<p>但是当前大部分的MoE架构都会遇到“knowledge hybridity”和“knowledge
redundancy”的问题，即知识的杂糅和冗余：</p>
<p>（1）知识冗余</p>
<p>有些基础的常识在不同的领域都需要用到，每个expert就都会学一点，这样这些常识就被多个expert重复学习了。</p>
<p>（2）知识杂糅</p>
<p>在expert数量不够多的情况下，一个expert就可能要负责学习多个领域的内容。以学习高中知识为例，在只有两个expert的时候，只能一个expert学习理科知识，另一个学习文科知识；当我们有8个expert的时候，不同expert就可以分别学习语文、英语、历史、地理、物理、生物、化学、数学知识。显然后者所学知识的专业化程度更高。</p>
<p>知识的杂糅和冗余阻碍了专家专业化(expert
specialization)的程度，也就阻碍了模型达到MoE结构理论上限性能。</p>
<p>我们期望每个expert能够学习到non-overlap &amp; foucusd
knowledge的知识。</p>
<p>针对上面的问题，DeepSeekMoE的架构设计有2个主要策略：</p>
<p>（1）Fine-Grained Expert Segmentation</p>
<p>参数总量不变的情况下，将expert分成更细的粒度（每个expert更小）。这样可以带来更灵活的激活组合，让每个expert可以有更强的specialization。比如原本是16个expert选择激活2个，那么总的组合数是120种；如果把每个expert缩小为原来的1/4，那在总参数量和激活数量不变的情况下，是64个expert选择激活8个，那么总的排列组合数就是
<span class="math inline">\(\binom{64}8=4,426,165,368\)</span>
，排列组合数比原来多了很多。</p>
<p>（2）Shared Expert Isolation</p>
<p>把部分expert分离出来，保持永远激活。我们期望这部分专家能够学到在多个领域间都通用的common
knowledge。这样的策略同样可以使得其他expert能够提高专业化的程度，并且减少不同expert间的知识冗余。还是以学习高中知识为例，数学、物理和化学都需要算术能力，如果让学这三个领域的expert都学习算术技能，就会有冗余；我们可以把通用算术的技能剥离出来，由一个助手专门负责算术任务，相当于给他们发了一个计算器，这样学习数学、物理和化学的expert就能把更多的精力放在专业知识上，也就能达到更好的专业化效果。</p>
<p>下图展示了在传统MoE结构上增加Fine-Grained Expert Segmentation和Shared
Expert Isolation策略的设计</p>
<img src="/44e38c1b/ds_moe_structure.png" class title="deepseek moe 结构">
<p>（expert isolation的思路最早可以追溯到2022年1月发表的《DeepSpeed-MoE:
Advancing Mixture-of-Experts Inference and Training to Power
Next-Generation AI Scale》，这里就不展开了。）</p>
<p>假设传统的MoE模型每层的expert数量为 <span class="math inline">\(N\)</span>，激活expert数为 <span class="math inline">\(K\)</span>，DeepSeekMoE使用的细粒度expert大小为原来的
<span class="math inline">\(1/m\)</span>，那DeepSeekMoE每层就有 <span class="math inline">\(mN\)</span> 个expert，激活的expert数量为 <span class="math inline">\(mK\)</span>个。假设 <span class="math inline">\(T\)</span> 为输入长度，<span class="math inline">\(L\)</span> 为模型层数，<span class="math inline">\(e_i^l\)</span> 表示第 <span class="math inline">\(i\)</span>
个expert，DeepSeekMoE可以公式化为以下表示（忽略了layernorm）</p>
<p><span class="math display">\[\mathbf{u}_{1:T}^l=\text{Self-Att}\left(\mathbf{h}_{1:T}^{l-1}\right)+\mathbf{h}_{1:T}^{l-1}\]</span></p>
<p><span class="math display">\[\mathbf{h}_t^l=\sum_{i=1}^{mN}\left(g_{i,t}\text{
FFN}_i\left(\mathbf{u}_t^l\right)\right)+\mathbf{u}_t^l\]</span></p>
<p><span class="math display">\[g_{i,t}=\begin{cases}s_{i,t},&amp;s_{i,t}\in\text{Topk}(\{s_{j,t}|1\leqslant
j\leqslant mN\},mK)\\0,&amp;\text{otherwise,}\end{cases}\]</span></p>
<p><span class="math display">\[s_{i,t}=\mathrm{Softmax}_i\left({\mathbf{u}_t^l}^T\mathbf{e}_i^l\right)\]</span></p>
<h2 id="负载均衡-2">负载均衡</h2>
<p>如之前工作反复提及的，如果任由MoE模型自主学习gating，可能会遇到两个问题</p>
<p>（1）routing
collapse：专家分配的不均衡，也就是gating倾向于总是选择特定的少量expert，并且这种情况还会自我增强。</p>
<p>（2）计算效率问题：多设备间，不平衡的负载可能会成为计算效率的瓶颈。</p>
<p>针对routing collapse的问题，DeepSeekMoE引入一个expert-level balance
loss，如下所示</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{L}_{\mathrm{ExpBal}}&amp; =\alpha_1\sum_{i=1}^{N&#39;}f_iP_i
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
f_{i}&amp;
=\frac{N^{\prime}}{K^{\prime}T}\sum_{t=1}^T\mathbb{1}(\text{Token
}t\text{ selects Expert }i)
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
P_{i}&amp; =\frac1T\sum_{t=1}^Ts_{i,t}
\end{aligned}\]</span></p>
<p><span class="math inline">\(\alpha_1\)</span> 叫做expert-level
balance factor，是人工设定的超参。</p>
<p>而 <span class="math inline">\(f_i\)</span> 和 <span class="math inline">\(P_i\)</span> 和Switch
Transformer里的设定基本一样。</p>
<p>在Switch Transformer里， <span class="math inline">\(f_i\)</span>
表示分配到第 <span class="math inline">\(i\)</span>
个expert的token数量。在DeepSeekMoE这里也是一样的含义，只是多乘了一个系数
<span class="math inline">\(N&#39;/K&#39;\)</span> ，其中 <span class="math inline">\(N&#39;=mN-K_s\)</span>，<span class="math inline">\(K&#39;=mK-K_s\)</span>，<span class="math inline">\(K_s\)</span>
是划分出来的共享expert的数量。这个系数是个常数，可以拿到求和符号外面，这样DeepSeekMoE里的
<span class="math inline">\(f_i\)</span> 就和Switch
Transformer里的完全一样了。</p>
<p><span class="math inline">\(N&#39;/K&#39;\)</span>
这个系数可以使得在使用不同的数量的expert时，在完美平均分配的情况下，负载均衡loss都是相同的常数。</p>
<p><span class="math inline">\(P_i\)</span> 表示所有每个token分配给第
<span class="math inline">\(i\)</span> 个expert的权重的总和，和Switch
Transformer里的含义一样。</p>
<p>注意这里 <span class="math inline">\(f_i\)</span> 是不可导的，<span class="math inline">\(P_i\)</span> 是可导的。</p>
<p>针对多设备间负载均衡的问题，DeepSeekMoE引入一个device-level balance
loss，如下所示</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{L}_{\mathrm{DevBal}}&amp; =\alpha_2\sum_{i=1}^Df_i&#39;P_i&#39;
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
f_i^{\prime}&amp; =\frac1{|\mathcal{E}_i|}\sum_{j\in\mathcal{E}_i}f_j
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
P_{i}^{\prime}&amp; =\sum_{j\in\mathcal{E}_i}P_j
\end{aligned}\]</span></p>
<p><span class="math inline">\(\alpha_2\)</span> 叫做device-level
balance factor，是人工设定的超参。</p>
<p><span class="math inline">\(\mathcal{E}_i\)</span> 指第 <span class="math inline">\(i\)</span> 个设备。</p>
<p>device-level balance loss形式上和expert-level balance loss一样，只是
<span class="math inline">\(f_i\)</span> 和 <span class="math inline">\(P_i\)</span>
对应的对象从单个expert变成单个设备了。</p>
<p>当我们的目标是缓解计算瓶颈时，我们不需要强制执行expert间的均匀分配，而只需确保设备之间计算量的平衡。比如我们每层有64个expert，均匀分布在8个设备上，我们只需要每个设备处理的token数平衡即可，在设备内部即使所有token都是同一个expert处理的，依然能满足设备间负载平衡的要求。</p>
<p>相比expert间严格的负载平衡，只要求设备间的平衡是更松的限制条件，这样缓解了因为过度的负载平衡而损害模型性能的问题。</p>
<h2 id="实验-3">实验</h2>
<ol type="1">
<li>小规模模型验证</li>
</ol>
<p>为了验证以上策略的有效性，先拿100B
token的语料数据在DeepSeekMoE-2B模型做实验。词表也是通过BPE在语料上训练的8k词表，后面训练更大规模模型的时候再扩大词表。</p>
<p>DeepSeekMoE-2B模型参数初始化方差为0.006，使用multi-head
attention，前向激活参数量约0.3B，具体参数如下表</p>
<img src="/44e38c1b/ds_model_param.png" class title="模型超参">
<p>relative expert
size指的是DeepSeekMoE所用的细粒度expert的大小和正常FFN层大小的比值。</p>
<p>训练的具体参数设置如下</p>
<center>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">属性</th>
<th style="text-align: center;">数值</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">optimizer</td>
<td style="text-align: center;">AdamW</td>
</tr>
<tr class="even">
<td style="text-align: center;">adam_beta_1</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr class="odd">
<td style="text-align: center;">adam_beta_2</td>
<td style="text-align: center;">0.95</td>
</tr>
<tr class="even">
<td style="text-align: center;">adam_weight_decay</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">warmup schedule</td>
<td style="text-align: center;">linear</td>
</tr>
<tr class="even">
<td style="text-align: center;">warmup step</td>
<td style="text-align: center;">2000</td>
</tr>
<tr class="odd">
<td style="text-align: center;">max lr</td>
<td style="text-align: center;">1.08e-3</td>
</tr>
<tr class="even">
<td style="text-align: center;">dropout</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">sequence length</td>
<td style="text-align: center;">2k</td>
</tr>
<tr class="even">
<td style="text-align: center;">batch size</td>
<td style="text-align: center;">2k</td>
</tr>
<tr class="odd">
<td style="text-align: center;">total step</td>
<td style="text-align: center;">25,000</td>
</tr>
</tbody>
</table>
</center>
<p>其他训练细节：<br>
- 所有expert放在单个GPU上，没有使用device-level balance loss<br>
- expert-level balance factor设为0.01<br>
- 训练到80%的时候，学习率乘以0.316，训练到90%的时候，再乘以0.316</p>
<p>使用相同的100B训练数据，训了DeepSeekMoE-2B，在包含语言模型和下游任务的benchmark上和其他4个模型做对比：dense，hash
layer（也是一种moe，《Hash layers for large sparse models》），Switch
Transformer，GShard。效果对比如下</p>
<img src="/44e38c1b/ds_moe_comparison.png" class title="deepseek moe 效果">
<p>可以得到几个结论：<br>
- 更大的模型参数量和稀疏的架构，使得Hash Layer和Switch
Transformer和具有同样激活参数的dense模型相比，有明显的优势。<br>
- 同样的模型参数下，GSshard比Hash Layer和Switch
Transformer有更多激活参数，效果也更好<br>
- 同样的模型参数和激活参数下，DeepSeekMoE效果比GShard有明显优势。</p>
<p>为了进一步探索DeepSeekMoE架构带来的收益，提升了dense模型和GShard模型的激活参数，直到效果和DeepSeekMoE-2B差不多。</p>
<p>结果dense模型和GShard模型需要分别扩大到16倍和1.5倍的参数量，才能达到DeepSeekMoE-2B相近的效果，如下表所示</p>
<img src="/44e38c1b/ds_moe_upper_bound_2b.png" class title="deepseek moe upper bound">
<p>DeepSeekMoE的优势在更大规模的情况下，依然成立。训了DeepSeekMoE-13B,
对比参数量提升至1.2和1.5倍的GShard，DeepSeekMoE-13B依然能match，具体如下表</p>
<img src="/44e38c1b/ds_moe_upper_bound_13b.png" class title="deepseek moe upper bound">
<ol start="2" type="1">
<li>DeepSeekMoE架构消融实验</li>
</ol>
<p>针对DeepSeekMoE架构的两个主要设计，shared expert和fine-grained
expert进行消融实验。使用不同数量的共享专家和不同粒度的expert进行效果对比，结果如下图。</p>
<img src="/44e38c1b/ds_moe_ablation.png" class title="deepseek moe upper bound 消融实验">
<p>（1）对比蓝色和橙色，可以看到增加共享专家带来了收益</p>
<p>（2）绿色和红色在橙色的基础上进一步把专家颗粒分得更细，效果进一步提升</p>
<p>（3）共享专家和路由专家的比例：在总共64个expert的情况下，对比了1/2/4个共享专家的情况，结果并没有显著差别，在pile上的loss分别是1.808,1.806,1.811。最终选择了共享专家和激活路由专家1:3（2+6）的比例。</p>
<ol start="3" type="1">
<li>expert specialization的分析</li>
</ol>
<p>通过实验来验证DeepSeekMoE中expert specialization的优化。</p>
<p>（1）前面实验看到DeepSeekMoE-2B和1.5倍参数量的GShard模型效果相当。在这个基础上，通过禁用不同数量的top专家，而只能从次优的专家中选择进行回答。</p>
<p>实验结果如下</p>
<img src="/44e38c1b/ds_moe_expert_specialization.png" class title="专家专门化">
<p>发现DeepSeekMoE损失更大，说明DeepSeekMoE每个专家的专业化程度更好，必要性更高。</p>
<p>（2）另外，通过禁用DeepSeekMoE的共享专家，而额外激活一个专家，发现loss也大大提升。这个结果突出了共享专家的关键功能，并表明共享专家捕捉到了与路由专家不共享的基本且重要的知识，使得它无法被路由专家替代。</p>
<p>（3）只激活更少专家，也能和GShard达到相同水平，这一观察结果支持了DeepSeekMoE可以更准确和高效地获取所需知识的观点。</p>
<img src="/44e38c1b/ds_moe_less_activated_expert.png" class title="激活更少专家">
<p>此外还从零训了一个只用1个共享专家和3个激活专家的2b模型（正常是2个共享专家+6个激活专家），也比GShard好，说明DeepSeekMoE的有效参数效率更高</p>
<img src="/44e38c1b/ds_2b_less_expert.png" class title="2B激活更少专家">
<ol type="1">
<li>DeepSeekMoE-16B</li>
</ol>
<p>DeepSeekMoE-16B模型使用了2T数据训练（和LLAMA2-7B对齐）训练，并使用了100k的词表。其他参数如下表所示</p>
<img src="/44e38c1b/ds_model_param.png" class title="模型超参">
<p>论文中提到，除了第一层以外，其他层都使用了MoE层。</p>
<p>第一层不使用MoE是因为观察到第一层的负载均衡loss在训练中收敛得特别慢。</p>
<p>DeepSeekMoE-16B每层有64个专家，其中有2个作为共享专家保持永远激活，加上6个通过gating
function选择激活的，每个token共使用8个专家。每个token会激活16.4B中的2.8B参数。</p>
<p>这里没有把专家的dimension再减小，是因为如果专家太小，计算效率就下降得太厉害。</p>
<p>训练中使用的其他设置：<br>
- lr = 4.2e-4<br>
- 训练进行到80%和90%的时候，lr都会缩小到0.316倍<br>
- batch size = 4.5k，训练窗口长度是4k，因此每个batch有18M
token，2T数据差不多是10.6w步<br>
- 使用了pipeline parallelism</p>
<p>expert level balance
loss的系数设得比较小，0.001，因为实验中发现设得再大并不能进一步优化负载平衡，反而会损害模型效果。</p>
<p>DeepSeekMoE-16B和DeepSeek-7B模型的对比如下</p>
<img src="/44e38c1b/ds_16b_perf_1.png" class title="和DeepSeek-7B对比">
<p>DeepSeekMoE-16B和LLAMA2-7B模型的对比如下</p>
<img src="/44e38c1b/ds_16b_perf_2.png" class title="和LLAMA2-7B对比">
<ol start="5" type="1">
<li>DeepSeekMoE-145B</li>
</ol>
<p>幻方还用245B的token训练了DeepSeekMoE-145B，模型效果上达到DeepSeek-67B的同等水平</p>
<img src="/44e38c1b/ds_moe_145b.png" class title="145b">
<h1 id="dbrx">DBRX</h1>
<p>2024年3月27日，Databricks开源了DBRX，一个拥有有132B参数，激活参数为36B的MoE模型。</p>
<p>结构上，DBRX使用了RoPE、GLU、GQA，采用了fine-grained
expert的设计，每层有16个专家，每个token激活其中4个。相比Mixtral和Grok-1在8个专家中激活2个，DBRX有更多的专家组合方式。</p>
<p>DBRX训练的上下文长度为32k，并使用了12T文本和代码token进行训练。DBRX在3072个H100上完成预训练，加上post-training、效果评估、red-team优化，整个过程耗费3个月时间。</p>
<p>DBRX整体效果超过GPT-3.5，与Gemini 1.0
Pro相当，并且具有比较强的代码能力，甚至超过了在代码上专门优化过的模型，如CodeLLaMA-70B，如下图所示。</p>
<img src="/44e38c1b/dbrx_perf.png" class title="DBRX效果">
<p>推理效率效率上，DBRX也领先于其他模型。</p>
<img src="/44e38c1b/dbrx_infer_efficiency.png" class title="推理效率">
<h1 id="qwen1.5-moe">Qwen1.5-MoE</h1>
<p>2024年3月28日，阿里放出了Qwen1.5-MoE-A2.7B，以2.7B的模型参数，达到了Qwen1.5-7B模型的相近效果。</p>
<p>Qwen1.5-MoE-A2.7B参考了DeepSeekMoE和DBRX的工作，采用了fine-grained
expert的做法，总共有64个专家，每个token激活8个专家，其中有4个为共享专家。</p>
<p>Qwen1.5-MoE-A2.7B使用Qwen-1.8B进行初始化，并在初始化阶段引入随机性，这样可以显著加快收敛速度，并得到更好的收敛结果。</p>
<p>Qwen1.5-MoE-A2.7B和其他模型效果对比如下</p>
<img src="/44e38c1b/qwen1.5_moe_perf.png" class title="Qwen1.5-MoE-A2.7B效果">
<p>虽然Qwen1.5-MoE-A2.7B总参数量较大，但激活的non-embedding参数量远小于7B模型，如下表所示</p>
<img src="/44e38c1b/qwen1.5_moe_params.png" class title="Qwen1.5-MoE-A2.7B参数量">
<p>实践中，Qwen1.5-MoE-A2.7B相比于Qwen1.5-7B，训练成本降低了75%。</p>
<p>推理性能上，在A100-80G用vLLM部署Qwen1.5-7B和Qwen1.5-MoE-A2.7B模型进行了性能测试。</p>
<p>输入/输出token数都设置为1000，输出token数设置为1000，TPS和throughput如下</p>
<img src="/44e38c1b/qwen1.5_moe_tps.png" class title="Qwen1.5-MoE-A2.7B TPS">
<p>虽然MoE模型对内存需求更大，但是由于稀疏激活以及共享专家的设计，但是在速度和吞吐量上都比dense模型更好。Qwen1.5-MoE-A2.7B与Qwen1.5-7B相比，速度提高了约1.74倍。</p>
<h1 id="mistral">Mistral</h1>
<h2 id="mistral-8x7b">Mistral 8x7B</h2>
<p>2023年12月11日，Mistral
AI开源Mistral-8x7B，每个token激活8个专家中的2个。</p>
<p>Mistral-8x7B支持32k推理窗口和多语言，并且代码能力较好。和LLAM2-70B以及GPT-3.5的对比如下。</p>
<img src="/44e38c1b/mistral_8_7b_perf.png" class title="Mistral 8x7B效果">
<p>Mistral-8x7B在大多数任务表现优于LLAM2-70B，且推理速度提高了6倍。</p>
<p>而和激活参数量相近的LLAM2-13B比，优势更为明显</p>
<img src="/44e38c1b/mistral_8_7b_active_perf.png" class title="Mistral 8x7B同样激活参数量下效果">
<h2 id="mistral-8x22b">Mistral 8x22B</h2>
<p>2024年4月17日，Mistral
AI开源Mistral-8x22B模型，一个总参数为141B，激活参数为39B的超大MoE模型。</p>
<p>Mistral-8x22B支持多语言，并且具有较强的数学和代码能力。此外，推理窗口长度也从Mistral-8x7B的32k增加到64k。Mistral-8x22B还具备function
call的能力。</p>
<p>在各个维度的评测结果如下</p>
<img src="/44e38c1b/mistral_8_22b_reasoning.png" class title="Mistral 8x22B reasoning效果">
<img src="/44e38c1b/mistral_8_22b_multiling.png" class title="Mistral 8x22B 多语言效果">
<img src="/44e38c1b/mistral_8_22b_code.png" class title="Mistral 8x22B 代码与数学效果">
<h1 id="小结">小结</h1>
<ul>
<li>现有的工作都表明，MoE模型相比dense模型具有更高的参数效率，即同样的计算量下，MoE模型普遍能有更优的效果<br>
</li>
<li>因此MoE不仅能支持更大规模模型的训练，在较小规模模型上使用MoE架构也有很大收益<br>
</li>
<li>但是相比dense模型，MoE模型的训练也需要考虑更多内容，包括专家数量、激活数量和专家容量的设计，负载均衡的问题，如何在多设备上的并行等，训练难度更大<br>
</li>
<li>结构上，共享专家和细粒度专家目前被验证效果较好<br>
</li>
<li>负载均衡上，GShard和Switch Transformer的负载均衡损失被广泛采用<br>
</li>
<li>推理时需要对底层框架进行优化以适配MoE机制，否则难以发挥MoE的性能优势</li>
</ul>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】Adaptive Mixtures of Local Experts
https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf<br>
【2】Outrageously Large Neural Networks: The Sparsely-Gated
Mixture-of-Experts Layer https://arxiv.org/abs/1701.06538<br>
【3】GShard: Scaling Giant Models with Conditional Computation and
Automatic Sharding https://arxiv.org/abs/2006.16668<br>
【4】Switch Transformers: Scaling to Trillion Parameter Models with
Simple and Efficient Sparsity https://arxiv.org/abs/2101.03961<br>
【5】GLaM: Efficient Scaling of Language Models with Mixture-of-Experts
https://arxiv.org/abs/2112.06905<br>
【6】ST-MoE: Designing Stable and Transferable Sparse Expert Models
https://arxiv.org/abs/2202.08906<br>
【7】DeepSeekMoE: Towards Ultimate Expert Specialization in
Mixture-of-Experts Language Models
https://arxiv.org/abs/2401.06066<br>
【8】Introducing DBRX: A New State-of-the-Art Open LLM
https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm<br>
【9】Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated
Parameters https://qwenlm.github.io/zh/blog/qwen-moe/</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>MoE</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型算法题(2)</title>
    <url>/ad0bba9d.html</url>
    <content><![CDATA[<p><img src="/images/cover.png"></p>
<p>往期文章</p>
<p><a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a></p>
<p><a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a></p>
<p><a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a></p>
<p><a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a></p>
<p><a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a></p>
<p><a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a></p>
<hr>
<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<p>本系列将整理一些LLM中关键的、细节的、值得关注的内容，持续更新<sub>如有错误，欢迎指正</sub></p>
<hr>
<h1 id="在bert中词向量token-embedding和绝对位置编码position-encoding为什么可以直接相加">1、在Bert中，词向量token
embedding和(绝对)位置编码position encoding为什么可以直接相加？</h1>
<p>1、两个向量相加，理论上其效果等价于维度拼接concat+线性变换，而相加的操作在实现上更为方便。</p>
<p>2、高维空间中(如768维)，两个随机向量近似为正交关系。模型在高维度有能力区分出所有组合的情况。假设共有2万个词向量，500个位置，则模型需要在768维空间区分1000万个点，即使768维每个维度只能取1和-1也具备足够的区分能力。</p>
<p>3、词向量和位置编码可以认为都是一个one-hot向量经过一层线性变换层得到的。两个向量相加等价于把它们的one-hot编码拼接后进行线性变换。</p>
<p>4、没有使用相乘则是出于工程考虑。相加相比相乘结果更为稳定，方便训练。</p>
<h1 id="lora和全参数训练在计算量和显存上相比如何为什么lora能提升大模型训练效率">2、LoRA和全参数训练在计算量和显存上相比如何？为什么LoRA能提升大模型训练效率？</h1>
<p>1、计算量上：LoRA训练时，在主干模型的（部分）全连接层增加了LoRA旁路，前向和后向的计算量都在主干模型的基础上，增加了旁路部分的计算，因此相比全参数训练，略有增加。</p>
<p>2、显存上：训练时，显存主要有①模型参数②梯度③中间激活值④优化器参数四个部分。模型参数/梯度/激活值相比全参数训练也略微增加；而优化器则不需要再存储原模型参数的部分，只需要存储LoRA旁路部分，这部分节省较多显存。</p>
<p>3、使用LoRA能提升训练效率主要是因为（1）优化器部分的显存需要减少了，可以增大batch（2）优化器参数减少了，分布式训练中多卡之间的通信量减少了（3）（optional）主干模型由于不用更新，可以进一步量化到int8/int4等。</p>
<h1 id="为什么模型需要normalizationbatchnormlayernorm等">3、为什么模型需要normalization（batchnorm/layernorm等）？</h1>
<p>1、输入数据包含多个特征，特征之间有不同的量纲和范围（如身高180和年龄18岁），通过normalization进行归一化再经过模型进行线性/非线性组合，能够防止部分特征占据主导，部分特征被忽略。</p>
<p>2、batchnorm论文认为：模型一般有多层，前一层的输出是后一层的输入，而训练中前一层的参数更新会导致后一层的输入数据分布变化导致ICS（internal
covariate
shift），这样后面的层就不得不频繁剧烈更新适应分布变化，导致分布偏移进入激活函数饱和区而出现梯度消失，另外分布变化也是对i.i.d.条件的破坏。使用normalization可以保持分布的稳定，减小方差，使模型训练可以正常进行。</p>
<p>3.《How Does Batch Normalization Help
Optimization?》设计了实验测量使用batchnorm前后的ICS，发现batchnorm实际上并没有缓解ICS，甚至有所增加。而batchnorm能优化模型训练的原因更多是使得损失函数平面更加光滑，而便于梯度下降收敛。</p>
<h1 id="transformer中pre-norm和post-norm各有什么优缺点">4、Transformer中pre-norm和post-norm各有什么优缺点?</h1>
<p>1.原始的Transformer用的是post-norm，它在残差之后进行归一化（add &amp;
norm），对参数正则化的效果更强，模型更为鲁棒；post-norm对每个通路都进行了归一化，使得梯度在回传的时候容易出现消失。</p>
<p>2.Pre-norm相对于post-norm，残差部分存在不经过归一化的通路，因此能够缓解梯度消失，能够训练更深的网络。但是模型的等效“深度”受到影响，L+1层网络近似于一个L层的宽网络。</p>
<p>3.也就是说，在层数较少，post-norm和pre-norm都能正常收敛的情况下，post-norm的效果更好一些；但是pre-norm更适合用于训练更深的网络。</p>
<h1 id="对于使用multi-head-attention的模型假设hidden-sized注意力头数量为h每个头维度为d假设有ddh输入上下文长度为sbatch-size1计算self-attention模块各个部分的计算量float-operations">5、对于使用Multi-Head
Attention的模型，假设hidden
size=D，注意力头数量为h，每个头维度为d（假设有D=d×h），输入上下文长度为s，batch
size=1，计算self-attention模块各个部分的计算量（Float
Operations）。</h1>
<p>1.QKV线性变换：6 × s ×
D^2（矩阵乘法，每个位置有加法和乘法两个运算，因此每个位置需要2D次计算）</p>
<p>2.QK内积：h × 2 × d × s^2（h组矩阵分别计算）</p>
<p>3.scaling：h × s^2</p>
<p>4.softmax：h × 3 ×
s^2（softmax是按列进行的，每列要计算s个exp，s个exp结果的求和，以及s次除法）</p>
<p>5.reduction（权重矩阵乘以V）：h × 2 × d × s^2</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>往期文章</p>
<p><a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a></p>
<p><a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a></p>
<p><a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a></p>
<p><a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a></p>
<p><a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a></p>
<p><a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a></p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>算法题</tag>
      </tags>
  </entry>
  <entry>
    <title>MiniCPM</title>
    <url>/376db710.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>MiniCPM是面壁智能和清华开源的模型，MiniCPM开源系列包括非embedding参数为1.2B和2.4B两个规模的模型，以及对应的MiniCPM-DPO，MiniCPM-MoE和MiniCPM-128K模型。</p>
<p>简单梳理一下MiniCPM提到的一些内容。</p>
<h1 id="背景">背景</h1>
<p>大模型的训练成本很高，而且很多机制还没搞清楚，训出来的大规模模型在很多设备上也跑不起来，因此现在有不少机构对小一点的模型，即SLM，进行更全面的探索，比如Phi系列、TinyLlama、MobileLLM和Gemma等。</p>
<p>MiniCPM也是对SLM的一次探索，从中得到的经验也可以推广到更大的模型上。</p>
<h1 id="风洞实验">风洞实验</h1>
<p>为了找到好的模型参数和训练参数，MiniCPM做了很多“风洞实验”（Model Wind
Tunnel Experiments）。</p>
<p>这些风洞实验主要包括三个部分：（1）搜索模型结构的超参（2）探索batch
size的scaling（3）寻找最佳的learning rate。</p>
<p>后续风洞实验所用的模型具体参数如下</p>
<img src="/376db710/exp_model.png" class title="模型结构">
<h2 id="模型超参">模型超参</h2>
<p>预训练资源消耗很大，即使是SLM也不可能把所有参数的排列组合都搜索一遍。</p>
<p>这里参考Tensor Program的做法（《Tensor programs v: Tuning large
neural networks via zero-shot hyperparameter transfer》和《Tensor
programs vi: Feature learning in infinite-depth neural
networks》），分别对模型的宽度和深度进行了搜索。</p>
<p>搜索所用的操作如下表所示，这里没有应用attention
softmax的scaling技术。</p>
<img src="/376db710/param_search_2.png" class title="超参搜索">
<p>关于超参搜索的一些细节：<br>
- 用Maximal Update Parametrization的方法进行了调参。<br>
-
在一系列预定义的参数空间上进行贝叶斯搜索，所用模型参数为在N=0.009B。发现使用规模为10N和20N大小的数据集进行超参数优化时，超参的有效性表现出了一致性。因此调参的过程就使用D=10N=0.09B个token来进行实验了。<br>
- 应用了QK-Norm（《Querykey normalization for
transformers》）和independent weight decay（《Decoupled weight decay
regularization》）之后，发现模型对learning
rate的敏感性显著降低。不过在找到最佳learning
rate之后，后面的训练就不用再调整参数了，因此后面的实验就没有继续使用QK-Norm和independent
weight decay。</p>
<p>最终从下图展示的参数搜索，确定了最佳的hyper-parameters为：<br>
- scale depth = 1.4<br>
- scale emb = 12<br>
- init std = 0.1<br>
- lr = 0.01</p>
<img src="/376db710/param_search.png" class title="超参搜索">
<h2 id="optimal-batch-size">Optimal Batch Size</h2>
<p>batch size决定了模型收敛速度与计算资源消耗之间的平衡。</p>
<p>如果batch
size太大，会导致很大的数据和计算成本（才能跑到足够的update次数，让模型收敛）。如果batch
size太小，将会有大量的update step，并且相比大一些的batch
size，loss的减小很有限，训练效率太低。</p>
<p>这里参考OpenAI的《Scaling laws for neural language
models》的方法来寻找最佳的batch size，并做了一些改动。</p>
<p>《Scaling laws for neural language models》研究的是loss
function和token数之间的关系。他们假设了更多的step=更多的训练时间。在这个假设下，OpenAI定义了一个critical
batch
size，在不消耗过多的step或者token的情况下，能达到一定的loss水平。</p>
<p>这在无限GPU资源下是合理的。由于GPU资源是无限的，增加batch
size不会增加单个step的耗时，但会减少总step数，因而提高了效率。但是实际上我们并没有无限GPU资源，因此将batch
size增大相当于增加的每个step的时间，所以实际上通过增加batch
size来减小step数，对总训练时间的影响并不大。</p>
<p>因此MiniCPM放弃了“not consuming too many
steps”的目标，转而追求“minimizing the token quantity to achieve the
lowest loss”。</p>
<p>关于optimal batch
size与loss之间关系的估计，类似于“先有鸡还是先有蛋”的悖论，因为暂时没法完全搞清楚这二者之间的决定关系。目前的做法是，对于给定的模型大小，通常会有一个初步估计的achievable
loss，这是由先前的初步实验得出的经验估计。</p>
<p>而optimal batch size和optimal learning
rate很可能并不是独立的。为了克服这种相关性，MiniCPM首先对learning
rate进行了初步研究，然后选择一个最优learning rate来进行batch
size实验，并使用batch size缩放再次进行learning
rate调整。这有点像Coordinate Descent optimization method。</p>
<p>细节上，MiniCPM分别对0.009B、0.03B和0.17B的模型进行了实验。每个模型大小都在6种不同的batch
size上进行训练，使用了global learning rate=0.01和cosine learning rate
scheduler。在C4数据集上，optimal batch size与loss的趋势如下图红线</p>
<img src="/376db710/batch_size.png" class title="超参搜索">
<p>这三条红线在log空间中很好连成一条直线，如下图。</p>
<img src="/376db710/batch_size_2.png" class title="超参搜索">
<p>这里就得到了在C4数据集上的训练loss和optimal batch size的关系。</p>
<p><span class="math display">\[bs=\frac{1.21\times10^9}{L^{6.24}}\]</span></p>
<h2 id="optimal-learning-rate">Optimal Learning Rate</h2>
<p>由于使用了Tensor Program，optimal learning
rate在模型缩放的过程中应该不会有明显变化。为了验证这一点，MiniCPM在0.04B、0.1B、0.3B和0.5B的模型上进行了六组learning
rate的实验。</p>
<p>在下图中，可以发现尽管模型大小增加了十倍，但optimal learning
rate并没有明显的偏移，基本上一致保持在0.01左右。</p>
<img src="/376db710/learning_rate.png" class title="超参搜索">
<p>MiniCPM进一步在2.1B规模的模型上进行了一个简单的验证，最终确认了0.01的learning
rate确实实现了最低loss。</p>
<h1 id="wsd">WSD</h1>
<h2 id="cosine-learning-rate-scheduler的分析">cosine learning rate
scheduler的分析</h2>
<p>cosine scheduler的周期很重要，一般是设置降到最小learning
rate的时间T和预训练的总step数S持平。为了验证这个设置的效果，用0.036B的模型的做了实验，按以下公式分别实现cosine和cosine
loop两种scheduler。</p>
<img src="/376db710/cos_lr.png" class title="LR">
<p>loss的变化如下图</p>
<img src="/376db710/cos_loss.png" class title="LR">
<p>可以看到确实总是T=S时效果最好。分析原因可能是：<br>
- 与T&lt;S的scheduler相比，T=S的scheduler有更长的高learning
rate持续时间。而这种高learning rate可能有助于模型找到更好的global
optimum。<br>
- 与T&gt;S的scheduler相比，T=S的scheduler有更彻底的learning rate
decay。这种衰减可能涉及到training dynamics，使模型能够找到更好的 local
optimum。</p>
<h2 id="warmup-stable-decay">Warmup-Stable-Decay</h2>
<p>基于上面的分析，MiniCPM把训练过程显式分成high learning rate
stage和learning decay stage，这个scheduler就叫Warmup-Stable-Decay
scheduler，公式如下</p>
<p><span class="math display">\[\left.WSD(T;s)=\begin{cases}&amp;\frac{s}{W}\eta,\quad
s&lt;W\\&amp;\eta,\quad W&lt;s&lt;T\\&amp;f(s-T)\eta,\quad
T&lt;s&lt;S\end{cases}\right.\]</span></p>
<p>其中W是warmup的step数，T是stable training step数，<span class="math inline">\(\eta\)</span> 是maximum learning rate，<span class="math inline">\(f\left(s-T\right)\)</span> 是关于s的 decreasing
function，取值在0到1之间。</p>
<p>一般来说W只要足够，对训练的效果影响就不大，因此所有后面就忽略W了。</p>
<p>继续做一些实验来探索WSD。</p>
<p>（1）Loss Decreases Dramatically in Decay Stage</p>
<p>首先在0.036B的模型上应用了WSD，并设置了不同的T和S（影响decay阶段的长度）。发现在decay阶段，随着learning
rate的下降，loss出现了显著的快速下降，并迅速降低到等于或低于T=S时的Cosine
LRS的loss，具体loss变化如下图</p>
<img src="/376db710/wsd_exp1.png" class title="WSD">
<p>由于stable
training阶段learning是保持不变的，所以这里可以重用decay前的模型checkpoint，继续进行的高learning
rate的训练。在原设置上增加了更多的stable training
step之后，还可以再进行learning rate退火，并且能够实现与Cosine
LRS在同样step下相同的loss。这进一步验证了“训练阶段可以明确地分为stable阶段和decay阶段”的假设。</p>
<p>（2）10% Steps are Enough</p>
<p>如上图所示，在40N、60N和80N训练数据的实验中，使用总token数的10%的进行learning
rate
decay就足以获得最好的结果，如果小于10%则效果会比较差。因此，在后续的训练实验中，都使用大约10%的step进行learning
rate decay，以确保完全收敛。</p>
<p>（3）Effective Data Scaling with WSD LRS</p>
<p>使用WSD可以把模型训练到极致收敛的状态。为了展示WSD训练固定大小模型到收敛的潜力，MiniCPM对0.036B的模型进行持续训练，然后与使用40N数据的0.17B模型进行比较，loss如下图。</p>
<img src="/376db710/wsd_exp2.png" class title="WSD">
<p>0.036B模型在使用更多的数据后，超过Chinchilla
Optimal，并且仍有收敛趋势，按这个趋势继续训练就能match
0.17B模型的loss水平。</p>
<h2 id="measuring-the-scaling-law-with-wsd-lrs">Measuring the Scaling
Law with WSD LRS</h2>
<p>利用WSD，可以把探索model size和data
size的scaling关系的成本变成线性，因为stable
stage阶段learning保持不变，可以把decay接在不同的step后面来获取不同数据量下的效果。</p>
<p>通过训练从0.04B到2B共6种大小的SLM来测量scaling
law。每种大小的模型都有从10N到60N数据共6个数据量开始decay的结果。</p>
<p>这36个模型的训练结果在5个数据集上进行比较。为了可以比较不同tokenizer的模型的损失，按《GPT-4
technical
report》里的做法，使用byte数的平均而非token数的平均来进行比较。然后用scipy
curvefit function，按下面这个公式拟合model size N and data size
D的关系。</p>
<p><span class="math display">\[L(N,D)=C_NN^{-\alpha}+C_DD^{-\beta}+L_0\]</span></p>
<p>实验结果和拟合结果如下图</p>
<img src="/376db710/scaling_law.png" class title="scaling law">
<p>然后参照《Scaling language models: Methods, analysis &amp; insights
from training gopher》、《Training compute-optimal large language
models》、《Scaling laws for neural language
models》的做法，推算出token数量应该是模型参数量的192倍，这比《Training
compute-optimal large language models》中给出的20倍要大得多。</p>
<p>MiniCPM还把LLAMA2的数据拿出来进行了验证。按LLAMA2报告中给出的数据计算出的token数应是模型参数量的70~100倍，这个值同样比20要大很多。</p>
<p>因此结论是，按照基于WSD的实验结果，语言模型比我们之前想象的可以吸收更多语料数据。</p>
<h1 id="two-stage-pre-training-strategy">Two Stage Pre-training
Strategy</h1>
<p>前面观察到WSD的衰减阶段loss有显著的减少，因此MiniCPM认为在learning
rate的退火阶段整合高质量SFT数据，混合进预训练数据中可以SFT效果：<br>
- 一方面，在退火阶段使用SFT数据能获得预SFT更相关的loss下降<br>
- 另一方面，和在整个预训练阶段都使用SFT数据相比，只在learning rate
decay阶段使用更不容易过拟合</p>
<p>为了验证这个猜想，设计以下训练配置：<br>
- A-1: 2.4B模型，decay阶段仅用无标签数据，之后进行4B的SFT训练<br>
- A-2:
2.4B模型，decay阶段使用无标签数据+SFT数据，之后进行4B的SFT训练<br>
- B-1: 1.2B模型，decay阶段仅用无标签数据，之后进行6B的SFT训练<br>
- B-2: 1.2B模型，decay阶段仅用无标签数据，之后进行12B的SFT训练<br>
- B-3:
1.2B模型，decay阶段使用无标签数据+SFT数据，之后进行6B的SFT训练</p>
<p>各个模型预训练+SFT之后的效果如下</p>
<img src="/376db710/2_stage.png" class title="2阶段训练">
<p>可以看到在预训练learning rate退火阶段加入SFT数据的模型效果更好。</p>
<h1 id="minicpm">MiniCPM</h1>
<h2 id="模型">模型</h2>
<p>MiniCPM有2.4B和1.2B两个规模。其中2.4B模型的词表大小为122,753，1.2B模型词表大小为73,440，都是通过BPE进行构建。在测试数据集上评测，MiniCPM的tokenizer的效率是比较高的，具体数值如下</p>
<img src="/376db710/tokenizer.png" class title="tokenizer">
<p>MiniCPM模型的输入输出共享了矩阵，因为小模型共享输入输出矩阵可以节省很多参数。</p>
<p>在层数和hidden
state的设计上，MiniCPM使用了相比Phi-2等SLM更深更瘦的模型结构，这和《Mobilellm:
Optimizing sub-billion parameter language models for on-device use
cases》的想法一致。具体的结构参数如下</p>
<img src="/376db710/layers.png" class title="更深更瘦的结构">
<p>1.2B模型上使用了GQA，可以进一步节省参数量。</p>
<h2 id="训练">训练</h2>
<p>在WSD的stable阶段，使用1T预训练数据，batch size=3.93M，max
lr=0.01。</p>
<p>在decay阶段，decay的策略为 <span class="math inline">\(f(s-T)=0.5^{(s-S)/T}\)</span>，其中T=5000 steps
(20B tokens)。</p>
<p>SFT阶段共使用了6B数据，learning rate和预训练阶段结束时的learning
rate对齐，同样使用了WSD。</p>
<p>预训练数据的分布如下</p>
<img src="/376db710/data.png" class title="训练数据">
<p>1.2B和2.4B模型的预训练loss如下图</p>
<img src="/376db710/train_loss.png" class title="training loss">
<p>左图loss的第一次突变是因为增大了batch size，效果相当于减小了learning
rate。</p>
<p>最终SFT模型在下游任务的评测结果如下</p>
<img src="/376db710/eval.png" class title="evaluation">
<h2 id="minicpm-dpo">MiniCPM-DPO</h2>
<p>在SFT的基础上，MiniCPM用UltraFeedback数据集进行DPO训练。</p>
<p>DPO训练使用了Cosine LRS, max learning
rate=1e-5，一共训练了一个epoch。</p>
<p>DPO使得模型在MT-bench上的得分从6.89提升到7.25，但是在原来通用benchmark的效果有所下降。</p>
<h2 id="minicpm-128k">MiniCPM-128k</h2>
<p>长文本训练把MiniCPM支持的窗口大小从4k拓展到128k。在这一阶段的训练禁用了输入输出矩阵共享，这会使得模型的实际参数略有上升。训练的初始模型用的是预训练中stable阶段的最后一个checkpoint。</p>
<p>MiniCPM将书籍、维基百科文章和论文分类为“长数据”，其他为“短数据”。那么在这一阶段的训练包含了44%的长数据和
56%的短数据。</p>
<p>训练时不直接训练到128k，而是使用curriculum
learning：先训练32k，再训练128k。4k-32k范围内应用ABF，32K到128K的范围内使用NTK-Aware
RoPE scaling。</p>
<p>如Yi的技术报告和《Zebra: Extending context window with layerwise
grouped local-global
attention》所指出的那样，使用合成的长QA数据，有助于提高模型在上下文感知任务中的性能，MiniCPM也使用了合成的长QA数据。</p>
<p>MiniCPM-128k在∞Bench（《∞bench: Extending long context evaluation
beyond 100k tokens》）上评测结果如下</p>
<img src="/376db710/128k_result.png" class title="128k evaluation">
<h2 id="minicpm-moe">MiniCPM-MoE</h2>
<p>MiniCPM-MoE使用Sparse Upcycling（《Sparse upcycling: Training
mixture-of-experts from dense
checkpoints》）进行初始化，使用了stable阶段的checkpoint。router用均值为0、方差为0.01的正态分布进行初始化。</p>
<p>MiniCPM-MoE共有13.6B参数，激活2个专家，共激活4B参数。</p>
<p>训练时使用switch transformer的负载均衡函数，权重系数为0.01。</p>
<p>learning rate使用了WSD，在4M的batch
size下共进行了130k步预训练，而在SFT阶段batch size减小了到2M。</p>
<p>MiniCPM-MoE的效果评测如下</p>
<img src="/376db710/moe_result.png" class title="moe evaluation">
<h1 id="小结">小结</h1>
<p>MiniCPM站在很多前人结果的肩膀上，把目前各种比较先进的做法融合到了1B/2B模型上，获得了不错的效果。其中用到的参数搜索、对scaling
law的刷新都挺有参考价值。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a><br>
<a href="https://www.linsight.cn/dd614e12.html">大模型算法题(7)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】MiniCPM: Unveiling the Potential of Small Language Models with
Scalable Training Strategies https://arxiv.org/abs/2404.06395</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>预训练</tag>
        <tag>技术报告</tag>
        <tag>学习率</tag>
      </tags>
  </entry>
  <entry>
    <title>RoPE的远距离衰减</title>
    <url>/f0902f1a.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>有朋友问到了关于RoPE远距离衰减的问题，这里给出几个示例，提供一个直观理解的视角。</p>
<p>之前对RoPE的梳理参考 <a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a>。</p>
<h1 id="公式">公式</h1>
<p>回顾一下RoPE的实现。RoPE通过在q和k上分别乘一个旋转矩阵，实现了相对距离编码的功能。</p>
<p>对于position为m的q或者k，旋转矩阵如下</p>
<p><span class="math display">\[
\boldsymbol{R}_{\Theta,m}^d=\begin{pmatrix}\cos m\theta_0&amp;-\sin
m\theta_0&amp;0&amp;0&amp;\cdots&amp;0&amp;0\\\sin m\theta_0&amp;\cos
m\theta_0&amp;0&amp;0&amp;\cdots&amp;0&amp;0\\0&amp;0&amp;\cos
m\theta_1&amp;-\sin m\theta_1&amp;\cdots&amp;0&amp;0\\0&amp;0&amp;\sin
m\theta_1&amp;\cos
m\theta_1&amp;\cdots&amp;0&amp;0\\\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots&amp;\vdots\\0&amp;0&amp;0&amp;0&amp;\cdots&amp;\cos
m\theta_{d/2-1}&amp;-\sin
m\theta_{d/2-1}\\0&amp;0&amp;0&amp;0&amp;\cdots&amp;\sin
m\theta_{d/2-1}&amp;\cos n\theta_{d/2-1}\end{pmatrix}
\]</span></p>
<p>实际实现时，高效率的实现如下</p>
<p><span class="math display">\[
\boldsymbol{R}_{
m}\boldsymbol{q}=\begin{pmatrix}q_0\\q_1\\q_2\\q_3\\q_4\\\vdots\\q_{d-2}\\q_{d-1}\end{pmatrix}\otimes\begin{pmatrix}\cos
m\theta_0\\\cos m\theta_0\\\cos m\theta_1\\\cos m\theta_1\\\cos
m\theta_1\\\vdots\\\cos m\theta_{d/2-1}\\\cos
m\theta_{d/2-1}\end{pmatrix}
+\begin{pmatrix}-q_1\\q_0\\-q_3\\\vdots\\-q_{d-1}\\q_{d-2}\end{pmatrix}\otimes\begin{pmatrix}\sin
m\theta_0\\\sin m\theta_0\\\sin m\theta_1\\\sin m\theta_1\\\sin
m\theta_1\\\vdots\\\sin m\theta_{d/2-1}\\\sin
m\theta_{d/2-1}\end{pmatrix}
\]</span></p>
<p>也可以让第二项保持输入向量的元素位置，变成</p>
<p><span class="math display">\[
\boldsymbol{R}_{
m}\boldsymbol{q}=\begin{pmatrix}q_0\\q_1\\q_2\\q_3\\q_4\\\vdots\\q_{d-2}\\q_{d-1}\end{pmatrix}\otimes\begin{pmatrix}\cos
m\theta_0\\\cos m\theta_0\\\cos m\theta_1\\\cos m\theta_1\\\cos
m\theta_1\\\vdots\\\cos m\theta_{d/2-1}\\\cos
m\theta_{d/2-1}\end{pmatrix}
+\begin{pmatrix}q_0\\q_1\\q_2\\q_3\\q_4\\\vdots\\q_{d-2}\\q_{d-1}\end{pmatrix}\otimes\begin{pmatrix}\sin
m\theta_0\\-\sin m\theta_0\\\sin m\theta_1\\-\sin m\theta_1\\\sin
m\theta_1\\\vdots\\\sin m\theta_{d/2-1}\\-\sin
m\theta_{d/2-1}\end{pmatrix}
\]</span></p>
<p>huggingface的实现中预先把各个位置的cos额sin向量都计算好了，可以重复利用，这样看后面这样实现的效率会更高一点。</p>
<h1 id="远距离衰减">远距离衰减</h1>
<p>远距离衰减指的是随着q和k的相对距离的增大，加入位置编码之后的内积应该随着距离增大而减小，这样相当于离得远的token分配到的attention会比较小，而离得近的token会得到更多的注意力。</p>
<p>这样的特性确实直觉上比较符合人类的注意力机制。</p>
<p>把各个参数（base、window size、head
size）下的内积值画出来看看是怎么衰减的。实现参考下面的代码。这里偷懒没有实现得很高效，勉强能用就行。</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> apply_rope(input_vec, position, base<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 获取维度</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> input_vec.shape[<span class="dv">0</span>]</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 获取theta</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    i <span class="op">=</span> np.arange(<span class="dv">1</span>, d <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> base <span class="op">**</span> (<span class="op">-</span><span class="dv">2</span> <span class="op">*</span> (i <span class="op">-</span> <span class="dv">1</span>) <span class="op">/</span> d)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.repeat(theta, <span class="dv">2</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 计算旋转后的向量</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    reranged_vec <span class="op">=</span> np.empty_like(input_vec)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    reranged_vec[<span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> <span class="op">-</span>input_vec[<span class="dv">1</span>::<span class="dv">2</span>]</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    reranged_vec[<span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> input_vec[:<span class="op">-</span><span class="dv">1</span>:<span class="dv">2</span>]</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    output_vec <span class="op">=</span> input_vec <span class="op">*</span> np.cos(position <span class="op">*</span> theta) <span class="op">+</span> reranged_vec <span class="op">*</span> np.sin(position <span class="op">*</span> theta)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output_vec</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot(x, y, name<span class="op">=</span><span class="st">&#39;&#39;</span>):</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    plt.plot(x, y, label<span class="op">=</span>name)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 显示图表</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>base <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>window_size <span class="op">=</span> <span class="dv">4096</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> np.ones(d)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> np.ones(d)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>rotated_q <span class="op">=</span> apply_rope(input_vec<span class="op">=</span>q, position<span class="op">=</span><span class="dv">0</span>, base<span class="op">=</span>base)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>inner_products <span class="op">=</span> []</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(window_size):</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    rotated_k <span class="op">=</span> apply_rope(input_vec<span class="op">=</span>k, position<span class="op">=</span>i, base<span class="op">=</span>base)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    product <span class="op">=</span> np.dot(rotated_q, rotated_k)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    inner_products.append(product)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>plot(x<span class="op">=</span><span class="bu">range</span>(window_size), y<span class="op">=</span>inner_products, name<span class="op">=</span><span class="ss">f&#39;base=</span><span class="sc">&#123;</span>base<span class="sc">&#125;</span><span class="ss">,window size=</span><span class="sc">&#123;</span>window_size<span class="sc">&#125;</span><span class="ss">,d=</span><span class="sc">&#123;</span>d<span class="sc">&#125;</span><span class="ss">&#39;</span>)</span></code></pre></div>
<p>（1）q = k = 1</p>
<p>假设q和k都是1向量，如果q在位置0，画出k在0~4096位置下和q在位置编码后的内积如下。</p>
<img src="/f0902f1a/1.png" class title="衰减">
<p>这里使用了base=10000，d=512。</p>
<p>可以看到整体趋势是震荡下降的</p>
<p>不过如果把窗口从4096增大到65536，图像会变成这样</p>
<img src="/f0902f1a/2.png" class title="衰减">
<p>可以看到图像不再是单纯的衰减，在距离超过大约15000的时候，出现了上升。</p>
<p>实际上这个包含多个周期函数的内积也具有一定的周期性，并不是在整个域上保持衰减的特性。只要相对距离够大，超过这个周期的1/4，内积就会再次上升。</p>
<p>而这个内积的周期受base调控，base越大，周期越长，因此现在的长窗口模型起步就是base=5M或者10M。</p>
<p>我们把base改成5M，图像如下</p>
<img src="/f0902f1a/3.png" class title="衰减">
<p>又呈现了震荡衰减的趋势。</p>
<p>前面画的是q在位置0，k在0~4096/65536的情况，那么把q放在中间看看内积结果怎么样。</p>
<img src="/f0902f1a/4.png" class title="衰减">
<p>可以看到在q两边的内积是对称的，同样的远距离衰减属性。</p>
<p>（2）q、k随机</p>
<p>前面是把q和k固定为1向量，现在试着把q和k初始化为随机向量，图像如下</p>
<img src="/f0902f1a/5.png" class title="衰减">
<p>相比1向量出现了更多的震荡，但是大体上还是能保持一定的远距离衰减特性。</p>
<h1 id="小结">小结</h1>
<ul>
<li>RoPE的远距离衰减是震荡的，并且整个内积本身也具有一定的周期性，只有把base设得足够大，才能让内积结果在模型窗口大小内保持远距离衰减的特性。<br>
</li>
<li>在q和k的相对距离小的时候，内积差距较大，也就是衰减较快；到了远距离之后，衰减变慢，也就是从内积角度来看，分辨率会变小。</li>
</ul>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a><br>
<a href="https://www.linsight.cn/dd614e12.html">大模型算法题(7)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】Transformer升级之路：2、博采众长的旋转式位置编码，https://spaces.ac.cn/archives/8265<br>
【2】RoFormer: Enhanced Transformer with Rotary Position Embedding
https://arxiv.org/abs/2104.09864<br>
【3】理解LLM位置编码:RoPE http://www.linsight.cn/c4da56c0.html</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>positional encoding</tag>
        <tag>RoPE</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM的重复生成和ICL</title>
    <url>/7381cae3.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>LLM的重复生成问题，俗称复读机问题。</p>
<p>对于这个问题的研究很多都和in-context learning相关。</p>
<h1 id="背景">背景</h1>
<p>在目前这个时间点，其实已经很少会遇到字符级别的重复生成问题。</p>
<p>自ChatGPT发布以来，模型规模的增大，训练数据数量和质量的提升，都让LLM的能力不断提升，复读机问题这种基础问题看起来确实不多见了。</p>
<p>不过在某些场景，比如手机的端侧智能助手，所用的模型相对较小，有时还是能遇到一些句子级别、段落级别的重复生成问题。</p>
<p>此外，在多轮对话下，随着对话轮次的增多，出现“重复生成/近似重复生成”的概率也会增加。在特定的对话中，用户新增了一些细致的要求或者进行关于细节的追问，这时模型有可能倾向于给出和前面回答几乎相同的答案。</p>
<p>当然这些情况都可以归因于模型训得不够好。但重复生成的问题显然和知识储备、回复质量这些问题有所不同，也有一些工作进行相关分析。</p>
<p>模型这种重复生成的特性部分可以为我们所用，但有时也会带来问题。</p>
<h1 id="induction-heads">induction heads</h1>
<p>Anthropic在22年的文章《In-context Learning and Induction
Heads》中对基于transformer的语言模型进行了ICL相关的分析。他们发现在这些生成模型里，存在着induction
heads的机制。</p>
<p>induction
heads是模型中的一条circuit。简单来说，其功能是回顾当前token前面的内容，找到前面出现当前token的地方，并按照前面出现过的模式来补全当前token后面的内容。</p>
<p>举个例子，比如现在的序列是</p>
<center>
... [token A] [token B] [token C] ... [toekn A]
</center>
<p>在生成下一个token的时候，induction heads就会从最后一个 [toekn A]
往前找，发现前面出现过相同的 [toekn
A]，那么模型后面就会倾向于按照前面的出现过的 [token A] [token B] [token
C] 这样的pattern来补全后面的内容，生成</p>
<center>
... [token A] [token B] [token C] ... [toekn A] [toekn B] [toekn C]
</center>
<p>induction
heads这样的复制能力也会扩展到“相似”token上。如果当前token在前面没有出现过，那么induction
heads就会去前面找和当前token相似的token所在的pattern，以此pattern作为后面生成的参考。比如现在的序列是</p>
<center>
... [token A] [token B] [token C] ... [toekn A‘]
</center>
<p>其中 [toekn A‘] 和 [toekn A] 具有相近的特征，那么induction
heads就会倾向于把序列补全为</p>
<center>
... [token A] [token B] [token C] ... [toekn A‘] [toekn B‘] [toekn C‘]
</center>
<p>其中 [toekn B‘]、[toekn C‘]分别和[token B]、[token
C]有相近的特征。</p>
<p>induction heads由2个attention
head组成。Anthropic在2层的transformer模型上精确地探测到了这样的行为，而且更多层更复杂的transformer模型上，也通过一些手段观察到类似功能的存在。</p>
<p>induction
heads这样的“复制”行为被认为是ICL能力的主要来源。前面的例子中，从[token
A] [token B] [toekn A‘] 生成 [toekn B‘]，这就已经是一个ICL的行为。</p>
<p>同时induction
heads这样的“复制”行为某种程度上其实也是一种重复生成行为。这样的“复制”行为很可能和LLM的训练方式有关：预训练next
token
prediction鼓励模型预测概率最大的token，而在上文出现过相似token/pattern会提升模型复制token的信心，从而加强了重复生成的行为。</p>
<h1 id="重复生成与icl">重复生成与ICL</h1>
<p>论文：《Learning to Break the Loop: Analyzing and Mitigating
Repetitions for Neural Text Generation》，2022年</p>
<p>这篇文论对生成模型的（句子级）重复生成问题做了一些实验和分析，找到一些和重复生成现象相关的发现，并提出DITTO（pseuDo-repetITion
penalizaTiOn）缓解重复生成的问题。（这缩写的方式让人想起 NEural
contextualiZed representation for cHinese lAnguage understanding）</p>
<p>这样的研究基于一个前提：使用maximization-based decoding算法（如greedy
decoding）。一些带有随机性的算法本身是具有缓解重复生成问题的能力的。</p>
<p>发现一：模型倾向于提升前面出现过的句子的生成概率</p>
<p>并且只要重复一次，这个概率就会飙升很多（下图）。这个发现和induction
heads中的类似。</p>
<p>发现二：重复生成具有self-reinforcement的特点</p>
<p>重复的次数越多，越容易重复，越难以打破这个循环，如下图，横轴表示重复次数，纵轴红色表示某个token的概率，蓝色表示最大的概率。</p>
<img src="/7381cae3/ditto_1.png" class title="重复生成">
<p>发现三：Sentences with higher initial probabilities usually have a
stronger self-reinforcement effect</p>
<p>句子本身概率越大（模型认为越通顺），重复的自我加强效应越强。把重复的句子换成随机token，在不同重复次数下解码token的概率变化如下图，增强的趋势比上图（通顺句子）要弱很多</p>
<img src="/7381cae3/ditto_2.png" class title="重复生成">
<p>而DITTO的做法简单来说是构造了一些重复句子的训练样本，并在训练时显示加入对重复token的惩罚。</p>
<p>另一个工作《UNDERSTANDING IN-CONTEXT LEARNING FROM
REPETITIONS》，对self-reinforcement进行了进一步的测试，发现：<br>
- self-reinforcement是LLM的共同属性，多个模型都具有这样的特点<br>
- self-reinforcement强度随着重复token的距离减小而增强，如下图所示</p>
<img src="/7381cae3/3.png" class title="重复生成">
<ul>
<li>即使重复的token个数只有1个，这种self-reinforcement也会出现；而随着重复片段的增长，self-reinforcement强度也在提升，如下图所示</li>
</ul>
<img src="/7381cae3/4.png" class title="重复生成">
<p>而通过计算token重复的次数和token的概率，发现正是预训练next token
prediction的任务赋予了模型self-reinforcement的特点。</p>
<img src="/7381cae3/5.png" class title="重复生成">
<p>模型这种自我加强的作用对我们来说，既有好处又有坏处，可以说令人又爱又恨。</p>
<p>好处一：constraining output space</p>
<p>在ICL中，给定多个demonstration，通过利用self-reinforcement的特点，可以让模型的输出不要跑偏。比如多项选择题的ICL，可以让模型输出ABCD，而不是其他无法解析的内容。</p>
<p>为了验证这个假设，对用ICL数据做了实验，如下图。橙色线是对demonstration的问题内容和答案内容进行mask处理，蓝色线是在橙色的基础上进一步把ABCD替换成别的符号，红色线是在橙色线的基础上把“Answer：”替换成相同意思的内容。</p>
<img src="/7381cae3/6.png" class title="重复生成">
<p>可以看到，如果仅仅对问题和答案内容进行mask，并不影响模型输出ABCD的概率，但是如果把ABCD/“Answer：”这种在demonstration中多次重复的内容替换成意思相近的符号，就会使得模型在答案空间的分布概率降低。</p>
<p>好处二：learning to follow patterns</p>
<p>和好处一类似，CoT的成功正是这个好处的一个例子。</p>
<p>坏处一：spurious connections</p>
<p>对于ICL来说，self-reinforcement的坏处也很明显。不合理的prompt，比如不平均的答案分布，都会影响模型的能力，甚至可能成为用户注入攻击的入口。</p>
<h1 id="缓解复读机问题">缓解复读机问题</h1>
<p>模型重复生成的self-reinforcement可以在ICL中发挥作用，但是也会让模型在生成回复的时候不断重复相同内容，停不下来。</p>
<p>一个可能的原因是训练数据中存在较多重复内容，这在从网页爬取的预训练数据中还是有一定比例的，因此对数据的清洗需要加入筛选这种重复内容的逻辑。</p>
<p>但是即使把训练数据中的重复数据比例降到很低，依然不能完全杜绝复读机问题，因此有很多方法是在解码的时候进行处理（decoding-base），缓解复读机问题：<br>
- stochastic
sampling：通过引入随机性，让模型不要总是选择概率最大的token输出，比如top-k、top-p采样。<br>
- 重复惩罚：对已经出现过的token进行惩罚，减少生成结果的重复token。<br>
- contrastive
decoding：对比采样，降低生成表征相近的token。（实操对效果有比较大的影响）<br>
- beam search（比较慢）<br>
- locally typical sampling<br>
- no repeat ngram：和重复惩罚类似，保证没有重复的ngram出现</p>
<p>此外也有training-based的方法： -
在训练的时候对已经出现过的token进行惩罚<br>
-
通过强化学习对模型的重复生成进行惩罚（但强化学习成本和难度都比较高）</p>
<h1 id="小结">小结</h1>
<p>自回归模型的重复生成不仅和数据有关，跟训练方法、模型结构、解码策略都有关。</p>
<p>这种特性既好处也有坏处，在ICL可以成为我们控制模型效果的抓手，但也有可能带来生成内容的崩溃问题。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a><br>
<a href="https://www.linsight.cn/dd614e12.html">大模型算法题(7)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】如何解释大模型的重复生成现象？
https://www.zhihu.com/question/616130636<br>
【2】Learning to Break the Loop: Analyzing and Mitigating Repetitions
for Neural Text Generation https://arxiv.org/abs/2206.02369<br>
【3】Understanding In-Context Learning from Repetitions
https://arxiv.org/abs/2310.00297<br>
【4】In-context Learning and Induction Heads
https://arxiv.org/abs/2209.11895<br>
【5】https://zhuanlan.zhihu.com/p/671697479</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>复读机</tag>
        <tag>重复生成</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型算法题(7)</title>
    <url>/dd614e12.html</url>
    <content><![CDATA[<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a></p>
<hr>
<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<p>本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~</p>
<p>如有错漏，欢迎指正~</p>
<hr>
<h1 id="moe模型训练中如果不对专家的路由进行适当干预可能会遇到什么问题有什么解决方法">1.MoE模型训练中，如果不对专家的路由进行适当干预，可能会遇到什么问题，有什么解决方法？</h1>
<p>MoE使用多个并行的expert，每次推理只选择其中的一小部分expert使用。</p>
<p>如果让模型完全自行学习，有可能出现routing
collapse的问题，也就是模型倾向于总是选择那几个常用的专家。</p>
<p>而这些常用的专家由于使用得更多，训练得更好，又会提升被路由到的概率，导致大部分模型参数几乎没有被用上。</p>
<p>一般可以通过增加一个负载平衡的loss来缓解。负载平衡loss有不同的设计和计算方式，但是大致的思路都是迫使模型均匀地使用不同的专家，如果出现某些专家被选中的概率过高，就会进行惩罚。</p>
<h1 id="bert的预训练方式是mlm通过mask-token对部分输入进行掩盖要求模型预测为什么要使用mask-token而不直接修改attention-mask矩阵">2.Bert的预训练方式是MLM，通过[Mask]
token对部分输入进行掩盖，要求模型预测。为什么要使用[Mask]
token而不直接修改attention mask矩阵？</h1>
<p>直接修改attention
mask矩阵也可以让模型看不到对应位置的输入，但是相比使用[Mask]
token缺少了位置编码的信息。</p>
<p>另外使用[Mask]
token掩盖要预测的值这种做法在实现上相对方便，只需要对输入数据进行处理即可，而不需要修改modeling的内容，更加通用。</p>
<h1 id="为什么大模型训练的时候需要warmup">3.为什么大模型训练的时候需要warmup？</h1>
<p>在训练前期，刚随机初始化的模型参数离收敛值很远，此时的loss会比较大，梯度也会很大。如果直接使用固定的较大learning
rate，模型容易过拟合到早期step见过的这些数据。</p>
<p>依照ResNet的实验结果，如果一开始模型就跑偏了，那后面再怎么训练，收敛效果都会比较差，说明早期太大的学习率导致模型过早收敛到不太好的局部最优了。</p>
<p>另外，模型刚开始训练的时候，大学习率带来的大更新值，会导致模型参数的震荡会很大，使得模型学到的参数很不稳定，这也不利于训练。</p>
<p>使用少数step让模型进行热身，可以很大程度规避这些问题。</p>
<h1 id="roberta在bert的基础上做了什么优化">4.Roberta在Bert的基础上做了什么优化？</h1>
<ol type="1">
<li><p>Bert在MLM训练中，提早把要mask的token处理好，在训练时训了多个epoch。这样在不同的epoch中，同一条sample总是使用相同的mask进行训练。Roberta使用了dynamic
mask，即每条数据在训练前才随机决定进行mask的位置，这样不同的epoch之间同一条样本也有不同的mask结果，提升了数据多样性。</p></li>
<li><p>Bert使用了MLM和NSP两个任务，而Roberta通过实验发现NSP的作用不大，因此直接取消了NSP任务。</p></li>
<li><p>Roberta增大了batch
size，提高训练效率，以获取更好的训练结果。</p></li>
<li><p>Roberta增大了训练数据和训练step数，实验表明模型继续训练还能进一步收敛。</p></li>
<li><p>Bert使用WordPiece分词，而Roberta使用BBPE，增大了词表。</p></li>
<li><p>再后来，Google发布了WWM全词mask，改进了mask方式。Roberta-WWM也成了最广泛使用的版本。</p></li>
</ol>
<h1 id="lora的参数是怎么初始化的">5.LoRA的参数是怎么初始化的？</h1>
<p>LoRA包含一个降维矩阵A，和一个升维矩阵B。矩阵A用随机高斯分布初始化，而矩阵B用初始化为0。这样可以使得训练开始的时候，LoRA的参数不产生效果，模型能够保持增加LoRA前的输出。但是A、B矩阵不能同时为0，这样会有对称性问题，降低了模型的表达能力。</p>
<img src="/dd614e12/lora.png" class title="lora">
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a></p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>算法题</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型偏好对齐-IPO</title>
    <url>/4fe7b810.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>前面我们对DPO、ODPO、simPO的思路做了整理：<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a>，<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a>，<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a>。</p>
<p>而《A General Theoretical Paradigm to Understand Learning from Human
Preferences》提出了可以将RLHF和DPO的目标函数视为其中一个特例的更general的目标函数ΨPO，并对ΨPO的一些问题进行了分析，最终设计了Identity-PO
(IPO)来绕过这些问题。</p>
<h1 id="ψpo">ΨPO</h1>
<p>回顾一下RLHF，它的目标函数是</p>
<p><span class="math display">\[\mathbb{E}_\pi[r(x,y)]-\beta
D_{\text{KL}}(\pi\mid\mid\pi_{\text{ref}})\]</span></p>
<p>而DPO从等价的目标函数推导出DPO的损失函数如下</p>
<p><span class="math display">\[\begin{aligned}\min_{\pi}\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}\Bigg[-\log\sigma\Bigg(\beta\log\Bigg(\frac{\pi(y_w|x)}{\pi(y_l|x)}\Bigg)-\beta\log\left(\frac{\pi_{\mathrm{ref}}(y_w|x)}{\pi_{\mathrm{ref}}(y_l|x)}\Bigg)\Bigg)\right]\end{aligned}\]</span></p>
<p>IPO这篇论文则提出一个general的目标函数。考虑一个对preference
probability进行非线性变换的non-decreasing function Ψ</p>
<p><span class="math display">\[\Psi:\begin{bmatrix}0,1\end{bmatrix}\to\mathbb{R}\]</span></p>
<p>Ψ-preference optimisation objective定义为</p>
<p><span class="math display">\[\max_\pi\quad\mathbb{E}_{x\thicksim\rho}\quad[\Psi(p^*(y\succ
y&#39;|x))]-\beta
D_{\mathrm{KL}}(\pi\mid\mid\pi_{\mathrm{ref}})\]</span></p>
<p>如果我们给Ψ一个具体定义，如下式</p>
<p><span class="math display">\[\Psi(q)=\log(q/(1-q))\]</span></p>
<p>那么在Bradley-Terry model的假设下，我们有</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}_{y&#39;\thicksim\mu}[\Psi(p^*(y\succ y&#39;))]&amp;
=\underset{y&#39;\thicksim\mu}{\operatorname*{\mathbb{E}}}\left[\Psi\left(\frac{e^{r(y)}}{e^{r(y)}+e^{r(y&#39;)}}\right)\right]  \\
&amp;=\mathbb{E}_{y^{\prime}\thicksim\mu}[\log(e^{r(y)}/e^{r(y^{\prime})})]
\\
&amp;=\mathbb{E}_{y&#39;\thicksim\mu}[r(y)-r(y&#39;)] \\
&amp;=r(y)-\underset{y&#39;\thicksim\mu}{\mathbb{E}}[r(y&#39;)]
\end{aligned}\]</span></p>
<p>右边最终结果里的第二项可视为常数。除去这个常数，ΨPO的优化目标和RLHF的优化目标是等价的，同时也就和DPO的目标是等价的。</p>
<p>同DPO的做法一样，这里我们可以推出ΨPO在Bradley-Terry
model下的解析解</p>
<p><span class="math display">\[\pi^*(y)\propto\pi_{\mathrm{ref}}(y)\exp\left(\beta^{-1}\mathbb{E}_{y^{\prime}\thicksim\mu}[\Psi(p^*(y\succ
y^{\prime}))]\right)\]</span></p>
<p>我们把Ψ(q)的图像画出来，如下所示</p>
<img src="/4fe7b810/curve.png" class title="log">
<p>可以看到在两端，Ψ(q)的曲线有很强的非线性化特征，并且值会趋向于无穷大。</p>
<p>那么当我们对一对质量差异很大的样本，即</p>
<p><span class="math display">\[p^*(y\succ y&#39;)=1\]</span></p>
<p>进行学习时，在BT模型的假设下，就有</p>
<p><span class="math display">\[(r(y)-r(y&#39;))\to+\infty\]</span></p>
<p>把 <span class="math inline">\((r(y)-r(y&#39;))\to+\infty\)</span>
代入到ΨPO上面退出来的解析解里，有</p>
<p><span class="math display">\[\begin{aligned}
&amp;\frac{\pi^*(y_l)}{\pi^*(y_w)}\\
=&amp;\frac{\pi_{\mathrm{ref}}(y_l)}{\pi_{\mathrm{ref}}(y_w)}\mathrm{exp}\left(\beta^{-1}\sum_{y^{\prime}}[\Psi(p(y_l\succ
y^{\prime}))-\Psi(p(y_w\succ y^{\prime}))]\right)\\
=&amp;\frac{\pi_{\mathrm{ref}}(y_l)}{\pi_{\mathrm{ref}}(y_w)}\mathrm{exp}(\beta^{-1}\sum_{y^{\prime}}[r(y_l)-r(y_w)])\\
=&amp;\frac{\pi_{\mathrm{ref}}(y_l)}{\pi_{\mathrm{ref}}(y_w)}\mathrm{exp}(\beta^{-1}\sum_{y^{\prime}}[-\infty])\\
=&amp;0
\end{aligned}\]</span></p>
<p>那么此时无论 <span class="math inline">\(\beta\)</span>
取什么值，都有 <span class="math inline">\(\pi^*(y_l)=0\)</span>。说明当偏好越确定，KL项的约束能力越弱，模型就很容易摆脱KL项的约束，过度追求reward的最大化，最终导致过拟合。</p>
<p>不过RLHF在实践上并没有表现出如这里推算结果一样特别容易过拟合的特性，原因是因为训练出来的reward模型通常由于欠拟合，没有给出那么极端的偏好概率。反而是DPO因为节省了reward模型的训练，因此更加容易受到这种过拟合的困扰。</p>
<h1 id="ipo">IPO</h1>
<p>既然高度非线性化（且极值无限大）的Ψ(q)会导致DPO容易过拟合，那么一个自然的想法就是把Ψ(q)替换成一个有界的函数，identity
mapping恒等变换就是一个符合要求的选择。这样就得到IPO的目标函数</p>
<p><span class="math display">\[\max_\pi\quad\mathbb{E}_{x\thicksim\rho}\quad[p^*(y\succ
y&#39;|x)]-\beta
D_{\mathrm{KL}}(\pi\mid\mid\pi_{\mathrm{ref}})\]</span></p>
<p>根据这个，可以推导出IPO的损失函数为</p>
<p><span class="math display">\[\mathbb{E}_{(y_w,y_l,x)\thicksim
D}\left(h_\pi(y_w,y_l,x)-\frac{\beta^{-1}}2\right)^2\]</span></p>
<p><span class="math display">\[h_\pi(y,y&#39;,x)=\log\left(\frac{\pi(y|x)\pi_{\text{ref}}(y&#39;|x)}{\pi(y&#39;|x)\pi_{\text{ref}}(y|x)}\right)\]</span></p>
<h1 id="小结">小结</h1>
<p>ΨPO/IPO从理论上对DPO进行了一系列的分析，也推出了一个相对更不容易过拟合的偏好学习方法。不过在实践上的证明没有完善，可以作为一个理解的DPO的角度来参考吧。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】A General Theoretical Paradigm to Understand Learning from Human
Preferences https://arxiv.org/abs/2310.12036</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>强化学习</tag>
        <tag>微调</tag>
        <tag>SFT</tag>
        <tag>偏好对齐</tag>
      </tags>
  </entry>
  <entry>
    <title>从loss视角理解大模型涌现能力</title>
    <url>/f5fb75e4.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>智谱在《Understanding Emergent Abilities of Language Models from the
Loss Perspective》中提出一个观察大模型涌现能力的视角 --
预训练loss，主要内容是通过一系列实验结果来解释一些关于涌现能力的观察。可以作为一个理解大模型的参考角度，也可以用于指导预训练模型的开发和优化。</p>
<h1 id="背景">背景</h1>
<p>《Emergent abilities of large language models》把emergent
ability定义为在大规模模型中有，而在参数量较小的模型没有的能力。</p>
<p>这个看法现在受到一些挑战：<br>
-
目前很多在更大规模数据集训练出来的小模型，展现出比之前大规模模型更强的能力，比如LLaMA3在大部分评测指标上就比GPT-3强，很多以前千亿模型才能做到的任务，现在百亿甚至十亿的模型也能做好。<br>
- 《Are emergent abilities of large language models a
mirage?》认为产生涌现能力现象的因为是数据评测指标的非线性和不连续性带来的，如果使用更细粒度的连续指标，就能观察到指标的平滑提升。</p>
<p>而《Training compute-optimal large language
models》指出，相同的计算量下，不同的模型规模和数据量的组合会产生不同的效果。这说明单纯的模型规模或者数据规模并不是一个好的下游任务能力的indicator，预训练loss才是更合适的指标。</p>
<p>但是训练loss和下游任务表现具体是什么关系却还没有确定的说法，智谱针对这个问题做了一些预训练实验，并从预训练loss角度定义了emergent
ability。</p>
<h1 id="pretraining-loss和下游任务表现的关系">pretraining
loss和下游任务表现的关系</h1>
<h2 id="设置">设置</h2>
<p>后续所有预训练实验使用相同的模型结构和同一份预训练数据（但是训练数据量可能有区别），一些通用设置如下：<br>
- 分词用BPE<br>
- 模型结构在LLaMA基础上，全部使用GQA，而RoPE只在一半的Q/K上应用<br>
- 使用AdamW优化器，<span class="math inline">\(\beta_1=0.9\)</span>，<span class="math inline">\(\beta_2=0.95\)</span><br>
- 训练窗口长度为2048<br>
-
所有模型都在中英文比例为1:4的预训练数据集上训练，英文数据集分布如下</p>
<img src="/f5fb75e4/eng_data.png" class title="英文数据">
<p>所有模型都是从零开始预训练。</p>
<p>评测模型的下游任务共有6类12个数据集，具体信息如下</p>
<img src="/f5fb75e4/downstream_dataset.png" class title="下游任务">
<img src="/f5fb75e4/downstream_dataset_num.png" class title="下游任务">
<h2 id="实验一pretraining-loss-vs.-performance">实验一：pretraining loss
vs. performance</h2>
<p>第一个实验训练了3个规模的模型：1.5B、6B、32B，训练数据量分别为3T、3T、2.5T。具体设置如下</p>
<img src="/f5fb75e4/exp1_param.png" class title="实验设置">
<p>大约每训练43B
token就会保存一次checkpoint。把3个模型所有checkpoint下，对应的预训练loss和下游任务评测结果画出来，如下所示</p>
<img src="/f5fb75e4/exp1_plot.png" class title="loss vs. performance">
<p>从上图可以观察到3个现象：<br>
-
无论模型规模如何，下游任务评测结果都随着预训练loss的降低而提升。从提升的具体情况可以分成两类，这个后面部分再分析。<br>
-
各个规模的模型所画出的点都落在了同一条曲线上，这说明下游任务的评测结果和预训练loss高度相关，而和模型规模没有直接关系。这点很重要。<br>
-
预训练loss对下游任务指标的表征能力同时适用于中英文，这说明中英文token在多语言预训练中具有相似的learning
dynamics。</p>
<p>而把计算量和下游任务指标的关系画出来，则有如下结果</p>
<img src="/f5fb75e4/exp1_compute.png" class title="下游任务效果和预训练计算量的关系">
<p>可以看到各个规模的模型所画出的点并没有落在同一条曲线上，这说明计算量并不是表征下游任务效果的好指标。</p>
<h2 id="实验二training-token-count-vs.-performance">实验二：training
token count vs. performance</h2>
<p>第二个实验使用了不同的数据量训练了28个小一些的模型，具体设置如下</p>
<img src="/f5fb75e4/exp2_param.png" class title="实验设置">
<p>第一个实验中，每个规模的模型设置了一个固定的训练token数，然后取中间checkpoint进行评测。第二个实验是对每个规模的模型设置了多个不同的总训练token数。二者的区别在于，预训练的最后阶段会逐渐把学习率decay到最小值，而这样的学习率退火策略对效果有很大的影响。</p>
<p>取28个模型的最终checkpoint，画出对应的预训练loss和下游任务评测结果如下</p>
<img src="/f5fb75e4/exp2_plot.png" class title="token count vs. performance">
<p>结果和实验一类似，各个模型的点都落在了同一条曲线上。说明无论模型规模和训练量如何，只要loss相同，在下游任务上就有相同的表现。</p>
<p>由于这28个模型相比实验一的较小，在图中最后一排的任务上效果都接近于随机。这个现象后续分析。</p>
<h2 id="llamas-loss-vs.-performance">LLaMA’s loss vs. performance</h2>
<p>实验一和二是在从零开始训练的模型上评测的，这里用LLaMA来验证前面得到的结论。</p>
<p>由于LLaMA没有放出中间checkpoint，这里直接从LLaMA的报告里抽出相应的数据点，在6个下游任务上的结果如下图</p>
<img src="/f5fb75e4/exp3_plot.png" class title="loss vs. performance">
<p>可以看到基本上各个模型的点也是落在同一条曲线上。LLaMA和实验一实验二的训练框架、模型结构、训练数据都有所不同，但是也有相同的结论，说明这样的结论是具有普遍性的。</p>
<blockquote>
<p>pre-training loss is a good indicator of LMs’ performance on
downstream tasks, independent of model sizes, training tokens,
languages, and pretraining frameworks</p>
</blockquote>
<h1 id="进一步分析">进一步分析</h1>
<h2 id="不同任务的趋势">不同任务的趋势</h2>
<p>12个下游任务可以分为2类：<br>
- 第一类：TriviaQA, HellaSwag, RACE, WinoGrande, NLPCC-KBQA, ClozeT,
CLUEWSC, C3。这些任务的效果随着预训练loss的下降，平滑上升。<br>
- 第二类：MMLU, C-Eval, GSM8K,
GSM8K-Chinese。这些任务上，只有当预训练loss低于一定阈值，评测结果才开始提升。可以观察到，在实验一实验二的配置下，大概在预训练loss小于2.2这个阈值之后，下游任务表现开始提升。整体来说，第二类任务难度是大于第一类的。所以虽然第一类中有些任务的prompt或者形式与第二类中的任务有些相似，但是依然有不同的表现。</p>
<p>第二类任务这个现象和《Grokking: Generalization beyond overfitting on
small algorithmic datasets》提出的grokking有关联。</p>
<p>grokking描述了下游任务的效果从随机水平（乱猜）提高到perfect
generalization的improvement。这种improvement只有在过拟合到一定程度才会发生。在预训练中，模型整体上通常是欠拟合的。不过由于预训练语料库是不同文档的混合，因此模型可能在某些能力上过拟合（比如数值计算的能力，情感分析的能力），而在整体上依然欠拟合。</p>
<p>当然第二类任务这个现象也和emergent ability有关联。按scaling
law的说法，在训练token数固定的情况下，预训练loss与模型规模呈幂律关系。也就是说，模型大小和预训练损失之间存在单调关系。对于第二类任务，存在一个与预训练loss中的临界点相对应的模型规模阈值。当模型大小超过这个阈值时，模型就可以展现出超过随机猜测的能力。</p>
<h2 id="评测指标的影响">评测指标的影响</h2>
<p>前面提到，emergent
ability这个现象有可能是因为评测指标的非线性和不连续性带来的。比如
MMLU这样的多项选择题，打分结果只能是0分或者满分。</p>
<p>现在把这个评测指标换成两个连续的指标：<br>
- 一个是probability of the correct answer (CorrectChoiceProb)<br>
- 第二个是《Are emergent abilities of large language models a
mirage?》中提出的Brier Score：</p>
<p><span class="math display">\[\text{BrierScore}=\frac1N\sum_{i=1}^N\sum_{j=1}^C(y_{ij}-\hat{y}_{ij})^2\]</span></p>
<p>N是样本数，C的类别数。</p>
<p>把MMLU和C-Eval在这两个新指标上的评测结果画出来，如下所示</p>
<img src="/f5fb75e4/metrics.png" class title="指标">
<p>可以发现涌现能力的现象依然存在。</p>
<p>值得注意的是，Brier Score的下降并不总是表示下游任务效果的提升。</p>
<p>比如对于有A/B/C/D四个选项的多项选择题任务，假设正确答案的分布是均匀的。现在有两个模型，一个总是预测A，即（1，0，0，0），另一个总是给出平均分布的预测，即（0.25，0.25，0.25，0.25，0.25）。</p>
<p>那么前者的Brier
Score是1.5，而后者是0.75，但这并不能说明后者就更好。对于这个任务，实际上高于0.75的Brier
Score都说明比随机猜测差。而低于随机猜测的指标变化并不能当做真正的提升，比如Brier
Score从1.5提升到1.0并不能算作提升。</p>
<p>另外《Training trajectories of language models across
scales》提出用perplexity of correct
options来作为评测，可以看到平滑的提升。但perplexity of correct
options其实不能作为一个合适的指标。</p>
<p>比如对于多项选择题，区分各个答案的能力才是我们想要的。而随着预训练进行，正确答案和错误答案的perplexity都在下降，只有当训练到二者的perplexity差异开始变大的时候，才能算是有提升。因此单纯的正确答案perplexity下降也能作为能力提升的指标，因为错误答案的perplexity可能下降更多。</p>
<h1 id="从loss角度定义emergent-abilities">从loss角度定义emergent
abilities</h1>
<p>基于前面的实验和分析，现在从预训练loss角度重新定义emergent
ability：</p>
<blockquote>
<p>Definition. An ability is emergent if it is not present in models
with higher pre-training loss but is present in models with lower
pre-training loss.</p>
</blockquote>
<p>一个emergent ability的normalized
performance（比如多项选择题随机猜测的得分是0.25分，那这个任务原始的0.25分在normalized
performance下就是0分）是预训练loss <span class="math inline">\(L\)</span> 的函数</p>
<p><span class="math display">\[\begin{cases}f(L)&amp;\mathrm{if~}L&lt;\eta\\0&amp;\mathrm{otherwise}&amp;\end{cases}\]</span></p>
<p>其中f是一个单调递减函数，<span class="math inline">\(\eta\)</span>
是阈值。</p>
<p>《Scaling laws for autoregressive generative
modeling》中提出，在固定的训练token数 <span class="math inline">\(D\)</span> 下，模型规模 <span class="math inline">\(N\)</span> 和预训练损失的关系是</p>
<p><span class="math display">\[L(N)=L_\infty+\left(\frac{N_0}N\right)^{\alpha_N}\]</span></p>
<p>其中 <span class="math inline">\(L_{\infty}\)</span> 是irreducible
loss，<span class="math inline">\(\alpha_{N}\)</span> 是固定的系数。</p>
<p>把上面两个式子结合起来，就有</p>
<p><span class="math display">\[\begin{cases}f\left(L_\infty+\left(\frac{N_0}N\right)^{\alpha_N}\right)&amp;\text{if
}N\geq
N_0\cdot\left(\eta-L_\infty\right)^{-\frac1{\alpha_N}}\\0&amp;\text{otherwise}&amp;\end{cases}\]</span></p>
<p>当模型规模小于 <span class="math inline">\(N_0\cdot(\eta-L_\infty)^{-1/\alpha_N}\)</span>
这个阈值时，normalized
performance为0；当模型规模超过这个阈值时，模型规模的增长带来了预训练loss的下降，从而带来了normalized
performance的提升。</p>
<h1 id="小结">小结</h1>
<p>通过预训练loss来预测下游任务的提升，这点用在预训练模型的分析和优化上还是有些帮助的。比如在loss较高的时候，在下游任务上的效果的变化可能更多是随机波动而不是真正的提升。</p>
<p>不过文中只对一个model
family做了实验，而loss和模型结构，词表等都有关系，因此还需要进一步的探索。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a><br>
<a href="https://www.linsight.cn/dd614e12.html">大模型算法题(7)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】Understanding Emergent Abilities of Language Models from the
Loss Perspective https://arxiv.org/abs/2403.15796</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>涌现能力</tag>
      </tags>
  </entry>
  <entry>
    <title>昆仑万维-SkyworkMoE</title>
    <url>/1d5bcd45.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>之前我们对比较热门的十个MoE工作进行了整理：<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a>。</p>
<p>最近昆仑万维开源了Skywork-MoE，一个总参数量为146B，激活参数量为22B的MoE模型。</p>
<p>Skywork-MoE技术报告中针对几个实操会遇到的问题做了一些实验，还是挺有借鉴意义的。</p>
<h1 id="skywork-moe模型">Skywork-MoE模型</h1>
<p>分析之前，先看下Skywork-MoE的模型设计：<br>
- Llama-like architecture<br>
- RoPE<br>
- RMSNorm<br>
- SwiGLU activation function</p>
<p>其他参数如下表</p>
<img src="/1d5bcd45/structure.png" class title="模型结构">
<p>Skywork-MoE共有146B参数，16个专家，激活参数量为22B。</p>
<p>训练集群用到了192个NVIDIAHGX-A800节点，共1536个A800-80G显卡。</p>
<p>训练框架是基于Megatron搭建的，data
parallelism开了ZeRO-1优化，训练速度能达到690token/GPU/second，GPU利用率是38%。</p>
<h1 id="训练路线选择">训练路线选择</h1>
<p>在当前的情况下，要训练一个MoE模型有两条路线可以选择：<br>
-
upcycling：用一个dense模型做MoE模型的初始化，进行一定的继续预训练。这样的好处是MoE模型能在一个比较好的初始化点开始训练，直觉上这样的模型应该收敛得相对比较快，成本也比较低。存在的问题是dense模型的选择可能存在一些权衡取舍，且从dense进行初始化可能对最终效果存在负面影响。<br>
- from
scratch：直接随机初始化一个MoE模型，从零开始训练。这样成本相比upcycling就比较高，但是效果可能比upcycling更好。</p>
<p>当然还有一种方法是，先从零训一个dense模型，再从这个dense模型训练一个MoE模型。但是后面的实验告诉我们，如果这个dense模型纯粹是为最终的MoE模型服务的话，那这种方法是费力不讨好的。</p>
<p>要决定是upcycling还是from
scratch，需要看现有的dense模型的水平，以及MoE模型的训练预算。首先如果预算根本支持不了MoE模型这个规模的训练，那我们当然只能选择upcycling。只有当预算充足，我们才有机会选择from
scratch这条路。而如果没有可用的dense模型，那就只能选择from scratch。</p>
<p>前面我们从直觉上认为from
scratch效果会更好，下面就从实验上来验证这个想法。</p>
<p>首先，在300B
token的数据上训练一个0.3B的dense模型，并分别取100B和300B时的checkpoint作为后续实验的起始点。这两个checkpoint起个名字叫"checkpoint-100B"和"checkpoint-300B"。</p>
<p>然后在相同结构下，把dense模型扩成有8个专家的MoE模型，并使用3种不同的初始化策略：from-scratch
/ checkpoint-100B / checkpoint-300B。</p>
<p>假设我们现在有两种MoE模型的训练预算，100B和300B（token）。</p>
<p>对于100B训练预算，对比以下几个模型</p>
<img src="/1d5bcd45/100B.png" class title="100B">
<p>同样地，对于300B预算的情况，训练了init_scratch-decay_300b和init_100b-decay_300b。另外还训练了一个init_300b-3xLR，相比init_300b-const提升了3倍的学习率，用于验证学习率的影响。</p>
<p>各个模型的训练结果如下图所示</p>
<img src="/1d5bcd45/exp_1.png" class title="实验">
<p>左图：在100B的训练预算下，from
scratch已经可以和从dense初始化的MoE模型loss持平，甚至比init_300b-const好。报告认为init_300b-const效果不好有一部分原因是学习率太小了。</p>
<p>中图：在300B的训练预算下，from
scratch模型已经超越所有其他模型。另外学习率最小的模型表现最差。</p>
<p>右图：把中图几个模型的expert similarity画出来，发现expert
similarity越低的模型，表现越好，并且对于upcycling的模型，expert
similarity在训练过程中越来越低，对应着模型效果越来越好。而from
scratch的模型的expert
similarity基本上一直保持为0，这也说明从dense模型初始化会使得专家多样性比较弱，从而使得模型收敛到suboptimal的点。</p>
<p>据此，报告给出路线选择的经验法则。假设 <span class="math inline">\(C_{\mathrm{dense}}\)</span>
是dense模型的训练成本，<span class="math inline">\(C_{\mathrm{MoE}}\)</span>
是MoE模型的训练预算，那么：<br>
- 如果 <span class="math inline">\(C_{\mathrm{MoE}}\ll
C_{\mathrm{dense}}\)</span>，选择upcycling，upcycling能更好利用上dense模型已投入的成本。<br>
- 如果 <span class="math inline">\(C_{\mathrm{MoE}}\geq2C_{\mathrm{dense}}\)</span>，选择from
scratch，能获得更好的效果。</p>
<p>另外，学习率的影响很大，这个要仔细设置。</p>
<h1 id="模型设计">模型设计</h1>
<p>模型设计上，Skywork-MoE提出了两个主要的改进：gating logit
normalization和adaptive auxiliary loss coefficients。</p>
<h2 id="gating-logit-normalization">gating logit normalization</h2>
<p>研究人员在训练过程中发现有一个现象，那就是有时gating
layer会输出熵很高的分布，也就是分配给各个专家的概率接近平均分布。这样的结果就是MoE层的输出基本上相当于是各个专家的平均值，而不是一个weighted
average。</p>
<p>而出现这种现象说明gating
layer没有很好地区分各个专家，无法把相应的输入分配给最合适的专家。</p>
<p>针对这个问题，Skywork-MoE给出的方法就是在gating
layer的softmax之前引入一个normalization step，如下式</p>
<p><span class="math display">\[\begin{aligned}&amp;z=Wx+b\\&amp;\tilde{z}=\lambda\cdot\frac{z-\mu}{\sigma}\\&amp;g=\operatorname{softmax}(\tilde{z})\end{aligned}\]</span></p>
<p>其中 <span class="math inline">\(\lambda\)</span> 是一个超参。</p>
<p>这样归一化之后我们就得到一个均值为0，而方差受 <span class="math inline">\(\lambda\)</span> 控制的向量。大的 <span class="math inline">\(\lambda\)</span>
值会使得softmax之后的分布更显著，更不均匀。这就相当于给softmax加上一个放大器，把原本不显著的差异进行放大。</p>
<p>为了验证这个设计的有效性，Skywork-MoE在2.5B参数16个专家的MoE模型上，分别使用和不使用gating
logit normalization进行了训练。</p>
<p>两个模型的gating分布差异如下图所示，normalization确实可以增大各个专家分配到的概率的差异。</p>
<img src="/1d5bcd45/gate_dist.png" class title="gating distribution">
<p>使用了normalization的模型在training loss和token drop
rate上都有更好的表现，如下图所示。</p>
<img src="/1d5bcd45/normaization.png" class title="gating logit normalization">
<p>而统计gating
layer输出的分布中的Max1/Max2和Max2/Max3比值也同样说明了各个expert被更有效地区分开了。</p>
<p>在千亿Skywork-MoE模型的训练中，使用了 <span class="math inline">\(\lambda=1\)</span>。</p>
<h2 id="adaptive-auxiliary-loss-coefficients">adaptive auxiliary loss
coefficients</h2>
<p>一般来说，MoE模型在训练中都会加入一个auxiliary
loss，帮助平衡专家的选择分布，提升训练效率，也增强专家的多样性。对于有M个MoE层的模型，最终loss如下式所示。</p>
<p><span class="math display">\[\mathcal{L}_{\mathrm{total}}=\mathcal{L}_{\mathrm{ce}}+\sum_{l=1}^M\alpha\mathcal{L}_{\mathrm{aux}}^{(l)}\]</span></p>
<p>每个MoE层都有对应的auxiliary loss。</p>
<p>Skywork-MoE认为每层的auxiliary loss的系数 <span class="math inline">\(\alpha\)</span>
不一定要相同，并且随着训练进行，在gating的平衡已经比较好的时候，可以放宽auxiliary
loss的限制强度，避免影响模型的最终效果。</p>
<p>基于这两个想法，Skywork-MoE提出adaptive auxiliary loss
coefficients。</p>
<p>每个MoE层的auxiliary
loss有自己的系数，而这个系数和当前这个MoE层的token drop
rate联系了起来。大的token drop
rate表示gating的分配不平衡，因此要加强auxiliary
loss的约束，反之则可以减小约束。</p>
<p>对于第l个MoE层，在第i个step的时候，auxiliary loss的系数计算如下</p>
<p><span class="math display">\[\begin{array}{rcl}\hat\alpha_{i+1}^{(l)}&amp;=&amp;f(d_i^{(l)})\\\alpha_{i+1}^{(l)}&amp;=&amp;\beta\alpha_i^{(l)}+(1-\beta)\hat\alpha_{i+1}^{(l)}\end{array}\]</span></p>
<p>其中d表示token drop rate，f是一个单调递增函数。<span class="math inline">\(\alpha\)</span> 会随着训练，通过moving
average更新。<span class="math inline">\(\beta\)</span> 是moving
average的权重，是一个超参。</p>
<p>实际实现中，f设计成：</p>
<p><span class="math display">\[f(d)=\left\{\begin{array}{ll}\xi
d&amp;\text{if
}d\leq\alpha_{\text{max}}/\xi\\\alpha_{\text{max}}&amp;\text{if
}d&gt;\alpha_{\text{max}}/\xi\end{array}\right.\]</span></p>
<p><span class="math inline">\(\xi\)</span> 表示auxiliary loss
coefficient对token drop rate的敏感程度。</p>
<p>最终训练中，各个超参的设置为：<br>
- <span class="math inline">\(\xi=1/5\)</span><br>
- <span class="math inline">\(\alpha_{\max}=0.01\)</span><br>
- <span class="math inline">\(\beta=0.99\)</span></p>
<h1 id="其他尝试">其他尝试</h1>
<p>报告中还给出了训练中一些其他尝试，虽然没有直接效果，但是也有参考意义。</p>
<h2 id="学习率">学习率</h2>
<p>MoE模型由于路由策略的存在，每个专家平均接受到的输入token数比global
batch size要小。</p>
<p>假设共有n个专家，激活专家数为k，那么平均每个专家接受到的输入只有模型输入的k/n。</p>
<p>而有效batch
size的减小意味着更容易引入noise，对此一般的应对方案就是减小learning
rate，可以进行linear scaling（<span class="math inline">\(k/n\)</span>），或者square root scaling（<span class="math inline">\(\sqrt{k/n}\)</span>）。</p>
<p>那么减小learning
rate是否能提升效果呢？Skywork-MoE用一个1.8B参数，共32个专家，激活专家数为2的模型，按square
root scaling，进行了以下3个实验</p>
<img src="/1d5bcd45/lr_exp.png" class title="lr实验">
<p>所有模型在训了300B数据之后，lr会降到peak
lr的10%，然后会再继续训10B，在这个过程里lr逐渐降为0。</p>
<p>训练的loss如下图</p>
<img src="/1d5bcd45/lr_result.png" class title="lr实验">
<p>虽然在300B的训练量下，减小lr有一点收益，但是随着最后10B的训练，三个模型都收敛到同样的loss。这说明前面的loss差异并不是不可弥补的，更可能只是因为在300B时三个模型的lr
decay到不同的绝对值而已。</p>
<p>这也说明根据专家数量减少MoE模型的训练学习率并没有太大必要。</p>
<h2 id="多样化初始化">多样化初始化</h2>
<p>前面提到，用一个dense模型进行初始化，会导致各个专家相似度过高，从而损害MoE模型的效果。那么我们自然想到用多样化的几个dense模型进行MoE的初始化，效果是不是会更好。</p>
<p>Skywork-MoE对此进行了实验。把原始dense模型分别用不同的100B数据进行训练，从而获得多个dense模型，并用这些多样化的dense模型初始化MoE模型。</p>
<p>具体来说，基于原始dense模型 <span class="math inline">\(M_{\mathrm{base}}\)</span>，用了中文、英文、代码三个不同的100B数据集进行训练，获得
<span class="math inline">\(M_{\mathrm{cn}},M_{\mathrm{en}},M_{\mathrm{code}}\)</span>
三个dense模型。之后把 <span class="math inline">\(M_{\mathrm{cn}}\)</span> 复制3份，<span class="math inline">\(M_{\mathrm{en}}\)</span> 复制3份，<span class="math inline">\(M_{\mathrm{code}}\)</span> 复制1份，<span class="math inline">\(M_{\mathrm{base}}\)</span>
复制1份，共同初始化一个有8个专家的MoE模型。</p>
<p>多样化和无多样化的初始化方法，训练loss对比如下</p>
<img src="/1d5bcd45/diff_dense.png" class title="初始化实验">
<p>可以看到多样化的初始化方法确实有一点收益，不过随着训练进行，差异在逐渐减小。</p>
<p>经过90B数据的训练之后，二者的loss只有不到0.01的差距。相较于dense模型的多次继续预训练成本，这个收益并不明显，因此Skywork-MoE最终没有采用多样化的初始化方法。</p>
<h1 id="效果">效果</h1>
<p>146B参数的Skywork-MoE是从Skywork-13B初始化而来的。</p>
<p>训练数据使用了SkyPile中的一部分数据，再加上一批合成数据。</p>
<p>中文、英文、代码数据的比例为7:2:1。</p>
<p>Skywork-MoE在和一些主流模型，在一些benchmark上的对比如下</p>
<img src="/1d5bcd45/perf.png" class title="效果">
<p>基本上达到了同归模型比较好的效果。</p>
<h1 id="小结">小结</h1>
<p>Skywork-MoE开源了一个效果不错的MoE模型，同时对于初始化策略的探索也颇有借鉴意义。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】Skywork-MoE: A Deep Dive into Training Techniques for
Mixture-of-Experts Language Models
https://github.com/SkyworkAI/Skywork-MoE/blob/main/skywork-moe-tech-report.pdf</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>MoE</tag>
      </tags>
  </entry>
  <entry>
    <title>成本10w刀的JetMoE</title>
    <url>/f3acf042.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>JetMoE是由MIT、Princeton等几个学术机构发布的MoE模型，其总参数量为8B，激活参数量为2B。</p>
<p>训练JetMoE的总花费约为10w美元，而JetMoE在各个benchmark上都有不错的效果，这样看训练这个模型算是比较经济实惠的了。</p>
<p>MoE的基础内容可以看之前梳理的 <a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a>。</p>
<h1 id="模型设计">模型设计</h1>
<h2 id="结构">结构</h2>
<p>在模型结构上，和目前一些主流的模型如Deepseek MoE、Mixtral
8x7B、Qwen-MoE等有点不同，JetMoE不仅在FFN层应用Sparsely-gated
Mixtureof-Experts（SMOE）的设计，而且参考了《Moduleformer: Learning
modular large language models from uncurated
data》的做法，把attention层也设计成了混合专家的结构，如下图所示。</p>
<img src="/f3acf042/structure.png" class title="结构">
<p>attention层的混合专家结构也叫MoA（ Mixture of Attention heads
(MoA)，是由《Mixture of Attention Heads: Selecting Attention Heads Per
Token》提出的。</p>
<p>MoA和FFN层的MoE一样，每个attention层包含多个attention
expert。而每个attention expert e包括4个形状为 <span class="math inline">\(\mathbf{R}^{D_{emb}\times D_{att}}\)</span>
的矩阵： <span class="math inline">\(\mathbf{W}_q^e,\mathbf{W}_k,\mathbf{W}_v,\mathbf{W}_o^e\)</span>。其中
<span class="math inline">\(D_{att}=H\times
D_{head}\)</span>，H是每个attention expert的attention
head数量。每个attention expert内部和常规的注意力层是一样的。</p>
<p>每层attention expert中的 <span class="math inline">\(\mathbf{W}_k\)</span> 和 <span class="math inline">\(\mathbf{W}_v\)</span>
这两个矩阵的参数在同个attention层的多个expert之间共享，这样可以减少一些参数量和计算量，提升计算效率。而每个attention
expert保留各自的 <span class="math inline">\(\mathbf{W}_q^e\)</span> 和
<span class="math inline">\(\mathbf{W}_o^e\)</span>。</p>
<p>对于一个输入的vector x，首先用2个共享的矩阵获得k和v</p>
<p><span class="math display">\[\begin{aligned}\mathbf{k}&amp;=\mathbf{W}_{k}\mathbf{x}\\\mathbf{v}&amp;=\mathbf{W}_{v}\mathbf{x}\end{aligned}\]</span></p>
<p>而在gating function选择了expert之后，再在attention
expert内部进行标准的attention计算：</p>
<p><span class="math display">\[\begin{aligned}&amp;\mathbf{q}_{e}=\mathbf{W}_{q}^{e}\mathbf{x}\\&amp;\mathbf{a}_{e}=\mathrm{МНА}\left(\mathbf{q}_{e},\mathbf{k},\mathbf{v}\right)\\&amp;\mathbf{o}_{e}=\mathbf{W}_{o}^{e}\mathbf{a}\end{aligned}\]</span></p>
<p>JetMoE的FFN层的设计和gating的设计就是常规的top-k gating
MoE，就不再赘述。</p>
<p>JetMoE的具体模型参数如下</p>
<img src="/f3acf042/model_param.png" class title="模型参数">
<p>每层有8个expert，每个token激活2个expert。</p>
<h2 id="负载均衡">负载均衡</h2>
<p>在负载均衡上，参考Switch Transformer，加入了frequency-based auxiliary
loss：</p>
<p><span class="math display">\[loss_b=N\sum_{i=1}^Nf_iP_i\]</span></p>
<p>其中N是expert数量，<span class="math inline">\(f_i\)</span>
是分配给expert i的token占比，<span class="math inline">\(P_i\)</span>
是router分配给expert i的概率占比。</p>
<p>此外还加入了ST-MoE中的z-loss来提升训练稳定性：</p>
<p><span class="math display">\[loss_z=\frac1B\sum_{i=1}^B\left(\log\sum_{j=1}^N\exp(x_j^i)\right)^2\]</span></p>
<p>x是router给出的logits，B是token数。</p>
<p>通过两个超参把这两个负载平衡的loss加入到训练loss中</p>
<p><span class="math display">\[loss=loss_{lm}+\alpha loss_b+\beta
loss_z\]</span></p>
<p>训练中 <span class="math inline">\(\alpha=0.01\)</span>，<span class="math inline">\(\beta=0.001\)</span>。</p>
<h1 id="训练数据">训练数据</h1>
<p>JetMoE预训练数据使用了真实数据和合成数据两种。</p>
<p>真实数据：<br>
- RefinedWeb：从总共5T的token里抽取了600B来训练<br>
- StarCoder：包含86种代码语言<br>
- Dolma：包含3T token的英文数据集<br>
- The Pile：825GB的英文数据集<br>
-
其他：还使用了Proof-Pile-2、OpenWebMath、StackMathQA、OpenAssistant、xP3x、CommitPackFT这些规模比较小、质量比较高的数据集。</p>
<p>合成数据：<br>
- OpenHermes 2.5<br>
- UltraTextbooks<br>
- UltraChat 200k<br>
- 其他：还使用了TemplateGSM、Magicoder-Evol-110K、Evol-Code
Alpaca、Code-290k-ShareGPT这些规模比较小、质量比较高的数据集。</p>
<h1 id="训练">训练</h1>
<p>JetMoE基于Megatron框架进行训练，仅使用pipeline parallelism而不expert
parallelism。训练过程用了96个H100，消耗约30,000个GPU
hour，训练了大概1.25T token的数据。</p>
<p>一些训练设置：<br>
- 使用AdamW优化器<br>
- maximum learning rate = 5e-4<br>
- batch size = 4M<br>
- sequence length = 4096<br>
- learning rate schedule = WSD，warmup = 10B token，decay = 250B
token</p>
<p>参考MiniCPM的做法，把训练分为两个阶段： - phase1：warmup and stable
learning rate；使用的数据集包括RefinedWeb, Starcoder, The Pile, peS2o
from Dolma, and OpenWebMath<br>
- phase2:decay learning rate；使用了更多的高质量数据。</p>
<p>phase1和phase2的具体数据混合情况如下</p>
<img src="/f3acf042/data1.png" class title="数据">
<img src="/f3acf042/data2.png" class title="数据">
<h1 id="alignment">Alignment</h1>
<p>JetMoE用Distilled Supervised
Fine-Tuning（dSFT）的方法对模型进行微调。dSFT就是用prompt获取更强模型的应答结果，用来训练别的模型。</p>
<p>JetMoE使用Zephyr-7b-beta的chat
template获取GPT-4和Claude的答案用来训练JetMoE，所用的数据有：<br>
- UltraChat 200k<br>
- Airoboros-3.2<br>
- Code-Feedback<br>
- Orca-math-word-problems-200k<br>
- SystemChat<br>
- Capybara</p>
<p>训练配置：<br>
- lr = 2e-5<br>
- batch size = 128<br>
- epoch = 3</p>
<p>此外，在SFT的基础上，还用了Distilled Direct Preference Optimization
(dDPO)进一步优化模型。</p>
<p>所用的数据集是UltraFeedback，包含了preference数据对。</p>
<p>训练配置：<br>
- lr = 5e-7<br>
- batch size = 128<br>
- epoch = 1</p>
<h1 id="效果">效果</h1>
<p>在各个benchmark的效果如下</p>
<img src="/f3acf042/evaluation.png" class title="评测">
<img src="/f3acf042/mtbench.png" class title="评测">
<h1 id="小结">小结</h1>
<p>JetMoE算是一次比较低成本的MoE训练实践，其中大部分的训练设置都是沿用了之前多个工作总结下来的经验。这些经验基本上可以保证训练不出什么大问题了，是相对比较成熟的了。</p>
<p>常规的内容之外，attention expert可能是一个可以探索的方向。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a><br>
<a href="https://www.linsight.cn/dd614e12.html">大模型算法题(7)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】JetMoE: Reaching Llama2 Performance with 0.1M Dollars
https://arxiv.org/abs/2404.07413</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>MoE</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型推理加速-MEDUSA</title>
    <url>/7bbe2df6.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>之前对speculative decoding的做法做了介绍：<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a>。</p>
<p>本篇介绍一下另外一个热门的解码加速算法，MEDUSA。MEDUSA在不同的训练方法下能提供×2.2~×2.8的解码加速效果。</p>
<h1 id="背景">背景</h1>
<p>自回归大模型推理下一个token的时候，需要依赖前面的结果。而在实际使用GPU进行计算时，需要将相关矩阵移至片上内存进行运算，而一般来说片上内存带宽比计算性能要低两个数量级，这就使得大模型推理是memory-bandwidth-bound的。</p>
<p>要解决这个问题，一个思路是increasing the arithmetic
intensity，即提高“浮点数计算量/数据传输量”这个比值，让数据传输不要成为瓶颈。另一个思路是reducing
the number of decoding steps。投机解码就属于后者。</p>
<p>不过投机解码有几个问题：<br>
- 一个好的draft model不容易获取：draft模型和原模型存在distribution
shift<br>
- 推理时有多个模型参与，在分布式系统上的部署难度增大</p>
<p>而MEDUSA相比投机解码，不需要新增一个模型，而是基于原模型进行并行推理，这样训练难度更低，部署也更容易。</p>
<h1 id="medusa">MEDUSA</h1>
<p>MEDUSA的大致思路是和投机解码类似：<br>
-
首先生成各个位置的候选token；MEDUSA通过接在原模型的多个解码头来获取多个位置的候选token<br>
-
把各个位置的候选token进行处理，选出一些候选序列，进行验证；MEDUSA通过tree
attention来处理<br>
- 最后通过typical acceptance选择最终输出的结果</p>
<p>MEDUSA的pipeline如下图所示</p>
<img src="/7bbe2df6/intro.png" class title="introduction">
<p>MEDUSA的这些分类头需要经过训练才能有比较好的预测效果。针对不同的条件，可以选择不同的训练方式：<br>
-
MEDUSA-1：冻结原模型的backbone（包括原模型的解码头），只训练增加的解码头。这种方案适用于计算资源比较少，或者不想影响原模型的效果的情况。还可以使用QLoRA对解码头进行训练，进一步节省memory和计算资源。<br>
-
MEDUSA-2：原模型和MEDUSA的解码头一起训练。MEDUSA-1这样的训练方法虽然可以节省资源，但是并不能最大程度发挥多个解码头的加速效果，而MEDUSA-2则可以进一步发挥MEDUSA解码头的提速能力。MEDUSA-2适用于计算资源充足，或者从Base模型进行SFT的场景。</p>
<p>另外，如果原模型的SFT数据集是available的，那可以直接进行训练。如果不能获得原模型的SFT数据，或者原模型是经过RLHF训练的，则可以通过self-distillation来获取MEDUSA
head的训练数据。</p>
<h1 id="模型设计medusa-heads">模型设计：MEDUSA HEADS</h1>
<p>先来看下第一步，MEDUSA的多个解码头是怎么给出各个位置的候选token的。</p>
<p>假设原始模型最后一层的hidden state在时间 <span class="math inline">\(t\)</span> 的输出是 <span class="math inline">\(h_{t}\)</span>，我们给模型额外加上 <span class="math inline">\(K\)</span> 个解码头。那么第 <span class="math inline">\(k\)</span> 个头就可以用来预测位置 <span class="math inline">\(t+k+1\)</span> 的输出token（这里 <span class="math inline">\(k\)</span> 的取值为 <span class="math inline">\(1\)</span> ~ <span class="math inline">\(K\)</span>）。这里注意原模型自己还有一个解码头，它依然用来预测位置
<span class="math inline">\(t+1\)</span> 的输出，相当于 <span class="math inline">\(k=0\)</span>。</p>
<p>把第 <span class="math inline">\(k\)</span>
个解码头在vocabulary上的输出分布写作 <span class="math inline">\(p_t^{(k)}\)</span>，其计算方式如下</p>
<p><span class="math display">\[\begin{aligned}p_t^{(k)}=\text{softmax}\left(W_2^{(k)}\cdot\left(\text{SiLU}(W_1^{(k)}\cdot
h_t)+h_t\right)\right),\\\mathrm{where~}W_2^{(k)}\in\mathbb{R}^{d\times
V},W_1^{(k)}\in\mathbb{R}^{d\times d}.\end{aligned}\]</span></p>
<p><span class="math inline">\(d\)</span> 是hidden
state的输出维度，<span class="math inline">\(V\)</span>
是词表大小。每个解码头其实就是一个FFN网络，实践上发现这样简单的设计已经有足够好的效果。</p>
<p>在初始化各个解码头的参数时，把 <span class="math inline">\(W_2^{(k)}\)</span>
初始化成和原模型的解码头一样，而把 <span class="math inline">\(W_1^{(k)}\)</span>
设置成0。这样能使得在一开始训练的时候，增加的这些解码头就有一定的预测能力。</p>
<p>这 <span class="math inline">\(K\)</span>
个新增的解码头直接在原模型的基础上进行训练，因此相比投机解码的draft
model，MEDUSA的训练方式缓解了distribution shift的问题。</p>
<h1 id="候选校验tree-attention">候选校验：TREE ATTENTION</h1>
<h2 id="cartesian-product">Cartesian product</h2>
<p>增加额外的解码头之后，模型每次前向推理都会给出 <span class="math inline">\(K+1\)</span> 个位置的候选token。</p>
<p>投机解码里是直接选出draft
model最有信心的一个候选序列给原模型进行验证。</p>
<p>显然，如果增加候选序列的数量，那么最终接受token的命中率就会提升，acceleration
rate（即每个decoding
step能获得的token数，不是实际解码时间）也就更高，但是验证更多候选序列也会带来额外的计算消耗。为了获得一个效果和性能比较好的平衡，MEDUSA使用tree
attention来同时对多个候选序列进行处理。</p>
<p>假设第 <span class="math inline">\(k\)</span>
个解码头给出的候选token数量是 <span class="math inline">\(s_k\)</span>
个，那么可以通过Cartesian
product来获取多个解码头组成的所有可能的候选序列，然后用tree
attention对所有候选序列进行验证。</p>
<p>对于两个解码头的情况，tree attention验证的示意图如下</p>
<img src="/7bbe2df6/tree_attention.png" class title="tree attention">
<p>通过使用这样的mask，我们可以在不扩大batch
size的情况下同时处理多个候选序列。（注意，这里要对各个候选token的位置编码进行处理。）</p>
<h2 id="更高效的tree-attention构建">更高效的tree attention构建</h2>
<p>上面这个例子使用了Cartesian
product对两个解码头的结果进行处理，获得所有候选序列。</p>
<p>但是如果解码头数量数量比较多，每个头给出的候选token也比较多，那么实际要验证的序列数量会极大地增长。</p>
<p>直觉上，这些解码头应该有不同的准确率，因此可以利用这一点来构建tree
attention，而不需要使用所有可能的排列组合。</p>
<p>具体来说，可以使用一个calibration dataset（比如Alpaca-eval
dataset）来获取不同解码头给出的各个token的准确率：把第 <span class="math inline">\(k\)</span> 个解码头给出的第 <span class="math inline">\(i\)</span> 个token的准确率记为 <span class="math inline">\(a_k^{(i)}\)</span>。</p>
<p>假设各个token的准确率之间是独立的，那么一个由 <span class="math inline">\([i_1,i_2,\cdots,i_k]\)</span>
构成的候选序列的准确率可以写作 <span class="math inline">\(\prod_{j=1}^ka_j^{(i_j)}\)</span>。</p>
<p>每个候选序列可以表示所构建的tree上的一条路径上所有的node（而不只是leaf
node，因为tree attention验证的时候会把路径上所有token都进行验证）。用
<span class="math inline">\(I\)</span>
表示候选序列的集合，那么集合里的候选序列的expectation of acceptance
length就表示为</p>
<p><span class="math display">\[\sum_{[i_1,i_2,\cdots,i_k]\in
I}\prod_{j=1}^ka_j^{(i_j)}\]</span></p>
<p>在构建tree的时候，优先加入当前有最大准确率的候选序列，直到tree的节点数量达到上限，这样能最大化expectation
of acceptance length，也就能最大化acceleration rate。</p>
<p>下图是一个按这种方法构建的tree的例子。可以看到这棵树向左偏，这是因为这个方法倾向于使用更高准确率的token。</p>
<img src="/7bbe2df6/construct_tree.png" class title="tree attention">
<h1 id="训练策略">训练策略</h1>
<p>MEDUSA的解码头需要进行训练。训练策略根据是否有“与模型输出分布对齐的训练数据”而有所不同。</p>
<h2 id="有训练数据">有训练数据</h2>
<p>MEDUSA-1冻结了原模型的参数，而只对新增的解码头进行训练。</p>
<p>第 <span class="math inline">\(k\)</span>
个解码头的训练loss可以写作</p>
<p><span class="math display">\[\mathcal{L}_k=-\log
p_t^{(k)}(y_{t+k+1})\]</span></p>
<p>总的训练loss为</p>
<p><span class="math display">\[\mathcal{L}_{\text{MEDUSA-l}}=\sum_{k=1}^K-\lambda_k\log
p_t^{(k)}(y_{t+k+1})\]</span></p>
<p>这里的 <span class="math inline">\(\lambda_{k}\)</span>
是每个解码头的缩放系数，是一系列超参。因为 <span class="math inline">\(k\)</span>
越大，对应解码头的预测难度越大，loss也就越大，为了防止靠后的解码头过分主导训练，因此使用一个缩放系数进行调整。</p>
<p>实际使用中，<span class="math inline">\(\lambda_{k}=0.8^{k}\)</span>。</p>
<p>训练时，由于冻结了原模型，因此可以对原模型的参数进行量化而不会对训练效果有明显影响，比如使用QLoRA。</p>
<p>MEDUSA-1冻结了原模型，比较适用于计算资源有限，或者希望保持原模型能力的场景。如果要进一步发挥MEDUSA多个解码头的加速效果，那就需要使用MEDUSA-2。</p>
<p>MEDUSA-2把原模型和多个解码头一起训练，因此各个解码头的准确率能达到更高的水平，acceleration
rate也更高。但是为了保持原模型的输出质量，需要使用以下三个措施。</p>
<p>（1）Combined loss</p>
<p>首先是加入原模型next-token
prediction的loss，即把原模型解码头的loss也加上，如下式</p>
<p><span class="math display">\[\mathcal{L}_{\text{MEDUSA-}2}=\mathcal{L}_{\text{LM}}+\lambda_0\mathcal{L}_{\text{MEDUSA-}1}\]</span></p>
<p><span class="math display">\[\mathcal{L}_{\text{LM}}=-\log
p_t^{(0)}(y_{t+1})\]</span></p>
<p>实际使用中，直接训练时 <span class="math inline">\(\lambda_0=0.2\)</span>，使用self-distillation时<span class="math inline">\(\lambda_0=0.01\)</span>。</p>
<p>（2）Differential learning rates</p>
<p>原模型已经是训练好了的，因此和新加入的解码头使用相同的学习率并不合适，因此可以让新的解码头使用更大的学习率，而原模型参数使用相对小的学习率。实践中把学习率差距设为4倍，比如分别使用2e-3和5e-4。</p>
<p>（3）Heads warmup</p>
<p>新加入的解码头在一开始训练会有比较大的loss，从而导致更大的梯度，有可能损害原模型的能力。</p>
<p>针对这个问题，可以使用two-stage
training的方式，先在MEDUSA-1的策略下训练解码头，然后再进行MEDUSA-2的训练。这其实相当于把
<span class="math inline">\(\lambda_0\)</span>
在训练过程中逐渐增大。two-stage training和逐渐增大 <span class="math inline">\(\lambda_0\)</span> 的方法在实践中都是可行的。</p>
<h2 id="self-distillation">SELF-DISTILLATION</h2>
<p>前面讲的这些训练方式都有一个前提，那就是有与模型输出分布对齐的训练数据可供使用。但是实际上这个前提并不总是成立。比如大部分开源模型并没有发布相应的SFT数据，或者模型使用了RLHF等对齐方式，而不是直接SFT。</p>
<p>解决方法是使用self-distillation：通过原模型为MEDUSA解码头生成训练数据。</p>
<p>首先选择一个和target
model的domain相近的数据集，然后把prompt输入给原模型，获得原模型的输出。对于对话模型，需要生成多轮对话，可以使用self-talk。</p>
<p>对于MEDUSA-1，这样生成的数据集已经够用，但是对于MEDUSA-2，这样的训练会降低生成质量。</p>
<p>实际上，即使不进行MEDUSA解码头的训练，只用生成的数据训练原模型，原模型的效果也会变差。因此MEDUSA-2训练的时候，原模型的训练不应该直接使用ground
truth进行训练，而是进行蒸馏，按下式计算损失</p>
<p><span class="math display">\[\mathcal{L}_{\text{LM-distill}}=KL(p_{\text{original},t}^{(0)}||p_t^{(0)})\]</span></p>
<h1 id="接受策略typical-acceptance">接受策略：TYPICAL ACCEPTANCE</h1>
<p>投机解码随着temperature的提升，命中率会降低。因为temperature提升，draft
model所选择的候选token的多样性就增大，也就降低了命中原模型token，从而被接受的概率。</p>
<p>但是这种特性并不合理。通常更高的temperature参数一般对应更强的creativity特性，因此合理的情况应该是随着温度提高，候选序列有更大的概率被接受。这和投机解码的情况是相反的。</p>
<p>另外，MEDUSA认为候选序列的分布没有必要完全match原模型的分布。我们要做的应该是选出typical的候选，也就是只要候选序列不是极不可能的结果，就可以被接受。</p>
<p>给定context <span class="math inline">\(x_1,x_2,\cdots,x_n\)</span>，候选序列 <span class="math inline">\((x_{n+1},x_{n+2},\cdots,x_{n+K+1})\)</span>，我们按以下这个条件来接受候选token</p>
<p><span class="math display">\[\begin{aligned}p_{\text{original}}(x_{n+k}|x_1,x_2,\cdots,x_{n+k-1})&amp;&gt;\\\min\left(\epsilon,\delta\exp\left(-H(p_{\text{original}}(\cdot|x_1,x_2,\cdots,x_{n+k-1})))\right)\right),\end{aligned}\]</span></p>
<p>其中 <span class="math inline">\(H(\cdot)\)</span> 表示entropy
function，<span class="math inline">\(\epsilon,\delta\)</span>
分别是hard threshold和entropy-dependent threshold。</p>
<p>两个threshold的解释：（1）<span class="math inline">\(\epsilon\)</span>
保证所选的token的概率不能低于特定值，保证不选出可能性很低的结果（2）当一个位置的多个候选token的entropy较高时，表示多个候选都是reasonable的，那么
<span class="math inline">\(\delta\)</span>
和exp(entropy)的乘积会更小，各个token都有更大的机会被接受。</p>
<p>当temperatrue为0的时候，相当于贪心解码，这个时候只有概率最大那个token有非0概率。随着温度提升，其他token的概率也提升，因此它们也有一定的机会被接受。随着温度提升，这些token被接受的概率会增大。</p>
<p>最后选择被接受的解码长度最长的候选序列作为最终结果。</p>
<h1 id="消融实验">消融实验</h1>
<h2 id="configuration-of-tree-attention">CONFIGURATION OF TREE
ATTENTION</h2>
<p>对比通过准确率构建tree attention的方式，和随机构建tree
attention的方式，结果如下</p>
<img src="/7bbe2df6/tree_attention_exp.png" class title="消融实验">
<p>基于准确率构建的tree attention有更高的acceleration rate。</p>
<p>但随着候选token数量的增加，两种方式的实际速度反而有所下降，因为更多的候选token引入了额外的计算成本。</p>
<h2 id="thresholds-of-typical-acceptance">THRESHOLDS OF TYPICAL
ACCEPTANCE</h2>
<p>随着 $$ 增加，输出质量得到提升，但代价是acceleration
rate降低，如下图</p>
<img src="/7bbe2df6/threshold.png" class title="消融实验">
<h2 id="各环节对速度的影响">各环节对速度的影响</h2>
<p>各个技术优化点对速度的影响如下表</p>
<img src="/7bbe2df6/speed.png" class title="消融实验">
<h1 id="小结">小结</h1>
<p>MEDUSA引入了tree attention、typical
acceptance的做法，在加速比上相比投机解码有进一步提升。</p>
<p>但是MEDUSA不保证解码结果和原模型一致，因此应该更适用于对模型生成质量的没有那么严格要求的场景。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a><br>
<a href="http://www.linsight.cn/7c04944d.html">大模型算法题(6)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】MEDUSA: Simple LLM Inference Acceleration Framework with
Multiple Decoding Heads https://arxiv.org/abs/2401.10774</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>推理加速</tag>
      </tags>
  </entry>
  <entry>
    <title>从代码实现看normalization-到底做了什么</title>
    <url>/b70b4a2d.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>之前在<a href="http://www.linsight.cn/6a40bfa5.html">《transformer中normalization的二三事》</a>从思路上梳理了关于常用的normalization的内容。发出之后收到了一些反馈，关于这些norm在实际使用中是怎么实现的，有一些疑问。</p>
<p>因此本篇从实现的角度，来看下这些norm在不同的场景下，到底做了什么。</p>
<p>代码已上传至<a href="https://github.com/Saicat/normalization_exp">https://github.com/Saicat/normalization_exp</a></p>
<h1 id="二维数据">二维数据</h1>
<p>先看下二维数据的情况下normalization是怎么做的。二维数据一般可以对应到神经网络中的全连接层，比如CNN中分类网络最后几个特征层。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># epsilon</span></span><br><span class="line">eps = <span class="number">1e-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个随机二维输入</span></span><br><span class="line">batch_size = <span class="number">3</span></span><br><span class="line">feature_num = <span class="number">4</span></span><br><span class="line">torch.manual_seed(<span class="number">0</span>)  <span class="comment"># 设置随机种子，方便复现</span></span><br><span class="line">inputs = torch.randn(batch_size, feature_num)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;二维输入:\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>
<p>这里定义了一个3×4的矩阵，相当于batch
size=3，特征向量维度为4。得到的随机二维输入是</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">二维输入:</span><br><span class="line"> tensor([[ <span class="number">1.5410</span>, -<span class="number">0.2934</span>, -<span class="number">2.1788</span>,  <span class="number">0.5684</span>],</span><br><span class="line">        [-<span class="number">1.0845</span>, -<span class="number">1.3986</span>,  <span class="number">0.4033</span>,  <span class="number">0.8380</span>],</span><br><span class="line">        [-<span class="number">0.7193</span>, -<span class="number">0.4033</span>, -<span class="number">0.5966</span>,  <span class="number">0.1820</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="batchnorm">batchnorm</h2>
<p>用pytorch自带的BatchNorm1d对二维输入进行操作</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># torch自带的batchnorm</span></span><br><span class="line">torch_bn = nn.BatchNorm1d(num_features=feature_num, affine=<span class="literal">True</span>)  <span class="comment"># 注意完整的batchnorm要包括仿射变换</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果</span></span><br><span class="line"><span class="comment"># 手动改成别的值，用于对比包含仿射变换的效果</span></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)  <span class="comment"># 设置随机种子，方便复现</span></span><br><span class="line">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(feature_num))</span><br><span class="line">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(feature_num))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;weight:\n&#x27;</span>, torch_bn.weight)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;bias:\n&#x27;</span>, torch_bn.bias, <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">torch_normed = torch_bn(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;torch bn结果:\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>
<p>注意完整的batchnorm/layernorm等，是包括①归一化和②仿射变换（缩放+平移，也就是有可训练参数这部分）两步的。在BatchNorm接口中通过参数"affine"来决定是否进行放射变换。如果"affine"为False，相当于只是在某个维度上对数据进行了归一化处理。</p>
<p>而且pytorch中各种norm的接口初始化都把缩放系数初始化为1.0，平移系数初始化为0，相当于没有进行变换。为了把仿射变换的影响也一起对比，这里手动给缩放和平移系数都添加了一个随机数，变成如下数值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weight:</span><br><span class="line"> Parameter containing:</span><br><span class="line">tensor([<span class="number">0.6614</span>, <span class="number">0.2669</span>, <span class="number">0.0617</span>, <span class="number">0.6213</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">bias:</span><br><span class="line"> Parameter containing:</span><br><span class="line">tensor([-<span class="number">0.4519</span>, -<span class="number">0.1661</span>, -<span class="number">1.5228</span>,  <span class="number">0.3817</span>], requires_grad=<span class="literal">True</span>) </span><br></pre></td></tr></table></figure>
<p>这里缩放系数weight和平移系数bias的维度都是4，对应特征向量的维度。</p>
<p>输入矩阵用官方接口batchnorm之后得到的结果如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch bn结果:</span><br><span class="line"> tensor([[ <span class="number">0.4756</span>,  <span class="number">0.0513</span>, -<span class="number">1.6033</span>,  <span class="number">0.4715</span>],</span><br><span class="line">        [-<span class="number">1.0197</span>, -<span class="number">0.5421</span>, -<span class="number">1.4535</span>,  <span class="number">1.0937</span>],</span><br><span class="line">        [-<span class="number">0.8117</span>, -<span class="number">0.0077</span>, -<span class="number">1.5115</span>, -<span class="number">0.4202</span>]],</span><br><span class="line">       grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>接下来手动实现batchnorm</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 手动bn</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算均值和标准差</span></span><br><span class="line">mean = torch.mean(inputs, dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;均值:\n&#x27;</span>, mean)</span><br><span class="line">std = torch.std(inputs, dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>, unbiased=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;标准差:\n&#x27;</span>, std, <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">manual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;手动bn结果:\n&#x27;</span>, manual_normed)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动操作和torch自带操作有点误差，&lt;1e-4</span></span><br><span class="line">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class="number">1e-4</span>, atol=<span class="number">1e-4</span>)</span><br><span class="line"><span class="built_in">print</span>(isclose)</span><br></pre></td></tr></table></figure>
<p>在dim=0这个维度上计算均值和标准差，即对整个batch内所有sample的同一个feature，进行操作，获得结果如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">均值:</span><br><span class="line"> tensor([[-<span class="number">0.0876</span>, -<span class="number">0.6985</span>, -<span class="number">0.7907</span>,  <span class="number">0.5295</span>]])</span><br><span class="line">标准差:</span><br><span class="line"> tensor([[<span class="number">1.1612</span>, <span class="number">0.4971</span>, <span class="number">1.0630</span>, <span class="number">0.2692</span>]]) </span><br></pre></td></tr></table></figure>
<p>均值和标准差的维度也是和特征向量的维度一致。这里计算mean和std的时候keepdim设置为True和False都可以，最后都会自动broadcast。</p>
<p>一个要注意的点是，计算std的时候unbiased要设置为False，表明这里是对标准差的有偏估计，否则算出来的结果和torch的batchnorm接口不一致。</p>
<p>用手动计算出来的均值和标准差对输入进行归一化，再进行放射变换，得到手动计算的batchnorm结果如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">手动bn结果:</span><br><span class="line"> tensor([[ <span class="number">0.4756</span>,  <span class="number">0.0514</span>, -<span class="number">1.6033</span>,  <span class="number">0.4715</span>],</span><br><span class="line">        [-<span class="number">1.0197</span>, -<span class="number">0.5421</span>, -<span class="number">1.4535</span>,  <span class="number">1.0937</span>],</span><br><span class="line">        [-<span class="number">0.8117</span>, -<span class="number">0.0077</span>, -<span class="number">1.5115</span>, -<span class="number">0.4202</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>这里用torch.isclose接口验证官方batchnorm和手动计算的batchnorm是否相同</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]])</span><br></pre></td></tr></table></figure>
<p>为什么没有用equal，因为发现两个结果会有一点点误差，相对误差大概在1e-5~1e-4之间，应该是因为使用的eps不同导致。</p>
<h2 id="layernorm">layernorm</h2>
<p>看下layernorm对于二维数据的操作，还是用同样的3×4的输入</p>
<p>使用torch官方接口</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># torch自带的layernorm</span></span><br><span class="line">torch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=<span class="literal">True</span>)  <span class="comment"># 注意完整的layernorm要包括仿射变换</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果</span></span><br><span class="line"><span class="comment"># 手动改成别的值，用于对比包含仿射变换的效果</span></span><br><span class="line">torch.manual_seed(<span class="number">2</span>)  <span class="comment"># 设置随机种子，方便复现</span></span><br><span class="line">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(feature_num))</span><br><span class="line">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(feature_num))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;weight:\n&#x27;</span>, torch_ln.weight)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;bias:\n&#x27;</span>, torch_ln.bias, <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">torch_normed = torch_ln(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;torch ln结果:\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>
<p>得到layernorm仿射变换的系数如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weight:</span><br><span class="line"> Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.3923</span>, -<span class="number">0.2236</span>, -<span class="number">0.3195</span>, -<span class="number">1.2050</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">bias:</span><br><span class="line"> Parameter containing:</span><br><span class="line">tensor([ <span class="number">1.0445</span>, -<span class="number">0.6332</span>,  <span class="number">0.5731</span>,  <span class="number">0.5409</span>], requires_grad=<span class="literal">True</span>) </span><br></pre></td></tr></table></figure>
<p>维度依然是和特征向量的维度一致。</p>
<p>官方layernorm的结果是这样的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch ln结果:</span><br><span class="line"> tensor([[ <span class="number">1.5120</span>, -<span class="number">0.6001</span>,  <span class="number">1.0604</span>, -<span class="number">0.0392</span>],</span><br><span class="line">        [ <span class="number">0.7249</span>, -<span class="number">0.3772</span>,  <span class="number">0.3331</span>, -<span class="number">0.9155</span>],</span><br><span class="line">        [ <span class="number">0.6645</span>, -<span class="number">0.6209</span>,  <span class="number">0.7693</span>, -<span class="number">1.4324</span>]],</span><br><span class="line">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>接下来手动实现一下，和官方结果作对比。</p>
<p>在dim=1计算均值和向量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 手动ln</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算均值</span></span><br><span class="line">mean = torch.mean(inputs, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;均值:\n&#x27;</span>, mean)</span><br><span class="line">std = torch.std(inputs, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>, unbiased=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;标准差:\n&#x27;</span>, std, <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">manual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;手动ln结果:\n&#x27;</span>, manual_normed)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动操作和torch自带操作有点误差，&lt;1e-4</span></span><br><span class="line">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class="number">1e-4</span>, atol=<span class="number">1e-4</span>)</span><br><span class="line"><span class="built_in">print</span>(isclose)</span><br></pre></td></tr></table></figure>
<p>得到的均值和标准差是这样的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">均值:</span><br><span class="line"> tensor([[-<span class="number">0.0907</span>],</span><br><span class="line">        [-<span class="number">0.3104</span>],</span><br><span class="line">        [-<span class="number">0.3843</span>]])</span><br><span class="line">标准差:</span><br><span class="line"> tensor([[<span class="number">1.3691</span>],</span><br><span class="line">        [<span class="number">0.9502</span>],</span><br><span class="line">        [<span class="number">0.3458</span>]]) </span><br></pre></td></tr></table></figure>
<p>对输入进行归一化和仿射变换，结果如下，和官方接口结果一致</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">手动ln结果:</span><br><span class="line"> tensor([[ <span class="number">1.5120</span>, -<span class="number">0.6001</span>,  <span class="number">1.0604</span>, -<span class="number">0.0392</span>],</span><br><span class="line">        [ <span class="number">0.7249</span>, -<span class="number">0.3772</span>,  <span class="number">0.3331</span>, -<span class="number">0.9155</span>],</span><br><span class="line">        [ <span class="number">0.6645</span>, -<span class="number">0.6209</span>,  <span class="number">0.7693</span>, -<span class="number">1.4325</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">验证结果:</span><br><span class="line"> tensor([[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="对比">对比</h2>
<p>对于二维输入，batchnorm和layernorm在做第①步归一化的时候，方向如下图</p>
<img src="/b70b4a2d/bn_and_ln.png" class title="bn和ln">
<p>batchnorm在dim=0，即batch方向操作；而layernorm在dim=1，即特征向量内部进行操作。</p>
<p>但是无论是batchnorm还是layernorm，在做仿射变换的时候，使用的系数形状都和输入的特征向量相同，可以认为在放射变化这一步上，二者的操作是一样。</p>
<h1 id="cv数据">CV数据</h1>
<p>再看下CV场景下的情况。</p>
<p>CV数据形状一般为[N,C,H,W]，N为batch
size，C为channel即特征数，H和W分别是feature
map的高和宽。先定义一个CV输入数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义一个随机四维输入，[N,C,H,W]</span></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line">channel = <span class="number">2</span></span><br><span class="line">height = <span class="number">2</span></span><br><span class="line">width = <span class="number">3</span></span><br><span class="line">torch.manual_seed(<span class="number">3</span>)  <span class="comment"># 设置随机种子，方便复现</span></span><br><span class="line">inputs = torch.randn(batch_size, channel, height, width)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;四维输入:\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>
<p>输入如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">四维输入:</span><br><span class="line"> tensor([[[[-<span class="number">0.0766</span>,  <span class="number">0.3599</span>, -<span class="number">0.7820</span>],</span><br><span class="line">          [ <span class="number">0.0715</span>,  <span class="number">0.6648</span>, -<span class="number">0.2868</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">1.6206</span>, -<span class="number">1.5967</span>,  <span class="number">0.4046</span>],</span><br><span class="line">          [ <span class="number">0.6113</span>,  <span class="number">0.7604</span>, -<span class="number">0.0336</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[-<span class="number">0.3448</span>,  <span class="number">0.4937</span>, -<span class="number">0.0776</span>],</span><br><span class="line">          [-<span class="number">1.8054</span>,  <span class="number">0.4851</span>,  <span class="number">0.2052</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.3384</span>,  <span class="number">1.3528</span>,  <span class="number">0.3736</span>],</span><br><span class="line">          [ <span class="number">0.0134</span>,  <span class="number">0.7737</span>, -<span class="number">0.1092</span>]]]])</span><br></pre></td></tr></table></figure>
<h2 id="batchnorm-1">batchnorm</h2>
<p>图像数据需要用BatchNorm2d，设置的特征数为channel</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># torch自带的batchnorm</span></span><br><span class="line">torch_bn = nn.BatchNorm2d(num_features=channel, affine=<span class="literal">True</span>)  <span class="comment"># 注意完整的batchnorm要包括仿射变换</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果</span></span><br><span class="line"><span class="comment"># 手动改成别的值，用于对比包含仿射变换的效果</span></span><br><span class="line">torch.manual_seed(<span class="number">4</span>)  <span class="comment"># 设置随机种子，方便复现</span></span><br><span class="line">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(channel))</span><br><span class="line">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(channel))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;weight:\n&#x27;</span>, torch_bn.weight)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;bias:\n&#x27;</span>, torch_bn.bias, <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">torch_normed = torch_bn(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;torch bn结果:\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>
<p>仿射变换的参数如下，形状和channel数是一致的，和二维数据的情况一样。这里同样手动给缩放和平移系数加了个随机数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weight:</span><br><span class="line"> Parameter containing:</span><br><span class="line">tensor([-<span class="number">1.6053</span>,  <span class="number">0.2325</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">bias:</span><br><span class="line"> Parameter containing:</span><br><span class="line">tensor([<span class="number">2.2399</span>, <span class="number">0.8473</span>], requires_grad=<span class="literal">True</span>) </span><br></pre></td></tr></table></figure>
<p>用torch官方batchnorm2d得到的结果是</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch bn结果:</span><br><span class="line"> tensor([[[[<span class="number">2.2043</span>, <span class="number">1.1275</span>, <span class="number">3.9442</span>],</span><br><span class="line">          [<span class="number">1.8388</span>, <span class="number">0.3753</span>, <span class="number">2.7226</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">1.2185</span>, <span class="number">0.2591</span>, <span class="number">0.8559</span>],</span><br><span class="line">          [<span class="number">0.9175</span>, <span class="number">0.9620</span>, <span class="number">0.7252</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[<span class="number">2.8658</span>, <span class="number">0.7975</span>, <span class="number">2.2066</span>],</span><br><span class="line">          [<span class="number">6.4684</span>, <span class="number">0.8186</span>, <span class="number">1.5090</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">0.8362</span>, <span class="number">1.1387</span>, <span class="number">0.8467</span>],</span><br><span class="line">          [<span class="number">0.7392</span>, <span class="number">0.9660</span>, <span class="number">0.7027</span>]]]], grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>再来手动实现一下batchnorm2d</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 手动bn</span></span><br><span class="line"></span><br><span class="line">manual_normed = []</span><br><span class="line"><span class="comment"># 每个channel分别处理</span></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(channel):</span><br><span class="line">    <span class="comment"># 计算均值和标准差</span></span><br><span class="line">    mean = torch.mean(inputs[:, c, :, :])</span><br><span class="line">    std = torch.std(inputs[:, c, :, :], unbiased=<span class="literal">False</span>)</span><br><span class="line">    normed = (inputs[:, c, :, :] - mean) / (std + eps) * torch_bn.weight[c] + torch_bn.bias[c]</span><br><span class="line">    normed = normed.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    manual_normed.append(normed)</span><br><span class="line">manual_normed = torch.cat(manual_normed, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;手动bn结果:\n&#x27;</span>, manual_normed)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动操作和torch自带操作有点误差，&lt;1e-4</span></span><br><span class="line">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class="number">1e-4</span>, atol=<span class="number">1e-4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;验证结果:\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>
<p>如同之前文章所解释，由于CV的卷积计算是通过二维滑动窗口在同一个输入平面上遍历所有位置，因此同一个channel下的多个值对于这个卷积和也是一种"batch"。</p>
<p>相当于对于每一个特征值，计算平均和标准差的范围是N×H×W。</p>
<img src="/b70b4a2d/cv_batchnorm.png" class title="CV数据batchnorm">
<p>手动计算得到的结果如下，和官方接口一致</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">手动bn结果:</span><br><span class="line"> tensor([[[[<span class="number">2.2043</span>, <span class="number">1.1275</span>, <span class="number">3.9442</span>],</span><br><span class="line">          [<span class="number">1.8388</span>, <span class="number">0.3752</span>, <span class="number">2.7226</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">1.2185</span>, <span class="number">0.2591</span>, <span class="number">0.8559</span>],</span><br><span class="line">          [<span class="number">0.9175</span>, <span class="number">0.9620</span>, <span class="number">0.7252</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[<span class="number">2.8658</span>, <span class="number">0.7975</span>, <span class="number">2.2066</span>],</span><br><span class="line">          [<span class="number">6.4685</span>, <span class="number">0.8186</span>, <span class="number">1.5089</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">0.8362</span>, <span class="number">1.1387</span>, <span class="number">0.8467</span>],</span><br><span class="line">          [<span class="number">0.7392</span>, <span class="number">0.9660</span>, <span class="number">0.7027</span>]]]], grad_fn=&lt;CatBackward0&gt;)</span><br><span class="line">验证结果:</span><br><span class="line"> tensor([[[[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">          [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">          [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">          [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">          [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]]]])</span><br></pre></td></tr></table></figure>
<h2 id="layernorm-1">layernorm</h2>
<p>按照<a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html">torch的layernorm官方接口文档</a>，对于图像数据，layernorm是这样做的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># torch自带的layernorm</span></span><br><span class="line">torch_ln = nn.LayerNorm(</span><br><span class="line">    normalized_shape=[channel, height, width], </span><br><span class="line">    elementwise_affine=<span class="literal">True</span></span><br><span class="line">)  <span class="comment"># 注意完整的layernorm要包括仿射变换</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果</span></span><br><span class="line"><span class="comment"># 手动改成别的值，用于对比包含仿射变换的效果</span></span><br><span class="line">torch.manual_seed(<span class="number">5</span>)  <span class="comment"># 设置随机种子，方便复现</span></span><br><span class="line">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(channel, height, width))</span><br><span class="line">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(channel, height, width))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;weight:\n&#x27;</span>, torch_ln.weight)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;bias:\n&#x27;</span>, torch_ln.bias, <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">torch_normed = torch_ln(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;torch ln结果:\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>
<p>如同下面这个图所表示</p>
<img src="/b70b4a2d/cv_layernorm.jpeg" class title="CV数据layernorm">
<p>此时仿射变化系数的形状是这样的，为[channel, height, width]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weight:</span><br><span class="line"> Parameter containing:</span><br><span class="line">tensor([[[-<span class="number">0.4868</span>, -<span class="number">0.6038</span>, -<span class="number">0.5581</span>],</span><br><span class="line">         [ <span class="number">0.6675</span>, -<span class="number">0.1974</span>,  <span class="number">1.9428</span>]],</span><br><span class="line"></span><br><span class="line">        [[-<span class="number">1.4017</span>, -<span class="number">0.7626</span>,  <span class="number">0.6312</span>],</span><br><span class="line">         [-<span class="number">0.8991</span>, -<span class="number">0.5578</span>,  <span class="number">0.6907</span>]]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">bias:</span><br><span class="line"> Parameter containing:</span><br><span class="line">tensor([[[ <span class="number">0.2225</span>, -<span class="number">0.6662</span>,  <span class="number">0.6846</span>],</span><br><span class="line">         [ <span class="number">0.5740</span>, -<span class="number">0.5829</span>,  <span class="number">0.7679</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.0571</span>, -<span class="number">1.1894</span>, -<span class="number">0.5659</span>],</span><br><span class="line">         [-<span class="number">0.8327</span>,  <span class="number">0.9014</span>,  <span class="number">0.2116</span>]]], requires_grad=<span class="literal">True</span>) </span><br></pre></td></tr></table></figure>
<p>即每个channel内的每一个特征值，都有单独的可训练的仿射变换系数。</p>
<p>layernorm的结果如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch ln结果:</span><br><span class="line"> tensor([[[[ <span class="number">0.3594</span>, -<span class="number">0.8338</span>,  <span class="number">1.3456</span>],</span><br><span class="line">          [ <span class="number">0.5128</span>, -<span class="number">0.7147</span>, -<span class="number">0.3012</span>]],</span><br><span class="line"></span><br><span class="line">         [[-<span class="number">2.5939</span>,  <span class="number">0.5089</span>, -<span class="number">0.3546</span>],</span><br><span class="line">          [-<span class="number">1.3715</span>,  <span class="number">0.4607</span>,  <span class="number">0.0553</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ <span class="number">0.5477</span>, -<span class="number">0.9583</span>,  <span class="number">0.8526</span>],</span><br><span class="line">          [-<span class="number">1.2112</span>, -<span class="number">0.6760</span>,  <span class="number">0.9378</span>]],</span><br><span class="line"></span><br><span class="line">         [[-<span class="number">0.3219</span>, -<span class="number">2.4580</span>, -<span class="number">0.3647</span>],</span><br><span class="line">          [-<span class="number">0.6744</span>,  <span class="number">0.4171</span>, -<span class="number">0.0264</span>]]]], grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>手动进行layernorm的归一化和仿射变换，和官方接口对比一下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 手动ln</span></span><br><span class="line"></span><br><span class="line">manual_normed = []</span><br><span class="line"><span class="comment"># 每个channel分别处理</span></span><br><span class="line"><span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">    <span class="comment"># 计算均值和标准差</span></span><br><span class="line">    mean = torch.mean(inputs[b, :, :, :])</span><br><span class="line">    std = torch.std(inputs[b, :, :, :], unbiased=<span class="literal">False</span>)</span><br><span class="line">    normed = (inputs[b, :, :, :] - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class="line">    normed = normed.unsqueeze(<span class="number">0</span>)</span><br><span class="line">    manual_normed.append(normed)</span><br><span class="line">manual_normed = torch.cat(manual_normed, <span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;手动ln结果:\n&#x27;</span>, manual_normed)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动操作和torch自带操作有点误差，&lt;1e-4</span></span><br><span class="line">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class="number">1e-4</span>, atol=<span class="number">1e-4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;验证结果:\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>
<p>这里计算均值和标准差，是把所有channel内的所有特征值放在一起算的，即每个样本只有一个标量的均值和一个标量的标准差。但是仿射变换的时候就每个特征值都有自己的参数。</p>
<p>手动计算的结果如下，和官方接口一致</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">手动ln结果:</span><br><span class="line"> tensor([[[[ <span class="number">0.3594</span>, -<span class="number">0.8338</span>,  <span class="number">1.3456</span>],</span><br><span class="line">          [ <span class="number">0.5128</span>, -<span class="number">0.7147</span>, -<span class="number">0.3012</span>]],</span><br><span class="line"></span><br><span class="line">         [[-<span class="number">2.5939</span>,  <span class="number">0.5090</span>, -<span class="number">0.3546</span>],</span><br><span class="line">          [-<span class="number">1.3715</span>,  <span class="number">0.4607</span>,  <span class="number">0.0553</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ <span class="number">0.5477</span>, -<span class="number">0.9583</span>,  <span class="number">0.8527</span>],</span><br><span class="line">          [-<span class="number">1.2112</span>, -<span class="number">0.6760</span>,  <span class="number">0.9378</span>]],</span><br><span class="line"></span><br><span class="line">         [[-<span class="number">0.3219</span>, -<span class="number">2.4581</span>, -<span class="number">0.3647</span>],</span><br><span class="line">          [-<span class="number">0.6744</span>,  <span class="number">0.4171</span>, -<span class="number">0.0264</span>]]]], grad_fn=&lt;CatBackward0&gt;)</span><br><span class="line">验证结果:</span><br><span class="line"> tensor([[[[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">          [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">          [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">          [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">          [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]]]])</span><br></pre></td></tr></table></figure>
<h1 id="nlp数据">NLP数据</h1>
<p>再看下在NLP场景下的情况。</p>
<p>先定义输入，N是batch size，S是sequence length，H是hidden size。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义一个随机三维输入，[N,S,H]</span></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line">seq_len = <span class="number">3</span></span><br><span class="line">hidden_size = <span class="number">4</span></span><br><span class="line">torch.manual_seed(<span class="number">6</span>)  <span class="comment"># 设置随机种子，方便复现</span></span><br><span class="line">inputs = torch.randn(batch_size, seq_len, hidden_size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;三维输入:\n&#x27;</span>, inputs)</span><br></pre></td></tr></table></figure>
<h2 id="batchnorm-2">batchnorm</h2>
<p>用官方接口计算</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># torch自带的batchnorm</span></span><br><span class="line">torch_bn = nn.BatchNorm1d(num_features=hidden_size, affine=<span class="literal">True</span>)  <span class="comment"># 注意完整的batchnorm要包括仿射变换</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果</span></span><br><span class="line"><span class="comment"># 手动改成别的值，用于对比包含仿射变换的效果</span></span><br><span class="line">torch.manual_seed(<span class="number">7</span>)  <span class="comment"># 设置随机种子，方便复现</span></span><br><span class="line">torch_bn.weight = nn.Parameter(torch_bn.weight * torch.randn(hidden_size))</span><br><span class="line">torch_bn.bias = nn.Parameter(torch_bn.bias + torch.randn(hidden_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;weight:\n&#x27;</span>, torch_bn.weight)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;bias:\n&#x27;</span>, torch_bn.bias, <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 结果</span></span><br><span class="line">torch_normed = torch_bn(inputs.transpose(<span class="number">1</span>, <span class="number">2</span>)).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;torch bn结果:\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>
<p>根据官方接口的描述，输入的第二维应该为特征数，第三维为序列长度，因此这里对输入做了transpose，再把结果transpose回来。</p>
<p>结果如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weight:</span><br><span class="line"> Parameter containing:</span><br><span class="line">tensor([-<span class="number">0.1468</span>,  <span class="number">0.7861</span>,  <span class="number">0.9468</span>, -<span class="number">1.1143</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">bias:</span><br><span class="line"> Parameter containing:</span><br><span class="line">tensor([ <span class="number">1.6908</span>, -<span class="number">0.8948</span>, -<span class="number">0.3556</span>,  <span class="number">1.2324</span>], requires_grad=<span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line">torch bn结果:</span><br><span class="line"> tensor([[[ <span class="number">1.8740</span>, -<span class="number">0.7037</span>, -<span class="number">1.8222</span>,  <span class="number">2.3385</span>],</span><br><span class="line">         [ <span class="number">1.7413</span>, -<span class="number">1.8119</span>,  <span class="number">0.3641</span>,  <span class="number">0.0200</span>],</span><br><span class="line">         [ <span class="number">1.4615</span>, -<span class="number">0.2676</span>,  <span class="number">0.1081</span>,  <span class="number">1.3450</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.7084</span>, -<span class="number">1.9653</span>,  <span class="number">1.0169</span>,  <span class="number">0.5785</span>],</span><br><span class="line">         [ <span class="number">1.8213</span>, -<span class="number">0.8614</span>, -<span class="number">0.8056</span>,  <span class="number">2.9892</span>],</span><br><span class="line">         [ <span class="number">1.5383</span>,  <span class="number">0.2409</span>, -<span class="number">0.9949</span>,  <span class="number">0.1231</span>]]], grad_fn=&lt;TransposeBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>可以看到batchnorm的仿射变化系数形状在各种情况下都保持和特征向量维度相同。</p>
<p>再来手动计算验证一下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 手动bn</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算均值</span></span><br><span class="line">mean = torch.mean(inputs, dim=(<span class="number">0</span>, <span class="number">1</span>) , keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;均值:\n&#x27;</span>, mean)</span><br><span class="line">std = torch.std(inputs, dim=(<span class="number">0</span>, <span class="number">1</span>), keepdim=<span class="literal">True</span>, unbiased=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;标准差:\n&#x27;</span>, std, <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">manual_normed = (inputs - mean) / (std + eps) * torch_bn.weight + torch_bn.bias</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;手动bn结果:\n&#x27;</span>, manual_normed)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动操作和torch自带操作有点误差，&lt;1e-4</span></span><br><span class="line">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class="number">1e-4</span>, atol=<span class="number">1e-4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;验证结果:\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>
<p>这里计算用于归一化均值和方差，是在dim=(0,1)范围内计算的，相当于把[N,
S, H]的输入拉平为[N×S,
H]的二维输入，再按二维输入的方式进行batchnorm。</p>
<p>结果如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">均值:</span><br><span class="line"> tensor([[[-<span class="number">0.2151</span>,  <span class="number">0.5444</span>, -<span class="number">0.2633</span>, -<span class="number">0.5424</span>]]])</span><br><span class="line">标准差:</span><br><span class="line"> tensor([[[<span class="number">0.7984</span>, <span class="number">0.3537</span>, <span class="number">0.7799</span>, <span class="number">0.7986</span>]]]) </span><br><span class="line"></span><br><span class="line">手动bn结果:</span><br><span class="line"> tensor([[[ <span class="number">1.8740</span>, -<span class="number">0.7037</span>, -<span class="number">1.8222</span>,  <span class="number">2.3385</span>],</span><br><span class="line">         [ <span class="number">1.7413</span>, -<span class="number">1.8119</span>,  <span class="number">0.3641</span>,  <span class="number">0.0200</span>],</span><br><span class="line">         [ <span class="number">1.4615</span>, -<span class="number">0.2676</span>,  <span class="number">0.1081</span>,  <span class="number">1.3450</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.7084</span>, -<span class="number">1.9653</span>,  <span class="number">1.0169</span>,  <span class="number">0.5785</span>],</span><br><span class="line">         [ <span class="number">1.8213</span>, -<span class="number">0.8614</span>, -<span class="number">0.8056</span>,  <span class="number">2.9892</span>],</span><br><span class="line">         [ <span class="number">1.5383</span>,  <span class="number">0.2409</span>, -<span class="number">0.9950</span>,  <span class="number">0.1231</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">验证结果:</span><br><span class="line"> tensor([[[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">         [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">         [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">         [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">         [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]]])</span><br></pre></td></tr></table></figure>
<h2 id="layernorm-2">layernorm</h2>
<p>终于来到NLP数据的layernorm，先确认一下，huggingface中bert是这么使用layernorm的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BertSelfOutput</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states: torch.normTensor, input_tensor: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<p>用我们的数据跑一下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># torch自带的layernorm</span></span><br><span class="line">torch_ln = nn.LayerNorm(normalized_shape=hidden_size, elementwise_affine=<span class="literal">True</span>)  <span class="comment"># 注意完整的layernorm要包括仿射变换</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 仿射变化初始化的weigh=1，bias=0，相当于没有进行变换，看不出效果</span></span><br><span class="line"><span class="comment"># 手动改成别的值，用于对比包含仿射变换的效果</span></span><br><span class="line">torch.manual_seed(<span class="number">8</span>)  <span class="comment"># 设置随机种子，方便复现</span></span><br><span class="line">torch_ln.weight = nn.Parameter(torch_ln.weight * torch.randn(hidden_size))</span><br><span class="line">torch_ln.bias = nn.Parameter(torch_ln.bias + torch.randn(hidden_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;weight:\n&#x27;</span>, torch_ln.weight)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;bias:\n&#x27;</span>, torch_ln.bias, <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">torch_normed = torch_ln(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;torch ln结果:\n&#x27;</span>, torch_normed)</span><br></pre></td></tr></table></figure>
<p>仿射变化参数的形状和hidden size一致</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weight:</span><br><span class="line"> Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.2713</span>, -<span class="number">1.2729</span>,  <span class="number">0.5027</span>,  <span class="number">0.4181</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">bias:</span><br><span class="line"> Parameter containing:</span><br><span class="line">tensor([-<span class="number">0.6394</span>, -<span class="number">0.6608</span>, -<span class="number">0.1433</span>, -<span class="number">0.1043</span>], requires_grad=<span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line">torch ln结果:</span><br><span class="line"> tensor([[[-<span class="number">0.7547</span>, -<span class="number">2.8528</span>, -<span class="number">0.5092</span>, -<span class="number">0.3423</span>],</span><br><span class="line">         [-<span class="number">1.0957</span>, -<span class="number">0.8780</span>,  <span class="number">0.2388</span>,  <span class="number">0.2097</span>],</span><br><span class="line">         [-<span class="number">0.3502</span>, -<span class="number">1.6158</span>, -<span class="number">0.3133</span>, -<span class="number">0.7224</span>]],</span><br><span class="line"></span><br><span class="line">        [[-<span class="number">0.9134</span>, -<span class="number">0.4490</span>,  <span class="number">0.6868</span>, -<span class="number">0.3029</span>],</span><br><span class="line">         [-<span class="number">0.7116</span>, -<span class="number">2.5589</span>, -<span class="number">0.1039</span>, -<span class="number">0.6493</span>],</span><br><span class="line">         [-<span class="number">0.5076</span>, -<span class="number">2.1031</span>, -<span class="number">0.9346</span>, -<span class="number">0.1230</span>]]],</span><br><span class="line">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>再来手动验证一下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 手动ln</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算均值</span></span><br><span class="line">mean = torch.mean(inputs, dim=<span class="number">2</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;均值:\n&#x27;</span>, mean)</span><br><span class="line">std = torch.std(inputs, dim=<span class="number">2</span>, keepdim=<span class="literal">True</span>, unbiased=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;标准差:\n&#x27;</span>, std, <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">manual_normed = (inputs - mean) / (std + eps) * torch_ln.weight + torch_ln.bias</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;手动ln结果:\n&#x27;</span>, manual_normed)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动操作和torch自带操作有点误差，&lt;1e-4</span></span><br><span class="line">isclose = torch.isclose(torch_normed, manual_normed, rtol=<span class="number">1e-4</span>, atol=<span class="number">1e-4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;验证结果:\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>
<p>得到的均值和标准差如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">均值:</span><br><span class="line"> tensor([[[-<span class="number">0.8469</span>],</span><br><span class="line">         [ <span class="number">0.0745</span>],</span><br><span class="line">         [ <span class="number">0.3386</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.1364</span>],</span><br><span class="line">         [-<span class="number">0.7003</span>],</span><br><span class="line">         [ <span class="number">0.2831</span>]]])</span><br><span class="line">标准差:</span><br><span class="line"> tensor([[[<span class="number">0.8578</span>],</span><br><span class="line">         [<span class="number">0.3354</span>],</span><br><span class="line">         [<span class="number">0.6505</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.4426</span>],</span><br><span class="line">         [<span class="number">0.8448</span>],</span><br><span class="line">         [<span class="number">0.6816</span>]]]) </span><br></pre></td></tr></table></figure>
<p>每个sample中的每个token，都有各自的均值和标准差，用于归一化。</p>
<p>最终结果如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">手动ln结果:</span><br><span class="line"> tensor([[[-<span class="number">0.7547</span>, -<span class="number">2.8528</span>, -<span class="number">0.5092</span>, -<span class="number">0.3423</span>],</span><br><span class="line">         [-<span class="number">1.0957</span>, -<span class="number">0.8780</span>,  <span class="number">0.2388</span>,  <span class="number">0.2097</span>],</span><br><span class="line">         [-<span class="number">0.3502</span>, -<span class="number">1.6158</span>, -<span class="number">0.3133</span>, -<span class="number">0.7224</span>]],</span><br><span class="line"></span><br><span class="line">        [[-<span class="number">0.9134</span>, -<span class="number">0.4490</span>,  <span class="number">0.6868</span>, -<span class="number">0.3029</span>],</span><br><span class="line">         [-<span class="number">0.7116</span>, -<span class="number">2.5590</span>, -<span class="number">0.1039</span>, -<span class="number">0.6493</span>],</span><br><span class="line">         [-<span class="number">0.5076</span>, -<span class="number">2.1031</span>, -<span class="number">0.9347</span>, -<span class="number">0.1230</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">验证结果:</span><br><span class="line"> tensor([[[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">         [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">         [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">         [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">         [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]]])</span><br></pre></td></tr></table></figure>
<h1 id="归一化的输入能变回原输入吗">归一化的输入能变回原输入吗</h1>
<p>既然这些操作是先计算均值和标准差进行归一化，再进行仿射变换，那把仿射变换的参数设置为输入的均值和标准差，是不是就可以把归一化过的数据变回跟原数据一模一样了呢？</p>
<p>以二维情况为例，看下batchnorm是否能变回去。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义一个随机二维输入</span></span><br><span class="line">batch_size = <span class="number">3</span></span><br><span class="line">feature_num = <span class="number">4</span></span><br><span class="line">torch.manual_seed(<span class="number">0</span>)  <span class="comment"># 设置随机种子，方便复现</span></span><br><span class="line">inputs = torch.randn(batch_size, feature_num)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;二维输入:\n&#x27;</span>, inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算均值和标准差</span></span><br><span class="line">mean = torch.mean(inputs, dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># print(&#x27;均值:\n&#x27;, mean)</span></span><br><span class="line">std = torch.std(inputs, dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>, unbiased=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># print(&#x27;标准差:\n&#x27;, std, &#x27;\n&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># torch自带的batchnorm</span></span><br><span class="line">torch_bn = nn.BatchNorm1d(num_features=feature_num, affine=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把仿射变换的缩放和平移替换为标准差和均值</span></span><br><span class="line">torch_bn.weight = nn.Parameter(std)</span><br><span class="line">torch_bn.bias =  nn.Parameter(mean)</span><br><span class="line"><span class="comment"># print(&#x27;weight:\n&#x27;, torch_bn.weight)</span></span><br><span class="line"><span class="comment"># print(&#x27;bias:\n&#x27;, torch_bn.bias, &#x27;\n&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">torch_normed = torch_bn(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;torch bn结果:\n&#x27;</span>, torch_normed)</span><br><span class="line"></span><br><span class="line">isclose = torch.isclose(torch_normed, inputs, rtol=<span class="number">1e-4</span>, atol=<span class="number">1e-4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;验证结果:\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>
<p>结果如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">二维输入:</span><br><span class="line"> tensor([[ <span class="number">1.5410</span>, -<span class="number">0.2934</span>, -<span class="number">2.1788</span>,  <span class="number">0.5684</span>],</span><br><span class="line">        [-<span class="number">1.0845</span>, -<span class="number">1.3986</span>,  <span class="number">0.4033</span>,  <span class="number">0.8380</span>],</span><br><span class="line">        [-<span class="number">0.7193</span>, -<span class="number">0.4033</span>, -<span class="number">0.5966</span>,  <span class="number">0.1820</span>]])</span><br><span class="line">torch bn结果:</span><br><span class="line"> tensor([[ <span class="number">1.5410</span>, -<span class="number">0.2934</span>, -<span class="number">2.1788</span>,  <span class="number">0.5684</span>],</span><br><span class="line">        [-<span class="number">1.0845</span>, -<span class="number">1.3986</span>,  <span class="number">0.4033</span>,  <span class="number">0.8380</span>],</span><br><span class="line">        [-<span class="number">0.7193</span>, -<span class="number">0.4033</span>, -<span class="number">0.5966</span>,  <span class="number">0.1821</span>]],</span><br><span class="line">       grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br><span class="line">验证结果:</span><br><span class="line"> tensor([[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]])</span><br></pre></td></tr></table></figure>
<p>确认了batchnorm是可以变回去的。</p>
<p>再来看下layernorm</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;二维输入:\n&#x27;</span>, inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算均值和标准差</span></span><br><span class="line">mean = torch.mean(inputs, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># print(&#x27;均值:\n&#x27;, mean)</span></span><br><span class="line">std = torch.std(inputs, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>, unbiased=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># print(&#x27;标准差:\n&#x27;, std, &#x27;\n&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># torch自带的layernorm</span></span><br><span class="line">torch_ln = nn.LayerNorm(normalized_shape=feature_num, elementwise_affine=<span class="literal">True</span>)  <span class="comment"># 注意完整的layernorm要包括仿射变换</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 把仿射变换的缩放和平移替换为标准差和均值</span></span><br><span class="line">torch_bn.weight = nn.Parameter(std)</span><br><span class="line">torch_bn.bias =  nn.Parameter(mean)</span><br><span class="line"><span class="comment"># print(&#x27;weight:\n&#x27;, torch_bn.weight)</span></span><br><span class="line"><span class="comment"># print(&#x27;bias:\n&#x27;, torch_bn.bias, &#x27;\n&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">torch_normed = torch_ln(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;torch ln结果:\n&#x27;</span>, torch_normed)</span><br><span class="line"></span><br><span class="line">isclose = torch.isclose(torch_normed, inputs, rtol=<span class="number">1e-4</span>, atol=<span class="number">1e-4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;验证结果:\n&#x27;</span>, isclose)</span><br></pre></td></tr></table></figure>
<p>结果如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">二维输入:</span><br><span class="line"> tensor([[ <span class="number">1.5410</span>, -<span class="number">0.2934</span>, -<span class="number">2.1788</span>,  <span class="number">0.5684</span>],</span><br><span class="line">        [-<span class="number">1.0845</span>, -<span class="number">1.3986</span>,  <span class="number">0.4033</span>,  <span class="number">0.8380</span>],</span><br><span class="line">        [-<span class="number">0.7193</span>, -<span class="number">0.4033</span>, -<span class="number">0.5966</span>,  <span class="number">0.1820</span>]])</span><br><span class="line">torch ln结果:</span><br><span class="line"> tensor([[ <span class="number">1.1918</span>, -<span class="number">0.1481</span>, -<span class="number">1.5251</span>,  <span class="number">0.4814</span>],</span><br><span class="line">        [-<span class="number">0.8146</span>, -<span class="number">1.1451</span>,  <span class="number">0.7512</span>,  <span class="number">1.2086</span>],</span><br><span class="line">        [-<span class="number">0.9685</span>, -<span class="number">0.0551</span>, -<span class="number">0.6140</span>,  <span class="number">1.6376</span>]],</span><br><span class="line">       grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br><span class="line">验证结果:</span><br><span class="line"> tensor([[<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">        [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">        [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>]])</span><br></pre></td></tr></table></figure>
<p>发现layernorm并不能通过这种方式把归一化的输入变回原始值，因为layernorm归一化是在特征向量内进行的，所有特征值共享一个均值和方差，但是仿射变换的时候每个特征却有单独的系数。</p>
<p>对于CV数据和NLP数据也有一样的结论。</p>
<p>可以认为batchnorm的归一化和仿射变换是互为可逆的一对操作，而layernorm的归一化和仿射变换是在不同范围内的操作，是不可逆的。</p>
<h1 id="小结">小结</h1>
<p>本篇从各种输入数据对batchnorm和layernorm做了手动复现。</p>
<p>需要注意到，batchnorm、layernorm等实际都包含两步操作：①归一化②仿射变换。</p>
<p>基本上，batchnorm可以总结为，对于特征向量中的每一个特征值，在一个"大范围"内进行归一化，这个"大范围"根据输入数据形状，可能是batch，可能是batch×序列长度，或者batch×feature
map大小。并且归一化和仿射变换在同一个方向上进行，因此这两个操作是互为可逆的。</p>
<p>而layernorm是在每个特征向量内部进行归一化处理，然后在另一个方向上使用仿射变换。由于归一化和仿射变换的方向不同，因此无法通过把仿射变换，把已经归一化的数据变换为原输入数据。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】LAYERNORM
https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html<br>
【2】BATCHNORM1D
https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html<br>
【3】BATCHNORM2D
https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>layernorm</tag>
        <tag>normalization</tag>
        <tag>batchnorm</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型算法题(3)</title>
    <url>/17360081.html</url>
    <content><![CDATA[<p><img src="/images/cover.png"></p>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a></p>
<hr>
<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<p>本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新<sub>如有错漏，欢迎指正</sub></p>
<hr>
<h1 id="旋转位置编码rope有什么优缺点">1.旋转位置编码RoPE有什么优缺点？</h1>
<p>优点：RoPE以绝对位置编码的方式实现了相对位置编码，使得能够在不破坏注意力形式的情况下，以“加性编码”的方式让模型学习相对位置。①相比其他相对位置编码来说，实现简单，计算量少。②可以应用于线性注意力。③RoPE具有远程衰减的特性，使得每个位置天然能够更关注到附近的信息。</p>
<p>缺点：RoPE相比训练式的绝对位置编码具有一定的外推能力，如可以在2k数据长度训练的模型进行略长于2k的推理。但是相比于Alibi等位置编码，其直接外推能力并不算特别好，需要通过线性插值、NTK插值、YaRN等方式来优化外推能力。</p>
<h1 id="batchnorm中的momentum怎么影响训练效果">2.batchnorm中的momentum怎么影响训练效果</h1>
<p>batchnorm在训练时计算每个batch内的均值和方差用于normalization，同时统计一个全局均值和方差用于推理。全局均值和方差计算公式为：</p>
<p>moving_mean = momentum × moving_mean + (1.0 − momentum) × mean</p>
<p>moving_var = momentum × moving_var + (1.0 − momentum) × var</p>
<p>小的momentum值对应快的更新速度，能够更快地向真实分布靠近，但是同时也会导致更大的波动；如果更新过慢，则可能导致训练结束时还没有统计到真实的分布，是欠拟合的状态。如果batch
size比较小，每个mini batch和全局差异较大，就不应该用太大的momentum。</p>
<h1 id="多头注意力相比单头有什么好处">3.多头注意力相比单头有什么好处？</h1>
<p>多头注意力使用多个维度较低的子空间分别进行学习。</p>
<p>一般来说，相比单头的情况，多个头能够分别关注到不同的特征，增强了表达能力。多个头中，会有部分头能够学习到更高级的特征，并减少注意力权重对角线值过大的情况。</p>
<p>比如部分头关注语法信息，部分头关注知识内容，部分头关注近距离文本，部分头关注远距离文本，这样减少信息缺失，提升模型容量。</p>
<p>另外虽然多头注意力的整体计算量比单头要大一点，但是并行度也高一些。</p>
<h1 id="kv-cache为什么能加速推理">4.kv cache为什么能加速推理？</h1>
<p>对于GPT类模型，使用的是单向注意力，每个位置只能看到自己和前面的内容。</p>
<p>在进行自回归解码的时候，新生成的token会加入序列，一起作为下一次解码的输入。</p>
<p>由于单向注意力的存在，新加入的token并不会影响前面序列的计算，因此可以把已经计算过的每层的kv值保存起来，这样就节省了和本次生成无关的计算量。</p>
<p>通过把kv值存储在速度远快于显存的L2缓存中，可以大大减少kv值的保存和读取，这样就极大加快了模型推理的速度。</p>
<h1 id="relu有什么优缺点">5.ReLU有什么优缺点？</h1>
<p>优点：（1）计算快，前向只需要进行max(0,
x)计算，后向则是直接透传；（2）有激活值的时候，梯度恒定为1，不会爆炸/消失；</p>
<p>缺点：（1）均值不为0，分布产生偏移（2）输入值小于0时，梯度再也无法回传过来，导致神经元坏死。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a></p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>算法题</tag>
      </tags>
  </entry>
  <entry>
    <title>训练数据合成(一)</title>
    <url>/85132189.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>现在大模型的训练方法大部分都比较固定了，那么最重要的问题就是搞数据。真实世界的高质量数据虽然好用，但是成本高数量少，于是合成数据就成了一条很重要的路子。较新的专门模型如数学模型、代码模型或者阅读理解模型，基本上都已经使用上了大量的合成数据。这些领域的合成数据和训练的模型经过多次迭代，又会反哺下一代通用模型，左脚踩右脚直接起飞。Llama-3就是这么干的。</p>
<p>最近在搞代码能力的提升，很有必要学习一下(代码)数据合成的方法。</p>
<h1 id="big-picture">big picture</h1>
<p>正好找到一篇新出的综述，《A Survey on Data Synthesis and Augmentation
for Large Language
Models》，梳理了“面向LLM的数据生成”相关的250篇文献，做了一些分类和总结。参考这篇综述先看下有那些思路。</p>
<p>首先，文中把数据的生成分成两大类：data augmentation和data
synthesis。</p>
<p>data
augmentation是一种“数据-&gt;类似数据”的做法，并在这个过程中保持原始数据的显著特征。最典型的做法就是CV领域中对图像数据做的各种畸变，如旋转翻转调整histogram等，但是并不破坏原图的重要内容；除此之外，借助较强的大模型，通过CoT等方式对无标签数据进行打标也是一种数据增强的方式，这个过程中还可以通过和人类标注结合进一步提升效果和效率。</p>
<p>data synthesis是创造全新数据的方法，文中把它分成三大类：<br>
- general model
distillation：借助强大的通用模型如GPT-4，生成可增强较弱模型的数据，典型的例子就是Phi系列<br>
- domain model
distillation：在一些领域如数学或者代码领域，通用模型的效果可能不够好，就需要借助专门模型来生成数据<br>
- model
self-improvment：比如基于现有文本数据，生成不同难度或者不同风格的文本数据</p>
<p>按时间线整理，上面各类方法对应的工作：</p>
<img src="/85132189/survey_timeline.png" class title="数据合成">
<p>data augmentation和data
synthesis在LM生命周期又有不同的做法。文中把LM的生命周期分为6段：（1）data
preparation（2）pretraining（3）finetuning（4）instruction-tuning（5）preference
alignment（6）application。（这里个人感觉（1）和（2）可以放一起，（3）和（4）可以放一起）</p>
<p>各个生命周期下，各种方法的整理和分类如下图：</p>
<img src="/85132189/survey_all.png" class title="数据合成">
<p>这里我个人主要关注在文本预训练阶段，以及和code相关的内容。</p>
<p>文中给出了数据准备和预训练阶段的相关工作：</p>
<img src="/85132189/survey_data_prepare.png" class title="数据合成">
<img src="/85132189/survey_pretrain.png" class title="数据合成">
<p>从中pick一些重要的/较新的工作：<br>
- OSS-Instruct<br>
- Case2Code<br>
- TinyStories<br>
- *Iterative Question Composing<br>
- Generator prompts<br>
- MathInstruct<br>
- SciLitLLM<br>
- TRAIT<br>
- Persona Hub<br>
- AceCoder<br>
- Repocoder<br>
- Evol-Instruct</p>
<h1 id="persona-hub">Persona Hub</h1>
<p>论文：《Scaling Synthetic Data Creation with 1,000,000,000
Personas》</p>
<p>时间：2024年06月</p>
<p>机构：腾讯</p>
<p>数据合成要解决的几个问题可以总结为3个：<br>
-
多样性：真实的数据来自现实世界不同场景和不同人物，具有很强的多样性，而合成数据往往受限于prompt和model的特性，多样性有限<br>
-
一致性：合成的数据分布要和真实数据分布一致，否则在推理的时候遇到了不同分布的输入，效果就会大打折扣<br>
-
高质量：合成数据在多样化、拟合真实分布的情况下，还应尽量具有高质量的内容（毕竟真实数据中也有很多低质量内容，但这些内容已经被证实价值不高）</p>
<p>腾讯这篇论文主要就是要解决多样性的问题。通常来说，不考虑使用抽样的情况下，一个固定的prompt在固定的LLM只能获得固定的一条输出样例。而LLM不可能大规模地变换，因此要获得多样化的输出，就需要改变prompt。</p>
<p>现有在合成数据中提升多样化的方法基本上可以分成两种paradigm：<br>
-
instance-driven：使用种子语料库帮获得多样化的prompt，代表性的工作有《Self-instruct:
Aligning language models with self-generated instructions》和《Metamath:
Bootstrap your own mathematical questions for large language
models》。这种方法prompt的多样性受限于种子语料库的规模。<br>
-
key-point-driven：通过在关键维度的排列组合，提升prompt的多样性，代表性的工作有《Synthetic
data (almost) from scratch: Generalized instruction tuning for language
models》和《Key-point-driven data synthesis with its enhancement on
mathematical
reasoning》。但是对于通用数据，关键维度可以很多，这就需要投入大量的人力，因此这种方法更适合于特定领域的数据合成，比如数学。</p>
<p>无论哪种方法，其实就是给模型输入一个“随机数”，但是这个随机数并不是一个数字，而可以认为是“字符串化”后的随机数。</p>
<p>那么腾讯就提出了一种角色驱动的数据合成方法（来提供这样一种字符串化的随机数）。角色描述可以是这样的：<br>
- a moving company driver<br>
- a chemical kinetics researcher</p>
<p>然后让模型为给定的角色创造符合要求的数据：“create {data} with
{persona}”：</p>
<img src="/85132189/personahub_intro.png" class title="数据合成">
<p>这种方法的好处是，角色这个维度不影响原来prompt的设定，因此几乎可以和任意的数据合成方法相结合。那么只要合成足够多的角色，理论上就可以获取和真实世界完全一样的多样性了。</p>
<h2 id="角色的获取">角色的获取</h2>
<p>第一个问题就是怎么获取足够多样的角色。文中给出了两种获取角色的方法：<br>
- text-to-persona<br>
- persona-to-persona</p>
<p>1、text-to-persona</p>
<p>利用海量网络数据构建角色：首先找一篇任意文档，然后让模型按“谁可能读/写/喜欢/不喜欢这段文字”的prompt输出，并给出对应角色的描述：</p>
<img src="/85132189/personahub_t2p.png" class title="数据合成">
<p>实践上来说，让模型给出详细一些的粒度效果更好。当然模型给出的角色描述粒度和类型和输入文本也有很大的关系，比如当输入文本是数学或者物理相关的文档时，给出的角色描述就比较细。</p>
<p>输出的角色描述可以是自然语言，也可以是结构化的文本，这个可以根据需求选择。</p>
<img src="/85132189/personahub_t2p_fine.png" class title="数据合成">
<p>2、persona-to-persona</p>
<p>虽然使用网上多样化的文本可以生成很多样的角色，但是依然有可能存在一些遗漏。因此除了使用text生成角色外，还可以通过已有的角色泛化更多的角色。比如关于“儿童”的角色可以从儿童医院护士的角色（患者-照顾者关系）中推断出来。
类似地，“乞丐”可以从避难所工作人员（援助关系）的角色衍生出来，“幕后电影工作人员”可以衍生出来来自电影主角的角色（同事关系）。</p>
<img src="/85132189/personahub_p2p.png" class title="数据合成">
<p>根据六度分离理论，文中对通过text-to-persona获得的每个角色进行六次persona-to-persona关系扩展迭代，从而进一步丰富角色多样性。</p>
<p>获得大量角色之后还需要去重。论文中使用了两种方法进行去重。</p>
<p>1、minhash</p>
<p>角色的描述一般都比较短，因此简单地使用 1-gram 和 128 的签名大小来进行
MinHash 重复数据删除，阈值设置为0.9。</p>
<p>2、embedding</p>
<p>使用embedding模型，比如OpenAI 的 text-embedding-3-small
模型来计算不同角色描述之间的相似度，然后按阈值过滤，这里的相似度阈值设置为0.9。</p>
<p>过滤时相似度阈值可以根据需求设置，比如当所需的量不大，而多样性要求更高时，可以选择更高的阈值，以保留少量差异更大的角色。</p>
<h2 id="角色的使用">角色的使用</h2>
<p>获得角色之后，就是怎么使用的问题。角色可以插入在0-shot、few-shot
prompt里：</p>
<img src="/85132189/personahub_prompt.png" class title="数据合成">
<p>角色信息可以使用在不同数据的合成上。</p>
<p>1、数学</p>
<img src="/85132189/personahub_math_1.png" class title="数据合成">
<img src="/85132189/personahub_math_2.png" class title="数据合成">
<p>可以看到对于不同的角色，模型会给出难度不同，类型不同的数据。</p>
<p>2、逻辑推理</p>
<img src="/85132189/personahub_reasoning.png" class title="数据合成">
<p>3、instruction</p>
<img src="/85132189/personahub_instruction.png" class title="数据合成">
<h1 id="oss-instruct">OSS-Instruct</h1>
<p>论文：《Magicoder: Empowering Code Generation with OSS-Instruct》</p>
<p>时间：2023年12月</p>
<p>Magicoder利用OSS-INSTRUCT的方法（OSS=open-source code
snippets），合成了75k的指令数据，并获得了不错的效果。</p>
<p>OSS-INSTRUCT的流程如下：</p>
<img src="/85132189/oss_intro.png" class title="数据合成">
<ul>
<li><p>首先，从开源代码数据中，获取种子代码片段</p></li>
<li><p>对于每个代码文档，随机提取1-15行连续行作为种子片段</p></li>
<li><p>每个代码片段用下面这个prompt模板获取coding
problem和solution</p></li>
</ul>
<img src="/85132189/oss_prompt.png" class title="数据合成">
<p>文字版：</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>You are exceptionally skilled at crafting high<span class="op">-</span>quality programming problems <span class="kw">and</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>offering precise solutions.</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>Please gain inspiration <span class="im">from</span> the following random code snippet to create a</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>high<span class="op">-</span>quality programming problem. Present your output <span class="kw">in</span> two distinct sections:</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>[Problem Description] <span class="kw">and</span> [Solution].</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>Code snippet <span class="cf">for</span> inspiration:</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>``</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>&#123;code&#125;</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>``</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>Guidelines <span class="cf">for</span> each section:</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fl">1.</span> [Problem Description]: This should be <span class="op">**</span>completely <span class="va">self</span><span class="op">-</span>contained<span class="op">**</span>, providing</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="bu">all</span> the contextual information one needs to understand <span class="kw">and</span> solve the problem.</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>Assume common programming knowledge, but ensure that <span class="bu">any</span> specific context,</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>variables, <span class="kw">or</span> code snippets pertinent to this problem are explicitly included.</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="fl">1.</span> [Solution]: Offer a comprehensive, <span class="op">**</span>correct<span class="op">**</span> solution that accurately</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>addresses the [Problem Description] you provided.</span></code></pre></div>
<p>一些生成的样例如下：</p>
<img src="/85132189/oss_case1.png" class title="数据合成">
<img src="/85132189/oss_case2.png" class title="数据合成">
<img src="/85132189/oss_case3.png" class title="数据合成">
<h1 id="case2code">Case2Code</h1>
<p>论文：《Case2Code: Learning Inductive Reasoning with Synthetic
Data》</p>
<p>时间：2024年7月</p>
<p>这篇论文发现在代码领域，deductive
reasoning的数据比较常见，而inductive
reasoning的数据就比较少见，这也导致模型在归纳推理能力上较弱。</p>
<img src="/85132189/case2code_intro.png" class title="数据合成">
<p>因此提出case2code，和LLM归纳推理能力相关的一个任务。</p>
<p>case2code要求受测模型根据给定的代码输入和输出，归纳出代码的执行逻辑。</p>
<p>case2code的数据是合成得到的，合成框架如下图：</p>
<img src="/85132189/case2code_framework.png" class title="数据合成">
<p>首先使用基于规则的filter收集program，然后利用LLM对这些program编写示例输入，用代码解释器获取这些输入的输出结果。最后根据输出过滤掉低质量的program，获得高质量的（program，input，output）三元组数据。</p>
<p>1、program的获取</p>
<p>从The
Stack数据集中用解析工具获取python函数，保留满足以下规则的函数：<br>
- 通过语法检查<br>
- 具有一个或多个输入参数和有返回值<br>
- 不依赖第三方或者外部IO操作</p>
<p>符合这些规则的函数可以轻易地使用代码解释器运行。</p>
<p>2、生成输入</p>
<p>利用LLM给收集到的python函数编写输入，这里发现这一步可以使用较小的LLM，以提高效率降低成本。</p>
<p>生成输入的prompt：</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>Given the function, first analyze the types of the function arguments, then write</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="dv">10</span> different example inputs <span class="cf">for</span> the function, each example should be a <span class="bu">dict</span> <span class="cf">with</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>function arguments<span class="st">&#39; names and their values.</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="er">Output format:</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>``python</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>examples <span class="op">=</span> [</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="bu">dict</span>(argname<span class="op">=</span>argvalue),</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>....</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>``</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>Function:</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>``python</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_func(a: <span class="bu">int</span>, b: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="cf">return</span> <span class="bu">str</span>(a) <span class="op">+</span> b</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>``</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>Examples:</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>``python</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>examples <span class="op">=</span> [</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="bu">dict</span>(a<span class="op">=</span><span class="dv">1</span>, b<span class="op">=</span><span class="st">&#39;a&#39;</span>),</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="bu">dict</span>(a<span class="op">=</span><span class="dv">2</span>, b<span class="op">=</span><span class="st">&#39;b&#39;</span>),</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="bu">dict</span>(a<span class="op">=</span><span class="dv">3</span>, b<span class="op">=</span><span class="st">&#39;c&#39;</span>),</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="bu">dict</span>(a<span class="op">=</span><span class="dv">4</span>, b<span class="op">=</span><span class="st">&#39;d&#39;</span>),</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="bu">dict</span>(a<span class="op">=</span><span class="dv">5</span>, b<span class="op">=</span><span class="st">&#39;e&#39;</span>),</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="bu">dict</span>(a<span class="op">=</span><span class="dv">6</span>, b<span class="op">=</span><span class="st">&#39;f&#39;</span>),</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="bu">dict</span>(a<span class="op">=</span><span class="dv">7</span>, b<span class="op">=</span><span class="st">&#39;g&#39;</span>),</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="bu">dict</span>(a<span class="op">=</span><span class="dv">8</span>, b<span class="op">=</span><span class="st">&#39;h&#39;</span>),</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="bu">dict</span>(a<span class="op">=</span><span class="dv">9</span>, b<span class="op">=</span><span class="st">&#39;i&#39;</span>),</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="bu">dict</span>(a<span class="op">=</span><span class="dv">10</span>, b<span class="op">=</span><span class="st">&#39;j&#39;</span>),</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>``</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>Function:</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>``python</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>&#123;code&#125;</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>``</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>Examples:</span></code></pre></div>
<p>3、获取输出</p>
<p>获取输出之后，会引入一些过滤规则筛选掉无效的输入或者函数。比如如果一个函数的输出在输入变化时保持不变，那么这个函数可能就是有问题的，就会被筛掉。</p>
<p>此外，还会过滤掉输出值很长的函数，确保case2code的数据不会超过LLM的窗口范围（不过目前LLM的窗口都很长，这种情况应该不多）。</p>
<p>4、post-processing</p>
<p>最后会把函数和输入输出构建成prompt。对于有n个输入输出对的case，会抽取m&lt;=n个输入输出对作为观察集。</p>
<p>另外还发现prompt的多样性会极大地影响模型推理性能的泛化能力，因此构建了10个不同风格的prompt模板。使用多样化prompt训练效果更好：</p>
<img src="/85132189/case2code_diverse_prompt.png" class title="数据合成">
<p>拼接的prompt样例：</p>
<img src="/85132189/case2code_prompt.png" class title="数据合成">
<p>在预训练或者微调阶段加入case2code数据训练，对模型的代码能力都有提升：</p>
<img src="/85132189/case2code_perf.png" class title="数据合成">
<h1 id="tinystories">TinyStories</h1>
<p>论文：《TinyStories: How Small Can Language Models Be and Still Speak
Coherent English?》</p>
<p>时间：2023年5月</p>
<p>机构：微软</p>
<p>这篇文章时间比较久了。首先，观察到对于比较小（比如1.25B参数）的LLM的生成效果一般，会出现无法生成连贯且一致内容的情况。为了研究这个问题，就利用GPT-3.5和GPT-4合成了TinyStories数据集，这个数据集只包含典型3到4岁儿童通常理解的词语。在这个数据集下训练的模型，即使参数只有一千万，也能够生成流畅且一致的故事。</p>
<p>TinyStories数据的合成方式大致是这样的：首先收集一个包含1500个基本单词的vocabulary，其中的单词分为名词、动词和形容词。每次从这个vocabulary中挑选3个单词（一个名词，一个动词，一个形容词），让模型根据这三个词生成一个故事。这种生成方式使得故事具有多样性，并包含了大量将不同的概念结合起来的方式。</p>
<p>下面是一个prompt的例子：</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>Write a short story (<span class="dv">3</span><span class="op">-</span><span class="dv">5</span> paragraphs) which only uses very simple words that a <span class="dv">3</span> year old child would likely understand.</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>The story should use the verb ”decorate”, the noun ”thunder” <span class="kw">and</span> the adjective ”ancient”. The story</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>should have the following features: the story should contain at least one dialogue, the story has a bad ending.</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>Remember to only use simple words<span class="op">!</span></span></code></pre></div>
<p>利用这个prompt，GPT-3.5给出的结果如下：</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>Once upon a time, <span class="kw">in</span> an ancient house, there lived a girl named Lily. She loved to decorate her room <span class="cf">with</span> pretty things. One</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>day, she found a big box <span class="kw">in</span> the attic. She opened it <span class="kw">and</span> saw many shiny decorations. Lily was very happy <span class="kw">and</span> decided to use</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>them <span class="kw">in</span> her room.</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>As Lily was decorating her room, the sky outside became dark. There was a loud thunder sound, <span class="kw">and</span> Lily got scared. She ran</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>to her mom <span class="kw">and</span> said, ”Mommy, the thunder <span class="kw">is</span> so loud<span class="op">!</span>” Her mom hugged her <span class="kw">and</span> said, ”Don’t worry, it will <span class="cf">pass</span> soon.”</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>But the thunder did <span class="kw">not</span> stop. It got louder <span class="kw">and</span> louder, <span class="kw">and</span> the ancient house started to shake. Suddenly, the roof fell down</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>on the pretty decorations. Lily was sad because her room was <span class="kw">not</span> pretty anymore. The end.</span></code></pre></div>
<p>除了生成连贯的语言，指令的遵循能力也是LLM的一个重点。为此构建了TinyStories-Instruct数据集。数据集里每个故事都以一组指令开头，指令分为四种类型：
- 故事中要包含的词语列表。<br>
- 故事中应该出现的句子。<br>
- 特征，比如对话、冲突、转折、bad ending等<br>
- 故事的简短摘要（1-2行）</p>
<p>一个例子如下：</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>Summary: Lily <span class="kw">and</span> Timmy build a sandcastle together <span class="kw">and</span> learn to compromise, but it gets knocked over by a gust of wind.</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>They find beauty <span class="kw">in</span> the broken sandcastle <span class="kw">and</span> play happily <span class="cf">with</span> a butterfly.</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>Features: Dialogue, Foreshadowing, Twist</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>Sentence: One day, she went to the park <span class="kw">and</span> saw a beautiful butterfly.</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>Words: disagree, network, beautiful</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>Story: Once upon a time, there was a little girl named Lily. She loved to play <span class="cf">with</span> her toys <span class="kw">and</span> her friends. One day, she went to the park <span class="kw">and</span> saw a beautiful butterfly. She tried to catch it, but it flew away. Lily’s friend, Timmy, came to play <span class="cf">with</span> her.</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>They decided to build a sandcastle, but they disagreed on how to build it. Lily wanted to make it tall, <span class="cf">while</span> Timmy wanted to</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>make it wide. They argued <span class="cf">for</span> a <span class="cf">while</span>, but <span class="cf">finally</span> agreed to make it both tall <span class="kw">and</span> wide. As they were building the sandcastle,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>they noticed a group of ants working together. They realized that the ants were building a network of tunnels. Suddenly, a gust</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>of wind came <span class="kw">and</span> knocked over their sandcastle. They were sad, but then they saw the butterfly again. It landed on the remains</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>of their sandcastle <span class="kw">and</span> they realized that it was still beautiful, even <span class="kw">in</span> its broken state. They smiled <span class="kw">and</span> played together happily.</span></code></pre></div>
<h1 id="小结">小结</h1>
<p>数据的合成最终就是要解决这几个问题： -
多样性：真实的数据来自现实世界不同场景和不同人物，具有很强的多样性，而合成数据往往受限于prompt和model的特性，多样性有限<br>
-
一致性：合成的数据分布要和真实数据分布一致，否则在推理的时候遇到了不同分布的输入，效果就会大打折扣<br>
-
高质量：合成数据在多样化、拟合真实分布的情况下，还应尽量具有高质量的内容（毕竟真实数据中也有很多低质量内容，但这些内容已经被证实价值不高）</p>
<p>其中一致性来自于prompt的编写，和用于合成数据的LLM的理解能力；质量提升通常使用后筛选模型来做，或者如执行反馈这样的方法；而多样性则是最考验数据合成设计的一个环节，这个环节在数据量大的情况下也是最重要的。</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门--CLIP</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】A Survey on Data Synthesis and Augmentation for Large Language
Models https://arxiv.org/abs/2410.12896<br>
【2】Scaling Synthetic Data Creation with 1,000,000,000 Personas
https://arxiv.org/abs/2406.20094<br>
【3】Magicoder: Empowering Code Generation with OSS-Instruct
https://arxiv.org/abs/2312.02120<br>
【4】Case2Code: Learning Inductive Reasoning with Synthetic Data
https://arxiv.org/pdf/2407.12504<br>
【5】TinyStories: How Small Can Language Models Be and Still Speak
Coherent English? https://arxiv.org/abs/2305.07759</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>预训练</tag>
        <tag>数据合成</tag>
      </tags>
  </entry>
  <entry>
    <title>代码大模型(二)--OpenCoder</title>
    <url>/7856bcc1.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>最近由M-A-P、无限光年、墨尔本大学、复旦大学等机构共同开发的OpenCoder开源了模型和部分数据，并且后续还会有更多资料放出。先来学习下技术报告的内容。</p>
<p>目前各个规模和阶段的模型在 https://huggingface.co/OpenCoder-LLM
可下载。</p>
<p>OpenCoder有1.5B和8B两个规模的模型，分别有base模型和instruct模型放出。</p>
<img src="/7856bcc1/opencoder_model.png" class title="代码模型">
<p>base模型和instruct模型的效果如下表，从表上数据来看还是不错的，基本上达到Qwen2.5-Coder的水平。</p>
<img src="/7856bcc1/opencoder_perf1.png" class title="代码模型">
<img src="/7856bcc1/opencoder_perf2.png" class title="代码模型">
<h1 id="预训练数据">预训练数据</h1>
<p>OpenCoder构建了RefineCode数据集用于预训练，RefineCode主要包含两部分数据：raw
code和code-related web data。raw
code主要来自github（截至2023年11月），并从The Stack
V2数据集中补充非github数据；而code-related web
data则从web语料库抽取。</p>
<p>RefineCode和The
Stack数据集的对比如下，RefineCode包含了更多的code-related
data，更多的rules，还有language specific rules。</p>
<img src="/7856bcc1/opencoder_refinecode.png" class title="代码模型">
<p>raw code和code-related web data的处理流程示意图如下：</p>
<img src="/7856bcc1/opencoder_dataprocess.png" class title="代码模型">
<h2 id="raw-code">raw code</h2>
<p>1、preprocessing</p>
<p>首先是去除了8M个非文本文件，此外根据文件扩展名，仅保留与编程语言相关的文件，包括代码、数据和文本文件（具体类型参考https://github.com/github-linguist/linguist/blob/main/lib/linguist/languages.yml），low
capacity（这里low capacity大概是指比较小的文件？）或者low
quality的类型也会被筛选掉。</p>
<p>2、去重</p>
<p>因为包含有大量的分支和版本，github中源码数据重复率比较高（大约75%的文件完全重复），因此去重的影响很大。</p>
<p>目前对于代码数据，MinHash+LSH去重是比较成熟的方案了（StarCoder2、DeepSeekCoder）。在这个基础上，DeepSeekCoder提出了repository
level的去重，OpenCoder这里做了实验对比file level和repository
level去重的效果。具体来说，在485M个python文件上分别使用file
level和repository level去重，结果上，repository
level保留的token数大约是file level是三倍：</p>
<img src="/7856bcc1/opencoder_deduplication_level.png" class title="代码模型">
<p>用这两份数据分别训练1.5B的模型，结果是file level的效果更好：</p>
<img src="/7856bcc1/opencoder_deduplication_perf.png" class title="代码模型">
<p>去重细节上，先基于SHA256进行了精确去重，之后进行模糊去重。模糊去重使用5-gram，计算2048个MinHash函数；而LSH设置为band=16，row=128。遇到重复文件时，会保留star数更多，commit时间更晚的那一份文件。这一步大约去除了6%的文件。</p>
<p>3、transformation</p>
<p>有一些问题本身在每个文件中占比不多，但是在各个文件重都普遍存在，比如源码文件开头的版权声明：“Copyright
Intel Corporation (C)
2014-2016”。对于这种情况，直接删除整个文件显然是不合适的，因此在过滤模块之前需要针对这些问题进行识别和转换改写。</p>
<p>4、Filtering</p>
<p>在《Textbooks Are All You
Need》中，关于代码数据质量有一些评价和准则：<br>
- Many samples are not self-contained, meaning that they depend on other
modules or files that are external to the snippet, making them hard to
understand without additional context.<br>
- Typical examples do not involve any meaningful computation, but rather
consist of trivial or boilerplate code, such as defining constants,
setting parameters, or configuring GUI elements.<br>
- Samples that do contain algorithmic logic are often buried inside
complex or poorly documented functions, making them difficult to follow
or learn from.<br>
- The examples are skewed towards certain topics or use cases, resulting
in an unbalanced distribution of coding concepts and skills across the
dataset.</p>
<p>参考这些原则，OpenCoder过滤数据时考虑这几个做法：<br>
- 将self-containment差的文件过滤掉<br>
- 将logical structure差的文件过滤掉<br>
- 将和standard format差很多的文件过滤掉</p>
<p>基于这些guildline，OpenCoder开发了一个启发式过滤框架。在RedPajama的基础上，完善了StarCoder的规则。过滤规则分为三类：<br>
- Natural Language Filtering
Rules：通用的rules，比如文件大小、行数等所有数据通用的指标，代码数据和文本数据都共享<br>
- General Code Filtering
Rules：适用于所有代码文件，比如变量数量、平均函数长度等<br>
- Language-Specific Filtering
Rules：语言定制的规则，比如python语言中pass的频率，或者C语言中goto语句的使用</p>
<p>启发式过滤会计算很多指标，这就涉及到很多阈值的调整。开发过程中，会先按经验设置一个大概阈值，然后根据运行结果再进行精细调整。调整阈值的原则就是在尽量过滤掉低质量数据的情况下，保持数据的整体分布不受明显的影响。</p>
<p>在检查一个阈值的有效性时，可以引入入PPL等评估手段，看被过滤掉的数据是否是PPL极高或者极低的数据。</p>
<p>OpenCoder实践中一些阈值设置的example：</p>
<img src="/7856bcc1/opencoder_rules.png" class title="代码模型">
<img src="/7856bcc1/opencoder_python_rule.png" class title="代码模型">
<p>4、sampling</p>
<p>在尽量保持数据分布的情况下，对资源过多的语言类型进行下采样。比如Java数据从409GB减少到200GB，HTML从213GB减少到64GB，最终得到了730B
token的数据。</p>
<p>用PCA对从CodeBert获得的The Stack
V2和RefineCode的embedding进行可视化，如下图：</p>
<img src="/7856bcc1/opencoder_data_dist.png" class title="代码模型">
<p>可以看到The Stack
V2有更多的异常数据，这些数据包括纯文本注释、16进制的数据文件以及过短代码等。这些数据都是对训练有害的。相比之下RefineCode的分布更为紧密，有更少的异常数据。</p>
<h2 id="code-related-web-data">code-related web data</h2>
<p>受DeepSeekMath启发，OpenCoder也从web数据中收集代码相关的数据。</p>
<p>首先参考《Automathtext: Autonomous data selection with language
models for mathematical texts》的方法，AutoDS，从CommonCrawl中选取50w
high-quality code-like data。</p>
<p>这些数据会用于训练fasttext。为了在fasttext中保持vocab不要太大，会用BPE
tokenizer处理预料库，再进行fasttext训练。</p>
<p>训练好的fasttext会从大量的web数据中筛选出代码相关数据。对召回的数据进行分析，把来自相同base
url（如stackoverflow）的页面定义为同一个domain。这一步大概把召回的数据中的10%判断为code-related。</p>
<p>code-relate的域名比如stackoverflow会被手动标注出来，那些域名为code-related，但是又没有被fasttext分对的数据会被手动加入到正类中。跑了3个iteration之后（重复标注&amp;打标），总共获得了220G的code-related
web data。</p>
<p>这个pipeline也应用到FineWeb、Skypile和AutoMathText中，recall了330GB的code-related
web
data。另外，发现github数据里也有部分类似的数据，从中也抽了178GB。</p>
<p>手动标注属于代码和数学的url如下：</p>
<img src="/7856bcc1/opencoder_url.png" class title="代码模型">
<h2 id="refinecode数据集">RefineCode数据集</h2>
<p>最终得到用于预训练的RefineCode数据集包含960B数据，组成如下：</p>
<img src="/7856bcc1/opencoder_ptm_data.png" class title="代码模型">
<p>其中代码数据语言的分布如下：</p>
<img src="/7856bcc1/opencoder_lang_dist.png" class title="代码模型">
<h2 id="annealing-data">annealing data</h2>
<p>现在我们已经知道，预训练的退火阶段对模型的效果影响很大。</p>
<p>首先这期间的数据分布不能有太显著的变化，否则会导致模型的灾难性遗忘。因此在退火阶段，84%的数据和来自RefineCode的原始分布。</p>
<p>在这个基础上，加入高质量的数据来提升模型的最终效果。</p>
<p>1、algorithmic corpus</p>
<p>从包含"leetcode"、"def solution"、"class
solution"等关键字的原始预训练数据中采样了部分算法数据，这些数据有很强的逻辑性，和self-contain的特性。</p>
<p>2、合成数据</p>
<p>合成数据有两种形式：High Quality Code Snippet和Code Textbooks。</p>
<p>（1）High Quality Code Snippet</p>
<p>参照Phi系列合成CodeExercises数据集的做法，用algorithmic
corpus作为种子，让LLM合成self-contained independent
functions，以及对应的测试csae，并且通过执行反馈保留通过测试case的函数。这个方法在多种编程语言都使用了。</p>
<p>（2）Code Textbooks</p>
<p>使用Qwen2-72B-Instruct在hqcode上生成educational text snippets。</p>
<p>hqcode是由gpt-4o-mini合成的代码数据，每条数据包含一段自然语言描述的问题，以及对应的solution。（https://huggingface.co/datasets/yuxiang630/hqcode）</p>
<p>合成数据时要求Qwen2-72B-Instruct在hqcode上对代码进行分析（interactive
analysis），并解释相关的代码知识。</p>
<h1 id="预训练">预训练</h1>
<ul>
<li>使用WSD lr schedule<br>
</li>
<li>warmup = 2000<br>
</li>
<li>seqlen = 8192<br>
</li>
<li>global bs = 1024 sample -&gt; 8M token<br>
</li>
<li>lr = 3e-4</li>
</ul>
<p>前13w步使用的maxseq=4096，bs=2048。</p>
<p>整个训练在512个H100跑了187.5小时。</p>
<h1 id="post-training">post training</h1>
<h2 id="数据">数据</h2>
<p>包括四个部分。</p>
<p>1、Open-source Training Data</p>
<p>收集了Evol-Instruct、Infinity-Instruct、McEval数据。在这个基础上训了个二分类模型用于从Infinity-Instruct中抽取代码相关数据。</p>
<p>另外还从WildChat和Code-290k-ShareGPT中抽取真实的用户query以及和代码相关的对话历史。对于质量较低的部分还用LLM来重新生成内容。</p>
<p>2、Educational Instruction Synthesis</p>
<p>和前面合成python数据类似，不过在这个基础上使用了一个评分模型，用于筛选出高质量的种子数据，以进一步提高合成的数据的质量。</p>
<p>所用prompt：</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>You are a teaching assistant helping to create a Python programming task <span class="im">from</span> a given code snippet. You must provide the best response to the Python programming task, including reasoning thought, reference solutions, explanation of test cases, <span class="kw">and</span> test code.</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>[Code Snippet]</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>&#123;Code&#125;</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>Your response must have these parts:</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>[Task]</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>&#123;Create an independent <span class="kw">and</span> detailed Python programming task&#125;</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>[Analysis]</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>&#123;Analyze the task <span class="kw">and</span> reason about the given task step by step&#125;</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>[Solution]</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>&#123;Write a high<span class="op">-</span>quality reference solution <span class="kw">in</span> a <span class="va">self</span><span class="op">-</span>contained script that solves the task&#125;</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>[Test]</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>&#123;Provide ten <span class="cf">assert</span> statements to check the correctness of your solution&#125;</span></code></pre></div>
<p>3、Package-related Instruction Synthesis</p>
<p>package经常会更新，而LLM有可能在训练数据集中学了一些过时的用法和接口。因此搞了一个最新的代码集，用来微调模型，让模型能尽量给出最新版本的答案。</p>
<p>所用prompt：</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>You are exceptionally skilled at crafting high<span class="op">-</span>educational level problems <span class="kw">and</span> offering precise solutions. Please gain inspiration <span class="im">from</span> the following code snippet to create a highquality programming problem, which <span class="kw">is</span> beneficial <span class="cf">for</span> learning the use of corresponding</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>libraries. Present your output <span class="kw">in</span> two distinct sections: [Problem Description] <span class="kw">and</span> [Solution].</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>[Code Snippet]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>&#123;Code&#125;</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>[Library Api Requirements]</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>&#123;Api Requirements&#125;</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>[Library Api Doc]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>&#123;Api Doc&#125;</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>Guidelines <span class="cf">for</span> each section:</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="fl">1.</span> [Problem Description]: This should be <span class="op">**</span>completely <span class="va">self</span><span class="op">-</span>contained<span class="op">**</span>, providing <span class="bu">all</span> the contextual information one needs to understand <span class="kw">and</span> solve the problem. Assume common programming knowledge, but ensure that <span class="bu">any</span> specific context, variables, <span class="kw">or</span> code snippets pertinent to this problem are explicitly included. This problem should be <span class="op">**</span>educational <span class="cf">for</span> learning the provided Library api, <span class="kw">and</span> please explicitly request the use of the relevant package <span class="kw">in</span> the question. This question should only concern the writing of <span class="op">**</span>one function<span class="op">**</span>, <span class="kw">and</span> you need to be clear about the function name <span class="kw">and</span> role of this function.</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="fl">2.</span> [Solution]: Offer a comprehensive, <span class="op">**</span>correct<span class="op">**</span> solution that addresses the [Problem Description] you provided. This solution should follow the standard of corresponding Library Api doc. Please ensure that the Solution only involves answering the Problem, <span class="op">**</span>without addressing the requirements I provided<span class="op">!**</span> Please provide essential explanation abouth this solution, especially the use of requiremed Library Api.</span></code></pre></div>
<p>4、Large-scale Diverse Instruction Synthesis</p>
<p>参考《Mammoth2: Scaling instructions from the
web》的做法来提升instruction的多样性。</p>
<p>（1）首先，使用web数据中有用的句子作为生成问题的种子。（2）用一个task
specification
module随机选择语言、难度、任务类型，基于这些设置生成prompt。（3）更大参数量的LLM生成问题和对应的答案，并结合执行反馈来筛选正确样本。（4）用大模型给代码添加注释和解释。</p>
<p>所用prompt：</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>You are an expert <span class="kw">in</span> designing high<span class="op">-</span>quality programming questions based on the given text.</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>[Guidelines]</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> You can draw inspiration <span class="im">from</span> the given text to create the programming questions.</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> The created question should be a <span class="va">self</span><span class="op">-</span>contained question, which does <span class="kw">not</span> depend on <span class="bu">any</span> external context.</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> The created response must contain the complete code snippet.</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>[Given Text]</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>&#123;Given Text&#125;</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>[Created Question]</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>&#123;Created Question&#125;</span></code></pre></div>
<h2 id="训练">训练</h2>
<p>fine-tuning分成两个阶段。</p>
<p>第一阶段注重理论知识，让模型学习计算机原理、算法、数据结构等。</p>
<p>第二阶段把重点从理论转到实际任务。</p>
<p>两个阶段的数据组成如下：</p>
<img src="/7856bcc1/opencoder_sft_data.png" class title="代码模型">
<h1 id="autonomous-data-selectionautods">Autonomous Data
Selection（AutoDS）</h1>
<p>前面提到从web数据中收集代码相关数据的方法的时候，参考了AutoDS的做法。</p>
<p>比如我们想要用LLM给一段文本在某些维度上打分，比如代码风格、质量等，一般来说可能需要给一些训练数据线微调一下。这些微调数据的打分基本上也都是离散的，因为人类标注结果就是离散的，1分2分5分这样。这种traditional的做法一方面需要引入人类的打分，不能完全自动化，另一方面人类本身的打分也是有偏，并且打分的结果也只能是离散的，无法连续。</p>
<p>AutoDS提出的做法，不需要引入人类标注数据，也不需要微调，完全依赖模型自身学到的内容，并且可以得到更加公正、连续的打分结果。</p>
<p>具体来说，AutoDS就是在打分prompt中要求打分模型回答关于输入样本的问题，而这些问题只能用"YES"和"NO"来回答。比如问题是“这条数据是否和数学相关”，那么模型的打分结果就是：</p>
<p><span class="math display">\[\mathrm{LM-Score}(\cdot)=\frac{\exp(\mathrm{logit}(`\text{YES&#39;}))}{\exp(\mathrm{logit}(`\text{YES&#39;}))+\exp(\mathrm{logit}(`\text{NO&#39;}))}\]</span></p>
<p>一个prompt的例子如下：</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>system<span class="op">&gt;</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>You are ChatGPT, equipped <span class="cf">with</span> extensive expertise <span class="kw">in</span> mathematics <span class="kw">and</span> coding, <span class="kw">and</span> skilled <span class="kw">in</span> <span class="bu">complex</span> reasoning <span class="kw">and</span> problem<span class="op">-</span>solving. In the following task, I will present a text excerpt <span class="im">from</span> a website. Your role <span class="kw">is</span> to evaluate whether this text exhibits mathematical intelligence <span class="kw">and</span> <span class="cf">if</span> it <span class="kw">is</span> suitable <span class="cf">for</span> educational purposes <span class="kw">in</span> mathematics. Please respond <span class="cf">with</span> only YES <span class="kw">or</span> NO</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;/</span>system<span class="op">&gt;</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>User: &#123;</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>“url”: “&#123;url&#125;”,</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>“text”: “&#123;text&#125;”</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>&#125;</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="fl">1.</span> Does the text exhibit elements of mathematical intelligence?</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>Respond <span class="cf">with</span> YES <span class="kw">or</span> NO</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="fl">2.</span> Is the text suitable <span class="cf">for</span> educational purposes <span class="cf">for</span> YOURSELF <span class="kw">in</span> the field of mathematics? Respond <span class="cf">with</span> YES <span class="kw">or</span> NO</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>Assistant: <span class="fl">1.</span></span></code></pre></div>
<p>这里的输入样例除了text之外还给了url，可以在一定程度上帮助模型识别内容，回答问题（比如是常用的数学/代码网站）。</p>
<p>一条prompt里可以同时问多个问题，这多个问题的打分通过相乘结合起来，得到最终分数。</p>
<p><span class="math display">\[\mathrm{LM-Score}(Q_1,Q_2)=\mathrm{LM-Score}(Q_1)\cdot\mathrm{LM-Score}(Q_2)\]</span></p>
<p>论文中使用的是Qwen-72B-base模型作为打分模型，这里不使用instruct模型估计是为了更好地适配打分的prompt格式。</p>
<p>文中用这个方法从多个source数据集来收集数学相关的内容，不同的来源的prompt可能略有不同，比如对arXiv数据，prompt中输入数据会把text拆分成abstract和text正文两条：</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>system<span class="op">&gt;</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>You are ChatGPT, the most capable large language model equipped <span class="cf">with</span> extensive expertise <span class="kw">in</span> mathematics <span class="kw">and</span> coding, particularly skilled <span class="kw">in</span> <span class="bu">complex</span> reasoning <span class="kw">and</span> problem<span class="op">-</span>solving. In the following interaction, I will provide you <span class="cf">with</span> a text excerpt <span class="im">from</span> the arXiv website. Your task <span class="kw">is</span> to evaluate whether this text contains elements of mathematical intelligence <span class="kw">and</span> <span class="cf">if</span> it <span class="kw">is</span> suitable <span class="cf">for</span> educational purposes <span class="cf">for</span> YOURSELF <span class="kw">in</span> the field of mathematics. Please respond <span class="cf">with</span> only YES <span class="kw">or</span> NO</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;/</span>system<span class="op">&gt;</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>User: &#123;</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>“Title”: “&#123;title&#125;”,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>“Abstract”: “&#123;abstract&#125;”,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>“Text”: “&#123;text&#125;”</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>&#125;</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="fl">1.</span> Does the text contain elements of mathematical intelligence? Reply <span class="cf">with</span> only YES <span class="kw">or</span> NO</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="fl">2.</span> Is the text suitable <span class="cf">for</span> educational purposes <span class="cf">for</span> YOURSELF <span class="kw">in</span> the field of mathematics? Reply <span class="cf">with</span> only YES <span class="kw">or</span> NO</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>Assistant: <span class="fl">1.</span> </span></code></pre></div>
<p>用AutoDS选择数据训练的效果：</p>
<img src="/7856bcc1/autods_perf.png" class title="代码模型">
<h1 id="小结">小结</h1>
<p>在数据上，OpenCoder引入了更细致的清洗和合成方法，获得了更高质量的数据。希望预训练数据和pipeline能早点开源。</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门--CLIP</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】OpenCoder: The Open Cookbook for Top-Tier Code Large Language
Models https://arxiv.org/abs/2411.04905v1<br>
【2】Textbooks Are All You Need https://arxiv.org/abs/2306.11644<br>
【3】Autonomous Data Selection with Language Models for Mathematical
Texts https://arxiv.org/abs/2402.07625</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>预训练</tag>
        <tag>对齐</tag>
        <tag>数据</tag>
        <tag>代码能力</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</title>
    <url>/569d722c.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>这篇主要包括BLIP系列、LLaVA系列和Flamingo，大致上按时间顺序排列。</p>
<h1 id="blip">BLIP</h1>
<p>论文：《BLIP: Bootstrapping Language-Image Pre-training for Unified
Vision-Language Understanding and Generation》</p>
<p>时间：2022年1月</p>
<p>机构：Salesforce</p>
<p>在这个时间点，大多数多模态模型只在理解任务或者生成任务中的一个能做得比较好。BLIP的目的是训练一个同时具备理解和生成能力的模型，提出了一个unified
VLP（Vision-Language
Pre-Training）framework。BLIP主要的工作在于模型架构的设计，以及多模态数据集的优化。</p>
<h2 id="med模型">MED模型</h2>
<p>BLIP提出了一个MED模型，MED = Multimodal mixture of
Encoder-Decoder。MED模型的设计如下：</p>
<img src="/569d722c/blip_archi.png" class title="多模态入门">
<p>可以看到MED包含好几个模块，这是一个multi-task模型。左边两个模块分别是image
encoder和text encoder，和CLIP中使用的类似。这里使用的image
encoder是在ImageNet预训练好的ViT，而text
encoder是预训练好的Bert-base模型。右边两个是混合图文模态的encoder和decoder。</p>
<p>MED有三种功能：</p>
<ul>
<li>Unimodal encoder：分别对图像和文本进行编码<br>
</li>
<li>Image-grounded text
encoder：用一个位于text-encoder的FFN层与注意力层之间的cross-attention模块，注入视觉信息<br>
</li>
<li>Image-grounded text decoder：和Image-grounded text
encoder类似，只是decoder里的注意力层是causal
attention而不是双向注意力</li>
</ul>
<p>预训练的时候，针对不同的功能，有不同的loss：</p>
<ul>
<li>Image-Text Contrastive Loss (ITC)：和Unimodal
encoder相关，类似CLIP，使用对比学习，目标是对齐vision encoder和text
encoder；这里参考《Align before fuse: Vision and language representation
learning with momentum distillation》，使用了ITC损失，其中应用了momentum
encoder来获得feature<br>
</li>
<li>Image-Text Matching Loss
(ITM)：ITM是一个二分类任务，任务是让Image-grounded text
encoder判断图像和文本是不是匹配的<br>
</li>
<li>Language Modeling Loss (LM)：要求Image-grounded text
decoder在给定图像特征的情况下，给出文字描述的预测</li>
</ul>
<p>在实际训练中，为了提升训练效率，text encoder和text
decoder共享除了注意力层以外的参数。这里做了消融实验，共享参数比不共享参数效果更好，而且效率更高：</p>
<img src="/569d722c/blip_share_sa.png" class title="多模态入门">
<h2 id="capfilt-captioning-and-filtering">CapFilt (Captioning and
Filtering)</h2>
<p>多模态的标注数据成本是比较高的，因此很多工作都会使用从网上自动获取的大规模图文数据对作为训练数据。但是这样又引入了一个问题，那就是网络图文数据本身是带有很多噪音的，比如解析的时候图文错配，或者图文数据本身无关等。</p>
<p>BLIP用来提升图文数据质量的方法称为CapFilt，大致流程如下：</p>
<img src="/569d722c/blip_train_framework.png" class title="多模态入门">
<p>CapFilt包含两个模块：captioner和filter。这两个模块都由同一个预训练好的MED模型初始化而来。</p>
<p>captioner是image-grounded text
decoder，用来给图片生成caption。而filter是image-grounded text
encoder，基于ITC和ITM目标进行了学习，用来判别数据集中的图文是否匹配，不匹配的数据将会被删除。</p>
<p>文中强调了captioner生成的时候多样性很重要，这点和其他数据合成方法一样。</p>
<p>消融实验对比了captioner和filter共享参数与否的效果，结果上看不共享参数更好：</p>
<img src="/569d722c/blip_share_filter.png" class title="多模态入门">
<h1 id="flamingo">Flamingo</h1>
<p>论文：《Flamingo: a Visual Language Model for Few-Shot Learning》</p>
<p>时间：2022年4月</p>
<p>机构：DeepMind</p>
<p>之前我们讲了CLIP。CLIP的训练方式结合了图文对，使得图像特征能够在一定程度上和文本特征对齐。不过CLIP也有限制，一方面在形式上使用的是固定的图文对，另一方面训练出来的模型主要是image
encoder，应用场景主要是表征、分类、检索等，比较有限。</p>
<p>而Flamingo的目的是能够处理sequences of arbitrarily interleaved visual
and textual data，这样就能完成更多复杂场景下的任务了。</p>
<p>Flamingo-80B处理的一些example如下：</p>
<img src="/569d722c/flam_example.png" class title="多模态入门">
<p>这种few-shot / in-context learning的能力在NLP已经显现出重要性。</p>
<p>LLaVA论文中提到，Flamingo的工作可以被称之为多模态领域的GPT-3
moment，可见这个工作的重要性。</p>
<h2 id="architecture">architecture</h2>
<p>Flamingo整体的架构如下：</p>
<img src="/569d722c/flam_archi.png" class title="多模态入门">
<p>Flamingo的输入是任意图文交织的序列，输出是text序列。</p>
<p>1、Vision Encoder</p>
<p>首先是模型中的Vision Encoder。使用的是预训练好的Normalizer-Free
ResNet (NFNet)，具体使用的是F6
model；其中的模型参数都是冻结的，不会再训练。这里的Vision
Encoder采用了CLIP的训练方式进行了预训练。</p>
<p>对于视频数据，以1FPS进行采样并独立编码。</p>
<p>2、Perceiver Resampler</p>
<p>Perceiver Resampler是Flamingo结构的一个重要部件。Vision
Encoder把像素数据变成feature，而Perceiver Resampler把不同size的feature
map映射到固定的少量visual token（64个），从而降低vision-text
cross-attention的计算量。</p>
<p>那Perceiver Resampler具体是怎么做的？</p>
<blockquote>
<p>Similar to《Perceiver: General perception with iterative
attention》and《End-to-end object detection with transformers》, we
learn a predefined number of latent input queries which are fed to a
Transformer and cross-attend to the visual features.</p>
</blockquote>
<p>结构上，Perceiver Resampler如下图：</p>
<img src="/569d722c/flam_resampler.png" class title="多模态入门">
<p>Vision
Encoder获得的图像token序列，再加上时间（类似Bert中的位置编码，比如对于视频就是第1帧、第2帧...）embedding，得到一个变长的序列，然后加上固定个数的learned
latent
queries，一起放进模型中。模型中的每一层包含attention和FFN模块，attention中把图像输入特征作为K和V，而把learned
latent queries作为Q进行计算。</p>
<p>最后获取learned latent
queries的输出表征，就作为这些输入图像的表示。</p>
<p>3、GATED XATTN-DENSE layers</p>
<p>下一步是利用固定长度的图像特征，让语言模型进行文本生成。把图像特征注入到语言模型就用到GATED
XATTN-DENSE的结构：</p>
<img src="/569d722c/flam_xattn.png" class title="多模态入门">
<p>如上图，这些层其实就是cross-attention加上一个FFN层，再加上一个门控。在最开始的时候，这些门控会让GATED
XATTN-DENSE layers的输出为0，从而保持在初始化的时候语言模型的效果。</p>
<p>GATED XATTN-DENSE layers插入到预训练好的语言模型的层中，用vision
feature作为K和V，而用语言模型的输入作为Q。</p>
<h2 id="训练">训练</h2>
<p>数据上，共使用3种数据训练Flamingo：</p>
<ul>
<li>专门收集MultiModal MassiveWeb (M3W)
数据集，这里面都是交错的图像和文本数据，来自于4300万个网页；在这些网页中，会随机采样256个token长度的数据，每条最多包含5个图片<br>
</li>
<li>成对的图像/视频和文本数据，大约有1.8B对数据</li>
</ul>
<p>这里有个细节，训练的时候，一段文本在注意力上只能看到前一张图片，而看不到更早的图片，更早的图片信息都被mask掉了。</p>
<img src="/569d722c/flam_inter_data.png" class title="多模态入门">
<p>论文对很多设计都做了消融实验，包括所用的数据、门控机制、attention机制、xattn的frequency等：</p>
<img src="/569d722c/flam_ablation.png" class title="多模态入门">
<h1 id="blip-2">BLIP-2</h1>
<p>论文：《BLIP-2: Bootstrapping Language-Image Pre-training with Frozen
Image Encoders and Large Language Models》</p>
<p>时间：2023年1月</p>
<p>机构：Salesforce</p>
<p>目前多模态模型的训练需要同时用到语言和图像模型，这两部分参数的训练成本随着模型和数据规模的提升也越来越高。BLIP-2一个出发点就是用比较低的训练成本获得好的效果。这里只使用了Flamingo的1/54的训练参数，就获得了超过Flamingo的效果。</p>
<p>一个方法就是使用已经训练好的单模态模型，为了避免灾难性遗忘，在训练过程中冻结单模态模型的参数，而把主要工作放在两个模态的对齐上。</p>
<p>在模态对齐这个方向，已经有一些工作，比如Frozen（《Multimodal few-shot
learning with frozen language models》）和Flamingo（《Flamingo: a visual
language model for fewshot learning》）。这两个工作主要使用image-to-text
generation loss来对齐，但是这并不足够。</p>
<p>因此BLIP-2提出一个两阶段的方案，在第一个阶段从冻结的image
encoder学习vision-language
representation，在第二个阶段从冻结的LLM学习image-to-text
generation：</p>
<img src="/569d722c/blip2_intro.png" class title="多模态入门">
<p>BLIP-2的模型包含三个部分：</p>
<ul>
<li>image encoder：预训练好的ViT<br>
</li>
<li>LLM：预训练好的语言模型<br>
</li>
<li>Q-Former：用于对齐图像和文本特征的模型</li>
</ul>
<h2 id="querying-transformer-q-former">Querying Transformer
(Q-Former)</h2>
<p>Q-Former的结构和一阶段的使用如下图：</p>
<img src="/569d722c/blip2_qformer.png" class title="多模态入门">
<p>Q-Former包含两个部分，image transformer和text
transformer，二者共享self-attention部分的参数。</p>
<p>在image transformer中：</p>
<ul>
<li>使用了一组learnable query
embeddings作为输入，这组query的长度固定为32个，这组query的参数可以认为属于模型参数；这组query的目的是学习抽取图像特征中最重要的部分<br>
</li>
<li>冻结的image encoder抽取的图像特征，会在image
transformer里和这组query通过cross-attention进行交互<br>
</li>
<li>每两个层transformer层插入一个cross-attention<br>
</li>
<li>corss-attention的参数随机初始化，而其他参数则是使用Bert-base初始化的</li>
</ul>
<p>而text-transformer：</p>
<ul>
<li>根据训练任务，选择不同的attention mask以及训练目标<br>
</li>
<li>即是encoder也是decoder</li>
</ul>
<h2 id="训练-1">训练</h2>
<p>1、阶段一</p>
<p>阶段一是representation learning
stage。这个阶段的目标是训练Q-Former，使得那组输入query可以提取能表达最多信息的visual
representation。</p>
<p>这里集成BLIP的做法，联合使用3个优化目标：</p>
<ul>
<li>Image-Text Contrastive Learning (ITC) ：image
transformer给出图像的representation，记为Z，text
transformer则给出文本的representation，记为t；这里Z有多个，因此使用Z中和t相似度最大的作为图文对representation的相似度；为了避免信息泄露，这里使用unimodal
self-attention mask（见Q-Former图）<br>
</li>
<li>Image-grounded Text Generation (ITG)
：以输入图像为条件，生成文本；对于LLM，使用[DEC] token替换了[CLS]
token，以指示解码任务<br>
</li>
<li>Image-Text Matching (ITM)：二分类任务，这时使用bi-directional
self-attention mask</li>
</ul>
<p>2、阶段二</p>
<p>阶段二是generative pre-training stage。这一阶段的训练如下图：</p>
<img src="/569d722c/blip2_stage2.png" class title="多模态入门">
<p>先把Q-Former的输出query
embedding转化为和LLM相同大小的维度，然后LLM进行解码。</p>
<p>这里尝试了decoder-based LLM和encoder-decoder-based
LLM。对于纯decoder的模型，直接进行生成就行；而对于encoder-decoder模型，使用prefix
language modeling loss，即把文本切成两段，前一段和visual
representation拼在一起输入encoder，后一段就是生成的目标。</p>
<h2 id="使用">使用</h2>
<p>BLIP-2首先可以做Zero-shot
VQA，只需使用简单的prompt即可让LLM生成答案，比如“Question: {}
Answer:”。</p>
<p>另外论文中微调了BLIP-2来做image
captioning（期间保持LLM的参数固定，训练Q-Former和image
encoder），效果也比较好。</p>
<p>而在图像检索任务上，因为不涉及LLM，可以直接使用第一阶段的模型进行微调。</p>
<h1 id="llava">LLaVA</h1>
<p>论文：《Visual Instruction Tuning》</p>
<p>时间：2023年4月</p>
<p>机构：微软</p>
<p>LLaVA = Large Language and Vision Assistant。</p>
<p>如论文标题，LLaVA做的事情主要就是多模态空间的指令微调。微调后的模型能够进行多模态问答：</p>
<img src="/569d722c/llava_example.png" class title="多模态入门">
<h2 id="数据">数据</h2>
<p>要做微调，首先就要有数据。multimodal instruction-following
data在这个时间点还是很缺的，受到NLP领域的启发，这里也使用ChatGPT/GPT-4，基于已有的图文数据来构建多模态指令数据。</p>
<p>怎么做呢？对于图像Xv和它的caption
Xc，很自然地想到可以创建一系列的问题Xq，目的是instruct the assistant to
describe the image content。而GPT-4就可以用来生成这一系列的问题。</p>
<p>那么，一个原有的图文数据对就可以这样来扩展成它的instruction-following
version：</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>Human:XqXv<span class="op">&lt;</span>STOP<span class="op">&gt;</span>Assistant:Xc<span class="op">&lt;</span>STOP<span class="op">&gt;</span>  </span></code></pre></div>
<p>不过这种方式虽然简便，但是获得的数据在多样性和难度上都比较欠缺。为了缓解这些问题，需要用ChatGPT/GPT-4创建包含视觉内容的指令数据。但是ChatGPT只能接收文字信息，因此需要一个把图像信息装换成文字信息的工具，而object
detection给出的bounding box就是这样一个工具。</p>
<p>有了一张图片的caption，以及其中物体的bounding
box信息之后，就可以要求ChatGPT生成数据，LLaVA设计了三类数据：</p>
<ul>
<li>Conversation：提问题提问关于图片的问题，而assistant“看着”图片回答问题；问题包括图片中的物体类型、计数、动作、位置、位置关系等<br>
</li>
<li>Detailed
description：人工构造了一系列prompt（下图），每次随机抽取一条生成包含详细图片信息的描述<br>
</li>
<li>Complex
reasoning：前面两种都属于直接的视觉信息，而推理任务则包含一些遵循严谨和复杂逻辑推理的过程</li>
</ul>
<img src="/569d722c/llava_instr_list.png" class title="多模态入门">
<p>下图是数据合成的一个例子，上半部分是图片的caption以及物体bounding
box的信息，下半部分是根据三种任务合成的指令数据：</p>
<img src="/569d722c/llava_syn_example.png" class title="多模态入门">
<p>基于COCO数据集，共获得了158k条多模态指令数据，其中对话数据58k，详细描述23k，复杂推理77k。</p>
<h2 id="模型">模型</h2>
<p>LLaVA的多模态模型架构如下：</p>
<img src="/569d722c/llava_archi.png" class title="多模态入门">
<p>主模型是一个LLM，这里选用的Vicuna。</p>
<p>对于输入的图像X，会使用CLIP训练的ViT-L/14作为vision
encoder获得图像的representation
Z，之后通过一个投影矩阵W把Z转换到LLM空间。</p>
<h2 id="训练-2">训练</h2>
<p>在训练的时候，会随机地把第一轮的输入中，图像和文本问题的顺序进行交换，以提升模型在多模态生成能力中的泛化性。</p>
<p>LLaVA的多模态微调使用两阶段的训练。</p>
<p>1、Pre-training for Feature Alignment</p>
<p>使用CC3M的595k条数据。这一阶段的训练过程中，vision
encoder和LLM的参数保持冻结不训练，只训练对齐的投影矩阵W。</p>
<p>2、Fine-tuning End-to-End</p>
<p>这一阶段，依然保持vision
encoder的冻结，而训练W和LLM的参数。微调模型主要针对两个场景：</p>
<ul>
<li>多模态聊天机器人：使用前面从COCO中获得的158k数据<br>
</li>
<li>science QA：在ScienceQA上开发，这里把数据组织成一个单轮对话</li>
</ul>
<h1 id="instructblip">InstructBLIP</h1>
<p>论文：《InstructBLIP: Towards General-purpose Vision-Language Models
with Instruction Tuning》</p>
<p>时间：2023年5月</p>
<p>机构：Salesforce，HKUST，NTU</p>
<p>在BLIP-2上进行instruction
tuning，就获得InstructBLIP了。为什么要进行指令微调？因为指令微调之后模型在没见的任务上的泛化能力能够大大提升。</p>
<p>论文中收集了11个任务，共26个数据集，训练的时候使用其中的13个数据集，保留另外13个作为评测数据。</p>
<img src="/569d722c/instructblip_data.png" class title="多模态入门">
<p>在BLIP-2的基础上，InstructBLIP使用instruction-aware
Q-former。instruction-aware
Q-former将指令文本token作为额外的输入，以提取和任务更加相关的图像特征。</p>
<img src="/569d722c/instructblip_model.png" class title="多模态入门">
<p>在指令微调的时候，由于每个数据集的大小不同，差异很大，直接混合随机采样有可能导致较小的数据集学习不足，而大数据集的任务则容易过拟合。因此文中提出根据数据集大小的平方根进行采样，再加上一些手动调整，防止过拟合。</p>
<p>对于每个任务，设计了多个prompt：</p>
<img src="/569d722c/instructblip_template.png" class title="多模态入门">
<p>这里有个细节：对于回复都比较短的数据集，在prompt里加入了“short”或者“briefly”这样的指令，“假装”回复短时应为prompt要求的，从而避免在使用模型的时候总是生成太短的结果。</p>
<h1 id="llava-1.5">LLaVA-1.5</h1>
<p>论文：《Improved Baselines with Visual Instruction Tuning》</p>
<p>时间：2023年10月</p>
<p>机构：微软</p>
<p>LLaVA-1.5是LLaVA的升级版，包括数据和模型。</p>
<img src="/569d722c/llava1.5_intro.png" class title="多模态入门">
<h2 id="模型-1">模型</h2>
<ul>
<li>首先是LLM的升级，增加了Vicuna-13B的版本，获得更强的理解和生成能力。</li>
</ul>
<blockquote>
<p>Results on MM-Vet shows the most significant improvement when scaling
the LLM to 13B, suggesting the importance of the base LLM’s capability
for visual conversations.</p>
</blockquote>
<ul>
<li><p>另外就是对齐文本和图像的模块由简单的dense层升级为两层的MLP层。</p></li>
<li><p>此外就是把vision
encoder从ViT-L/14-224px升级为ViT-L/14-336px。</p></li>
</ul>
<p>虽然使用更强的ViT模型可以提供更高的分辨率，获得更好的效果，但是这种替换模型的方法总有个头，因为预训练模型就那么大，从base到large，顶多就到xxl。以往的一些提升分辨率的做法是使用positional
embedding
interpolation，并且在微调的时候让模型适应到新的分辨率，这往往需要很大量的数据，效率不高。</p>
<p>为了解决分辨率的问题，LLaVA-1.5的做法如下：</p>
<img src="/569d722c/llava1.5_hd.png" class title="多模态入门">
<p>把大图像分割成适配vision
encoder的大小（比如224/336）的块，然后独立对这些小块进行编码，获得各个块的representation，最后再合并成target
resolution 的feature
map。这种分割难免会引入一些artifact，因此在这之外还加了一路操作，对原图做resize，以提供全局的信息。</p>
<p>这种做法让LLaVA-1.5理论上可以处理resolution的数据，同时也保证了数据效率（不用重训）。</p>
<h2 id="数据-1">数据</h2>
<p>1、增加Academic task oriented data</p>
<p>数据上，首先纳入了更多的数据集，包括VQA、OCR，region-level
perception的数据：OKVQA、A-OKVQA、OCRVQA、TextCaps等。</p>
<p>实验上，发现加入region-level
perception数据能提升模型处理fine-grained视觉信息的能力。</p>
<p>2、数据格式 &amp; prompt</p>
<p>LLaVA在回答短问题的时候，效果没那么好，因为训练的过程中缺乏相关的数据。而InstructBLIP引入了相关数据，但是却没法在长回答和短回答的VQA任务上很好地平和，即在需要长回答的情况下，可能也会给出短的答案，如下图：</p>
<img src="/569d722c/llava1.5_example.png" class title="多模态入门">
<p>造成InstructBLIP这个问题的原因：（1）没有微调LLM（2）prompt不够明确。</p>
<p>因此LLaVA-1.5优化了prompt的设计，使用一个统一的能够指示输出格式的prompt。当需要简短答案是，会在prompt加上“Answer
the question using a single word or
phrase.”，如上图。在这样的prompt下，</p>
<h2 id="实验">实验</h2>
<p>各种改进的效果如下表，蓝色是数据的部分，粉色是模型部分，而黄色是分辨率的变化：</p>
<img src="/569d722c/llava1.5_scaling.png" class title="多模态入门">
<p>而和其他模型在各个指标上的对比如下：</p>
<img src="/569d722c/llava1.5_compare.png" class title="多模态入门">
<p>LLaVA-1.5整体的训练成本大约是20小时 ×
8卡A100机器，相比LLM来说并不大。</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】BLIP: Bootstrapping Language-Image Pre-training for Unified
Vision-Language Understanding and Generation
https://arxiv.org/abs/2201.12086<br>
【2】BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image
Encoders and Large Language Models
https://arxiv.org/abs/2301.12597<br>
【3】InstructBLIP: Towards General-purpose Vision-Language Models with
Instruction Tuning https://arxiv.org/abs/2305.06500<br>
【4】https://zhuanlan.zhihu.com/p/685233706<br>
【5】Flamingo: a Visual Language Model for Few-Shot Learning
https://arxiv.org/abs/2204.14198<br>
【6】Visual Instruction Tuning https://arxiv.org/abs/2304.08485<br>
【7】Improved Baselines with Visual Instruction Tuning
https://arxiv.org/abs/2310.03744<br>
【8】https://zhuanlan.zhihu.com/p/721428034</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>transformer</tag>
        <tag>预训练</tag>
        <tag>多模态</tag>
        <tag>CV</tag>
        <tag>CNN</tag>
        <tag>无监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型算法题(4)</title>
    <url>/1736008.html</url>
    <content><![CDATA[<p><img src="/images/cover.png"></p>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a></p>
<hr>
<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<p>本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新<sub>如有错漏，欢迎指正</sub></p>
<hr>
<h1 id="为什么transformer用layernorm而不是batchnorm">1.为什么Transformer用layernorm而不是batchnorm</h1>
<p>首先，NLP数据中由于每条样本可能不一样长，会使用padding，如果对padding部分进行normalization，对效果有负面影响。直观来说，batchnorm会对同一个特征以batch为组进行归一化，而对于文本数据，同一个位置的token很可能是没有关联的两个token，对这样一组数据进行归一化没有什么实际意义。《PowerNorm:
Rethinking Batch Normalization in
Transformers》论文的实验也表明，在NLP数据使用batchnorm，均值和方差相对layernorm会更加震荡，因此效果欠佳。</p>
<h1 id="transformer中encdoer和decoder是怎么进行交互的">2.transformer中，encdoer和decoder是怎么进行交互的？</h1>
<p>decoder部分的输入，在每层中，先进行一次self-attention；之后用encoder的输出作为attention计算中的K、V，decoder的输入作为Q，进行cross-attention。</p>
<img src="/1736008/transformer.png" class title="transformer">
<h1 id="pytorch中tensor的view和reshape两个方法有什么区别">3.PyTorch中，Tensor的view()和reshape()两个方法有什么区别？</h1>
<p>1.功能上：view()与reshape()方法都可以用来改变tensor的形状，但是使用条件不同，view()能做的是reshape的子集。</p>
<p>2.view()方法需要tensor满足连续性，操作后返回一个引用，返回值是视图，没有改变原储存空间的值，多个视图共享同一个物理储存空间的内容。</p>
<p>3.reshape()方法不需要tensor一定满足连续性。如果tensor不满足连续性的要求，则会使用新的储存空间并返回。如果满足连续性需求，则功能和view()一致。</p>
<p>4.连续性：比如一个二维张量，如果按行优先展开成一维的结果，和物理储存顺序是一致的，就是连续的。可以用is_contiguous()来判断一个张量是否连续，如果不连续，可以用contiguous()得到一份新空间中的连续副本。</p>
<h1 id="rlhf中ppo需要哪几个模型分别是什么作用">4.RLHF中，PPO需要哪几个模型，分别是什么作用？</h1>
<p>一般来说，PPO需要使用4个模型。</p>
<p>1.Actor模型：由SFT初始化，就是进行强化学习的主模型，是我们想要最终获得的模型；它不断产生action并被Critic模型所评价，计算loss进行训练。</p>
<p>2.Reference模型：一般也是从SFT模型初始化，RLHF中Reference模型并不更新参数，只是作为Actor模型的参考使用；通过约束Actor模型和Reference模型的KL
penalty等，可以防止Actor模型被训得跑得太偏。</p>
<p>3.Reward模型：提前训练好的，对SFT模型进行打分的模型，RLHF中参数是冻结的。</p>
<p>4.Critic模型：一般由Reward模型进行初始化，参数可训练，用于预测Actor模型生成的token的收益。</p>
<h1 id="gpt类模型训练过程中消耗显存的主要有哪些部分分别是多少哪部分占用最多假设模型有l层词表大小为vhidden-size为hbatch-size为b训练窗口长度为s使用adam优化器混合精度训练需要存一阶和二阶动量注意力头数为n">5.GPT类模型训练过程中，消耗显存的主要有哪些部分？分别是多少？哪部分占用最多？假设模型有L层，词表大小为V，hidden
size为H，batch
size为B，训练窗口长度为S，使用Adam优化器混合精度训练（需要存一阶和二阶动量），注意力头数为N。</h1>
<p>训练过程中，显存消耗主要有模型参数、梯度、optimizer状态值和中间激活值。</p>
<p>1.模型参数Φ：词表部分VH，每层参数12H<sup>2+13H，总共有Φ=VH+L(12H</sup>2+13H)，如果是半精度就是2Φ</p>
<p>2.梯度：每个参数对应有一个梯度，总量为Φ，如果是半精度就是2Φ</p>
<p>3.optimizer状态值：每个参数有一个对应梯度，每个参数又对应优化器一个一阶动量和二阶动量。在混合精度训练中，使用半精度进行前向计算和梯度计算，同时优化器备份一份单精度的优化器状态、梯度和参数用于更新参数，因此共有(Φ+Φ)<em>2+(Φ+Φ+2Φ)</em>4=20Φ，除去参数和梯度，优化器占部分16Φ</p>
<p>4.激活值：保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。总共是34BSH+5BNS^2，如果都是半精度，就乘以2。</p>
<p>模型参数、梯度和优化器状态和输入长度无关，是固定值，而激活值随着长度增加，以平方速度增长。
以GPT3（175B）为例，H=12288，L=96，N=96。模型参数量显存越为350G。以B=1计算，如果S=1024，激活值约为90G；如果S=8192，激活值约为3420G。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a></p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>算法题</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</title>
    <url>/f16505b3.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>这篇主要包括Qwen-VL系列，MiniGPT-4，InternVL和DeepSeek-VL，大致上按时间顺序排列。</p>
<h1 id="minigpt-4">MiniGPT-4</h1>
<p>论文：《MiniGPT-4: Enhancing Vision-Language Understanding with
Advanced Large Language Models》</p>
<p>时间：2023年4月</p>
<p>MiniGPT-4是对GPT-4的多模态能力的一次探索和尝试。</p>
<p>先说结论：</p>
<ul>
<li>固定vision encoder和LLM，仅训练对齐部分，就能获得不错的效果<br>
</li>
<li>仅使用image
caption进行对齐并不能获得很好的效果，而用小部分的detailed image
description pairs进行further finetuning就能打破这个限制<br>
</li>
<li>MiniGPT-4表现出了和GPT-4类似的能力，比如生成复杂的image
descriptions，从手写文本生成网页，还有GPT-4没有的能力，比如从食物照片生成详细的烹饪食谱，根据图像编写故事或诗歌，为图像中的产品编写广告等</li>
</ul>
<h2 id="模型结构">模型结构</h2>
<p>MiniGPT-4的模型结构如下：</p>
<img src="/f16505b3/minigpt4_intro.png" class title="多模态入门">
<p>LLM使用的是Vicuna，而图像部分和BLIP-2相同，采用ViT +
Q-Former的方式，对齐部分使用的是简单的linear layer。</p>
<p>其中image encoder（和Q-Former）和LLM的参数都保持固定，只训练linear
layer，因此训练很快。</p>
<h2 id="训练">训练</h2>
<p>MiniGPT-4采用两阶段的训练方式。</p>
<p>1、First pretraining stage</p>
<p>在这一阶段，模型的目的是从大量对齐的image-text
pair获取vision-language knowledge。使用的数据集有Conceptual
Caption、SBU和LAION，batch size = 256，训了2w步左右。</p>
<p>第一阶段完成后，MiniGPT-4有一定的图文响应能力，但是生成并不流畅。</p>
<p>2、Second-stage finetuning</p>
<p>为了优化效果，需要做第二阶段的对齐。为此MiniGPT-4专门构建了一个数据集。</p>
<p>首先，让第一阶段获得的模型，基于下面的prompt生成图像描述：</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">###Human: &lt;Img&gt;&lt;ImageFeature&gt;&lt;/Img&gt;Describe this image in detail. Give as many details as possible. Say everything you see. ###Assistant:</span></span></code></pre></div>
<p>生成的结果里包含了噪声或者不连贯的内容，因此使用ChatGPT来修复这些问题。</p>
<p>给ChatGPT的prompt如下：</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>Fix the error <span class="kw">in</span> the given paragraph. Remove <span class="bu">any</span> repeating sentences, meaningless characters, <span class="kw">not</span> English sentences, <span class="kw">and</span> so on. Remove unnecessary repetition. Rewrite <span class="bu">any</span> incomplete sentences.</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>Return directly the results without explanation. Return directly the <span class="bu">input</span> paragraph <span class="cf">if</span> it <span class="kw">is</span> already correct without explanation.</span></code></pre></div>
<p>获得结果之后，再利用一些人工制定的规则进行筛选，比如包含“I’m sorry I
made a mistake…”, 或者 “I apologize for that …”的回复。</p>
<p>最后得到了3500条高质量的数据，用于第二阶段的训练。</p>
<p>第二阶段训练使用的模板如下：</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">###Human: &lt;Img&gt;&lt;ImageFeature&gt;&lt;/Img&gt;&lt;Instruction&gt;###Assistant:</span></span></code></pre></div>
<p>其中Instruction是从预定义的指令集中随机采样的指令。</p>
<p>一个细节是，这个prompt的损失不算入训练中。</p>
<p>第二阶段由于数据非常少，使用单个A100只要7分钟就训练完了。</p>
<p>下面是最终模型的生成样本：</p>
<img src="/f16505b3/minigpt4_example.png" class title="多模态入门">
<h1 id="qwen-vl">Qwen-VL</h1>
<p>论文：《Qwen-VL: A Versatile Vision-Language Model for Understanding,
Localization, Text Reading, and Beyond》</p>
<p>时间：2023年8月</p>
<p>机构：阿里</p>
<p>Qwen-VL有几个特点：</p>
<ul>
<li>支持随意交错的图文输入<br>
</li>
<li>支持多语言，多种图文任务<br>
</li>
<li>支持图片细粒度的理解</li>
</ul>
<img src="/f16505b3/qwenvl_intro1.png" class title="多模态入门">
<img src="/f16505b3/qwenvl_intro2.png" class title="多模态入门">
<h2 id="模型结构-1">模型结构</h2>
<p>Qwen-VL的模型包含3个部分：</p>
<ul>
<li>LLM：用Qwen-7B模型初始化的<br>
</li>
<li>Vision Encoder：使用来自Openclip的ViT-bigG（patch size =
14）初始化；训练和推理的时候图像会resize为固定大小<br>
</li>
<li>Position-aware Vision-Language
Adapter：adapter一方面用于把图像representation压缩到固定的大小（和Flamingo中的类似），一方面用于对齐图文特征；adapter是一个随机初始化的单层cross-attention；</li>
</ul>
<p>3个部分的大小：</p>
<img src="/f16505b3/qwenvl_model.png" class title="多模态入门">
<p>adapter要使用多长的token压缩图像特征是个超参，文中实验了不同的设置（64/144/256/400），结果如下图：</p>
<img src="/f16505b3/qwenvl_feature_len.png" class title="多模态入门">
<h2 id="输入输出">输入输出</h2>
<p>为了区分图像和文本输入，会使用两个特殊的token，&lt;img&gt; 和
&lt;/img&gt;， 放在图像特征序列的开头结尾。</p>
<p>为了增强模型 fine-grained visual understanding 和
grounding的能力，训练数据中还会涉及region
description和detection等，这些任务需要模型准确理解并生成对应的描述。那么对于输入中的bounding
box（直接以文本的形式输入），会添加下面两个特殊符号：&lt;box&gt; 和
&lt;/box&gt;。</p>
<p>另外，还会把bounding box对应的文本段也用特殊token框起来</p>
<blockquote>
<p>Additionally, to appropriately associate bounding boxes with their
corresponding descriptive words or sentences, another set of special
tokens (&lt;ref&gt; and &lt;/ref&gt;) is introduced, marking the content
referred to by the bounding box.</p>
</blockquote>
<h2 id="训练-1">训练</h2>
<p>Qwen-VL的训练分为三个阶段：两个预训练阶段和指令微调：</p>
<img src="/f16505b3/qwenvl_train.png" class title="多模态入门">
<p>1、stage1：预训练</p>
<p>这一阶段使用从网上爬取的大规模图文对数据。原始数据包含5B条图文对，在经过清理后保留了其中的1.4B，其中77%是英文数据，23%是中文数据。具体的来源和比例如下：</p>
<img src="/f16505b3/qwenvl_pt_data.png" class title="多模态入门">
<ul>
<li>这一阶段的训练冻结了LLM的参数，训练vision encoder和VL adapter<br>
</li>
<li>输入图像都被resize到224×224<br>
</li>
<li>训练的batch size = 30720，step = 50000</li>
</ul>
<p>2、stage2：多任务预训练</p>
<p>这一阶段引入了高质量和细粒度的 fine-grained VL annotation
data，包含交错的图文数据，使用的分辨率也更大（224×224 -&gt;
448×448）。所用的任务数据如下表：</p>
<img src="/f16505b3/qwenvl_mtpt_data.png" class title="多模态入门">
<p>这一阶段模型所有参数都参与训练。</p>
<p>3、stage3：SFT</p>
<p>这一阶段进行微调获得Qwen-VL-Chat。数据上，除了使用来自LLM
self-instruction的数据，还通过人工标注、模型生成获取一批额外的数据，以提升模型
localization 和 multi-image comprehension的能力。</p>
<p>训练的时候，除了多模态数据，还混合一些纯文本的对话数据，以确保模型对话能力的泛化性能。</p>
<p>最后总共使用了350k的数据。这一阶段vision
encoder的参数不参与训练，只训练LLM和VL adapter。</p>
<h1 id="internvl">InternVL</h1>
<p>论文：《InternVL: Scaling up Vision Foundation Models and Aligning
for Generic Visual-Linguistic Tasks》</p>
<p>时间：2023年12月</p>
<p>InternVL认为之前的VLM有几个问题：</p>
<ul>
<li>LLM和vision encoder在规模上有比较大的差异，vision
encoder比较小，可能对LLM的容量使用不足<br>
</li>
<li>用于对齐部分也比较小，且一般是随机初始化，因此对齐效果不够好</li>
</ul>
<p>针对这些问题，InternVL给出改进的版本：</p>
<ul>
<li>增大vision encoder<br>
</li>
<li>增大对齐层，并用预训练模型初始化<br>
</li>
<li>progressive image-text alignment，融合对比学习和生成学习</li>
</ul>
<img src="/f16505b3/internvl_model.png" class title="多模态入门">
<h2 id="模型">模型</h2>
<p>InternVL模型架构上的主要工作在Vision Encoder和用于对齐的Language
Middleware。</p>
<p>1、Vision Encoder</p>
<p>InternVL通过超参搜索的实验，设计出了InternViT-6B。这个模型在100M的LAION-en上训练。</p>
<img src="/f16505b3/internvl_vit.png" class title="多模态入门">
<p>2、Language Middleware</p>
<p>之前用于对齐图文特征的模型有Q-Former，或者简单的MLP层。这里使用基于LLaMA的模型，QLLaMA：在预训练好的LLaMA基础上，增加了96个随机初始化的query（用于压缩图像representation），以及一个1B参数的随机初始化的cross-attention层。</p>
<p>这样的中间件由于规模足够大，在对齐的时候效果更好，而且具有一定的zero-shot能力。</p>
<h2 id="训练-2">训练</h2>
<p>InternVL的对齐训练包括三个阶段：</p>
<img src="/f16505b3/internvl_train.png" class title="多模态入门">
<p>1、stage1：Vision-Language Contrastive Training</p>
<p>这一阶段把InternViT-6B和LLaMA-7B进行对齐，这一阶段所有参数都是可训练的。</p>
<p>所用的数据包括LAION-en、LAIONmulti、LAION-COCO、COYO、Wukong等，总共有6B个图文对数据，清洗后留下5B左右。</p>
<p>2、stage2：Vision-Language Generative Training</p>
<p>QLLaMA集成第一阶段训练得到的LLaMA-7B参数，保持InternViT-6B和QLLaMA的参数冻结，使用进一步过滤后剩下的1B高质量图文数据进行训练。</p>
<p>这一阶段的损失函数由三部分组成：</p>
<ul>
<li>image-text contrastive (ITC) loss<br>
</li>
<li>mage-text matching (ITM) loss<br>
</li>
<li>mage-grounded text generation (ITG) loss</li>
</ul>
<p>stage1和stage2所用数据如下：</p>
<img src="/f16505b3/internvl_train_data_12.png" class title="多模态入门">
<p>3、stage3：SFT</p>
<p>这一阶段把vision
encoder和QLLaMA和现成的LLM，Vicuna用MLP连接在一起。训练的时候使用了4M条数据：</p>
<img src="/f16505b3/internvl_train_data_3.png" class title="多模态入门">
<p>对于数据中不是对话的部分，参考LLaVA-1.5的方式进行转换。</p>
<p>训练的时候可以仅训练MLP层，由于LLM和QLLaMA的feature
space比较相似，因此这里即使冻结LLM，效果也很好。</p>
<h2 id="使用">使用</h2>
<p>通过灵活使用InternVL的不同组件，可以完成各种任务：</p>
<img src="/f16505b3/internvl_use.png" class title="多模态入门">
<ul>
<li>InternViT-6B可以作为视觉任务的backbone，进行图像分类或者dense
prediction task<br>
</li>
<li>对于contrastive task，有两种推理模式，InternVL-C和InternVL-G<br>
</li>
<li>对于生成任务，QLLaMA天然具有比较强的caption能力<br>
</li>
<li>而对于多模态对话，也有InternVL-Chat(w/o QLLaMA)和InternVL-Chat(w/
QLLaMA)两种模式</li>
</ul>
<h1 id="deepseek-vl">DeepSeek-VL</h1>
<p>论文：《DeepSeek-VL: Towards Real-World Vision-Language
Understanding》</p>
<p>时间：2024年3月</p>
<p>DeepSeek-VL认为这之前的VLM普遍有几个问题：</p>
<ul>
<li>将比较多的资源放在SFT阶段，不够重视通用知识学习的预训练阶段<br>
</li>
<li>微调的时候用了精挑细选的数据集，在评测上效果不错，但是和实际体验不符<br>
</li>
<li>使用预训练的ViT模型，分辨率不足<br>
</li>
<li>在图文训练后，模型的语言能力下降</li>
</ul>
<h2 id="数据">数据</h2>
<p>数据依然是重中之重，DeepSeek-VL做了比较精细的工作。预训练和微调阶段的数据详细分布如下：</p>
<img src="/f16505b3/dsvl_ptdata.png" class title="多模态入门">
<img src="/f16505b3/dsvl_sftdata.png" class title="多模态入门">
<p>首先可以看到有一个特点，那就是text-only的数据集比重相对比较高，就是为了保持VLM的语言能力而增加的文本数据。</p>
<p>另外就是任务和类型的多样性，有交错的图文数据，图表数据，OCR数据等。其中Web
Code是DeepSeek-VL专门构建的，为了让模型能从图片构建代码，利用Websight和MATCHA等工具获取了1.1M的数据。</p>
<p>SFT数据的三级分类体系如下：</p>
<img src="/f16505b3/dsvl_class.png" class title="多模态入门">
<p>SFT数据分布上，多样化的任务使得模型既能获得较好的评测效果，在使用时又能有较好的体验。</p>
<h2 id="模型-1">模型</h2>
<p>模型包括三个部分。</p>
<p>1、Hybrid Vision Encoder</p>
<p>vision
encoder使用SigLIP-L，但是仅使用SigLIP存在一些问题。SigLIP和CLIP这样的训练方式主要是为图像语义服务的，会把不同的图像encoder成相近的内容，原因是“CLIP-blind
pairs”（《Eyes wide shut? exploring the visual shortcomings of
multimodal
llms》）。再加上这些模型的输入分辨率都有限（大部分224×224、336×336等），这些vision
encoder就难以捕捉细粒度的low-level的特征，干不了比较精细的活。</p>
<p>因此有些工作开始使用 additional vision-only self-supervised
encoders，增强视觉编码的能力。受这个做法启发，DeepSeek-VL也使用额外的SAM-B模型来处理low-level的视觉信息，输入的分辨率为1024×1024。接收384×384输入的SigLIP模型，和接收1024×1024输入的SAM-B就构成了hybrid
vision encoder，能同时保留语义信息和细节特征。</p>
<p>2、Vision-Language Adaptor</p>
<p>使用two-layer hybrid MLP连接vision
encoder和LLM。高分辨率的特征和低分辨率的特征分别由单层的MLP处理，之后concat在一起。</p>
<p>3、Language Model</p>
<p>使用DeepSeek LLM。</p>
<h2 id="训练-3">训练</h2>
<p>DeepSeek-VL的训练分为三个阶段：</p>
<img src="/f16505b3/dsvl_train.png" class title="多模态入门">
<p>Stage 1: Training Vision-Language Adaptor</p>
<p>这一阶段vision
encoder和LLM保持冻结，训练MLP层。数据包括从ShareGPT4V获取的1.25M数据，和2.5M文档OCR数据。</p>
<p>一个问题是，在只训练MLP的情况下，是否存在scaling
law？即增大训练数据量有没有收益？这里做了实验，结果表明增大训练量没有明显收益，甚至效果会变差：</p>
<img src="/f16505b3/dsvl_ptdata_scale.png" class title="多模态入门">
<p>Stage 2: Joint Vision-Language pretraining</p>
<p>这一阶段只冻结vision encoder。</p>
<p>这里DeepSeek-VL验证了不同多模态和纯文本数据下模型效果的变化：</p>
<img src="/f16505b3/dsvl_ratio.png" class title="多模态入门">
<p>在只使用多模态数据的情况下，模型的语言性能严重下降，这是因为多模态数据中的语料比较简单，复杂性不足，并且多模态能力和语言能力之间也存在竞争关系，导致语言能力出现灾难性遗忘。</p>
<p>上面的实验还能看到，增加文本数据后，多模态能力不会出现明显的损失。结合一系列观察，选择了7（语言）:3（多模态）的数据比例。</p>
<p>这一阶段的训练成本比第一阶段高很多，不过好消息是在小模型（1.3B）上的实验结论很大部分都能迁移到更大的模型上（7B）。</p>
<p>Stage 3: Supervised Fine-tuning</p>
<p>这一阶段，由于内存有限，SAM-B保持了冻结。</p>
<p>各个阶段训练的参数设置如下：</p>
<img src="/f16505b3/dsvl_hp.png" class title="多模态入门">
<h1 id="internvl-1.5">InternVL 1.5</h1>
<p>论文：《How Far Are We to GPT-4V? Closing the Gap to Commercial
Multimodal Models with Open-Source Suites》</p>
<p>时间：2024年4月</p>
<p>InternVL 1.5一个特点是最大能够支持到4k分辨率的输入。</p>
<img src="/f16505b3/internvl1.5_intro.png" class title="多模态入门">
<h2 id="architecture">architecture</h2>
<p>InternVL
1.5整体的架构如下，和之前的很多工作一样，采用的是ViT-MLP-LLM配置，使用随机初始化的MLP层将预训练过的InternViT-6B和预训练过的InternLM2-20B连接在一起：</p>
<img src="/f16505b3/internvl1.5_archi.png" class title="多模态入门">
<p>首先，InternVL 1.5在InternVL基础上升级了InternViT-6B。</p>
<p>（1）InternViT-6B-448px-V1.2</p>
<p>首先，研究人员发现倒数第四层的feature在多模态任务上效果最好，因此直接丢弃掉最后三层，InternViT-6B从48层降为45层。然后把InternViT-6B的分辨率提升到448×448，并把它和Nous-Hermes-2-Yi-34B通过MLP连接，使用image
captioning和OCR数据进行训练，获得InternViT-6B-448px-V1.2。</p>
<p>（2）InternViT-6B-448px-V1.5</p>
<p>在InternViT-6B-448px-V1.2基础上，把分辨率从固定的448×448，扩展为动态的448×448。怎么扩展的呢？这里面有一个dynamic
resolution的策略，在训练的时候，会根据输入图像大小切分成1<sub>12个448×448的块，而在推理的时候则把块的范围扩大到1</sub>40个，这样使得模型最大可以处理4k分辨率的输入。</p>
<p>对不同图像的大小，会从一组（35个）预定义的长宽比中找到最佳匹配来切分，使得图像不会被过分变形，比如一个800×1300的图像将被调整为896×1344。然后，调整后的图像被分成448×448像素的块：</p>
<img src="/f16505b3/internvl1.5_reso.png" class title="多模态入门">
<p>不过这样一来输入token的数量可能就有点多，因此会使用一个叫pixel
shuffle的操作来降低token的数量。pixel
shuffle是图像超分里常用的操作，用于提高分辨率，不过这里改了factor，相当于down-sample了。</p>
<p>除了图像本身切分出来的块，还会增加一个缩略图放在最后面，以提供全局信息。</p>
<h2 id="数据-1">数据</h2>
<p>InternVL 1.5在预训练和微调阶段所用数据如下：</p>
<img src="/f16505b3/internvl1.5_data.png" class title="多模态入门">
<p>在微调阶段也加入了纯文本数据。</p>
<h2 id="训练-4">训练</h2>
<p>在预训练阶段，InternViT-6B和MLP进行优化，而LLM冻结；在微调阶段则所有参数参与训练。</p>
<h1 id="qwen2-vl">Qwen2-VL</h1>
<p>论文：《Qwen2-VL: Enhancing Vision-Language Model’s Perception of
theWorld at Any Resolution》</p>
<p>时间：2024年9月</p>
<p>机构：阿里</p>
<p>Qwen2-VL的内容还是比较多的，这里先简单看下关键点。Qwen2-VL有2B、8B、72B三种规模，支持动态分辨率，支持视频（20分钟以上）图文模态以及多语言的多种任务。</p>
<img src="/f16505b3/qwen2vl_ability.png" class title="多模态入门">
<h2 id="模型-2">模型</h2>
<p>三个规模的模型的描述如下：</p>
<img src="/f16505b3/qwen2vl_model.png" class title="多模态入门">
<p>可以看到各种规模的LLM下，都是用相同大小的vision
encoder，这样保证ViT的computation
load能够恒定。（估计是因为处理长视频的情况会对ViT的速度造成挑战）</p>
<p>Qwen2-VL具体的模型架构和第一代Qwen-VL保持一致：</p>
<img src="/f16505b3/qwen2vl_archi.png" class title="多模态入门">
<p>在这个大框架基础下，有几个关键改进点。</p>
<p>1、Naive Dynamic Resolution</p>
<p>采用Navit（《Patch n’pack: Navit, a vision transformer for any aspect
ratio and
resolution》）动态分辨率的做法，来支持各种分辨率的输入。为了支持动态分辨率，这里把ViT的绝对位置编码换成了苏神在https://www.spaces.ac.cn/archives/8397提出的2D-RoPE。</p>
<p>为了减少图像的token，在ViT后会用一个MLP把相邻的2×2的token压缩成单个token，并在前后加上两个特殊token：
&lt;|vision_start|&gt; 和 &lt;|vision_end|&gt; 。</p>
<p>2、Multimodal Rotary Position Embedding (M-RoPE)</p>
<p>M-RoPE把原始的rotaty
embedding拆解成三个分量：temporal、height和width。处理图像时，每个token的时间embedding保持不变；而对于视频每帧的temporal
id则会递增。</p>
<img src="/f16505b3/qwen2vl_mrope.png" class title="多模态入门">
<p>3、Unified Image and Video Understanding</p>
<p>Qwen2-VL对视频输入以每秒两帧的频率进行采样，并使用3D卷积来处理视频输入。为了在处理视频和图像时保持一致，图像在输入时会被视为两个一样的视频帧。</p>
<p>为了保持效率，会动态调整视频帧的分辨率，使得每个输入视频的token数限制在16384以内。</p>
<h2 id="训练-5">训练</h2>
<p>Qwen2-VL的训练和一代Qwen-VL保持一致：</p>
<ul>
<li>第一阶段训练ViT<br>
</li>
<li>第二阶段全部训练<br>
</li>
<li>第三阶段冻结ViT</li>
</ul>
<p>整个训练过程包含1.4T token，训练过程中只BP文本的loss。</p>
<p>数据上，会使用&lt;|vision_start|&gt;和&lt;|vision_end|&gt;标识视觉内容，帮助模型区分视觉信息和文本信息。</p>
<p>对话数据使用ChatML的格式：</p>
<img src="/f16505b3/qwen2vl_chatml.png" class title="多模态入门">
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】MiniGPT-4: Enhancing Vision-Language Understanding with Advanced
Large Language Models https://arxiv.org/abs/2304.10592<br>
【2】Qwen-VL: A Versatile Vision-Language Model for Understanding,
Localization, Text Reading, and Beyond
https://arxiv.org/abs/2308.12966<br>
【3】Qwen2-VL: Enhancing Vision-Language Model’s Perception of theWorld
at Any Resolution https://arxiv.org/abs/2409.12191<br>
【4】InternVL: Scaling up Vision Foundation Models and Aligning for
Generic Visual-Linguistic Tasks https://arxiv.org/abs/2312.14238<br>
【5】DeepSeek-VL: Towards Real-World Vision-Language Understanding
https://arxiv.org/abs/2403.05525<br>
【6】Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and
Resolution https://arxiv.org/abs/2307.06304<br>
【7】How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal
Models with Open-Source Suites https://arxiv.org/abs/2404.16821</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>transformer</tag>
        <tag>预训练</tag>
        <tag>多模态</tag>
        <tag>CV</tag>
        <tag>CNN</tag>
        <tag>无监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title>训练数据合成(二)</title>
    <url>/2a22baeb.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>继续看一些重要的数据合成工作。</p>
<h1 id="self-instruct">self-instruct</h1>
<p>self-instruct算是大模型数据合成的经典工作了，它的目的主要是为了低成本获得大量用于大模型微调的指令数据。self-instruct的整体流程如下：</p>
<img src="/2a22baeb/selfinstr_intro.png" class title="数据合成">
<p>文中首先定义一下什么“指令数据”。一个“指令”是一个用自然语言描述的任务，用I表示，一个任务下可以有多组input-output的实例。比如<br>
指令I = “写一篇关于以下主题的文章”<br>
输入X = “学校安全”<br>
输出Y就是对应话题的一篇文章。</p>
<p>不过在很多情况下，“指令”和“实例”并没有很明确的区分界限，“指令”本身就可以是一个实例。比如这样一个指令“写一篇关于学校安全的文章”就即是指令又是实例，它作为指令时输入X就为空了。</p>
<p>回到self-instruct指令数据合成的流程上来，共分四步。</p>
<p>1、Instruction Generation</p>
<p>一开始，先收集175个人类编写的指令数据来作为task
pool的初始化。每步生成时，从这个池子里随机采样8个instruction作为in-context
example，这8个example会混合人工数据和之前轮次的合成数据，prompt如下：</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>Come up <span class="cf">with</span> a series of tasks:</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>Task <span class="dv">1</span>: &#123;instruction <span class="cf">for</span> existing task <span class="dv">1</span>&#125;</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>Task <span class="dv">2</span>: &#123;instruction <span class="cf">for</span> existing task <span class="dv">2</span>&#125;</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>Task <span class="dv">3</span>: &#123;instruction <span class="cf">for</span> existing task <span class="dv">3</span>&#125;</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>Task <span class="dv">4</span>: &#123;instruction <span class="cf">for</span> existing task <span class="dv">4</span>&#125;</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>Task <span class="dv">5</span>: &#123;instruction <span class="cf">for</span> existing task <span class="dv">5</span>&#125;</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>Task <span class="dv">6</span>: &#123;instruction <span class="cf">for</span> existing task <span class="dv">6</span>&#125;</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>Task <span class="dv">7</span>: &#123;instruction <span class="cf">for</span> existing task <span class="dv">7</span>&#125;</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>Task <span class="dv">8</span>: &#123;instruction <span class="cf">for</span> existing task <span class="dv">8</span>&#125;</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>Task <span class="dv">9</span>:</span></code></pre></div>
<p>这个开放式的prompt允许模型一次生成多个结果，实验设置上以达到最大长度，或者生成到“Task
16”的时候为止。</p>
<p>2、Classification Task Identification</p>
<p>这里把任务分成了两类，分类任务和非分类任务。这里需要先判断任务属于哪一类。不过在目前阶段，似乎不太需要再区分这样的任务了，而且分类任务在大模型应用中应该也越来越少了，而复杂任务则是更多了。</p>
<p>这里用few-shot prompt的方式让大模型做判断：</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>Can the following task be regarded <span class="im">as</span> a classification task <span class="cf">with</span> finite output labels?</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>Task: Given my personality <span class="kw">and</span> the job, tell me <span class="cf">if</span> I would be suitable.</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>Is it classification? Yes</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>Task: Give me an example of a time when you had to use your sense of humor.</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>Is it classification? No</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>Task: Replace the placeholders <span class="kw">in</span> the given text <span class="cf">with</span> appropriate named entities.</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>Is it classification? No</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>Task: Fact checking <span class="op">-</span> tell me <span class="cf">if</span> the statement <span class="kw">is</span> true, false, <span class="kw">or</span> unknown, based on your</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>knowledge <span class="kw">and</span> common sense.</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>Is it classification? Yes</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>Task: Return the SSN number <span class="cf">for</span> the person.</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>Is it classification? No</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>Task: &#123;instruction <span class="cf">for</span> the target task&#125;</span></code></pre></div>
<p>3、Instance Generation</p>
<p>给定指令和任务类型（分类任务还是非分类任务），让模型生成具体的instance，包括输入和输出。这里有几点经验：<br>
- 所使用的few-shot
example不要和要生成的指令属于同一类任务效果更好，这里这样做可以增加生成的多样性<br>
-
生成的时候，一般来说是让模型先生成输入再生成输出，但是发现总是使用这个顺序的话，模型会倾向于在分类任务中总是生成某一个标签的实例，所以也可以调整顺序，让模型先生成输出再生成输入，以打破这种有偏的行为。</p>
<p>4、Filtering and Postprocessing</p>
<p>生成的数据难免会和数据库里的存在相似的情况，为了增加数据多样性，只保留和所有已有数据的ROUGE-L相似度低于0.7的数据，并添加到task
pool中。也可以用其他相似度计算方式。</p>
<p>另外有时模型生成的任务可能要求一些LLM无法完成的事情，比如和多媒体相关的任务，这些指令就需要通过关键词过滤掉了。</p>
<h1 id="evol-instruct">evol-instruct</h1>
<p>evol-instruct是提升现有指令数据多样性和难度的方法，重点在数据质量而不在数据数量。evol-instruct的整体流程如下：</p>
<img src="/2a22baeb/evolinstr_intro.png" class title="数据合成">
<p>从最开始的原始数据集D_0开始，每轮指令进化会把D_t（第t轮的数据）中所有指令数据都进化，得到新的数据集D_t+1，那么在进化M次之后，就会得到{D_0,
D_1, D_2,...,D_M}这个多个数据集。<br>
evol-instruct主要包含三个步骤：(1) instruction evolving, (2) response
generation, 以及 (3) elimination evolving。</p>
<p>1、Instruction Evolution</p>
<p>每一轮进化中，会使用LLM在现有指令的基础上提高难度，或者增加多样性。指令进化成功就会把新指令加入到新数据池中，如果失败就会把原指令放回，在下一轮的时候重新处理。</p>
<p>Instruction Evolution分成两种类型，in-depth evolving和in-breadth
evolving。</p>
<p>（1）in-depth evolving 深度进化</p>
<p>深度进化的目的是提升指令的难度，共有5种类型的prompt，代表着5个不同的具体方向：add
constraints, deepening, concretizing, increased reasoning steps, 以及
complicating input。</p>
<p>深度进化要求每次进化的难度“困难一点点”，并限制最多增加10~20个单词，不能一下子变得太困难，导致出现大部分人类无法理解的内容。</p>
<p>各种prompt具体如下：</p>
<p>a.add constraints</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>I want you act <span class="im">as</span> a Prompt Rewriter.</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>Your objective <span class="kw">is</span> to rewrite a given prompt into a more <span class="bu">complex</span> version to make those famous AI systems (e.g., ChatGPT <span class="kw">and</span> GPT4) a bit harder to handle.</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>But the rewritten prompt must be reasonable <span class="kw">and</span> must be understood <span class="kw">and</span> responded by humans.</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>Your rewriting cannot omit the non<span class="op">-</span>text parts such <span class="im">as</span> the table <span class="kw">and</span> code <span class="kw">in</span> <span class="co">#Given Prompt#:. Also, please do not omit the input in #Given Prompt#.</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>You SHOULD complicate the given prompt using the following method:</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>Please add one more constraints<span class="op">/</span>requirements into <span class="co">#Given Prompt#</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>You should <span class="cf">try</span> your best <span class="kw">not</span> to make the <span class="co">#Rewritten Prompt# become verbose, #Rewritten Prompt# can only add 10 to 20 words into #Given Prompt#.</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>‘<span class="co">#Given Prompt#’, ‘#Rewritten Prompt#’, ‘given prompt’ and ‘rewritten prompt’ are not allowed to appear in #Rewritten Prompt#</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">#Given Prompt#:</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>Here <span class="kw">is</span> instruction.<span class="op">&gt;</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">#Rewritten Prompt#:</span></span></code></pre></div>
<p>b.deepening</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>I want you act <span class="im">as</span> a Prompt Rewriter.</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>Your objective <span class="kw">is</span> to rewrite a given prompt into a more <span class="bu">complex</span> version to make those famous AI systems (e.g., ChatGPT <span class="kw">and</span> GPT4) a bit harder to handle.</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>But the rewritten prompt must be reasonable <span class="kw">and</span> must be understood <span class="kw">and</span> responded by humans.</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>Your rewriting cannot omit the non<span class="op">-</span>text parts such <span class="im">as</span> the table <span class="kw">and</span> code <span class="kw">in</span> <span class="co">#Given Prompt#:. Also, please do not omit the input in #Given Prompt#.</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>You SHOULD complicate the given prompt using the following method:</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>If <span class="co">#Given Prompt# contains inquiries about certain issues, the depth and breadth of the inquiry can be increased. or You should try your best not to make the #Rewritten Prompt# become verbose, #Rewritten Prompt# can only</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>add <span class="dv">10</span> to <span class="dv">20</span> words into <span class="co">#Given Prompt#.</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>‘<span class="co">#Given Prompt#’, ‘#Rewritten Prompt#’, ‘given prompt’ and ‘rewritten prompt’ are not allowed to appear in #Rewritten Prompt#</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">#Given Prompt#:</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>Here <span class="kw">is</span> instruction.<span class="op">&gt;</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">#Rewritten Prompt#:</span></span></code></pre></div>
<p>c.concretizing</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>I want you act <span class="im">as</span> a Prompt Rewriter.</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>Your objective <span class="kw">is</span> to rewrite a given prompt into a more <span class="bu">complex</span> version to make those famous AI systems (e.g., ChatGPT <span class="kw">and</span> GPT4) a bit harder to handle.</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>But the rewritten prompt must be reasonable <span class="kw">and</span> must be understood <span class="kw">and</span> responded by humans.</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>Your rewriting cannot omit the non<span class="op">-</span>text parts such <span class="im">as</span> the table <span class="kw">and</span> code <span class="kw">in</span> <span class="co">#Given Prompt#:. Also, please do not omit the input in #Given Prompt#.</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>You SHOULD complicate the given prompt using the following method:</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>Please replace general concepts <span class="cf">with</span> more specific concepts. <span class="kw">or</span> You should <span class="cf">try</span> your best <span class="kw">not</span> to make the <span class="co">#Rewritten Prompt# become verbose, #Rewritten Prompt# can only add 10 to 20 words into #Given Prompt#.</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>‘<span class="co">#Given Prompt#’, ‘#Rewritten Prompt#’, ‘given prompt’ and ‘rewritten prompt’ are not allowed to appear in #Rewritten Prompt#</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">#Given Prompt#:</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>Here <span class="kw">is</span> instruction.<span class="op">&gt;</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co">#Rewritten Prompt#:</span></span></code></pre></div>
<p>d.increased reasoning steps</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>I want you act <span class="im">as</span> a Prompt Rewriter.</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>Your objective <span class="kw">is</span> to rewrite a given prompt into a more <span class="bu">complex</span> version to make those famous AI systems (e.g., ChatGPT <span class="kw">and</span> GPT4) a bit harder to handle.</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>But the rewritten prompt must be reasonable <span class="kw">and</span> must be understood <span class="kw">and</span> responded by humans.</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>Your rewriting cannot omit the non<span class="op">-</span>text parts such <span class="im">as</span> the table <span class="kw">and</span> code <span class="kw">in</span> <span class="co">#Given Prompt#:. Also, please do not omit the input in #Given Prompt#.</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>You SHOULD complicate the given prompt using the following method:</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>If <span class="co">#Given Prompt# can be solved with just a few simple thinking processes, you can rewrite it to explicitly request multiple-step reasoning.</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>You should <span class="cf">try</span> your best <span class="kw">not</span> to make the <span class="co">#Rewritten Prompt# become verbose, #Rewritten Prompt# can only add 10 to 20 words into #Given Prompt#.</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>‘<span class="co">#Given Prompt#’, ‘#Rewritten Prompt#’, ‘given prompt’ and ‘rewritten prompt’ are not allowed to appear in #Rewritten Prompt#</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co">#Given Prompt#:</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>Here <span class="kw">is</span> instruction.<span class="op">&gt;</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co">#Rewritten Prompt#:</span></span></code></pre></div>
<p>e.complicating input</p>
<p>和前面几种prompt有所不同，complicating input需要使用few-shot
prompt。</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>I want you act <span class="im">as</span> a Prompt Rewriter.</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>Your objective <span class="kw">is</span> to rewrite a given prompt into a more <span class="bu">complex</span> version to make those famous AI systems (e.g., ChatGPT <span class="kw">and</span> GPT4) a bit harder to handle.</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>But the rewritten prompt must be reasonable <span class="kw">and</span> must be understood <span class="kw">and</span> responded by humans.</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>You must add [XML data] <span class="bu">format</span> data <span class="im">as</span> <span class="bu">input</span> data <span class="kw">in</span> [Rewritten Prompt]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">#Given Prompt#:</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>Here <span class="kw">is</span> Demonstration instruction <span class="fl">1.</span><span class="op">&gt;</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co">#Rewritten Prompt#:</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>Here <span class="kw">is</span> Demonstration Example <span class="fl">1.</span><span class="op">&gt;</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>... N <span class="op">-</span><span class="dv">1</span> Examples ...</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>I want you act <span class="im">as</span> a Prompt Rewriter.</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>Your objective <span class="kw">is</span> to rewrite a given prompt into a more <span class="bu">complex</span> version to make those famous AI systems (e.g., ChatGPT <span class="kw">and</span> GPT4) a bit harder to handle.</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>But the rewritten prompt must be reasonable <span class="kw">and</span> must be understood <span class="kw">and</span> responded by humans.</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>You must add [<span class="co">#Given Dataformat#] format data as input data, add [#Given Dataformat#] code as input code in [Rewritten Prompt]</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>Rewrite prompt must be a question style instruction</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co">#Given Prompt#:</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>Here <span class="kw">is</span> instruction.<span class="op">&gt;</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co">#Rewrite prompt must be a question style instruction Rewritten Prompt(MUST contain a specific JSON data as input#:</span></span></code></pre></div>
<p>（2）in-breadth evolving 广度进化</p>
<p>广度进化目的是增加topic的覆盖，提升数据集的多样性。prompt如下：</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>I want you act <span class="im">as</span> a Prompt Creator.</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>Your goal <span class="kw">is</span> to draw inspiration <span class="im">from</span> the <span class="co">#Given Prompt# to create a brand new prompt.</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>This new prompt should belong to the same domain <span class="im">as</span> the <span class="co">#Given Prompt# but be even more rare.</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>The LENGTH <span class="kw">and</span> difficulty level of the <span class="co">#Created Prompt# should be similar to that of the #Given Prompt#.</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>The <span class="co">#Created Prompt# must be reasonable and must be understood and responded by humans.</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>‘<span class="co">#Given Prompt#’, ‘#Created Prompt#’, ‘given prompt’ and ‘created prompt’ are not allowed to appear in #Created Prompt#.</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">#Given Prompt#:</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>Here <span class="kw">is</span> instruction.<span class="op">&gt;</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co">#Created Prompt#:</span></span></code></pre></div>
<p>下面是一个各种方向进化的例子：</p>
<img src="/2a22baeb/evolinstr_example.png" class title="数据合成">
<p>2.Response Generation</p>
<p>response的生成就比较常规了，把前面进化出来的prompt输入给LLM即可。</p>
<p>3.Elimination Evolving</p>
<p>有了进化好的prompt和response还不算完。有时候进化会出现失败的情况。进化失败的case会放弃进化的结果，而把原数据用于下次进化。</p>
<p>什么样的情况算是进化失败呢？主要有下面四种。</p>
<p>（1）进化后的指令相比原指令没有任何information
gain，这个可以通过prompt让ChatGPT进行判断：</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>Here are two Instructions to ChatGPT AI, do you think they are equal to each other, which meet the following requirements:</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fl">1.</span> They have same constraints <span class="kw">and</span> requirments.</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="fl">2.</span> They have same depth <span class="kw">and</span> breadth of the inquiry.</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>The First Prompt: <span class="op">&lt;</span>Here <span class="kw">is</span> first instruction.<span class="op">&gt;</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>The Second Prompt: <span class="op">&lt;</span>Here <span class="kw">is</span> second instruction.<span class="op">&gt;</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>Your Judgement (Just answer: Equal <span class="kw">or</span> Not Equal. No need to explain the reason.):</span></code></pre></div>
<p>（2）进化后的指令让LLM给不出response，比如生成的包含“抱歉”，且长度较短。</p>
<p>（3）response的生成异常的，比如只有标点符号，内容为空等。</p>
<p>（4）进化后的指令质量不好，复制了进化prompt中的一些内容，比如“#Rewritten
Prompt#”等。</p>
<h1 id="mammoth2">MAmmoTH2</h1>
<p>《MAmmoTH2: Scaling Instructions from the
Web》里介绍了从web数据构建instruction数据的做法，获得了用于微调LLM的WebInstruct数据集。不过与其说是数据合成，这里的做法更多的是和数据抽取相关。</p>
<p>构建WebInstruct主要包括三步：</p>
<img src="/2a22baeb/mammoth2_intro.png" class title="数据合成">
<p>1、文档抽取</p>
<p>使用涵盖想要的领域的种子数据（比如数学），以及从web数据中随机抽取的100k负样本，训练一个fasttext分类器。这个分类器用于从web数据中recall想要的领域数据。召回的文档会根据url（domain）做分组，一个domain如果最终文档量少于1000个，则不会被保留。</p>
<p>最终从common crawl里recall了大约100B的数据。</p>
<p>2、Q-A Pair Extraction</p>
<p>web数据里有很多QA数据，但是会和其他内容混杂在一起，比如广告、网页模板等，因此先用规则进行简单清洗之后，再用Qwen-72B从这些文档里提取相对干净的QA内容。</p>
<p>3、Q-A Pair Refinement</p>
<p>第二步里抽取的QA质量不一定好（其实很多是不好的），因此需要对这些QA进行改写。比如如果Answer里没有包含完整的solution，就用LLM对这些内容进行补全。</p>
<h1 id="genie">GENIE</h1>
<p>GENIE = Generate information &amp;
elucidate，主要是合成content-grounded generation
task相关的数据，比如长文本问答LFQA、摘要和信息提取等任务。</p>
<p>GENIE合成数据的流程其实相当简单，就是（1）准备content数据（2）生成任务相关内容（3）检查过滤，这里主要看一下细节。</p>
<p>1、准备content数据</p>
<p>这一步是从网页抽取文档的过程，会保留文档结构，比如列表、表格、章节信息等，文档用markdown格式保存。实验中使用的是NQ
passages数据集。</p>
<p>2、生成任务相关内容</p>
<p>根据文档，使用few-shot
prompt获得问答QA、摘要等任务数据。prompt如下：</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>Instruction: Given the <span class="bu">next</span> [document], create a [question] <span class="kw">and</span> [answer] pair that are grounded <span class="kw">in</span> the main point of the document, don<span class="st">&#39;t add any additional information that is not in the document. The [question] is by an information-seeking user and the [answer] is provided by a helping AI Agent.</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="er">[document]: Scrumptious Sweet Co. factory ..</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="er">### Response:</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>[question]: Where was the movie Chitty Chitty Bang Bang filmed?</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>[answer]: Chitty Chitty Bang Bang was filmed <span class="kw">in</span> a variety of locations <span class="kw">in</span> England <span class="kw">and</span> Germany. Some ..</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>[document]: The series stars Eugene Levy <span class="im">as</span> Johnny Rose …</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">### Response:</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>[question]: What <span class="kw">is</span> the plot of the show Schitt<span class="st">&#39;s Creek?</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="er">[answer]: The show Schitt&#39;s Creek is about a wealthy family who loses their fortune and is forced to rebuild their lives in a small town. The show follows the family as they adjust to their new life in the town and learn to appreciate the simple things in life.</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="er">[document]: 2016&#39;s countdown broke several Hottest 100 records…</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="er">### Response:</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>[question]: What was the most popular song on the <span class="dv">2016</span> Hottest <span class="dv">100</span>?</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>[answer]: The most popular song on the <span class="dv">2016</span> Hottest <span class="dv">100</span> was <span class="st">&quot;Never Be Like You&quot;</span> by Flume. This was the first time that an electronic dance music producer topped the countdown.</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>[document]: In Greek mythology, Persephone ..</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="co">### Response:</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>[question]: Who <span class="kw">is</span> Persephone <span class="kw">in</span> Greek mythology <span class="kw">and</span> what <span class="kw">is</span> her significance?</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>[answer]: In Greek mythology, Persephone, also known <span class="im">as</span> Kore, <span class="kw">is</span> the daughter of Zeus <span class="kw">and</span> Demeter. She ..</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>[document]: Terry, the dog that played Toto <span class="kw">in</span> the <span class="dv">1939</span> screen …</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="co">### Response:</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>[question]: What breed of dog was Toto <span class="kw">in</span> the Wizard of Oz?</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>[answer]: Terry, the dog that played Toto <span class="kw">in</span> the <span class="dv">1939</span> screen adaptation of The Wizard of Oz, was a brindle Cairn terrier.</span></code></pre></div>
<p>3、检查过滤</p>
<p>（1）格式</p>
<p>对于生成的结果太短，或者太长（摘要不能比原文还长吧）的结果进行过滤。</p>
<p>（2）Faithfulness</p>
<p>使用NLI模型，把原文档和问答、摘要内容进行评测，确定二者是否有逻辑关系。</p>
<p>（3）Quality</p>
<p>使用reward模型对数据进行打分，所用的模型是reward-model-deberta-v3-large-v2。</p>
<h1 id="orca-math">Orca-Math</h1>
<p>Orca-Math里合成了包含20w个数学问题的Orca-Math-dataset，把SLM的GSM8k
pass@1做到了81.50%。</p>
<p>Orca-Math合成数据的目的是获得一批多样化的小学数学应用题，包括简单和困难的问题。为了合成数据，首先参照《Lila:
A unified benchmark for mathematical
reasoning》的基准从各种开源的数据集收集了36,217个问题。</p>
<p>1、Ask Me Anything agent</p>
<p>这个种子问题集通过下面的prompt扩展更多的问题，论文中把使用这个prompt专门用于生成问题的模型称为“Ask
Me Anything” agent：</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>Your goal <span class="kw">is</span> to create multiple word problems <span class="im">from</span> a given word problem <span class="kw">and</span> its answer. First convert the question of the word problem into a statement.</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>Then <span class="cf">for</span> each number <span class="kw">in</span> the converted problem create a new word problem.</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>Here are some examples:</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>Example <span class="dv">1</span>: Q: Natalia sold clips to <span class="dv">48</span> of her friends <span class="kw">in</span> April, <span class="kw">and</span> then she sold half <span class="im">as</span> many clips <span class="kw">in</span> May. How many clips did Natalia sell altogether <span class="kw">in</span> April <span class="kw">and</span> May?</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>Answer: <span class="dv">72</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>Replacing question <span class="cf">with</span> statement: Natalia sold clips to <span class="dv">48</span> of her friends <span class="kw">in</span> April, <span class="kw">and</span> then she sold half <span class="im">as</span> many clips <span class="kw">in</span> May. Natalia sold altogether <span class="dv">72</span> clips <span class="kw">in</span> April <span class="kw">and</span> May.</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>All questions:</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>target<span class="op">&gt;</span> <span class="dv">48</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>question<span class="op">&gt;</span> Natalia sold clips to some of her friends <span class="kw">in</span> April, <span class="kw">and</span> then she sold half <span class="im">as</span> many clips <span class="kw">in</span> May. Natalia sold altogether <span class="dv">72</span> clips <span class="kw">in</span> April <span class="kw">and</span> May. How many clips did she sell <span class="kw">in</span> April?</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>target<span class="op">&gt;</span> half</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>question<span class="op">&gt;</span> Natalia sold clips to <span class="dv">48</span> of her friends <span class="kw">in</span> April, <span class="kw">and</span> then she sold some clips <span class="kw">in</span> May. Natalia sold altogether <span class="dv">72</span> clips <span class="kw">in</span> April <span class="kw">and</span> May. What <span class="kw">is</span> the ratio of the number clips sold <span class="kw">in</span> April to number clips sold <span class="kw">in</span> May?</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>target<span class="op">&gt;</span> <span class="dv">72</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>question<span class="op">&gt;</span> Natalia sold clips to <span class="dv">48</span> of her friends <span class="kw">in</span> April, <span class="kw">and</span> then she sold half <span class="im">as</span> many clips <span class="kw">in</span> May. How many clips did Natalia sell altogether <span class="kw">in</span> April <span class="kw">and</span> May?</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>Example <span class="dv">2</span>: Q: Weng earns $<span class="dv">12</span> an hour <span class="cf">for</span> babysitting. Yesterday, she just did <span class="dv">50</span> minutes of babysitting. How much did she earn?</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>Answer: <span class="dv">10</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>Replacing question <span class="cf">with</span> statement: Weng earns $<span class="dv">12</span> an hour <span class="cf">for</span> babysitting. Yesterday, she just did <span class="dv">50</span> minutes of babysitting. She earned $<span class="fl">10.</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>All questions:</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>target<span class="op">&gt;</span> <span class="dv">12</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>question<span class="op">&gt;</span> Weng earns a certain amount per hour <span class="cf">for</span> babysitting. Yesterday, she just did <span class="dv">50</span> minutes of babysitting <span class="kw">and</span> earned <span class="fl">10.</span> How much does she earn per hour?</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>target<span class="op">&gt;</span> <span class="dv">50</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>question<span class="op">&gt;</span> Weng earns <span class="dv">12</span> an hour <span class="cf">for</span> babysitting. Yesterday, she just did some babysitting <span class="kw">and</span> earned <span class="fl">10.</span> How much time did she spend on babysitting?</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>target<span class="op">&gt;</span> <span class="dv">10</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>question<span class="op">&gt;</span> Weng earns <span class="dv">12</span> an hour <span class="cf">for</span> babysitting. Yesterday, she just did <span class="dv">50</span> minutes of babysitting. How much did she earn?</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>Example <span class="dv">3</span>: Q: Betty <span class="kw">is</span> saving money <span class="cf">for</span> a new wallet which costs <span class="fl">100.</span> Betty has only half of the money she needs. Her parents decided to give her <span class="dv">15</span> <span class="cf">for</span> that purpose, <span class="kw">and</span> her grandparents twice <span class="im">as</span> much <span class="im">as</span> her parents. How much more money does Betty need to buy the wallet?</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>Answer: <span class="dv">5</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>Replacing question <span class="cf">with</span> statement: Betty <span class="kw">is</span> saving money <span class="cf">for</span> a new wallet which costs <span class="fl">100.</span> Betty has only half of the money she needs. Her parents decided to give her <span class="dv">15</span> <span class="cf">for</span> that purpose, <span class="kw">and</span> her grandparents gave her twice <span class="im">as</span> much <span class="im">as</span> her parents. She needs <span class="dv">5</span> more to buy the wallet.</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>All questions:</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>target<span class="op">&gt;</span> <span class="dv">100</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>question<span class="op">&gt;</span> Betty <span class="kw">is</span> saving money <span class="cf">for</span> a new wallet. Betty has only half of the money she needs. Her parents decided to give her <span class="dv">15</span> <span class="cf">for</span> that purpose, <span class="kw">and</span> her grandparents twice <span class="im">as</span> much <span class="im">as</span> her parents. She needs <span class="dv">5</span> more to buy the wallet. What <span class="kw">is</span> the cost of the wallet?</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>target<span class="op">&gt;</span> half</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>question<span class="op">&gt;</span> Betty <span class="kw">is</span> saving money <span class="cf">for</span> a new wallet which costs <span class="fl">100.</span> She has some money saved, her parents decided to give her <span class="dv">15</span>, <span class="kw">and</span> her grandparents gave her twice <span class="im">as</span> much <span class="im">as</span> her parents. Now, Betty needs <span class="dv">5</span> more to buy the wallet. What <span class="kw">is</span> the ratio of the money Betty have saved initially to the cost of wallet?</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>target<span class="op">&gt;</span> <span class="dv">15</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>question<span class="op">&gt;</span> Betty <span class="kw">is</span> saving money <span class="cf">for</span> a new wallet which costs <span class="fl">100.</span> She has half of the money she needs, her parents decided to give her some money, <span class="kw">and</span> her grandparents gave her twice <span class="im">as</span> much <span class="im">as</span> her parents. Now, Betty needs <span class="dv">5</span> more to buy the wallet. How much money did her parents give her?</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>target<span class="op">&gt;</span> twice</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>question<span class="op">&gt;</span> Betty <span class="kw">is</span> saving money <span class="cf">for</span> a new wallet which costs <span class="fl">100.</span> Betty has only half of the money she needs. Her parents decided to give her <span class="dv">15</span> <span class="cf">for</span> that purpose, <span class="kw">and</span> her grandparents also chipped <span class="kw">in</span>. Now, Betty needs <span class="dv">5</span> more to buy the wallet. What <span class="kw">is</span> the ratio of the amount given by her grandparents to the amount given by her parents?</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>target<span class="op">&gt;</span> <span class="dv">5</span></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>question<span class="op">&gt;</span> Betty <span class="kw">is</span> saving money <span class="cf">for</span> a new wallet which costs <span class="fl">100.</span> Betty has only half of the money she needs. Her parents decided to give her <span class="dv">15</span> <span class="cf">for</span> that purpose, <span class="kw">and</span> her grandparents twice <span class="im">as</span> much <span class="im">as</span> her parents. How much more money does Betty need to buy the wallet?</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>Now solve this:</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>Example <span class="dv">4</span>: Q: Your teacher <span class="kw">is</span> giving a test worth <span class="dv">200</span> points. There <span class="kw">is</span> a total of <span class="dv">30</span> <span class="dv">5</span><span class="op">-</span>point <span class="kw">and</span> <span class="dv">10</span><span class="op">-</span>point questions. How many <span class="dv">5</span><span class="op">-</span>point questions are on the test? Answer: <span class="dv">20</span></span></code></pre></div>
<p>“Ask Me Anything” agent最终从种子数据生成了120,445个问题。</p>
<p>2、Suggester &amp; Editor</p>
<p>为了增加问题的难度，引入了Suggester &amp; Editor。</p>
<p>Suggester提出一些抽象、high
level的建议，而Editor就根据现有的问题和Suggester的建议来修改问题。一个示例如下：</p>
<img src="/2a22baeb/orca_path_example.png" class title="数据合成">
<h1 id="小结">小结</h1>
<ul>
<li>其实很多数据合成的工作在prompt的设计上，需要根据所用的模型多次迭代<br>
</li>
<li>多样性是数据合成不变的核心，无论是使用多样的种子、检索的few-shot
prompt还是多次采样都是为了得到更加拟合真实世界的多样化的数据<br>
</li>
<li>后续有一些更加自动化的做法，下一篇来讲</li>
</ul>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门--CLIP</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】SELF-INSTRUCT: Aligning Language Models with Self-Generated
Instructions https://arxiv.org/abs/2212.10560<br>
【2】Alpaca: A Strong, Replicable Instruction-Following Model
https://crfm.stanford.edu/2023/03/13/alpaca.html<br>
【3】WizardLM: Empowering Large Language Models to Follow Complex
Instructions https://arxiv.org/abs/2304.12244<br>
【4】MAmmoTH2: Scaling Instructions from the Web
https://arxiv.org/abs/2405.03548<br>
【5】Genie: Achieving Human Parity in Content-Grounded Datasets
Generation https://arxiv.org/abs/2401.14367<br>
【6】Orca-Math: Unlocking the potential of SLMs in Grade School Math
https://arxiv.org/abs/2402.14830</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>预训练</tag>
        <tag>数据合成</tag>
      </tags>
  </entry>
  <entry>
    <title>训练数据合成(三)</title>
    <url>/e259c7b2.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>书接上回，<a href="https://www.linsight.cn/85132189.html">训练数据合成(二)</a>，继续看一些数据合成相关工作，这次很多都是和代码相关的。</p>
<h1 id="codellms的数据合成综述">CodeLLMs的数据合成综述</h1>
<p>论文：《Mastering the Craft of Data Synthesis for CodeLLMs》</p>
<p>时间：2024年10月</p>
<p>这是一篇关于代码模型中所使用的数据合成方法的综述。根据不同的分类维度，各个代码数据合成相关工作被分类成以下这样：</p>
<img src="/e259c7b2/survey_classes.png" class title="数据合成">
<h2 id="preliminaries">Preliminaries</h2>
<p>先科普一下CodeLLMs通常的data curation
pipeline。一般有这么四个step：<br>
- Seed Input Collection：收集原始启动数据，用于指导、启发数据合成<br>
- Data Synthesis：合成新数据的过程<br>
- Data
Filtering：过滤掉低质量、不符合要求的合成数据，避免引入幻觉等问题<br>
- Data Evaluation：验证得到的数据的效果，通常需要训练模型验证</p>
<img src="/e259c7b2/survey_pipeline.png" class title="数据合成">
<h2 id="data-synthesis-techniques">Data Synthesis Techniques</h2>
<p>如图1，综述从三个维度来划分代码数据合成的方法，从里面提取了一些工作出来看看。</p>
<h3 id="model-building-phases-维度">Model Building Phases 维度</h3>
<p>首先是按模型训练阶段划分。</p>
<p>1、预训练</p>
<ul>
<li>Phi系列工作（《Textbooks Are All You
Need》），通过合成的教科书进行预训练，用少量的数据就能到比较好的效果<br>
</li>
<li>CodeLlama提出代码执行反馈的方法，提高数据质量<br>
</li>
<li>《Instruction pretraining: Language models are supervised multitask
learners》用instruction synthesizer从无监督数据获取instruction-response
pair，使用量级更小的instruction数据进行预训练就能获得比同量级下无监督数据更好的效果</li>
</ul>
<p>2、SFT</p>
<ul>
<li>经典工作Self-Instruct &amp; Evol-Instruct<br>
</li>
<li>Magicoder利用OSS-INSTRUCT的方法（OSS=open-source code
snippets），合成了75k的指令数据，并获得了不错的效果。<br>
</li>
<li>Auto Evol-Instruct：《Automatic instruction evolving for large
language models》，可以自动端到端进行指令进化<br>
</li>
<li>WaveCoder：《WaveCoder: Widespread and versatile enhancement for
code large language models by instruction
tuning》，合成CodeSeaXDataset，涵盖了代码摘要、生成、翻译和修复等任务。<br>
</li>
<li>Semcoder：《Semcoder: Training code language models with
comprehensive semantics》，合成PyX数据集<br>
</li>
<li>Autocoder：《Autocoder: Enhancing code large language model with
AIEV-INSTRUCT》，引入了AIEV-INSTRUCT，一个 two-stage agent interaction
framework</li>
</ul>
<p>3、Preference alignment</p>
<ul>
<li>CodeUltraFeedback：偏好数据集<br>
</li>
<li>PLUM：用于训练 CodeLLM 的偏好学习框架</li>
</ul>
<h3 id="core-objectives-维度">Core Objectives 维度</h3>
<p>1、Quality</p>
<ul>
<li>《LLM-assisted code cleaning for training accurate code
generators》通过增强代码结构和可读性来提高质量<br>
</li>
<li>《Personalized distillation: Empowering opensourced LLMs with
adaptive learning for code generation》<br>
</li>
<li>《Language models can teach themselves to program
better》使用self-play来增强CodeLLM<br>
</li>
<li>Autocoder通过模拟人类编写代码 +
单元测试执行反馈来获得高质量数据</li>
</ul>
<p>2、Diversity</p>
<p>《What makes good data for alignment? a comprehensive study of
automatic data selection in instruction tuning》和《#instag: Instruction
tagging for analyzing supervised fine-tuning of large language
models》都指出数据多样性对模型alignment的影响很大。</p>
<ul>
<li>MagicCoder从真实数据里采样来提升多样性<br>
</li>
<li>《Automatic instruction evolving for large language
models》用LLM自主设计计划规则<br>
</li>
<li>WaveCoder用KCenterGreedy算法选择多样化样本<br>
</li>
<li>《Training language models on synthetic edit sequences improves code
synthesis》</li>
</ul>
<p>3、Reasoning</p>
<ul>
<li>《LLM-assisted code cleaning for training accurate code
generators》对function进行summarization，并加到注释里<br>
</li>
<li>Semcoder模仿“小黄鸭调试法”逐步解释代码<br>
</li>
<li>《Enhancing code generation performance of smaller models by
distilling the reasoning ability of
LLMs》从已有代码生成plan，用plan和代码在微调代码生成模型<br>
</li>
<li>《Beyond code: Evaluate thought steps for complex code
generation》利用ChatGPT获得为复杂代码生成的复杂步骤<br>
</li>
<li>Case2code帮助模型提高归纳推理能力</li>
</ul>
<p>4、Iterative programming</p>
<p>有些情况下单步无法获得正确代码，需要迭代修改。</p>
<ul>
<li>《OpenCodeInterpreter: Integrating code generation with execution
and
refinement》给出Code-Feedback数据集，包含68,000次交互，反复迭代修改代码。<br>
</li>
<li>《Semcoder》引入了 PyX-R
调试数据集，其中包含描述、错误代码、跟踪和reasoning，用于训练 LLM
进行调试和自我改进。<br>
</li>
<li>《Cycle: Learning to self-refine the code
generation》通过整合问题描述、已有代码和执行反馈来改进有缺陷的代码。<br>
</li>
<li>《LETI: Learning to generate from textual
interactions》使用自然语言指令、生成的代码和来自error的文本反馈来微调模型。<br>
</li>
<li>《Reflexion: language agents with verbal reinforcement
learning》使用口头和启发式反馈来强化language agents。</li>
</ul>
<h3 id="其他特别任务">其他特别任务</h3>
<p>包括</p>
<ul>
<li>NL2SQL<br>
</li>
<li>代码修复<br>
</li>
<li>代码翻译<br>
</li>
<li>代码重构</li>
</ul>
<p>等特别任务，都分别有一些数据相关工作。</p>
<h2 id="data-filtering-techniques">Data Filtering Techniques</h2>
<p>合成的数据往往混杂着badcase，因此数据过滤就很重要。</p>
<p>1、基于规则</p>
<p>比如太长的行、太长的文件、字母字符太少等过滤规则。</p>
<p>另外还是有SimHash + LSH的去重。</p>
<p>2、基于interpreter</p>
<p>通过代码解释器执行反馈。</p>
<p>3、基于SLM</p>
<p>使用小模型进行数据过滤，效果可以超过规则或者代码解释器的方法。</p>
<ul>
<li>《Superfiltering: Weak-to-strong data filtering for fast
instruction-tuning》评估了弱模型和强模型之间的一致性，以确定指令调优样本的难度，证明指令遵循难度
(IFD) 分数在捕捉样本复杂性方面优于困惑度。<br>
</li>
<li>《Instruction mining: Instruction data selection for tuning large
language models》利用自然语言指标来预测推理损失，这比微调 LLM
提供了更有效的评估数据的方法。<br>
</li>
<li>《CodeBERTScore: Evaluating code generation with pretrained models
of code》用bert计算相似度。<br>
</li>
<li>《WaveCoder》利用 KCenterGreedy
算法来选择近似于完整分布的数据子集。<br>
</li>
<li>Llama-3则是利用fasttext、Roberta等来识别高质量的token。</li>
</ul>
<p>4、基于LLM</p>
<ul>
<li>《Alpagasus: Training a better alpaca with fewer data》利用 ChatGPT
作为评分器。<br>
</li>
<li>《ICE-score: Instructing large language models to evaluate
code》通过 LLM 评估代码有用性和功能正确性。<br>
</li>
<li>《WaveCoder》使用 GPT-4 作为鉴别器来分析和过滤指令数据，利用 CoT
推理逐步评估每个实例，将它们分类为有效或无效。<br>
</li>
<li>Llama-3用早期的Llama版本对代码的正确定和风格进行打分。</li>
</ul>
<h1 id="instruction-pre-training">Instruction Pre-Training</h1>
<p>论文：《Instruction Pre-Training: Language Models are Supervised
Multitask Learners》</p>
<p>时间：2024年6月</p>
<p>机构：清华，微软</p>
<p>模型、代码、数据都在https://github.com/microsoft/LMOps/tree/main/instruction_pretrain可找，instruction-synthesizer也在https://huggingface.co/instruction-pretrain/instruction-synthesizer了，可以直接下下来使用。</p>
<p>近些年来大模型的成功主要得益于大规模的无监督预训练，但是一些工作如《Scaling
instruction-finetuned language
models》证明了有监督多任务学习还是有价值的。Instruction
Pre-Training这篇论文就通过使用instruction-synthesizer把无监督数据增强为instruction-response
pair，再进行有监督多任务训练，来获得不错的效果。</p>
<img src="/e259c7b2/instructionpt_intro.png" class title="数据合成">
<h2 id="instruction-synthesizer">Instruction Synthesizer</h2>
<p>《Skill-it! A Data-Driven Skills Framework for Understanding and
Training Language Models》的分析表明，raw corpora里包含很多intrinsic
task，这也是能够从无监督数据中获取instruction数据的前提。</p>
<p>用于从无监督数据合成instruction数据的模型叫instruction
synthesizer，它是从通用LLM进行multi-task fine-tuning得到的：</p>
<img src="/e259c7b2/instructionpt_tune.png" class title="数据合成">
<p>微调synthesizer的数据保证了多样性，使得instruction
synthesizer有比较好的泛化性。收集的数据和任务包括以下这些：</p>
<img src="/e259c7b2/instructionpt_data.png" class title="数据合成">
<p>微调instruction synthesizer的时候训练数据被组织成few-shot
examples的样式，多个example都来自同一个数据集，这样能保证任务格式和任务类别的一致性。计算loss的时候只算了instruction-response
pairs部分token的loss。</p>
<p>推理的时候也是用类似的方式，通过把多轮的结果concat起来能够得到few-shot
example的prompt。</p>
<img src="/e259c7b2/instructionpt_syn.png" class title="数据合成">
<p>为了方便提取合成的数据，实验中给各个任务设计了不同的模板，用一些特殊的token帮助区分各个成分：</p>
<img src="/e259c7b2/instructionpt_template.png" class title="数据合成">
<p>实验上使用的synthesizer模型是Mistral-7B-v0.1。在数据合成中，平均每段无监督文本能创建大约5对数据，每对数据的长度52个token。训练的参数如下：</p>
<img src="/e259c7b2/instructionpt_hyperparam.png" class title="数据合成">
<h2 id="lm-pre-training">LM Pre-Training</h2>
<p>获得instruction-response数据后，参考《The flan collection: Designing
data and methods for effective instruction
tuning》的模板来提升指令格式的多样化。另外还采用《Adapting large
language models via reading
comprehension》里的模板把原始文本和instruction-response数据合到一起训练。按上图inference的得到的结果，同样的M-shot
prompt也用来训练。</p>
<p>instruction
pre-training除了数据不同外，其他训练配置保持和无监督训练一样。实验中文章使用了两种方式：从零预训练和（domain-adaptive）继续预训练。</p>
<p>由于instruction数据在数据量上相对还是比无监督数据少很多，所以从零预训练的时候采用两种数据混合的方式。实验上无监督数据的数据量是100B
token，而指令数据约0.8B token。</p>
<p>而继续预训练也不只使用这里生成的指令数据，而会和通用的指令数据混在一起，混合的比例参考《Adapting
large language models via reading comprehension》。</p>
<h2 id="效果">效果</h2>
<p>在通用领域上，不同训练方式下的效果：</p>
<img src="/e259c7b2/instructionpt_perf.png" class title="数据合成">
<p>和外部模型对比：</p>
<img src="/e259c7b2/instructionpt_perf_2.png" class title="数据合成">
<p>看起来在某些任务略有提升吧，似乎不太明显。</p>
<p>而domain-specific的task上，提升可能更明显一点：</p>
<img src="/e259c7b2/instructionpt_domain_perf.png" class title="数据合成">
<p>毕竟合成的数据量相对来说还是比较少，专注在少量方向效果更好。</p>
<h1 id="auto-evol-instruct">Auto Evol-Instruct</h1>
<p>auto evol-instruct可以说是evol-instruct的升级版、自动版，不过auto
evol-instruct主要目的是提升instruct的复杂性，多样性上可能没有特别关注。</p>
<p>原本evol-instruct需要人为设计进化的具体方向，这样在数据/领域变更的时候就不太方便（比如“提高空间复杂度/时间复杂度要求”的限制在code领域很合理，但是在情感分析领域就没什么意义）。而auto
evol-instruct提出的方法可以自动适配到不同的数据，而不需要人工修改。</p>
<p>大致的方法如下：</p>
<img src="/e259c7b2/auto_evol_intro.png" class title="数据合成">
<p>首先，会有一个initial进化方法，进化所使用的prompt如下：</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>You are an Instruction Rewriter that rewrites the given <span class="co">#Instruction# into a more complex version.</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>Please follow the steps below to rewrite the given <span class="st">&quot;#Instruction#&quot;</span> into a more <span class="bu">complex</span> version.</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>Step <span class="dv">1</span>: Please read the <span class="st">&quot;#Instruction#&quot;</span> carefully <span class="kw">and</span> <span class="bu">list</span> <span class="bu">all</span> the possible methods to make this instruction more <span class="bu">complex</span> (to make it a bit harder <span class="cf">for</span> well<span class="op">-</span>known AI assistants such <span class="im">as</span> ChatGPT <span class="kw">and</span> GPT4 to handle). Please do <span class="kw">not</span> provide methods to change the language of the instruction<span class="op">!</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>Step <span class="dv">2</span>: Please create a comprehensive plan based on the <span class="co">#Methods List# generated in Step 1 to make the #Instruction# more complex. The plan should include several methods from the #Methods List#.</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>Step <span class="dv">3</span>: Please execute the plan step by step <span class="kw">and</span> provide the <span class="co">#Rewritten Instruction#. #Rewritten Instruction# can only add 10 to</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="dv">20</span> words into the <span class="st">&quot;#Instruction#&quot;</span>.</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>Step <span class="dv">4</span>: Please carefully review the <span class="co">#Rewritten Instruction# and identify any unreasonable parts. Ensure that the #Rewritten Instruction# is only a more complex version of the #Instruction#. Just provide the #Finally Rewritten Instruction# without any</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>explanation.</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>Please reply strictly <span class="kw">in</span> the following <span class="bu">format</span>:</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>Step <span class="dv">1</span> <span class="co">#Methods List#:</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>Step <span class="dv">2</span> <span class="co">#Plan#:</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>Step <span class="dv">3</span> <span class="co">#Rewritten Instruction#:</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>Step <span class="dv">4</span> <span class="co">#Finally Rewritten Instruction#:</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">#Instruction#:</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>&#123;Instruction&#125;</span></code></pre></div>
<p>auto
evol-instruct会迭代多轮来优化进化方法（也就是上面这个prompt）。在每一轮中，会从前一轮数据集中随机抽样一小批指令，并对每个指令类进行l次进化，这l次进化就构成Evol
Trajectory。</p>
<p>对于被进化后的指令，用optimizer
LLM进行分析，找到本次进化中这些指令中存在的问题。optimizer
LLM所用的prompt如下：</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>The following <span class="bu">list</span> shows cases where an Instruction evolves into a more <span class="bu">complex</span> version of an Instruction.</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>For each case, stage <span class="dv">0</span> represents the Instruction <span class="kw">in</span> its initial state, <span class="kw">and</span> each subsequent stage requires an increase <span class="kw">in</span> complexity based on the previous stage.</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>Please identify cases that failed to evolve, <span class="kw">and</span> provide their case ID <span class="kw">and</span> reasons.</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>&#123;Evolutionary Trajectory&#125;</span></code></pre></div>
<p>下面是一些进化结果分析样例：</p>
<img src="/e259c7b2/auto_evol_example.png" class title="数据合成">
<p>收集到分析结果之后，会要求optimizer
LLM来改进现有的进化方法，prompt如下：</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>&#123;Feedback&#125;</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>I will provide you <span class="cf">with</span> the method <span class="cf">for</span> evolving the above instructions.</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>You need to optimize this method based on the feedback <span class="im">from</span> the evolution failure case, without harming the performance on other cases, <span class="kw">and</span> ensure that the complexity increase brought by the optimized method <span class="kw">is</span> <span class="kw">not</span> lower than the previous method.</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>Please provide the optimized method <span class="kw">in</span> the following <span class="bu">format</span>.</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;Optimized Method</span><span class="ch">\n</span><span class="co">&lt;Optimized Method Here&gt;</span><span class="ch">\n</span><span class="co">&#39;&#39;&#39;</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>&#123;Evol Prompt&#125;</span></code></pre></div>
<p>在改进进化方法的时候，optimizer
LLM并不能保证总是能给出好的结果，因此这里会使用采样解码（而非贪婪解码之类的）的方式获得m个结果，这样就有m个候选的改进方案。</p>
<p>这m个改进方案会分别进行验证：在特意保留的dev数据集上进化指令，并获得response。利用设计的一系列规则来判断进化是否失败，然后选择这m个候选改进中失败率最低的方案，进入下一轮优化。</p>
<p>判断是否失败的规则如下：</p>
<img src="/e259c7b2/auto_evol_fail.png" class title="数据合成">
<p>多轮迭代之后，迭代数达到设置的上限，或者指令的复杂度难以进一步进化之后，就得到了最终的进化方法。这个最终方法就会应用在所有数据上。</p>
<h1 id="wavecoder">WaveCoder</h1>
<p>论文：《WaveCoder: Widespread And Versatile Enhancement For Code
Large Language Models By Instruction Tuning》</p>
<p>时间：2023年12月</p>
<p>机构：清华，微软</p>
<p>按照《Lima》《Instruction tuned models are quick
learners》的研究结果，SFT阶段的数据质量和多样性影响很大。WaveCoder优化代码模型的工作主要就在于合成了高质量的instruction数据，数量也并不是很多，总共有19,915条。整体的流程如下：</p>
<img src="/e259c7b2/wavecoder_intro.png" class title="数据合成">
<p>这一万多条数据主要涵盖四类代码相关任务（参考《Codexglue: A machine
learning benchmark dataset for code understanding and
generation》的分类）：<br>
- Code Generation：从文本到代码，或者从代码到代码，生成solution<br>
- Code Summarization：给定代码，写文本摘要<br>
- Code Repair：修改给定代码中的潜在问题<br>
- Code Translation：将一种变成语言转换成另一种</p>
<p>生成的数据分布如下：</p>
<img src="/e259c7b2/wavecoder_task.png" class title="数据合成">
<h2 id="数据合成">数据合成</h2>
<p>1、raw data 收集</p>
<p>要合成数据，首先需要一些raw
data。这里选择的是CodeSearchNet，它包含github上的200万个 (comment, code)
pair。在这些数据里，首先过滤掉了太长或者太短的代码，然后参考《Code
alpaca: An instruction-following llama model for code
generation》的分析，排除掉在blacklist中的关键词，因为包含这些关键词可能会影响训练模型的效果。</p>
<p>在这个基础上，为了保证原始代码数据的多样性，使用KCenterGreedy算法获取一批core
samples。这样原始数据天然具有多样性，因此对后续用于合成数据的LLM和prompt的随机性和能力的要求就没那么高了。KCenterGreedy获得的数据的语言分布如下：</p>
<img src="/e259c7b2/wavecoder_lang.png" class title="数据合成">
<p>2、Generator-Discriminator框架</p>
<p>数据合成分为两个阶段：（1）Generation Phase（2）Discrimination
Phase。</p>
<p>（1）Generation Phase</p>
<p>在这一阶段，首先用GPT-4为前面介绍的4个代码任务写定义。之后人工给每个任务添加生成的要求。这两部分内容会整合到prompt里，让ChatGPT根据raw
data生成指令数据。</p>
<img src="/e259c7b2/wavecoder_framework.png" class title="数据合成">
<p>（2）Discrimination Phase</p>
<p>为了保证合成的数据质量，使用GPT-4作为判别器，分析和过滤上一步合成的数据。判别的prompt里把需要判别的内容细分成多个小点，方便模型分别给出结果，另外还会输入few-shot
example，这些example不仅包含好的样本，也会包含不好的样本。一个判别的例子如下：</p>
<img src="/e259c7b2/wavecoder_example.png" class title="数据合成">
<h1 id="semcoder">Semcoder</h1>
<p>论文：《SemCoder: Training Code Language Models with Comprehensive
Semantics Reasoning》</p>
<p>时间：2024年6月</p>
<p>Semcoder在代码数据优化上，也是以合成可执行代码为目标，因为原来的代码数据中大约有四分之一的代码是不能执行的。具体来说，Semcoder在OSS-Instruct方案的基础上进行了改进：OSS-Instruct是随机抽取一些代码片段，而Semcoder则是将raw
data解析成AST，并抽取子树以获得可解析的种子。OSS-Instruct之前在《训练数据合成(一)》介绍过了。可解析的代码会被执行，可执行的代码会用LLM进行调试，直到代码可正确运行。</p>
<p>Semcoder构造的数据集称为PyX，构造的大致过程如下：</p>
<img src="/e259c7b2/semcoder_intro.png" class title="数据合成">
<p>PyX和OSS-Instruct数据集的效果对比：</p>
<img src="/e259c7b2/semcoder_compare.png" class title="数据合成">
<p>Semcoder还在PyX基础上，模仿“小黄鸭调试法”的思路，弄了一个提升模型debug和reasoning的数据集，这个这里暂时不展开，不过“拟人”这个思路到时挺有启发性。</p>
<h1 id="autocoder">Autocoder</h1>
<p>论文：《AutoCoder: Enhancing Code Large Language Model with
AIEV-INSTRUCT》</p>
<p>时间：2024年5月</p>
<p>Autocoder提出一个用于合成代码相关多轮对话数据的方法AIEV-Instruct（AIEV
= Agent-Interaction and
Execution-Verified）。一共合成了169k的代码指令数据，用这份数据训练的33B参数的AutoCoder，在效果上超越了2024年4月份的GPT-4o，humaneval达到90.9%。</p>
<p>AIEV-Instruct分为两个阶段：（1）Teaching Stage和（2）Self-Learning
Stage。第一个阶段用于让student模型从teacher模型学习知识，而当student模型的效果达到了teacher模型的水平，就需要进行自学习了。</p>
<p>整体的流程如下：</p>
<img src="/e259c7b2/autocder_intro.png" class title="数据合成">
<p>1、Teaching Stage</p>
<p>在这个阶段，会用GPT-4 Turbo作为teacher model。GPT-4
Turbo会被赋予两个角色，questioner和programmer，二者会进行交互，以获得对话数据。</p>
<p>在一开始，先利用GPT-4 Turbo执行OSS-Instruct方案，从raw
data中设计代码问题的描述，以及solution。在这个基础上，相比原始的OSS-Instruct，这里还会生成单元测试。</p>
<p>这些生成的问题描述（上图①）、solution和单元测试（上图②）会被放进对话列表中作为对话历史的一部分。之后会进行执行反馈来验证代码，如果执行发生错误，将会把具体的error信息也加入到对话信息列表中（上图③）。同时会让questioner根据报错信息生成自然语言描述，加到对话信息列表中（上图④）。之后会由programmer重新修改代码（上图⑤），并迭代这个过程，直到单元测试通过。不过这个迭代过程有最大上限，设置为7次。如果迭代其次之后代码仍然无法执行成功，那么就放弃这一条数据。</p>
<p>每处理2000条数据，就会对student模型，也就是AutoCoder进行一次训练。这2000条数据会按1:9被分成验证集和训练集，如果在验证集上student
model的效果超过了teacher模型，就会进入到Self-Learning
Stage，否则就继续进行下一轮的Teaching Stage。</p>
<p>2、Self-Learning Stage</p>
<p>Self-Learning Stage所做的事情其实跟Teaching
Stage基本上是一样的，只是teacher模型换成了AutoCoder自己。</p>
<p>这里合成数据所使用的raw
data是来自Magicoder-Evol-Instruct和Magicoder-OSS-Instruct数据集的数据，总共186,000条。最终合成的数据集称为AutoCoder-AIEV-Instruct，包含169,000条数据，共241,000轮对话。AutoCoder-AIEV-Instruct和其他数据集的case对比如下：</p>
<img src="/e259c7b2/autocoder_compare.png" class title="数据合成">
<h1 id="代码格式优化">代码格式优化</h1>
<p>论文：《LLM-assisted code cleaning for training accurate code
generators》</p>
<p>时间：2023年11月</p>
<p>这篇论文核心就在于优化已有代码数据的格式。具体来说优化3点：<br>
-
重命名：变量命名的优化，把一些和上下文无关的，不清晰的命名改成一看就知道是什么的名字<br>
- 模块化：把长代码段落split成单独的功能块，写成小函数的形式<br>
- planning：用原子化的函数重新组装成完整的代码，并为每个操作写上注释</p>
<img src="/e259c7b2/format_intro.png" class title="数据合成">
<p>而这些转换都是通过ChatGPT来实现的。转换完成之后会使用测试样例进行验证，验证不通过的话最多会重试5次转换。</p>
<p>1、重命名</p>
<p>重命名的prompt如下：</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>QUESTION:</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>&#123;problem_statement&#125;</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>ANSWER:</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>‘’‘python</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>&#123;solution&#125;</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>’‘’</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>Rename the variables <span class="kw">in</span> the program to be descriptive, meaningful, <span class="kw">and</span> consistent. Do <span class="kw">not</span> change the original semantics of the program. Enclose the program within backticks <span class="im">as</span> shown above <span class="kw">and</span> remember to use descriptive variable names.</span></code></pre></div>
<p>2、模块化</p>
<p>模块化的时候，如果代码比较长，那么就执行两轮的模块化。第一轮模块化的prompt如下：</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>QUESTION:</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>&#123;problem_statement&#125;</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>ANSWER:</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>‘’‘python</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>&#123;renamed_solution&#125;</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>’‘’</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>Refactor the above program. Follow the guidelines:</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="op">*</span> Make the program more modular <span class="cf">with</span> smaller <span class="kw">and</span> meaningful helper functions.</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="op">*</span> Use good descriptive names <span class="cf">for</span> the helper functions.</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="op">*</span> Have an entry function called <span class="st">&#39;main()&#39;</span>.</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="op">*</span> Call <span class="st">&#39;main()&#39;</span> inside <span class="st">&#39;if name == &#39;</span>main<span class="st">&#39;&#39;</span>.</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>Do <span class="kw">not</span> change the original semantics of the program significantly <span class="kw">and</span> there <span class="kw">is</span> no need to perform optimizations. Enclose the program within backticks <span class="im">as</span> shown above.</span></code></pre></div>
<p>第一轮完成后，如果代码中还包含一个超过20行的函数，那么就进一步进行模块，并指出具体要分解哪些函数。实践中大约有20%~40%需要二次模块化。所用prompt：</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>QUESTION:</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>&#123;problem_statement&#125;</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>ANSWER:</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>‘’‘python</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>&#123;modularized_solution&#125;</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>’‘’</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>Refactor the above program by modularizing it <span class="kw">and</span> breaking down <span class="bu">long</span> <span class="kw">and</span> <span class="bu">complex</span> functions into smaller meaningful helper functions. Particularly refactor <span class="kw">and</span> decompose the following function(s) into smaller helper functions <span class="op">-</span> &#123;function_names_string&#125; Only <span class="cf">return</span> the refactored program enclosed <span class="kw">in</span> backticks <span class="im">as</span> shown above.</span></code></pre></div>
<p>3、planning</p>
<p>prompt如下：</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>QUESTION:</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>&#123;problem_statement&#125;</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>ANSWER:</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>‘’‘python</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>&#123;modularized_solution&#125;</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>’‘’</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>Generate a summary <span class="cf">for</span> the following functions <span class="kw">and</span> classes <span class="kw">in</span> the program within four lines each. The summaries should be descriptive <span class="kw">and</span> helpful <span class="cf">for</span> understanding the program (however yet concise <span class="kw">in</span> four lines).</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>The functions <span class="kw">and</span> classes are <span class="op">-</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>&#123;list_of_function_names&#125;</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>Follow the provided <span class="bu">format</span> <span class="cf">for</span> the summaries <span class="cf">while</span> being informative <span class="kw">and</span> concise. Enclose the signatures <span class="kw">in</span> backticks <span class="im">as</span> shown above.</span></code></pre></div>
<h1 id="personalised-distillation">Personalised Distillation</h1>
<p>论文：《Personalized distillation: Empowering opensourced LLMs with
adaptive learning for code generation》</p>
<p>时间：2023年10月</p>
<p>之前获取数据的做法一般是用指令从teacher模型（比如ChatGPT）获取答案，然后用得到的指令+答案数据来训练student模型。这种方法就没有考虑到不同的student模型的现有效果和优缺点。</p>
<p>而Personalised Distillation则是先让student
model尝试解决问题，然后teacher
model再在这个基础上提供针对性的修改，帮助student
model实现个性化的学习。</p>
<img src="/e259c7b2/distill_intro.png" class title="数据合成">
<p>具体来说，对于代码任务，如果student
model已经能够解决，那么这个问题就不再需要teacher model。只有哪些student
model解决不了的，才会把问题、student model的方案、执行反馈输入给teacher
model，以获得改进方案，并给student model学习，这就是personalised
distillation。</p>
<p>直接学习teacher model的答案，和personalised
distillation两种方法对比的一个例子如下：</p>
<img src="/e259c7b2/distill_example.png" class title="数据合成">
<p>可以看到个性化蒸馏后，student model的结果和teacher
model直接生成的结果还是有比较大的不同。个性化蒸馏帮助student
model在自己的思路上改进，而不单单只是学习teacher model的输出。</p>
<h1 id="关于数据过滤">关于数据过滤</h1>
<p>数据过滤也是数据合成中一个重要的环节。</p>
<h2 id="superfiltering">Superfiltering</h2>
<p>论文：《Superfiltering: Weak-to-Strong Data Filtering for Fast
Instruction-Tuning https://arxiv.org/abs/2402.00530》</p>
<p>时间：2024年2月</p>
<p>Superfiltering通过实验发现，大模型和小模型在计算指令的IFD分数时具有一致性，因此可以用小模型通过IFD来选择指令，从而达到低成本高效果的目的。</p>
<p>IFD = Instruction-Following
Difficulty。IFD分数是一种用于衡量指令对模型生成回复帮助程度的指标。</p>
<p>以下是一个计算IFD分数的具体例子：</p>
<p>假设有一个instruction - response pair：<br>
- Instruction（x）：“请介绍一下苹果这种水果。”<br>
-
Response（y）：“苹果是一种常见的水果，通常呈圆形，颜色有红色、绿色等，口感酸甜可口，富含维生素C和纤维素等营养成分。”</p>
<p>使用模型计算PPL： -
比如用规模比较小的GPT-2计算在没有instruction情况下生成该回复的PPL，<span class="math inline">\(\mathrm{PPL}(y_i)\)</span>，假设计算结果=3<br>
-
然后使用GPT-2计算在给定指令“请介绍一下苹果这种水果。”情况下生成该回复的PPL
，<span class="math inline">\(\mathrm{PPL}(y_i|x_i)\)</span>，假设计算结果=5</p>
<p>然后根据IFD分数的计算公式计算IFD score：</p>
<p><span class="math display">\[\mathrm{IFD}(y_i|x_i)=\frac{\mathrm{PPL}(y_i|x_i)}{\mathrm{PPL}(y_i)}\]</span></p>
<p>那么这条指令的IDF = 3 / 5 = 0.6。</p>
<p>一般来说，IFD
分数越低，表示指令对模型生成回复的帮助越大，该指令相对更容易被模型处理；反之，IFD
分数越高，表示指令对模型生成回复的帮助越小，指令难度相对较大。论文里选择了IFD分数高的部分数据进行训练。</p>
<h2 id="codebertscore">CodeBERTScore</h2>
<p>论文：《CodeBERTScore: Evaluating Code Generation with Pretrained
Models of Code》</p>
<p>时间：2023年2月</p>
<p>用代码相关指令数据，训练了5种语言（Python、C、Java、CPP、JavaScript）的Bert打分模型，在https://huggingface.co/neulab上可以下载。</p>
<h1 id="小结">小结</h1>
<ul>
<li>不同领域的数据合成方法也很不相同<br>
</li>
<li>代码数据需要考虑可执行、可读性、结构化等维度</li>
</ul>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门--CLIP</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】Mastering the Craft of Data Synthesis for CodeLLMs
https://arxiv.org/pdf/2411.00005<br>
【2】Instruction Pre-Training: Language Models are Supervised Multitask
Learners https://arxiv.org/abs/2406.14491<br>
【3】Automatic instruction evolving for large language models
https://arxiv.org/abs/2406.00770<br>
【4】WaveCoder: Widespread And Versatile Enhancement For Code Large
Language Models By Instruction Tuning
https://arxiv.org/abs/2312.14187<br>
【5】SemCoder: Training Code Language Models with Comprehensive
Semantics Reasoning https://arxiv.org/abs/2406.01006<br>
【6】AutoCoder: Enhancing Code Large Language Model with AIEV-INSTRUCT
https://arxiv.org/abs/2405.14906<br>
【7】LLM-Assisted Code Cleaning For Training Accurate Code Generators
https://arxiv.org/abs/2311.14904<br>
【8】Personalized distillation: Empowering opensourced LLMs with
adaptive learning for code generation
https://arxiv.org/abs/2310.18628<br>
【9】Superfiltering: Weak-to-Strong Data Filtering for Fast
Instruction-Tuning https://arxiv.org/abs/2402.00530<br>
【10】CodeBERTScore: Evaluating Code Generation with Pretrained Models
of Code https://arxiv.org/abs/2302.05527</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>预训练</tag>
        <tag>数据合成</tag>
      </tags>
  </entry>
  <entry>
    <title>长文详解--LLM高效预训练(一)</title>
    <url>/dcb57672.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>大模型在生产和生活中的应用越来越多，这对大模型开发者来说是利好消息。</p>
<p>不过随着应用场景增多，对大模型的需求也多种多样。比如有些场景需要参数量为5B的模型，但是开源模型中正好没有这个规模的；也可能有些场景需要一个“总共5个专家激活其中2个专家”的MoE模型，开源模型中很可能也没有能满足这个设置的。</p>
<p>如果每次需要新的模型参数量，或者遇到某些模型结构维度有特定需求的情况，都从零（随机初始化）开始，对模型进行完整的预训练，成本就太高了。</p>
<p>在当下，训练数据动辄5T、10T，预训练的计算成本起步就是几万甚至几十万的GPU
hours，即使是头部大公司也扛不住经常这么做。</p>
<p>因此如何对LLM进行高效的预训练，用较低的计算成本获得我们想要的模型，就是一个很重要的方向。</p>
<h1 id="简介">简介</h1>
<p>如今开源LLM已经有不少，各个大模型团队内部积累没有公开的LLM则更多。复用这些已有模型的参数来初始化新的模型，就是高效预训练的一个方法。</p>
<p>那么具体怎么复用参数呢？最自然的一个想法就是用已经训练好的小模型（reference
model）来初始化一个更大的模型（destinatioin
model），使得大模型能够在初始化就继承了小模型的（部分）知识。在这个基础上通过继续预训练，利用大模型多出来的容量进一步学习，从而快速获得比较好的效果。</p>
<p>这样的升级不仅可以进行一次，而且可以迭代多次进行，这么一来我们从一个很小的模型出发，也可以以较低成本获得规模很大的模型。</p>
<p>在这里，这样的做法统一归类为“从小到大”。正文涉及的“从小到大”的工作如下：</p>
<center>
<table>
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">方法/模型</th>
<th style="text-align: center;">发布时间</th>
<th style="text-align: center;">论文</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Net2Net</td>
<td style="text-align: center;">2015年11月</td>
<td style="text-align: center;">Net2Net: Accelerating Learning via
Knowledge Transfer</td>
</tr>
<tr class="even">
<td style="text-align: center;">Stacking</td>
<td style="text-align: center;">2019年</td>
<td style="text-align: center;">Efficient Training of BERT by
Progressively Stacking</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Stacking 2.0</td>
<td style="text-align: center;">2020年11月</td>
<td style="text-align: center;">Progressively Stacking 2.0: A
Multi-stage Layerwise Training Method for BERT Training Speedup</td>
</tr>
<tr class="even">
<td style="text-align: center;">bert2BERT</td>
<td style="text-align: center;">2021年10月</td>
<td style="text-align: center;">bert2BERT: Towards Reusable Pretrained
Language Models</td>
</tr>
<tr class="odd">
<td style="text-align: center;">LiGO</td>
<td style="text-align: center;">2023年3月</td>
<td style="text-align: center;">Learning to Grow Pretrained Models for
Efficient Transformer Training</td>
</tr>
<tr class="even">
<td style="text-align: center;">MSG</td>
<td style="text-align: center;">2023年5月</td>
<td style="text-align: center;">Masked Structural Growth for 2x Faster
Language Model Pre-training</td>
</tr>
<tr class="odd">
<td style="text-align: center;">FLM-101B</td>
<td style="text-align: center;">2023年9月</td>
<td style="text-align: center;">FLM-101B: An Open LLM and How to Train
It with $100K Budget</td>
</tr>
<tr class="even">
<td style="text-align: center;">Apollo</td>
<td style="text-align: center;">2024年1月</td>
<td style="text-align: center;">Preparing Lessons for Progressive
Training on Language Models</td>
</tr>
<tr class="odd">
<td style="text-align: center;">LLAMA PRO</td>
<td style="text-align: center;">2024年1月</td>
<td style="text-align: center;">LLAMA PRO: Progressive LLaMA with Block
Expansion</td>
</tr>
<tr class="even">
<td style="text-align: center;">52B to 1T</td>
<td style="text-align: center;">2024年7月</td>
<td style="text-align: center;">52B to 1T: Lessons Learned via Tele-FLM
Series</td>
</tr>
</tbody>
</table>
</center>
<p>除了复用小模型参数来获得大模型，也可以复用大模型的参数，来提升小模型的训练速度和效果。本文会讲到的“从大到小”的工作如下：</p>
<center>
<table>
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">方法/模型</th>
<th style="text-align: center;">发布时间</th>
<th style="text-align: center;">论文</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Sheared LLaMA</td>
<td style="text-align: center;">2023年10月</td>
<td style="text-align: center;">Sheared LLaMA: Accelerating Language
Model Pre-training via Structured Pruning</td>
</tr>
<tr class="even">
<td style="text-align: center;">Weight Subcloning</td>
<td style="text-align: center;">2023年12月</td>
<td style="text-align: center;">Weight Subcloning: Direct Initialization
of Transformers Using Larger Pretrained Ones</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Inheritune</td>
<td style="text-align: center;">2024年4月</td>
<td style="text-align: center;">Pre-training Small Base LMs with Fewer
Tokens</td>
</tr>
</tbody>
</table>
</center>
<p>实际上，“从大到小”的方法有很多，我们比较熟悉的有蒸馏、剪枝和甚至量化等，都算是从大模型到小模型。这里这几个工作主要是从初始化的角度来提升小模型的训练效率和效果。其他“从大到小”的方案待下次再整理。</p>
<p>除了“从小到大”和“从大到小”，还可以“从dense到sparse”，用dense模型来加速MoE模型的训练：</p>
<center>
<table>
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">方法/模型</th>
<th style="text-align: center;">发布时间</th>
<th style="text-align: center;">论文</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Sparse Upcycling</td>
<td style="text-align: center;">2022年12月</td>
<td style="text-align: center;">Sparse Upcycling: Training
Mixture-of-Experts from Dense Checkpoints</td>
</tr>
<tr class="even">
<td style="text-align: center;">Qwen2-57B-A14B</td>
<td style="text-align: center;">2024年07月</td>
<td style="text-align: center;">QWEN2 TECHNICAL REPORT</td>
</tr>
<tr class="odd">
<td style="text-align: center;">AquilaMoE</td>
<td style="text-align: center;">2024年08月</td>
<td style="text-align: center;">AquilaMoE: Efficient Training for MoE
Models with Scale-Up and Scale-Out Strategies</td>
</tr>
</tbody>
</table>
</center>
<h1 id="从小到大">从小到大</h1>
<h2 id="net2net">Net2Net</h2>
<p>论文：《Net2Net: Accelerating Learning via Knowledge Transfer》</p>
<p>收益：相比随机初始化，Net2Net收敛更快，在Inception模型实验中，前期的收敛速度能达到1~2倍。</p>
<p>实验模型：inception model</p>
<h3 id="tldr">TL;DR</h3>
<p>15年的Net2Net是模型参数复用的经典工作，核心就是function-preserving
initialization（FPI）这个初始化方法，即用训练好的小模型初始化一个大模型，使得大模型在初始化完之后，就立刻有和小模型（基本）完全一致的输出。</p>
<p>然后在这样的初始化基础上进行训练，比完全从零开始训练效率更高。</p>
<img src="/dcb57672/net2net_intro.png" class title="LLM高效训练">
<h3 id="fpi">FPI</h3>
<p>1、FPI分析</p>
<p>把FPI公式化：要求对训练好的小模型（参数为 $$
），以及要初始化的大模型（参数为 <span class="math inline">\(\theta^{\prime}\)</span> ），对于所有的输入
x，有</p>
<p><span class="math display">\[\forall
x,f(x;\theta)=g(x;\theta&#39;)\]</span></p>
<p>这里FPI隐含了几个限制：<br>
-
大小模型的基本结构要一致，如每层内的操作要一样，都是同样设计的卷积层/transformer层等，而只有模型深度和模型宽度不同，这样才能通过FPI进行张量的扩展<br>
- 大模型的层数 ≥ 小模型层数（如果大模型层数 &lt;
小模型层数的话当然也可以直接裁剪，但是这样就没法保证效果相同了）<br>
- 大模型张量维度 ≥
小模型张量维度（同上，如果大模型维度小了就没法保证效果相同了）</p>
<p>FPI有几个好处：<br>
- 大模型初始化之后的训练基本上可以保证收益总是正的 --
在模型容量更大的情况下，基于小模型学到的知识进行继续预训练<br>
-
所有参数可以一起训练，这相比拼接随机初始化的参数或者新模型层的做法更稳定一些；拼接随机初始化参数的做法可能需要先冻结训练过的参数，对随机初始化的参数进行warmup，待所有参数都收敛得差不多之后，才能所有参数共同学习</p>
<p>2、FPI实现</p>
<p>FPI包括两个维度的扩展，横向和纵向。</p>
<p>（1）纵向</p>
<p>纵向的扩展就是增加层数，要保持模型在增加层数之后输出结果不变，最简单的方法就是增加一个identity
mapping层：</p>
<img src="/dcb57672/net2net_depth.png" class title="LLM高效训练">
<p>由于提出的时间比较早，这个时候还没有残差连接。如果是在有残差连接的模型，想要在新增层之后保持模型的输出不变，那么应该要让新增加的层的输出为0，从而不影响residual
connection中恒等路径的值。</p>
<p>（2）横向</p>
<p>FPI横向的扩展相对复杂一点。横向扩展对应着张量维度的扩展。</p>
<p>比如对于linear层，对应的就是矩阵的行和列的增加，而对于卷积层，那就对应着channel数量的增加。</p>
<p>那么怎么扩展这些张量，使得扩展前后在相同的输入下，输出不变呢？</p>
<p>这里用bert2BERT论文里的图举例：</p>
<img src="/dcb57672/b2b_fpi.png" class title="LLM高效训练">
<p>这里默认向量是列向量，因此增加矩阵输入维度相当于增加矩阵的列数，而增加矩阵输出维度相当于增加矩阵的行数。</p>
<p>首先，要扩展输入维度，可以从前面的列随机采样一列，比如上图中间增加的红色x1输入。扩展的第三列参数来自第一列。</p>
<p>增加一个输入当然就会改变输出值。比如原来是</p>
<p>h1 = o·x1 + p·x2</p>
<p>h2 = q·x1 + r·x2</p>
<p>复制x1输入和对应的权重之后就变成</p>
<p>h1' = o·x1 + p·x2 + o·x1</p>
<p>h2' = q·x1 + r·x2 + q·x1</p>
<p>保证h1、h2接收到的数值不变，需要把第一列和第三列都乘以1/2。</p>
<p>h1' = (o/2)·x1 + p·x2 + (o/2)·x1 = h1</p>
<p>h2' = (q/2)·x1 + r·x2 + (q/2)·x1 = h2</p>
<p>乘的这个系数和采样的列重复次数相关：某列的参数重复了n次，那么所有对应的参数都要乘以1/n。</p>
<p>类似地，矩阵输出维度的扩展也可以通过随机从前面的行采样得到。</p>
<p>这里要注意的是，新增加的红色h2神经元相当于增加下一个矩阵的输入维度，所以下一个矩阵也要调整输入列的系数，保证输出结果不变。</p>
<p>把模型的横向（宽度）扩展和纵向（深度）扩展结合起来，就可以同时在两个维度扩展模型。</p>
<p>3、实验</p>
<p>Net2Net论文以inception network为例，进行了实验。</p>
<p>横向扩展得到的网络叫做Net2WiderNet，纵向扩展得到的网络叫Net2DeeperNet。</p>
<p>作为和对比，还有两个baseline方案：<br>
- random
pad：把横向扩展的参数进行随机初始化，作为Net2WiderNet的对比照<br>
- random
initialization：把加深的层进行随机初始化，作为Net2DeeperNet的对照</p>
<p>Net2WiderNet的效果如下：</p>
<img src="/dcb57672/net2net_wide.png" class title="LLM高效训练">
<p>各种lr下，Net2WiderNet能够和随机初始化/random
pad的模型达到相同的水平，而收敛速度更快。在前期大约能有一倍的收敛提速。</p>
<p>Net2DeeperNet的效果如下：</p>
<img src="/dcb57672/net2net_deep.png" class title="LLM高效训练">
<p>Net2DeeperNet比随机初始化和random
initialization达到相同accuracy的速度更快。这里DeeperNet最终的validation
accuracy比原模型也要略高一些，毕竟模型参数量更大。</p>
<p>除了可以利用小模型快速训练更大的目标模型，Net2Net的另一个应用场景就是可以基于一个训练好的小模型，快速探索各种模型设计下的实际模型容量和效果。</p>
<p>比如试下扩大线性层的维度、扩大卷积数量、增加层数等，进行多次的实验，而耗费的资源和时间相比全部随机初始化可以节省很多。</p>
<h2 id="stacking">Stacking</h2>
<p>论文：《Efficient Training of BERT by Progressively Stacking》</p>
<p>收益：相比随机初始化，Bert模型整体训练速度可以有25%左右的提升。</p>
<p>实验模型：Bert</p>
<h3 id="tldr-1">TL;DR</h3>
<p>进入到transformer时代，《Efficient Training of BERT by Progressively
Stacking》提出渐进式堆叠（progressive
stacking）的方法，来提升训练的效率，整体训练速度可以有25%左右的提升。</p>
<p>简单来说，就是先训练一个n层的模型，然后把训练好的参数复制多次，填满一个2n层的模型（宽度相同），再进行继续预训练。以此类推，可以较快得到层数2n、4n、8n等的模型。</p>
<h3 id="观察">观察</h3>
<p>论文实验主要是针对Bert模型的。</p>
<p>首先，随机输入一个数据，观察Bert模型的注意力分布（下图），有两个发现：<br>
- 在同一个模型中，不同层的注意力分布都有两种模式：（1）local
attention，即关注本token周围的内容（2）CLS
token会吸引大量注意力，这现象我们之前在讲长文本的方案里也有其他工作提及，叫attention
sink<br>
- 在浅层模型和深层模型两个模型中，注意力的分布是类似的</p>
<img src="/dcb57672/stack_observation.png" class title="LLM高效训练">
<h3 id="堆叠">堆叠</h3>
<p>基于这些观察，可以推测深层模型和浅层模型的知识是可以共享的，因此可以考虑通过在浅层模型上堆叠注意力层，以此来低成本获得效果更好的大模型：</p>
<img src="/dcb57672/stack_intro.png" class title="LLM高效训练">
<p>复制时，浅层模型中的第i层（i &lt;
L）会复制到深层模型的L+i层中。比如原来有一个3层的模型[1,2,3]，通过stacking就得到一个6层模型[1,2,3,1,2,3]。</p>
<p>这样的复制不单可以进行一次，还可以多次进行，也就是“渐进式”（progressive
stacking）：比如先训练一个3层模型，通过stacking +
继续预训练得到6层的模型，然后再重复一次stacking +
继续预训练，获得12层的模型，以此类推。</p>
<h3 id="实验">实验</h3>
<p>那么这里我们就要问，通过stacking获得的模型是否可以获得和baseline模型（参数随机初始化）相同的效果？论文里做了实验，设置如下：<br>
- 先用3层模型训练50,000步<br>
- stack到6层之后训练70,000步<br>
- stack到12层之后训练280,000步</p>
<p>训练过程loss的下降如下图</p>
<img src="/dcb57672/stack_train.png" class title="LLM高效训练">
<p>其中：<br>
- baseline是随机初始化的模型<br>
- identity是把增加的层数初始化为identity
mapping的方法，和Net2Net中提出的深度扩展方法相同<br>
- identity + noise加入了随机噪音，是为了打破identity
mapping带来的对称性，帮助模型收敛</p>
<p>从训练的loss曲线上来看，stacking的趋势和速度都是比较好的。另外identity的效果看起来并不太好。</p>
<p>在下游测试集上，stacking也基本上能保持和bseline相同的performance：</p>
<img src="/dcb57672/stack_eval.png" class title="LLM高效训练">
<h3 id="分析">分析</h3>
<p>为了验证前面“深层模型和浅层模型attention分布相近”的假设，把各个方法下的attention分布可视化如下：</p>
<img src="/dcb57672/stack_pattern.png" class title="LLM高效训练">
<p>可以看到stacking的分布和baseline模型是比较接近的，并且stacking在训练前后变化不大，这说明深层和浅层参数确实共享了一些注意力模式，因此可以支持这样的stacking方式。</p>
<p>前面的实验从3层切换到6层，和6层切换到12层模型，分别是在50,000步和50,000步
+ 70,000步 =
120,000步，那么这个切换的时间有什么讲究，对训练的效率和结果有什么影响呢？</p>
<p>论文做了消融实验：先用3层模型训练100,000，然后后面每10,000步都实验切换到6层模型的效果；6层到12层也做相同的实验。实验的结果如下：</p>
<img src="/dcb57672/stack_ablation.png" class title="LLM高效训练">
<p>结论是，存在一个时间点，在这个时间点之前切换到更大模型，那么就能获得一定的训练速度提升，否则难以获得明显的速度提升。</p>
<p>这个结论也很符合直觉，因为只要我们训的时间足够长，容量更大的深层模型在后期的训练速度就会逐渐超过浅层模型。</p>
<p>这个时间点会随着模型深度增大而增大，上面左图的结果更能体现切换时间的影响。可以看到在5~15小时切换，训练效率的收益还是比较大的，而到了25小时之后再切换，基本上就和直接训练6层小模型的效率一样了。</p>
<h2 id="stacking-2.0">Stacking 2.0</h2>
<p>论文：《Progressively Stacking 2.0: A Multi-stage Layerwise Training
Method for BERT Training Speedup》</p>
<p>收益：相比随机初始化，Bert模型的预训练速度能提升110%。</p>
<p>实验模型：Bert</p>
<h3 id="tldr-2">TL;DR</h3>
<p>《PROGRESSIVELY STACKING 2.0: A MULTI-STAGE LAYERWISE TRAINING METHOD
FOR BERT TRAINING
SPEEDUP》在前面的stacking方法上进一步改进了，提出了multi-stage layerwise
training(MSLT)，让Bert的训练速度能够提升110%，而效果上没有明显的退化。</p>
<p>Stacking
2.0相比Stacking主要的改进在训练策略上，通过冻结已经在上一阶段训练好的参数，只训练新增加层的参数，进一步提升训练速度。</p>
<h3 id="分析-思路">分析 &amp; 思路</h3>
<p>前面的stacking方法存在几个问题：<br>
-
只有在层数比较少的几个训练阶段，训练速度会比较快（这也是stacking方法加速收益的主要阶段）；当层数逐渐变多了之后，速度也慢下来了<br>
- bottom
layer在整个过程中都在训练，但是通过可视化观察attention的分布，发现实际上到后期，bottom
layer的参数已经不怎么变了，换句话说，bottom
layer在早期几个阶段的训练中，就已经饱和了，因此后续阶段对bottom
layer的训练其实收益很少</p>
<p>基于以上的思路，提出了stacking
2.0的做法：依然是多阶段的训练，但是每次增加新的层之后，会把在上一阶段已经训练的层固定，而只训练新增加的层（和分类头）：</p>
<img src="/dcb57672/stack2_intro.png" class title="LLM高效训练">
<h3 id="实验-1">实验</h3>
<p>在实验中，对于12层Bert模型的训练，分成了5个阶段：<br>
- 训练3层的模型，训练步数为总步数的20%<br>
-
基于3层的模型，初始化6层的训练，固定embed层和1-3层，只训练4-6层，训练步数为总步数的20%<br>
-
基于6层的模型，初始化9层的训练，固定embed层和1-6层，只训练6-9层，训练步数为总步数的20%<br>
-
基于9层的模型，初始化12层的训练，固定embed层和1-9层，只训练9-12层，训练步数为总步数的20%<br>
- retraining：最后整个模型所有参数一起训练，训练步数为总步数的20%</p>
<p>每个阶段增加的3层，参数都是来自第一阶段训练的那3层。</p>
<p>24层模型的训练也是类似的，只是每个阶段新增和训练的层数变多了（3 -&gt;
6）。</p>
<p>在同样的训练step数下（总共1M步，MSLT每个stage训练200k步），MSLT和baseline的训练和测试结果如下：</p>
<img src="/dcb57672/stack2_result.png" class title="LLM高效训练">
<p>MSLT在Bert的base和large模型上的效果都和原生模型基本一致，而所需的训练时间则短很多。</p>
<p>上面的实验是步数相同，如果保持训练时间相同都是40小时，MSLT和baseline的对比如下：</p>
<img src="/dcb57672/stack2_sametime.png" class title="LLM高效训练">
<p>MSLT由于在各个stage只有一部分层更新参数，因此同样时间内可以跑更多的步数，效果也更好。</p>
<p>论文中还探索了最后一个步骤retraining，即全部参数一起训练的影响。</p>
<p>有无使用retraining phase的模型在下游任务上的效果如下：</p>
<img src="/dcb57672/stack2_retrain.png" class title="LLM高效训练">
<p>可以看到retraining还是有收益的。由于前面的训练让模型已经是near-optimal的状态了，因此retraining所需的步数并不用太多，大约是10%~20%的总训练步数就够了。</p>
<p>总体上，MSLT的方法在相同步数下所需的时间更短（因为每个阶训练的层数减少），而效果基本可以保持不变；而在相同时间下，MSLT可以跑更多的训练步数，效果会更好。</p>
<h2 id="bert2bert">Bert2BERT</h2>
<p>论文：《bert2BERT: Towards Reusable Pretrained Language Models》</p>
<p>收益：训练Bert-base和GPT-base分别可以节省45%和47%的计算成本</p>
<p>实验模型：Bert，GPT</p>
<h3 id="tldr-3">TL;DR</h3>
<p>Bert2BERT可以认为是Net2Net在transformer上的应用，是Net2Net的升级版。除了FPI在Bert上各个模块的应用，Bert2BERT还提出了效果更好的初始化方法advanced
knowledge
initialization（AKI），是FPI的改进版，能够更好地把小模型的知识迁移到大模型上。</p>
<p>另外Bert2BERT还提出了two-stage的训练方法：第一阶段训练中，每次会抽取部分层和分类头进行训练和更新，待收敛之后，再进入第二阶段的全参数训练。这样相比直接进行全参数训练的成本更低。</p>
<h3 id="aki">AKI</h3>
<p>Net2Net中主要的初始化方法是FPI，Bert2BERT除了实验FPI，还提出了一个效果更好的advanced
knowledge initialization（AKI）。</p>
<p>AKI的灵感来自之前论文的观察（《What does BERT learn about the
structure of language?》，《What does BERT look at? an analysis of
BERT’s attention》）：<br>
- transformer中相邻层有相近的功能<br>
- 因此可以把已经训练好的小模型的相邻层知识，用于大模型的初始化</p>
<p>这样做还有一个好处，就是可以打破FPI中的对称性，从而解放大模型的能力。和FPI不同，AKI并不保证初始化后的大模型输出结果和小模型一致。</p>
<p>AKI的大致做法如下图：</p>
<img src="/dcb57672/b2b_aki.png" class title="LLM高效训练">
<p>AKI输入维度的扩展和FPI相同，而输出维度的扩展则把从当前矩阵抽样，变成从下一层的参数中抽样。</p>
<p>针对transformer模型，不同模块的处理略有不同。</p>
<p>1、embedding</p>
<p>（1）FPI：在词表大小不变的情况下，只要增加hidden
size，增加的维度从已经训练好的维度随机采样并缩放得到（这个随机采样结果要和下一层输入维度增加的采样结果相同）。</p>
<p>（2）AKI：和FPI做法一样。</p>
<p>2、MHA</p>
<p>（1）FPI：MHA的扩展使用head-wise
expansion，也就是大模型增加的注意力头参数会整个从小模型已经训练的注意力头复制过来，然后每个头如果维度变化了，再按标准的FPI矩阵扩展方法来做。</p>
<p>（2）AKI：在注意力头数量的扩展上和FPI一样，但是在注意力头输出维度的扩展上，会采用AKI的方法
-- 把下一层的参数纳入到扩展的参数中。</p>
<p>3、FFN</p>
<p>（1）FPI：对两个线性层采用标准的FPI矩阵扩展。</p>
<p>（2）AKI：对两个线性层采用标准的AKI矩阵扩展。</p>
<p>4、LayerNorm</p>
<p>方法和FFN类似。</p>
<p>LayerNorm扩展维度之后，没法保证和原来的模型输出结果完全一致，不过实验上的结果表明LayerNorm扩展带来的gap并不大。</p>
<p>从小模型到大模型，除了宽度扩展，还需要做深度扩展。bert2BERT采用前面提到的stacking的做法来扩展深度：</p>
<img src="/dcb57672/b2b_init.png" class title="LLM高效训练">
<p>bert2BERT还使用了two-stage的训练策略：</p>
<img src="/dcb57672/b2b_train_algo.png" class title="LLM高效训练">
<p>在用小模型初始化大模型之后，第一阶段训练中，每一步会抽取部分层和分类头进行训练和更新（sub-model
training），待收敛之后，再进入第二阶段的全参数训练（full-model
training）。这样相比直接进行全参数训练的成本更低。</p>
<h3 id="实验-2">实验</h3>
<p>1、主实验</p>
<p>主实验用一个12层hidden size=512的模型，初始化12层hidden
size=768的Bert-base模型。</p>
<p>总共训练40个epch，其中第一个阶段训练5个epoch，全参数训练35个epoch。</p>
<p>下表给出bert2BERT以及其他对比方法，达到和原生Bert模型相同效果所需的计算量：</p>
<img src="/dcb57672/b2b_exp.png" class title="LLM高效训练">
<p>DirectCopy是把小模型参数直接复制到大模型，而剩余没有覆盖的部分则是保持随机初始化。</p>
<p>FPI和AKI是仅使用了相应的初始化手段，而bert2BERT是加上了two-stage训练方案的做法。</p>
<p>相比MSLT、StackBERT和DirectCopy，FPI、AKI和bert2BERT的训练效率提升比较明显。bert2BERT由于用two-stage的训练策略，因此提升最大。</p>
<p>2、消融实验：source模型大小</p>
<p>前面的实验中，大小模型的规模差异不大，那么如果用更小的模型的参数进行初始化，是否还能有好的效果？论文中尝试了使用6层hidden
size=512的模型（参数量约35M）训练Bert-base：</p>
<img src="/dcb57672/b2b_smaller_source.png" class title="LLM高效训练">
<p>和DirectCopy方法对比，在效果和效率上依然有一定的提升。不过这里和AKI、FPI和完全随机初始化没有进行对比，缺了点说服力。</p>
<p>3、消融实验：sub-model训练量</p>
<p>two-stage训练包括sub-model的训练（Eb个epoch），和full-model的训练（E-Eb个epoch），总的训练epoch数为E。</p>
<p>那么sub-model应该训练多少才比较好？下表给出了实验结果。注意从的训练epoch数为40。</p>
<img src="/dcb57672/b2b_epoch.png" class title="LLM高效训练">
<p>结果上看，Eb=5的时候是最好的。论文的解释是，一定的sub-model训练可以帮助模型快速收敛到一个稳定状态，但是过多的sub-model训练会让模型遗忘前面已经学习的知识，导致结果下降。</p>
<p>4、GPT</p>
<p>同样的实验用到GPT上，bert2BERT的方法也有47%的训练效率提升：</p>
<img src="/dcb57672/b2b_gpt.png" class title="LLM高效训练">
<h2 id="linear-growth-operatorligo">Linear Growth Operator（LiGO）</h2>
<p>论文：《LEARNING TO GROW PRETRAINED MODELS FOR EFFICIENT TRANSFORMER
TRAINING》</p>
<p>收益：相比随机初始化，大约可以节约50%的训练量。</p>
<p>实验模型：Bert</p>
<h3 id="tldr-4">TL;DR</h3>
<p>LiGO是《LEARNING TO GROW PRETRAINED MODELS FOR EFFICIENT TRANSFORMER
TRAINING》提出的做法，可以应用到transformer模型上。相比从零预训练，LiGO大约可以节约50%的训练量。</p>
<p>大致来说，LiGO的做法就是通过学习一个线性映射，把小模型的参数映射到大模型上。这个线性映射包括宽度算子和深度算子：</p>
<img src="/dcb57672/ligo_intro.png" class title="LLM高效训练">
<p>LiGO算子需要通过少量的训练来学习，论文里的使用是100步。</p>
<h2 id="masked-structural-growth-msg">Masked Structural Growth
(MSG)</h2>
<p>论文：《Masked Structural Growth for 2x Faster Language Model
Pre-training》</p>
<p>收益：训练效率提升为2.2倍，且下游效果更好</p>
<p>实验模型：Bert，GPT</p>
<h3 id="tldr-5">TL;DR</h3>
<p>MSG方法来自《Masked Structural Growth for 2x Faster Language Model
Pre-training》。主要思路是把小模型参数迁移到大模型之后，把未覆盖的，随机初始化参数先用一个mask屏蔽。在之后的训练中，逐渐降低mask的强度，让新增的参数逐渐参与训练。</p>
<h3 id="分析-1">分析</h3>
<p>从小模型增长到大模型（progressive
growth），主要有两个问题要解决：<br>
- 使用什么增长算子（growth
operator），也就是在什么维度增大模型，以及怎么增大<br>
- 使用什么样的growth schedule，也就是什么时候扩展模型</p>
<p>模型增大的维度共有4个：<br>
- layer number<br>
- hidden size<br>
- intermediate size<br>
- head num</p>
<p>目前的模型扩展方法存在问题：扩展的时候难以保证扩展前后的模型有严格的函数一致性（strict
function
preservation），也就是初始化后的大模型，输出结果和训练好的小模型没法保持一样。</p>
<p>现有一些模型扩展方法支持的扩展维度，以及一致性的情况如下表：</p>
<img src="/dcb57672/msg_other.png" class title="LLM高效训练">
<p>比如stacking这种方式支持模型在layer
number的维度进行扩展，但是并不能保持扩展之后大模型有严格的一致性。</p>
<p>而Net2Net的宽度扩展方法，虽然能保持函数一致性，但是存在对称性，不利于模型最终训练结果的提升。</p>
<h3 id="msg">MSG</h3>
<p>MSG的大致思路则是这样：<br>
-
比如对于全连接层，如果我们要扩展输出维度，那么就会引入新的没有训练过的参数，这些参数通过随机初始化得到；<br>
- 输出维度扩展完成之后，在训练时会引入一个针对新的输出的mask
vector，把随机初始化参数的输出置0，而原来已经训练过的维度不变，这样就能保证刚刚利用小模型权重初始化得到的大模型，其输出结果是和小模型严格一致的；<br>
-
随着训练的进行，新参数部分的mask会逐渐增大（最大为1，相当于没有mask），让新增加的参数可以逐步参与训练。</p>
<p>对于深度扩展，也是类似的做法，通过对新增加的残差层的结果进行mask，在初始化时保持输出的一致性，然后逐渐减少对新增加层的屏蔽，让参数参与训练。</p>
<p>上面是growth operator的思路，还有另外一个要解决的问题就是growth
schedule的选择。</p>
<p>论文中定义了一个pre-training rate γ 来表征学习速度：</p>
<p><span class="math display">\[\gamma\triangleq\frac{\Delta\mathrm{loss}}{\Delta\mathrm{t}}=\frac{\Delta\mathrm{loss}}{\Delta\mathrm{step}}\cdot\frac{\Delta\mathrm{step}}{\Delta\mathrm{t}}\triangleq\alpha\cdot\tau
\]</span></p>
<p>γ 随着训练进行会逐渐减小，如果 γ 太小了就要扩展模型结构了。</p>
<p>论文通过网格搜索探索了各个增长维度对 γ 的影响，得到一些结论：<br>
- 对于Bert，较大的layer num会严重影响 τ，且对 α
的提升不大，因此应该放在后期来做；<br>
- 较大的 hidden size能为浅层模型带来更好的
γ，因此应该从较大的值开始；<br>
- intermediate size对 α 和 τ 有一定的平衡作用，对 γ 的影响较小；<br>
- 较小的head number对深层模型的 γ 更有利。</p>
<img src="/dcb57672/msg_hparam.png" class title="LLM高效训练">
<p>基于以上这些发现，为Bert-base和large模型构建了训练时间表：</p>
<img src="/dcb57672/schedule.png" class title="LLM高效训练">
<p>根据这个schedule进行训练，相比从头训练，能够获得1.4~2.2倍不等的速度提升。</p>
<h3 id="实验-3">实验</h3>
<p>Bert和GPT模型的实验对比如下：</p>
<img src="/dcb57672/msg_result.png" class title="LLM高效训练">
<p>另外关于mask从0增长到1需要多少时间，论文做了实验。结论是这个增长的过程&gt;=500，就足够保证模型的稳定收敛了。当然目前的预训练step数很多，因此这个值设大一点也没有问题，比如5k。</p>
<img src="/dcb57672/msg_mask_step.png" class title="LLM高效训练">
<h2 id="flm-101btele-flm-1t">FLM-101B，Tele-FLM-1T</h2>
<p>论文：《FLM-101B: An Open LLM and How to Train It with $100K
Budget》，《52B to 1T: Lessons Learned via Tele-FLM Series》</p>
<p>收益：10w美元的成本训练出101B模型，较低成本训出1T参数的模型</p>
<p>实验模型：FLM-101B，Tele-FLM-1T</p>
<h3 id="flm-101b">FLM-101B</h3>
<p>《FLM-101B: An Open LLM and How to Train It with $100K
Budget》是MSG的应用，这篇论文的亮点是仅用10w美元就训出可用的101B参数量的模型，总的训练量为310B
token。</p>
<p>首先直观感受一下不同的growth
schedule对训练成本的影响。下图(b)(c)(d)给出了三种典型的growth
schedule，横轴为训练的总token数，纵轴为模型参数。由于训练的总计算量和模型参数以及训练数据量成正比，可以认为全曲线和横轴围成的面积就是所需的计算量成本。直观上来说，模型参数量在前期增长慢，在后期增长快，这样的策略是比较节省计算成本的。</p>
<img src="/dcb57672/flm101_growth_schedule.png" class title="LLM高效训练">
<p>论文中串行地训练了3个规模的模型，从小到大：16B、51B和101B。可以认为这是训练101B的三个阶段。下一个阶段的模型由上一个阶段的模型通过MSG的方法扩展而来。各个阶段的训练量和设置如下：</p>
<img src="/dcb57672/flm101_stage.png" class title="LLM高效训练">
<p>这个训练策略相比直接从零训练101B模型节省了72%的时间，或者说速度提升到了3.56倍。</p>
<p>文中还训练稳定性做了一些探索。大规模的模型，特别是千亿级别的模型，在训练的时候很容易出现不稳定的情况。训练的不稳定令训练成本的评估变得困难，因为可能需要重新调参，回退模型checkpoint重新训练等。</p>
<p>那么为了让训练稳定，需要找到一个比较好的超参。论文使用了μP（《Tuning
large neural networks via zero-shot hyperparameter
transfer.》，《Research without re-search: Maximal update
parametrization yields accurate loss prediction across
scales》）的方法，在100M的proxy model对learning rate、initialization
standard deviation和softmax temperature in the output
layer这三个超参进行搜索，然后再把得到的最佳组合通过μP的策略，应用在16B以及更大的模型上。</p>
<p>文中还提到，在μP策略下，在其他参数都相同的情况下，更宽的模型总是比窄的模型loss更低。那么只要宅模型能够收敛，那么宽的模型一定能够收敛。</p>
<p>另外，bf16在靠近0的时候的精度比fp16的精度更高一些，更适合配合μP使用。</p>
<p>介绍一下μP：μP（maximum update
parameterization）主要是通过对初始化和模型forward值进行一定的缩放和配置，使得transformer模型可以在不同的规模下，保证在同样超参下的训练结果。比如如果我们要训练一个100B的模型，但是100B模型的网格搜索调参成本太高，我们可以在μP的规则下初始化一个100M的模型，然后再100M的模型上调整batch
size和learning
rate等参数，得到的最佳超参组合可以保证在100B模型上也是最佳的超参组合。</p>
<p>dense模型的具体μP做法可以参考《Tensor Programs V: Tuning Large Neural
Networks via Zero-Shot Hyperparameter
Transfer》，而MoE模型的则可以参考《Sparse maximal update
parameterization: A holistic approach to sparse training
dynamics》。</p>
<p>最终三个阶段模型训练的loss如下：</p>
<img src="/dcb57672/flm101_loss.png" class title="LLM高效训练">
<p>而101B模型和Llama等模型的benchmark评测对比如下：</p>
<img src="/dcb57672/flm101_eval.png" class title="LLM高效训练">
<p>有一说一，从效果上看，101B还Llama同规模甚至低一两级规模的模型比并不算好，不过胜在成本比较低。</p>
<h3 id="tele-flm-1t">Tele-FLM-1T</h3>
<p>《52B to 1T: Lessons Learned via Tele-FLM
Series》是FLM-101B的一个后续工作，进一步把模型规模通过MSG的训练方式提升到了1T参数。各个阶段的训练设置如下：</p>
<img src="/dcb57672/flm1t_train.png" class title="LLM高效训练">
<p>可以看到训练的token数相比FLM-101B更多了。不过论文没有给出FLM-1T模型的详细效果。</p>
<h2 id="apollo">Apollo</h2>
<p>论文：《Preparing Lessons for Progressive Training on Language
Models》</p>
<p>收益：最大约41.6%的加速</p>
<p>实验模型：Bert，GPT</p>
<h3 id="tldr-6">TL;DR</h3>
<p>Apollo是《Preparing Lessons for Progressive Training on Language
Models》提出的训练方法。参考知乎上论文原作者的解释<a href="https://zhuanlan.zhihu.com/p/678392914">AAAI2024: Preparing
Lessons for Progressive Training on Language Models
https://zhuanlan.zhihu.com/p/678392914</a>，概括一下做法。</p>
<p>首先前面提到的stacking的方法，会把3层的模型[1,2,3]堆叠成[1,2,3,1,2,3]这样的6层模型，但是这样对于堆叠的层1，其输入空间变化比较大（从emb层变成原来的输出层），这样会带来训练的不稳定。因此这篇文章把stacking改成interpolation。</p>
<img src="/dcb57672/apollo_interpolation.png" class title="LLM高效训练">
<p>实验结果表明interpolation的loss更小一些，并且gradient
norm也比stacking要小一些，而小的gradient
norm在训练时会更稳定一些，更少出现loss spike等情况：</p>
<img src="/dcb57672/apollo_init.png" class title="LLM高效训练">
<p>整个训练过程也是分为多阶段的，如下图所示。在每个阶段，要训练的模型有N(s)层，为了让模型提前学到后续会被怎么扩展，每个训练step会通过interpolation构建一个L(t)层的模型，N(s)&lt;=L(t)。每步基于构建出的L(t)层模型进行训练。</p>
<img src="/dcb57672/apollo_intro.png" class title="LLM高效训练">
<p>L(t)的值会通过一定的随机抽样策略来获得。抽样策略会更倾向于给出小的值，这样会让训练成本更低。</p>
<p>整个算法具体如下：</p>
<img src="/dcb57672/apollo_algo.png" class title="LLM高效训练">
<h2 id="llama-pro">Llama Pro</h2>
<p>论文：《LLaMA PRO: Progressive LLaMA with Block Expansion》</p>
<p>收益：基于LLaMA2-7B，在不影响原能力的情况下，用80B的数据，提升数学和代码能力，得到LLaMA-PRO-8B</p>
<p>实验模型：LLaMA2</p>
<h3 id="方法">方法</h3>
<p>一般来说，如果想要提升预训练模型在某些领域的能力，可以使用对应的数据进行继续预训练。但是由于灾难性以往，这样的继续预训练对其他没有强化的能力会有明显的损害。比如基于LLaMA2-7B训练的codeLLaMA-7B，除了代码能力有提升，其他都下降了。看下面雷达图的橙线和蓝线对比。</p>
<p>在已经预训练好的模型的基础上，通过少量增加参数 &amp;
轻量级的继续预训练，提升某些领域上的效果的同时，避免灾难性以往，保持在通用能力上不会有明显损失。</p>
<p>论文在LLaMA2-7B模型上，把模型参数扩大到8B，从而在数学和代码能力上获得明显提升，而其他能力基本不变，看下图黄线和橙线：</p>
<img src="/dcb57672/llama_pro_intro.png" class title="LLM高效训练">
<p>第一个问题是怎么搞扩展参数。LLAMA PRO只在模型层维度上进行扩展。</p>
<p>LLaMA2-7B本身有32层，这32层会被分成8组，每组是连续的4层，即[1,2,3,4],[5,6,7,8],...,[29,30,31,32]。之后在每组内进行扩展，使用每组最后一层的参数进行Identity
Copy，扩展成[1,2,3,4,4'],[5,6,7,8,8'],...,[29,30,31,32,32']。新扩展的层通过把attention层和FFN层的输出linear置0，保持在初始化的时候不影响原模型的输出。</p>
<img src="/dcb57672/llama_pro_copy.png" class title="LLM高效训练">
<p>继续预训练的时候，原模型的层都会被冻结，不参与训练，只有新扩展的参数会进行训练，这样可以保持原模型已有的能力不会被破坏。</p>
<h3 id="实验-4">实验</h3>
<p>论文里训练了80B的数学和代码数据，得到LLaMA-PRO-8B。</p>
<img src="/dcb57672/llama_pro_data.png" class title="LLM高效训练">
<p>LLaMA-PRO-8B和其他模型对比如下</p>
<img src="/dcb57672/llama_pro_perf.png" class title="LLM高效训练">
<p>从结果上看还是，LLaMA-PRO-8B还是很有前途的，原模型的能力基本上都没有受损，而数学和代码能力有很大的提升。</p>
<p>1、消融实验：其他reference model</p>
<img src="/dcb57672/llama_pro_mistral.png" class title="LLM高效训练">
<p>使用Mistral-7B作为reference
model，训练Mistral-Pro，效果同样不错。</p>
<p>2、消融实验：增加的参数量</p>
<p>对比总共增加1层、2层、4层、8层、16层和32层参数的训练结果：</p>
<img src="/dcb57672/llama_pro_add_block.png" class title="LLM高效训练">
<p>整体上，随着参数量增多，效果是在提升的。不过8层的性价比是比较高的，8层-&gt;16层的提升就很小了，而增加32层的效果还出现略微下降。</p>
<h1 id="从大到小">从大到小</h1>
<h2 id="sheared-llama">Sheared LLaMA</h2>
<p>论文：《Sheared LLaMA: Accelerating Language Model Pre-training via
Structured Pruning》</p>
<p>收益：和随机初始化相比，Sheared-LLaMA-2.7B只需要约3%的训练量就能达到相同的训练效果。</p>
<p>实验模型：LLaMA</p>
<h3 id="tldr-7">TL;DR</h3>
<p>Sheared LLaMA通过对Llama2-7B进行裁剪（1.3B和2.7B） +
少量继续预训练，获得效果超越其他从零训练的同规模模型模型。</p>
<p>Sheared-LLaMA-2.7B模型在下游任务上的平均得分随训练量变化，以及和其他相近规模模型的规避如下：</p>
<img src="/dcb57672/sheared_llama_intro.png" class title="LLM高效训练">
<p>和从零训练的模型相比，Sheared-LLaMA-2.7B只需要约3%的训练量就能达到相同的训练效果。</p>
<h3 id="方法-1">方法</h3>
<p>Sheared LLaMA的方案包括2个操作：<br>
- 裁剪：targeted structured pruning<br>
- 继续预训练：dynamic batch loading</p>
<h4 id="裁剪targeted-structured-pruning">裁剪：targeted structured
pruning</h4>
<p>以往也有通过模型裁剪，从大模型获取小模型的方法，比如CoFiPruning（《Structured
pruning learns compact and accurate models》）。</p>
<p>这些裁剪方法主要从保持最佳效果的目的出发，因此模型裁剪结果可能出现每层异构的情况，比如第一层保留了原模型16个注意力头中的8个，第二层保留了原模型16个注意力头中的10个。这样的异构在推理时很可能不被现有的加速框架支持，或者出现推理资源分配的不合理，导致裁剪模型的实际推理效率相比原模型提升不大。</p>
<p>除了异构问题之外，还有可能出现模型深度和宽度等参数组合不够合理的情况。现在各个规模的主流开源模型结构可以说都是经过大量验证和对比保留下来的，是目前能搜索到的最佳设置。如果裁剪模型的结构和这些成熟的结构差别很大，那么很可能就得不到最佳效果。比如主流的7B模型层数都在32层左右，而裁剪得到的模型是16层，就可能效果不好。</p>
<p>targeted structured pruning就是为了解决裁剪模型结构的问题。</p>
<p>模型裁剪会在4个维度进行：<br>
- 模型层数<br>
- hidden size<br>
- 注意力头数量<br>
- intermediate size</p>
<p>通过在这些维度设定我们想要的数值，可以限制裁剪后的模型结构，保证推理时的效率。</p>
<p>裁剪通过对原模型的所有参数使用一个可学习的mask来实现：</p>
<img src="/dcb57672/sheared_llama_mask.png" class title="LLM高效训练">
<p>理想状态下mask应该只输出0或者1，告诉我们哪些参数是要的，哪些参数是不要的，但是由于这个mask需要通过训练来获取，所以需要时连续的。因此这里使用了《Learning
sparse neural networks through l_0 regularization》中引入的hard concrete
distribution来参数化mask。hard concrete
distribution是一个大部分概率集中在靠近0或者1的分布：</p>
<img src="/dcb57672/hard_concrete_dist.png" class title="LLM高效训练">
<p>训练mask的时候，把各个维度裁剪的损失，和模型预训练的language modeling
loss加在一起，获得最终训练目标：</p>
<p><span class="math display">\[\mathcal{L}_\text{prune}{(\theta,z,\lambda,\phi)}=\mathcal{L}(\theta,z)+\sum_{j=1}^{L_S}\tilde{\mathcal{L}}_j^\text{head}+\sum_{j=1}^{L_S}\tilde{\mathcal{L}}_j^\text{int}+\tilde{\mathcal{L}}^\text{layer}+\tilde{\mathcal{L}}^\text{hidden}\]</span></p>
<p>mask的训练相对正常的预训练会比较慢（roughly 5x
slower），不过好在所需的训练量并不大。论文里使用了不超过1B的token训练mask。</p>
<h4 id="继续预训练dynamic-batch-loading">继续预训练：dynamic batch
loading</h4>
<p>对大模型进行参数裁剪，得到小模型之后，需要进行一定量的继续预训练来恢复损失的效果。</p>
<p>研究人员发现，从零训练得到的模型，和通过裁剪得到的模型，在同样的训练数据上训练，各个领域（比如Github、Book、Wiki、C4等）的loss变化情况有所不同。</p>
<p>具体来说，通过deepmind的scaling function（scaling
law）对Llama2系列的模型进行拟合，可以预测特定规模下各个领域的loss。通过这个方法，可以预测从零训练的1.3B/2.7B模型在各个领域应该达到的loss水平。下图蓝色图（左边）是通过scaling
function预测的loss，和实际从零预训练的模型的loss的对比：</p>
<img src="/dcb57672/loss_diff.png" class title="LLM高效训练">
<p>右边是对裁剪后的模型用在相同数据进行继续预训练之后，在各个领域上的loss和预测loss的对比。可以看到在相同的训练数据下，从零训练的模型和裁剪+继续预训练的模型在某些领域的loss有较大的差异，比如C4和Github。</p>
<p>按上图分析，同规模下，裁剪得到的模型，相比从零预训练的模型，更容易保留low
entropy和相对封闭的领域的知识，比如Github，而对high-entropy以及更加开放的领域，则保留的知识少一些。</p>
<p>这就说明按照原预训练数据的配比进行继续预训练，可能不是最佳的选择。于是参考《Doremi:
Optimizing data mixtures speeds up language model
pretraining》，提出dynamic batch loading的方法。</p>
<p>简单来说就是在没m步训练之后，都会测一下模型在各个领域验证集上的loss情况，然后根据各个领域的loss，调整后面m步的数据配比。调整配比的方法和《Doremi》一样：</p>
<p><span class="math display">\[\alpha_t=w_{t-m}\cdot\exp(\Delta_t);\quad
w_t=\frac{\alpha_t}{\sum_i\alpha_t[i]}\]</span></p>
<p><span class="math display">\[\Delta_t(D_i)=\ell_\text{ref}[i]\mathrm-\ell_t[i]\]</span></p>
<p>i表示domain index。</p>
<p>Sheared LLaMA在模型裁剪和继续预训练阶段都使用了dynamic batch
loading。</p>
<p>两个阶段所用的初始分布：</p>
<blockquote>
<p>For pruning, we use the original pre-training data’s domain weights
as w0. For continued pre-training, we use the final weights from the
pruning stage as w0.</p>
</blockquote>
<p>这里调整配比时还会用到一个reference
loss。文中给出了两种做法，一个是直接使用scaling
function所预测的，一个是使用裁剪前的原模型的loss。实践上，两种都可以行，使用scaling
function所预测的loss在下游任务上的效果会略好一点。</p>
<h4 id="实验-5">实验</h4>
<p>Sheared
LLaMA用LLaMA2-7B裁剪了一个1.3B的模型，和一个2.7B的模型。用于对比的模型，和对应的训练数据如下：</p>
<img src="/dcb57672/sheared_llama_baseline.png" class title="LLM高效训练">
<p>使用了0.4B的数据进行模型裁剪，并继续预训练了50B模型后，Sheared-LLaMA-1.3B和Sheared-LLaMA-2.7B效果如下：</p>
<img src="/dcb57672/sheared_llama_perf.png" class title="LLM高效训练">
<p>结果上看，相比规模的模型，Sheared
LLaMA在评测benchmark上是有一点优势的。</p>
<p>相比使用原数据分布，dynamic batch
loading在整个训练过程都展现出更好的效果：</p>
<img src="/dcb57672/sheared_llama_dynamic.png" class title="LLM高效训练">
<p>和其他模型裁剪方案相比，Sheared
LLaMA更合理的结构使得推理效率更高：</p>
<img src="/dcb57672/sheared_llama_speed.png" class title="LLM高效训练">
<p>研究人员对训练资源在模型裁剪和继续预训练的分配比例也做了一下消融实验。保持两个阶段的总训练量为4.8B，对比裁剪阶段使用0.2B、0.4B、0.8B和1.6B时的最终效果：</p>
<img src="/dcb57672/sheared_llama_train_token.png" class title="LLM高效训练">
<p>结果上看，是裁剪阶段的token效率更高，裁剪阶段训练的token越多效果越好。不过最终还是使用了0.4B也不是更高的比例，原因是裁剪阶段的训练速度更慢。不过个人觉得这不是很成立，毕竟裁剪才几B的数据量，哪怕训练慢5倍，总量相差也不大，可能还有别的原因这里没有说吧。</p>
<h2 id="weight-subcloning">Weight Subcloning</h2>
<p>论文：《Weight Subcloning: Direct Initialization of Transformers
Using Larger Pretrained Ones》，苹果</p>
<p>收益：相比随机初始化，预训练速度提升至4倍左右</p>
<p>实验模型：VIT，GPT2</p>
<h3 id="tldr-8">TL;DR</h3>
<p>weight
subcloning的目的是通过利用已有的预训练模型，通过一定的手段选择重要度比较高的神经元，用于初始化一个小模型，从而加速小模型的训练。论文中把已训练好的模型称为parent
model，而小模型称为destination model。</p>
<p>weight subcloning方法相比随机初始化，预训练速度能提升4倍左右：</p>
<img src="/dcb57672/subclone_intro.png" class title="LLM高效训练">
<h3 id="分析-2">分析</h3>
<p>我们知道transformer模型中用了残差连接，对于输入x，一个transformer
block的输出是y=x+f(x)，这个transformer block拟合的是非残差部分f(x)。</p>
<p>使用残差连接的一个原因：随着模型层数的加深，新增加的层的输出y和x会很相近，通过增加恒等路径，新增加的层可以专注在拟合差值上。也就是靠后的层的f(x)相比这一层的输入x在数值上应该小很多。</p>
<p>下图中，第一行的两个子图给出了VIT模型和GPT-2模型分别在图像分类数据和语言建模数据上，模型各层的<span class="math inline">\(\frac{||f(x)||}{||x+f(x)||}\)</span>值，可以看到最前面和最后面的层的值会大一些，而中间层的值相对小一些，特别是在GPT模型上。</p>
<img src="/dcb57672/subclone_residule.png" class title="LLM高效训练">
<p>既然模型中间层对输出值的影响很小，那也就是说移除一个中间层，或者额外复制一个中间层在模型中，对输出值的影响也很小。上图的第二行就给出了增加或者减少某一个中间层对整个模型validation
loss的影响。</p>
<p>可以看到在GPT2上，除了第一层和最后一层外，移除或者增加一个中间层对最终的loss几乎没有任何影响。</p>
<p>这些对模型效果影响较小的中间层就是我们后面对模型结构进行裁剪或者复制的理想候选。</p>
<p>既然输出值主要来自前面的层，那么各个神经元的重要性差异也是在early
transformer
block建立的。换句话说，如果某个神经元在第i层的重要性比较高，那么在第i+n层中，同样位置的神经元应该也有相应的重要度。</p>
<p>下图画出模型第一层和第二层神经元的输出值。</p>
<img src="/dcb57672/subclone_neuron.png" class title="LLM高效训练">
<p>可以看到有少量的神经元的值要远超其他神经元，且一二层对应神经元的输出值有很强的相关关系，正如上面分析的那样。</p>
<h3 id="方法-2">方法</h3>
<p>第一步要获得模型各个维度上的重要neuron。</p>
<p>1、Neuron importance ordering in linear layers</p>
<p>所用数据分别为0.5%的ImageNet数据，和0.003%的Pile数据。</p>
<p>通过这些数据得到各个线性层各个neuron的输出值的绝对值，再取平均。这个均值就是各个neuron的重要性得分。下图给出了模型某层中重要性得分前50个neuron：</p>
<img src="/dcb57672/subclone_linear_importance.png" class title="LLM高效训练">
<p>2、Importance ordering in attention layers</p>
<p>论文中以attention
head为单位计算重要性得分。方法和linear层类似，只是最终算的是整个attention
head输出向量的magnitude。</p>
<img src="/dcb57672/subclone_head_order.png" class title="LLM高效训练">
<p>3、Importance ordering in residual blocks</p>
<p>由于模型存在残差链接，所以要保证相加的两部分所用的神经元是一致的，如下图蓝色部分</p>
<img src="/dcb57672/subclone_order_res.png" class title="LLM高效训练">
<p>而没有用于残差链接的输出，即上图的黄色部分，则是独立的，可以单独选择神经元而不受其他层的影响。</p>
<p>既然蓝色部分的神经元要一一对应，那么计算重要性的时候就要把所有层放在一起考虑，选出global重要度最高的神经元（的位置）。</p>
<p>4、Weight scaling</p>
<p>完成神经元重要度的选择之后，我们就可以用来初始化destination
model。需要注意的是，把权重值复制到destination
model之后，位置保证输出值standard
deviation不变，需要对参数进行缩放，假设D和d分别是parent
model和destination model的hidden size，那么缩放的系数就是 <span class="math inline">\(\sqrt\frac{D}{d}\)</span> 。</p>
<p>layer norm、batch norm、bias部分的参数则不需要处理。</p>
<h3 id="实验-6">实验</h3>
<p>论文用VIT和GPT2做实验，并分别增加一个随机初始化的模型作为对比。</p>
<img src="/dcb57672/subclone_exp_model.png" class title="LLM高效训练">
<p>这几个模型随训练量增加，效果的变化如下：</p>
<img src="/dcb57672/subclone_exp_train.png" class title="LLM高效训练">
<p>通过weight subcloning初始化的模型收敛速度要快很多。</p>
<p>1、消融实验：Effect of learning rate and weight decay</p>
<p>weight
subcloning得到的模型，需要用小一点的lr才能收敛，太大的lr反而可能会造成灾难性以往，这和随机初始化的模型不同。另外由于模型权重已经在parent
model中训练过，经历过了weight
decay和其他形式的regularization，因此在继续预训练时不需要太大的weight
decay值。不同lr和weight decay下，destination model的收敛情况：</p>
<img src="/dcb57672/subclone_lr.png" class title="LLM高效训练">
<p>2、消融实验：Effect of weight scaling</p>
<p>如果不对weight
subcloning之后的模型权重做scaling，模型收敛会慢一下，看下图橙色和绿色线：</p>
<img src="/dcb57672/subclone_scaling.png" class title="LLM高效训练">
<p>3、消融实验：Effect of parent Model Architecture</p>
<p>前面的实验看到weight
subcloning+继续预训练的效果不错，那么一个自然的想法是，如果使用更强的模型作为parent
model，那么效果是不是能进一步提升？收敛速度的收益会不会更大？</p>
<p>前面的实验使用GPT2-L作为parent
model训练GPT2-M，这里实验一下用GPT2-XL作为parent
model，同样训练GPT2-M，收敛情况如下</p>
<img src="/dcb57672/subclone_larger_parent.png" class title="LLM高效训练">
<p>更大的parent
model并没有得到更好的收敛效果。这说明和模型蒸馏类似，如果parent
model和destination
model之间的gap比较大，有可能小模型没法很好地继承大模型的能力。</p>
<p>4、消融实验：Effect of neuron reordering</p>
<p>如果不对nueron做reorder，而是随机挑选，进行weight
subcloning，那么效果会大打折扣：</p>
<img src="/dcb57672/subclone_exp_reorder.png" class title="LLM高效训练">
<h2 id="inheritune">Inheritune</h2>
<p>论文：《Pre-training Small Base LMs with Fewer Tokens》</p>
<p>收益：仅用1B的数据，10B左右的训练量，就在小模型上保持了大模型90%的能力</p>
<p>实验模型：GPT</p>
<h3 id="方法-3">方法</h3>
<p>Inheritune是《Pre-training Small Base LMs with Fewer
Tokens》提出的方法，目标是低成本地从已经训练好的大模型，获取一个效果可用的小模型。</p>
<p>包含两步：<br>
- 用大模型初始化小模型<br>
- 对小模型进行轻量级继续预训练</p>
<p>这里初始化小模型的方法比较简单，就是直接复制大模型的embedding层，前n个transformer层，以及最后的分类头。这样的初始化保留了原模型前n层所有学到的知识，但是缺点是小模型参数在各个维度（hidden
size、注意力头数、模型层数等）可能不是最佳的。</p>
<p>Inheritune在继续预训练时所用的数据是原模型训练所用的数据的子集。这样论文没有对数据分布做其他的探索，这个条件其实有点难达成，毕竟大部分所谓的开源模型并不会给出真实使用的训练数据。</p>
<p>Inheritune的算法描述如下：</p>
<img src="/dcb57672/inheritune_algo.png" class title="LLM高效训练">
<h3 id="实验-7">实验</h3>
<p>论文用26层的OpenLLaMA-3B作为reference模型，初始化了13层的1.5B。</p>
<p>并在1B的训练数据上训练了8个epoch，batch size为121k
token，总的update数大约为66115步。</p>
<p>和同规模下的其他模型的效果对比：</p>
<img src="/dcb57672/inheritune_perf.png" class title="LLM高效训练">
<p>这里黑体的表示小模型保留了reference model 90%+的效果。</p>
<p>把训练所用数据和效果画出来是这样的：</p>
<img src="/dcb57672/inheritune_perf_token.png" class title="LLM高效训练">
<p>不过这里用的是“数据量”，而不是“训练token数”，感觉有点不太公平，毕竟Inheritune训了8个epoch。</p>
<p>1、消融实验：参数复用层数</p>
<p>前面是把26层的OpenLLaMA-3B直接取了13层。那么取的层数对效果影响如何？论文了做了使用，分别对比了n=<span class="math inline">\(\{4,6,8,10,13,16,18,20\}\)</span>
时，在MMLU上的效果：</p>
<img src="/dcb57672/inheritune_n.png" class title="LLM高效训练">
<p>大致的趋势是往上的。不过这里用的指标是MMLU，而小模型在MMLU这种困难任务上的波动可能是比较大的，个人觉得这里这个指标换一个会更好。</p>
<p>2、不同的reference model &amp; 数据</p>
<p>如果使用不同的reference
model，以及更多的数据，Inheritune效果会怎么样呢？</p>
<img src="/dcb57672/inheritune_larger_ref.png" class title="LLM高效训练">
<p>相比1B的数据，使用50B的数据在效果上略好一些。而使用更大的reference
model提升则更加明显。</p>
<p>3、训练数据重复次数</p>
<p>《Scaling data-constrained language
models》中指出，（足够大的）预训练数据训练4个epoch以内不会因为重复而带来明显的负面影响。那么对于Inheritune可以重复多少次呢？</p>
<img src="/dcb57672/inheritune_repeat.png" class title="LLM高效训练">
<p>从上表的结果上来看，1B的数据重复20次仍然有收益。</p>
<h1 id="从dense到sparse">从dense到sparse</h1>
<h2 id="sparse-upcycling">Sparse Upcycling</h2>
<p>论文：《Sparse Upcycling: Training Mixture-of-Experts from Dense
Checkpoints》</p>
<p>收益：相同的训练时间和计算成本下，MoE模型的提升更大</p>
<p>实验模型：transformer</p>
<h3 id="背景">背景</h3>
<p>目前已经有很多优秀的dense大模型，那么要通过MoE获得更强的模型，用已有的dense模型进行初始化是一个自然的想法。Google的sparse
upcycling对此做了一些实验，由于实验是在2022年做的，模型用的是T5系列语言模型和Vision
Transformer系列视觉模型。</p>
<p>文中给出两个适合使用sparse upcycling的场景：<br>
- 已有dense模型，想在有限的计算资源下提升模型效果。<br>
-
要训一个模型，不知道dense模型和MoE哪个会有更好的效果（虽然通常来说MoE更好，但是训练难度和结果不确定也更大），那么就可以先训练一个dense模型保底，然后再在dense模型的基础上扩展成MoE结构继续优化。</p>
<p>下面具体看下一些实验细节。</p>
<h3 id="设置">设置</h3>
<p>对于transformer模型，sparse upcycling的操作如下图：</p>
<img src="/dcb57672/upcycling_intro.png" class title="upcycling">
<p>除了原模型的MLP层替换成MoE层外，其他组件包括layernorm、attention都直接从原dense模型copy到MoE模型。</p>
<p>实验上，一些具体的基础设置如下：<br>
- 在原模型基础上，每2层替换一个MoE层，从第二层开始替换<br>
- MoE模型的总层数的dense模型层数相同<br>
-
每个MoE层专家数为32个；虽然使用更多的专家不会明显增大训练的FLOPS，但是更多的专家会带来larger
initial quality drop relative to baseline dense
model，而需要更多的计算资源来恢复这个quality
drop；后续会有实验探索expert数量的影响<br>
- 每个expert都用原模型的MLP层参数初始化<br>
- router使用standard deviation=0.02的zero-mean normal
distribution随机初始化<br>
- 在encoder使用expert choice routing，基础的设置是capacity factor C =
2，后面也做了关于capacity factor的消融实验<br>
- 在decoder使用token choice routing（top-k
routing），k=2，并加上auxiliary
loss帮助负载均衡，权重为0.01；在decoder使用top-k routing的原因是"to
avoid train time (full batch teacher forcing) versus inference time
(single token auto-regressive decoding) discrepancies"（和expert choice
routing的设计相关）</p>
<p>MoE模型训练时使用和dense模型一致的batch size、learning rate
schedule和weight decay等。</p>
<p>其中learning rate schedule用的是inverse square root learning rate
schedule，因此MoE的训练可以接着dense模型的schedule接着进行。</p>
<p>实验中所用的一些模型参数如下表</p>
<img src="/dcb57672/upcycling_models.png" class title="模型">
<h3 id="实验-8">实验</h3>
<h4 id="core-results">core results</h4>
<p>1、dense模型继续训练 vs upcycling</p>
<p>随着训练量的增加，upcycling相比dense模型继续预训练的优势逐渐扩大，如下图所示</p>
<img src="/dcb57672/upcycling_1.png" class title="实验">
<p>2、下游任务模型效果</p>
<p>上面对比的是预训练模型。这些预训练模型经过微调后的效果对比如下。</p>
<img src="/dcb57672/upcycling_2.png" class title="实验">
<p>相比预训练模型，微调模型表现出相对更大的震荡，不过大致趋势还是可以看出MoE模型更有优势。</p>
<p>3、MoE from scratch vs upcycling</p>
<p>从零开始训练的MoE和upcycling方法的对比如下</p>
<img src="/dcb57672/upcycling_3.png" class title="实验">
<ul>
<li>从零开始预训练的MoE模型效果提升得更快，这可能得益于多样化的专家初始化和更大的lr。<br>
</li>
<li>只要给的计算资源足够多，从零开始训练的模型最终会赶上甚至超过upcycling的模型。<br>
</li>
<li>在有限的训练资源下，upcycling的训练效率更高，从零开始训练的模型大约需要相当于原dense模型1.2倍的训练资源才能达到upcycling模型的效果。如果现在的训练资源&lt;=训练dense模型的资源，那么sparse
upcycling是更划算的。</li>
</ul>
<p>4、sparse upcycling vs dense upcycling</p>
<p>对比《Scaling language models: Methods, analysis &amp; insights from
training gopher》中的depth tiling（dense upcycling） 和 sparse
upcycling的预训练效果，结果当然是sparse
upcycling效率更高点，如下图所示</p>
<img src="/dcb57672/upcycling_4.png" class title="实验">
<p>（不过这里没有提及depth tiling之后的模型规模）</p>
<h4 id="消融实验">消融实验</h4>
<p>1、Amount of dense pretraining</p>
<p>upcycling的效果可能受用于初始化的dense模型的收敛情况影响，因此取了不同step的dense模型checkpoint作为upcycling的初始化，并且都继续训练了200k个step，结果如下图</p>
<img src="/dcb57672/upcycling_a1.png" class title="实验">
<p>结论是基本上无论从哪个checkpoint初始化MoE模型，收益都比较稳定。</p>
<p>2、Router type</p>
<p>使用不同的router（expert choice和token choice）对比结果如下</p>
<img src="/dcb57672/upcycling_a2.png" class title="实验">
<p>结论是，在相同的step下，expert choice和token
choice的效果基本一样，但是如果从时间上来看，使用expert choice
routing的模型训练更快。</p>
<p>3、Expert capacity factor</p>
<p>每个专家处理的token越多，计算量就越大，理论上效果也越好。</p>
<p>使用不同的capacity factor，模型效果对比如下</p>
<img src="/dcb57672/upcycling_a3.png" class title="实验">
<p>结论是，虽然理论上增加专家容量可以提升效果，但时间上，通常C =
2的效率比较好，即一定的时间内提升的效果最多（注意计算资源是有限的）。</p>
<p>4、Number of MoE layers</p>
<p>在视觉模型上对MoE层数的效果进行了实验。</p>
<p>如下图右边两个小图，是使用不同的MoE层的效果，比如1表示只把最后一层MLP层替换为MoE层，以此类推</p>
<img src="/dcb57672/upcycling_a4.png" class title="实验">
<p>结论是，更多的MoE层并不总是更好，大概是把5<sub>6层替换成MoE层的时候效果最好（40%</sub>50%的层数）。</p>
<p>5、Initialization of experts</p>
<p>对比了使用dense模型的MLP层初始化专家，和随机初始化专家，结果如下</p>
<img src="/dcb57672/upcycling_a5.png" class title="实验">
<p>结果上看，使用dense模型的参数初始化专家效果更好。</p>
<p>6、Number of experts</p>
<p>如前面提到的，增加专家数并不会增大计算量，下图实验了2~128个专家下的效果</p>
<img src="/dcb57672/upcycling_a6.png" class title="实验">
<p>结果上来看，效果是随着专家的增加而提升的，虽然最后表现出了收益递减的情况。</p>
<h3 id="其他">其他</h3>
<p>1、optimizer</p>
<p>在vision模型上，还尝试了使用dense模型的optimizer状态来训练MoE模型，但是并没有带来任何收益。</p>
<p>2、router normalization</p>
<p>另外，为了减少从dense到MoE初始化的performace
drop，尝试了对router的输出进行normalization，以保持每个token得到的weight总和是1。</p>
<p>这个做法直觉上应该是有益的，不过会有一个小问题，那就是对于只被一个expert选中的token，会有vanishing
routing gradients。</p>
<p>实践上，router
normalization在视觉模型上基本和不进行normalization的效果差不多，但是在语言模型上，会使得MoE模型效果变差。这二者的表现差异可能是因为语言模型上部分router使用了token
choice routing。</p>
<p>实际上目前大部分最新的MoE模型都没有开router
normalization，但这里的原因感觉还有待深入验证。</p>
<h3 id="qwen2moe">Qwen2MoE</h3>
<p>Qwen2技术报告中的做法：MoE层通过部分复制MLP层的参数，再加上随机的shuffle，可以打破多个专家的对称性，从未增加专家的多样性，提升效果。</p>
<h2 id="aquilamoe">AquilaMoE</h2>
<p>论文：《AquilaMoE: Efficient Training for MoE Models with Scale-Up
and Scale-Out Strategies》</p>
<p>收益：训练了8*16B的MoE模型，相比随机初始化节省了75.8%的训练时间</p>
<p>实验模型：GPT</p>
<h3 id="简介-1">简介</h3>
<p>AquilaMoE介绍了一种名为EfficientScale的高效MoE训练方法，能以较小的数据成本获得较好的结果。</p>
<p>EfficientScale则是一个two-stage的过程，包括Scale-Up和Scale-Out两个步骤。AquilaMoE就是用EfficientScale训练出来的一个8*16B
MoE模型。</p>
<p>在开始进行Scale-Up和Scale-Out的知识迁移前，还有一个准备阶段。准备阶段要干3个事情：<br>
-
训练小规模dense模型：从零训练一个dense模型，或者找一个已经训练好的模型都可以；这个小规模的dense模型需要已经学习了一定的知识，它将作为后续工作的起点<br>
- 数据准备：准备预训练数据和validation数据<br>
- validation
setup：构建验证流程，用于跟踪后续的phase在起始模型基础上的收益</p>
<h3 id="scale-up-phase">Scale-Up Phase</h3>
<p>Scale-Up包含两步：<br>
- 用准备阶段获得的小模型初始化大模型（dense）<br>
- 对大模型进行基于预训练</p>
<p>初始化的方法，论文初始实验FPI、AKI，还改进了AKI，提出了AKI-Pro。</p>
<p>1、AKI-Pro</p>
<p>（1）Depth Growing Method</p>
<p>深度扩展上，这里参考《Preparing lessons for progressive training on
language models》，选择interpolation的方法扩展模型层数。</p>
<img src="/dcb57672/aquilamoe_depth_growth.png" class title="LLM高效训练">
<p>（2）GQA</p>
<p>原来的AKI方法是不支持GQA的，这里模型使用了GQA，因此需要稍微作一些改动来支持。</p>
<p>通过AKI-Pro初始化的模型会先进行少量的继续预训练。</p>
<p>2、验证</p>
<p>为了验证scale-up的效果，做了实验：用Aquila2-1.3B的dense模型（表示为M(24,
2048)），扩展到Aquila2-7B（表示为M(32,
4096)）。此外还有一个值扩展了宽度的中间模型，M(24, 4096)。</p>
<p>不同方法下，扩展的dense模型validation loss如下（没有进行训练）：</p>
<img src="/dcb57672/aquilamoe_scaleup_loss.png" class title="LLM高效训练">
<p>对于FPI和AKI，interpolation都比stacking的效果更好。</p>
<p>scale-up
phase继续预训练的loss如下，FPI和AKI相比从零预训练都快很多，而AKI相比FPI，loss下降更快。</p>
<img src="/dcb57672/aquilamoe_scaleup_train.png" class title="LLM高效训练">
<h3 id="scale-out-phase">Scale-Out Phase</h3>
<p>这一阶段的目的是把大的dense模型升级为MoE模型。</p>
<p>扩展到MoE模型的方法参考Sparse
Upcycling。MoE层通过复制dense模型的MLP层得到，router的参数随机初始化，随机初始化的采样分布设置为variance=0.02，mean=0。</p>
<p>继续预训练时，除了常规的负载均衡函数之外，还加上了max z-loss。</p>
<p>为了验证scale-out的效果，论文里先用3.6T的数据训练一个1.8B的dense模型，然后通过scale-out的方法，把dense模型扩展为8*1.8B的MoE模型，再进行400B的训练。</p>
<p>训练过程的loss如下：</p>
<img src="/dcb57672/aquilamoe_scale_out.png" class title="LLM高效训练">
<h3 id="实验-9">实验</h3>
<p>通过上面的实验，确定了主模型AquilaMoE的训练规划：<br>
- 先使用3.6T token，从零训练一个7B的dense模型（AquilaDense-7B）<br>
- 只用把7B模型scale
up到16B（AquilaDense-16B），用1.2T的数据进行继续预训练<br>
- 把训练好的16B模型scale
up到8*16B的MoE模型，再训练545B，最终获得AquilaMoE</p>
<p>前面实验中用到模型，以及训练AquilaMoE的所有模型结构如下</p>
<img src="/dcb57672/aquilamoe_models.png" class title="LLM高效训练">
<p>论文给出了AquilaDense-7B、AquilaDense-16B和AquilaMoE三个阶段预训练模型的评测效果，各个任务效果逐步提升（但是没有给出和其他模型的效果对比，emmm这不太合理）：</p>
<img src="/dcb57672/aquilamoe_perf.png" class title="LLM高效训练">
<p>相比直接从零训练一个8*16B的MoE模型，scale up + scale
out节省了不少计算成本和时间。不同阶段下，资源的使用情况如下表：</p>
<img src="/dcb57672/aquilamoe_cost_detail.png" class title="LLM高效训练">
<p>从训练时间上看：</p>
<p><span class="math display">\[\text{Time Savings
Factor}=\frac{\frac{\sum_{i=1}^nN_{\text{tokens,
}i}}{R_{\text{tokens/day, from
scratch}}}}{\sum_{i=1}^n\frac{N_{\text{tokens, }i}}{R_{\text{tokens/day,
}i}}}\]</span></p>
<p><span class="math display">\[=\frac{\frac{3600+1200+545}{25}}{\frac{3600}{279}+\frac{1200}{70}+\frac{545}{25}}=\frac{213.80}{51.84}\approx4.12\]</span></p>
<p>大约节省了75.8%的训练时间。</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】AquilaMoE: Efficient Training for MoE Models with Scale-Up and
Scale-Out Strategies https://www.arxiv.org/abs/2408.06567<br>
【2】Net2Net: Accelerating Learning via Knowledge Transfer
https://arxiv.org/abs/1511.05641<br>
【3】Efficient Training of BERT by Progressively Stacking
https://proceedings.mlr.press/v97/gong19a/gong19a.pdf<br>
【4】Progressively Stacking 2.0: A Multi-stage Layerwise Training Method
for BERT Training Speedup https://arxiv.org/abs/2011.13635<br>
【5】bert2BERT: Towards Reusable Pretrained Language Models
https://arxiv.org/abs/2110.07143<br>
【6】Preparing Lessons for Progressive Training on Language Models
https://arxiv.org/abs/2401.09192<br>
【7】https://zhuanlan.zhihu.com/p/678392914<br>
【8】https://medium.com/smash-those-papers/net2net-explained-7d0321f03594<br>
【9】Learning to Grow Pretrained Models for Efficient Transformer
Training https://arxiv.org/abs/2303.00980<br>
【10】Masked Structural Growth for 2x Faster Language Model Pre-training
https://arxiv.org/abs/2305.02869<br>
【11】https://zhuanlan.zhihu.com/p/664383073<br>
【12】FLM-101B: An Open LLM and How to Train It with $100K Budget
https://arxiv.org/abs/2309.03852<br>
【13】52B to 1T: Lessons Learned via Tele-FLM Series
https://arxiv.org/abs/2407.02783<br>
【14】Pre-training Small Base LMs with Fewer Tokens
https://arxiv.org/abs/2404.08634<br>
【15】Sheared LLaMA: Accelerating Language Model Pre-training via
Structured Pruning https://arxiv.org/abs/2310.06694<br>
【16】Learning sparse neural networks through l_0 regularization
https://arxiv.org/abs/1712.01312<br>
【17】LLAMA PRO: Progressive LLaMA with Block Expansion
https://arxiv.org/abs/2401.02415<br>
【18】Weight Subcloning: Direct Initialization of Transformers Using
Larger Pretrained Ones https://arxiv.org/abs/2312.09299<br>
【19】Sparse Upcycling: Training Mixture-of-Experts from Dense
Checkpoints https://arxiv.org/abs/2212.05055</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>预训练</tag>
        <tag>MoE</tag>
        <tag>高效训练</tag>
        <tag>参数复用</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型算法题(9)</title>
    <url>/fb9c8882.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<p>本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~</p>
<p>如有错漏，欢迎指正~</p>
<hr>
<h1 id="大模型训练过程中有什么可以缓解显存不足的办法">1.大模型训练过程中有什么可以缓解显存不足的办法？</h1>
<p>（1）模型结构：使用LoRA、adaptor等训练</p>
<p>（2）注意力计算底层优化：flash attention、paged attention、ring
attention等</p>
<p>（3）训练框架：使用混合精度训练，使用ZeRO、recomputation、cpu-offload等</p>
<p>（4）训练策略：梯度累积</p>
<h1 id="为什么moe模型的训练相比dense模型更容易受到精度不足的影响">2.为什么MoE模型的训练相比dense模型，更容易受到精度不足的影响？</h1>
<p>模型训练中经常使用的float32/float16/bfloat16等，都存在舍入误差，并且表达的数值越大，舍入误差越大。而MoE模型中，gating
function大量使用了exponential计算，而exponential会把输入数据中的误差放大很多倍，从而使得输出结果大大偏离。因此有些工作会限制在gating
function使用更高精度的表达，并通过一些前置手段压缩数值的大小，从而缓解误差在多个MoE层累计的情况。</p>
<h1 id="模型蒸馏中为什么要使用温度t">3.模型蒸馏中，为什么要使用温度T？</h1>
<p>经典的模型蒸馏过程中，student model会学习teacher
model的softmax输出，在teacher
model的输出进入softmax前，会除以一个温度T&gt;1。</p>
<p>经过较好训练的teacher
model一般会以较高的置信度给出最终结果，而我们使用蒸馏而不从groud
truth（即one-hot
label）直接学习的原因，是因为除了置信度最高的结果之外，其他类别的分值也能够提供有价值的信息。比如在水果图片三分类任务，共有三个类别
[菠萝，榴莲，甘蔗]，teacher model 对一张图片给出 [0.7, 0.2, 0.1]
的结果，除了正确答案0.7以外，0.2和0.1这两个分支提供了一个信息：榴莲和菠萝的相似程度大于甘蔗和菠萝，这是one-hot
label提供不了的。</p>
<p>而除以温度T，能够让teacher
model给出的label变得更加soft，信息熵更大，从而可以让student
model更好地学习。</p>
<h1 id="大模型有什么加速推理的方法">4.大模型有什么加速推理的方法？</h1>
<p>1.针对训练好的模型：模型蒸馏、模型量化、模型剪枝等</p>
<p>2.模型结构设计：MoE</p>
<p>3.训练策略：early-exit</p>
<p>4.解码策略：投机解码</p>
<h1 id="为什么llama12的intermiedia-sizehidden-size相比bert的减小了从4下降到83">5.为什么LLAMA1/2的intermiedia
size/hidden size相比Bert的减小了，从4下降到8/3？</h1>
<p>LLAMA使用SwiGLU
FFN，相比Bert的FFN，多了一个线性变化矩阵。为了保持模型的参数量不变，因此把intermediate
size从4倍hidden size降为8/3倍。</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a></p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>算法题</tag>
      </tags>
  </entry>
  <entry>
    <title>InternLM系列模型</title>
    <url>/7f3d361.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>InternLM系列模型的参与方有上海AI实验室、商汤、香港中文大学，以及复旦和上交。主力应该是前两个，InternLM中的Intern这个名字也是继承自它们之前的视觉模型项目的名字。</p>
<p>最近InternLM2.5发布，在HuggingFace的大模型榜单上有不错的成绩，因此梳理一下InternLM相关的资料，学习一下。</p>
<h1 id="internlm一代">InternLM一代</h1>
<p>首先是最早发布的一代模型。</p>
<p>InternLM第一代是2023年6月发布的，时间比较早，技术报告中透露的内容也不多，把关键信息简单整理一下：<br>
- 104B参数的模型<br>
- 1.6T的多语言数据，以英语为主，部分中文，少量其他语言<br>
- 窗口长度为2k<br>
-
使用多阶段的预训练策略，在每个阶段分别进行lr退火；不同的阶段在数据比例上有所不同；使用多阶段的训练策略好处是方便调整效果，并且如果需要回退不用全部重新训练<br>
- 对齐阶段包括SFT和RLHF；5M条SFT数据，部分来自self-instruct<br>
- 效果上，和Llama一代可比，和ChatGPT还有差距</p>
<p>后来发布了新版，模型规模提升到了123B。在2023年8、9月又发布了7B和20B规模的base模型和chat模型。</p>
<h1 id="internlm2">InternLM2</h1>
<p>2024年初，InternLM2发布。</p>
<h2 id="internlm2概览">InternLM2概览</h2>
<p>InternLM2包括1.8B、7B和20B三个规模的模型，最大支持200k的窗口长度（Needle-in-a-Haystack评测），不同规模的模型训练数据量从2.0~2.6T不等。</p>
<p>对齐阶段包括SFT和COnditional OnLine Reinforcement Learning from Human
Feedback (COOL RLHF)，细节后面说。</p>
<h2 id="模型">模型</h2>
<p>开源模型比如Llama、Qwen、Baichuan和Mistral等基本上把当前LLM的标准结构定了下来，而很多训练和推理相关的配套优化也是支持这样的标准化设置的。</p>
<p>出于对目前业界优化复用的考虑，在模型结构上InternLM并没有做什么改变，三个规模的模型结构超参如下：</p>
<img src="/7f3d361/model.png" class title="InternLM系列模型">
<h2 id="预训练">预训练</h2>
<h3 id="数据">数据</h3>
<p>预训练数据主要包括三类：通用的文本数据、代码相关的数据，以及长文本数据。</p>
<p>1、通用文本数据</p>
<p>通用数据来源主要包含web数据、书籍和technical
literature（techlit）等，这几类数据在预训练数据中的占比如下：</p>
<img src="/7f3d361/pt_data_dist.png" class title="InternLM系列模型">
<p>通用数据的处理流程基本上也是标准做法：</p>
<img src="/7f3d361/pt_data_pipeline.png" class title="InternLM系列模型">
<p>highlight几个细节：<br>
- 去重的时候，对文档的5-gram使用128个hash function，threshold =
0.7<br>
- 出现重复的时候保留most recent的数据<br>
- 安全过滤使用了kaggle的“Toxic Comment Classification
Challenge”数据集</p>
<p>2、代码数据</p>
<p>代码数据主要来自github爬取、公开数据，以及一些和代码/编程相关的论坛、网页等。</p>
<p>不同编程语言在代码数据的分布如下</p>
<img src="/7f3d361/code_lang_dist.png" class title="InternLM系列模型">
<p>代码数据会经历format cleaning、deduplication、quality
filtering和dependency sorting几个阶段。</p>
<p>（1）format cleaning</p>
<p>所有代码数据都要转成unified markdown
format。会有少量数据由于原始数据的问题，没有直接转换格式成功，因此会再用一些规则进行处理。</p>
<p>使用markdown格式是因为它比较简单，并且可以兼容自然语言和代码。</p>
<p>预训练中实际使用的格式会更加复杂一点，包括多段代码和dependecy文件的拼接处理。</p>
<blockquote>
<p>The main idea is to utilize the interleaving data, which is pivotal
for teaching the model about programming.</p>
</blockquote>
<p>这和《DeepSeek-Coder》提到的类似。</p>
<p>（2）deduplication</p>
<p>去重的过程和通用的文本数据大致一样，除了一个地方 -- tokenizer。</p>
<p>比如python中，有的代码indent会用2个空格，有些是4个token，还有写是tab。这就对去重效果造成了影响。因此需要用一个能把这些情况合并的tokenizer。</p>
<p>实际上目前有很多数据去重方法已经做到了段落或者line的粒度，这样当然去重效果更好。不过这里还是用了file
level的去重。</p>
<p>（3）quality filtering</p>
<p>几个要点：<br>
-
基于规则的打分可以对各种维度进行打分，不过代码风格可能不是一个好的评分维度，会很容易把数据错分为低质量<br>
-
模型评分和人类评分的一致性并不很高，因此只在“模型评分和人类评分一致性足够高”的语言上，应用了模型分数进行过滤<br>
-
为了提高模型打分的准确率，使用了三轮的迭代，每轮包括人类标注和重新训练</p>
<p>（4）dependency sorting</p>
<p>InternLM2的最大训练窗口达到了32k。而在之前的数据处理流程中，来自同一个代码仓库的文件已经被打乱，因此需要把这些文件重新按顺序排列好。</p>
<p>最终代码数据质量被分成高中低三类，其中高质量数据会训练多轮，中等质量数据训练一轮，而低质量数据则不会用于训练。</p>
<img src="/7f3d361/code_quality.png" class title="InternLM系列模型">
<p>3、长文本数据</p>
<p>长文本数据的处理参考《Longwanjuan: Towards systematic measurement for
long text quality》的做法。</p>
<p>长文本数据过滤包括：<br>
- 长度选择，选择至少32k byte长度的数据<br>
- statistical filters<br>
- perplexity filters</p>
<p>（1）statistical filters</p>
<p>用于过滤掉无意义的数据，而不是用于筛选高质量数据。统计过滤器在长文本特别有效，因为长文本的统计特征相比短文本更加稳定和可靠。</p>
<p>（2）perplexity filters</p>
<p>这里ppl的用法和以往的有所不同。</p>
<p>对于一段文本S，计算其ppl可能会受到模型和tokenizer的影响。InternLM2这里的做法是把文本切分成S1和S2，对于正常长文本，P(S2|S1)应该比P(S2)要低，因为前文提供了更多的信息。而如果P(S2|S1)&gt;P(S2)，那么就说明S1对S2造成了干扰，那么这段文本上下文关联就很弱甚至是负向的。这样的文本就会被过滤掉。</p>
<p>（3）threshold selection</p>
<p>关于阈值的选择，有两个要点：<br>
- 对不同的domain使用不同的阈值能够比使用统一阈值效果更好<br>
-
更多关注在得分在阈值周围的数据，因为统计过滤器和ppl相比model-based的打分更加平滑，这些边界case可以很大程度反映过滤器的打分标准</p>
<h3 id="tokenizer">tokenizer</h3>
<p>基于cl100k，抽取了top
60,004个token，加上32,397个中文token获得新词表。为了让词表的大小是128的倍数，方便分布式训练，再加上147个spare
token。</p>
<h3 id="预训练设置">预训练设置</h3>
<p>各个模型的预训练设置如下：</p>
<img src="/7f3d361/model.png" class title="InternLM系列模型">
<ul>
<li>AdamW optimizer，beta_1 = 0.9, beta_2 = 0.95<br>
</li>
<li>epsilon = 1e-8<br>
</li>
<li>weight decay = 0.1<br>
</li>
<li>final lr = 10% * max lr</li>
</ul>
<h3 id="多阶段预训练">多阶段预训练</h3>
<p>不同规模的模型总训练量为2.0T~2.6T不等。预训练过程分为3个阶段。</p>
<p>1、阶段1：4k</p>
<p>窗口长度为4096，消耗90%的token。</p>
<p>2、阶段2：长窗口</p>
<p>窗口长度为32,768，消耗9%的token。</p>
<p>虽然窗口长度为32,768，不过超过50%的数据本身长度还是 &lt; 4096的。</p>
<p>这一阶段把RoPE的base frequency从50,000提升到100,000。得益于flash
attention和可扩展的训练框架，32k训练窗口下的训练速度相比4k只降低了40%。</p>
<p>3、阶段3：能力增强</p>
<p>这一阶段是针对reasoning、数学能力、知识学习的提升的，共消耗约24B的数据。</p>
<p>所用数据分布如下</p>
<img src="/7f3d361/specific_data.png" class title="InternLM系列模型">
<h2 id="对齐">对齐</h2>
<p>对齐阶段包括SFT和RLHF。</p>
<h3 id="sft">SFT</h3>
<p>共有10M的SFT数据，分布如下：</p>
<img src="/7f3d361/sft_data.png" class title="InternLM系列模型">
<p>数据格式使用ChatML（《chat markup
language》https://github.com/MicrosoftDocs/azure-docs/blob/main/
articles/ai-services/openai/includes/chat-markup-language.md），7B和20B模型都进行了一个epoch的微调，lr
= 4e-5。</p>
<h3 id="cool-rlhf">COOL RLHF</h3>
<p>RLHF有两个问题：<br>
- preference
conflict：比如满足helpful的response更容易在安全性上出现问题，目前的做法通常是使用多个preference模型，导致训练慢了<br>
- reward
hacking：随着训练进行，actor模型学到一些获得高reward的捷径，但实际上response的质量并没有提升</p>
<p>为了解决这两个问题，InternLM把RLHF框架修改为Conditional OnLine
RLHF。</p>
<h4 id="conditional-reward-model">Conditional Reward Model</h4>
<p>简单地说，conditional reward model就是利用不同的system
prompt，达到一个模型对多个维度进行打分的目的，如下图：</p>
<img src="/7f3d361/crm.png" class title="InternLM系列模型">
<p>reward模型的训练用了2.4M对偏好数据，覆盖了不同的能力。</p>
<p>1、loss function</p>
<p>reward模型训练的loss function做了一些改动。</p>
<p>首先，参考focal
loss的思路，为了让难样本的loss更大，而简单样本的loss更小，loss函数修改如下：</p>
<p><span class="math display">\[L_{ranking}=-(1-2\times\max(0,P_{i,j}-\frac12))^\gamma\log(P_{i,j}))\]</span></p>
<p>图像画出来是这样的</p>
<img src="/7f3d361/l_rank.png" class title="InternLM系列模型">
<p>附上代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the function again for plotting</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">g</span>(<span class="params">P</span>):</span><br><span class="line">    <span class="keyword">return</span> -(<span class="number">1</span> - <span class="number">2</span> * np.maximum(<span class="number">0</span>, P - <span class="number">0.5</span>))**<span class="number">2</span> * np.log(P)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate P values</span></span><br><span class="line">P = np.linspace(<span class="number">0.01</span>, <span class="number">1</span>, <span class="number">400</span>)  <span class="comment"># P values from a small number above 0 to 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate y values</span></span><br><span class="line">y = g(P)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plotting the corrected graph</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(P, y, label=<span class="string">r&#x27;$-(1-2 \times \max(0, P-\frac&#123;1&#125;&#123;2&#125;))^&#123;2&#125; \log(P)$&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">r&#x27;Graph of $-(1-2 \times \max(0, P-\frac&#123;1&#125;&#123;2&#125;))^&#123;2&#125; \log(P)$&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;P&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Function Value&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>模型输出的win vs lose
response的概率越大，说明case越简单，loss就趋向于0，反之则loss会增大。</p>
<p>另外就是如果不对reward的分数进行限制，那么就有可能出现绝对值特别大的数，这样可能导致训练不稳定。因此再加上一个对reward
score的logarithmic barrier penalty：</p>
<p><span class="math display">\[L_{penalty}=-(\log(x+5)+\log(5-x))\]</span></p>
<p>这个函数只在(-5, 5)之间有定义，画一下图像如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> -np.log(x + <span class="number">5</span>) - np.log(<span class="number">5</span> - x)</span><br><span class="line">    </span><br><span class="line">x = np.linspace(-<span class="number">10</span>, <span class="number">10</span>, <span class="number">1000</span>)  <span class="comment"># x values from -10 to 10</span></span><br><span class="line">x = x[(x + <span class="number">5</span>) &gt; <span class="number">0</span>]  <span class="comment"># Filter out values where the function is undefined for the first log</span></span><br><span class="line">x = x[(<span class="number">5</span> - x) &gt; <span class="number">0</span>]  <span class="comment"># Filter out values where the function is undefined for the second log</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate y values for the filtered x values</span></span><br><span class="line">y = f(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plotting the updated range</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(x, y, label=<span class="string">r&#x27;$-(\log(x+5)+\log(5-x))$&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">r&#x27;Graph of $-(\log(x+5)+\log(5-x))$&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img src="/7f3d361/u_shape.png" class title="InternLM系列模型">
<p>对于绝对值比较大的reward
score（靠近-5或者5），这个loss值就会增大。</p>
<p>最终reward模型的训练loss如下：</p>
<p><span class="math display">\[L=L_{ranking}+\lambda
L_{penalty}\]</span></p>
<p><span class="math inline">\(L_{penalty}\)</span> 的系数 <span class="math inline">\(\lambda=0.02\)</span>。</p>
<p>2、训练</p>
<p>一些训练细节：<br>
- RM训练的时候保持每个batch的token数为16,384，不管数据条数为多少<br>
- cosine lr schedule，max lr = 1e-5, final lr = 5e-6<br>
- weight decay = 0.01<br>
- 为了防止过拟合，只训练一个epoch</p>
<h4 id="online-rlhf">Online RLHF</h4>
<p>Online RLHF说白了就是建立feedback机制，多次迭代。</p>
<p>InternLM2的online RLHF建立了两条反馈机制的path： fast path和slow
path。</p>
<p>1、Fast Path</p>
<p>fast path用于修正在训练中出现的reward hacking的问题。</p>
<p>随着PPO训练进行，actor模型很可能会找到一些获得高reward的捷径，相当于是作弊了。这些情况大部分都源自RM训练时的漏洞，或者是覆盖不全，或者是过拟合了。</p>
<p>总之fast
path就是在每次进行完RLHF的训练之后，找到这些容易触发很高reward值的case，并根据这些case构建对应的训练样例，修补漏洞。20~100条样本就足够修补对应的漏洞了。</p>
<p>2、Slow Path</p>
<p>相对于fast path，slow
path可以说是regular的、更全面的反馈修复。其中会用到更多的数据和标注。</p>
<p>由于大量数据的标注需要时间，所以slow
path的训练数据可能是滞后的，比如在第二次RLHF完成之后，第一轮的slow
path标注数据才完成，那么这些数据就会加到第三轮的训练中去。</p>
<p>整个online RLHF一共进行了3个round。</p>
<p>PPO训练用了200k左右的query，模型更新了约400次。</p>
<p>文中还提到了一些训练的实用细节。</p>
<p>1、关于初始化</p>
<p>在PPO训练开始的时候，会先固定actor模型，单独对critic模型训练50步。</p>
<p>因为critic模型的目标和sft模型或者reward模型都有所不同，所以从这个两个模型初始化之后，critic模型在训练前期其实是出于一性能不佳的状态的。在这个阶段如果参考critic模型的反馈对actor模型进行更新，可能会带来不稳定。</p>
<img src="/7f3d361/critic_loss.png" class title="InternLM系列模型">
<p>2、conditional reward</p>
<p>如前面提到的，RM是使用不同的system prompt获取对应维度的reward
score的，而在PPO训练时也需要根据输入的prompt调整给RM的system
prompt。</p>
<img src="/7f3d361/condition_ppo.png" class title="InternLM系列模型">
<p>3、pretrain gradient</p>
<p>为了缓解灾难性遗忘的问题，PPO训练中加入了预训练数据，并计算pretrain
loss，加入到模型的整体损失中。pretrain
loss的系数为0.5，预训练数据量大约相当于PPO训练数据的50%。</p>
<p>4、超参</p>
<ul>
<li>KL divergence coefficent = 0.01<br>
</li>
<li>actor model lr = 1e-6<br>
</li>
<li>critic model lr = 5e-6<br>
</li>
<li>actor model解码top_p = 0.9<br>
</li>
<li>larger λ value for PPO leads to higher rewards in our case, so we
set it to 0.99</li>
</ul>
<h3 id="长文本finetune">长文本finetune</h3>
<p>在SFT和RLHF中都使用了长文本数据。数据主要来源有两个：<br>
- 书籍<br>
- github仓库</p>
<p>代码数据使用了DS-1000（《Ds-1000: A natural and reliable benchmark
for data science code generation》）中超过10k
star的仓库，并按如下流程进行拼接，以获取32k长度的数据：</p>
<img src="/7f3d361/long_code_data.png" class title="InternLM系列模型">
<h3 id="tool-augmented-llms">Tool-Augmented LLMs</h3>
<p>为了让模型具备一定的工具调用能力，修改了ChatML格式，加入了新的角色 --
“environment”，让模型可以从外部接口获取反馈。</p>
<p>下面是一个模型调用工具的例子：</p>
<img src="/7f3d361/tool_case.png" class title="InternLM系列模型">
<p>工具训练的方式是按照《Agent-flan: Designing data and methods of
effective agent tuning for large language models》的做法进行的。</p>
<h1 id="internlm2.5">InternLM2.5</h1>
<p>InternLM2.5的模型结构和InternLM2一样。</p>
<p>InternLM2.5主要有几个提升：</p>
<blockquote>
<p>卓越的推理性能：在数学推理方面取得了同量级模型最优精度，超越了 Llama3
和 Gemma2-9B。</p>
</blockquote>
<blockquote>
<p>有效支持百万字超长上下文：模型在 1
百万字长输入中几乎完美地实现长文“大海捞针”，而且在 LongBench
等长文任务中的表现也达到开源模型中的领先水平。</p>
</blockquote>
<blockquote>
<p>工具调用能力整体升级：InternLM2.5
支持从上百个网页收集有效信息进行分析推理，相关实现将于近期开源到
Lagent。InternLM2.5
具有更强和更具有泛化性的指令理解、工具筛选与结果反思等能力，新版模型可以更可靠地支持复杂智能体的搭建，支持对工具进行有效的多轮调用，完成较复杂的任务。</p>
</blockquote>
<p>评测结果如下：</p>
<img src="/7f3d361/internlm25.png" class title="InternLM系列模型">
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】InternLM: A Multilingual Language Model with Progressively
Enhanced Capabilities
https://github.com/InternLM/InternLM-techreport/blob/main/InternLM.pdf<br>
【2】InternLM2 Technical Report https://arxiv.org/abs/2403.17297<br>
【3】书生·浦语
https://www.baike.com/wikiid/7382383761788551219?anchor=lxmlbsze188r<br>
【4】https://github.com/InternLM/InternLM/blob/main/README_zh-CN.md</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>phi系列模型</title>
    <url>/fe13b56f.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>最近在做端侧模型和数据合成的工作，微软的phi系列是受到关注比较多的一个小规模模型，整理一下细节，看看有什么可以借鉴使用的。</p>
<h1 id="phi-1">phi-1</h1>
<p>phi-1包括两个模型：350M参数的phi-1-small和1.3B参数的phi-1-base。</p>
<p>相比其他SLM/LLM，phi-1的特点是所用数据极少，预训练只有7B：<br>
- 6B从web数据筛选而来<br>
- 1B使用GPT-3.5生成</p>
<p>训练资源也只用到了A100*8，共训练了4天。</p>
<p>由于模型规模较小，并且为了快速验证方法的有效性，phi-1把关注点放在模型的code能力上（python语言）。phi-1-base和其他通用LM/代码LM在参数量、训练数据量，以及在HumanEval和MBPP上的效果对比如下表：</p>
<img src="/fe13b56f/phi_1_result.png" class title="phi系列">
<h2 id="模型">模型</h2>
<p>phi-1关注点在数据上，因此模型结构上没有特别设计，使用的标准的decoder-only，phi-1两个模型的参数如下：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>phi-1-base</th>
<th>phi-1-small</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>参数量</td>
<td>1.3B</td>
<td>350M</td>
</tr>
<tr class="even">
<td>层数</td>
<td>24</td>
<td>20</td>
</tr>
<tr class="odd">
<td>hidden size</td>
<td>2048</td>
<td>1024</td>
</tr>
<tr class="even">
<td>intermediate size</td>
<td>8192</td>
<td>4096</td>
</tr>
<tr class="odd">
<td>attention head num</td>
<td>32</td>
<td>16</td>
</tr>
</tbody>
</table>
<p>两个模型都使用了MHA，位置编码RoPE的dimension为32（每个注意头的大小为64，即只有一半的维度加上了位置编码的信息）。</p>
<p>而tokenizer则是复用了codegen-350M-mono的。</p>
<h2 id="数据">数据</h2>
<p>以往的scaling
law关注在模型的参数量和训练的数据量。而phi-1的工作则主要关注在另一个维度：数据质量。</p>
<p>参考《Tinystories: How small can language models be and still speak
coherent english?》的工作，数据质量是能够极大地改变scaling
law的形状的。</p>
<p>如文章标题《Textbooks Are All You
Need》，phi-1最主要的工作就是提高训练数据质量，达到“textbook”的水平。</p>
<p>1、现有数据集的问题</p>
<p>对于代码领域，公开的数据集比如The
Stack、StackOverflow和CodeContest等，都存在指导性不足的问题，具体来说有以下问题：<br>
- not
self-contained：这些代码片段或者文件依赖外部的库或者文件，因此无法单纯从当前的文件理解代码在干什么<br>
- not
meaningful：一些代码片段可能主要内容是大量的定义、参数设置或者GUI配置相关的内容，而没有计算和逻辑推理内容，这样的内容显然并不有足够的教育意义，下图就是一个例子<br>
-
部分文件或者代码过于复杂，且缺乏注释或者文档说明，这也让模型的学习变得困难<br>
- topic分布不均匀</p>
<img src="/fe13b56f/phi_1_code_case.png" class title="phi系列">
<p>设想一下，如果让一个人类初学者用这些资料进行代码学习，效果一定也是很差的，因为里面包含了很多噪音、不完整的内容以及概念的模糊不清。</p>
<p>从这个角度出发，打造textbook级别质量的数据就是很自然的想法。</p>
<p>2、数据过滤</p>
<p>首先就是从已有的数据中，过滤提取高质量数据。</p>
<p>The
Stack和StackOverflow两个数据集的python子集，在去重之后有35M个文件，约35B的数据。微软从中抽取了100k个文件，让GPT-4进行打分，“determine
its educational value for a student whose goal is to learn basic coding
concepts”。</p>
<p>（可惜这里没有给出具体的prompt）</p>
<p>由此获得100k的训练数据，之后用codegen模型的output
embedding作为feature，训练random forest
classifier，再回头对35M的数据进行打分、筛选。</p>
<p>单独使用这些过滤后的数据进行训练，效果已经比不过滤的好（且训练的step数更少，96k
vs 36k），如下图所示（橙色和浅蓝色对比）</p>
<img src="/fe13b56f/phi_1_compare.png" class title="phi系列">
<p>3、synthetic textbook dataset</p>
<p>另一个获取高质量预训练数据的方法是数据合成。</p>
<p>数据合成主要问题之一就是「多样性」： -
训练数据的多样性很重要，因为多样性的内容可以让模型学到不同topic的内容、同一问题的不同表达和不同解法，以此提升模型泛化性，以及对未见过case的稳定性<br>
-
生成数据的多样性不容易获得，因为LLM训练的时候就是学习输出最大概率的内容，因此模型天然倾向于给出少量几个最可能的结果</p>
<p>这里phi-1参考了《Tinystories》的做法，通过prompt给GPT-3.5的输出结果注入了一些随机性，限制了topic和（模型输出文本的）目标观众，获取了约1B
token的数据。</p>
<p>这些数据包含自然语言和相关的代码片段，下图是一个示例：</p>
<img src="/fe13b56f/phi_1_example_1.png" class title="phi系列">
<p>4、CodeExercises dataset</p>
<p>数据合成也用于生成高质量的SFT数据。</p>
<p>微软用GPT-3.5生成了约180M
token的微调数据，要求模型根据自然语言指令写出代码。这里提升多样性的方法是限定了function
name。下图是一个示例：</p>
<img src="/fe13b56f/phi_1_example_2.png" class title="phi系列">
<h2 id="训练">训练</h2>
<p>phi-1的预训练和微调都使用以下配置：<br>
- AdamW optimizer，weight decay = 0.1<br>
- linear-warmup-linear-decay learning rate schedule<br>
- dropout = 0.1<br>
- 窗口大小 = 2048<br>
- 数据精度 = fp16</p>
<p>预训练共训练了36000 step，最终选择了24000
step的checkpoint，相当于7B的预训练数据共训了8个epoch，约50B；训练参数如下：<br>
- batch size = 1024<br>
- max lr = 1e-3<br>
- warmup step = 750</p>
<p>微调共进行了6000个step，参数如下：<br>
- batch size = 256<br>
- max lr = 1e-4<br>
- warmup step = 50</p>
<p>论文还指出，微调对模型代码能力的提升很大，只要体现在指令遵循能力，和使用外部代码库的能力上。</p>
<h2 id="小结">小结</h2>
<ul>
<li>phi-1使用极小的数据量，和较小规模的模型，在代码能力验证了高质量数据的影响，可惜没有给出更具体的prompt等<br>
</li>
<li>合成数据会是一条通往更强只能的重要道路，苹果和Meta都已经做了很多工作<br>
</li>
<li>这样的方案是否能scaling
up？数据有没有可能存在污染？后面工作继续探索这些问题</li>
</ul>
<h1 id="phi-1.5">phi-1.5</h1>
<p>phi-1.5延续phi-1的思路，使用和phi-1完全一样的模型结构，把目标领域扩展到了代码
+ common sense reasoning，探索“how small can a LLM be to achieve certain
capabilities”这个问题的答案。</p>
<img src="/fe13b56f/phi_15_result.png" class title="phi系列">
<h2 id="数据-1">数据</h2>
<p>phi-1.5的预训练数据在phi-1预训练数据集（7B
token）的基础上，加入了约20B的高质量合成数据，用于让模型学习common sense
reasoning和general knowledge。</p>
<p>这20B数据来自于精心挑选的20k topics，并通过在prompt中加入来自web
dataset的sample提升模型生成数据的多样性。</p>
<p>文中指出，数据的生成不仅需要算力，“It requires intricate iterations,
strategic topic selection, and a deep understanding of knowledge gaps to
ensure quality and diversity of the data.”</p>
<h2 id="训练-1">训练</h2>
<p>phi-1.5的预训练设置：<br>
- 模型随机初始化<br>
- max lr = 2e-4<br>
- no warmup<br>
- AdamW，beta_1 = 0.9, beta_2 = 0.98, epsilon = 1e-7<br>
- DeepSpeed ZeRO stage 2<br>
- fp16数据格式<br>
- batch size = 2048</p>
<p>共训练了150B（多个epoch），其中20%来自phi-1的数据集，80%来自新合成的数据。</p>
<h2 id="filtered-web-data">filtered web data</h2>
<p>为了探索traditional web
data的效果，研究人员还搞了phi-1.5-web-only模型（只使用web
data训练），和phi-1.5-web模型（混合了phi-1数据、合成数据和web
data，比例为2:4:4）。所用的filtered web data共有95B，其中88B来自Falcon
refined web dataset，7B来自The Stack和StackOverflow。</p>
<h2 id="效果">效果</h2>
<p>phi-1.5、phi-1.5-web和phi-1.5-web-only与其他模型，在几个common
sense的benchmark的效果对比如下</p>
<img src="/fe13b56f/phi_15_bench_1.png" class title="phi系列">
<ul>
<li>phi-1.5-web-only已经比很多其他模型效果好，微软把这个提升归功于数据过滤<br>
</li>
<li>phi-1.5-web相比phi-1.5提升不大，说明合成数据已经够好</li>
</ul>
<p>在language understanding task上，phi-1.5的效果如下</p>
<img src="/fe13b56f/phi_15_bench_2.png" class title="phi系列">
<p>最后，通过数学能力和代码能力来评估模型的reasoning
ability，结果如下：</p>
<img src="/fe13b56f/phi_15_bench_3.png" class title="phi系列">
<ul>
<li>phi-1.5在reasoning上相比其他模型优势很大<br>
</li>
<li>phi-1.5-web则在phi-1.5的基础上，有明显提升，说明web
data对reasoning能力有帮助<br>
</li>
<li>phi-1.5的代码能力和phi-1差不多，这也说明高质量数据的训练更高效（加入更多非代码数据没有太多帮助）</li>
</ul>
<h1 id="phi-2">phi-2</h1>
<p>phi-2（2.7B）是基于phi-1.5模型参数进行scale up的工作。</p>
<h2 id="scale-up">scale up</h2>
<p>以phi-1-small和phi-1-base为例，直接train from
scratch，结果是这样的：</p>
<img src="/fe13b56f/phi_2_0.png" class title="phi系列">
<p>而另外一个做法，就是复用小模型训练好的参数，用于初始化更大的模型。大模型和小模型的数和hidden
size不同，因此需要做一些处理。</p>
<p>1、scaling number of layers</p>
<p>参考《Scaling language models: Methods, analysis &amp; insights from
training
gopher》，通过以下映射，把phi-1.5的每层的参数复制到更大的模型（20层--&gt;24层）：</p>
<p>round_int(range(num_layers_new)/num_layers_new * num_layers_old)</p>
<p>2、Scaling attention layer dimensions</p>
<p>大小模型的QKV投影矩阵维度不同，最简单的复用方法就是大的矩阵部分直接使用小矩阵参数，其余多出来的参数直接随机初始化（weight
reuse，WR），如下图</p>
<img src="/fe13b56f/phi_2_1.png" class title="phi系列">
<p>更进一步，还可以使用tiling，把大矩阵多出来的维度用小矩阵的参数填满，如下图</p>
<img src="/fe13b56f/phi_2_2.png" class title="phi系列">
<p>直接训练大模型、WR和WR + tiling的效果如下</p>
<img src="/fe13b56f/phi_2_3.png" class title="phi系列">
<h2 id="phi-2-1">phi-2</h2>
<p>用WR + tiling，从phi-1.5初始化phi-2（2.7B）的效果如下</p>
<img src="/fe13b56f/phi_2.png" class title="phi系列">
<h1 id="phi-3">phi-3</h1>
<p>phi-3包括3个模型：<br>
- phi-3-mini，3.8B参数，适用于移动设备<br>
- phi-3-small，7B参数<br>
- phi-3-medium，14B参数</p>
<h2 id="模型-1">模型</h2>
<ul>
<li>phi-3使用和Llama-2相似的模型结构<br>
</li>
<li>不同规模的模型(mini、small &amp; medium)词表大小不同<br>
</li>
<li>通过LongRoPE的方法把窗口扩展到了128k<br>
</li>
<li>phi-3-small使用了GQA</li>
</ul>
<p>推理时，使用了blocksparse attention对KV
cache进行压缩：每个头仅保留部分不同的KV
block，这样在减少缓存用量的同时，可以保障模型的一定程度的正常推理和长文本能力，示意图如下：</p>
<img src="/fe13b56f/phi_3_sparse.png" class title="phi系列">
<h2 id="数据-训练">数据 &amp; 训练</h2>
<p>按照《Textbooks Are All You Need》的路径，phi-3使用了”heavily
filtered publicly web data“进行训练，这些数据通过”educational
level“进行清洗和分类。</p>
<p>预训练包括两个phase：<br>
- phase1：大量的web sources，让模型学习通用知识 &amp; 语言理解<br>
- phase2：使用更多heavily filtered
webdata（phase-1数据的子集），以及一些可以提升模型reasoning能力和其他技能的合成数据</p>
<p>phi-3-mini的训练数据总共有3.3T。</p>
<p>和”compute optimal“相似，微软认为给定规模下的小模型存在一个”data
optimal“的状态，即把数据调到最优状态。</p>
<p>有些数据是不适合给小规模的模型训练的，但是可以给更大的模型使用。例如，某一天英超联赛中一场比赛的结果可能是大模型的良好训练数据，但对于小型模型，需要去除这类信息，以便为“推理”留出更多的模型容量。</p>
<p>训练中发现在部分benchmark上，phi-3-medium（14B）和phi-3-small（7B）的差距远小于phi-3-small和phi-3-mini的差距，这可能说明目前的这份数据目前并不是phi-3-medium这个规模下的”data
optimal“，而需要进一步的调试。</p>
<h2 id="效果-1">效果</h2>
<p>在MMLU上，phi系列和Llama-2系列模型的对比如下</p>
<img src="/fe13b56f/phi_3_result.png" class title="phi系列">
<p>phi系列的效率看起来更高，处于”data optimal regime“。</p>
<h1 id="数据污染">数据污染？</h1>
<p>phi系列，以及其他一些模型，在模型参数量较小/训练数据量较小的情况下获得了媲美模型规模数倍于这些模型的效果。其中是否存在过拟合的情况？</p>
<p>《A Careful Examination of Large Language Model Performance on Grade
School
Arithmetic》针对这个问题做了实验。研究人员参考GSM8k，构造了GSM1k数据集。GSM1k数据在长度、难度、覆盖范围等方面都和GSM8k吻合。</p>
<p>如果一个模型没有过拟合到GSM8k，那么用GSM1k进行测试，应该获得和GSM8k相近的结果，反之则在GSM1k上的效果会比较差。</p>
<p>文章选取了Mixtral系列、phi系列、Llama系列等模型，测试结果如下：</p>
<img src="/fe13b56f/overfit.png" class title="phi系列">
<p>phi-3在GSM1k上的效果和GSM8k的gap排在前列，某种程度上说明phi-3是有过拟合到测试数据集上的。</p>
<p>不过这也未必说明phi毫无可取之处，如果在过拟合的情况下，能够保证目标领域内的效果，那从业务上来说完全是可以接受的。在ChatGPT之前，模型训练基本就是在单个任务上做数据工程。我们进入到”通用智能“的时代才不到两年，大部分的业务逻辑和形态并没有转换过来。</p>
<p>当然，通用的能力依然是我们追求的目标。只是目前来看，除了Claude、GPT-4和Llama，其他模型都还有一定差距。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】Textbooks Are All You Need
https://arxiv.org/abs/2306.11644<br>
【2】Textbooks Are All You Need II: phi-1.5 technical report
https://arxiv.org/abs/2309.05463<br>
【3】Phi-2: The Surprising Power of Small Language Models
https://nips.cc/media/neurips-2023/Slides/83968_5GxuY2z.pdf<br>
【4】Phi-3 Technical Report: A Highly Capable Language Model Locally on
Your Phone https://arxiv.org/abs/2404.14219<br>
【5】A Careful Examination of Large Language Model Performance on Grade
School Arithmetic https://arxiv.org/abs/2405.00332</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>端侧模型</tag>
        <tag>微软</tag>
      </tags>
  </entry>
  <entry>
    <title>一个模型支持智能助手系统</title>
    <url>/9c593ccd.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>这是一篇关于三四年前的旧项目的回顾。</p>
<p>前一阵看<a href="https://www.linsight.cn/1e34e252.html">苹果AFM端侧模型</a>的做法，让我想起了前几年做的项目：用一个Bert模型支持智能助手多个任务。方便起见，本文后续就把这个Bert模型叫BFM（Bert-based
Foundation Model）吧。</p>
<p>项目虽然有些旧，但是有些思路还是可以参考的，整理一下，看能不能提炼出一些有启发的内容。</p>
<h1 id="智能助手">智能助手</h1>
<p>项目主要是智能助手的NLU能力相关。</p>
<h2 id="任务">任务</h2>
<p>先看下早期智能助手在干什么。早期的智能助手（比如Siri），能力上基本可以分成三大类：</p>
<p>1、指令类</p>
<p>包括手机和智能设备如iot的操作，经典的用户query有“打开窗帘”（开关设置），“明天早上八点叫我起床”（设定闹钟）等。</p>
<p>指令类的任务基本上可以认为是封闭域任务，因为大部分设置/操作状态是有限的，可遍历的。当然其中也存在部分包含开放域内容的情况，比如“添加一条明天参加小A生日派对的日程”，其中“参加小A生日派对”是要写入日程的值，理论上这个值可以是任意内容，这就属于开放域。不过一般来说，我们通过识别“添加日程”的意图，可以为此制定一些特殊的逻辑处理对应的开放域内容，所以难度还是比真·开放域任务要低一些的。</p>
<p>指令类任务的NLU处理方式相对比较straightforward，几乎所有的指令都可以通过intent
+
slot的方式来确定用户的唯一意图。比如“明天早上八点叫我起床”的intent就是“设定闹钟”，而slot值就是“八点”，通过intent和slot值我们就可以确定需要进行什么操作。（部分intent不需要slot值，比如”打开窗帘“；而部分intent需要多个slot值，比如“删除早上8点以前的重复闹钟”，就有两个条件“早上8点以前”和“重复”；不过不同的系统对intent和slot的定义并不完全相同，比如“打开窗帘”也可以把“窗帘”看做是slot值；细节就暂不展开了，以后有机会再单独开篇讨论）</p>
<p>intent的范围是有限的，因为如果设备不支持对应的操作，那么这条指令自然就是无效的。因此intent的识别可以建模成一个分类任务。类似地，每个intent下的slot提取可以建模成一个NER任务。</p>
<p>一个智能助手需要支持成千上万的intent，因此intent的分类一般会使用多级分类体系，降低训练难度。比如query“明天早上八点叫我起床”，一级分类会分到“闹钟”，之后再分到二级分类“设定新闹钟”。</p>
<p>总之，对于指令类任务，NLU所做的事情可以简单拆分为intent分类 +
NER识别slot值。</p>
<p>2、知识类</p>
<p>知识类任务主要目的是帮助用户解决信息获取的问题，主要包括实时信息如“美国加州现在几点”和“港币兑人民币汇率多少”，以及一些冷门或者领域属性较强的知识，比如“偷窃5000元以上一般怎么判刑”，或者“梅西和C罗分别有几次欧冠冠军”这样对于大部分人来说需要查资料才能回答的问题。</p>
<p>知识类的任务相对指令类的任务更加开放，基本上不太可能把用户所有可能问到的内容整理成分类体系，因此会在基础分类（是否问知识类、是否问时效性相关问题）之上，使用知识库匹配的方式：通过QQ或者QA匹配，再附以后校验的方式回答用户问题。</p>
<p>对于时效性问题也会涉及到接口调用和结果的处理等，从今天的角度看可以认为是一个简化版本的agent
+ RAG。</p>
<p>整体上，知识类的任务会用到分类、NER以及相似度/相关性匹配的能力。</p>
<p>3、聊天类</p>
<p>在ChatGPT出来之前，聊天机器人做得其实并不好，包括当时很火的小冰在内，都会大量使用人工设计的模板、路径、状态机等来维护聊天状态。</p>
<p>在聊天这样的完全开放域场景，由于生成的内容很不可控，也很容易出现风险（bot的暴论），因此当时很大一部分内容还是会使用QA匹配式的方案，所以多轮能力很弱。</p>
<p>业界有些方案会把聊天话题拿出来分析，对于用户经常聊到的话题训练专门的“领域对话模型”，比如专门聊“情感”类的，或者专门聊“体育运动”类的垂域聊天模型。</p>
<p>总之，聊天类单独拉出来是因为理论上来说，它应该是一个用模型几乎全自动完成聊天的能力，虽然最终的实现方式还是插入了很多人为控制的环节。</p>
<p>（业界很多人，包括我们，从很早就开始探索让模型自己把控整个对话的做法，但是确实受模型能力所限，当时的使用范围有限）</p>
<p>4、客服类</p>
<p>不是说智能助手的能力分为3大类吗，怎么冒出来个第4类？</p>
<p>从用户输入特点和解决方案上，客服类实际上是前三种的融合版本，不过客服在很多应用上重要性都比较高，，所以这里单独再拉出来说下。</p>
<p>总结一下，上面提到的智能助手的这些能力，从算法模型角度看，主要是通过分类、NER和相似/相关度计算能力来支持。</p>
<p>（实际上整个智能系统内还有很多其他工作，包括输入端的语音+语义的拒绝识别，纠正由于语音输入带来的错别字，还有用户个性化设置相关的特别响应等）</p>
<h2 id="体系划分">体系划分</h2>
<p>如前面提到的，智能助手要支持的intent很多，轻易就超过一千个，如果直接对这些intent由一个模型进行统一分类，会带来几个问题：<br>
- 很容易出现intent分布的不平衡，增加训练难度<br>
-
在修复错误case，或者定期更新模型的时候，如果部分intent的数据有所改变，就会对所有其他intent的结果产生影响，导致出现新的问题，这样显然是不合理的</p>
<p>因此，会采用多个子系统同时并行处理的方案，方便隔离和屏蔽部分intent更新带来的影响。每个子系统内部可能包含一个或者多个模型，用于对所负责的intent进行响应，如下图：</p>
<img src="/9c593ccd/system_1.png" class title="一个模型支持智能助手系统">
<p>子系统A只负责处理和闹钟相关的指令操作，比如新增闹钟、打开闹钟、关闭闹钟、删除闹钟等，在识别到相关intent之后进行NER获取slot值。而其他所有非闹钟操作相关的用户输入，都会被这个子系统统一拒绝，不给出任何处理结果。类似地，会有专门处理iot设备操作的子系统、专门负责运动健康的子系统，以及专门处理知识问答相关的子系统等。</p>
<p>最后会把多个子系统的结果综合起来，通过后置的rank模块来获取最终结果。</p>
<p>对intent体系进行这样的划分之后，每个子系统内部的修改，比如分类模型更新等，不会直接影响到其他子系统，而能够将影响范围尽量限制在子系统内部，使得智能助手整体更加稳定，多人协作也容易一些。</p>
<p>不过，这样的体系划分也带来一些问题：<br>
-
随着系统覆盖范围的扩展，rank模块的压力会越来越大，比如多任务的情况，已经存在歧义的输入<br>
- 随着子系统的增多，模型用量也随之上升，使得计算成本增大</p>
<p>第一个问题先放下，看下第二个问题。</p>
<p>最早期的时候，分类能力主要是规则配合fasttext来实现。这个阶段计算量不大，只用cpu完全可以支持。</p>
<p>随着业务发展，在一些复杂的领域开始使用textcnn和lstm模型的各种变体。在输入长度不大的情况下（如&lt;=50
token），还是可以在几十毫秒内完成。</p>
<p>再后来，Bert出来之后显示出了一统NLU江湖的能力，我们自然也要用上。Bert模型的分类和NER能力效果确实不错，相比textcnn和lstm基本上都能有三五个点的提升，而且对困难任务提升有加成
-- 难度越大，Bert优势越大。</p>
<p>Bert效果是好，但是如果所有子系统都单独使用各自的Bert微调模型，那么计算压力会大很多倍，毕竟我们可能有几十个子系统，而大部分子系统内都包含多个模型。如果使用CPU推理的话，在高并发下Bert-base模型的p99耗时基本上可以轻易突破200ms，在对时延要求较高的场景下就需要上GPU推理了，而这又是一笔不小的成本。</p>
<p>直观上来说，这么强大的Bert模型只用来微调之后做一小块的分类或者NER工作，其实是有点浪费的，那么有什么办法可以减少浪费呢？</p>
<h1 id="基础方案">基础方案</h1>
<p>有人可能会想到可以使用adapter/LoRA这样的微调方式，然后让每个模型训练并保留自己的adapter，在使用的时候调用自己的adapter参数和主干Bert模型一起进行推理。</p>
<p>这样确实可以只保留一份主干模型，但是注意多个子系统是并行的，而adapter的结果会合并到主干网络中，因此没法做到“一次推理获得多个子系统结果”。实际总的推理计算量和使用单独微调全微调模型时是一样的。</p>
<p>这里我们参考的原型方案是小米的《A Flexible Multi-Task Model for BERT
Serving》中的做法。</p>
<p>小米的方案如下图所示：</p>
<img src="/9c593ccd/xiaomi_1.png" class title="一个模型支持智能助手系统">
<p>首先采用top-k层微调的方式获取每个task下的分类任务，然后把各个task下的教师模型蒸馏到层数更少的student模型上。训练teacher和student模型的时候，都保持使用Bert模型frozen的部分bottom层
+ 微调部分top层的方式。（这里Bert是12层的Bert-base模型）</p>
<p>这样最终获取的student模型相当于共享了同一个冻结的Bert模型的不同层输出，无论下游有多少个任务，这个共享的Bert模型都只要进行一次推理就行，各个下游任务模型则各取所需，在各层的中间输出结果上继续进行少量层的推理，获得最终结果。</p>
<p>小米论文中的几个细节：</p>
<p>1、第一步训练teacher
model时为什么不是全微调？因为对于不同的任务，有时候全微调未必就是最好的，只微调10层的效果可能比微调12层的更好。在不同任务下，不同层的微调效果如下表：</p>
<img src="/9c593ccd/xiaomi_2.png" class title="一个模型支持智能助手系统">
<p>论文中基于实验结果，把微调层数范围限制在[4,10]。</p>
<p>2、student模型和teacher模型所用的Bert frozen层数相同。</p>
<h1 id="模型设计">模型设计</h1>
<p>在小米方案的基础上，我们做了几点改变。</p>
<p>首先就是基模型，也就是我们BFM主干模型的设计。</p>
<p>使用BFM +
部分层微调方案的一大目标就是节省推理计算量，因此我们想要微调的层数越少越好。</p>
<p>假设BFM总共有12层，共有10个下游模型，我们估算下一次推理总计算量随下游模型微调层数的变化情况。如果是全微调，那么推理时所需计算的层数为12
* 10 = 120层，BFM + 部分层微调的计算量如下表：</p>
<table>
<thead>
<tr class="header">
<th>下游模型平均微调层数</th>
<th>总计算量（层）</th>
<th>节省计算量（%）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>8层</td>
<td>12 + 8 * 10 = 92</td>
<td>23%</td>
</tr>
<tr class="even">
<td>7层</td>
<td>12 + 7 * 10 = 82</td>
<td>32%</td>
</tr>
<tr class="odd">
<td>6层</td>
<td>12 + 6 * 10 = 72</td>
<td>40%</td>
</tr>
<tr class="even">
<td>5层</td>
<td>12 + 5 * 10 = 62</td>
<td>48%</td>
</tr>
<tr class="odd">
<td>4层</td>
<td>12 + 4 * 10 = 52</td>
<td>56%</td>
</tr>
<tr class="even">
<td>3层</td>
<td>12 + 3 * 10 = 42</td>
<td>65%</td>
</tr>
<tr class="odd">
<td>2层</td>
<td>12 + 2 * 10 = 32</td>
<td>73%</td>
</tr>
<tr class="even">
<td>1层</td>
<td>12 + 1 * 10 = 22</td>
<td>82%</td>
</tr>
</tbody>
</table>
<p>减少推理总计算量的最直接方法就是限制下游模型微调层数。实际上我们确实也这么做了，限制了下游模型微调层数范围为[1，6]。实际上最终大部分的下游模型微调层数都&lt;=3层，效果就已经有明显的提升。</p>
<p>另外一个减少推理计算量的方法就是重新设计模型结构，把更多的参数放在frozen的层，而把可能微调的层参数量减小。我们的做法就是把Bert中靠近输入的第1~6层的hidden
size从768提升到1024，而把靠近输出的第7~12层（也是可以被用于微调的层）的hidden
size减小到512。</p>
<img src="/9c593ccd/bfm.png" class title="一个模型支持智能助手系统">
<p>由于hidden
size在第6层和第7层之间存在变化，所以中间会用一个bottleneck模块进行维度的转换。</p>
<p>这样的模型设计和苹果最近的OpenELM有些相似，不过OpenELM的做法看起来更加高级一点，通过一个缩放参数来设计模型每层的宽度。</p>
<p>这样模型的总参数和计算量保持基本不变，但用于微调的部分参数量则减少了。这样一来，对于只微调了一两层的任务，用CPU也可以在几十毫秒内完成推理了。</p>
<p>计算一下这样的模型结构在微调不同层时的计算量情况。方便起见，这里简单粗暴地把hidden
size = 512的层的计算量当做原Bert层计算量的一半：</p>
<table>
<thead>
<tr class="header">
<th>下游模型平均微调层数</th>
<th>总计算量（层）</th>
<th>节省计算量（%）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>6层</td>
<td>12 + 6 * 10 * 0.5 = 42</td>
<td>65%</td>
</tr>
<tr class="even">
<td>5层</td>
<td>12 + 5 * 10 * 0.5 = 37</td>
<td>69%</td>
</tr>
<tr class="odd">
<td>4层</td>
<td>12 + 4 * 10 * 0.5 = 32</td>
<td>73%</td>
</tr>
<tr class="even">
<td>3层</td>
<td>12 + 3 * 10 * 0.5 = 27</td>
<td>78%</td>
</tr>
<tr class="odd">
<td>2层</td>
<td>12 + 2 * 10 * 0.5 = 22</td>
<td>82%</td>
</tr>
<tr class="even">
<td>1层</td>
<td>12 + 1 * 10 * 0.5 = 17</td>
<td>85%</td>
</tr>
</tbody>
</table>
<h1 id="预训练">预训练</h1>
<p>重新设计了Bert模型结构之后，就需要进行预训练。我们选择了从随机初始化开始训练。</p>
<p>训练方式采用了RoBerta的MLM训练方式，没有NSP等其他任务。在RoBerta的while
word mask基础上，进一步增加了短语级别的mask，提升学习难度。</p>
<p>短语识别是在百度的LAC（Lexical Analysis of
Chinese）工具基础上开发的，通过分词、词性识别和一些人工设定的规则，可以找出一些边界较明显的短语。</p>
<p>数据上以开源数据为主，比如悟道的200G数据，CLUE的100G数据，以及维基百科和百度百科等。</p>
<p>当时在batch size、learning
rate和optimizer等各种参数上也做了很多实验，参考了很多业界的先进论文，现在回想起来，最重要的一条经验就是：没有开源可复现代码的工作不要轻易相信。有太多的改进和设置建议最终都会让训练loss起飞，一去不复返。</p>
<p>最终得到的预训练模型在常规的分类和NER任务上效果都比RoBerta更好一些，但是在句子相似度任务上效果比较一般，想来想要提升相似度任务效果，还是需要在预训练中显式地加入对比学习任务。</p>
<h1 id="微调">微调</h1>
<p>微调上也应用了不少技术来提升效果。</p>
<h2 id="领域继续预训练">领域继续预训练</h2>
<p>首先是继续预训练，包括domain-adaptive
pretraining（DAPT）和task-adaptive pretraining（TAPT）。</p>
<p>TAPT就是把本来要用来微调的数据先用来做继续预训练，而DAPT则是扩大范围，把和微调任务相同domain的数据用来做继续预训练。比如所有操作相关的指令就和“设定闹钟”这个任务是属于同一个domain的。</p>
<p>虽然叫继续预训练，但我们用到的数据规模要比预训练小很多，基本上都在1B以下，所以这个阶段还是更贴近微调一些。</p>
<p>关于DAPT和TAPT，更详细的内容可以参考《Don't Stop Pretraining: Adapt
Language Models to Domains and Tasks》。</p>
<p>从论文中的实验结果和我们自己的实验结果来看，任务/领域的继续预训练基本上总是有正收益的，收益的大小则取决于下游任务的难度，以及继续预训练所用的数据量和数据内容。</p>
<p>每个下游任务的模型都会进行自己的继续预训练，因此这里的继续预训练只会修改将用于微调的层。</p>
<p>继续预训练中参考《Training Language Models with Memory
Augmentation》，通过内存增强提升模型效果，基本能有比较稳定的收益。</p>
<h2 id="task-specific-embedding">task-specific embedding</h2>
<p>对于继续预训练数据量较大，而改动层数较少的任务，由于参数量的限制，可能使得继续预训练的效果不明显，为此我们专门设计task-specific
embedding，作为微调层的旁路分支加入模型，以提升可训练的数据量。</p>
<p>task-specific embedding的设计如下图所示：</p>
<img src="/9c593ccd/task_emb.png" class title="一个模型支持智能助手系统">
<p>由于embedding层的操作很快，所以虽然参数量增大了不少，但是基本上并不影响下游模型的推理时延。</p>
<p>task-specific embedding的做法有点像在Bert中加入一个fasttext。</p>
<h2 id="蒸馏">蒸馏</h2>
<p>和小米的方案一样，我们也使用蒸馏提升最终下游模型的效果。在实现上和小米有几点区别：<br>
-
我们的teacher模型不限制微调的层数，实际上大部分teacher模型都是使用全微调训练得来的<br>
- student模型的微调层 +
frozen层总是保持12；这主要是因为我们已经把微调层的范围限制得比较小了，因此也没有比较再对微调层数进行进一步的压缩</p>
<p>蒸馏的时候，student模型除了学习teacher模型的logits，同时也会学习ground
truth。两个loss按一定的权重相加。</p>
<h2 id="实用微调技巧">实用微调技巧</h2>
<p>一些实用的微调技巧：</p>
<p>1、FGM（Fast Gradient Method）</p>
<p>对抗学习在某些场景下能够获得一定的效果提升：<br>
- 超参不好调的情况下，对抗学习基本上能够获得一个平均值以上的结果<br>
- 训练数据量较少的情况下，对抗学习能够提升模型的泛化性</p>
<p>而如果训练数据量足够多，超参也比较成熟的话，对抗学习就不太有收益。而且FGM由于要计算对抗样本，训练速度要慢一半。</p>
<p>2、multi-sample dropout</p>
<p>multi-sample
dropout通过使用多次不同的dropout而要求模型输出稳定的结果来提升模型的泛化性，且实现和训练都比较方便。</p>
<p>3、r-drop</p>
<p>和multi-sample
dropout类似，r-drop也是把两次dropout后增加一个输出概率分布的KL散度作为auxiliary
loss加到训练中，来提升模型的稳定性和泛化性。</p>
<p>也有一些试验过但没什么效果的方案，比如《NoisyTune: A Little Noise Can
Help You Finetune Pretrained Language Models
Better》，不知道是否是打开方式不对。</p>
<p>以上的方案都做成了可配置的形式。为了进一步方便微调，用超参微调框架optuna把batch
size、learning
rate以及上面这些方案都打包起来，可一键启动任务，自动进行超参搜索，把一个任务的微调人力投入压缩到分钟级别：只要配置好数据、参数范围就可以等拿最佳结果了。</p>
<h1 id="工程">工程</h1>
<p>1、服务</p>
<p>BFM模型部署位GPU服务，对所有输入统一计算出后6层的中间输出，并写入临时缓存（1分钟后就会释放）；下游服务在需要某层的中间输出是，会去缓存进行查询，获取结果。</p>
<p>由于BFM后6层的维度减小了，相比原Bert模型，所需的缓存和传输量也更少。</p>
<p>2、时延</p>
<p>通过onnx +
fp16，在推理GPU上，下游模型基本上能把p99控制在10ms以内，如果不想用GPU，在CPU上也能在几十毫秒的时间内完成2层模型的推理。</p>
<p>关于onnx有一个小坑，使用onnx进行fp16下的算子优化之后，模型的输出结果可能会有所变化。对于分类模型大概在logits的数值上有1%以内的difference，这个影响不大；但是在NER任务上对最终实体准确率的影响却恨到，甚至可以达到5%的差异，这基本上磨平了从lstm到Bert的提升了。</p>
<h1 id="效果">效果</h1>
<p>BFM模型为下游的多个分类和NER模型提供了服务，相比fasttext/textcnn/lstm平均能有3%的提升。</p>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】A Flexible Multi-Task Model for BERT Serving
https://arxiv.org/pdf/2107.05377<br>
【2】Don't Stop Pretraining: Adapt Language Models to Domains and Tasks
https://arxiv.org/abs/2004.10964<br>
【3】OpenELM: An Efficient Language Model Family with Open Training and
Inference Framework https://arxiv.org/abs/2404.14619<br>
【4】Apple Intelligence Foundation Language Models
https://arxiv.org/pdf/2407.21075<br>
【5】Stochastic Weight Averaging in PyTorch
https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/<br>
【6】Multi-Sample Dropout for Accelerated Training and Better
Generalization https://arxiv.org/abs/1905.09788<br>
【7】Adversarial Training Methods for Semi-Supervised Text
Classification https://arxiv.org/abs/1605.07725<br>
【8】Training Language Models with Memory Augmentation
https://aclanthology.org/2022.emnlp-main.382.pdf<br>
【9】R-Drop: Regularized Dropout for Neural Networks
https://arxiv.org/abs/2106.14448<br>
【10】NoisyTune: A Little Noise Can Help You Finetune Pretrained
Language Models Better https://arxiv.org/abs/2202.12024</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>Bert</tag>
        <tag>智能助手</tag>
      </tags>
  </entry>
  <entry>
    <title>适合移动设备的语言模型--MobileLLM</title>
    <url>/5ac36d34.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>移动设备对端侧模型的需求日益显现，语言模型先做大后做小的趋势和之前CNN的发展历程相似。Meta提出的MobileLLM就是专门为移动设备而生，主要是125M和350M规模的模型。（让人想起七八年前的MobileNet）</p>
<img src="/5ac36d34/mobilellm.png" class title="mobilellm">
<h1 id="背景">背景</h1>
<p>大模型在生活中使用的频率越来越高，以后可能会占据到每个人每天活动时间的5%。按这个使用量，以GPT-4为准，可能需要1亿个H100才能支持推理所需的算力，这显然是不现实的。</p>
<p>另外，对于在端侧使用的模型，虽然Llama-2-7B在4-bit量化之后可以运行到手机上了，但是按0.1J/token
per million parameters的能耗计算（《Eie: Efficient inference engine on
compressed deep neural network》，《Towards energyproportional
datacenter memory with mobile dram》），搭载4-bit
Llama-2-7B的iPhone，电池大约只能支持2小时的模型推理使用。而如果使用125M或者350M这样规模的模型，则电量足够一整天的使用。</p>
<p>而运行模型所需的空间对于移动设备也是个必须要考虑的问题。按下图的典型设备的memory
hierarchy，7B规模的模型会占据大部分的空间，这会明显影响其他app和系统的运行。</p>
<img src="/5ac36d34/device.png" class title="mobilellm">
<p>基于这些考虑，Meta把模型目标规模定在1B以下。</p>
<h1 id="结构设计">结构设计</h1>
<p>小规模模型参数有限，怎么把这些有限的参数分配给不同的部分，以获得最好的效果，是核心问题。</p>
<p>在标准transformer
decoder的基础上，MobileLLM在结构设计上主要有4个设计：<br>
- 1、deep and thin的结构设计<br>
- 2、embedding sharing<br>
- 3、使用SwiGLU<br>
- 4、GQA</p>
<p>各项设计下，模型在zero-shot common sense reasoning任务的效果如下</p>
<img src="/5ac36d34/structure.png" class title="mobilellm">
<p>更详细的数据如下表</p>
<img src="/5ac36d34/structure_ablation.png" class title="mobilellm">
<p>下面看一下各项改进的分析。</p>
<h2 id="depth-vs-width">depth vs width</h2>
<p>在保持总参数量基本不变的条件下，设计不同层数的模型比较效果，结果如下</p>
<img src="/5ac36d34/deep.png" class title="mobilellm">
<p>更详细的数据如下表</p>
<img src="/5ac36d34/deep_ablation.png" class title="mobilellm">
<p>整体上，层数更多的模型在同样总参数量下，在多个zero-shot评测上都有较好的效果。基本上30层左右的模型就能得到一个比较好的效果了，而10层以下的模型对复杂问题明显较差。</p>
<p>而目前大部分1B以下的模型层数都在20层以下，这可能限制了1B以下模型的潜力发挥。</p>
<h2 id="embedding-sharing">embedding sharing</h2>
<p>对于比较大的模型，embedding占总参数量的比例比较低，比如Llama-7B是3.7%，而Llama-70B更是只有0.7%。因此对于这些模型，embedding
sharing并不能在参数效率上带来多少好处，反而会因为共享参数对效果有损害。</p>
<p>但是对于小规模的模型就不一样了。对于125M参数的模型，embedding的参数量甚至能占到总参数量的20%（embedding
dimension = 512，vocab = 32k）。因此是否使用embedding
sharing在小模型这里是一个需要重新考虑的事情。</p>
<p>用30层的135M模型做实验，如下表所示：共享embedding减少了16M的参数量，同时带来了0.2的平均效果损失；把由于embedding共享减少的参数部分加上模型层数上去，从30层提升到32层，模型的参数量恢复到125M（仍然比135M小），而效果则提升了0.4。</p>
<img src="/5ac36d34/emb.png" class title="mobilellm">
<p>也就是说独立的embedding参数的参数效率不如增加模型层数，因此这样的参数置换是划算。</p>
<h2 id="gqa">GQA</h2>
<p>每个注意力头的大小要多大？</p>
<blockquote>
<p>The trade-off between more semantics per head dimension and more
non-linear combinations of multiple heads is a key consideration in
choosing the head size.</p>
</blockquote>
<p>此外，对于大模型来说，GQA的主要作用是减少推理时所需的KV
cache，而对于小模型来说，GQA也是节省参数量的一个手段。</p>
<p>改变head size和kv head的数量，125M和350M上的实验结果如下表</p>
<img src="/5ac36d34/head.png" class title="mobilellm">
<p>从实验结果来看，16个query head的效果是比较好的。而kv
head的数量为4的时候，350M模型的效果损失是0.2，但是参数规模能减少10%左右。</p>
<h2 id="layer-sharing">Layer Sharing</h2>
<p>除了以上的结构设计以外，Meta还实验了layer
sharing的效果，即对模型一个层的参数重复使用，在不增加总参数量的情况下，通过提升计算量增加模型复杂度。</p>
<p>这和Albert的做法是一样的，某种程度上算是耍赖，这样的设计和不使用layer
sharing的模型比较是不公平的。使用了layer
sharing的系列模型和没有使用的就分开来了，单独命名为MobileLLM-LS。</p>
<p>文中提出三种layer sharing的方式，如下图：</p>
<img src="/5ac36d34/share.png" class title="mobilellm">
<p>三种方式的效果如下表：</p>
<img src="/5ac36d34/share_2.png" class title="mobilellm">
<p>从实验结果来看是repeat-all-over的方式最好，不过最后Meta选择使用immediate
block-wise共享的方式，因为这种方式有个好处：相邻层参数共享，可以减少设备SRAM加载数据的次数，从而提高推理速度。</p>
<p>此外，对block-wise共享的次数消融实验结果如下：</p>
<img src="/5ac36d34/repeat.png" class title="mobilellm">
<p>随着共享次数增多，收益逐渐减小，因此选择使用repeat×2的方案。</p>
<h1 id="实验">实验</h1>
<h2 id="效果对比">效果对比</h2>
<p>125M和350M这两个主力规模的模型和其他相近规模模型效果对比如下：</p>
<img src="/5ac36d34/result.png" class title="mobilellm">
<p>看起来有比较明显的提升。不过所对比的模型很多都不是最新一代的了（当然也是因为最小模型的人少了），还需要更全面测试一下。</p>
<h2 id="scale-up">scale up</h2>
<p>为了看这个模型设计的方案在更大一些的参数上是否也有效，Meta在几个规模稍大一些的模型进行实验：</p>
<img src="/5ac36d34/model.png" class title="mobilellm">
<p>结果如下：</p>
<img src="/5ac36d34/zero_shot.png" class title="mobilellm">
<p>基本上在1B左右，这个设计还能保持比较好的效果。</p>
<h2 id="蒸馏">蒸馏</h2>
<p>Meta试了把Llama-2-7B作为教师模型，给两个小规模模型进行蒸馏。</p>
<p>但是从结果上看，蒸馏在效果上没有什么收益，而收敛时间还更长了：</p>
<img src="/5ac36d34/kd.png" class title="mobilellm">
<h1 id="小结">小结</h1>
<ul>
<li>端侧模型需求确实越来越大，最近苹果也已经出招，其他家要跟进，推理效率是最重要的问题之一<br>
</li>
<li>业界从scaling
law的大力出奇迹，慢慢又回到精细雕花的阶段，是否预示着，下一个潮流正在酝酿？</li>
</ul>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】MobileLLM: Optimizing Sub-billion Parameter Language Models for
On-Device Use Cases https://arxiv.org/abs/2402.14905</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>Meta</tag>
        <tag>端侧模型</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态入门--CLIP</title>
    <url>/3069051d.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>放假了，小小水一篇多模态的经典之作，CLIP。</p>
<p>论文：《Learning Transferable Visual Models From Natural Language
Supervision》</p>
<p>时间：2021年3月</p>
<p>机构：OpenAI</p>
<p>又是Ilya参与的一个工作。</p>
<p>CLIP = Contrastive Language-Image
Pre-training，顾名思义，这是一个基于对比学习的语言图像多模态学习方法。CLIP训练的目的其实主要还是获得通用的图像表征模型，因此在CLIP框架里，语言数据可以认为是作为监督信号存在的，类似图像分类任务中的类别信号，只是从一个one
hot
label扩展成了自然语言的形式。使用自然语言作为监督信号的好处是，自然语言信号更加灵活，可以支持扩展到zero-shot的推理，并且能够提供更加丰富的监督信息。</p>
<h1 id="数据">数据</h1>
<p>其实在CLIP之前就有好些多模态训练的工作，但是效果没有这么好，原因主要是数据量不够大，另外就是对自然语言数据使用不够好，未能充分发挥自然语言的作用。因此一个很重要的工作就是构建数据集。CLIP是这么干的：<br>
- 以英文维基百科中出现至少 100
次的所有单词为基础构建词集，并增加了双词组合和所有 WordNet 同义词<br>
- 爬取网上的数据，试（图像，文本）数据对中的文本包含词集中的一个词<br>
- 为了尽可能覆盖广泛的视觉概念，对结果进行平衡，每个概念最多包括 20,000
个（图像，文本）对<br>
- 构建的 WIT（WebImageText） 数据集包含 4 亿个（图像，文本）对</p>
<p>WIT数据集比之前很多多模态数据集都大，包含的内容也更丰富。</p>
<h1 id="训练框架">训练框架</h1>
<p>CLIP预训练框架如下图：</p>
<img src="/3069051d/contrastive_pt.png" class title="CLIP">
<p>text encoder和image encoder分别对文本和图像进行编码。text
encoder通过对比学习，把文本的表征向match的图像靠拢，而和batch内其他图像，也就是负样本的距离尽量拉大。image
encoder也是同样地学习图像表征。</p>
<p>训练的pseudo-code如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># image_encoder - ResNet or Vision Transformer</span></span><br><span class="line"><span class="comment"># text_encoder - CBOW or Text Transformer</span></span><br><span class="line"><span class="comment"># I[n, h, w, c] - minibatch of aligned images</span></span><br><span class="line"><span class="comment"># T[n, l] - minibatch of aligned texts</span></span><br><span class="line"><span class="comment"># W_i[d_i, d_e] - learned proj of image to embed</span></span><br><span class="line"><span class="comment"># W_t[d_t, d_e] - learned proj of text to embed</span></span><br><span class="line"><span class="comment"># t - learned temperature parameter</span></span><br><span class="line"><span class="comment"># extract feature representations of each modality</span></span><br><span class="line">I_f = image_encoder(I) <span class="comment">#[n, d_i]</span></span><br><span class="line">T_f = text_encoder(T) <span class="comment">#[n, d_t]</span></span><br><span class="line"><span class="comment"># joint multimodal embedding [n, d_e]</span></span><br><span class="line">I_e = l2_normalize(np.dot(I_f, W_i), axis=<span class="number">1</span>)</span><br><span class="line">T_e = l2_normalize(np.dot(T_f, W_t), axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># scaled pairwise cosine similarities [n, n]</span></span><br><span class="line">logits = np.dot(I_e, T_e.T) * np.exp(t)</span><br><span class="line"><span class="comment"># symmetric loss function</span></span><br><span class="line">labels = np.arange(n)</span><br><span class="line">loss_i = cross_entropy_loss(logits, labels, axis=<span class="number">0</span>)</span><br><span class="line">loss_t = cross_entropy_loss(logits, labels, axis=<span class="number">1</span>)</span><br><span class="line">loss = (loss_i + loss_t)/<span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>在clip的训练框架中，text encoder和image encoder的地位是对称的。</p>
<p>和之前的对比学习一样，为了提升学习的效果，负样本需要尽量多，因此实验中使用32,768的batch
size。</p>
<p>理论上，text encoder和image
encoder可以是任意模型。OpenAI选择了ResNet/EfficientNet-style的模型和几个ViT（ViT-B/32、ViT-B/16、ViT-L/14）作为image
encoder进行实验，而text encoder则是使用GPT-2的结构，最后一层的 [EOS]
token 就作为text representation。</p>
<p>训练中，image encoder和text
encoder都是随机初始化的，不需要预先训练。</p>
<h1 id="使用">使用</h1>
<p>完成预训练之后，一个常规的用法是基于image
encoder进行微调，包括仅训练classifier，和完整模型的训练。</p>
<p>CLIP另一个强项就是可以做zero-shot
predictor。比如我们想要知道让预训练模型对一张图片的类别进行预测，可以把所有可能的类别填进一个prompt里：“A
photo of {object}”，然后让text
encoder给出所有representation，并计算不同类别下的text
representation和image
representation的相似度，取最高的那个就是预测结果了：</p>
<img src="/3069051d/clip_infer.png" class title="CLIP">
<p>当然CLIP的用法不仅是可以做zero-shot的图像分类，后续还有很多其他应用方法，挖个坑后面来填。</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】Learning Transferable Visual Models From Natural Language
Supervision https://arxiv.org/abs/2103.00020</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>transformer</tag>
        <tag>预训练</tag>
        <tag>多模态</tag>
        <tag>CV</tag>
        <tag>CNN</tag>
        <tag>无监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</title>
    <url>/e00debee.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>这篇主要包括CogVLM，VILA，MM1，MM1.5和Pixtral-12B。</p>
<h1 id="cogvlm">CogVLM</h1>
<p>论文：《CogVLM: Visual Expert for Pretrained Language Models》</p>
<p>时间：2023年11月</p>
<p>在这之前的VLM工作大致有两个做法：</p>
<ul>
<li>浅对齐 shallow
alignment：即只训练对齐部分的参数，比如InstructBLIP和MiniGPT-4；这样带来的问题是，LLM中缺乏很多图像领域的概念，因此会出现很多无法对齐的情况，导致效果不好<br>
</li>
<li>训练LLM：比如Qwen-VL，在预训练或者SFT阶段训练LLM，这样做的话多模态的效果明显会比浅对齐好些，但是带来的问题是在语言能力上却会受到较大的损害，如下图：</li>
</ul>
<img src="/e00debee/cogvlm_loss_and_nlp.png" class title="多模态入门">
<p>CogVLM想要做的是，既能更好对齐图文空间，又能不损害LLM的语言能力。</p>
<h2 id="模型结构">模型结构</h2>
<p>为次CogVLM提出了这样的结构：</p>
<img src="/e00debee/cogvlm_archi.png" class title="多模态入门">
<p>注意只有紫色部分的参数才会参与训练，其他参数是冻结的。</p>
<p>包含4个部分：</p>
<ul>
<li>ViT
encoder：这个比较常规，使用EVA2-CLIP-E的ViT模型，去掉了最后一层，因为最后一层在原来的训练里是用于对比学习的<br>
</li>
<li>MLP adapter：这个也是常规做法，两层的MLP（SwiGLU）<br>
</li>
<li>LLM：使用Vicuna1.5-7B<br>
</li>
<li>Visual expert
module：这个是CogVLM设计里的核心部分，在LLM中的每一层加入用于处理图像部分信息的attention和MLP层，参数大小和LLM中的对应模块一致，参数初始化也是从LLM中来的；visual
expert
module让模型在处理文本信息和图像信息的时候可以分开来做，处理完后再合并，这样既能保持LLM的语言能力，又能加强图文模态的交互</li>
</ul>
<p>这里一个细节是，图像token全部共享同一个位置编码。</p>
<h2 id="训练">训练</h2>
<p>1、预训练</p>
<p>使用的数据是LAION-2B和COYO-700M，在经过NSFW、政治敏感和图像比例等规则的清洗之后保留了1.5B条数据。CogVLM还另外搞了40M的图像grounding数据加入这一阶段。</p>
<p>预训练分为两个阶段，第一阶段是caption的训练，而第二阶段是image
captioning 和 Referring Expression
Comprehension(REC)两个训练目标混合。</p>
<p>REC是在给定对象的文本描述的情况下预测图像中边界框的任务，以 VQA
的形式进行训练，BP的时候只使用answer部分的loss。在最后的3w个step中，将输入分辨率从224×224提升为490×490。</p>
<p>2、对齐</p>
<p>对齐阶段分别训练CogVLM-Chat和CogVLM-Grounding两个模型。</p>
<p>CogVLM-Chat注重通用性，整合多种任务的多个数据集。而CogVLM-Grounding则是转为模型grounding能力而训练，包含Grounded
Captioning (GC)、Referring Expression Generation (REG)、Referring
Expression Comprehension (REC)和Grounded Visual Question An-swering
(GroundedVQA)四类任务。</p>
<p>针对不同的改进，消融实验的结果如下：</p>
<img src="/e00debee/cogvlm_ablation.png" class title="多模态入门">
<h1 id="vila">VILA</h1>
<p>论文：《VILA: On Pre-training for Visual Language Models》</p>
<p>时间：2023年12月</p>
<p>机构：英伟达，MIT</p>
<p>VILA = VIsual LAnguage</p>
<p>VILA的模型架构比较常规：</p>
<img src="/e00debee/vila_archi.png" class title="多模态入门">
<p>和CogVLM有些类似，VILA主要有几点发现：</p>
<ul>
<li>LLM的训练对效果提升很重要，全程冻结不行<br>
</li>
<li>交错的图文数据更好，单纯的图文对数据不够<br>
</li>
<li>在instruction
tuning的时候混合纯文本数据不仅能恢复LLM的语言能力，对VLM任务的效果也是有益的</li>
</ul>
<p>针对这几点做了实验。</p>
<p>1、LLM训练</p>
<p>（1）Fine-tuning vs. prompt tuning</p>
<p>prompt
tuning在训练的时候冻结LLM，可以防止LLM的语言能力退化，但是也阻碍了图文空间的深度对齐。对此做了一系列实验：</p>
<img src="/e00debee/vila_llm_freeze.png" class title="多模态入门">
<p>结论是：</p>
<ul>
<li>SFT时仅训练projector效果不好<br>
</li>
<li>预训练时冻结LLM不会影响zero-shot能力，但会影响in-context
learning的能力<br>
</li>
<li>使用参数更少更简单的projector效果更好，猜测原因是这样会迫使LLM更多学习关于图像的知识</li>
</ul>
<p>（2）The deep embedding alignment hypothesis</p>
<p>为什么fine-tune
LLM效果好？一个假设是对齐图像和文本的分布很重要。为了验证这个猜想，计算了每层中图像和文本embedding的相似度：</p>
<img src="/e00debee/vila_align_simi.png" class title="多模态入门">
<p>可以看到使用linear层 + LLM微调的相似度更高，特别是在更深的层中。</p>
<p>2、图文交错的预训练</p>
<p>找了两个数据集，一个是图文交错的数据集，一个是图文对数据集：</p>
<img src="/e00debee/vila_interleave_data.png" class title="多模态入门">
<p>为了避免是因为文本分布带来的影响，还从MMC4数据集了只保留了图文对信息构造出了MMC4-pair数据集。这几个数据集在相同的预训练+SFT流程下，效果如下：</p>
<img src="/e00debee/vila_interleava_data_2.png" class title="多模态入门">
<p>可以看到图文交错的属性确实带来了比较大的提升。</p>
<p>3、Recover LLM Degradation with Joint SFT</p>
<p>虽然图文交错数据比单纯图文对数据好，但是LLM的语言能力还是有5%左右的损失，为了挽回这部分损失，一个方式就是SFT时添加纯文本数据：</p>
<img src="/e00debee/vila_joint_train.png" class title="多模态入门">
<p>使用了来自FLAN的1M文本数据后，不仅能保持LLM的语言能力不下降，在图文能力上也略有提升。</p>
<h1 id="mm1">MM1</h1>
<p>论文：《MM1: Methods, Analysis &amp; Insights from Multimodal LLM
Pre-training》</p>
<p>时间：2024年3月</p>
<p>机构：苹果</p>
<p>苹果先在较小的模型上进行了实验，得出了一些能够知道模型设计和数据选择的结论，并把这些结论应用到更大的（30B、64B的MoE模型）的MLLM，获得预训练指标SOTA的模型MM1：</p>
<img src="/e00debee/mm1_intro.png" class title="多模态入门">
<img src="/e00debee/mm1_intro2.png" class title="多模态入门">
<h2 id="消融实验">消融实验</h2>
<p>为了构建强大的MLLM，需要在各个维度找到最佳的方案。训练中，三个最主要的维度分别是：<br>
- Architecture：包括不同的image encoder，以及把image
encoder和LLM连接的方式<br>
- Data：不同的数据，和它们的权重<br>
- Training
Procedure：包括多阶段的训练（多少个阶段，每个阶段干什么），以及超参</p>
<p>预训练是比较消耗资源的，因此搜索所有可能的排列组合显然是不合理的，因此这里的消融实验使用更简单的设置：从base
configuration出发，每次只改动一个维度。基础的配置如下：<br>
- Image Encoder: A ViT-L/14 model trained with a CLIP loss on DFN-5B and
VeCap-300M; images of size 336×336.<br>
- Vision-Language Connector: C-Abstractor with 144 image tokens.<br>
- Pre-training Data: A mix of captioned images (45%), interleaved
imagetext documents (45%), and text-only (10%) data.<br>
- Language Model: A 1.2B transformer decoder-only language model.</p>
<p>模型和数据的消融设置如下：</p>
<img src="/e00debee/mm1_ablations.png" class title="多模态入门">
<p>每次改动之后就需要评测改动的效果，这里使用的是一系列的zero-shot和few-shot(4-shot/8-shot)的captioning和VQA任务。</p>
<h3 id="architecture">Architecture</h3>
<p>Architecture主要关注：<br>
- 怎么预训练image encoder<br>
- 如何将image encoder和LLM对齐: VL connector的设置</p>
<p>1、关于image encoder</p>
<p>关于image encoder的预训练，各种实验和结果如下：</p>
<img src="/e00debee/mm1_exp.png" class title="多模态入门">
<p>基于这个表格，有一些观察发现：<br>
- 提升图像分辨率对效果提升显著，从224提升到336，各指标提升~3%<br>
-
提升模型大小通常也有效果提升，从ViT-L到ViT-H基本能带来1%的zero-shot提升<br>
- 而对比contrastive loss和reconstructive
loss，差异则没那么显著；从表上看contrastive
loss略好一些，但是由于数据不完全相同，所以可能有部分性能差异是来自数据的</p>
<p>2、关于VL connector</p>
<p>对比使用不同image size（224、336）和token数（64、144），以及VL
connector类型（average pooling、attention pooling、convolutional
mapping）下的效果：</p>
<img src="/e00debee/mm1_vl_connector.png" class title="多模态入门">
<p>从结果上来看，图像size和token数影响比较大，而connector类型则相对影响较小。</p>
<h3 id="pre-training-data">Pre-training Data</h3>
<p>多模态预训练数据主要有2类：<br>
- captioning
data：成对的图文数据，通常较短，数据中的图片和文本信息有比较高的关联性<br>
-
图文交错的web数据：通常较长，相比captioning数据，其中的图文信息相关性较低</p>
<p>在上面两种主要数据之外，实验中还加入了纯文本数据，用于保留LLM的语言能力。实验所用的数据如下：</p>
<img src="/e00debee/mm1_ptm_data.png" class title="多模态入门">
<p>因为要关注MLLM的语言能力，因此这个实验的评测里还加入了纯语言的任务，包括ARC/PIQA/LAMBADA等一些常用语言任务。</p>
<p>基于这些数据所做的实验和结果如下：</p>
<img src="/e00debee/mm1_ptmdata_ablation.png" class title="多模态入门">
<ul>
<li>从图（a）可以看到，captioning数据对zero-shot效果更重要，而图文交错数据对few-shot和纯语言能力更重要<br>
</li>
<li>从图（b）可以看到，captioning数据结合纯文本数据有助于提升few-shot任务和文本任务的效果；而纯文本数据与图文交错数据结合虽会使得分略降，但能提升纯文本任务指标<br>
</li>
<li>从图（c）可以看到，captioning / 图文交错数据 / 纯文本数据比例为
5:5:1
时（橙色柱），能在保持较强纯文本理解能力的同时，实现比较好的多模态效果<br>
</li>
<li>从图（d）可以看到，合成captioning数据 VeCap
质量较高，虽占比较小（所有captioning数据的 7%），但在few-shot中可带来
2.4% 至 4% 的提升</li>
</ul>
<h2 id="最终预训练">最终预训练</h2>
<p>基于上面的消融实验，选择了这样的设置进行最终训练：<br>
- Image Encoder: Motivated by the importance of image resolution, we use
a ViT-H model with 378×378 resolution, pre-trained with a CLIP objective
on DFN-5B.<br>
- Vision-Language Connector: As the number of visual tokens is of
highest importance, we use a VL connector with 144 tokens. The actual
architecture seems to matter less, we opt for C-Abstractor.<br>
- Data: In order to maintain both zero- and few-shot performance, we use
the following careful mix of 45% interleaved image-text documents, 45%
imagetext pair documents, and 10% text-only documents.</p>
<p>为了获得更好的效果，模型规模也提升了，扩展到了3B、7B和30B。用已有的LLM和image
encoder初始化之后，进行200k步，约400B token数据的多模态预训练：<br>
- seq len = 4096<br>
- resolution = 378 * 378<br>
- batch size = 512<br>
- 所有参数unfrozen</p>
<p>在百亿参数规模进行超参搜索成本太高，因此这里在小尺度上（9M、85M、302M和1.2B）进行了网格搜索。</p>
<p>各个规模的MM1模型预训练评测：</p>
<img src="/e00debee/mm1_ptm_eval.png" class title="多模态入门">
<h2 id="sft">SFT</h2>
<p>更高的分辨率效果更好，为了支持更高的分辨率，SFT阶段使用了两种方法：<br>
- Positional embedding
interpolation：ViT在SFT过程中适应新的分辨率，让模型支持448 * 448、560 *
560和672 * 672的分辨率。这种情况下每个图像所用的token数提升很多。<br>
- Sub-image decomposition：如下图（a），比如输入图像的大小为1344 *
1344，那么会把它切分成4张672 * 672的图像，再加上downsample到672 *
672的原始图像，这样处理这5张图像比一次处理几千个图像token的attention就要高效。</p>
<img src="/e00debee/mm1_sft.png" class title="多模态入门">
<p>另外，从上图（b）可以看到，输入图像分辨率对效果的影响很大，1344是比较好的输入分辨率。</p>
<p>上图（c）则表明，随着预训练数据的增加，SFT的效果也在提升，很符合直觉的结果。</p>
<h1 id="mm1.5">MM1.5</h1>
<p>论文：《MM1.5: Methods, Analysis &amp; Insights from Multimodal LLM
Fine-tuning》</p>
<p>时间：2024年9月</p>
<p>机构：苹果</p>
<p>MM1.5是MM1的升级版，做了一些更加细致的实验和改进，使得模型能处理的任务更丰富一些，效果也更好：</p>
<img src="/e00debee/mm1.5_intro.png" class title="多模态入门">
<p>另外苹果还在通用MM1.5的基础上开发了专门针对视频的MM1.5-Video，以及专门针对mobile
UI的MM1.5-UI。这MM1.5-UI应该就是为iPhone以后的AI系统所做的一次尝试。</p>
<p>通用的MM1.5训练流程如下：</p>
<img src="/e00debee/mm1.5_train.png" class title="多模态入门">
<p>主要的优化点集中在：<br>
- Continual Pre-training<br>
- SFT<br>
- Dynamic High-resolution</p>
<h2 id="sft-1">SFT</h2>
<p>从论文篇幅和实验数来看，MM1.5有相当部分的成本是用在优化SFT上的。</p>
<p>首先，苹果收集了各种高质量的SFT数据，并把它们分成六类：</p>
<img src="/e00debee/mm1.5_sft_data.png" class title="多模态入门">
<p>1、Impact of Different Data Categories</p>
<p>为什么要分类，因为这些类型的数据在内容和形式上都有所不同，搞清楚各种数据之间的训练和效果是怎么影响的很重要。比如我们知道LLM的数学数据和代码数据能提升Agent能力，那么在面向Agent任务场景的开发中，就可以提升数学数据和代码数据的比例，以此获得更好的效果。</p>
<p>SFT数据类型的实验和效果如下：</p>
<img src="/e00debee/mm1.5_sft_exp.png" class title="多模态入门">
<p>从上图来看有几个结论：<br>
- 使用text-rich数据对text-rich和knowledge
benchmark的分数有提升；数学数据也有类似的趋势，只是程度比较小<br>
- 使用science data对knowledge
benchmark也有提升，另外也对text-rich任务有略微作用<br>
- 使用code data则只对text-rich任务有效<br>
-
添加refer&amp;ground数据对referring和grounding数据有提升，但是对其他类型的数据则略有损害</p>
<p>2、Data Mixture Ratio Study</p>
<p>了解了各种类型的数据的大致影响之后，下一步就是要找出具体的最佳配比。</p>
<p>首先是single-image的类型。这里用的指标是MMBase
score，即general、text-rich和knowledge三者的平均得分。以general类的数据量为基准，其他数据和它的比例设为
<span class="math inline">\(\alpha\)</span>；science、math、code以及refer&amp;ground数据在不同
<span class="math inline">\(\alpha\)</span> 下的表现如下图：</p>
<img src="/e00debee/mm1.5_single_image_alpha.png" class title="多模态入门">
<p>最终选择了图上红色x的比例作为最佳比例使用。</p>
<p>获得了single-image数据内部的比例之后，下一步就是研究single-image、multi-image和text-only这几个数据大类之间的比例了。</p>
<p>遍历三者的所有可能比例显然不可能，因此分开观察分别加入不同比例的text-only和multi-image数据对模型最终效果的影响，获得
<span class="math inline">\(w_{text}\)</span> 和 <span class="math inline">\(w_{multi}\)</span>，最终计算 <span class="math inline">\(1-w_{text}-w_{multi}\)</span>
获得single-image数据的比例。</p>
<p>实验的结果如下图：</p>
<img src="/e00debee/mm1.5_all_alpha.png" class title="多模态入门">
<p>观察到几个现象：<br>
- text-only数据对MMBase分数的影响不大<br>
-
增加multi-image数据会提升图像处理多图数据的能力，但是也会损害MMBase的分数，即对模型的基本多模态能力有影响</p>
<p>基于上面这些实验，文中给出三种mixture方案：<br>
- base
mixture：包括general、text-rich、science（α=0.1），math（α=0.5）和code（α=0.2）数据<br>
- single-image
mixture：在base的基础上，加上refer&amp;ground数据（α=2）<br>
- all
mixture：包括所有single-image（w=0.8）、multi-image（w=0.1）和text-only（w=0.1）数据</p>
<p>三种mixture方案的效果如下：</p>
<img src="/e00debee/mm1.5_mixture_exp.png" class title="多模态入门">
<h2 id="continual-pre-training">Continual Pre-training</h2>
<p>MM1.5比MM1多了一个高分辨率的继续预训练，这里的实验就研究了这个阶段一些设置的影响。</p>
<p>1、分辨率的影响</p>
<p>使用三种分辨率进行继续预训练，而保持其他所有设置相同，然后进行相同的SFT，效果随着分辨率提升而提升：</p>
<img src="/e00debee/mm1.5_resolution.png" class title="多模态入门">
<p>值得注意的是，这里并不是直接观察预训练模型的效果，而是SFT之后再进行评测。个人觉得这样更能贴近真实效果，毕竟实验中经常发现有一些预训练阶段的优势或者劣势并不会直接带到SFT模型里。当然这样成本和对SFT的要求都更高了。</p>
<p>2、OCR数据和合成caption数据的影响</p>
<p>使用了两个合成的caption数据集LLaVA-Recap-3M和ShareGPT4V-PT，以及OCR数据进行继续预训练实验，结果如下：</p>
<img src="/e00debee/mm1.5_syndata.png" class title="多模态入门">
<ul>
<li>所有的继续预训练效果都比不继续预训练好<br>
</li>
<li>加入合成数据并不能比简单的OCR数据带来更多的提升</li>
</ul>
<h2 id="pre-training">Pre-training</h2>
<p>一些知识密集型的任务，比如MMMU，对模型的文本理解能力要求比较高。一般来说，LLM在进行多模态预训练之后，都会有一定程度的文本理解能力损失。因此苹果整合了一批高质量的文本数据HQ-text，包含高质量的常识、数学和代码数据，旨在提高模型在语言方面的推理能力。</p>
<p>MM1中，caption、interleaved
image-text和text-only数据的比例为45:45:10，把文本数据替换成HQ-text，并把文本数据的比例提升之后，能获得更好的效果：</p>
<img src="/e00debee/mm1.5_text_data.png" class title="多模态入门">
<h2 id="dynamic-image-splitting">Dynamic Image Splitting</h2>
<p>动态数据分割在很多工作里其实都有使用了。</p>
<p>基本的考虑就是静态的图像分割固定把图片切分成4个子图，这样的效率并不太高：低分辨率的图片是可以不进行切分的，而比例比较异常的数据在切分之后可能会出现比较多的padding。</p>
<p>动态图像分割则是设置了子图数量的最小n_min和n_max，并对候选图像考虑所有候选的网格，只要有一个网格可以覆盖图像，就把网格的长边resize和和图像match，再对需要的区域进行padding：</p>
<img src="/e00debee/mm1.5_dynamic_split.png" class title="多模态入门">
<p>从实验上看，动态图像分割比静态的效果更好：</p>
<img src="/e00debee/mm1.5_ablation_1.png" class title="多模态入门">
<p>使用图像分割还有一个问题：resize后的原始图像时放在子图前面还是后面，如果放在前面，那么处理子图细节的时候，模型可以参考全局信息，而如果放在子图后面，那么模型在处理全局信息的时候就可以参考子图细节。</p>
<p>另外，子图的位置信息可以通过index或者prompt格式（sep）的方式输入给模型。</p>
<p>实验上看，子图位置信息的输入方式影响不大（前三行），而全局图摆放的位置则是放在后面略好一点点：</p>
<img src="/e00debee/mm1.5_after.png" class title="多模态入门">
<h2 id="final-model-and-training-recipe">Final Model and Training
Recipe</h2>
<p>最终MM1.5的训练设置： - 架构：采用与 MM1 相同的模型架构<br>
- 预训练：数据包括 2B 图像 - 文本对、600M 交错图像 - 文本文档（共 1B
图像）和 2T tokens 的纯文本数据。与 MM1
相比，除更新纯文本数据外，数据比例从 45:45:10 调整为
50:10:40，大幅降低交错数据比例（从 45% 到 10%），增加纯文本数据比例（从
10% 到 40%）<br>
- 继续预训练：使用 45M OCR
数据增强对文本丰富图像的理解，基于实验结果未包含额外合成图像字幕。 -
监督微调（SFT）：最终mixture包含 80% 单图像数据、10% 多图像数据和 10%
纯文本 SFT 数据。单图像数据可进一步分为 37.2% text-rich数据、22.5%
refer&amp;ground数据、11.3% 通用数据、5.6% 数学数据、2.3% 代码数据和
1.1% science数据<br>
-
动态高分辨率：设置n_min=4，n_max=9，只有输入的图像数量少于3张才会使用动态分割的策略，分辨率最大可以支持到4M
pixel，相当于2016 * 2016的图像大小</p>
<p>MM1.5和其他模型效果对比如下：</p>
<img src="/e00debee/mm1.5_eval.png" class title="多模态入门">
<h2 id="mm1.5-ui">MM1.5-UI</h2>
<p>大模型可以和使用者一样，看到并操作手机UI界面是一个能给开发者和使用者带来极大想象空间的能力：</p>
<img src="/e00debee/mm1.5_ui.png" class title="多模态入门">
<p>苹果通过在MM1.5的通用版本上进行further
finetune得到MM1.5-UI，使用的数据主要的Ferret-UI的SFT训练数据。</p>
<p>评测任务除了Ferret-UI中的评测集外，还有一些public
benchmark。评测结果如下：</p>
<img src="/e00debee/mm1.5_ui_eval.png" class title="多模态入门">
<p>MM1.5-UI规模较小的版本就有不错的效果。上表最后两行是做的一个消融实验：为了验证MM1.5通用模型SFT对UI任务的影响，这里用3B预训练模型直接进行UI相关任务的SFT，效果确实比先进行通用SFT要差一些，说明MM1.5的通用SFT对UI任务是有正面影响的。</p>
<h1 id="pixtral-12b">Pixtral 12B</h1>
<p>论文：《Pixtral 12B》</p>
<p>时间：2024年10月</p>
<p>机构：Mistral</p>
<p>Pixtral使用了Mistral Nemo
12B作为LLM，并且专门从头训练了一个名为PixtralViT的vision encoder。</p>
<img src="/e00debee/pixtral_vit.png" class title="多模态入门">
<p>PixtralViT的参数大小为400M，它相比其他ViT主要有几点改进：<br>
- Break
tokens：在一些情况下，不同长宽比的图像可以会有相同数量的patch，为了帮助模型区分这种情况，在每行patch后面添加一个特殊的token
[IMAGE BREAK]，而在最后一个patch后面添加[IMAGE END] token。可以认为Break
token的本质是辅助模型定位patch位置的一个标识符。<br>
- Gating in FFN：论文中的说法是不在attention
block里使用gating，而在hidden layer使用。<br>
- Sequence packing：图像的patch串起来，并用一个block-diagonal
mask保证不同图像之间的patch不会有leakage。<br>
-
RoPE-2D：原始的RoPE是给文本场景设计的，因此只有一个“距离”的概念，但是图像是二维的，单纯patch间的距离并不足以建模完整的patch位置信息，因此这里使用RoPE-2D。对于一个位置为i，j的图像patch
x，它的旋转矩阵是M：</p>
<p><span class="math display">\[RoPE-2D \left(x^{(i, j)},
\Theta\right)=M_{\Theta}^{(i, j)} x^{(i, j)}\]</span></p>
<p><span class="math display">\[M_{\Theta}^{(i,
j)}=\left(\begin{array}{ccccccc}
\cos i \theta_{1} &amp; -\sin i \theta_{1} &amp; 0 &amp; 0 &amp; \cdots
&amp; 0 &amp; 0 \\
\sin i \theta_{1} &amp; \cos i \theta_{1} &amp; 0 &amp; 0 &amp; \cdots
&amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \cos j \theta_{2} &amp; -\sin j \theta_{2} &amp; \cdots
&amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \sin j \theta_{2} &amp; \cos j \theta_{2} &amp; \cdots
&amp; 0 &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots
&amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \cos j \theta_{\frac{d}{2}}
&amp; -\sin j \theta_{\frac{d}{2}} \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \sin j \theta_{\frac{d}{2}}
&amp; \cos j \theta_{\frac{d}{2}}
\end{array}\right).\]</span></p>
<p>和RoPE-1D的情况相比，其实就把index为奇数的小旋转矩阵用于捕捉patch的高度距离，而把index为偶数的小旋转矩阵用于捕捉patch的宽度距离。</p>
<p>为了验证PixtralViT的有效性，把PixtralViT替换成其他ViT来观察效果的变化：</p>
<img src="/e00debee/pixtral_vit_abaltion.png" class title="多模态入门">
<p>从实验结果上看，PixtralViT确实有一些优势。</p>
<p>Pixtral 12B整体的架构如下：</p>
<img src="/e00debee/pixtral_archi.png" class title="多模态入门">
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】MM1: Methods, Analysis &amp; Insights from Multimodal LLM
Pre-training https://arxiv.org/abs/2403.09611<br>
【2】MM1.5: Methods, Analysis &amp; Insights from Multimodal LLM
Fine-tuning https://arxiv.org/abs/2409.20566<br>
【3】VILA: On Pre-training for Visual Language Models
https://arxiv.org/abs/2312.07533<br>
【4】Pixtral 12B https://arxiv.org/abs/2410.07073<br>
【5】CogVLM: Visual Expert for Pretrained Language Models
https://arxiv.org/abs/2311.03079</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>transformer</tag>
        <tag>预训练</tag>
        <tag>多模态</tag>
        <tag>CV</tag>
        <tag>CNN</tag>
        <tag>无监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态入门(五)--InternVL系列</title>
    <url>/52c8a4f9.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>最近InternVL2.5和Mini-InternVL-2相继发布。看了下，发现Intern模型在MLLM领域的相关工作还挺多的。</p>
<p>前面的文章已经讲解过InternLM-1/2/2.5，而在多模态入门(三)中，也学习了InternVL-1/1.5，这里就再继续学习下InternLM-XComposer系列、Mini-InternVL系列以及InternVL-2/2.5。</p>
<p>重点的内容应该是在InternVL-2.5部分，披露的细节比较多，模型的效果也最好。</p>
<h1 id="internlm-xcomposer">InternLM-XComposer</h1>
<p>论文：《InternLM-XComposer: A Vision-Language Large Model for
Advanced Text-image Comprehension and Composition》</p>
<p>时间：2023年9月</p>
<p>机构：上海AI Lab</p>
<p>InternLM-XComposer是基于InternLM开发的一个多模态模型。它的一个特点就是不仅能生成纯文本的结果，还能在这个基础上（通过搜索）配图，给出图文交错的文章。</p>
<h2 id="训练">训练</h2>
<p>1、模型结构</p>
<p>模型结构上，依然是包含三大常规部分：<br>
- vision encoder：使用EVA-CLIP，标准CLIP ViT的一个变体<br>
- perceive
sampler：用于把原始的257个token减少到64个，来和LLM对齐；初始化参数来自BLIP2；所有输入都会resize到224×224，patch
size=14<br>
- LLM：用InternLM初始化</p>
<p>2、训练</p>
<p>基于这个参数初始化后，训练包括预训练和SFT两个阶段。两个阶段的冻结和训练参数情况如下：</p>
<img src="/52c8a4f9/ixc_train.png" class title="多模态入门">
<p>可以看到整个过程中vision encoder都是保持冻结的，而perceive
sampler则是全程保持可训练。</p>
<p>文中对冻结哪部分参数做了消融实验：</p>
<img src="/52c8a4f9/ixc_freeze_ablation.png" class title="多模态入门">
<p>从实验结果看，冻结perceive
sampler会对最终模型的效果带来比较大的损失，因此最终选择在预训练和微调都保持perceive
sampler参数可训练。</p>
<p>3、数据</p>
<p>预训练数据如下，总共有1.1B张图片（相当于70.4B
token），77B文本token的数据：</p>
<img src="/52c8a4f9/ixc_ptm_data.png" class title="多模态入门">
<ul>
<li>包含了训练InternLM所用的文本数据，帮助模型在多模态训练过程中保持语言能力<br>
</li>
<li>in-house concept
data：这部分私有数据是从web数据中挖掘的，包含了大量visual
concept的优质数据，以及相关的detail explanation</li>
</ul>
<p>而SFT阶段包含了两大类数据，multi-task
training和instruction-tuning：</p>
<img src="/52c8a4f9/ixc_sft_data.png" class title="多模态入门">
<p>multi-task
training的数据都构造成多轮对话的格式。训练这些数据时，有一个特别的点是使用LoRA来微调，这样可以帮助模型保持原LLM的语言能力。</p>
<h2 id="interleaved-image-text-composition">Interleaved Image-Text
Composition</h2>
<p>前面说到InternLM-XComposer可以生成图文交错的文章，其方法主要分成三步：<br>
- （1）生成纯文本结果<br>
- （2）找到文本结果中可以/需要插入图像的位置，并生成对应的caption<br>
-
（3）用上一步的caption进行图像检索，获得候选，并选择最符合的图像，获得最终结果</p>
<img src="/52c8a4f9/ixc_inter_gen.png" class title="多模态入门">
<p>第（2）步中，训练数据使用真实的图文交错文章，并利用GPT-4获取caption；第（3）步中，图像的检索使用CLIP。</p>
<h1 id="internlm-xcomposer-2">InternLM-XComposer-2</h1>
<p>论文：《InternLM-XComposer2: Mastering Free-form Text-Image
Composition and Comprehension in Vision-Language Large Models》</p>
<p>时间：2024年1月</p>
<p>InternLM-XComposer2
7B模型下载：https://github.com/InternLM/InternLM-XComposer</p>
<h2 id="模型">模型</h2>
<p>InternLM-XComposer2认为，之前的多模态对齐方法主要有两大类：<br>
-
把图像token和文本token视为完全同等的存在，一视同仁地处理；这种方法忽视了各个模态之间的固有区别<br>
-
把图像tokne和文本tokne视为完全不同的存在，分别进行处理；这种方法导致对齐的成本大大增加</p>
<p>InternLM-XComposer2里则是提出了另外一种方法，partial
LoRA，来进行LLM和vision encoder的对齐：</p>
<img src="/52c8a4f9/ixc2_plora.png" class title="多模态入门">
<p>训练时模型的设置：</p>
<ul>
<li>LLM：基于InternLM-2-7B-ChatSFT初始化<br>
</li>
<li>Vision encoder：基于OpenAI CLIP
ViT-L-14-336初始化，并把分辨率提升到490x490以提升效果<br>
</li>
<li>Partial LoRA：rank256</li>
</ul>
<p>另外预训练中还使用了layer-wise LR，LR随层数递减，衰减系数为0.9。</p>
<h2 id="训练-1">训练</h2>
<p>1、预训练</p>
<p>预训练中，LLM的参数保持冻结，训练vision encoder和p-lora。</p>
<p>预训练的数据包含三个层级的目标：<br>
- general semantic
alignment：一般的通用对齐，让模型学习理解图像的基本内容，比如能知道一张爱因斯坦的照片包含“人类”这个概念；所用数据包括各种caption数据<br>
- world knowledge
alignment：包含知识，比如能够知道图像上的人是爱因斯坦，是物理学家；这里使用InternLM-XComposer中构建的概念数据集<br>
- vision capability
enhancement：高级的能力，比如OCR、grounding以及结构化信息的识别（图表）等</p>
<p>2、SFT</p>
<p>和InternLM-XComposer类似，几个细节是：<br>
- 也使用decay factor=0.9的layer-wise LR策略<br>
- 学习率是预训练的20%</p>
<h1 id="internlm-xcomposer-2.5">InternLM-XComposer-2.5</h1>
<p>论文：《InternLM-XComposer-2.5: A Versatile Large Vision Language
Model Supporting Long-Contextual Input and Output》</p>
<p>时间：2024年7月</p>
<p>InternLM-XComposer-2.5，简称IXC-2.5，相比上一代，效果上主要有三个升级：<br>
- 更高分辨率：复用IXC-2的ViT，分辨率从490x490提升到560x560<br>
- 细粒度视频理解：把视频视为由几十帧到几百帧构成的超大图像<br>
- 多轮多图像对话</p>
<p>IXC-2.5整体framework如下：</p>
<img src="/52c8a4f9/ixc2.5_framework.png" class title="多模态入门">
<p>预训练和微调的大致流程和IXC-2是差不多的，只是数据上进行了一些更新。</p>
<p>在IXC-2.5的基础上，研究人员构建了两个应用：网页生成和文章撰写。</p>
<p>网页生成就是根据输入的网页图片和命令，自动构建网页。训练这个能力最重要的任务就是搞数据。这里的数据来自于Stack
v2，进行了一些清洗之后获得了250k的网页数据，然后用这些数据进行LoRA训练。</p>
<p>而文章撰写能力则是在SFT的基础上，增加了DPO，以提升生成的效果。</p>
<h1 id="internvl-2">InternVL-2</h1>
<p>时间：2024年7月</p>
<p>InternVL2没有技术报告，只有一篇blog，https://internvl.github.io/blog/2024-07-02-InternVL-2.0/。</p>
<p>InternVL-2是在InternVL-1.5上的优化版本，主要有3点优化：<br>
- 多阶段的训练策略<br>
- 更精细的训练数据<br>
- 支持多模态的输出</p>
<img src="/52c8a4f9/internvl2_model.png" class title="多模态入门">
<h1 id="internvl-2.5">InternVL-2.5</h1>
<p>论文：《Expanding Performance Boundaries of Open-Source Multimodal
Models with Model, Data, and Test-Time Scaling》</p>
<p>时间：2024年12月</p>
<p>InternVL-2.5相比InternVL-1.5和InternVL-2在效果上有了比较大的提升。</p>
<h2 id="architecture">Architecture</h2>
<p>大的架构上，InternVL-2.5依然保持和InternVL-1.5相同的设计，使用“ViT-MLP-LLM”的范式：</p>
<img src="/52c8a4f9/ivl2.5_archi.png" class title="多模态入门">
<p>1、vision encoder</p>
<p>InternVL-2.5中使用了两个版本的vision
encoder，大小分别为6B和0.3B。</p>
<p>在InternVL-1中首次使用了6B的ViT模型，InternViT-6B-224px；不过那时候的效果还没有那么好，为了提升vision
encoder的效果，这里进一步对这个6B模型进行训练，优化点包括：<br>
- 使用更大的分辨率（224--&gt;448）/动态分辨率<br>
- 进行增量预训练（连接LLM后）</p>
<p>获得了InternViT-6B之后，通过蒸馏得到InternViT-300M-448px-Distill。和6B模型不同，300M模型使用standard
layernorm，并且没有使用QK-Norm。</p>
<p>为了降低蒸馏成本，300M模型并不是从零初始化，而是借用了CLIP-ViT-Large-336px的参数。虽然二者的架构并不完全相同，不过效果总比随机初始化要好。</p>
<p>蒸馏之后，还做了一个增量预训练，在更多样化的数据上训练300M模型，得到
InternViT-300M-448px-V2.5。</p>
<p>还有一个细节是，InternVL-2.5使用pixel unshuffle
operation降低了输入token的数量，因此448x448的输入图像最后会表达成256个token。</p>
<p>前面这些操作中间产生了好几个版本的模型，都在下面的表格里：</p>
<img src="/52c8a4f9/ivl2.5_vits.png" class title="多模态入门">
<p>2、LLM</p>
<p>InternVL早期版本使用的LLM有很多来源，而InternVL-2.5中主要是使用InternLM-2.5和Qwen-2.5，这两个更新的LLM：</p>
<img src="/52c8a4f9/ivl2.5_models.png" class title="多模态入门">
<h2 id="训练-2">训练</h2>
<p>1、Dynamic High-Resolution</p>
<p>动态分辨率在InternVL-1.5的基础上进一步优化了。主要有这几个步骤：<br>
-
首先计算输入图像的原始宽高比，并从一系列预定义的宽高比中选择最接近的一个；这一步的目的是要把图像切分成多个448x448的子图，子图的数量被预定义在n_min和n_max之间，那么预定的宽高比其实就由预定义的子图的样式决定；输入图像的宽高比和预定义的宽高比的差异计算为W/H的绝对值差<br>
- 如果出现多个相同的候选宽高比，优先选择图像大小差异小的<br>
- 按照选择预定宽高，resize原图，并把图像切分成448x448的子图</p>
<p>如果原始图像被切分了（如果原始图像太小就不需要切分），那么还会生成一张缩略图，贴在子图列表后面。这样的动态分辨率方法也可以扩展到多图或者视频输入：</p>
<img src="/52c8a4f9/ivl2.5_datatype.png" class title="多模态入门">
<ul>
<li>多图：把子图的数量按比例分配给各张图片<br>
</li>
<li>视频：每帧图片都直接resize到448x448</li>
</ul>
<p>实际使用中，对于多图数据集、高分辨率数据集以及图表信息数据等，n_max设得比较大，比如24/36；而对于低分辨率的数据集，则设得比较低，比如6/12。</p>
<p>2、training pipeline</p>
<p>训练分成3个stage：<br>
-
stage1（对齐）：只训练MLP，使用的LR也比较大，能够让MLP快速学习对齐；另外这个阶段就开始使用动态分辨率了<br>
-
stage1.5（预训练）：这个阶段训练MLP和ViT；所使用的LR相对低一些，防止灾难性遗忘<br>
-
stage2（SFT）：全部参数进行训练，这一阶段的数据质量最为重要，即使是少量的异常样本比如几千条不好的数据，都已经足以影响模型的生成效果了；这个阶段超参策略相对简单，对所有参数都保持相同的LR</p>
<p>各个阶段的训练设置：</p>
<img src="/52c8a4f9/ivl2.5_trainconfig.png" class title="多模态入门">
<p>那么其中的stage1.5是需要拿出来说一下的。研究人员实验发现，使用较小的LLM在stage1.5训练了ViT之后，把ViT连接到更大的LLM上，就可以保持比较好的效果了，因此也就引出一个被称为progressive
scaling
strategy的方法：在小LLM上训练好ViT，再连接大的LLM，就可以直接只使用stage1（对齐）
+ stage2（SFT）的训练，从而减少很多训练量，降低成本。</p>
<img src="/52c8a4f9/ivl2.5_train_pipeline.png" class title="多模态入门">
<p>3、训练优化</p>
<p>训练中，额外还使用了两个优化策略。</p>
<p>（1）JPEG compression</p>
<p>为了避免过拟合，随机使用quality
level在75到100之间的设置进行JPEG图像压缩。</p>
<p>需要注意的是，这个方法不用在视频数据中，因为要保持每帧都有相同的质量。</p>
<p>（2）Loss reweighting</p>
<p>一般来说NTP的训练是对整个训练batch中的token进行平均，这样每个训练token对最终loss的贡献和影响是一样大的。这种情况下可能会导致训练结果受长度较长的数据影响更大。而如果以样本为单位进行平均，那么每条训练样本对loss的贡献是一样的，这又会导致短的样本在loss中的权重更大。两种情况的公式表达如下，x为样本的token数量：</p>
<p><span class="math display">\[\mathcal{L}=\frac{w_i}{\sum_jw_j}\cdot\mathcal{L}_i,\quad
w_i=\begin{cases}\frac{1}{x^0},&amp;\text{for token
averaging}\\\frac{1}{x^1},&amp;\text{for sample
averaging},&amp;\end{cases}\]</span></p>
<p>为了平衡这两种方案，InternVL-2.5的做法是使用</p>
<p><span class="math display">\[w_{i}=\frac{1}{x^{0.5}}\]</span></p>
<h2 id="数据">数据</h2>
<p>首先，不同类型的数据（单图、多图、视频、文本）有不同的增强、采样设置：</p>
<img src="/52c8a4f9/ivl2.5_data_config.png" class title="多模态入门">
<p>此外，还有以下几个操作。</p>
<p>1、data packing</p>
<p>data
packing是大模型训练的常规做法了，这样可以减少padding，提升训练效率。对于多模态数据的packing，有两个要考虑的因素：（a）sequence长度（b）image
tile number for ViT。</p>
<p>考虑到这两个因素，整个packing策略分为四步：</p>
<p>（1）select：这一步不用还不用考虑packing的事，而从数据池采样一些单条的数据；对于过长的单条数据，会把它们切分成更小的item；“过长”有两个指标，只要命中一个就是过长：要么总的token数据量超过L_max，或者image
tile的数量超过T_max；切分后的item保证长度≤L_max，image
tile数量≤T_max；切分后的item都会被视作independent的样本</p>
<p>（2）search：这一步从buffer中搜索一个sample，和当前已有的sample拼接在一起；这里要保证拼接后的结果依然满足不过长的标准，也就是总token数和image
tile数都不超过最大值；如果buffer中有多条sample都符合要求，那么选择最长、image
tile最多的那个；出于这个考虑，维护buffer的时候可以按长度和image
tile从大到小排序，方便使用二分检索快速搜索</p>
<p>（3）pack：从上一步搜索的sample拼接到一起，注意每个独立sample在attention中只能看到自己的token，不能看到其他拼接样本的token；基于此，一条独立样本中的多个item的positional
index也独立计数的</p>
<p>（4）maintain：如果当前的packed sample达到长度或者image
tile上限，那么就会输出用于训练；否则就会把他放回到buffer
list中；为了避免buffer耗费太多空间，buffer有一个最大size，如果buffer已经达到最大size，那么就把当前最长的样本输出用于训练，释放空间</p>
<p>2、数据过滤</p>
<p>有一个观察，LLM对噪音数据的敏感程度明显高于vision
encoder，即使只有少量几千个异常样本也会影响最终模型的效果。这些噪音数据中，影响最大的就是具有重复模式的数据，这些数据可能会让模型输出陷入重复循环的样式，严重影响用户体验，并且也使得test-time
scaling的策略没法进行。</p>
<img src="/52c8a4f9/ivl2.5_samples.png" class title="多模态入门">
<p>那么一个最直接的方法就是进行数据过滤，pipeline如下图：</p>
<img src="/52c8a4f9/ivl2.5_filter.png" class title="多模态入门">
<p>对于纯文本数据，有三个策略：<br>
-
LLM-Based质量打分：首先把数据分成不同的领域，包括code、math、general、stem等，然后用LLM加上各个领域对应的prompt给每个样本打分，分数从0~10，然后删除低于阈值（e.g.
7）的数据<br>
- 重复检测：也是利用LLM +
prompt检测重复样本，然后后面会有人工review，决定阈值再删除对应数据<br>
- 规则：包括长度异常、过长的zero sequence、重复行过多的样本</p>
<p>对于多模态数据，鉴于现有的MLLM在打分方面的能力还不是很行，因此专注在重复检测和规则两种策略来过滤。几个细节：<br>
- 重复检测的时候略过了高质量的学术数据集<br>
- 规则检验找出来的异常数据会人工review再确认是否合理</p>
<p>3、data mixture</p>
<p>（1）预训练</p>
<p>在预训练阶段所包含的数据还是很多的：</p>
<img src="/52c8a4f9/ivl2.5_ptm_data.png" class title="多模态入门">
<p>值得注意的是，所有数据都被构建成对话格式，对于那些本身不是对话格式的数据，会构建问题把它们转成对话格式。这个阶段的数据尽可能多地包含各种内容，提高模型的泛化能力，而没有特别考虑数据的质量，高低质量的数据都会用（因为只有MLP和ViT会被训练）。</p>
<p>研究人员认为，理想的状况是在预训练阶段把SFT(stage
2)要用的数据都包含进去，即SFT数据应该是预训练数据的子集，这样让模型能够充分学习。不过实践中，由于stage1.5的成本比较高，所以往往只能包含部分stage
2的数据，那些持续新增的SFT数据就没法放进预训练中了。</p>
<p>（2）SFT</p>
<p>从InternVL-1.5的5.1M，InternVL-2的7.3M，到InternVL-2.5的16.3M，SFT数据量有了比较大的提升。InternVL-2.5所包含的SFT数据如下：</p>
<img src="/52c8a4f9/ivl2.5_sft_data.png" class title="多模态入门">
<h1 id="mini-internvl">Mini-InternVL</h1>
<p>论文：《Mini-InternVL: A Flexible-Transfer Pocket Multimodal Model
with 5% Parameters and 90% Performance》</p>
<p>时间：2024年10月</p>
<p>Mini-InternVL相比前面InternVL各个版本，在技术上没有太多改变，可以认为是一次小型化的实践。Mini-InternVL系列有1B、2B、4B三个规模。如文章标题，Mini-InternVL用5%的参数量实现了大模型90%的效果。（至于10%的损失作为小型化的代价是否是可以接受的就见仁见智了）</p>
<p>Mini-InternVL的效果：</p>
<img src="/52c8a4f9/miniivl_perf.png" class title="多模态入门">
<p>1、框架和训练</p>
<p>Mini-InternVL使用CLIP-ViT-L-336px初始化一个300M的ViT，并用6B的ViT做teacher进行蒸馏，获得InternViT-300M。</p>
<p>Mini-InternVL的训练分成两个阶段：<br>
- 对齐：对齐阶段只训练MLP<br>
- instruction tuning：全参微调</p>
<p>这里有点意外的是在对齐阶段之后，没有对ViT或LLM进行大规模的预训练。整体框架和训练如下图所示：</p>
<img src="/52c8a4f9/miniivl_archi.png" class title="多模态入门">
<p>2、数据</p>
<p>数据上，Mini-InternVL对各种任务都设计了对话数据格式，各个任务有特殊的token用于标识重要内容，比如bounding
box、类别候选等，如下图：</p>
<img src="/52c8a4f9/miniivl_format.png" class title="多模态入门">
<h1 id="mini-internvl-2">Mini-InternVL-2</h1>
<p>时间：2024年12月</p>
<p>Mini-InternVL2没有技术报告，有一篇blog，https://internvl.github.io/blog/2024-10-21-Mini-InternVL-2.0/。从blog看，只有两个训练阶段不冻结的可参数参数多了些：</p>
<img src="/52c8a4f9/miniivl2_model.png" class title="多模态入门">
<p>但是blog里report的结果又和一代是相同的，这就有点奇怪了。</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】InternLM-XComposer2: Mastering Free-form Text-Image Composition
and Comprehension in Vision-Language Large Models<br>
【2】InternVL2博客：https://internvl.github.io/blog/2024-07-02-InternVL-2.0/<br>
【3】Mini-InternVL: A Flexible-Transfer Pocket Multimodal Model with 5%
Parameters and 90% Performance<br>
【4】Expanding Performance Boundaries of Open-Source Multimodal Models
with Model, Data, and Test-Time Scaling<br>
【5】Mini-InternVL2博客：https://internvl.github.io/blog/2024-10-21-Mini-InternVL-2.0/<br>
【6】InternLM-XComposer: A Vision-Language Large Model for Advanced
Text-image Comprehension and Composition<br>
【7】InternLM-XComposer-2.5: A Versatile Large Vision Language Model
Supporting Long-Contextual Input and Output</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>transformer</tag>
        <tag>SFT</tag>
        <tag>预训练</tag>
        <tag>多模态</tag>
        <tag>CV</tag>
        <tag>无监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepSeek-V3细节探索</title>
    <url>/a9c496e3.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>DeepSeek-R1以一己之力正面刚OpenAI和Anthropic。DeepSeek-R1能有这么强力的表现和DeepSeek-V3这个基模型的强大是分不开的。</p>
<img src="/a9c496e3/perf.png" class title="dsv3">
<p>现在就来盘一下DeepSeek-V3的一些细节。（不包括infra部分）</p>
<p>相关文章链接：</p>
<p><a href="http://www.linsight.cn/44e38c1b.html">DeepSeekMoE</a></p>
<p><a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2</a></p>
<p><a href="https://www.linsight.cn/9e4b4e6d.html">DeepSeek-R1详解</a></p>
<h1 id="mla">MLA</h1>
<p>DeepSeek-V3模型的基础架构和V2一样：</p>
<img src="/a9c496e3/ds3_archi.png" class title="dsv3">
<p>先来看下MLA是怎么做的。（很熟悉MLA的朋友可以跳过这部分）</p>
<h2 id="从mha出发">从MHA出发</h2>
<p>先回顾下标准的MHA。假设 <span class="math inline">\(n_h\)</span>
是注意力头的数量，<span class="math inline">\(d_h\)</span>
是每个注意力头的大小，<span class="math inline">\(\mathbf{h}_{t}\in\mathbb{R}^{d}\)</span>
是第t个输入token。</p>
<p>MHA首先通过三个投影矩阵 <span class="math inline">\(W^{Q},W^{K},W^{V}\in\mathbb{R}^{d_{h}n_{h}\times
d}\)</span> 获得<span class="math inline">\(\mathbf{q}_t,\mathbf{k}_t,\mathbf{v}_t\in\mathbb{R}^{d_hn_h}\)</span>：</p>
<p><span class="math display">\[\mathbf{q}_t=W^Q\mathbf{h}_t\]</span></p>
<p><span class="math display">\[\mathbf{k}_t=W^K\mathbf{h}_t\]</span></p>
<p><span class="math display">\[\mathbf{v}_t=W^V\mathbf{h}_t\]</span></p>
<p>之后 <span class="math inline">\(\mathbf{q}_t,\mathbf{k}_t,\mathbf{v}_t\)</span>
就会被切成 <span class="math inline">\(n_h\)</span>
份，分别进行注意力计算：</p>
<p><span class="math display">\[[\mathbf{q}_{t,1};\mathbf{q}_{t,2};...;\mathbf{q}_{t,n_{h}}]=\mathbf{q}_{t}\]</span></p>
<p><span class="math display">\[[\mathbf{k}_{t,1};\mathbf{k}_{t,2};...;\mathbf{k}_{t,n_{h}}]=\mathbf{k}_{t}\]</span></p>
<p><span class="math display">\[[\mathbf{v}_{t,1};\mathbf{v}_{t,2};...;\mathbf{v}_{t,n_{h}}]=\mathbf{v}_{t}\]</span></p>
<p><span class="math display">\[\mathbf{o}_{t,i}=\sum_{j=1}^t\mathrm{Softmax}_j(\frac{\mathbf{q}_{t,i}^T\mathbf{k}_{j,i}}{\sqrt{d_h}})\mathbf{v}_{j,i}\]</span></p>
<p><span class="math display">\[\mathbf{u}_t=W^O[\mathbf{o}_{t,1};\mathbf{o}_{t,2};...;\mathbf{o}_{t,n_h}]\]</span></p>
<p>其中 <span class="math inline">\(\mathbf{q}_{t,i},\mathbf{k}_{t,i},\mathbf{v}_{t,i}\in\mathbb{R}^{d_{h}}\)</span>，<span class="math inline">\(W^O\in\mathbb{R}^{d\times d_hn_h}\)</span>。</p>
<p>在推理的时候，为了加速，会对前面已经计算过的K、V值进行缓存，那么每个token在模型每层就要保存
<span class="math inline">\(2{n}_{h}{d}_{h}\)</span> 个数值。</p>
<p>那么要减少缓存的量，一个方法就是减少使用的K/V。GQA/MQA就是通过共享参数减少K、V头的数量并重复使用，从而减少了需要缓存的KV的量。</p>
<h2 id="mla-1">MLA</h2>
<p>MLA通过对K和V做low-rank joint compression来压缩KV
cache，理论上可以更有效地压缩KV缓存值。</p>
<img src="/a9c496e3/ds3_MLA.png" class title="MLA">
<p>下面看下MLA具体是怎么做的。</p>
<p>在MHA中，K和V是对 <span class="math inline">\(h_t\)</span>
分别用投影矩阵进行变化得到的，而MLA把KV的变换改成使用一个共用的down-projection
matrix和两个up-projection matrices进行操作：</p>
<p><span class="math display">\[\mathbf{c}_t^{KV}=W^{DKV}\mathbf{h}_t\]</span></p>
<p><span class="math display">\[\mathbf{k}_t^C=W^{UK}\mathbf{c}_t^{KV}\]</span></p>
<p><span class="math display">\[\mathbf{v}_t^C=W^{UV}\mathbf{c}_t^{KV}\]</span></p>
<p><span class="math inline">\(\mathfrak{c}_t^{KV}\in\mathbb{R}^{d_c}\)</span>
就是K和V的compressed latent vector，这也是推理时要缓存的部分。</p>
<p>这里相当于把MHA中的 <span class="math inline">\(W^{K},W^{V}\)</span>
拆成两个矩阵：</p>
<p><span class="math display">\[\mathbf{k}_t=W^K\mathbf{h}_t\rightarrow
W^{UK}W^{DKV}\mathbf{h}_t\]</span></p>
<p><span class="math display">\[\mathbf{v}_t=W^V\mathbf{h}_t\rightarrow
W^{UV}W^{DKV}\mathbf{h}_t\]</span></p>
<p><span class="math inline">\(d_c\)</span> 是KV的压缩维度，让 <span class="math inline">\(d_c\ll
d_hn_h\)</span>，就可以大大减少需要推理时需要缓存的数据量。</p>
<p>再看回attention计算，在得到q、k、v之后，会计算权重矩阵并获得最终注意力输出结果：</p>
<p><span class="math display">\[\operatorname{Attention}(Q,K,V)=\operatorname{softmax}(\frac{Q^TK}{\sqrt{d}})V\]</span></p>
<p>而 <span class="math inline">\(Q^TK=H^T(W^Q)^TW^{UK}C\)</span>，因此
<span class="math inline">\(W^{UK}\)</span> 可以被吸收进 <span class="math inline">\(W^{Q}\)</span>
中，而不用在计算时显式算出K，只需调整 <span class="math inline">\(W^Q\)</span> 的shape后直接输入C即可。同理 <span class="math inline">\(W^{UV}\)</span> 可以被吸收进 <span class="math inline">\(W^{O}\)</span>。实操上，这样的矩阵合并可能会带来一些精度损失，这是一个值得注意的问题。</p>
<p>此外，MLA还对Q也做了low-rank compression，跟对K、V的操作类似：</p>
<p><span class="math display">\[\mathbf{c}_t^Q=W^{DQ}\mathbf{h}_t,\\\mathbf{q}_t^C=W^{UQ}\mathbf{c}_t^Q,\]</span></p>
<p>关于对Q进行压缩的原因，虽然V2原文说的是为了减少训练时的activation，但是两个矩阵所得的activation按道理应该比直接使用单个投影矩阵还要多一些。这里Q的压缩更可能是为了减少参数量和梯度，而非激活值。</p>
<h2 id="兼容rope">兼容RoPE</h2>
<p>到这里似乎MLA已经完成了，即减少了缓存的量，也不用引入其他overhead（两个up-projection
matrices都不用算了）。</p>
<p>但是实际上还有一个问题没有解决：位置编码使用的是RoPE，而RoPE是通过在Q、K上乘一个旋转矩阵来编码位置的。</p>
<p>而在上面MLA的设计中，已经没有显式计算K了，而RoPE也不能加在latent
vector上。一个方法是重新把K和V显式计算出来，但是这样计算量就会增加，MLA的推理加速效果就会打折扣。</p>
<p>针对这个问题，解决方案是使用decoupled RoPE：使用额外的multi-head
queries <span class="math inline">\(\mathbf{q}_{t,i}^R\in\mathbb{R}^{d_h^R}\)</span>
和一个shared key <span class="math inline">\(\mathbf{k}_t^R\in\mathbb{R}^{d_h^R}\)</span>
来携带RoPE的位置信息，<span class="math inline">\(d_h^R\)</span>
是decoupled queries的维度。</p>
<p>新增的q和k维度使用常规的RoPE计算，用于携带位置信息；而原来的维度依然使用低秩分解的方式计算，最后再计算attention的时候两个部分拼接起来。</p>
<p>最终完整的MLA计算如下</p>
<img src="/a9c496e3/MLA_formula.png" class title="MLA公式">
<p>蓝框中的部分就是推理时需要缓存的内容。</p>
<p>MLA所需的缓存量约等于组数为2.5的GQA：</p>
<img src="/a9c496e3/MLA_cache.png" class title="MLA缓存量">
<h1 id="moe">MoE</h1>
<h2 id="基础结构">基础结构</h2>
<p>DeepSeek-V3的MoE结构设计和DeepSeekMoE/DeepSeek-V2基本一致。和V2相比，有一些设置是一样的：</p>
<ul>
<li>初始化 standard deviation = 0.006<br>
</li>
<li>128个attention head，head size = 128<br>
</li>
<li>KV的compression dimension dc = 512<br>
</li>
<li>Q的compression dimension dc' = 1536<br>
</li>
<li>decoupled queries and key per head dimension = 64</li>
</ul>
<p>此外，也有一些具体设置和V2相比有变化：<br>
- layers = 61（比V2多1层）<br>
- hidden dimension = 7168（比V2的5120更大）<br>
- 前3层不使用MoE<br>
- 1个共享专家 +
8/256个路由专家，专家大小为2048（更多专家，专家维度更大）<br>
- 每个token最多只会被分发到4个节点<br>
- 总参数671B，激活参数37B<br>
- gating在计算affinity
score的时候先用sigmoid函数，再在选定的分数上进行归一化，而V2是直接使用softmax</p>
<p>V2的总参数为236B，激活参数为21B；而V3的总参数为671B，激活参数为37B。可以看到相比V2，V3多的参数主要在模型宽度和专家数量，而且MoE的激活更为稀疏。</p>
<h2 id="负载平衡">负载平衡</h2>
<p>1、Auxiliary-Loss-Free Load Balancing</p>
<p>先看下V3的MoE计算：</p>
<p><span class="math display">\[\mathbf{h}_t^{\prime}=\mathbf{u}_t+\sum_{i=1}^{N_s}\mathrm{FFN}_i^{(s)}\left(\mathbf{u}_t\right)+\sum_{i=1}^{N_r}g_{i,t}\mathrm{FFN}_i^{(r)}\left(\mathbf{u}_t\right)\]</span></p>
<p>第一项来自残差连接，第二项是共享专家的输出，第三项是路由专家的输出；Ns是shared
expert的数量，Nr是routed expert的数量，DeepSeek-V3中Ns=1，Nr=128。</p>
<p><span class="math display">\[g_{i,t}=\frac{g_{i,t}^\prime}{\sum_{j=1}^{N_r}g_{j,t}^\prime}\]</span></p>
<p>g'只保留top Nr个（DeepSeek-V3中Nr=8），其他都置零了。</p>
<p><span class="math display">\[g_{i,t}^{\prime}=\begin{cases}s_{i,t},&amp;s_{i,t}\in\mathrm{Topk}(\{s_{j,t}|1\leqslant
j\leqslant
N_r\},K_r)\\0,&amp;\text{otherwise}&amp;&amp;\end{cases}\]</span></p>
<p><span class="math display">\[s_{i,t}=\mathrm{Sigmoid}\left(\mathbf{u}_t{}^T\mathbf{e}_i\right)\]</span></p>
<p>Kr是activated routed expert的数量。</p>
<p>之前的版本使用auxiliary loss来对top affinity
score的分配不平衡进行惩罚，以此来缓解专家分配不平衡的问题。由于auxiliary
loss的设计并不关注模型的效果，因此过大的权重会对模型的训练效果产生损害。</p>
<p>为了避免模型效果的损失，DeepSeek-V3不使用auxiliary
loss来平衡负载，而是在affinity score上加了一个bias term，这个bias
term和expert是一一对应的：</p>
<p><span class="math display">\[g_{i,t}^{\prime}=\begin{cases}s_{i,t},&amp;s_{i,t}+b_i\in\mathrm{Topk}(\{s_{j,t}+b_j|1\leqslant
j\leqslant
N_r\},K_r)\\0,&amp;\text{otherwise}&amp;\end{cases}\]</span></p>
<p>这个bias term只用于routing，不用于和FFN的结果相乘输出专家的feature
vector。在每个训练step后，如果一个expert的负载过大了，就会把对应的bias
term减小𝛾，反之则把bias
term的数值增大𝛾。𝛾是个超参，控制负载平衡系统的变化速度。</p>
<p>2、Complementary Sequence-Wise Auxiliary Loss</p>
<p>虽然加了bias
term控制负载均衡，但是为了防止极端不平衡状况的出现，还是额外加了一个Auxiliary
Loss。</p>
<p>complementary sequence-wise balance loss是这么算的：</p>
<p><span class="math display">\[\mathcal{L}_\mathrm{Bal}=\alpha\sum_{i=1}^{N_r}f_iP_i\]</span></p>
<p>其中</p>
<p><span class="math display">\[P_i=\frac{1}{T}\sum_{t=1}^Ts_{i,t}^{\prime}\]</span></p>
<p>s'其实就是归一化的affinity score</p>
<p><span class="math display">\[s_{i,t}^\prime=\frac{s_{i,t}}{\sum_{j=1}^{N_r}s_{j,t}}\]</span></p>
<p>另外</p>
<p><span class="math display">\[f_i=\frac{N_r}{K_rT}\sum_{t=1}^T\mathbb{1}\left(s_{i,t}\in\mathrm{Topk}(\{s_{j,t}|1\leqslant
j\leqslant N_r\},K_r)\right)\]</span></p>
<p>求和部分其实就是某个token是否选择了expert i。训练中𝛼 = 0.0001。</p>
<p>fi是不可导的，Pi是可导的。</p>
<p>在完美负载平衡的情况下，affinity
score均匀分配，每个expert的得分相同，那么有</p>
<p><span class="math display">\[P_i=\frac{1}{T}\times T\times
\frac{1}{N_r}=\frac{1}{N_r}\]</span></p>
<p><span class="math display">\[f_i=\frac{N_r}{K_rT}\sum_{t=1}^T\frac{K_r}{N_r}=1\]</span></p>
<p>那么</p>
<p><span class="math display">\[\mathcal{L}_\mathrm{Bal}=\alpha\sum_{i=1}^{N_r}\frac{1}{N_r}=\alpha\]</span></p>
<p>complementary sequence-wise balance
loss其实就是DeepSeekMoE中的expert-level balance loss。</p>
<p>而在极端不平衡的情况下，比如所有token都选择了前Kr个expert激活，那么对于激活的expert
i，有</p>
<p><span class="math display">\[P_i=\frac{1}{T}\times T\times
1=1\]</span></p>
<p><span class="math display">\[f_i=\frac{N_r}{K_rT}\sum_{t=1}^T1=\frac{N_r}{K_r}\]</span></p>
<p>那么就有</p>
<p><span class="math display">\[\mathcal{L}_\mathrm{Bal}=\alpha\sum_{i=1}^{K_r}\frac{N_r}{K_r}=\alpha
N_r\]</span></p>
<p>3、Node-Limited Routing</p>
<p>在前面的基础上，最后还加了一个机制，限制每个token最多只能分发到M个节点上，而节点的选择是基于每个节点上的affinity
score的总和的。</p>
<p>举个例子，在Kr=8，M=4的情况下：</p>
<ul>
<li>如果8个得分最高的专家都分布在不同的node，那么只有top4个专家会被激活，其余的专家虽然得分排在top
Nr，但是由于激活节点的限制，不会被使用；<br>
</li>
<li>top8个专家分配在5个节点上：
<ul>
<li>节点1：0.1,0.1</li>
<li>节点2：0.1,0.1</li>
<li>节点3：0.1,0.1</li>
<li>节点4：0.25</li>
<li>节点5：0.15<br>
在这样的情况下，虽然节点5上的专家得分是第二高的，但是由于它所在的节点的得分总和不高，因此不会被激活</li>
</ul></li>
</ul>
<h2 id="no-token-dropping">No Token-Dropping</h2>
<p>由于前面的几个负载平衡策略基本上已经可以保持完全的负载平衡，因此DeepSeek-V3就不再使用token
dropping的策略了。</p>
<h1 id="multi-token-prediction">Multi-Token Prediction</h1>
<p>Multi-Token Prediction（MTP），顾名思义，在前向计算的时候一步可以预测
&gt;1 个token。</p>
<p>这样的多token预测策略可以在训练中使用，提升模型的远距离的理解能力；也可以用在推理中，加速inference输出，不过推理加速算是副产品了。</p>
<h2 id="原始的mtp">原始的MTP</h2>
<p>DeepSeek-V3中使用的MTP参考了24年4月的《Better &amp; Faster Large
Language Models via Multi-token
Prediction》，因此先来了解下这个工作。</p>
<p>1、MTP方案</p>
<p>标准的语言建模使用next-token
prediction，基于第1~t个token预测第t+1个token，loss是这样的：</p>
<p><span class="math display">\[\begin{aligned}L_1=-\sum_t\log
P_\theta(x_{t+1}\mid x_{t:1})\end{aligned}\]</span></p>
<p>和NTP不同，MTP要求模型在每一步要预测n个token，即第t+1~t+n个token，loss就写作这样：</p>
<p><span class="math display">\[\begin{aligned}L_n=-\sum_t\log
P_\theta(x_{t+n:t+1}\mid x_{t:1})\end{aligned}\]</span></p>
<p>那么怎么在一步内预测多个token呢？论文里的做法是利用多个output
head，每个head负责预测一个token。下面这个图就是当n=4的例子：</p>
<img src="/a9c496e3/mtp_example.png" class title="dsv3">
<p>head1根据token 1~t预测token
t+1，这和标准的NTP任务是一样的。而head2则是根据token 1~t预测token
t+2，head3和head4也是类似的，分别预测token t+3和token t+4。</p>
<p>所有的这些head共享同一个主干transformer fs的输出，单独的output
head参数fh，另外还共享着unembedding matrix
fu。第i个head的输出可以写作：</p>
<p><span class="math display">\[P_\theta(x_{t+i}\mid
x_{t:1})=\operatorname{softmax}(f_u(f_{h_i}(f_s(x_{t:1}))))\]</span></p>
<p>由于使用了多个输出头，计算的时候就多了额外的参数和激活值，因此相比NTP，MTP会使用更多的memory。为了缓解这个问题，文中给出串行计算（而不是并行）这些output
head的forward和backward的方法：</p>
<img src="/a9c496e3/mtp_order.png" class title="dsv3">
<p>多个head回传的梯度可以在共享的transformer主干处积累，这样就把增加的memory量从O(nV+d)降到了O(V+d)。</p>
<p>正常推理的时候就只使用head1，其他的head就可以不用了，这和标准的推理形式是一致的。但是如果在推理时使用类似<a href="https://mp.weixin.qq.com/s/wOIGg9pJCXQxz3GgXApUQw?token=1318369845&amp;lang=zh_CN">投机解码</a>或者<a href="https://mp.weixin.qq.com/s/e3Cn_zbPlbRUUd4-ngSLTg?token=1318369845&amp;lang=zh_CN">MEDUSA</a>这样的推理加速方案，其他的head2、head3、head4都可以直接派上用场，作为draft
model使用。</p>
<p>2、MTP的效果</p>
<p>MTP的效果怎么样呢？论文在&gt;=91B的代码数据上训练了从0.3B到13B参数量的模型，对比NTP和MTP的效果。在各个模型上，MTP相比NTP，在两个经典代码评测集MBPP和human-eval的效果对比如下：</p>
<img src="/a9c496e3/mtp_code_result.png" class title="dsv3">
<p>随着模型规模的提升，MTP的效果逐步提升，相比NTP的收益越来越大。</p>
<p>文中更多的预训练实验结果还有一些发现：</p>
<ul>
<li>随着训练的epoch数的提升，MTP的收益有所收窄，不过还是有一些的；不过现在通用预训练数据量基本够大，不太可能出现超过1个epoch的情况<br>
</li>
<li>MBPP和human-eval最佳的n为4，不过在APPS/Intro上n=6效果更好，n的设置可能和数据相关</li>
</ul>
<img src="/a9c496e3/mtp_exps.png" class title="dsv3">
<p>另一个需要了解的问题是，MTP在预训练上有效，那么对于在MTP上预训练的模型，微调时n应该设置为多少。下图中，n为预训练中每步预测的token数，n'为SFT训练中每步预测的token数，红线就是预训练和SFT都是NTP，黑色虚线预训练用MTP，SFT用NTP，而浅蓝色虚线是与预训练和SFT都用MTP：</p>
<img src="/a9c496e3/mtp_sft.png" class title="dsv3">
<p>结果上看，对MTP预训练模型使用NTP微调的效果是最好的。</p>
<p>前面的评测都是在code相关的任务上进行的，而在一些NLP
benchmark上，MTP的效果就不如NTP：</p>
<img src="/a9c496e3/mtp_nlp_benchmark.png" class title="dsv3">
<p>这里有几个可能的原因：</p>
<ul>
<li>可能需要更大的模型让MTP发挥效果<br>
</li>
<li>概率类或者选择题类的评测并不能很好地评估MTP学到的更远距离依赖的能力</li>
</ul>
<p>针对第二个猜测，另外使用了8个评测指标为ROUGH-L的任务，这些任务要求模型输出较长的文本（比如摘要）。在这类任务上，MTP模型的效果就比较好了</p>
<img src="/a9c496e3/mtp_summary.png" class title="dsv3">
<p>3、结构上的变体</p>
<p>上面的MTP设计是使用n个output
head，每个head「独立」地进行token预测，逻辑上这些输出头是并行的。实际上这些output
head的设计可以有多种变化，比如他们之间是并行还是串行，每个头的层数和类型。针对这些变体，研究人员也做了实验，各种变体的效果如下：</p>
<img src="/a9c496e3/mtp_archi.png" class title="dsv3">
<p>其中parallel就是前面的独立方式。causal就是head2的输出是以head1的输出为基础的，而anticausal则是先预测n个token中最后一个，然后第n-1个output
head再根据它的结果输出，以此类推，第1个token反而是最后输出的，并且参考后面的所有token。</p>
<p>除此之后，还有一种变体，那就是每个output head维护自己的unembedding
matrix，单独训练，不过这么一来参数量和训练的内存需求就会增大不少。</p>
<h2 id="deepseek-v3中的mtp">DeepSeek-V3中的MTP</h2>
<p>说回DeepSeek-V3。</p>
<p>DeepSeek-V3中的MTP在原实现的基础上做了一些细化和改进。</p>
<p>1、MTP module</p>
<p>首先是MTP
module的设计。DeepSeek-V3中，多个预测的token是有causal关系的，也就是output2会根据output1的特征进行输出。</p>
<p>前一个MTP module的输出向量经过RMSNorm之后和embedding
layer的feature拼接在了一起，再经过transformer block进行输出：</p>
<img src="/a9c496e3/ds3_mtp_module.png" class title="dsv3">
<p>这个图值得细细看，有几个要注意的地方：</p>
<ul>
<li>MTP module原始的输入来自embedding layer，而不是主干transformer
model的最后一层输出<br>
</li>
<li>main model的output
head是预测第t+1个token的，第一个MTP是预测第t+2个token的，第二个MTP是预测第t+3个token的<br>
</li>
<li>output head的参数是共享的，也就是每个MTP
module中，预测不同token的能力主要是由linear projection和transformer
block部分的参数习得；使用参数共享的考虑和《EAGLE: speculative sampling
requires rethinking feature
uncertainty》有些相近，不过目的不同，EAGLE是为了加速推理，而DeepSeek-V3是为了优化MTP的训练效果</li>
</ul>
<p>在推理的时候MTP
module可以完全不使用，回到正常的NTP的方式来生成结果。当然如果要考虑推理加速，这些module也可以用上。</p>
<p>2、训练的损失函数</p>
<p>MTP的损失是作为附加损失和main model一起训练的。几个MTP
module的损失就是取平均，再通过权重λ加入到总loss里：</p>
<p><span class="math display">\[\mathcal{L}_{MTP
}^{k}=CrossEntropy\left(P_{2+k: T+1}^{k}, t_{2+k:
T+1}\right)=-\frac{1}{T} \sum_{i=2+k}^{T+1} \log
P_{i}^{k}\left[t_{i}\right]\]</span></p>
<p><span class="math display">\[\mathcal{L}_{MTP}=\frac{\lambda}{D}
\sum_{k=1}^{D} \mathcal{L}_{MTP}^{k}\]</span></p>
<p>实际使用中，MTP深度D=1，也就是除了主模型的output head，只有一个MTP
module。</p>
<h1 id="数据构建">数据构建</h1>
<p>数据建设上，DeepSeek-V3没有给出特别详细的内容。相比V2，V3强调了几点变化：</p>
<ul>
<li>增加了数学和代码数据的比例<br>
</li>
<li>增加中英文之外其他语言的覆盖<br>
</li>
<li>强调了去重了保留多样性</li>
</ul>
<p>最终获得了14.8T的训练数据。</p>
<p>此外，文中还透露了以下几点。</p>
<h2 id="document-packing">document packing</h2>
<p>目前大部分的模型都是采用concat-then-split的方式，把文档分割成训练样本。这样的方式可以避免padding，从而提高训练效率。但是频繁的文档切分也会带来问题：训练数据的实际有效上下文缩短；被分割的文档缺失上下文信息，让模型在生成时需要靠想象补充缺失的部分，从而导致幻觉的产生。</p>
<p>DeepSeek-V3就参考《Better &amp; Faster Large Language Models via
Multi-token Prediction》的做法Best-fit Packing，优化document
packing。</p>
<p>那么简单介绍一下best-fit packing。</p>
<p>首先，假设模型的训练窗口长度时L，那么对于长度大于L的文档，首先就要切成长度为L的小块。这一步是无论什么训练策略都要做的，即使不进行任何拼接而对每个文档单独进行padding，也需要切分过长的文档。</p>
<p>那么接下来的任务就是把这些切分出来的文档chunk拼接成长度&lt;=L的训练样本，并且样本数量越少越好。样本数量越少，意味着数据密度越高，padding越少。</p>
<p>到这里，其实就转化成了一个背包问题。但是背包问题是NP-hard的，没法直接得到最优解，因此可以借用已有的高效近似解法，First-Fit-Decreasing
(FFD) 和Best-Fit-Decreasing (BFD) 来获得近似解。</p>
<p>算法如下：</p>
<img src="/a9c496e3/BFD_FFD.png" class title="dsv3">
<p>C就是文档集合，l(c)是文档的长度。每一步拼接中，FFD是对文档长度降序排序，然后选择第一个fit的文档加入；BFD是对文档长度降序排序，然后选择让bin的剩余空间最小的文档。实践中，使用segment
tree实现BFD上的快速搜索。</p>
<p>直观看下best-fit packing和concat-the-split的对比：</p>
<img src="/a9c496e3/best_fit_packing.png" class title="dsv3">
<p>那么best-fit
packing的会带来多少的额外padding呢？由于实际训练数据大部分其实不是很长，所以更容易pack得很紧密。在2k和8k的训练窗口下，best-fit
packing和concat-then-split相比基本没有可感知的训练样本增加，小于万分之一，并且随着训练窗口增大，这个差距还在减小：</p>
<img src="/a9c496e3/packing_padding.png" class title="dsv3">
<p>最终训练效果上，相比concat-then-split，best-fit
packing在阅读理解、NLI、Context Following上有明显的提升：</p>
<img src="/a9c496e3/bfp_perf1.png" class title="dsv3">
<img src="/a9c496e3/bfp_perf2.png" class title="dsv3">
<h2 id="fill-in-middlefim">Fill-in-Middle（FIM）</h2>
<p>为什么需要FIM的训练方式。我们知道GPT模型相比Bert类模型，有更高的训练效率；而从左到右自由生成的方式也使得GPT模型能够应用在更多场景，上限更高。但是传统的left-to-right的训练方式也有限制：如在代码补全的场景，需要模型同时兼顾上文和下文，对中间部分的内容进行补全，这种情况下left-to-right的训练方式就无法提供有效的信息，因为看不见下文。</p>
<p>为了解决这个问题，可以对模型的输入数据做一个transformation：把原本顺序正常的文档，切分成三部分，即prefix、middle和suffix，并把middle部分放到最后面。</p>
<p>document -&gt; (prefix; middle; suffix)  -&gt; (prefix; suffix;
middle)</p>
<p>训练的时候，模型需要根据给定的上文prefix和下文suffix，来生成中间的部分。</p>
<p>DeepSeek-V3中有10%的数据采用了FIM的格式变换，使用PSM的顺序。</p>
<h2 id="tokenizer和token-boundary">tokenizer和token boundary</h2>
<p>DeepSeek-V3的tokenizer除了加入其他语言的token之外，还增加了包含标点符号和line
break的token。这些新加的token可能会引入prompt boundary的问题。</p>
<p>什么是prompt
boundary？先来看一个例子。用stabilityai的stablelm-base-alpha-3b模型来给这句话进行补全：</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;The link is &lt;a href=&quot;http:&#39;</span></span></code></pre></div>
<p>正常来说，我们希望补全的结果是一个格式正确的链接。实际生成的结果是</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;The link is &lt;a href=&quot;http: //www.google.com/search?q&#39;</span></span></code></pre></div>
<p>注意"http:"后面多了个空格，这显然是无效的。这就有点奇怪了，按道理这样的格式在训练数据里是足够多的，模型没有道理学习不到有效的格式。</p>
<p>重新试一下生成，这次把输入prompt最后的冒号去掉</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;The link is &lt;a href=&quot;http&#39;</span></span></code></pre></div>
<p>再让模型补全：</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;The link is &lt;a href=&quot;http://www.youtube.com/v/s&#39;</span></span></code></pre></div>
<p>这下就可以正常生成了。</p>
<p>看来问题就出在 : 这里。把第一个prompt的token打印出来看看：</p>
<img src="/a9c496e3/token_boundary_1.png" class title="dsv3">
<p>再看看一个正常链接的token：</p>
<img src="/a9c496e3/token_boundary_2.png" class title="dsv3">
<p>发现 :// 是被当成一个token处理的。</p>
<p>大多数的tokenizer都是greedy
tokenization的策略。训练时可以看到完整的文本，因此所有链接中，://
都被当做一个token处理，也就是模型在训练时几乎没有见过 : token后面跟诊 //
token的情况，这就导致如果prompt中给了 : ，模型就会输出错误的结果。</p>
<p>词表中有很多以 :
开头的token，它们在训练时都被当做一个token处理了：</p>
<img src="/a9c496e3/token_boundary_3.png" class title="dsv3">
<p>也就是说，对于这34个token，模型几乎没有训练过它们的冒号被拆分出来的情况，那在推理时自然也就无法正常生成。</p>
<p>这个情况不仅存在于和 : 相关的token中，而是广泛存在于整个词表。</p>
<p>这个现象可以称之为token boundary bias。缓解token boundary
bias大致有两个方法。</p>
<p>第一个方法叫做token
healing。既然输入prompt中最后一个token有可能是训练数据的token中的一部分，那么就先把最后的一个token删去，然后再在后续的生成结果中，选择包含被删去字符的token作为生成结果。</p>
<p>比如前面的链接生成，输入的prompt是</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;The link is &lt;a href=&quot;http:&#39;</span></span></code></pre></div>
<p>tokenization之后 :
是最后一个token，那么就把它去掉。假设后续模型生成的top k个结果是</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>s</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>:\<span class="op">\</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>google</span></code></pre></div>
<p>那么就选择包含 : 的第二个结果。</p>
<p>token healing方法在guidance-ai中有实现。</p>
<p>另外一个方法是subword
regularization，就是在训练时，随机拆分已经分好的token，获得sub-optimal
tokenization的结果。这些结果不是最好的切分结果，但是可以帮助模型缓解token
boundary bias。</p>
<p>DeepSeek-V3用的就是第二种方法。</p>
<h1 id="训练设置">训练设置</h1>
<h2 id="预训练">预训练</h2>
<p>DeepSeek-V3有多阶段的预训练。</p>
<p>第一阶段（基础通用预训练）：</p>
<ul>
<li>长度4k<br>
</li>
<li>gradient clipping norm = 1.0<br>
</li>
<li>前2k步中，lr从0整张到2.2e-4，然后保持constant lr训练10T token<br>
</li>
<li>在之后的4.3T token，lr用cosine schedule下降到2.2e-5<br>
</li>
<li>在之后的333B，保持lr=2.2e-5<br>
</li>
<li>在最后的167B，lr切换到7.3e-6<br>
</li>
<li>batch
size在最初的469B数据，逐渐从3072提升到15360，之后保持15360<br>
</li>
<li>expert分配在8个节点64个GPU上<br>
</li>
<li>负载平衡速度𝛾在最初的14.3T设为1，之后设为0<br>
</li>
<li>MTP loss weight在前10T token 𝜆 = 0.3，后4.8T设为0.1</li>
</ul>
<p>第二阶段（长窗口预训练）：</p>
<ul>
<li>窗口长度从4k提升到32k，lr=7.3e-6，batch size =
1920，训练1000步<br>
</li>
<li>窗口长度从32k提升到128k，lr=7.3e-6，batch size =
1920，训练1000步</li>
</ul>
<h2 id="对齐">对齐</h2>
<p>SFT：</p>
<ul>
<li>SFT数据共有1.5M条，训练2个epoch<br>
</li>
<li>lr从5e-6降到1e-6，cosine schedule<br>
</li>
<li>使用sample masking strategy，各个sample不互相看见<br>
</li>
<li>reasoning data
<ul>
<li>部分来自DeepSeek-R1<br>
</li>
<li>对于每个领域，比如代码，数学，通过SFT +
RL训练领域专家模型，用于生成对应领域的数据<br>
</li>
<li>主要有两类格式：&lt;problem, original response&gt;，&lt;system
prompt, problem, R1 response&gt;<br>
</li>
</ul></li>
<li>non-reasoning data
<ul>
<li>用DeepSeek-V2.5来生成response<br>
</li>
<li>人力来对数据进行检查和更正</li>
</ul></li>
</ul>
<p>RL:</p>
<ul>
<li>使用Group Relative Policy Optimization</li>
</ul>
<h1 id="小结">小结</h1>
<ul>
<li>细节部分有不少优化，包括MTP，tokenizer，document packing等<br>
</li>
<li>MoE还是延续之前的做法，所谓的新的负载平衡应该没有特别大的影响<br>
</li>
<li>MLA + MTP在降低推理成本上有应该有比较重要的地位<br>
</li>
<li>实际上infra做了大量的工作，用于提升训练效率，这块有机会再盘<br>
</li>
<li>总的来说，DeepSeek-V3是算法和工程的优秀实践；踏实把每个细节做好最重要</li>
</ul>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】DeepSeek-V3 Technical Report
https://arxiv.org/abs/2412.19437v1<br>
【2】Better &amp; Faster Large Language Models via Multi-token
Prediction https://arxiv.org/abs/2404.19737<br>
【3】大模型推理加速-投机解码，https://zhuanlan.zhihu.com/p/699670010<br>
【4】大模型推理加速-MEDUSA，https://zhuanlan.zhihu.com/p/703461293<br>
【5】DeepSeek-V2和MLA，https://zhuanlan.zhihu.com/p/708622695<br>
【6】DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts
Language Model，https://arxiv.org/abs/2405.04434<br>
【7】理解Attention:从起源到MHA,MQA和GQA，https://zhuanlan.zhihu.com/p/686149289<br>
【8】MoE模型的前世今生，http://www.linsight.cn/44e38c1b.html<br>
【9】Fewer Truncations Improve Language
Modeling，https://arxiv.org/abs/2404.10830<br>
【10】代码大模型(一)--业界现状，https://www.linsight.cn/a0b50049.html#fim<br>
【11】The Art of Prompt Design: Prompt Boundaries and Token
Healing，https://medium.com/towards-data-science/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>SFT</tag>
        <tag>DeepSeek</tag>
        <tag>pretrain</tag>
      </tags>
  </entry>
  <entry>
    <title>代码大模型(一)--业界现状</title>
    <url>/a0b50049.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>借助代码大模型进行开发几乎已经是每个开发人员的日常了。代码模型是如何具备强大的代码能力的呢？今天来学习下业界几个比较热门的代码模型。</p>
<h1 id="评测指标">评测指标</h1>
<p>在了解代码模型的训练之前，先了解下目前常用的一些评测指标。</p>
<h2 id="humaneval">HumanEval</h2>
<p>HumanEval由OpenAI在《Evaluating Large Language Models Trained on
Code》提出，共包含164个python编程问题。为了尽量避免和模型的训练数据重复（目前代码模型的训练数据几乎包含了网络上能找到的所有代码数据），这些编程问题是人工专门编写的。</p>
<p>HumanEval原始数据中，每个问题包含以下字段：<br>
- task_id：每个问题的id，如“HumanEval/0”<br>
- prompt：编程问题的主体，形如</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> has_close_elements(numbers: List[<span class="bu">float</span>], threshold: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">bool</span>:</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot; Check if in given list of numbers, are any two numbers closer to each other than</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">    given threshold.</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; has_close_elements([1.0, 2.0, 3.0], 0.5)</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    False</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">    True</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span>    </span></code></pre></div>
<ul>
<li>canonical_solution：参考答案，形如</li>
</ul>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx, elem <span class="kw">in</span> <span class="bu">enumerate</span>(numbers):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx2, elem2 <span class="kw">in</span> <span class="bu">enumerate</span>(numbers):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx <span class="op">!=</span> idx2:</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>                distance <span class="op">=</span> <span class="bu">abs</span>(elem <span class="op">-</span> elem2)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> distance <span class="op">&lt;</span> threshold:</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">False</span></span></code></pre></div>
<ul>
<li>test：单元测试的样例，HumanEval平均每个问题有 7.7 个测试样例</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>METADATA <span class="op">=</span> &#123;</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;author&#39;</span>: <span class="st">&#39;jt&#39;</span>,</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;dataset&#39;</span>: <span class="st">&#39;test&#39;</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>&#125;</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> check(candidate):</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> candidate([<span class="fl">1.0</span>, <span class="fl">2.0</span>, <span class="fl">3.9</span>, <span class="fl">4.0</span>, <span class="fl">5.0</span>, <span class="fl">2.2</span>], <span class="fl">0.3</span>) <span class="op">==</span> <span class="va">True</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> candidate([<span class="fl">1.0</span>, <span class="fl">2.0</span>, <span class="fl">3.9</span>, <span class="fl">4.0</span>, <span class="fl">5.0</span>, <span class="fl">2.2</span>], <span class="fl">0.05</span>) <span class="op">==</span> <span class="va">False</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> candidate([<span class="fl">1.0</span>, <span class="fl">2.0</span>, <span class="fl">5.9</span>, <span class="fl">4.0</span>, <span class="fl">5.0</span>], <span class="fl">0.95</span>) <span class="op">==</span> <span class="va">True</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> candidate([<span class="fl">1.0</span>, <span class="fl">2.0</span>, <span class="fl">5.9</span>, <span class="fl">4.0</span>, <span class="fl">5.0</span>], <span class="fl">0.8</span>) <span class="op">==</span> <span class="va">False</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> candidate([<span class="fl">1.0</span>, <span class="fl">2.0</span>, <span class="fl">3.0</span>, <span class="fl">4.0</span>, <span class="fl">5.0</span>, <span class="fl">2.0</span>], <span class="fl">0.1</span>) <span class="op">==</span> <span class="va">True</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> candidate([<span class="fl">1.1</span>, <span class="fl">2.2</span>, <span class="fl">3.1</span>, <span class="fl">4.1</span>, <span class="fl">5.1</span>], <span class="fl">1.0</span>) <span class="op">==</span> <span class="va">True</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> candidate([<span class="fl">1.1</span>, <span class="fl">2.2</span>, <span class="fl">3.1</span>, <span class="fl">4.1</span>, <span class="fl">5.1</span>], <span class="fl">0.5</span>) <span class="op">==</span> <span class="va">False</span></span></code></pre></div>
<ul>
<li>entry_point：代码运行的入口，用于评测</li>
</ul>
<p>论文中，使用的是pass@k的评测指标，即模型会多次进行生成采样，计算通过评测的期望值。而在OpenCompass中，默认是使用pass@1，即只生成一次，并在对应的评测样例上进行测试。</p>
<p>OpenCompass在生成的时候，默认会加上个prompt：“Complete the following
python code:”，即模型的输入是</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>Complete the following python code:</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> has_close_elements(numbers: List[<span class="bu">float</span>], threshold: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">bool</span>:</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot; Check if in given list of numbers, are any two numbers closer to each other than</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">    given threshold.</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; has_close_elements([1.0, 2.0, 3.0], 0.5)</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">    False</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">    True</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span></code></pre></div>
<p>OpenCompass调用的是OpenAI的evaluate_functional_correctness接口进行评测的，输入就是模型生成的代码以及测试样例。</p>
<h2 id="mbpp">MBPP</h2>
<p>MBPP = Mostly Basic Programming
Problems，而OpenCompass上最常用就是MBPP-python。</p>
<p>MBPP由谷歌在《Program Synthesis with Large Language
Models》中引入。每个问题也是由人工编写的，每个问题包含3个测试用例。</p>
<p>MBPP中，有 58% 的问题是数学相关的（如计算球体体积），43%
涉及列表处理，19% 需要字符串处理，9% 涉及整数序列，2%
涉及其他数据结构。参考解决方案的代码行数平均为 6.8 行，中位数为 5
行，最多为 50 行，自然语言描述通常简短，一般为一句话。</p>
<p>原始数据中，每个问题包含以下字段： - task_id：每个问题的id<br>
- text：问题的文字描述，如：“Write a function to find the maximum
difference between available pairs in the given tuple list.”<br>
- code：参考答案，如</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>R <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> min_cost(cost, m, n): </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    tc <span class="op">=</span> [[<span class="dv">0</span> <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(C)] <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(R)] </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    tc[<span class="dv">0</span>][<span class="dv">0</span>] <span class="op">=</span> cost[<span class="dv">0</span>][<span class="dv">0</span>] </span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, m<span class="op">+</span><span class="dv">1</span>): </span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        tc[i][<span class="dv">0</span>] <span class="op">=</span> tc[i<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>] <span class="op">+</span> cost[i][<span class="dv">0</span>] </span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, n<span class="op">+</span><span class="dv">1</span>): </span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        tc[<span class="dv">0</span>][j] <span class="op">=</span> tc[<span class="dv">0</span>][j<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> cost[<span class="dv">0</span>][j] </span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, m<span class="op">+</span><span class="dv">1</span>): </span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, n<span class="op">+</span><span class="dv">1</span>): </span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>            tc[i][j] <span class="op">=</span> <span class="bu">min</span>(tc[i<span class="op">-</span><span class="dv">1</span>][j<span class="op">-</span><span class="dv">1</span>], tc[i<span class="op">-</span><span class="dv">1</span>][j], tc[i][j<span class="op">-</span><span class="dv">1</span>]) <span class="op">+</span> cost[i][j] </span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tc[m][n]</span></code></pre></div>
<ul>
<li>test_list：测试样例，如</li>
</ul>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>[<span class="st">&#39;assert min_cost([[1, 2, 3], [4, 8, 2], [1, 5, 3]], 2, 2) == 8&#39;</span>, <span class="st">&#39;assert min_cost([[2, 3, 4], [5, 9, 3], [2, 6, 4]], 2, 2) == 12&#39;</span>, <span class="st">&#39;assert min_cost([[3, 4, 5], [6, 10, 4], [3, 7, 5]], 2, 2) == 16&#39;</span>]</span></code></pre></div>
<ul>
<li>challenge_test_list：比较难的测试样例，只有部分问题有，如</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>[<span class="st">&#39;assert remove_Occ(&quot;hellolloll&quot;,&quot;l&quot;) == &quot;helollol&quot;&#39;</span>, <span class="st">&#39;assert remove_Occ(&quot;&quot;,&quot;l&quot;) == &quot;&quot;&#39;</span>]</span></code></pre></div>
<p>test_setup_code：关于评测条件的一些设置，只有极少量问题有，如</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>root <span class="op">=</span> Node(<span class="dv">1</span>) </span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>root.left <span class="op">=</span> Node(<span class="dv">2</span>) </span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>root.right <span class="op">=</span> Node(<span class="dv">3</span>) </span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>root.left.left <span class="op">=</span> Node(<span class="dv">4</span>) </span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>root.left.right <span class="op">=</span> Node(<span class="dv">5</span>) </span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>root.left.left.left <span class="op">=</span> Node(<span class="dv">8</span>) </span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>root1 <span class="op">=</span> Node(<span class="dv">1</span>) </span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>root1.left <span class="op">=</span> Node(<span class="dv">2</span>) </span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>root1.right <span class="op">=</span> Node(<span class="dv">3</span>) </span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>root1.left.left <span class="op">=</span> Node(<span class="dv">4</span>) </span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>root1.left.right <span class="op">=</span> Node(<span class="dv">5</span>) </span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>root1.right.left <span class="op">=</span> Node(<span class="dv">6</span>) </span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>root1.left.left.left <span class="op">=</span> Node(<span class="dv">7</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>root2 <span class="op">=</span> Node(<span class="dv">1</span>) </span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>root2.left <span class="op">=</span> Node(<span class="dv">2</span>) </span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>root2.right <span class="op">=</span> Node(<span class="dv">3</span>) </span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>root2.left.left <span class="op">=</span> Node(<span class="dv">4</span>) </span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>root2.left.right <span class="op">=</span> Node(<span class="dv">5</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>root2.left.left.left <span class="op">=</span> Node(<span class="dv">7</span>)</span></code></pre></div>
<p>OpenCompass在评测时，会使用其中500条，并加上few-shot样本，这些样本是固定的，也是来自MBPP数据集。prompt如下：</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>You are an expert Python programmer, <span class="kw">and</span> here <span class="kw">is</span> your task: Write a function to find the similar elements <span class="im">from</span> the given two <span class="bu">tuple</span> lists. Your code should <span class="cf">pass</span> these tests:</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a> <span class="cf">assert</span> similar_elements((<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>),(<span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">4</span>, <span class="dv">10</span>)) <span class="op">==</span> (<span class="dv">4</span>, <span class="dv">5</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a> <span class="cf">assert</span> similar_elements((<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>),(<span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">7</span>)) <span class="op">==</span> (<span class="dv">3</span>, <span class="dv">4</span>) </span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a> <span class="cf">assert</span> similar_elements((<span class="dv">11</span>, <span class="dv">12</span>, <span class="dv">14</span>, <span class="dv">13</span>),(<span class="dv">17</span>, <span class="dv">15</span>, <span class="dv">14</span>, <span class="dv">13</span>)) <span class="op">==</span> (<span class="dv">13</span>, <span class="dv">14</span>) </span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>[BEGIN]</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a> <span class="co">&#39;def similar_elements(test_tup1, test_tup2):</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="er">  res = tuple</span>(<span class="bu">set</span>(test_tup1) <span class="op">&amp;</span> <span class="bu">set</span>(test_tup2))</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> (res)<span class="st">&#39; </span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="er">[DONE] </span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="er"> </span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="er">You are an expert Python programmer, and here is your task: Write a python function to identify non-prime numbers. Your code should pass these tests:</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a> <span class="cf">assert</span> is_not_prime(<span class="dv">2</span>) <span class="op">==</span> <span class="va">False</span> </span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a> <span class="cf">assert</span> is_not_prime(<span class="dv">10</span>) <span class="op">==</span> <span class="va">True</span> </span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a> <span class="cf">assert</span> is_not_prime(<span class="dv">35</span>) <span class="op">==</span> <span class="va">True</span> </span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>[BEGIN]</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a> <span class="co">&#39;import math</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a><span class="er">def is_not_prime</span>(n):</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> <span class="va">False</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,<span class="bu">int</span>(math.sqrt(n)) <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> n <span class="op">%</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>            result <span class="op">=</span> <span class="va">True</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result<span class="st">&#39; </span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="er">[DONE] </span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="er"> </span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a><span class="er">You are an expert Python programmer, and here is your task: Write a function to find the largest integers from a given list of numbers using heap queue algorithm. Your code should pass these tests:</span></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> heap_queue_largest( [<span class="dv">25</span>, <span class="dv">35</span>, <span class="dv">22</span>, <span class="dv">85</span>, <span class="dv">14</span>, <span class="dv">65</span>, <span class="dv">75</span>, <span class="dv">22</span>, <span class="dv">58</span>],<span class="dv">3</span>)<span class="op">==</span>[<span class="dv">85</span>, <span class="dv">75</span>, <span class="dv">65</span>] </span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> heap_queue_largest( [<span class="dv">25</span>, <span class="dv">35</span>, <span class="dv">22</span>, <span class="dv">85</span>, <span class="dv">14</span>, <span class="dv">65</span>, <span class="dv">75</span>, <span class="dv">22</span>, <span class="dv">58</span>],<span class="dv">2</span>)<span class="op">==</span>[<span class="dv">85</span>, <span class="dv">75</span>] </span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> heap_queue_largest( [<span class="dv">25</span>, <span class="dv">35</span>, <span class="dv">22</span>, <span class="dv">85</span>, <span class="dv">14</span>, <span class="dv">65</span>, <span class="dv">75</span>, <span class="dv">22</span>, <span class="dv">58</span>],<span class="dv">5</span>)<span class="op">==</span>[<span class="dv">85</span>, <span class="dv">75</span>, <span class="dv">65</span>, <span class="dv">58</span>, <span class="dv">35</span>] </span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>[BEGIN]</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a> <span class="co">&#39;import heapq as hq</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a><span class="er">def heap_queue_largest</span>(nums,n):</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>  largest_nums <span class="op">=</span> hq.nlargest(n, nums)</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> largest_nums<span class="st">&#39; </span></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a><span class="er">[DONE] </span></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a><span class="er"> </span></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a><span class="er">You are an expert Python programmer, and here is your task: Write a python function to remove first and last occurrence of a given character from the string. Your code should pass these tests:</span></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> remove_Occ(<span class="st">&quot;hello&quot;</span>,<span class="st">&quot;l&quot;</span>) <span class="op">==</span> <span class="st">&quot;heo&quot;</span></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> remove_Occ(<span class="st">&quot;abcda&quot;</span>,<span class="st">&quot;a&quot;</span>) <span class="op">==</span> <span class="st">&quot;bcd&quot;</span></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> remove_Occ(<span class="st">&quot;PHP&quot;</span>,<span class="st">&quot;P&quot;</span>) <span class="op">==</span> <span class="st">&quot;H&quot;</span>  </span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>[BEGIN]</span></code></pre></div>
<p>最后一个[DONE]后面的才是需要模型解决的问题，前面都是固定的prompt样本。</p>
<p>和HumanEval略有不同，MBPP评测的时候模型可以看到将用于评测的case。</p>
<h2 id="其他评测">其他评测</h2>
<p>HumanEval和MBPP都属于代码生成评测的数据集，并且都是python的。除了这两个，代码生成的评测最近比较常见的还有BigCodeBench（BigCodeBench-Instruct），HumanEval的增强版本EvalPlus等。</p>
<p>另外还有评测代码补全的如RepoBench，以及在FIM（待会讲）中提出的各种Infilling
benchmark。</p>
<p>Code Fixing也有SWE-bench和Aider等评测数据。</p>
<p>不过最近优先关注HumanEval和MBPP的评测得分，所以其他能力以后再展开。</p>
<h1 id="fim">FIM</h1>
<p>FIM =
fill-in-the-middle，是一种模型训练方式，这里参考OpenAI的《Efficient
Training of Language Models to Fill in the Middle》来介绍一下。</p>
<p>首先需要了解为什么需要FIM的训练方式。目前我们知道GPT模型相比Bert类模型，有更高的训练效率；而从左到右自由生成的方式也使得GPT模型能够应用在更多场景，上限更高。但是传统的left-to-right的训练方式也有限制：如在代码补全的场景，需要模型同时兼顾上文和下文，对中间部分的内容进行补全，这种情况下left-to-right的训练方式就无法提供有效的信息，因为看不见下文。</p>
<p>为了解决这个问题，可以对模型的输入数据做一个transformation：把原本顺序正常的文档，切分成三部分，即prefix、middle和suffix，并把middle部分放到最后面。</p>
<p>document -&gt; (prefix; middle; suffix)  -&gt; (prefix; suffix;
middle)</p>
<p>训练的时候，模型需要根据给定的上文prefix和下文suffix，来生成中间的部分。</p>
<h2 id="fim效果">FIM效果</h2>
<p>为了让模型同时具备正常的从左到右的生成能力，需要使用left-to-right和FIM两个方式混合的数据，FIM数据的比例成为FIM
rate。</p>
<p>在实际训练模型中，OpenAI使用FIM rate =
0.5，即训练数据中一半进行了这种切分和转换，另一半保持正常的从左到右顺序。</p>
<p>实验中发现，混入FIM数据之后，模型在正常left-to-right的能力上基本上没有收到任何损害，如下图</p>
<img src="/a0b50049/fim_l2r_ppl.png" class title="代码模型">
<p>相当于模型没有付出任何代价就多学到了FIM的能力，OpenAI称之为FIM-for-free
property。</p>
<p>通常的PPL测试不能看出FIM带来的收益，为了评测模型FIM，OpenAI专门构建一个infilling
benchmark。infilling
benchmark中的数据来源于HumanEval，通过删除中间的部分代码，要求模型补全来检验FIM能力。下面是一个例子，绿色部分就是要模型补全的中间代码：</p>
<img src="/a0b50049/fim_example.png" class title="代码模型">
<p>有无使用FIM数据的模型，在FIM测试上的对比如下：</p>
<img src="/a0b50049/fim_infill_loss.png" class title="代码模型">
<p>加入FIM明显地提升模型的infilling能力。</p>
<h2 id="训练">训练</h2>
<p>前面简单介绍了FIM的做法，就是把数据切成前中后三部分，然后把中间部分挪到后面，要求模型补全。</p>
<p>更具体来说，为了让模型知道哪部分是prefix，哪部分是suffix，哪部分是middle，需要加入一些特殊token：</p>
<p>&lt; PRE &gt; ○ Enc(prefix) ○ &lt; SUF &gt; ○ Enc(suffix) ○ &lt; MID
&gt; ○ Enc(middle)</p>
<p>○ 表示concat。&lt; PRE &gt;、&lt; SUF &gt;和&lt; MID
&gt;就是用于标识数据位置的特殊token。</p>
<p>训练的时候，不止有middle部分的loss会bp，prefix和suffix也和left-to-right的数据一样会进行训练，这样FIM相比left-to-right并不会损失loss信号的量。</p>
<p>另外在每条训练数据最后还要记得加上&lt; EOT &gt;符号。</p>
<p>上面这种拼接方式，数据的顺序是Prefix、Suffix、Middle，简称为PSM。PSM是最符合直觉的一种拼接方式。而除了PSM，还可以使用SPM的拼接顺序。文中提到，SPM相比PSM有一个好处，那就是在推理的时候前面已经计算过的KV
cache可以复用。（不过这里感觉有点奇怪，只要不把新生成的token拼接到Prefix部分，其实PSM的KV
cache也是可以服用的）</p>
<p>那么PSM和SPM的效果如何？实验了SPM、PSM和二者一起训练，效果如下：</p>
<img src="/a0b50049/fim_psm_spm.png" class title="代码模型">
<p>从结果上看，二者都用的效果是最好的。</p>
<h2 id="fim-rate">FIM rate</h2>
<p>FIM rate应该设置为多少合适？文中做了消融实验，对比FIM rate = (0,
0.25, 0.5, 0.75, 0.9, 1.0)的效果。结果如下：</p>
<img src="/a0b50049/fim_rate.png" class title="代码模型">
<p>不同的FIM rate对left-to-right几乎没有什么影响（除了FIM rate =
1.0），而FIM loss则是只要使用FIM数据，就有明显提升。FIM rate =
0.5和0.9的效果差不多，最终使用的是0.5。</p>
<h2 id="数据切分">数据切分</h2>
<p>前面说了数据要被切成三部分，那么具体怎么切？论文提供了三种粒度的对比，Line-level
random span、Token-level random span、Character-level random
span。三种粒度的切分都保证prefix、suffix、middle的期望长度都是总长度的1/3。结果如下：</p>
<img src="/a0b50049/fim_split_level.png" class title="代码模型">
<p>character level的整体效果最好。</p>
<h1 id="code-llama">Code Llama</h1>
<p>接着来看下Code Llama的一些内容。</p>
<p>Code Llama是基于Llama 2开发的。Code Llama系列和Llama
2一样有4个规模，7B, 13B, 34B, 和
70B。各种规模都有3个版本（值得一提的是，Meta还专门做了个针对Python的版本）：<br>
- Code Llama: a foundational model for code generation tasks<br>
- Code Llama - Python: specialized for Python<br>
- Code Llama - Instruct: fine-tuned with human instructions and
self-instruct code synthesis data</p>
<p>各个版本的训练流程如下：</p>
<img src="/a0b50049/codellama_pipeline.png" class title="代码模型">
<p>Code Llama效果也是不错，和其他模型的对比：</p>
<img src="/a0b50049/codellama_eval.png" class title="代码模型">
<h2 id="数据">数据</h2>
<p>Code
Llama收集大约500B的训练数据，其中大部分是代码，另有少量从general数据集中抽取的代码相关数据，以及少量代码无关的数据：</p>
<img src="/a0b50049/codellama_train.png" class title="代码模型">
<p>代码的数据都是训练了多个epoch。</p>
<p>而在微调的时候，则是使用了Llama 2 70B生成interview-style programming
questions。所使用的prompt如下：</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>Prompt: [INST] Write <span class="dv">50</span> programming interview questions of easy <span class="kw">and</span> medium complexity. Provide questions on a</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>diverse <span class="bu">range</span> of subjects, <span class="kw">and</span> make sure no two questions are alike. Make sure the problems can be solved <span class="cf">with</span> a</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>single standalone Python function using standard libraries. [<span class="op">/</span>INST]</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="fl">1.</span> Write a function that finds the maximum depth of <span class="bu">list</span> nesting <span class="kw">in</span> a given <span class="bu">list</span>.</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="fl">2.</span> Given an integer array nums, rotate the array to the right by k steps, where k <span class="kw">is</span> non<span class="op">-</span>negative.</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="fl">3.</span> Write a function that gets the musical pitch that <span class="kw">is</span> closest to a given frequency <span class="kw">in</span> Hz. A pitch should consist of a</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>letter, possibly a <span class="co"># sign, and an octave number.</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="fl">1.</span> Write a function that removes <span class="bu">any</span> sequences of whitespace that are between numbers <span class="kw">in</span> an <span class="bu">input</span> string.</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="fl">2.</span> Write a function that counts the number of words <span class="kw">in</span> a string that have length n, where n <span class="kw">is</span> an <span class="bu">input</span>. The function</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>should ignore characters that aren’t letters, numbers, <span class="kw">or</span> spaces.</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="fl">1.</span> Write a function that returns the longest palindromic substring <span class="kw">in</span> a given string.</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="fl">2.</span> Create a function that will find the length of the longest substring without repeating characters <span class="kw">in</span> a given string.</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="fl">3.</span> Write a function that reverses the diagits <span class="kw">in</span> a number.</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="fl">4.</span> Write a function that finds the valleys <span class="kw">in</span> a <span class="bu">list</span> of integers.</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="fl">5.</span>  Write a python function that reverses every group of k words <span class="kw">in</span> a sentence.</span></code></pre></div>
<p>去重之后得到了~52,000条问题。再把这些问题输入给Code Llama
7B生成单元测试样例和solution，prompt分别是：</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>Prompt:</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>[INST] Your task <span class="kw">is</span> to write <span class="dv">5</span> tests to check the correctness of a function that solves a programming</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>problem.</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>The tests must be between [TESTS] <span class="kw">and</span> [<span class="op">/</span>TESTS] tags.</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>You must write the comment <span class="st">&quot;#Test case n:&quot;</span> on a separate line directly above each <span class="cf">assert</span> statement,</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>where n represents the test case number, starting <span class="im">from</span> <span class="dv">1</span> <span class="kw">and</span> increasing by one <span class="cf">for</span> each subsequent</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>test case.</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>Problem: Write a Python function to get the unique elements of a <span class="bu">list</span>.</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>[<span class="op">/</span>INST]</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>[TESTS]</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Test case 1:</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> get_unique_elements([]) <span class="op">==</span> []</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Test case 2:</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> get_unique_elements([<span class="dv">1</span>]) <span class="op">==</span> [<span class="dv">1</span>]</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Test case 3:</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> get_unique_elements([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>]) <span class="op">==</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>[<span class="op">/</span>TESTS]</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>[INST] Problem: <span class="op">%%%</span>question<span class="op">%%%</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>[<span class="op">/</span>INST]</span></code></pre></div>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>Prompt:</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>[INST] Your task <span class="kw">is</span> to write a Python function to solve a programming problem.</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>The Python code must be between [PYTHON] <span class="kw">and</span> [<span class="op">/</span>PYTHON] tags.</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>You are given one example test <span class="im">from</span> which you can infere the function signature.</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>Problem: Write a Python function to get the unique elements of a <span class="bu">list</span>.</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>Test: <span class="cf">assert</span> get_unique_elements([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>]) <span class="op">==</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>[<span class="op">/</span>INST]</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>[PYTHON]</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_unique_elements(my_list):</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="cf">return</span> <span class="bu">list</span>(<span class="bu">set</span>(my_list))</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>[<span class="op">/</span>PYTHON]</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>[INST] Problem: <span class="op">%%%</span>question<span class="op">%%%</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>Test: <span class="op">%%%</span>test<span class="op">%%%</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>[<span class="op">/</span>INST]</span></code></pre></div>
<p>最后把题目、单元测试样例和答案通过执行反馈的方式验证正确性。</p>
<h2 id="long-context-fine-tuning">Long Context Fine-Tuning</h2>
<p>为了让模型具备长上下文的能力，应用了Long Context
Fine-Tuning（LCFT）：<br>
- 把RoPE的base frequency从10,000增大到1,000,000<br>
- 把训练的最大长度从4k增大到100k</p>
<h2 id="fim训练">FIM训练</h2>
<p>Code
Llama训练的时候用到FIM的训练方式，同OpenAI的做法一样，FIM数据中一半使用PSM格式，另一半使用SPM格式，都是character
level的split。不过这里使用各大的FIM rate：0.9。</p>
<p>FIM的评测效果：</p>
<img src="/a0b50049/codellama_fim.png" class title="代码模型">
<h2 id="相比from-scratch">相比from scratch</h2>
<p>Code Llama是以LLama
2为基础训练的，相比从零开始，使用代码数据进行训练的效果更好。如下图（b），sratch
model相比Code Llama的loss明显更高：</p>
<img src="/a0b50049/codellama_scratch.png" class title="代码模型">
<h1 id="starcoder-2">StarCoder 2</h1>
<p>之前BigCode开源了StarCoder和Stack v1，几年继续打来StarCoder 2和Stack
v2。v2版本的数据集是v1的4倍大，在这些数据集上，训练出了3B, 7B, 和 15B
的StarCoder 2。</p>
<h2 id="数据-1">数据</h2>
<p>1、代码数据</p>
<p>基于Software
Heritage的数据，涵盖619种语言。这些代码数据经过过滤（过滤网页自动生成的内容，恶意内容等）、去重、语言识别等处理。</p>
<p>2、Github Issues</p>
<p>包括issue的操作和内容。这些内容大都和代码的内容和开发相关，虽然不一定是代码数据，但是和代码有很大的相关性。</p>
<p>3、Pull Requests</p>
<p>这些不同分支的merge内容也能提供重要度high level信息。</p>
<p>4、Notebooks</p>
<p>包括Jupyter Notebooks，和Kaggle
Notebooks，这些数据以外可能比较容易被忽略。</p>
<p>5、Documentation</p>
<p>来自各个包管理平台的文档，还有pdf文件，以及各种官方/教程网站。</p>
<p>6、其他高质量数据集</p>
<p>包含用于数学和编程的多个小数据集，如：<br>
- GSM8K<br>
- APPS<br>
- Proofsteps<br>
等</p>
<p>除了代码数据，还有必要加入一些自然语言的数据让模型学习，如 Stack
Overflow、ArXiv、Wikipedia 和 OpenWebMath。</p>
<p>上面这些所有数据都会经过以下处理：<br>
- simhash去重<br>
- 个人信息Personally Identifiable Information (PII)删除<br>
- Decontamination移除和评测集相关内容<br>
- 恶意检测Malware Removal（有0.009%的数据在这步被移除）</p>
<p>最终各个规模模型的训练数据组成如下：</p>
<img src="/a0b50049/starcoder2_train_data.png" class title="代码模型">
<h2 id="训练-1">训练</h2>
<p>收集的数据里有不同类型的数据，包括源码、notebook、issue、pull
request，每种数据都有单独的拼接处理方式。比如源码会拼接成：</p>
<p><repo_name>reponame<file_sep>filepath1<file_sep>filepath2 ...
&lt;|endoftext|&gt;</file_sep></file_sep></repo_name></p>
<p>在没有meta data的情况下则是：</p>
<p><file_sep>code1<file_sep>code2 ... &lt;|endoftext|&gt;</file_sep></file_sep></p>
<p>而pull request数据则是：</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>pr<span class="op">&gt;</span>Title: title\nusername_0: description</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>pr_status<span class="op">&gt;</span>opened</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>repo_name<span class="op">&gt;</span>reponame</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>pr_base<span class="op">&gt;</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>pr_file<span class="op">&gt;</span>filepath_1</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>pr_base_code<span class="op">&gt;</span>file_content<span class="op">/</span>changes_1</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>pr_file<span class="op">&gt;</span>filepath_N</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>pr_base_code<span class="op">&gt;</span>file_content<span class="op">/</span>changes_N</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>pr_diff<span class="op">&gt;</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>pr_file<span class="op">&gt;</span>filepath_1</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>pr_diff_hunk<span class="op">&gt;</span>diff_hunk_1</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>pr_diff_hunk<span class="op">&gt;</span>diff_hunk_K</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>pr_file<span class="op">&gt;</span>filepath_M</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>pr_diff_hunk<span class="op">&gt;</span>diff_hunk_1</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>pr_diff_hunk<span class="op">&gt;</span>diff_hunk_J</span></code></pre></div>
<p>因此增加了很多特殊token用于标识不同的内容：</p>
<img src="/a0b50049/starcoder2_special_token.png" class title="代码模型">
<p>模型结构和在各自数据上的训练配置如下：</p>
<img src="/a0b50049/starcoder2_train.png" class title="代码模型">
<p>可以看到各个模型都在代码数据上训练的多个epoch。</p>
<h1 id="deepseek-coder-v2">DeepSeek-Coder-V2</h1>
<p>DeepSeek-Coder-V2是在DeepSeek-V2（MoE模型）基础上训练的，对应DeepSeek-V2的两个规模：16B和236B，激活参数量分别为2.4B和21B。千亿模型的效果自然是很不错的：</p>
<img src="/a0b50049/dscoderv2_perf.png" class title="代码模型">
<h2 id="数据-2">数据</h2>
<p>DeepSeek-V2训练的4.2T的通用数据，DeepSeek-Coder-V2则是在这个基础，进一步训练6T
token的数据。这6T数据里，有60%的source
code，30%的数学语料，还有10%的自然语言数据，而重点自然就是代码数据和数学数据。</p>
<p>1、代码数据</p>
<p>原始代码数据来自2023年11月前的github数据。</p>
<p>文中给出了一些代码数据的筛选处理逻辑：<br>
- 筛掉平均行长度 &gt; 100 character或者最大行长度 &gt; 1000
character的file<br>
- 筛掉包含的alphabetic character比例 &lt; 25%的file<br>
- 除了XSLT语言，其他在开头100个character包含“&lt; ?xml
version=”的文件会被删掉<br>
- 对于HTML文件，要求visible
text的占比要大于20%，切至少100character<br>
-
对于JSON和YAML这种通常是数据文件的类型，只保留长度为50到5000character的文件</p>
<p>这样清洗过后得到了821B包含338种语言的代码数据和185B的code-related数据（如markdown和issue）。</p>
<p>2、数学数据</p>
<p>对于code-related 和 math-related，follow the same pipeline as
DeepSeekMath。</p>
<p>首先从一些相关网页，比如StackOverflow，PyTorch
documentation或者StackExchange爬取数据。之后训练一个fasttext模型用于从网页数据里recall
code-related 和 math-related 数据。</p>
<p>最终共收集到70B code-related数据和221B math-related数据。</p>
<p>为了检验这些数据的质量，用一个1B的模型在这些数据上进行训练。首先，使用
1T tokens 在新代码语料库上对 1B 模型进行预训练。然后，观察其在 HumanEval
和 MBPP 基准测试中的准确率变化。结果显示，在 HumanEval
基准测试中，准确率从 30.5% 提高到 36.0%，提高了 5.5%；在 MBPP
基准测试中，准确率从 44.6% 提高到 49.0%，提高了 4.4%。接着，进一步使用
2T tokens 对 1B 模型进行训练。再次观察在两个基准测试中的准确率，发现
HumanEval 和 MBPP 的分数分别上升到 37.2% 和 54.0%。</p>
<img src="/a0b50049/dscoderv2_data_perf.png" class title="代码模型">
<p>通过这些实验结果可以看出，新代码语料库在提高模型准确率方面表现更好，因此新代码语料库优于用于训练
DeepSeek-Coder-V1 的语料库。</p>
<h2 id="训练-2">训练</h2>
<p>对于16B模型，使用了left-to-right和FIM两种训练方式，而对于236B模型，则没有使用FIM。</p>
<img src="/a0b50049/dscoderv2_train_setting.png" class title="代码模型">
<p>另外，为了保持模型的长文本能力，进行了长窗口的训练。用32k长度的数据，以batch
size = 1152训练了1000个step，然后把长度扩展到128k，用288的batch
size再训练1000个step。之后再通过YARN进行长度扩展：s=40，alpha=1，beta=32。</p>
<p>而在alignment阶段，使用了20k条代码相关，30k条数学相关的instruction数据，同时从DeepSeek-V2中也采样了一些通用instruction数据，构成300M
token的训练数据集。</p>
<p>除了instruction
tuning，DeepSeek-Coder-V2还进行RL。虽然代码数据可以通过执行反馈作为feedback，不过仍然可能出现部分测试case覆盖不全的情况，因此还是训练了一个reward
model对生成的结果进行打分。</p>
<h1 id="qwen2.5-coder">Qwen2.5-Coder</h1>
<p>Qwen2.5-Coder有两个规模，1.5B和7B。</p>
<img src="/a0b50049/qwen2.5coder_model.png" class title="代码模型">
<img src="/a0b50049/qwen2.5coder_perf.png" class title="代码模型">
<h2 id="数据-3">数据</h2>
<p>Qwen2.5-Coder在Qwen2.5的基础上，又进行了5.5T数据的训练。</p>
<p>这些数据主要有5种：<br>
- Source Code Data<br>
- Text-Code Grounding Data<br>
- Synthetic Data<br>
- Math Data<br>
- Text Data</p>
<p>1、Source Code Data</p>
<p>来自github
2024年2月前的数据，包括92种语言。和StarCoder2类似，也会进行一系列的rule-based处理。处理源码之外，还有Pull
Requests, Commits, Jupyter Notebooks, 和 Kaggle datasets等。</p>
<p>2、Text-Code Grounding Data</p>
<p>包括代码相关的documentation, tutorials, blogs等。</p>
<p>这些数据使用小规模的模型（如fasttext）进行了清洗和过滤。发现更大的模型并不能带来更好的清洗效果，可能是因为小模型更加能够关注在surface-level的feature。</p>
<p>3、Synthetic Data</p>
<p>基于CodeQwen1.5生成了大量的合成数据，并通过执行反馈筛选。</p>
<p>4、Math Data</p>
<p>使用了Qwen2.5-Math的数据。</p>
<p>5、Text Data</p>
<p>从Qwen2.5的数据中抽取。会把代码相关的数据从这里面删除掉。</p>
<p>数据混合比例上，文中做了3中配比的实验：</p>
<img src="/a0b50049/qwen2.5coder_data_mix.png" class title="代码模型">
<p>Code：Text：Math =
7：2：1的效果是最好的，比其他代码比例更高的配比效果更好。</p>
<h2 id="预训练">预训练</h2>
<p>Qwen2.5-Coder的训练流程如下：</p>
<img src="/a0b50049/qwen2.5coder_train_pipeline.png" class title="代码模型">
<p>1、File-Level Pretraining</p>
<p>这一阶段的训练长度为8192，使用了left-to-right和FIM两个训练目标。</p>
<p>tokenizer中特意增加了一些token用于标识FIM的中各个部分的位置。</p>
<img src="/a0b50049/qwen2.5coder_fim_file.png" class title="代码模型">
<img src="/a0b50049/qwen2.5coder_fim_token.png" class title="代码模型">
<p>2、Repo-Level Pretraining</p>
<p>第二阶段是针对repo-level数据的训练。repo-level数据的一个特点就是文件多，样本长。因此需要模型具备长窗口的能力。这一阶段把RoPE的base
frequency从10,000提升到1,000,000，进行了32k长度的训练，在推理时再通过YARN把窗口扩展到132k。</p>
<p>这个阶段训练了大约300B
token的数据。FIM也从file-level改为repo-level的格式：</p>
<img src="/a0b50049/qwen2.5coder_fim_repo.png" class title="代码模型">
<h2 id="post-training">post-training</h2>
<p>在post-training阶段，有以下关键action。</p>
<p>1、Multilingual Programming Code Identification</p>
<p>微调 CodeBERT
进行语言识别，保留主流编程语言的指令数据，随机丢弃部分长尾语言部分指令数据。对于被识别为“无编程语言”的部分数据，则大部分都会被去掉，以免影响模型的代码生成能力。</p>
<p>2、代码指令合成</p>
<p>针对 GitHub
等网上大量存在的无监督数据（代码片段），构建监督指令数据集。</p>
<p>具体做法是使用 LLM 根据代码片段生成指令（长度在 1024
个标记内），然后再用代码 LLM 生成响应，最后使用 LLM
评分器过滤低质量的指令和响应对，以获得最终的配对。通过这种方式，可以从不同编程语言的代码片段中构建指令数据集。</p>
<p>为了增加指令数据集的多样性，也可以先生成答案，然后用 LLM
评分器过滤获得最终的三元组，从而构建出具有通用代码的指令数据集。此外，还将开源指令数据集纳入到种子指令数据集中。</p>
<p>3、Multilingual Code Instruction Data</p>
<p>提出了一个multilingual
multi-agent协作框架，来合成多语言指令语料库。</p>
<p>具体过程如下： -
创建一组特定于语言的agent，每个agent专门负责一种特定的编程语言，这些agent使用从有限的多语言指令语料库中提取的特定语言指令数据进行初始化；<br>
-
多个语言特定的agent通过结构化对话来制定新的指令和解决方案，这个过程可以增强现有语言的能力或为新编程语言生成指令；<br>
-
每个agent维护一个动态存储库来存储其生成历史，以避免生成相似的样本；<br>
-
开发了一种知识蒸馏方法，使各个agent能够跨越语言边界共享内容，促进对编程概念的更全面理解；</p>
<h1 id="小结">小结</h1>
<p>纵观几个最新的代码模型，有几点关键发现：<br>
- FIM训练对于代码补全能力至关重要<br>
- 代码能力和数学能力几乎总是成对出现的，这二者有很强的关联性<br>
-
代码数据量的需求很大，需要收集尽量多的数据，而且不止需要源码，和代码相关的文档、issue、教程等都很重要</p>
<p>在关于代码具体怎么清洗、组织和生成上，基本上依然是各家秘而不宣的核心，而这也是我们要探索的关键点。</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门--CLIP</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】Evaluating Large Language Models Trained on Code
https://arxiv.org/abs/2107.03374<br>
【2】Program Synthesis with Large Language Models
https://arxiv.org/abs/2108.07732<br>
【3】Efficient Training of Language Models to Fill in the Middle
https://arxiv.org/abs/2207.14255<br>
【4】Code Llama: Open Foundation Models for Code
https://arxiv.org/abs/2308.12950<br>
【5】StarCoder 2 and The Stack v2: The Next Generation
https://arxiv.org/abs/2402.19173<br>
【6】DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in
Code Intelligence https://arxiv.org/abs/2406.11931<br>
【7】Qwen2.5-Coder Technical Report https://arxiv.org/abs/2409.12186</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>预训练</tag>
        <tag>对齐</tag>
        <tag>数据</tag>
        <tag>代码能力</tag>
      </tags>
  </entry>
  <entry>
    <title>CV入门--关于Vision Transformer</title>
    <url>/a11e2633.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>Transformer在自然语言的应用很成功，而在CV领域的崛起相对来说就比较慢，毕竟Transformer最初是为自然语言设计的。</p>
<p>图片相比自然语言，多了2D的结构信息，因此从输入开始就需要对数据做一些处理。</p>
<p>一个最直接的处理方法就是把每个pixel当成一个token，并把二维的图片序列化成一维的序列。比如原来的图像可以看做一个二维数组：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line"> [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line"> [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</span><br></pre></td></tr></table></figure>
<p>flatten之后就成了1D的序列：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</span><br></pre></td></tr></table></figure>
<p>这样224×224的图片变成一个包含50176个pixel（token）的输入序列。不过这样做输入长度明显太长了。50k的长度放在今天的LLM依然是颇有挑战性的一个长度，而这还只是一张224×224的小图片，如果是更大的图片处理起来就更困难了。</p>
<p>因此要处理图像信息，还需要做一些改动。有几个工作的效果不错，包括iGPT、ViT、Swin
Transformer等，下面简单梳理一下这几个方法的思路和重要内容。</p>
<p>（以下内容需要读者具备基本的Transformer、CNN、GPT、Bert以及图像数据的相关知识）</p>
<h1 id="igpt">iGPT</h1>
<p>论文：《Generative Pretraining from Pixels》</p>
<p>时间：2020年</p>
<p>机构：OpenAI</p>
<p>这是Ilya参与的一个工作，不仅把Transformer应用到CV上，并且还是用无监督的方式进行训练。</p>
<h2 id="预训练方式">预训练方式</h2>
<p>论文主要是用GPT-2模型做的实验，顺带着把Bert也对比了一下。</p>
<p>下图给出了怎么用GPT和Bert进行图像预训练：</p>
<img src="/a11e2633/igpt_intro.png" class title="vision transformer">
<p>第一步是要对图像数据进行处理。如前面所说，直接用原图的一个pixel作为一个token，会导致输入长度太长，那么iGPT的做法就是对图像进行下采样，降低分辨率。</p>
<p>一开始，所有数据都会被resize到224×224（对于使用data
augmentation的情况，则是先resize到一个更大的size，然后random
crop一个224×224的image，不过总之结果都是给出一个224×224大小的图像）。</p>
<p>不要忘了RGB图像还有三个channel，所以实际的图像数据点是224×224×3。iGPT会在这个基础上进行下采样，下采样的目标分辨率称之为input
resolution（IR），论文里用了三种IR做实验：32×32×3、48×48×3、64×64×3。</p>
<p>关于为什么不进一步降低大小：因为参考《80 Million Tiny Images: A Large
Data Set for Nonparametric Object and Scene
Recognition》的研究，人类在32以下的分辨率，图像分类的准确率就会快速下降，说明再继续减小分辨率可能会损失很多重要信息。</p>
<p>32×32×3虽然已经很小，这些像素点拉直成序列之后，仍然有3072的长度，计算量仍然不小。为了进一步减少计算量，iGPT用9-bit
color
palette（也就是有512个cluster）对（R,G,B）像素点进行聚类，这样就把3个channel的数据合成为1个数据，并且在颜色上没有明显的损失。这样一来长度就减小到三分之一，这个长度（32×32/48×48/64×64）称之为model
resolution（MR）。</p>
<p>之后把得到的二位图像进行序列化，就得到了GPT/Bert的输入数据了。</p>
<p>而在训练目标上，和自然语言的情况一样，对于GPT模型，要求模型根据前面的输入pixel，预测下一个pixel：</p>
<p><span class="math display">\[L_{AR}=\mathbb{E}_{x\sim X}[-\log
p(x)]\]</span></p>
<p>类似地，对于Bert模型，mask掉部分pixel，并要求模型预测被mask的pixel。</p>
<p><span class="math display">\[L_{BERT}=\underset{x\sim
X}{\mathbb{E}}\underset{M}{\operatorname*{\mathbb{E}}}\sum_{i\in
M}\left[-\log p\left(x_i|x_{[1,n]\setminus M}\right)\right]\]</span></p>
<h2 id="评测方式">评测方式</h2>
<p>验证预训练模型效果好不好，有两个方法：finetune和linear probing。</p>
<p>finetune的时候会用最后一层输出的average
pooling，加上一个projection得到class logits。loss的计算上，除了使用cross
entropy之外，还会额外加上预训练损失L_GEN：</p>
<p><span class="math display">\[L_{FT}=L_{GEN}+L_{CLF}\]</span></p>
<p><span class="math display">\[L_{GEN}\in\{L_{AR},L_{BERT}\}\]</span></p>
<p>linear
probing则是仅用预训练模型给出的特征和projection进行图像分类，并且不会训练预训练模型的参数。还有一点不同就是linear
probing用的不一定是最后一层的特征，因为最后一层的特征未必是最好的。</p>
<h2 id="实验">实验</h2>
<p>实验用的模型如下：</p>
<center>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">模型</th>
<th style="text-align: center;">层数</th>
<th style="text-align: center;">参数量</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">iGPT-XL</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">6.8B</td>
</tr>
<tr class="even">
<td style="text-align: center;">iGPT-L</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="odd">
<td style="text-align: center;">iGPT-M</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">455M</td>
</tr>
<tr class="even">
<td style="text-align: center;">iGPT-S</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">76M</td>
</tr>
</tbody>
</table>
</center>
<p>1、对于linear probe，哪层的效果最好</p>
<p>如上面所说，实验中发现，对于linear
probe，最后一层的表征往往不是最好的。对于iGPT-L，在不同任务上用不同层的特征做线性分类的效果如下</p>
<img src="/a11e2633/igpt_layer.png" class title="vision transformer">
<p>基本上都是在20层左右效果最好，往前或者往后的层效果都下降了。</p>
<p>2、是不是预训练模型越好，下游任务效果越好</p>
<p>答案是yes。下图给出预训练的validation loss和linear
probe效果的关系：</p>
<img src="/a11e2633/igpt_pretrain_loss.png" class title="vision transformer">
<p>注意横轴的值往右是减小的。</p>
<p>3、iGPT和其他模型效果对比</p>
<p>用linear
probe任务评测，在CIFAT-10和CIFAT-100上，iGPT-L效果比SimCLR和ResNet-152都更好一些。</p>
<img src="/a11e2633/igpt_perf1.png" class title="vision transformer">
<p>而在ImageNet上测试时，iGPT则遇到问题。由于不能在标准 ImageNet
输入分辨率上进行高效训练，在model
resolution为32×32时效果并不好，只有把MR提升到64×64，并且使用iGPT-XL时，才达到还不错的效果。但是计算量远超其他方法了。</p>
<img src="/a11e2633/igpt_perf2.png" class title="vision transformer">
<p>4、微调效果</p>
<p>经过微调之后，iGPT的效果有所提升，不过还达不到其他最佳模型的效果。</p>
<img src="/a11e2633/igpt_ft.png" class title="vision transformer">
<p>5、对比GPT和Bert</p>
<p>在linear
probe上，Bert模型的效果比GPT差很多，而微调之后Bert能够赶上GPT的效果。</p>
<img src="/a11e2633/igpt_bert.png" class title="vision transformer">
<h1 id="vit">ViT</h1>
<p>论文：《An Image is Worth 16x16 Words: Transformers for Image
Recognition at Scale》</p>
<p>时间：2020年10月</p>
<p>机构：Google</p>
<h2 id="把transformer应用在图像">把transformer应用在图像</h2>
<p>这是来自Google的工作，也是把transformer成功应用到图像领域的经典工作。</p>
<p>所用的模型主体是transformer encoder：</p>
<img src="/a11e2633/vit_intro.png" class title="vision transformer">
<p>整体上和Bert和相似，最大的不同就是输入部分的处理。</p>
<p>对于文本来说，输入数据是一个1D序列，但是图像是2D的，需要把2D的图像适配到transformer的输入格式。一个简单的办法就是flatten。这样就能把图像信息输入到transformer里了。当然这样会有一个问题，就是flatten之后的图像丢失了部分pixel之间的结构化位置信息。一个折中的办法就是不以pixel为单位，而是以一个patch为单位来flatten，这样可以减少丢失的结构信息。</p>
<p>具体来说，对于一张图像 <span class="math inline">\(\mathbf{x}\in\mathbb{R}^{H\times W\times
C}\)</span>，在H和W维度切成大小为P×P的patch，这样图像数据就变成 <span class="math inline">\(\mathrm{x}_p\in\mathbb{R}^{N\times(P^2\cdot
C)}\)</span>，而 <span class="math inline">\(N=HW/P^2\)</span>。</p>
<p>每个patch再经过一个trainable linear projection之后，就是patch
embedding。patch embedding相当于文本模型里的token
embedding，图像模型patch进行线性变换的操作对应文本模型里从vocab取对应token
embedding的操作，而图像切分patch则相当于文本的tokenization。</p>
<p>不要忘记还有位置编码。论文里对下面集中情况做了实验：<br>
- 不使用位置编码，输入相当于是bag of patches（就像bag of words）<br>
- 使用1D的位置编码，和文本transformer一样<br>
-
使用2D的位置编码，embedding的前一半维度加上的是X-embedding，后一半维度加上的是Y-embedding<br>
- 1D的相对位置编码</p>
<p>各个方法的效果对比如下：</p>
<img src="/a11e2633/vit_pos.png" class title="vision transformer">
<p>基本上只要加了位置编码效果就差不多，都比不加要好，因此这里选择了传统的可学习式的1D位置编码，这样实现最简单。</p>
<p>由于transformer本身理论上是可以接受任意长度的输入的，所以预训练好的ViT也可以用于higher
resolution的场景，不过这个时候需要对位置编码做一下处理。对于可训练的绝对位置编码，一个方法是进行插值。</p>
<p>此外，ViT参考Bert的做法，在最开始加入一个[class]token，作为整个图像的表征。</p>
<p>最终ViT的输入就是：</p>
<p><span class="math display">\[\mathrm{z}_0=[\mathrm{x}_{\mathrm{class}};\mathrm{~x}_p^1\mathrm{E};\mathrm{~x}_p^2\mathrm{E};\cdots;\mathrm{~x}_p^N\mathrm{E}]+\mathrm{E}_{pos},\quad\mathrm{E}\in\mathbb{R}^{(P^2\cdot
C)\times D},\mathrm{~E}_{pos}\in\mathbb{R}^{(N+1)\times D}\]</span></p>
<p>后面的计算就和Bert一样了。</p>
<p>和自然语言以及iGPT的情况不同，ViT预训练使用的是有监督的数据。</p>
<h2 id="实验-1">实验</h2>
<p>在Google之前也有不少人尝试把transformer应用到CV上，但是效果没有那么显著，原因就是预训练数据量不够大。</p>
<p>实验用到的3个transformer模型如下：</p>
<img src="/a11e2633/vit_model.png" class title="vision transformer">
<p>作为对比，还有几个CNN：<br>
- ResNet<br>
- hybrids：把ResNet的中间feature map作为ViT的输入</p>
<p>还有3个不同规模的预训练数据集：<br>
- 1.3M张图片的ILSVRC-2012 ImageNet dataset<br>
- 14M张图片的ImageNet-21k<br>
- 303M张图片的JFT</p>
<p>实验发现，随着预训练的数据量增大，ViT相对ResNet的优势逐渐明显：</p>
<img src="/a11e2633/vit_perf.png" class title="vision transformer">
<h1 id="deitdata-efficient-image-transformers">DeiT（Data-efficient
image Transformers）</h1>
<p>论文：《Training data-efficient image transformers &amp; distillation
through attention》</p>
<p>时间：2020年12月</p>
<p>机构：Facebook AI</p>
<h2 id="tldr">TL;DR</h2>
<p>ViT里提到需要比较多的预训练数据，模型的效果才能比较好。而DeiT通过使用蒸馏，仅在ImageNet上训练，获得了综合throughput
&amp; 任务效果更佳的模型。</p>
<img src="/a11e2633/deit_intro.png" class title="vision transformer">
<h2 id="蒸馏">蒸馏</h2>
<p>DeiT主要内容是提出一种使用图像transformer学习的蒸馏策略。student模型和ViT一样，而teacher模型可以是任意模型。</p>
<p>通常使用的蒸馏，soft distillation，是在学习true
label之余，增加一个KL散度损失来学习teacher model的知识：</p>
<p><span class="math display">\[\mathcal{L}_\mathrm{global}=(1-\lambda)\mathcal{L}_\mathrm{CE}(\psi(Z_\mathrm{s}),y)+\lambda\tau^2\mathrm{KL}(\psi(Z_\mathrm{s}/\tau),\psi(Z_\mathrm{t}/\tau))\]</span></p>
<p>Z_t是teacher model的logits，Z_s是student
model的logits，τ是蒸馏的温度，而λ控制两项损失的比重。</p>
<p>相对soft distillation，DeiT提出hard-label distillation：不学习teacher
model的logits了，直接学习teacher model的最终label：</p>
<p><span class="math display">\[\mathcal{L}_{\mathrm{global}}^{\mathrm{hardDistill}}=\frac{1}{2}\mathcal{L}_{\mathrm{CE}}(\psi(Z_{s}),y)+\frac{1}{2}\mathcal{L}_{\mathrm{CE}}(\psi(Z_{s}),y_{\mathrm{t}})\]</span></p>
<p><span class="math display">\[y_\mathrm{t} =
\mathrm{argmax}_cZ_\mathrm{t}(c)\]</span></p>
<p>hard-label distillation也可以通过label smoothing手动转成soft
label：固定把 $1-$ 的概率给true
label，而剩余的概率就分给其他所有类别。DeiT使用了 $ $。</p>
<p>通常来说，来自true label和teacher model的信息都可以由class token（ +
classifier）来学。而DeiT的做法是增加一个distillation
token专门用来学习蒸馏的知识。</p>
<img src="/a11e2633/deit_framework.png" class title="vision transformer">
<p>实验中发现class token embedding和distillation token
embedding收敛到完全不同的表达，二者之间的cos相似度只有0.06；而随着层数增加，到输出层，二者的cos相似度提高到了0.93。作为对比，实验了使用2个class
token的情况，结果两个class
token在输出和输出层都收敛到几乎完全一样的vector。论文认为这说明使用distillation
token引入了新的信息。（感觉此处有点存疑）</p>
<h2 id="实验-2">实验</h2>
<p>DeiT所用实验所用模型如下：</p>
<img src="/a11e2633/deit_model.png" class title="vision transformer">
<p>1、不同teacher model</p>
<p>实验发现，使用卷积模型作为teacher
model比transformer更好，原因可能是相比transformer，卷积模型提供了一些inductive
bias，让student模型可以学到一些图像的结构化信息（《Transferring
inductive biases through knowledge distillation》）。</p>
<img src="/a11e2633/deit_teacher.png" class title="vision transformer">
<p>2、不同的蒸馏方法</p>
<p>下图是各种蒸馏方法的效果对比：</p>
<img src="/a11e2633/deit_distill.png" class title="vision transformer">
<ul>
<li>DeiT– no distillation：没有蒸馏，作为baseline对比<br>
</li>
<li>DeiT– usual distillation：使用soft label，没有distillation
token<br>
</li>
<li>DeiT– hard distillation：使用hard label，没有distillation
token<br>
</li>
<li>DeiT⚗: class embedding：使用hard label，仅使用class token<br>
</li>
<li>DeiT⚗: distil. embedding：使用hard label，仅使用distillation
token<br>
</li>
<li>DeiT⚗: class+distillation：使用hard label，使用class
token和distillation token</li>
</ul>
<p>结论是使用hard label和distillation token有提升。</p>
<h2 id="小结">小结</h2>
<p>和其他模型的整体对比如下：</p>
<img src="/a11e2633/deit_perf.png" class title="vision transformer">
<p>DeiT的效果略略比EfficientNet差一点。而相比ViT，DeiT的优势是训练的量比较少。</p>
<h1 id="swin-transformer">Swin Transformer</h1>
<p>论文：《Swin Transformer: Hierarchical Vision Transformer using
ShiftedWindows》</p>
<p>时间：2021年3月</p>
<p>机构：MSRA</p>
<p>Swin来自Shifted WINdow。</p>
<h2 id="模型结构">模型结构</h2>
<p>Swin
Transformer主要的改造是在模型设计上。下图是Swin-T（T是Tiny）模型的结构图，后面结合这个图来一步步看看Swin
Transformer是怎么做的。</p>
<img src="/a11e2633/swin_model.png" class title="vision transformer">
<p>首先是输入部分。Swin Transformer跟ViT一样，把多个pixel
group在一起作为一个patch，也相当于输入给模型的一个token。只是这里使用的patch
size比较小，是4×4的。再乘上RGB三个channel，那么每个patch就维度就是4×4×3=48。</p>
<p>同ViT类似，这里也会用一个linear
layer把每个patch的48维数据映射成一个C维的embedding。这时输入图片就变成H/4×W/4×C的大小。</p>
<p>linear embedding后面跟着两个Swin Transformer
Block，这两个block的attention部分和一般的transformer
block有所不同，后面详说。</p>
<p>linear embedding和两个Swin Transformer Block合一起成为stage 1，stage
1的输出特征维度是H/4×W/4×C。</p>
<p>为了得到hierarchical representation，stage 1的输出会经过一个patch
merging layer进行维度的转换。具体来说，patch merging
layer会把上一阶段输出中的2×2的相邻patch
concat在一起，得到H/8×W/8×4C的feature map，再经过一个linear层，把feature
map维度降为H/8×W/8×2C。</p>
<p>和stage 1类似，patch merging layer后会跟两个Swin Transformer
Block。它们合在一起就是stage 2。</p>
<p>后面是stage 3、stage 4就是重复stage
2的操作，输出的大小分别为H/16×W/16×4C和H/32×W/32×8C。</p>
<h2 id="shifted-window-attention">Shifted Window Attention</h2>
<p>现在回过头来看Swin Transformer Block具体是怎么做的。</p>
<p>首先，我们知道使用pixel直接作为token，或者用小的patch
size会使得输入给transformer模型的数据长度很长，导致计算成本不可控。那么Swin
Transformer为了限制计算量，就把attention计算限制在一个local的范围内，而不是全局attention。</p>
<p>限制attention计算的范围就是window
size。如下图（a），每个红框就是一个window，每个window包含4×4个patch。每个patch只和同window内的patch进行attention计算，而不和其他window的内容进行交互。</p>
<img src="/a11e2633/swin_hierachical.png" class title="vision transformer">
<p>这样一来只要保持window
size比较小，那么模型的计算复杂度就小了。具体来说，对于一张包含h×w个patch的图像，假设window
size=M×M个patch，那么传统的attention计算量和hw的平方成正比，而window
attention的计算量则是和hw成线性关系：</p>
<p><span class="math display">\[\Omega(\text{MSA})=4hwC^2+2(hw)^2C\]</span></p>
<p><span class="math display">\[\Omega(\text{W-MSA})=4hwC^2+2M^2hwC\]</span></p>
<p>这也是Swin Transformer能够使用2×2的小patch的原因。</p>
<p>Swin-T模型中每个stage的第一个Swin Transformer Block就是使用window
attention计算的。</p>
<p>但是window
attention也有缺点，那就是window之间的信息无法交互，这样就会大大限制了模型的学习能力。为了解决这个问题，Swin-T模型中每个stage的第二个Swin
Transformer Block使用的是shifted window attention。</p>
<p>shifted window attention，顾名思义就是把window shift一下，如下图</p>
<img src="/a11e2633/swin_window.png" class title="vision transformer">
<p>对于window size为M×M的情况，会把所有window往左上shift
M/2个patch。这样把window attention和shifted window
attention交错使用，就能让信息在不同window之间进行传递。</p>
<h2 id="实验-3">实验</h2>
<p>Swin Transformer的各个规模模型如下：<br>
- Swin-T: C = 96, layer numbers = {2; 2; 6; 2}<br>
- Swin-S: C = 96, layer numbers = {2; 2; 18; 2}<br>
- Swin-B: C = 128, layer numbers = {2; 2; 18; 2}<br>
- Swin-L: C = 192, layer numbers = {2; 2; 18; 2}</p>
<p>window size M = 7。</p>
<p>1、Image Classification</p>
<p>直接在ImageNet-1K上训练，和在ImageNet-22K预训练后再在ImageNet-1K上微调，Swin
Transformer和其他模型效果对比如下：</p>
<img src="/a11e2633/swin_perf1.png" class title="vision transformer">
<p>Swin Transformer的top-1
acc基本上达到了最好的一档，而计算量相比之前最好的CNN的增加也在可接受范围内。</p>
<p>2、Object Detection on COCO</p>
<p>最佳模型在 COCO 测试集上的性能超过了之前的最佳结果。</p>
<img src="/a11e2633/swin_perf2.png" class title="vision transformer">
<p>3、Semantic Segmentation on ADE20K</p>
<p>Swin-S 比 DeiT-S 和 ResNet-101 等模型性能更优，Swin-L 模型在
ImageNet-22K 预训练后在验证集上的 mIoU 达到
53.5，超过之前的最佳模型（SETR 的 50.3 mIoU）。</p>
<img src="/a11e2633/swin_perf3.png" class title="vision transformer">
<h2 id="小结-1">小结</h2>
<ul>
<li>Swin
Transformer的计算量不再随着图像大小平方增加，使得可以使用更小的patch
size，这是效果提升的一个要点<br>
</li>
<li>在效果到达/超过第一梯队CNN水平的同事，Swin
Transformer没有大幅提升<br>
</li>
<li>相比ViT，Swin Transformer的hierarchical结构使得它的特征在object
detection和semantic segmentation都好用</li>
</ul>
<h1 id="其他">其他</h1>
<p>其他一些Transformer/Attention相关做法，也简单梳理下。</p>
<h2 id="stand-alone-self-attention-in-vision-models">Stand-Alone
Self-Attention in Vision Models</h2>
<p>时间：2019年6月</p>
<p>机构：Google Brain</p>
<p>在CNN中，原本卷积计算是这样的：</p>
<img src="/a11e2633/att_conv.png" class title="vision transformer">
<p>这里把卷积替换成一个local attention：</p>
<img src="/a11e2633/att_attention.png" class title="vision transformer">
<p>相当于把卷积的内部计算替换成self-attention计算了。跟CNN一样，每层的attention会扫描当前输入feature
map的所有位置，给出所有输出值，构成输出feature map。</p>
<p>此外，论文认为位置信息在attention的缺失会带来permutation
equivariant，因此还引入了2D的相对位置编码：</p>
<img src="/a11e2633/att_pos.png" class title="vision transformer">
<p>这些相对位置编码加到每个attention计算内部的各个pixel的key上。</p>
<h2 id="on-the-relationship-between-self-attention-and-convolutional-layers">On
the Relationship between Self-Attention and Convolutional Layers</h2>
<p>时间：2019年11月</p>
<p>这个工作里，作者通过理论证明和实验验证来表明attention架构的前几层学会了关注每个query像素周围的网格状模式，类似于卷积层的行为。</p>
<p>attention模型训练时每个层有九个注意力头，与ResNet架构中使用的3×3内核相对应。训练后，观察到注意力头在query
pixel周围形成网格，如下图所示，前几层的头倾向于关注局部模式，而更深层的头关注更大的模式，这表明self-attention应用于图像时确实学会了在query
pixel周围形成类似于convolution filter的模式。</p>
<img src="/a11e2633/rel_1.png" class title="vision transformer">
<img src="/a11e2633/rel_2.png" class title="vision transformer">
<p>实际训练的时候，为了减少计算量，使用了2×2 pixel的patch。</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/dcb57672.html">长文详解--LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】Generative Pretraining from Pixels
https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf<br>
【2】An Image is Worth 16x16 Words: Transformers for Image Recognition
at Scale https://arxiv.org/abs/2010.11929<br>
【3】Swin Transformer: Hierarchical Vision Transformer using
ShiftedWindows https://arxiv.org/abs/2103.14030<br>
【4】Training data-efficient image transformers &amp; distillation
through attention https://arxiv.org/abs/2012.12877<br>
【5】How Do Vision Transformers work?
https://arxiv.org/abs/2202.06709<br>
【6】Stand-Alone Self-Attention in Vision Models
https://arxiv.org/abs/1906.05909<br>
【7】On the Relationship between Self-Attention and Convolutional Layers
https://arxiv.org/abs/1911.03584</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>CV</category>
      </categories>
      <tags>
        <tag>transformer</tag>
        <tag>预训练</tag>
        <tag>CV</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>深度求索DeepSeek-R1详解</title>
    <url>/9e4b4e6d.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>过年前这段时间好几个重磅工作相继发布，有深度求索的DeepSeek-V3、DeepSeek-R1、月之暗面的Kimi-K1.5，还有MiniMax的MiniMax-01、面壁智能的MiniCPM-o
2.6和智谱的GLM-Realtime，以及阶跃的Step-1o等，2025年才过了不到一个月，真·卷起来了。百花齐放的场景让人对AI充满期待，同时作为开发者也感到压力山大。</p>
<p>还好不少工作都有给出技术报告，让我们有机会站在它们的肩膀上，今天就来学习一下DeepSeek-R1。</p>
<img src="/9e4b4e6d/perf.png" class title="r1">
<h1 id="overview">overview</h1>
<p>先概括说说DeepSeek-R1是什么，大致干了什么：<br>
-
①：以671B参数的DeepSeek-V3-Base为起点，直接对预训练模型进行reasoning任务的强化学习，获得DeepSeek-R1-Zero，发现效果比单纯SFT更好，说明强化学习的self-evolution很有效果<br>
-
②：DeepSeek-R1-Zero虽然效果好，但是有一些小问题比如语言问题和格式问题，那么在强化学习RL之前，先做SFT，就可以缓解这些问题了<br>
-
③：②的方法得到的模型的reasoning效果很好，那就用它来搞reasoning数据；再加上DeepSeek-V3的SFT数据中的non-reasoning部分，合在一起获得高质量的SFT数据<br>
-
④：③中的数据用来对DeepSeek-V3-Base做微调，之后再进行RL，得到效果最好的模型DeepSeek-R1<br>
-
⑤：用③中的数据对Qwen/Llama模型进行SFT，可以视作是数据蒸馏；得到的模型效果也比非蒸馏的SFT要好</p>
<h1 id="训练pipeline">训练pipeline</h1>
<p>DeepSeek-R1技术报告给出了几个模型的训练流程，DeepMind的大佬给训练流程画了图，原图在<a href="https://x.com/SirrahChan/status/1881488738473357753?s=19&amp;mx=2">https://x.com/SirrahChan/status/1881488738473357753?s=19&amp;mx=2</a>。</p>
<p>在这个基础上，我稍稍加了一点点修改，理清一点细节。DeepSeek-R1中提到的模型训练流程如下：</p>
<img src="/9e4b4e6d/pipeline.png" class title="r1">
<p>左路就是DeepSeek-R1-Zero的训练（上面的①），中路是基于SFT和RL搞数据（上面的③），获得800k
Combined SFT
data之后，左边是对Qwen和Llama进行蒸馏（上面的⑤），右边就是训练DeepSeek-R1（上面的④）。</p>
<p>下面就一个个来看下细节。</p>
<h1 id="deepseek-r1-zero">DeepSeek-R1-Zero</h1>
<p>DeepSeek-R1-Zero以DeepSeek-V3-Base做初始化，在没有SFT阶段的情况下直接做RL，就获得了比较好的效果。</p>
<p>强化学习方法用的是Deepseekmath中提出的Group Relative Policy
Optimization(GRPO)，而训练的reward信号没有使用模型产生，而是仅使用规则来决定。主要包含两类reward：<br>
- accuracy
rewards：对于数学问题这类有确定答案的问题，要求模型给出特定格式下的结果，方便进行正确性校验；而对于LeetCode问题，则是给出test
case的执行结果作为反馈；<br>
- format
rewards：格式奖励，强制模型将思考过程放在输出的&lt;think&gt;和&lt;/think&gt;这两个特殊tag中间；</p>
<p>那么为什么不使用模型来获取reward呢？</p>
<p>因为使用reward model就有可能出现reward
hacking，也就是actor有可能会找到捷径来获取高reward值，从而偏离了我们想要的优化目标。比如一个常见的hacking就是模型经常会发现“输出越长得分越高”，从而给出又臭又长的回答。因此在一些RL框架中就有专门对长度进行惩罚来避免这个捷径被滥用。</p>
<p>reward
hacking没法预测，不知道在什么时候就会出现一个奇怪的reward漏洞，这么一来就可能需要多次迭代reward模型，堵上这些捷径和漏洞。但是对于大规模的模型，每次迭代的更新数据和训练都要耗费比较大的成本，从而导致整个训练流程变得困难。</p>
<p>用规则reward就不会有这样的问题，而且对于数学和代码类问题规则的效率也更高。</p>
<p>训练DeepSeek-R1-Zero的目的是希望模型能够通过长思考自我反馈来解决复杂问题，那么就需要它按我们想要的格式输出，起码能清晰给出哪些是思考过程（隐藏的输出），哪些是最终结果（用来呈现给用户）。因此给RL训练的输出设计了模板，引导模型按照这个格式输出，方法就是前面提到的format
reward：</p>
<img src="/9e4b4e6d/template.png" class title="r1">
<p>随着RL训练的进行，DeepSeek-R1-Zero的长思考能力持续提升。</p>
<p>下图是训练期间DeepSeek-R1-Zero在AIME 2024
benchmark上的得分变化情况，每400步进行一次评测：</p>
<img src="/9e4b4e6d/aime.png" class title="r1">
<p>可以看到相比Base模型，得分确实有显著的提升，从15.6%提升到了71.0%，达到了与OpenAI-o1-0912相当的水平。在其他benchmark上，DeepSeek-R1-Zero也有类似的提升：</p>
<img src="/9e4b4e6d/reasoning_benchmark.png" class title="r1">
<p>另外还可以观察到，随着RL的进行，DeepSeek-R1-Zero的思考内容在持续变多。这表现在输出结果的CoT中，即&lt;think&gt;和&lt;/think&gt;中间的内容长度在持续增加，最后达到了接近10k，而且增长的趋势完全没有减弱。可以预想继续训练的话还会变得更长（当然更长并不一定是更好）：</p>
<img src="/9e4b4e6d/length.png" class title="r1">
<p>这些效果提升说明不使用监督数据，而仅使用强化学习，模型可以自发探索和环境进行交互的方式，并且对复杂问题可以自发学会进行复杂的思考，从而提升处理困难问题的能力。</p>
<h2 id="aha-moment">Aha moment</h2>
<p>DeepSeek-R1-Zero的Aha
moment是在训练过程中观察到的一个有趣现象。在一些中间版本，模型在思考过程中对前面的方法进行重新审视，并为其中的问题重新分配了思考时间：</p>
<img src="/9e4b4e6d/aha.png" class title="r1">
<p>这是一个拟人化的心理活动，而且是在没有人类监督学习的情况下出现的。（这会不会说明RL真的可以通往更高层级的智能，真正到达人类水平甚至更高的水平呢？）</p>
<h2 id="drawback-of-deepseek-r1-zero">Drawback of DeepSeek-R1-Zero</h2>
<p>DeepSeek-R1-Zero虽然在reasoning任务上有了明显的提升，不过也有一些缺点：比如模型的输出可读性较差（会不会这就是模型的思考方式呢，所以人类看着费劲），对于部分语言会出现混用乱用的情况。</p>
<h1 id="deepseek-r1">DeepSeek-R1</h1>
<p>接下来就是DeepSeek-R1的出场了。在DeepSeek-R1-Zero的结果和分析之下，就有两个自然的问题：<br>
- RL已经这么好，那么先SFT再RL不得起飞？<br>
- 光有reasoning CoT的能力不够，能不能搞一个通用能力也很强的版本？</p>
<h2 id="reasoning版本r1">Reasoning版本R1</h2>
<p>首先来看第一个问题，SFT+RL。</p>
<p>为了防止Base模型在RL初期出现不稳定的情况，先收集几千个long CoT
data，用来对Base模型做了SFT。这些long CoT
data怎么收集的呢？就是用包含few-shot
example的prompt，让DeepSeek-R1-Zero输出可读性较好的、带有reflection和verification的结果，再经过人工校验获取的。</p>
<p>为了提升SFT后生成结果的可读性，专门给这些SFT数据设计readable
pattern：在response后面加上一个summary，格式如下：</p>
<p>|special_token|<reasoning_process>|special_token|&lt;summary&gt;</reasoning_process></p>
<p>其中reasoning_process是CoT的内容，而summary是reasoning结果的总结。</p>
<p>SFT之后就是进行和DeepSeek-R1-Zero一样的RL了。前面在训练DeepSeek-R1-Zero的时候，就发现模型输出会出现语言混用的情况，特别是当输入prompt涉及多种语言时。那么这次RL就为此专门设计了一个language
consistency reward，具体来说就是CoT中target language
word的比例。虽然在消融实验中发现加入这个语言一致性reward会带来一点效果损失，不过这样的输出结果对人类更友好。</p>
<p>有了这些SFT数据做冷启动之后，再进行RL，模型主要有两点变化：<br>
- readability：有了SFT作为冷启动，模型的可读性更好了<br>
- potential：增加SFT之后，整体的效果也更好了</p>
<h2 id="新一轮的数据收集">新一轮的数据收集</h2>
<p>1、reasoning数据</p>
<p>上一步通过少量人工参与的SFT数据+RL，获得了比DeepSeek-R1-Zero更好一点的模型。那这个模型是不是又可以用来收集更好的SFT数据了呢？答案当然是yes，不要忘了前面这些SFT数据就是从更早的版本DeepSeek-R1-Zero来的。</p>
<p>为了收集更好的数据，这里使用rejection sampling来采样reasoning
trajectory。之前的数据基本上至包含了可以使用规则来评估reward的样例，但是这次我们把范围扩大，增加了一些没法直接用规则判定的reasoning数据。这些新增的reasoning数据就需要用到模型来判别，而DeepSeek-V3就可以作为这个判别模型，通过输入的ground
truth和prediction来judge结果的好坏。</p>
<p>此外，还有一些清洗规则：<br>
- 语言混合<br>
- 长段落<br>
- 包含代码块（毕竟大脑不能跑代码？）</p>
<p>最终采样了600k条reasoning data。</p>
<p>2、non-reasoning data</p>
<p>回顾前面的两个问题，第一个已经验证了，再看看第二个：光有reasoning
CoT的能力不够，能不能搞一个通用能力也很强的版本？</p>
<p>想要提升通用能力，那就需要包含一些non-reasoning
data：比如writing、factual QA、self-cognition和translation等。</p>
<p>这些数据来自于DeepSeek-V3的SFT数据。对于某些任务，会调用DeepSeek-V3在回答问题之前先生成一个CoT；而对于某些比较简单的query，比如“hello”这样的打招呼，则不增加CoT。</p>
<p>最终整合大约200k的non-reasoning data。</p>
<h2 id="sft-rl">SFT + RL</h2>
<p>上面得到了600k + 200k =
800k的SFT数据，首先用这些数据在DeepSeek-V3-Base上训了2个epoch。接下来就要进行RL了。</p>
<p>RL的reward设置和前面又有点不同。对于数学、代码和logical
reasoning的任务，这里还是使用和DeepSeek-R1-Zero一样的规则reward。而对于general数据，就用上了reward
model。</p>
<p>reward model依然是基于DeepSeek-V3的。对于helpfulness，主要关注在final
summary，确保给到用户的response的实用性。而对于harmlessness，则会关注整个模型数据，包括过程和结果，识别和减轻在生成过程中任何可能出现风险的地方。</p>
<p>这样一套下来，就得到了最终DeepSeek-R1。</p>
<h2 id="评测">评测</h2>
<p>评测中，所有模型的设置都是：<br>
- 长度32768 token<br>
- 对于需要采样的，使用temperature = 0.6，top-p =
0.5，每条query生成64条response</p>
<p>DeepSeek-R1的评测结果如下：</p>
<img src="/9e4b4e6d/eval.png" class title="r1">
<h1 id="蒸馏">蒸馏</h1>
<p>在前面的流程中，SFT数据的产生来自DeepSeek-V3（或进一步训练的变体），reward来自DeepSeek-V3，所有流程都是基于DeepSeek-V3来做的。最后产生的这800k数据可以说是DeepSeek-V3这个模型能给出的精华内容。</p>
<p>用这800k数据训练其他更小的模型，也可以视为是一种数据蒸馏（就像大家都去拉取GPT-4/o1/o3的数据用来训练一样）。</p>
<p>具体选择的小模型有：Qwen2.5-Math-1.5B、Qwen2.5-Math-7B、Qwen2.5-14B、Qwen2.5-32B、Llama-3.1-8B
和 Llama-3.3-70B-Instruct。</p>
<p>这些蒸馏模型相比原模型也有很大的提升：</p>
<img src="/9e4b4e6d/distill_eval.png" class title="r1">
<p>DeepSeek-R1-Distill-Qwen-32B甚至超过了QwQ-32B-Preview（这是不是有点打Qwen脸了）。</p>
<p>如果这些模型不蒸馏，而是进行和DeepSeek-R1-Zero类似的强化学习，能不能比数据蒸馏强呢？为了回答这个问题，在Qwen-32B-Base上进行了10k+步的强化学习，得到DeepSeek-R1-Zero-Qwen-32B，效果和QwQ-32B-Preview差不多，但是还是不如数据蒸馏的SFT模型：</p>
<img src="/9e4b4e6d/distill_and_rl.png" class title="r1">
<h1 id="unsuccessful-attempts">Unsuccessful Attempts</h1>
<p>文中也提到一些失败的尝试。</p>
<p>1、Process Reward Model (PRM)</p>
<p>之前的PRM工作，比如：<br>
- Let’s verify step by step<br>
- Solving math word problems with process-and outcome-based
feedback<br>
- Math-shepherd: A labelfree step-by-step verifier for llms in
mathematical reasoning</p>
<p>都有一些局限性。首先，在推理中明确区分各个step就不是容易的事；此外确定各个step是否正确也缺乏有效手段；另外，reward
model的引入也会导致前面反复提到的reward hacking问题。</p>
<p>2、search algorithms such as Monte Carlo Tree Search and Beam
Search<br>
- Alphazero-like tree-search can guide large language model decoding and
training<br>
- Solving olympiad geometry without human demonstrations<br>
- Deepseek-prover-v1.5: Harnessing proof assistant feedback for
reinforcement learning and monte-carlo tree search</p>
<p>搜索是另外一个方法。但是语言的搜索空间比象棋更大，因此难度更高。如果在每个节点扩大搜索范围，则可能会陷入局部最优。此外，训练好的value
model也不是易事，这就到时模型的迭代比较困难。</p>
<h1 id="其他内容">其他内容</h1>
<p>其他一些相关内容，可以看看的。</p>
<p>1、reinforcement learning：<br>
- Training language models to self-correct via reinforcement
learning</p>
<p>2、强化学习在reasoning task的有效性：<br>
- Deepseekmath:Pushing the limits of mathematical reasoning in open
language models<br>
- Math-shepherd: A labelfree step-by-step verifier for llms in
mathematical reasoning</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via
Reinforcement Learning, https://arxiv.org/abs/2501.12948<br>
【2】DeepSeek-R1训练流程图，https://x.com/SirrahChan/status/1881488738473357753?s=19</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM预训练数据策略(一)</title>
    <url>/2c2cdc34.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<h1 id="code-based-english-models-surprising-performance-on-chinese-qa-pair-extraction-task">Code-Based
English Models Surprising Performance on Chinese QA Pair Extraction
Task</h1>
<p>时间：2024年1月</p>
<h2 id="tldr">TL;DR</h2>
<p>在“基于中文文档生成QA”的生成任务上，使用多个规模相同/相近的LLM进行实验。结果发现，代码模型效果比通用模型更好，并且英文模型表现出了优异性能。</p>
<h2 id="任务-数据">任务 &amp; 数据</h2>
<p>1、任务</p>
<p>基于中文的文档，给出中文QA数据。下面是一个样例：</p>
<img src="/2c2cdc34/codebased_intro.png" class title="LLM数据策略">
<p>2、数据</p>
<p>（1）训练数据</p>
<p>从wiki和新闻文章中获取的 143,846 个文档，每个文档有相应的问答对。</p>
<p>这些数据来自开放的人工标注数据集。</p>
<p>（2）评测数据</p>
<p>测试集由 300 个internet
technology相关的private文档组成。来自于实际业务中收集的hard case。</p>
<p>训练数据和评测数据的domain很不同，这就要求模型有比较强的泛化能力。</p>
<h2 id="指标-实验">指标 &amp; 实验</h2>
<p>1、指标</p>
<p>文中使用了5个细粒度的指标：<br>
- Coverage Analysis：使用 ROUGE - L
来评估summary在多大程度上涵盖了源文本中呈现的信息。ROUGE - L
值越高，表示模型输出的覆盖率越好。<br>
- Hallucination Analysis of Answers：定义了content creation
rate（CCR）（不过文中没有给出具体定义），代表模型产生幻觉的倾向。CCR
分数越高，表明幻觉越严重。<br>
- Completeness Analysis of Answers：定义了average text coverage
ratio（COV）（同样没有给出具体定义）。COV
指的是在所有结果中，输出文本与输入文本的最长公共子串的平均比值。这个比值反映了输出文本对输入文本内容的平均使用程度。COV
分数越高，表示输出文本对输入文本的复制保真度越高。<br>
- Reject Capability Analysis：定义了模型的reject
capability（REJ）（同样没有给出具体定义），它表明模型是否知道何时拒绝完成任务（训练和测试数据中包含一些无法用于生成有效QA的文档，需要模型拒绝）。REJ
分数越高，表明模型更好地辨别哪些材料更值得进行知识生成任务。<br>
- Summarize Capability Analysis：定义了text extraction aggregation
count（TEAC）（同样没有给出具体定义），用来代表模型的总结能力。TEAC分数越高表示模型从内容整体中多个地方提取信息的能力越强，即提取的信息更复杂。同时还定义了longest
increasing subsequence ratio（LISR），表示模型保持一致性的能力，LISR
分数越高，表明知识提取行为更有序。</p>
<p>还有一个EXPERTS指标，是用其他LLM对生成结果进行打分。</p>
<p>2、实验</p>
<p>论文具体做了以下四个实验。实验用的基模型都是预训练模型，没有经过对齐。</p>
<p>（1）Code-based LLMs better than other LLMs</p>
<p>比较了code model与非code
model在QA数据微调后各种指标上的性能差异，发现code model在 EXPERTS
等方面通常优于其他LLM，而在幻觉、泛化和拒绝等方面也有显著增强，如下表所示：</p>
<img src="/2c2cdc34/codebased_perf.png" class title="LLM数据策略">
<p>（2）Less Domain Knowledge, Better Performance</p>
<p>把 DeepSeek-code-6.7B 和 Code Llama-7B 拉出来看，发现Code
Llama-7B整体上更好一些。而根据二者透露的信息，DeepSeek-code-6.7B所用的中文数据比例应该是远高于Code
Llama-7B的。</p>
<img src="/2c2cdc34/codebased_codemodel.png" class title="LLM数据策略">
<p>DeepSeek-code-6.7B
的主要问题是容易出现幻觉。这可能说明，在预训练数据中包含大量的中文通用数据，更容易导致模型出现幻觉问题。</p>
<p>（3）A Moderate Amount of Chinese is Better</p>
<p>Code
Llama的词表中，中文的token很少。如果加入8076个中文token，并使用词表中其他相关token的平均值初始化这些新加入的token，那么在一些维度上有一些提升，比如TEAC和CCR。如果是对这些新加入的token进行随机初始化，效果则差很多。</p>
<img src="/2c2cdc34/codebased_emb_figure.png" class title="LLM数据策略">
<img src="/2c2cdc34/codebased_emb_result.png" class title="LLM数据策略">
<p>文中认为加入一定量的中文知识能够提升效果。不过个人认为这里也有可能是训练量太少，随机初始化没有能够收敛。利用平均值初始化新token可以加速收敛。</p>
<p>（4）QLoRA fails to replicate the effects</p>
<p>使用 QLoRA 对 Code Llama-7B 模型进行两种方法（scaling 和
noscaling）的训练，发现无论哪种 QLoRA
方法，在该任务下都无法获得与全参数微调相同的效果。</p>
<img src="/2c2cdc34/codebased_scaling.png" class title="LLM数据策略">
<p>EXPERTS得分的差距非常大。</p>
<h2 id="小结">小结</h2>
<p>感觉整个实验应该说indicate了一个事情：训练语言（中文、英文）之间的差距，比训练任务的的差距更小。如果一个模型在英文上具备了很好的摘要能力，那么这个摘要能力比较容易迁移到中文上；而如果一个中文模型在预训练中没有得到任何摘要相关能力的训练，那么想要这个模型获得摘要能力，那么难度会比前面语言上的迁移更大。</p>
<p>因此，想要在复杂任务上获得好效果，就应该拆解复杂任务所需的基本能力，然后在训练数据中加入支持相应能力的数据。比如agent能力，需要调用接口，总结多方信息，那么上游的代码数据、长文总结、阅读理解应该都对agent能力有帮助。</p>
<h1 id="rho-1-not-all-tokens-are-what-you-need">RHO-1: Not All Tokens
Are What You Need</h1>
<p>时间：2024年4月</p>
<h2 id="tldr-1">TL;DR</h2>
<p>论文的主要思想是“Not all tokens in a corpus are equally important for
language model training”。文中提出Selective Language Modeling
(SLM)的方法，通过仅选择部分有效的token参与继续预训练，能够在提升收敛速度（5x-10x）的同时，获得更好的效果。</p>
<img src="/2c2cdc34/rho_intro.png" class title="LLM数据策略">
<p>左边是常规的Causal Language Modeling (CLM)，右边是这里改进的Selective
Language Modeling (SLM)。</p>
<h2 id="分析">分析</h2>
<p>1、token粒度脏数据</p>
<p>预训练数据中存在着像下图这种，包含部分低质量token的数据。这些数据难以通过文档级别的过滤来处理，因为如果把过滤阈值卡得太严的话，可能会误伤很多正常数据。而如果直接移除这些token，又可能影响正常的训练。</p>
<img src="/2c2cdc34/rho_noise.png" class title="LLM数据策略">
<p>2、数据分布不一致</p>
<p>预训练的数据分布通常和下游的任务的数据分布不一致，直接把所有token一视同仁进行预训练，可能会有很多对下游无效的训练。</p>
<h2 id="selective-language-modeling-slm">Selective Language Modeling
(SLM)</h2>
<p>首先看下训练过程中token的loss变化情况。从OpenWebMath数据集中抽15B的数据，训练Tinyllama-1B，然后每训练1B
token保存一次checkpoint。然后用一个包含约320,000个token的validation
set，跑出所有checkpoint的token-level
loss。发现这些token根据训练过程loss的变化可以分成4类：<br>
- persistent high loss (H→H)，约11%<br>
- increasing loss (L→H)，12%<br>
- decreasing loss (H→L)，26%<br>
- consistent low loss (L→L)，51%</p>
<p>各个类型的loss变化如下图（a）</p>
<img src="/2c2cdc34/rho_types.png" class title="LLM数据策略">
<p>和我们设想的不同，随着训练进行，loss减小的token占比只占全量数据的1/4。L→L和H→H类型的token在训练过程中都呈现强烈的震荡状态。实际上，部分H→L类型的token的loss下降过程也并不平滑。</p>
<p>下图把这些震荡的token用橙色标记出来</p>
<img src="/2c2cdc34/rho_text_1.png" class title="LLM数据策略">
<p>发现有相当部分其实可以算是噪音，这和前面分析的一致：存在token粒度脏数据，会影响训练。</p>
<p>那么基于上面的这些分析和实验：</p>
<blockquote>
<p>If we can select the appropriate tokens for the model to focus on
during training, we may be able to stabilize the trajectory of the
model’s training and enhance its efficiency.</p>
</blockquote>
<p>受document-level filtering中使用reference mode的启发，文中提出
token-level data selection的pipeline，也就是Selective Language Modeling
(SLM)。</p>
<p>SLM有3个step：</p>
<img src="/2c2cdc34/rho_slm.png" class title="LLM数据策略">
<ul>
<li>step 1：在高质量数据集上训练reference
model；这个高质量数据集的分布反映我们想要的下游任务分布<br>
</li>
<li>step 2：用reference model获得将要使用的预训练数据集上的token-level
loss<br>
</li>
<li>step 3：正式的训练中，只使用LM和RM间excess
loss较大的token进行学习</li>
</ul>
<p>RM模型的训练使用标准的cross-entropy
loss。训练好的RM对预训练数据进行打分，按如下计算：</p>
<p><span class="math display">\[\mathcal{L}_{\mathrm{RM}}(x_i)=-\log
P(x_i|x_{&lt;i})\]</span><br>
excess loss则是当前训练的模型，和RM之间的打分差：</p>
<p><span class="math display">\[\mathcal{L}_\Delta(x_i)=\mathcal{L}_\theta(x_i)-\mathcal{L}_\mathrm{RM}(x_i)\]</span></p>
<p>正常的causal模型训练是对所有token的loss取平均训练的：</p>
<p><span class="math display">\[\mathcal{L}_{\mathrm{CLM}}(\theta)=-\frac1N\sum_{i=1}^N\log
P(x_i|x_{&lt;i};\theta)\]</span></p>
<p>SLM的训练则是基于excess loss，只选择每个训练batch中，excess
loss处于top k%（token selection ratio）的token进行训练：</p>
<p><span class="math display">\[\mathcal{L}_{\mathrm{SLM}}(\theta)=-\frac1{N*k\%}\sum_{i=1}^NI_{k\%}(x_i)\cdot\log
P(x_i|x_{&lt;i};\theta)\]</span></p>
<p><span class="math display">\[\begin{aligned}&amp;I_{k\%}(x_i)=\begin{cases}1&amp;\text{if
}x_i\text{ ranks in the top }k\%\text{ by
}\mathcal{L}_\Delta(x_i)\\0&amp;\text{otherwise}\end{cases}\end{aligned}\]</span></p>
<h2 id="实验">实验</h2>
<h3 id="设置">设置</h3>
<p>在数学领域和通用领域进行继续预训练，实验SLM的效果。</p>
<p>1、RM Training</p>
<ul>
<li>mathematical
RM：使用0.5B的高质量数学数据，这些数据包括来自GPT-4的生成数据和人工处理获得的数据（《Metamath:
Bootstrap your own mathematical questions for large language
models》，《Key-pointdriven data synthesis with its enhancement on
mathematical reasoning》；《Mammoth: Building math generalist models
through hybrid instruction tuning》，《Exploring the mystery of
influential data for mathematical reasoning》）<br>
</li>
<li>general RM：使用1.9B开源训练数据<br>
</li>
<li>训练3个epoch<br>
</li>
<li>1B模型和7B模型的lr分别是5e-5和1e-5<br>
</li>
<li>cosine decay schedule<br>
</li>
<li>1B模型和7B模型的max length分别为2048和4096<br>
</li>
<li>RM模型和继续预训练的模型使用相同的初始化</li>
</ul>
<p>2、Pretraining Corpus</p>
<p>在数学领域，使用包含14B token的OpenWebMath (OWM)
数据集；而在通用领域，则是把SlimPajama、StarCoderData和OpenWebMath按6:3:1混合得到80B的训练数据集。</p>
<p>3、Pretraining Setting</p>
<p>数学领域：<br>
- 基于Tinyllama-1.1B和Mistral-7B继续预训练<br>
- lr分别为8e-5和2e-5<br>
- batch size = 1M</p>
<p>通用领域：<br>
- 基于Tinyllama-1.1B继续预训练<br>
- lr = 1e-4<br>
- batch size = 1M</p>
<p>token selection ratio：<br>
- Tinyllama-1.1B：60%<br>
- Mistral-7B：70%</p>
<p>4、Baseline Setting</p>
<p>和SLM作为对比，分别用同样的数据进行常规的继续预训练，作为baseline：<br>
- Tinyllama-CT<br>
- Mistral-CT</p>
<p>还有其他一些效果比较好的同规模预训练模型，包括Gemma、Qwen1.5、Phi-1.5
、DeepSeekLLM、DeepSeekMath、CodeLlama、Mistral、Minerva、Tinyllama、
InternLM2-Math和LLemma等。还有几个SFT模型：MAmmoTH和ToRA。</p>
<h3 id="效果对比">效果对比</h3>
<p>各个预训练模型在Few-shot CoT reasoning的效果如下：</p>
<img src="/2c2cdc34/rho_math_perf.png" class title="LLM数据策略">
<p>RHO-1-Math-1B相比Tinyllama-CT有16.5%的提升，而7B模型则有10.4%的提升。1B模型在训练多几个epoch之后还能进一步提升。</p>
<p>而相比其他业界知名的模型，RHO-1基本上也能达到同一水准，但是训练所用的数据量相比这些模型则减少许多。</p>
<p>Tool-Integrated Reasoning的效果也做了对比：</p>
<img src="/2c2cdc34/rho_tool_perf.png" class title="LLM数据策略">
<p>相比预训练，SFT后RHO-1的收益没有那么大，但还是有提升。</p>
<p>而在通用数据上训练的模型，在各个benchmark的对比如下：</p>
<img src="/2c2cdc34/rho_general_perf.png" class title="LLM数据策略">
<p>也都有稳定的提升，其中代码和数学类提升最大。</p>
<h3 id="self-reference">Self-Reference</h3>
<p>前面的实验有一个前提，就是RM模型是用和下游任务比较分布相关的数据训练的。那么如果没有和下游任务分布相关的数据，是否还可以使用SLM来获得提升呢？</p>
<p>论文认为RM模型打分的关键并不是和下游任务的分布对齐，而是要把noisy数据过滤掉，因此尝试用两个不同的scoring
function来打分：一个就是上面用的reference loss score function（<span class="math inline">\(\mathcal{L}_\Delta(x_i)\)</span>），另一个是information
entropy score function：</p>
<p><span class="math display">\[\mathcal{H}_{\mathbf{RM}}(x_i)=-\sum_{k=1}^VP(t_k|x_{&lt;i})\log
P(t_k|x_{&lt;i})\]</span></p>
<p>V是词表大小。</p>
<p>如果一个token的information entropy
score越大，那么说明在这个context下它的不确定性越大。</p>
<p>不使用下游任务分布相关的数据，而仅用预训练数据，采用不同的score
function，用SLM方法训练，结果如下：</p>
<img src="/2c2cdc34/rho_self_ref.png" class title="LLM数据策略">
<p>无论用哪个score
function，仅用预训练数据训练RM也可以获得提升。如果选择使用reference loss
score function和information entropy score
function的交集token进行训练，效果更好。</p>
<h3 id="其他">其他</h3>
<p>1、Token Select Ratio</p>
<p>在1B模型上实验不同的token select ratio：</p>
<img src="/2c2cdc34/rho_ratio.png" class title="LLM数据策略">
<p>结果表明在60%~70%的效果最好。</p>
<p>2、训练了什么token</p>
<p>RM模型在一段文本上选择的token如下：</p>
<img src="/2c2cdc34/rho_text_2.png" class title="LLM数据策略">
<p>而在训练过程中选择的token有什么变化呢？4个checkpoint，分别是训练0%、33%、66%和100%时，这4个阶段在同一段文本下，选择用于训练的token如下，token选择的preference从高到低分别用深蓝、浅蓝、黑色橙色和深橙色标记出来：</p>
<img src="/2c2cdc34/rho_text_3.png" class title="LLM数据策略">
<h2 id="小结-1">小结</h2>
<p>仅选择重要度高的token进行训练，在成本上来说比较高，可以尝试用于预训练的第二甚至第三阶段。</p>
<h1 id="reuse-dont-retrain-a-recipe-for-continued-pretraining-of-language-models">Reuse,
Don’t Retrain: A Recipe for Continued Pretraining of Language
Models</h1>
<p>时间：2024年7月</p>
<h2 id="tldr-2">TL;DR</h2>
<p>一套针对general
abilities的LLM继续预训练的指南，包括数据分布的设计，和lr
schedule的设置。在一个15B（先经过8T数据的预训练）验证，在下游benchmark平均有9%的提升。实验结果适用于100B~1T数据量的继续训练。</p>
<p>以往的一些工作主要是垂域的：<br>
- 《Simple and scalable strategies to continually pre-train large
language models》<br>
- 《 Investigating continual pretraining in large language models:
Insights and implications》<br>
- 《Continual pre-training of language models》<br>
- 《Towards continual knowledge learning of language models》</p>
<h2 id="实验设置">实验设置</h2>
<h3 id="预训练">预训练</h3>
<p>现在的模型很多都在万亿（1T）/十万亿（10T）级数据量上训练。为了得到一个使用可靠的效果，论文先用一个15B参数模型，在8T的数据上进行实验。8T的预训练数据分布如下：</p>
<img src="/2c2cdc34/reuse_phase1_data.png" class title="LLM数据策略">
<h3 id="继续预训练">继续预训练</h3>
<p>在实际场景中，我们通常没有太多的新数据用于继续预训练，因此继续预训练绝大部分所用数据就是来自上面的8T数据集。在这个基础上，加入了2.8B的QA数据集。</p>
<p>QA数据有助于模型更好地抽取已经学到的知识（《Physics of language
models: Part 3.1, knowledge storage and extraction》）。</p>
<p>QA数据集的分布如下：</p>
<img src="/2c2cdc34/reuse_qa_data.png" class title="LLM数据策略">
<h3 id="模型">模型</h3>
<ul>
<li>15B参数：3.2B embedding参数，12.5B非 embedding参数<br>
</li>
<li>32层<br>
</li>
<li>hidden size = 6144<br>
</li>
<li>48个注意力头<br>
</li>
<li>RoPE<br>
</li>
<li>quared ReLU activations<br>
</li>
<li>vocabulary size = 256k<br>
</li>
<li>no bias terms<br>
</li>
<li>tied embedding weight<br>
</li>
<li>GQA，8个KV头<br>
</li>
<li>pretrain sequence length = 4096<br>
</li>
<li>在训练的前5%token，batch size逐渐从384增大到1152<br>
</li>
<li>cosine learning rate schedule<br>
</li>
<li>warmup = 16B token<br>
</li>
<li>lr = 4.5e-4 --&gt; 4.5e-5<br>
</li>
<li>AdamW，beta_1 = 0.9，beta_2 = 0.95，weight decay = 0.1<br>
在继续预训练中，只有lr会改变，其他都不变。</li>
</ul>
<h3 id="评测">评测</h3>
<p>评测数据集包括：<br>
- MMLU<br>
- Hellaswag<br>
- MGSM（Multilingual Grade School
Mathematics）：用于多语言评估，具体报告西班牙语、日语和泰语这三种分别代表高、中、低资源语言的子集的平均准确率。<br>
- HumanEval的 Python 代码生成任务：在 pass@1
设置下评估模型的编码能力。</p>
<h2 id="继续预训练recipe">继续预训练Recipe</h2>
<p>1、先在原预训练数据分布的基础上，提升高质量数据的比重进行训练，之后再切换到包含QA数据的数据集</p>
<p>2、lr从预训练阶段的min_lr开始，按cosine annealing decay到min_lr /
100</p>
<p>3、数据分布的切换要在min_lr / 5的时候进行</p>
<h2 id="实验-1">实验</h2>
<p>这里提到一点：发现在继续预训练时，是否加载预训练阶段的optimizer参数影响并不大。</p>
<p>预训练模型在评测数据集上的平均accuracy为48.9。</p>
<h3 id="数据分布">数据分布</h3>
<p>1、QA数据</p>
<p>如果把想要学习的新知识放在继续预训练的前面，模型会更容易学到（lr更大），但是同时也可能因此带来训练的不稳定性。为此使用了三种数据进行实验：<br>
- 纯使用预训练数据<br>
- 纯使用QA数据<br>
- 先使用预训练数据，然后混入10%的QA数据</p>
<p>三种策略的效果如下：</p>
<img src="/2c2cdc34/reuse_blend.png" class title="LLM数据策略">
<p>使用第三种策略效果有比较大的提升。</p>
<p>2、general blend实验</p>
<p>把initial blend称之为general
blend（GB），把后面包含QA的数据分布成为QA
blend（QB）。那么GB用什么分布最好呢？</p>
<p>假设最优的GB是注重高质量数据和模型薄弱的领域的数据，且不会大幅偏离预训练分布。实验不同GB分布下的效果：<br>
- Reweight Domains：对预训练数据中的不同领域进行重新加权。<br>
- Pretraining w/ High Quality Web：在预训练数据中使用高质量的 Web
数据。<br>
- No Web：去除预训练数据中的 Web 数据。<br>
- Upweight Non Web w/ High Quality Web：提高非 Web
数据的权重，并使用高质量的 Web 数据。</p>
<p>每种分布都训练300B数据，然后对比效果：</p>
<img src="/2c2cdc34/reuse_datamix.png" class title="LLM数据策略">
<p>Upweight Non Web with High
Quality虽然没有达到最高平均准确率，但在所有任务中最稳定地取得了较高分数，因此被选为后续实验的
GB。</p>
<img src="/2c2cdc34/reuse_task_perf.png" class title="LLM数据策略">
<p>3、QA blend实验</p>
<p>使用三种QA分布：</p>
<img src="/2c2cdc34/reuse_qa_version.png" class title="LLM数据策略">
<p>在训练250B的GB数据后，进行QB的训练：</p>
<img src="/2c2cdc34/reuse_qa_perf.png" class title="LLM数据策略">
<p>同时强调STEM和Chat数据的分布效果最好。</p>
<p>最终的训练方案是，先训练权重修改后的GB数据，然后把QB数据混合到GB数据中。</p>
<h3 id="lr-schedule">lr schedule</h3>
<p>对比3个lr的decay设置，结果是decay到1%的继续预训练的lr时是最好的：</p>
<img src="/2c2cdc34/reuse_qa_lr.png" class title="LLM数据策略">
<h3 id="切换分布的时间">切换分布的时间</h3>
<p>从GB切换到QB分布，应该在什么时候进行？结果如下：</p>
<img src="/2c2cdc34/reuse_time.png" class title="LLM数据策略">
<p>可以看到切换的时间点选择对结果的影响还是比较大的。</p>
<h2 id="小结-2">小结</h2>
<p>算是一次实验记录，不过不是很够完善，解释得也不够清楚。可以作为实验参考吧。</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/dcb57672.html">长文详解--LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】Code-Based English Models Surprising Performance on Chinese QA
Pair Extraction Task https://arxiv.org/abs/2401.10286<br>
【2】RHO-1: Not All Tokens Are What You Need
https://arxiv.org/abs/2404.07965<br>
【3】Reuse, Don’t Retrain: A Recipe for Continued Pretraining of
Language Models https://arxiv.org/abs/2407.07263</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>预训练</tag>
        <tag>数据</tag>
      </tags>
  </entry>
  <entry>
    <title>小米的移动UI多模态模型--MobileVLM</title>
    <url>/96393d3b.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>图文多模态模型的一大应用场景就是手机上的智能助手，一个能听能看能写能说的智能助手还是颇有吸引力的。</p>
<p>手机厂商里，除了苹果，小米也是一个不时能拿出一些实用AI技术和产品的厂商。（最近开出年薪千万挖人也是上了头条）</p>
<p>今天就来了解一下小米关于手机UI多模态模型的一个工作 --
MobileVLM。MobileVLM算是多模态模型在手机UI垂域场景的一个应用工作了。这个工作主要做了两件事：</p>
<ul>
<li>针对手机UI场景，增加了对应的任务和训练stage<br>
</li>
<li>构造了对应的数据集Mobile3M，用于训练 &amp;
评测模型的手机UI理解和操作能力</li>
</ul>
<p>（不过模型的大小并不是很mobile啊...）</p>
<h1 id="mobile-ui数据">Mobile UI数据</h1>
<p>关于UI，特别是手机UI的数据集目前已经有一些了。现有的这些数据集在这里根据dataset
structure被分为了两类：</p>
<ul>
<li>Dot：这些数据集中的每个数据实例仅包含一个UI页面，以及不同的细粒度任务和相应的答案。这些数据集只关注单个UI的内容，无法捕捉到用户在使用这些app的操作过程。
<ul>
<li>Rico（2017）：安卓UI数据集<br>
</li>
<li>UIBert（2021）：发布了两个从Rico扩展来的数据集<br>
</li>
<li>Ferret-UI（2024）：基于UI detection model打标的安卓 &amp;
苹果数据集<br>
</li>
</ul></li>
<li>Chain：包含a sequence of action-UI pages。
<ul>
<li>AITW（2023）：有715k的数据<br>
</li>
<li>Auto-UI（2023）：进一步过滤了AITW的GoogleApps子集，留下152k数据</li>
</ul></li>
</ul>
<p>UI页面包括截图和结构化的文档，结构化文档能够给出UI中各个组件的层级关系，但是AITW和Auto-UI都没有结构化文档的信息。</p>
<p>下表列出了现有的Dot和Chain类型的数据集：</p>
<img src="/96393d3b/datasets.png" class title="MobileVLM">
<h1 id="mobile3m数据集">Mobile3M数据集</h1>
<p>Mobile3M专注在Chinese apps，总共包含49个下载量超过1000万的app：<br>
- 20,138,332 actions<br>
- 3,098,786 screenshots and corresponding XML</p>
<p>XML就是每个UI截图对应的结构化信息。下面是一个例子：</p>
<img src="/96393d3b/xml.png" class title="MobileVLM">
<p>整个数据集被组织成49个directed
graph，每个graph对应一个app。可以认为每个有向图就是一个app（几乎）所有可能操作的集合，有向图里的一条路径就是一个用户操作的sequence。UI截图就是节点，action就是有向图的边。</p>
<p>49个app的选择中，确保AppStore中的每个主要品类至少包含两个app。Mobile3M的app分布如下：</p>
<img src="/96393d3b/apps.png" class title="MobileVLM">
<p>相比前面提到的Dot和Chain类型数据，Graph类型的Mobile3M能够提供更多样化更复杂的操作信息。下图是Dot、Chain和Graph的对比例子：</p>
<img src="/96393d3b/graph.png" class title="MobileVLM">
<h2 id="收集方式">收集方式</h2>
<p>那么这些UI和对应的结构化信息是怎么收集的呢？毕竟人工标注这么大规模的数据成本太高了。</p>
<p>小米的方法是在模拟器上运行这些app，然后用Appium收集UI，收集的数据包括截图和结构化信息的XML，包括按钮和文本框等元素，以及边界框和分层树等布局信息；每个元素又包含名称和bbox。</p>
<p>另外，人和UI的交互需要一些操作，这里定义了三种操作：</p>
<ul>
<li>Click<br>
</li>
<li>Scroll<br>
</li>
<li>Input</li>
</ul>
<p>参考APPAgent的做法，小米采用random walk
algorithm来探索app的空间，以此收集app的所有页面。在random
walk期间，优先考虑输入动作，小米为每个app设计了10个相关的输入关键词，每次输入时随机从这些关键词选择输入。</p>
<p>对于某一个UI page，它的action
trace定义为从app主页到该页面的最短操作序列。action
trace经过的每一个边（也就是操作）的id拼在一起，作为这个UI
page的唯一标识。比如下面这个图里，Baicizhan0经过click（Edit）进入Edit界面，因此Edit页面的标识就是Baicizhan0_1。</p>
<img src="/96393d3b/graph_example.png" class title="MobileVLM">
<p>在随机游走手机UI数据的过程中，会遇到很多重复数据，因此需要设计一个UI数据去重的策略。对于新的UI，会用BM25检测库中最相似的前5个XML，如果Element
Diff&lt;5 &amp; Pixel Diff &lt;30%，那么就认为这是两个相似的页面。</p>
<p>这样去重有几个好处：</p>
<ul>
<li>大大减少了搜索空间<br>
</li>
<li>避免了死循环，因为如果重复访问一个页面可以检测并终止它<br>
</li>
<li>这样可以把树状结构的数据构建为graph，不同的路径可以进入到相同UI，这有助于模型理解各个UI之间的连接关系</li>
</ul>
<h1 id="训练">训练</h1>
<p>小米使用的模型是Qwen-VL-Chat：</p>
<ul>
<li>Qwen-7B<br>
</li>
<li>ViT-bigG，1.9B参数<br>
</li>
<li>0.08B参数的adapter</li>
</ul>
<p>预训练和SFT过程的参数冻结设置如下：</p>
<img src="/96393d3b/train.png" class title="MobileVLM">
<h2 id="预训练">预训练</h2>
<p>在通用预训练的基础上，小米增加了2个stage的预训练：</p>
<img src="/96393d3b/ptm_stage.png" class title="MobileVLM">
<p>1、stage 1</p>
<p>这一阶段的预训练主要目标是增强模型对intro-UI
page的理解。所用任务主要是这三个：</p>
<ul>
<li>Element List Generation：要求模型识别页面中的所有interactive
element，包括文本和bbox，要求模型具备对应的OCR和grounding能力<br>
</li>
<li>Element Grounding：给定一个element description，让模型输出bbox<br>
</li>
<li>Action Space
Generation：生成当前UI的所有可能操作，模型需要分析每个元素的交互性，比如是可点击还是可输入等。这个能力对于stage
2的action prediction能力十分重要</li>
</ul>
<p>2、stage 2</p>
<p>这个阶段主要是提升inter-UI page的理解能力，任务主要是Action
Prediction：让模型输出从当前UI进入到目标UI的操作。</p>
<p>下面是以上这几个任务的一个示例：</p>
<img src="/96393d3b/task.png" class title="MobileVLM">
<h2 id="sft">SFT</h2>
<p>微调阶段，有3个任务：</p>
<ul>
<li>Page Navigation：这个任务不再像stage
2一样提供两个UI，而是提供一个UI和一个指令，模型要给出应该进行什么操作<br>
</li>
<li>VQA：根据UI截图回答问题<br>
</li>
<li>Auto-UI</li>
</ul>
<p>各个阶段的训练量：</p>
<img src="/96393d3b/train_data.png" class title="MobileVLM">
<h1 id="评测">评测</h1>
<p>各个版本的MobileVLM效果：</p>
<img src="/96393d3b/eval.png" class title="MobileVLM">
<h1 id="使用">使用</h1>
<p>MobileVLM的使用上还有几个问题：</p>
<ul>
<li>目前模型太大，即使是4-bit的量化，也需要46G
RAM和23G显存的设备才能跑<br>
</li>
<li>推理速度不够快，比如作出滚动屏幕这一决定会导致UI变化，需要后续的理解能力能跟上屏幕滚动<br>
</li>
<li>在手机上的使用存在权限问题：需要具有系统级操作的权限的工具来进行操作，但是，对于大多数闭源移动操作系统，向第三方应用程序授予系统级签名几乎是不可能的</li>
</ul>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】MobileVLM: A Vision-Language Model for Better Intra- and
Inter-UI Understanding, https://arxiv.org/abs/2409.14818v2</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>transformer</tag>
        <tag>SFT</tag>
        <tag>预训练</tag>
        <tag>多模态</tag>
        <tag>CV</tag>
        <tag>无监督学习</tag>
        <tag>UI</tag>
        <tag>小米</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM水印</title>
    <url>/2dee4921.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>给图片加水印大家都很熟悉了：</p>
<ul>
<li>比如给身份证照片加水印以确定用途保护个人信息，或者画师给自己的画加水印保护产权，想要无水印版本的就需要购买；这种水印一般比较明显，甚至会覆盖在图片上的关键位置；<br>
</li>
<li>也有些图片水印是隐形的，人眼极难发现；这种一般是用来确定数据来源（比如你要是给公司的数据文件截屏上面很可能就有你工号的隐形水印，一下就能定位到操作人），或者保护知识产权的；隐形水印可能需要借助工具自动识别，人眼的分辨能力没法处理。</li>
</ul>
<p>其实不只图片可以加水印，LLM生成的文字也可以加水印。LLM生成结果加水印有什么用呢？目前来看主要是两种主要应用场景：</p>
<p>（1）帮助人类辨别哪些文本是LLM生成，哪些是人类写的（比如检测AI生成的作业或者论文）</p>
<p>（2）对于开源协议不可以商用的模型，开发者能够有一定手段判断是否有人在未授权的情况下使用了自己的模型</p>
<p>不过个人感觉这两种应用场景其实都不是很solid：</p>
<p>（1）现在已经有大量开源模型，没法保证使用这些开源模型的人会愿意加入水印（或者说大概率不愿意），只有用户使用API生成，且模型提供商加入了水印，且水印的检测方式给到你的情况下，你才有机会检测水印；</p>
<p>（2）如果使用者进行了一定的微调，模型的参数变化了，那么水印检测的方法也可能会失效；不过这个场景应该会有一些开源大厂会关注：如何在保持开源的情况下，又能不被滥用，如果不能在一定程度上保持这点，那就有可能对关键大小的模型选择不开源，这对AI开发者就是个坏消息；另外如果是通过API提供的，那模型供应商自己插入水印就变得方便了。</p>
<p>来看下LLM水印具体是怎么做的。</p>
<h1 id="red-list-和-green-list">Red List 和 Green List</h1>
<p>这个方法出自23年初的《A Watermark for Large Language
Models》，这是一种比较简单朴素的LLM水印方式。</p>
<p>首先，我们需要知道，LLM
watermarking分成两部分：水印的添加和水印的检测。添加水印和检测水印形成一个完整的闭环，我们才能够追溯这些文字的源头。</p>
<p>文中提出理想水印的几个要求，还是比较合理的：</p>
<ul>
<li>水印的检测不需要知道模型参数和获取model API<br>
</li>
<li>加水印的模型不需要经过训练<br>
</li>
<li>只要有一个连续的文字片段就可以检测水印，甚至无需完整的上下文<br>
</li>
<li>水印无法被只修改少量文字就移除<br>
</li>
<li>有一个统计量能够判断水印检测的置信度</li>
</ul>
<p>更长远来看，个人认为可能还有一些更深层次的要求，比如：</p>
<ul>
<li>水印难以被迁移到无水印的文本上<br>
</li>
<li>水印的添加和检测成本显著地低于生成文本</li>
</ul>
<h2 id="hard-red-list-watermark">hard red list watermark</h2>
<p>看下文中提出的第一种最简单的加水印的方法，使用hard red list。</p>
<p>正常来说，LLM
decode的时候首先根据prompt计算出下一个token在vocab上的概率分布；如果是贪心解码，那就下一个token就直接选择概率最高那个，如果是top-k或者top-p解码，那就会在概率最高的几个token里选择一个。</p>
<p>现在如果在每次解码，都把vocab随机、均匀地分成red list和green
list两组，并且限定下一个token只能从green
list里选，那么后续就可以通过检测生成的文本中是否包含red
list的token来判断这段文本是否是这个LLM生成的了：如果这段文本很好地遵循了只从green
list选择的规则，那么就是LLM生成的（检测到了水印），反之如果这段文本根本不遵从这个rule，那么就很可能是人类写的（没有水印）。可以使用z检验作为指标来判断。</p>
<p>加入水印的具体方案：</p>
<img src="/2dee4921/algo1.png" class title="watermark">
<p>在这个方案下，检测水印需要两个前提：</p>
<ul>
<li>需要知道这个模型的vocab<br>
</li>
<li>需要知道随机数生成逻辑和split词表的具体逻辑</li>
</ul>
<p>有了这两个，就可以检测出来插入了这个水印的文本。</p>
<p>这种插入水印的方式显然会对生成的文本质量造成很大的影响，毕竟有一半的token不能使用了，运气最差的情况下，概率最高的token都被加入到red
list里，那下一个token就是概率很低的token，导致句子完全不通顺了。</p>
<p>我们知道无论哪种语言都有一些概率很高，近乎固定组合的sequence，比如成语、俗语、诗歌、歇后语。比如本来“落霞与孤鹜齐”的下一个字是“飞”，但是很不好运，使用hard
watermark之后“飞”字被ban了，那生成的结果肯定就出问题了。</p>
<p>这种搭配很固定的情况称为low entropy
sequence，因为只要你看到了上文就基本可以确定下一个字是什么。</p>
<h2 id="soft-red-list">soft red list</h2>
<p>既然hard
watermark会对生成的文本质量造成太大影响，那就尝试把hard变成soft，缓解一下这个问题。</p>
<p>直接看下方案：</p>
<img src="/2dee4921/algo2.png" class title="watermark">
<p>前两步和hard方案是一样的。生成完随机数之后，就要根据随机数，切分red
list和green list了，在soft方案里，red list和green
list不再是对半分，而是有一个超参数γ，只有γ|V|个token会被放到red
list里。明显γ越大，水印越强，但是对生成质量的影响也越大。</p>
<p>（其实再进一步，还可以对每个token设定predefine的red list。）</p>
<p>分出red list之后，也不直接把red list里的token禁用，而是对在green
list里的token的logits增加一个正数δ，这相当于人为降低了red
list里的token得分，但是并不100%禁用。δ越大，red
list中的token被压制得越多，水印强度越大(更容易检测)，同样也会生成质量影响更大。</p>
<p>soft方案对于low entropy的情况也有比较好的缓解，因为low entropy
sequence中，下一个token的概率是非常高的，因此即使加了一个δ，这个固定搭配的token仍然有很大的机会被选中，无论是什么解码方式。</p>
<p>γ和δ选择和模型是高度相关的，这就需要做实验确定了。文中在OPT模型上做了实验，随着γ和δ的增强，ppl是提升的：</p>
<img src="/2dee4921/influence.png" class title="watermark">
<p>在检测的时候同样用z检验判断是否加了水印。</p>
<h2 id="private-watermark">private watermark</h2>
<p>前面的hard watermark和soft watermark在生成red
list的时候都是根据当前最后一个token来生成的，这样简单的机制在抗破译抗攻击方面还是不够强。那么一个增强的方法就是使用pseudorandom
function (PRF)。</p>
<p>PRF使用一个密钥，根据当前token和前h个token作为输入，生成随机种子，以决定vocab里的token是否要加入red
list。这样在检测水印的时候，同样需要密钥和相同的PRF。</p>
<img src="/2dee4921/algo3.png" class title="watermark">
<p>最后看一个水印例子：</p>
<img src="/2dee4921/example.png" class title="watermark">
<h1 id="undetectable-watermarks">Undetectable Watermarks</h1>
<p>上面的方法中，无论是hard还是soft，都会明显地影响生成结果，那有没有办法再减小一点水印对生成结果的影响呢？《Undetectable
Watermarks for Language Models》就基于密码学的工具引入了undetectable
watermarks。</p>
<p>首先一个思路是，并不是所有文本都适合插入水印。比如对于low
entropy的文本，“团结就是力量，这力量是铁，这力量是钢”这段文字根本就没法判断是人写的还是LLM生成的，因为它的搭配太确定了。如果非要改变这个搭配，就会明显影响了生成内容。因此low
entropy sequence就不适合插入水印。</p>
<p>再进一步，一段文本，有些entropy高（词表上token的得分分布比较均匀），有些entropy低（词表上token的得分只有一个0.99999，其他基本都是0），那就只在entropy足够高的时候才插入水印。</p>
<p>原文有很多数学证明，这里就直接用一个例子来说明下。</p>
<h2 id="插入水印">插入水印</h2>
<p>比如对一个模型输入prompt=“今天天气怎么样”，正常情况下模型会输出“今天天气很好，适合去公园散步”。</p>
<p>step1：检测熵</p>
<p>如果我们要插入水印，首先就要计算empirical
entropy。经验熵是基于已生成文本的实际概率分布计算的熵值，反映当前生成过程的“不确定性”。对于序列x1、x2、...、xt，p(x)是这个token生成时的概率，经验熵计算所有x的log(p(x))的平均值。经验熵越大，说明当前序列的不确性高，反之则说明序列的确定性很高（就是前面说的固定搭配）。这里就需要一个阈值来判别当前的熵是高还是低。</p>
<p>比如生成到“今天”，发现下一个token是“天气”的概率特别高，经验熵很低，那么这个时候就不插入水印；把“天气”放进序列之后，发现“今天天气”后面的熵开始高了，p(“很好”)=0.6，p(“不错”)=0.4，这两者比较接近，导致经验熵高于阈值，那么就可以开始插入水印了。</p>
<p>step2：生成随机数</p>
<p>我们需要一个PseudoRandom
Function，来生成随机数。PRF的特点是，只要输入相同，输出就相同，这也是后面我们可以检测水印的前提。PRF的输入有两个：</p>
<ul>
<li>密钥，这个是生成水印的人加入的，只有持有这个密钥才能检测水印，密钥可以是一个字符串，比如"技术博客linsight.cn"<br>
</li>
<li>当前的上下文</li>
</ul>
<p>根据这两个输入，我们获得一个随机数u=0.55</p>
<p>step3：选择token</p>
<p>根据随机数，在两个候选token “很好”和“不错”之间进行选择：</p>
<ul>
<li>如果u ≤ p("很好")，选择"很好"<br>
</li>
<li>如果u &gt; p("很好")，选择"不错"</li>
</ul>
<p>这一步在外部看来，概率的分布不变。因为这里我们只是替换了随机选择的函数，把“所有人都不知道的随机逻辑”变成“持有密钥的人知道，其他人不知道的随机逻辑”。</p>
<h2 id="检测水印">检测水印</h2>
<p>先要需要验证句子 "今天天气很好，适合去公园散步" 是否含有水印。</p>
<p>step1：重建PRF随机数</p>
<p>使用相同的密钥"技术博客linsight.cn"和上下文"今天天气"，通过PRF重新生成u₁=0.55。</p>
<p>step2：计算单个token验证分数</p>
<p>实际生成的token是"很好"，基于对数似然：</p>
<p>如果选"很好"：s(x₁, u₁) = -ln(u₁)</p>
<p>如果选"不错"：如果选0：s(x₁, u₁) = -ln(1 - u₁)</p>
<p>s = -ln(0.55) ≈ 0.597</p>
<p>step3、累加所有token分数</p>
<p>假设整个句子有5个高熵token，每个token的分数如下：</p>
<p>s₁=0.597, s₂=0.8, s₃=0.3, s₄=0.9, s₅=0.4</p>
<p>总分数：C = 0.597 + 0.8 + 0.3 + 0.9 + 0.4 = 2.997</p>
<p>step4、阈值判断</p>
<p>假设一段文本高熵token的数量为L。</p>
<p>对于自然文本：若文本无水印，伪随机数uj服从均匀分布 U(0,1)，此时每个
s(xj,uj) 的期望值为1，总C值的期望为 E(C)=L。</p>
<p>对于含水印文本：若文本含水印，uj的分布被密钥约束（例如选词1时
uj≤pj(1)），导致每个 s(xj,uj) 的期望值大于1，总C值 E(C)&gt;L。</p>
<p>因此在上面这个例子里，2.997 &lt; L，可以认为没有包含水印。</p>
<h2 id="思路">思路</h2>
<p>总体来说，这种方法有两个核心思路：</p>
<ul>
<li>只对高熵部分的文本添加水印，减少对生成质量的影响<br>
</li>
<li>把高熵token的选择从「不可知的随机选择」变成「基于密钥的PRF选择」，让掌握密钥的人可以检测到水印。</li>
</ul>
<h1 id="unbiased-watermark">Unbiased Watermark</h1>
<p>《UNBIASED WATERMARK FOR LARGE LANGUAGE
MODELS》提出两种不影响生成质量的水印方案，δ-Reweighting和γ-Reweighting。</p>
<h2 id="δ-reweighting">δ-Reweighting</h2>
<p>δ-Reweighting前半部分和上面的undetectable
watermark类似，主要在后面选择包含水印的token的策略有所不同。还是直接看一个例子。</p>
<p>比如模型根据prompt“你喜欢吃什么水果？”，现在生成到了“我喜欢吃”，再下一个token的选择有：</p>
<ul>
<li>苹果，p=0.35<br>
</li>
<li>香蕉，p=0.25<br>
</li>
<li>橘子，p=0.20<br>
</li>
<li>梨子，p=0.15<br>
</li>
<li>葡萄，p=0.05</li>
</ul>
<p>还是通过一个密钥和PRF生成一个随机数，根据这个随机数落在的区间，选择一个token。比如现在生成随机数0.66，落在了“橘子”的区间，那就输出“橘子”。</p>
<p>从单次采样来看，这是一个delta分布：只有“橘子”的概率为1，其他token都是0。因为只要PRF是同样的，context和密钥也是同样的，那么每次都会输出相同的“橘子”。</p>
<p>而从多次采样取平均的结果来看，生成苹果的概率依然是0.35，生成橘子的频率依然是0.20。</p>
<img src="/2dee4921/delta.png" class title="watermark">
<h2 id="γ-reweighting">γ-Reweighting</h2>
<p>相比δ-Reweighting，γ-Reweighting主要是在调整概率的方案上有所不同：</p>
<p>γ-Reweighting将词表随机打乱（还是使用包含密钥的PRF）之后分成前后两段，每次decode都固定对词表后半段的token概率进行提升（翻倍），而对词表前半段的token进行缩减（变成0）。</p>
<p>同样地，单次来看，有一半的token被ban了，但是整体多次统计来看，还是保持正常的概率。</p>
<img src="/2dee4921/gamma.png" class title="watermark">
<p>那检测水印的时候，就看打乱后的词表后半段是否概率更高。</p>
<p>整体来看γ-Reweighting的实现和检测更为简单，可能被逆向破解。而δ-Reweighting更加动态随机，抗攻击性更强。</p>
<h1 id="其他">其他</h1>
<ul>
<li>清华等⾼校联合推出了开源的⼤模型⽔印⼯具包
MarkLLM，支持多种水印的嵌入和检测方式</li>
</ul>
<h1 id="小结">小结</h1>
<ul>
<li>目前加水印都多多少少会对生成质量产生影响<br>
</li>
<li>加水印 &amp; 检测水印是高成本的事情，不是特殊场景恐怕不会使用</li>
</ul>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a9c496e3.html">DeepSeek-V3细节探索</a><br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="https://www.linsight.cn/6c0f6207.html">Qwen2.5-1M技术解析</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
<a href="https://www.linsight.cn/b4d047c1.html">DeepSeek-VL2的细节</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】A Watermark for Large Language Models,
https://arxiv.org/abs/2301.10226<br>
【2】UNBIASED WATERMARK FOR LARGE LANGUAGE MODELS,
https://arxiv.org/abs/2310.10669<br>
【3】Undetectable Watermarks for Language Models,
https://arxiv.org/abs/2306.09194</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM高效预训练(二)</title>
    <url>/1e2e35a7.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>从目前的实践结果来看，从大模型通过裁剪、蒸馏等手段获取小模型，效果是比较好的，同时成本也相比直接从零预训练要低廉得多，而且也免去了大量收集数据和清洗数据的工作。</p>
<p>今天就集中讲一下模型裁剪的工作。</p>
<h1 id="裁剪-蒸馏">裁剪 + 蒸馏</h1>
<p>论文：《Compact Language Models via Pruning and Knowledge
Distillation》 &amp; 《LLM Pruning and Distillation in Practice: The
Minitron Approach》</p>
<p>时间：2024年7月 &amp; 2024年8月</p>
<p>机构：NVIDIA</p>
<p>这两篇实际上是一个内容，后一篇是前一篇的整合和完整版，增加了基于Llama-3.1和Mistral的实验。《Compact》更像是比较混杂的实验报告。</p>
<p>英伟达提出的方法简单来说就是通过对已有的大模型进行裁剪，并对裁剪后的小模型使用蒸馏训练进行效果恢复（效果恢复的训练称之为retrian）。这么做可以以&lt;3%的retrain训练量，获得比从零训练的小模型更好的效果。</p>
<p>Nemotron-4 15B裁剪到MINITRON 8B和4B的效果如下：</p>
<img src="/1e2e35a7/nvidia_perf_8b.png" class title="LLM高效预训练">
<img src="/1e2e35a7/nvidia_perf_4b.png" class title="LLM高效预训练">
<p>整体的方案示意图如下：</p>
<img src="/1e2e35a7/nvidia_framework.png" class title="LLM高效预训练">
<p>接下来看下每个步骤具体怎么做。</p>
<h2 id="teacher-correction">Teacher Correction</h2>
<p>很多情况下我们会使用开源的模型作为裁剪的teacher模型，因为选择比较多，效果更好。但是如果使用开源模型会有一个问题，那就是我们没有开源模型的训练数据。这样在后面使用自己的数据进行蒸馏的步骤中，就有可能出现因为数据分布的偏移，影响了teacher模型的效果，导致蒸馏训练效果不好的情况。</p>
<p>那么缓解这个问题的一个方法，就是在裁剪和蒸馏之前先用部分自有的训练数据对teacher模型进行微调。这个过程就叫teacher
correction。</p>
<p>下图对比了使用和不使用teacher correction的情况下，把Mistral NeMo
12B裁剪到8B后，8B模型的训练loss下降情况：</p>
<img src="/1e2e35a7/nvidia_tc.png" class title="LLM高效预训练">
<p>在对12B模型使用了teacher
correction的情况下，8B模型的loss保持相对较低。</p>
<p>实验中，teacher correction阶段使用了~127B的token。</p>
<h2 id="参数裁剪">参数裁剪</h2>
<p>和我们在上一篇讲到的weight
subcloning类似，NVIDIA也通过计算neuron的重要性来决定裁剪/保留哪些参数。而NVIDIA仅使用了1024个sample作为裁剪的calibration
dataset，用于获得各个参数的activation。</p>
<p>参数裁剪分为深度裁剪和宽度裁剪。</p>
<p>1、宽度裁剪</p>
<p>宽度的裁剪包括几个部分：neuron裁剪、attention
head裁剪和embedding裁剪。这几个部分的重要性计算方法如下：</p>
<ul>
<li>注意力部分的裁剪以头为单位，计算每个头输出的L2 norm：</li>
</ul>
<p><span class="math display">\[F_{\mathrm{head}}^{(i)}=\sum_{\mathrm{B,S}}\|\operatorname{Attn}(\mathbf{X}\boldsymbol{W}^{Q,i},\mathbf{X}\boldsymbol{W}^{K,i},\mathbf{X}\boldsymbol{W}^{V,i})\|_2\]</span></p>
<ul>
<li>对于MLP层，有 <span class="math inline">\(\mathrm{MLP}(\mathbf{X})=\delta\left(\mathbf{X}\cdot
W_1^T\right)\cdot
W_2\)</span>，以投影后的输出值作为neuron的重要性：</li>
</ul>
<p><span class="math display">\[F_{\mathrm{neuron}}^{(i)}=\sum_{\mathrm{B,s}}\mathrm{X}(W_1^i)^T\]</span></p>
<p>而embedding层则是以LN后的值作为重要性衡量：</p>
<p><span class="math display">\[F_{\mathrm{emb}}^{(i)}=\sum_{\mathrm{B,S}}LN(\mathrm{X})_i\]</span></p>
<p>注意这里在batch和sequence维度上都是直接求和。实际上论文中验证了三种不同的方法：mean、L2
norm和variance。在batch和sequence维度上分别使用这三种不同的aggregation方法，效果对比如下：</p>
<img src="/1e2e35a7/nvidia_aggregation.png" class title="LLM高效预训练">
<p>使用（L2，mean）和（mean，mean）的效果最好。</p>
<p>NVIDIA这里在宽度裁剪的时候，并没有特别关注使用残差连接的层，而是直接把它们当成独立的参数进行裁剪，这里稍微有点奇怪。</p>
<p>2、深度裁剪</p>
<p>论文使用了2种模型深度裁剪的方法：<br>
-
PPL：通过移除一层或多层的模型，对比移除前后同样输入的PPL变化，就可以得出被移除的层对模型输出的影响程度，这个影响程度就可以作为这一层或者多层的重要度衡量<br>
- Block Importance（BI）：对某一层或连续多层的输入输出按如下cosine
distance计算获得层重要度：</p>
<p><span class="math display">\[\mathrm{BI}_i=1-\mathbb{E}_{X,t}\frac{\mathrm{X}_{i,t}^T\mathrm{X}_{i+1,t}}{\|\mathrm{X}_{i,t}\|_2\|\mathrm{X}_{i+1,t}\|_2}\]</span></p>
<p>BI的计算用的就是ShortGPT的方法。</p>
<p>3、获取student模型</p>
<p>获得各个维度的重要度之后，就要决定各个维度分别保留多少参数。</p>
<p>一般来说，我们会有一个target
size，以及一系列的常用模型结构配置。可以把这些配置组合起来，保留那些总参数量符合我们要求的：</p>
<img src="/1e2e35a7/nvidia_search.png" class title="LLM高效预训练">
<p>按上图这些组合，最终有十几二十个符合总参数量要求的configuration，还算在可以接受的范围内。</p>
<p>然后对比所有裁剪出来的candidate的效果，保留效果最好的。由于裁剪会让模型的输出变差很多，所以并不直接对比裁剪后的candidate，而是会进行轻量级的训练。论文中是使用了~1.8B的参数对各个候选模型进行了训练：</p>
<img src="/1e2e35a7/nvidia_train_rank.png" class title="LLM高效预训练">
<p>轻量级的训练结果表明只需几百步的训练就能得到这些candidate效果的rank，并且比较稳定。</p>
<h2 id="retrain">retrain</h2>
<p>retrain阶段以蒸馏的方式进行训练，实验中总的训练量只有不到100B。</p>
<p>对于一个LM，一个token xi的输出分布是：</p>
<p><span class="math display">\[p(x_i,\tau)=\frac{\exp\left(\frac{x_i}\tau\right)}{\sum_{j=1}^{|V|}\exp\left(\frac{x_j}\tau\right)}\]</span></p>
<p>其中 <span class="math inline">\(\tau\)</span>
是温度。那么整个sequence的logit-based KD loss就是：</p>
<p><span class="math display">\[L_{\mathrm{logits}}=\frac1l\sum_{k=1}^l\mathrm{Loss}(p_t^k(x,\tau),p_s^k(x,\tau))\]</span></p>
<p>这是经典的蒸馏loss。也可以增加intermediate state-based KD
loss，也就是用中间层的hidden state计算loss：</p>
<p><span class="math display">\[L_{is}=\frac1l\sum_{k\in
H}\sum_{i=1}^lLoss_k(h_t^{ki},h_s^{ki})\]</span></p>
<p>那么最终的训练损失是：</p>
<p><span class="math display">\[L=L_{\mathbf{CLM}}+L_{logits}+\alpha\times
L_{is}\]</span></p>
<h2 id="实验">实验</h2>
<p>论文中给出的一些实践经验，挑了一些有用的放这。</p>
<p>1、iterative or not？</p>
<p>对于给定的student
model的总参数量，我们有各种不同大小的模型可以作为teacher
model。那么一个自然的想法就是，如果teacher模型和studnet模型的参数规模差得很多，那增加一些中间量级的操作（iteration）是不是效果能更好？比如teacher模型的大小是15B，目标的student模型大小为4B，那我先把15B裁剪成8B，训练好之后再把8B裁剪成4B效果会不会比直接把15B裁剪成4B更好？</p>
<p>答案是no。下图是实验的数据，T是iteration的次数，T=1是直接一步到位，T=2则是增加1个中间量级。</p>
<img src="/1e2e35a7/nvidia_iteration.png" class title="LLM高效预训练">
<p>虽然iteration的操作使得目标student模型裁剪后的初始loss较小，但是经过训练之后所有模型都收敛到相同的水平，因此没有必要浪费算力去做中间规模的模型，直接一步到位就行了。</p>
<p>2、宽度裁剪和深度裁剪的对比</p>
<p>用Nemotron-15B裁剪成8B，不同维度的裁剪结果如下：</p>
<img src="/1e2e35a7/nvidia_depth_width.png" class title="LLM高效预训练">
<p>结果上看，宽度裁剪的损失更少。这和我们已有的认知是符合的：相同参数的情况下，更深的模型一般会有更好的效果。深度裁剪保留了teacher模型的深度，使得student模型的实际容量更大。</p>
<p>3、蒸馏损失</p>
<p>前面提到使用KLD作为蒸馏损失，实际上还有很多其他变体可以选择：MSE、cosine
similarity、reverse KLD等。最终还是KLD最好：</p>
<img src="/1e2e35a7/nvidia_distill.png" class title="LLM高效预训练">
<p>4、Single vs Multi-Phase Retraining</p>
<p>这一点还是蛮重要的。现在的预训练模型基本都是包含两阶段的做法：<br>
- phase1：大量的通用数据<br>
- phase2：相比phase1量级较少，但是质量更高，和下游任务更紧密的数据</p>
<p>那么用哪个阶段的模型裁剪更好的呢？方案（1）是用phase1的teacher
checkpoint进行裁剪，然后对student进行phase1和phase2的训练；方案（2）是用teacher模型的phase2
checkpoint进行裁剪，然后对student模型进行phase2数据的训练。</p>
<p>两种方案的对比如下：</p>
<img src="/1e2e35a7/nvidia_phase.png" class title="LLM高效预训练">
<p>结果上看，第二种方案，也就是仅使用phase2的checkpoint和数据，效果更好。</p>
<h2 id="成本的节约">成本的节约</h2>
<p>假设我们原来的方案是从零训练Nemotron-4的15B、8B和4B三个规模的模型，三个模型每一个step的训练计算量分别是4.4e17,
2.5e17 and 1.2e17，那么总的训练量就是(4.4e17+2.5e17+1.2e17)×steps。</p>
<p>现在我们先训练了15B模型，然后从15B模型裁剪+蒸馏得到8B和4B模型，而蒸馏所需的数据很少，总的计算量只有从0训练的1/40，那么总的计算量为
(4.4e17 + 2.5e17/40 + 1.2e17/40) × steps。</p>
<p>结论是通过裁剪+蒸馏的方案，可以节省接近一半的总计算量，而两个小规模模型的效果甚至更好。</p>
<h1 id="基于l0剪枝">基于L0剪枝</h1>
<p>论文：《Learning Sparse Neural Networks through L0
Regularization》</p>
<p>时间：2017年12月</p>
<h2 id="tldr">TL;DR</h2>
<p>提出hard concrete distribution，把基于L0
norm的参数裁剪变得可以直接训练学习。</p>
<h2 id="方法">方法</h2>
<p>模型一般都是过参数化的（overparametrized），所以可以在裁剪掉部分参数的情况下，保持效果基本不变。</p>
<p>L0 regularization会对所有非0参数施加固定的惩罚：</p>
<p><span class="math display">\[\mathcal{R}(\boldsymbol{\theta})=\frac1N\bigg(\sum_{i=1}^N\mathcal{L}\big(h(\mathbf{x}_i;\boldsymbol{\theta}),\mathbf{y}_i\big)\bigg)+\lambda\|\boldsymbol{\theta}\|_0\]</span></p>
<p><span class="math display">\[\|\theta\|_0=\sum_{j=1}^{|\theta|}\mathbb{I}[\theta_j\neq0]\]</span></p>
<p>h是模型，L(·)是原模型的loss function，N是数据集的size。</p>
<p>通过最小化</p>
<p><span class="math display">\[\boldsymbol{\theta}^*=\arg\min_{\boldsymbol{\theta}}\{\mathcal{R}(\boldsymbol{\theta})\}\]</span></p>
<p>我们就能得到最佳的稀疏参数组合。</p>
<p>但是这里需要遍历所有可能的参数组合，也就是说最多需要验证 <span class="math inline">\(2^{|\theta|}\)</span>
次损失。这对于参数量巨大的模型显然是不可行的。</p>
<p>需要手动遍历所有可能是因为上面的这个计算是离散的，因此需要想办法把这个loss变得平滑可导。</p>
<p>考虑 <span class="math inline">\(\theta\)</span> 重参数化下的L0
norm：</p>
<p><span class="math display">\[\theta_j=\tilde{\theta}_jz_j,\quad
z_j\in\{0,1\},\quad\tilde{\theta}_j\neq0,\quad\|\boldsymbol{\theta}\|_0=\sum_{j=1}^{|\theta|}z_j\]</span></p>
<p>z就是一个控制是否保留对应参数的binary gate，而L0
norm则表示打开的gate的数量。</p>
<p>既然z是二值的（0/1），我们可以让z来自伯努利分布：</p>
<p><span class="math display">\[q(z_{j}|\pi_{j})=\mathrm{Bern}(\pi_{j})\]</span></p>
<p>基于此，可以把R改写一下：</p>
<p><span class="math display">\[\mathcal{R}(\tilde{\boldsymbol{\theta}},\boldsymbol{\pi})=\mathbb{E}_{q(\mathbf{z}|\boldsymbol{\pi})}\left[\frac1N\left(\sum_{i=1}^N\mathcal{L}\left(h(\mathbf{x}_i;\tilde{\boldsymbol{\theta}}\odot\mathbf{z}),\mathbf{y}_i\right)\right)\right]+\lambda\sum_{j=1}^{|\theta|}\pi_j\]</span></p>
<p><span class="math display">\[\tilde{\boldsymbol{\theta}}^*,\boldsymbol{\pi}^*=\arg\min\{\mathcal{R}(\tilde{\boldsymbol{\theta}},\boldsymbol{\pi})\}\]</span></p>
<p>$$ 是elementwise product。</p>
<p>这里第二项是连续可导的了，但是第一项里的离散z依然让第一项难以用梯度训练。</p>
<p>那就需要把z给变成连续的。</p>
<p>考虑一个来自分布q的s：</p>
<p><span class="math display">\[\mathrm{s}\sim
q(\mathrm{s}|\phi)\]</span></p>
<p>用hard-sigmoid把它转成0~1的值</p>
<p><span class="math display">\[\mathbf{z=min(1,max(0,s))}\]</span></p>
<p>那么就有</p>
<p><span class="math display">\[q(\mathbf{z}\neq0|\phi)=1-Q(\mathbf{s}\leq0|\phi)\]</span></p>
<p>Q是cumulative distribution function (CDF)。</p>
<p>基于这个分布，可以进一步把上面的R进行平滑：</p>
<p><span class="math display">\[\mathcal{R}(\tilde{\boldsymbol{\theta}},\phi)=\mathbb{E}_{q(\mathbf{s}|\boldsymbol{\phi})}\left[\frac1N\left(\sum_{i=1}^N\mathcal{L}\big(h(\mathbf{x}_i;\tilde{\boldsymbol{\theta}}\odot
g(\mathbf{s})),\mathbf{y}_i\big)\right)\right]+\lambda\sum_{j=1}^{|\theta|}\left(1-Q(s_j\leq0|\phi_j)\right)\]</span></p>
<p><span class="math display">\[\tilde{\boldsymbol{\theta}}^*,\phi*=\arg\min_{\tilde{\boldsymbol{\theta}},\boldsymbol{\phi}}\{\mathcal{R}(\tilde{\boldsymbol{\theta}},\phi)\},\quad
g(\cdot)=\min(1,\max(0,\cdot))\]</span></p>
<p>那么s具体一个用什么分布呢？</p>
<p>论文提出了hard concrete distribution。简单来说是这样一个分布：</p>
<img src="/1e2e35a7/L0.png" class title="LLM高效预训练">
<p>大部分的概率集中在靠近0和靠近1的两端，能够在保持平滑的同时，指出每个参数是否要保留。</p>
<h1 id="shortened-llama">Shortened LLaMA</h1>
<p>论文：《Shortened LLaMA: Depth Pruning for Large Language Models with
Comparison of Retraining Methods》</p>
<p>时间：2024年2月</p>
<p>Shortened
LLaMA的做法是仅对模型层数进行裁剪。为什么要仅对深度进行裁剪呢，论文的说法是可以提升裁剪后模型的推理速度，减少推理显存的需求。下面看看论文的一些细节，摘取一些可以参考的地方。</p>
<h2 id="层重要性">层重要性</h2>
<p>裁剪模型核心问题就是找到那些参数更重要。既然是对层进行裁剪，那么要做的就是计算整个transformer
block的重要性。</p>
<p>对于一个linear weight matrix，其参数是一个size为 <span class="math inline">\((d_\mathrm{out},d_\mathrm{in})\)</span>
的矩阵：</p>
<p><span class="math display">\[\mathrm{W}^{k,n}=\begin{bmatrix}W_{i,j}^{k,n}\end{bmatrix}\]</span></p>
<p>n是层index，k是这个矩阵的operation
type，比如是query的投影矩阵，或者是MLP层的up matrix。</p>
<p>文中提出了几种计算方法。</p>
<p>1、Magnitude (Mag)</p>
<p>参考《Pruning filters for efficient
convnets》中提出的计算方式，“weights with smaller norms are less
informative”，因此直接对所有参数的绝对值求和：</p>
<p><span class="math display">\[I_{\mathrm{Magnitude}}^{n}=\sum_{k}\sum_{i}\sum_{j}\left|W_{i,j}^{k,n}\right|\]</span></p>
<p>2、Taylor</p>
<p>移除一个transformer
block之后，如果模型的输出误差增大，那么说明这个层有影响。误差增大越多，层的重要性越大。</p>
<p>按泰勒展开，在忽略二阶导的情况下，这个误差可以表达为</p>
<p><span class="math display">\[\left|\mathcal{L}(W_{i,j}^{k,n};D)-\mathcal{L}(W_{i,j}^{k,n}=0;D)\right|\approx
\left|\frac{\partial\mathcal{L}(D)}{\partial
W_{i,j}^{k,n}}W_{i,j}^{k,n}\right|\]</span></p>
<p>L是training loss。因此整个层的重要性就是</p>
<p><span class="math display">\[I_{\mathrm{Taylor}}^n=\sum_k\sum_i\sum_j\left|\frac{\partial\mathcal{L}(D)}{\partial
W_{i,j}^{k,n}}W_{i,j}^{k,n}\right|\]</span></p>
<p>3、Mag+ and Taylor+</p>
<p>用Mag或者Taylor计算层重要性的时候，会把early
blocks判定为不重要。但是我们现在知道其实前几层和后基层对模型的整体结果影响都很大（《The
state of sparsity in deep neural networks》、《Layer-adaptive sparsity
for the magnitude-based
pruning》），因此需要结合这个经验，把前4层和后2层保留下来，不纳入裁剪候选层中。这个也和《Llm-pruner:
On the structural pruning of large language models》的做法一致。</p>
<p>4、PPL</p>
<p>除了可以使用training
loss，也可以通过计算PPL看移除一个层之后的影响。</p>
<p>以上各种方法的实验效果对比如下：</p>
<img src="/1e2e35a7/shortedllama_metrics.png" class title="LLM高效预训练">
<p>Taylor+ 和 PPL 的效果相对较好。</p>
<h2 id="训练">训练</h2>
<p>1、one-shot or iteration</p>
<p>对于裁剪应该是一步到位，还是一步一步迭代进行，文中给出的答案是one-shot就足够了。一方面，多次迭代会引入更多的成本，另一方面，多次迭代对结果的影响并不显著，相比之下，retrain阶段才是对结果影响更大的一步。</p>
<p>2、更细粒度的裁剪</p>
<p>前面的裁剪是以整个transformer
block为单位进行的，那么如果把粒度变小，对attention层和MLP层分开裁剪效果怎么样呢？结果是，还是以transformer
block整体进行裁剪效果更好：</p>
<img src="/1e2e35a7/shortedllama_fine_grain.png" class title="LLM高效预训练">
<p>3、效果</p>
<p>分别对LLaMA-7B和Vicuna-13B-v1.3进行不同程度的裁剪和训练，效果如下：</p>
<img src="/1e2e35a7/shortedllama_perf.png" class title="LLM高效预训练">
<h1 id="laco">LaCo</h1>
<p>论文：《Laco: Large language model pruning via layer collapse》</p>
<p>时间：2024年2月</p>
<p>LaCo = Layer Collapse</p>
<p>LaCo也是对模型的层数进行裁剪，不过被裁剪掉的层并不是直接抛弃，而是会通过参数合并的方式，尽量把效果保留下来。</p>
<h2 id="rdsc-layer-merge">RDSC Layer Merge</h2>
<p>RDSC = Reserving-Differences-while-Seeking-Common</p>
<p>对于模型中的某一层参数 <span class="math inline">\(\theta_{l}\)</span>，以及跟在其后面的m层的参数，可以通过下面的方式把它们合并起来：</p>
<p><span class="math display">\[\begin{aligned}\theta_{l}^{*}&amp;=\theta_l+(\theta_{l+1}-\theta_l)+\cdots+(\theta_{i+m}-\theta_l)\\&amp;=\theta_l+\sum_{k=1}^m(\theta_{l+k}-\theta_l)\end{aligned}\]</span></p>
<h2 id="模型层数裁剪">模型层数裁剪</h2>
<p>裁剪的算法如下：</p>
<img src="/1e2e35a7/laco_algo.png" class title="LLM高效预训练">
<p>解释一下：</p>
<ul>
<li>（1-6行）首先，在准备阶段，需要定义几个参数：①每次合并操作要合并的层数C；②合并操作的层范围[L,H]；③相邻合并层之间的最小间隔I；④模型输出相似性阈值T；此外还要准备calibration数据集D。<br>
</li>
<li>（10-11行）RDSC Layer
Merge：每次迭代，尝试将层l后面的K层合并到层l中，获得临时模型M_tmp。<br>
</li>
<li>（12行）Calculate
Similarity：前向计算获得M_tmp和原始模型M在D上的句子表示，并计算两个表示之间的cosine
similarity，得到平均相似度s。<br>
</li>
<li>（13-21行）Merge Evaluation and
Adjustment：如果s&gt;T，则合并成功，将M_tmp用于下一次迭代，否则l=l-1，进入下一次迭代。</li>
</ul>
<img src="/1e2e35a7/laco.png" class title="LLM高效预训练">
<h1 id="shortgpt">ShortGPT</h1>
<p>论文：《ShortGPT: Layers in Large Language Models are More Redundant
Than You Expect》</p>
<p>时间：2024年3月</p>
<p>ShortGPT也是仅对模型层数进行的裁剪。</p>
<img src="/1e2e35a7/shortgpt_intro.png" class title="LLM高效预训练">
<p>对于transformer模型，由于每一层的结构都一样，因此功能也类似，导致更容易出现层参数冗余的情况。对Llama2-7B-Base和Baichuan2-7B-Base移除某个层厚，看PPL和模型在MMLU上的变化，发现很多层其实对效果没有明显影响：</p>
<img src="/1e2e35a7/shortgpt_rm_layer.png" class title="LLM高效预训练">
<p>ShortGPT提出用下图的Block Influence计算每个层的重要性：</p>
<img src="/1e2e35a7/shortgpt_bi.png" class title="LLM高效预训练">
<p>文中还对模型宽度的冗余情况也做了一下分析，发现和层的冗余一样，注意力头之间也有类似的冗余情况：</p>
<img src="/1e2e35a7/shortgpt_head.png" class title="LLM高效预训练">
<p>不过注意力头的冗余相比层数的冗余来说，没有那么明显的规律，不同的模型之间冗余模式有所不同。</p>
<h1 id="wanda">Wanda</h1>
<p>论文：《A Simple and Effective Pruning Approach for Large Language
Models》</p>
<p>时间：2023年6月</p>
<p>Wanda = Weights and activations，Pruning by Weights and
activations的意思。</p>
<p>Wanda其实并不算是模型裁剪的方法，因为在得到低重要性的参数之后，并没有将这些参数抛弃，而是保留稀疏化的参数矩阵/向量。不过因此Wanda也有一个好处，就是不需要重新训练模型，而通过稀疏化也能获得一定的推理加速效果。</p>
<p>简单来说，Wanda就是通过下图的方法计算参数重要性：</p>
<img src="/1e2e35a7/wanda.png" class title="LLM高效预训练">
<p>而稀疏度是作为超参输入的，实验中使用两种稀疏度设置：<br>
- 4:8，对于每 8 个连续的权重，最多允许有 4
个非零权重。也就是说，在一个长度为 8 的连续权重块中，要剪掉至少 4
个权重，使得剩下的非零权重数量不超过 4 个。<br>
- 2:4，意义和4:8类似。</p>
<p>PyTorch code for Wanda：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># W: weight matrix (C_out, C_in);</span></span><br><span class="line"><span class="comment"># X: input matrix (N * L, C_in);</span></span><br><span class="line"><span class="comment"># s: desired sparsity, between 0 and 1;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prune</span>(<span class="params">W, X, s</span>):</span><br><span class="line">  metric = W.<span class="built_in">abs</span>() * X.norm(p=<span class="number">2</span>, dim=<span class="number">0</span>)</span><br><span class="line">  _, sorted_idx = torch.sort(metric, dim=<span class="number">1</span>)</span><br><span class="line">  pruned_idx = sorted_idx[:,:<span class="built_in">int</span>(C_in * s)]</span><br><span class="line">  W.scatter_(dim=<span class="number">1</span>, index=pruned_idx, src=<span class="number">0</span>)</span><br><span class="line">  <span class="keyword">return</span> W</span><br></pre></td></tr></table></figure>
<h1 id="sleb">SLEB</h1>
<p>论文：《SLEB: Streamlining LLMs through Redundancy Verification and
Elimination of Transformer Blocks》</p>
<p>时间：2024年2月</p>
<p>SLEB尝试用三种metric对模型中transformer
block的redundancy进行验证。</p>
<p>第一种metric是计算block的输入和输出之间的相似度。相似度越高，说明block的影响越小：</p>
<p><span class="math display">\[Metric_j^1=1-similarity(A_j,B_j)\]</span></p>
<p><span class="math display">\[similarity(x_i,x_j)=\frac{x_i\cdot
x_j}{||x_i||||x_j||}\]</span></p>
<p>但是按这种方法移除block会导致PPL显著增加。原因是虽然一个block的影响可能很小，但是在inference的过程中，这个影响很可能被放大。</p>
<p>第二种metric则把要验证的transformer
block移除，获取对输出结果的影响：</p>
<p><span class="math display">\[Metric_j^2=-\frac{1}{K}\sum_{k=0}^Klogp_{M_j}(w_k|w_{&lt;k})\]</span></p>
<p>这种方法在PPL上的表现比第一种好，但是随着移除的层数增多，也会上升。</p>
<p>第三种metric其实和第二种一样，只是第三种metric是迭代移除block的。每次移除一层，并用移除后的模型作为base模型，用于下次的层重要性计算。</p>
<p><span class="math display">\[Metric_j^3(M&#39;)=-\frac{1}{K}\sum_{k=0}^Klogp_{M_j^{&#39;}}(w_k|w_{&lt;k})\]</span></p>
<p>M'就是上一次移除之后的result model。</p>
<p>第三种方法一定程度上避免了移除连续的层，从而更大程度保留模型的效果。</p>
<img src="/1e2e35a7/sleb.png" class title="LLM高效预训练">
<h1 id="其他">其他</h1>
<p>一些文献：<br>
- 关于模型neuron、attention head、层重要性的论文：《A survey on deep
neural network compression: Challenges, overview, and solutions》、《An
survey of neural network compression》、《A comprehensive survey of
compression algorithms for language models》</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/dcb57672.html">长文详解--LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】Learning Sparse Neural Networks through L0 Regularization
https://arxiv.org/abs/1712.01312<br>
【2】Compact Language Models via Pruning and Knowledge Distillation
https://www.arxiv.org/abs/2407.14679<br>
【3】LLM Pruning and Distillation in Practice: The Minitron Approach
https://arxiv.org/abs/2408.11796<br>
【4】Shortened LLaMA: Depth Pruning for Large Language Models with
Comparison of Retraining Methods https://arxiv.org/abs/2402.02834<br>
【5】ShortGPT: Layers in Large Language Models are More Redundant Than
You Expect https://arxiv.org/abs/2403.03853<br>
【6】Laco: Large language model pruning via layer collapse
https://arxiv.org/abs/2402.11187<br>
【7】A Simple and Effective Pruning Approach for Large Language Models
https://arxiv.org/abs/2306.11695<br>
【8】SLEB: Streamlining LLMs through Redundancy Verification and
Elimination of Transformer Blocks https://arxiv.org/abs/2402.09025</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>预训练</tag>
        <tag>高效训练</tag>
        <tag>参数复用</tag>
      </tags>
  </entry>
  <entry>
    <title>CV入门--无监督学习</title>
    <url>/ae81a87b.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>标注数据总是缺乏的，特别是对于大规模预训练。因此无监督学习在大模型时代就更加重要了。相比有监督的训练，无监督学习不需要把最终结果都reduce到一个单一的label，因此模型也能学到更丰富的数据特征。</p>
<p>本篇整理图像领域一些无监督学习的方法，涉及的论文如下：</p>
<center>
<table>
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">模型/方法</th>
<th style="text-align: center;">时间</th>
<th style="text-align: center;">论文</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Memory Bank</td>
<td style="text-align: center;">2018年5月</td>
<td style="text-align: center;">Unsupervised Feature Learning via
Non-Parametric Instance-level Discrimination</td>
</tr>
<tr class="even">
<td style="text-align: center;">MoCo</td>
<td style="text-align: center;">2019年11月</td>
<td style="text-align: center;">Momentum Contrast for Unsupervised
Visual Representation Learning</td>
</tr>
<tr class="odd">
<td style="text-align: center;">SimCLR</td>
<td style="text-align: center;">2020年2月</td>
<td style="text-align: center;">A Simple Framework for Contrastive
Learning of Visual Representations</td>
</tr>
<tr class="even">
<td style="text-align: center;">BYOL</td>
<td style="text-align: center;">2020年6月</td>
<td style="text-align: center;">Bootstrap Your Own Latent：A New
Approach to Self-Supervised Learning</td>
</tr>
<tr class="odd">
<td style="text-align: center;">DINO</td>
<td style="text-align: center;">2021年4月</td>
<td style="text-align: center;">Emerging Properties in Self-Supervised
Vision Transformers</td>
</tr>
<tr class="even">
<td style="text-align: center;">BEiT</td>
<td style="text-align: center;">2021年6月</td>
<td style="text-align: center;">BEiT: BERT Pre-Training of Image
Transformers</td>
</tr>
<tr class="odd">
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">2021年11月</td>
<td style="text-align: center;">Masked Autoencoders Are Scalable Vision
Learners</td>
</tr>
</tbody>
</table>
</center>
<h1 id="moco">MoCo</h1>
<p>论文：《Momentum Contrast for Unsupervised Visual Representation
Learning》</p>
<p>时间：2019年11月</p>
<p>机构：FAIR</p>
<h2 id="tldr">TL;DR</h2>
<p>基于对比学习，MoCo通过动量更新的方法，改进图像的无监督学习，可以在多个下游任务获得接近/匹配有监督预训练的效果。</p>
<h2 id="无监督学习">无监督学习</h2>
<p>无监督学习在文本数据上效果很好，而在图像上则相对没那么突出。原因可能是二者在信号空间上的差异：文本数据天然有离散的自监督信号，而图像数据的信号在连续的高维空间中，相比之下没有那么有结构化。</p>
<p>无监督学习框架的设计一般包括两个主要方面：<br>
- pretext
task：这类任务本身的能力并不是我们想要的，只是通过这类任务让模型学到泛化性好的数据表征；“The
term “pretext” implies that the task being solved is not of genuine
interest, but is solved only for the true purpose of learning a good
data representation.”<br>
- loss function：决定了模型如何学习，往往也和pretext task密切相关</p>
<p>在此之前的一些无监督方法：<br>
- auto-encoder：任务是重建输入的图像，使用L1或者L2 loss衡量误差<br>
- GAN：包含生成网络和判别网络，衡量的是概率分布的差异</p>
<h2 id="对比学习">对比学习</h2>
<p>除了auto-encoder和GAN，使用对比学习也是一个自监督学习的思路。对比损失衡量一对数据在表示空间中的相似度，它们在表示空间中的具体值在训练过程中是会变化的，而不是固定的。</p>
<p>使用对比学习，可以看作是训练一个执行dictionary
look-up任务的encoder。</p>
<p>假设现在有一个输入query q，以及一系列由encoder编码过的样本 <span class="math inline">\(\{k_0,k_1,k_2,...\}\)</span>，其中只有一个样本
<span class="math inline">\(k_{+}\)</span>
和q是匹配的，即正样本（正样本最常用的来源就是对同一张图片使用不同的增强方法），其他样本都是负样本，那么contrastive
loss如下计算</p>
<p><span class="math display">\[\mathcal{L}_q=-\log\frac{\exp(q\cdot
k_+/\tau)}{\sum_{i=0}^K\exp(q\cdot k_i/\tau)}\]</span></p>
<p><span class="math display">\[q=f_{\mathbf{q}}(x^q)\]</span></p>
<p><span class="math display">\[k=f_{\mathbf{k}}(x^k)\]</span></p>
<p>其中τ是温度（实验中设为0.07），K是负样本数量。对比损失理论上会提升q和正样本的相似度，同时降低q和其他所有负样本的相似度。</p>
<p>fq和fk是query和key的encoder，通常来说这两个encoder可以一样，也可以不一样，取决于具体的pretext
task。而xq和xk可以是图片，图片的patch，或者一组patch的组合。</p>
<p>前面提到，训练的encoder的任务是做dictionary
look-up，类比到文本数据的预训练，可以认为这里的（K +
1）个表征就是dictionary的大小，正样本就是true
label，负样本则是其他的token。</p>
<p>那么这些负样本从哪里来呢？最简单的做法就是在一个mini-batch里，除了正样本以外的其他数据都看作是负样本：</p>
<img src="/ae81a87b/moco_a.png" class title="CV无监督学习">
<p>正样本的表征和负样本的表征同时通过bp进行训练。</p>
<p>这样的做法会把负样本数量和batch
size耦合了起来。而直观上来说，更大的负样本数量效果会更好。由于batch
size受制于显存大小（每个负样本都要通过bp更新），没法设得特别大，因此负样本的数量也就受到了限制。并且使用超大batch
size进行训练，对效果也会有影响（《Accurate, large minibatch SGD:
Training ImageNet in 1 hour》）。</p>
<p>为了增大负样本的数量，《Unsupervised feature learning via
non-parametric instance discrimination》中使用了memory bank的方法：</p>
<img src="/ae81a87b/moco_b.png" class title="CV无监督学习">
<p>memory bank中包含了训练数据集所有样本的编码表示。每次训练会从memory
bank中随机抽取一批作为负样本。这里memory
bank里的负样本不会随着query更新，因此能够在相同的显存下使用更大的batch
size。</p>
<p>但是memory
bank里的数据存在过时的数据（比如部分特征向量分别是由1000/2000/3000步前的encoder给出的结果），因此存在特征不一致的问题：q是最新的encoder编码的，k是不同时间的旧的encoder编码的。这种不一致的情况会对效果有影响。</p>
<h2 id="momentum-contrast">Momentum Contrast</h2>
<p>从上面的做法我们可以看到，构建好的dictionary需要满足两个条件：（1）dictionary要大（2）负样本的表征需要尽量保持编码的一致性。</p>
<p>MoCo在memory
bank的基础上，为了低成本地更新key的表示，不直接用bp更新，而是通过momentum更新key的encoder。</p>
<p>MoCo维护一个固定大小queue，每一步会把最新的一个mini-batch的表征放到这个queue里，并移除最旧的一个mini-batch。</p>
<p>假设fk的参数是θk，而fq的参数是θq，每一步θq还是正常按bp更新，而θk更新则通过momentum增量更新：</p>
<p><span class="math display">\[\theta_\mathbf{k}\leftarrow
m\theta_\mathbf{k}+(1-m)\theta_\mathbf{q}\]</span></p>
<p>其中m是momentum。在实际使用中，m的值比较大时（0.999）比较小值（0.9）效果更好，这也说明保持负样本的一致性的重要性。</p>
<p>MoCo的方案示意图如下：</p>
<img src="/ae81a87b/moco_c.png" class title="CV无监督学习">
<p>MoCo完整算法如下：</p>
<img src="/ae81a87b/moco_algo.png" class title="CV无监督学习">
<p>和memory
bank相比，MoCo对queue中的表征更新更加平缓，queue中的样本表示的一致性更好。</p>
<h1 id="simclr">SimCLR</h1>
<p>论文：《A Simple Framework for Contrastive Learning of Visual
Representations》</p>
<p>时间：2020年2月</p>
<p>机构：Google Brain</p>
<p>Hinton参与的一个工作。把无监督训练的模型在ImageNet上的效果做到和监督学习一样。</p>
<img src="/ae81a87b/simclr_intro.png" class title="CV无监督学习">
<p>SimCLR依然是基于对比学习的框架。为什么选择对比学习做无监督学习？生成式的方法包括encoder和decoder，一般会把重建图片作为任务，而这样复杂精细的任务其实对于对月学习representation并不是必须的，复杂的计算很多其实对下游没有什么帮助（decoder部分的学习）。而有些discriminative的方法会借用真实label或者生成的label，使用类似有监督的方式进行预训练。这种方式某种程度上是限制了模型学习的representation的泛化性。相比之下对比学习的框架在效率和效果上有更好的平衡。</p>
<p>SimCLR的训练框架如论文标题所说的，确实比较简单：</p>
<img src="/ae81a87b/simclr_framework.png" class title="CV无监督学习">
<p>其中有4个重要的组件：<br>
- data augmentation<br>
- encoder f(·)<br>
- projection head g(·)<br>
- contrastive loss function</p>
<p>整体来看，SimCLR更多是精细工程带来的成功。</p>
<img src="/ae81a87b/simclr_algo.png" class title="CV无监督学习">
<h2 id="data-augmentation">data augmentation</h2>
<p>论文对不同的图像增强方法和它们的组合进行实验：</p>
<img src="/ae81a87b/simclr_aug.png" class title="CV无监督学习">
<p>下图是在SimCLR其中一个branch不增强，而在另一个branch采用不同的数据增强方式下训练出来的模型在ImageNet上的效果。对角线的实验只用了一种增强，其他位置则是对应行和列的增强方式的组合。</p>
<img src="/ae81a87b/simclr_aug_exp.png" class title="CV无监督学习">
<p>发现crop和color distort这两个的组合效果特别好。</p>
<p>另外对比数据增强强度在监督学习和无监督学习的效果，发现无监督学习需要更强的增强强度来获取更好的效果：</p>
<img src="/ae81a87b/simclr_unsup.png" class title="CV无监督学习">
<h2 id="encoder">encoder</h2>
<p>SimCLR中的encoder理论上可以是任意模型。简单起见，论文中选择了ResNet。对比有监督学习，和使用不同大小的ResNet的SimCLR，发现随着模型规模的增大，无监督学习的效果和有监督学习的gap在减小。</p>
<img src="/ae81a87b/simclr_eval.png" class title="CV无监督学习">
<p>这说明无监督训练相比有监督，benefit more from更大的模型。</p>
<h2 id="nonlinear-projection-head">nonlinear projection head</h2>
<p>使用的projection head是一个MLP层</p>
<p><span class="math display">\[z_i=g(h_i)=W^{(2)}\sigma(W^{(1)}h_i)\]</span></p>
<p>对比三种projection的效果：（1）identity mapping（2）linear
projection（3）nonlinear projection</p>
<p>发现（3）比（2）提升3%，而比（1）提升则&gt;10%：</p>
<img src="/ae81a87b/simclr_proj.png" class title="CV无监督学习">
<p>在不同输出维度下，都有相同结果。</p>
<p>为什么projection head重要？论文认为，z = g (h)
被训练为对数据增强具有不变性，会去除可能对下游任务有用的信息（而在对比学习中不需要），如对象的颜色或方向，这样这些去除的操作可以在g，也就是projection
head中完成，从而使得encoder的输出h本身可以保留更多对下游任务有用的信息。</p>
<p>针对这个想法，用g(h)和h作为特征，用来预测数据变换的任务，发现h的效果更好，说明h相比g(h)确实保留了更多和数据变换相关的信息。</p>
<img src="/ae81a87b/simclr_proj_head.png" class title="CV无监督学习">
<h2 id="loss-batch-size">loss &amp; batch size</h2>
<p>SimCLR的loss：</p>
<p><span class="math display">\[\ell_{i,j}=-\log\frac{\exp(\sin(z_i,z_j)/\tau)}{\sum_{k=1}^{2N}\mathbb{I}_{[k\neq
i]}\exp(\sin(z_i,z_k)/\tau)}\]</span></p>
<p>SimCLR不使用memory
bank之类的方法，而只把同batch中的除了正样本之外的2（N-1）个样本全部作为负样本（batch
size = N）。</p>
<p>这么一来batch size应该就是越大越好了，实验结果也是如此：</p>
<img src="/ae81a87b/simclr_bs.png" class title="CV无监督学习">
<p>为了保证在大batch size下的训练稳定，SimCLR使用LARS optimizer。</p>
<h1 id="byolbootstrap-your-own-latent">BYOL（Bootstrap your own
latent）</h1>
<p>论文：《Bootstrap Your Own Latent：A New Approach to Self-Supervised
Learning》</p>
<p>时间：2020年6月</p>
<p>机构：DeepMind</p>
<img src="/ae81a87b/byol_intro.png" class title="CV无监督学习">
<p>之前的对比学习都需要使用正样本和负样本，并且会增大负样本的数量以提升效果。为什么要使用负样本，因为如果只使用正样本，那么模型的训练可能会collapse：只使用正样本，那么模型只需要对所有输入都输出相同的结果，比如全是0，也可以符合任务要求。</p>
<p>而BYOL则是设计了一个不使用负样本也不会collapse的框架：</p>
<img src="/ae81a87b/byol_framework.png" class title="CV无监督学习">
<p>框架中包含target和online两个模型，其中online模型是正常bp训练的，而target模型则是通过momentum进行更新：</p>
<p><span class="math display">\[\theta\leftarrow\mathrm{optimizer}(\theta,\nabla_\theta\mathcal{L}_{\theta,\xi}^{\mathbf{BYOL}},\eta)\]</span></p>
<p><span class="math display">\[\xi\leftarrow\tau\xi+(1-\tau)\theta
\]</span></p>
<p>输入部分则和其他对比学习一样，是对输入图片的数据增强，具体是参考了SimCLR的实现。</p>
<h1 id="dinoself-distillation-with-no-labels">DINO（self-DIstillation
with NO labels）</h1>
<p>论文：《Emerging Properties in Self-Supervised Vision
Transformers》</p>
<p>时间：2021年4月</p>
<p>机构：FAIR</p>
<h2 id="tldr-1">TL;DR</h2>
<p>ViT的效果不错，但是也存在缺点：计算量大训练量大，监督试的预训练任务信号丰富程度不如Bert或者GPT这种自监督的信号丰富，图片信息最后都被reduce到某一个类别上。</p>
<p>DINO通过蒸馏的形式实现了图像的无监督学习，并且发现无监督学习得到的ViT展现出一些在监督ViT或者CNN上没有的特性：<br>
- feature能给出物体的layout和边界（下图）<br>
- 自监督ViT的特征用来K-NN分类效果很好</p>
<img src="/ae81a87b/dino_intro.png" class title="CV无监督学习">
<p>并且强调了无监督学习过程中几个重要的组件：<br>
- momentum encoder<br>
- multi-crop training<br>
- ViT small patches</p>
<h2 id="方法">方法</h2>
<p>DINO的训练方法使用了知识蒸馏的框架，但是进行了一些改动。DINO训练示意图如下：</p>
<img src="/ae81a87b/dino_framework.png" class title="CV无监督学习">
<p>1、蒸馏</p>
<p>在知识蒸馏中，于student network <span class="math inline">\(g_{\theta_{s}}\)</span> 学习预测teacher network
<span class="math inline">\(g_{\theta_t}\)</span>
的输出。对于输入x，两个模型的输出分别是特征向量 <span class="math inline">\(P_{s}\)</span> 和 <span class="math inline">\(P_{t}\)</span>：</p>
<p><span class="math display">\[P_s(x)^{(i)}=\frac{\exp(g_{\theta_s}(x)^{(i)}/\tau_s)}{\sum_{k=1}^K\exp(g_{\theta_s}(x)^{(k)}/\tau_s)}\]</span></p>
<p>温度τ控制输出分布的sharpness。然后通过cross entropy让student
network学习teacher network的输出：</p>
<p><span class="math display">\[\min_{\theta_s}H(P_t(x),P_s(x))\]</span></p>
<p><span class="math display">\[H(a,b)=-a\log b\]</span></p>
<p>另外上图中，teacher的输出还会经过一个centering的操作。centering操作类似BN，对数据进行归一化，稳定训练的过程。具体的centering可以有多种实现方式：<br>
- 均值移动<br>
- Sinkhorn-Knopp<br>
- Softmax</p>
<p>2、teacher network</p>
<p>通常的蒸馏，会使用已经训练好的模型作为teacher
network。但是在DINO这样的无监督学习中，并没有一个已经训练好的模型。那teacher
network从哪来呢？</p>
<p>DINO中，teacher network会在训练中和student
network一定逐渐学习。具体来说，teacher network是student
network的exponential moving average (EMA)版本：</p>
<p><span class="math display">\[\theta_t\leftarrow\lambda\theta_t+(1-\lambda)\theta_s\]</span></p>
<p>λ会按cosine schedule从0.996逐渐增加到1。</p>
<p>这里的teacher network和MoCo中的momentum
encoder很类似，不过二者在各自框架中的角色有所不同。</p>
<p>3、multi-crop strategy</p>
<p>由于student network和teacher network都是随机初始化的，输入给student
network和teacher network并不是简单的两张相同的image，否则就student
network就无法学习到图像特征，而只需要模仿随机参数模型的输出就行了。</p>
<p>《Unsupervised learning of visual features by contrasting cluster
assignments》中的multi-crop是一个提升学习任务难度的方法。</p>
<p>具体来说，对于每个输入图片x，会通过数据增强，获得两个global view
xg1和xg2，以及一系列local view。global
view有较大的size，一般来说可以覆盖原图超过50%的内容（如224×224），而local
view的size则小一些（如96×96）。</p>
<p>所有的view都会输入给student network，而teacher network只会接收global
view的版本。这样能够鼓励模型学习到“local-to-global”的依赖关系：</p>
<p><span class="math display">\[\min_{\theta_s}\sum_{x\in\{x_1^g,x_2^g\}}\sum_{x^{\prime}\neq
x}H(P_t(x),P_s(x^{\prime}))\]</span></p>
<p>DINO的算法具体描述如下：</p>
<img src="/ae81a87b/dino_algo.png" class title="CV无监督学习">
<h2 id="消融实验">消融实验</h2>
<p>1、不同组件对效果的影响</p>
<img src="/ae81a87b/dino_module.png" class title="CV无监督学习">
<p>发现如果没有momentum更新teacher
network，整个框架就无法训练了。此外影响最大就是loss
function，把CE改成MSE之后效果也有极大的下降。</p>
<p>2、patch size的影响</p>
<p>patch
size越小效果越好，不过throughtput也越低，这是一个成本和效果的tradeoff：</p>
<img src="/ae81a87b/dino_patch_size.png" class title="CV无监督学习">
<h1 id="beit">BEiT</h1>
<p>论文：《BEiT: BERT Pre-Training of Image Transformers》</p>
<p>时间 2021年6月</p>
<p>机构：微软</p>
<p>BEiT的做法和之前在学习ViT的时候讲过的iGPT类似：</p>
<img src="/ae81a87b/beit_intro.png" class title="CV无监督学习">
<ul>
<li>相对输入图像进行patch（16×16）切分，然后获得各个patch的linear
embedding<br>
</li>
<li>使用类似MLM的MIM任务（masked image modeling），把部分patch
mask掉，要求模型从corrupted image预测这些被mask的patch</li>
</ul>
<p>不同的是，BEiT使用dVAE训练了image
tokenizer，用于把patch变成离散的token，这样Bert模型就可以和MLM训练一样，预测离散的token了。其中image
tokenizer的vocabulary size设置成了8192。</p>
<p>另外在选择mask的patch的时候，采用类似whole word
mask的方式，使用blockwise
masking。一个block包含多个patch（最小为16个）。对整个block进行mask可以提高训练的难度。总的mask比例设置为0.4。</p>
<img src="/ae81a87b/beit_blockmask.png" class title="CV无监督学习">
<h1 id="mae">MAE</h1>
<p>论文：《Masked Autoencoders Are Scalable Vision Learners》</p>
<p>时间：2021年11月</p>
<p>机构：FAIR</p>
<p>Kaiming参与的一个工作。</p>
<p>大致的框架还是follow auto-encoder的结构：</p>
<img src="/ae81a87b/mae_intro.png" class title="CV无监督学习">
<p>在这个基础上，做了一些优化。</p>
<p>1、masking</p>
<p>和ViT一样，图像会被分成多个patch，然后经过linear
transformation，再加上位置编码，获得patch
embedding。然后会随机选择一部分patch进行mask。</p>
<p>不一样的地方是，这些被mask掉的patch并不会用mask
token代替，而是直接被移除，因此encoder处理的时候只会见到没有被mask的部分。</p>
<p>实验中发现mask的比例在75%的时候，decoder仍然具备恢复原图的能力：</p>
<img src="/ae81a87b/mae_mask_ratio.png" class title="CV无监督学习">
<p>高mask率的好处是encoder的计算量少了，因此可以使用更强大的模型。此外高mask率带来的难度提升也使得训练效率更高了。</p>
<p>2、encoder</p>
<p>encoder使用的模型跟ViT类似。</p>
<p>3、decoder</p>
<p>虽然encoder的输入之后没有mask部分的patch，但是在把encoder的输出给到decoder之前，会添加上mask
token，这样decoder才能获取缺失部分的位置信息，并恢复图像。</p>
<p>decoder只会在预训练的时候使用，因此可以使用一个很小的decoder，从而把有用的训练更多放在encoder上。同时较弱的decoder会迫使encoder学习到更好的图像表征。</p>
<p>4、reconstruction target</p>
<p>decoder的训练目标是直接给出被mask掉的patch的pixel value，loss
function直接是预测的pixel value和真实pixel value之间的MSE误差。</p>
<p>针对各个组件，MAE做了消融实验：</p>
<img src="/ae81a87b/mae_ablation.png" class title="CV无监督学习">
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/dcb57672.html">长文详解--LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】Momentum Contrast for Unsupervised Visual Representation
Learning https://arxiv.org/abs/1911.05722<br>
【2】A Simple Framework for Contrastive Learning of Visual
Representations https://arxiv.org/abs/2002.05709<br>
【3】Bootstrap Your Own Latent：A New Approach to Self-Supervised
Learning https://arxiv.org/abs/2006.07733<br>
【4】Emerging Properties in Self-Supervised Vision Transformers
https://arxiv.org/abs/2104.14294<br>
【5】BEiT: BERT Pre-Training of Image Transformers
https://arxiv.org/abs/2106.08254<br>
【6】Masked Autoencoders Are Scalable Vision Learners
https://arxiv.org/abs/2111.06377<br>
【7】Unsupervised Feature Learning via Non-Parametric Instance-level
Discrimination https://arxiv.org/abs/1805.01978</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>CV</category>
      </categories>
      <tags>
        <tag>transformer</tag>
        <tag>预训练</tag>
        <tag>CV</tag>
        <tag>CNN</tag>
        <tag>无监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title>reasoning压缩(一)</title>
    <url>/bfa4f144.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>之前讲过了prompt压缩，追求把模型的输入减少一些：</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/7ugiuuhRaXV4P62C7GsA1w">prompt压缩(一)</a><br>
</li>
<li><a href="https://mp.weixin.qq.com/s/RKODtrYHzBL3bFlD2srzjA">prompt压缩(二)</a></li>
</ul>
<p>而如今长思考模型的使用越来越多，模型的输出长度也成了一个问题，于是也就有了压缩思考过程的尝试。</p>
<h1 id="token-budget-aware-llm-reasoning">Token-Budget-Aware LLM
Reasoning</h1>
<h2 id="寻找最佳budget">寻找最佳budget</h2>
<p>这篇文章里，他们发现了在prompt里加上生成budget的限制，比如要求模型使用50个token以内来回答问题，会改变模型生成的长度。比如下面这个例子：</p>
<img src="/bfa4f144/tba_examples.png" class title="reasoning_compression2">
<p>但是这个budget的选择也有讲究。首先，不合适的budget会影响模型的效果，准确率会受影响。另外，如果选择一个过低的budget，模型的生成长度上反而比合理budget下要长。比如上面这个例子，当budget=50的时候，模型最终生成长度为86；而如果把budget降到10，模型生成的长度就反而增长到157。文中把这个现象叫做token
elasticity
phenomenon。当给定的budget低于合理的范围，就会出现这个不减反增的现象。</p>
<p>因此怎么选择一个好的budget，在保证效果的情况下，又能让模型减少输出长度，就是关键所在。</p>
<p>文中首先用二分法来搜索一个budget，来保证模型输出的正确性。具体来说，就是用原CoT作为budget搜索的右边界(right)，0作为初始的左边界(left)。如果在budget=(left
+
right)/2的情况下模型输出依然正确，说明budget还能再压缩，那下一步就搜索左边，反之就要增加budget，去搜索右边：</p>
<img src="/bfa4f144/tba_algo1.png" class title="reasoning_compression2">
<p>为了对付token elasticity
phenomenon的问题，算法1里的isFeasible里用贪心策略，要求当前budget下的实际输出长度必须要比上次的短：</p>
<img src="/bfa4f144/tba_algo2.png" class title="reasoning_compression2">
<h2 id="tale-ep">TALE-EP</h2>
<p>基于前面的分析，文中提出两个方法来压缩reasoning长度，第一个是TALE-EP。</p>
<p>TALE = Token-budget-Aware Llm rEasoning</p>
<p>EP = Estimation and Prompting</p>
<img src="/bfa4f144/tba_ep.png" class title="reasoning_compression2">
<p>预测budget部分则是使用一个LLM，用下面的prompt来给出预测：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">Task: Analyze the given question and estimate the</span><br><span class="line">minimum number of tokens required to generate a</span><br><span class="line">complete and accurate response. Please Give the</span><br><span class="line">response by strictly following this format: [[budget]],</span><br><span class="line">for example, Budget: [[12]].</span><br></pre></td></tr></table></figure>
<p>再看一个TALE-EP的例子：</p>
<img src="/bfa4f144/tba_ep_example.png" class title="reasoning_compression2">
<p>TALE-EP不需要训练模型，只需要依赖LLM+prompt对budge进行预测。</p>
<p>TALE-EP能比CoT减少67%左右的token（从Vanilla CoT的461.25 → 148.72
tokens），不过效果也比CoT稍微差一点点（83.75% → 81.03%）。</p>
<h2 id="tale-post-training-tale-pt">TALE Post-Training (TALE-PT)</h2>
<p>另一个方式TALE-PT，则是直接把token的压缩训练到生成模型中去，相当于让生成模型自己隐式地去判断应该要用多少budget。训练方式有SFT和DPO两种。</p>
<p>1、SFT</p>
<p>把原来的CoT数据按前面的algorithm1和2改造成短思考的形式，然后直接微调生成模型。</p>
<p>2、DPO</p>
<p>另外一个方法就是使用强化学习DPO，用改造过的短思考数据作为正例，而原CoT的答案作为负例，进行训练。</p>
<p>经过SFT训练的TALE-PT效果还是比没有训练的TALE-EP更好一些：</p>
<ul>
<li>SFT版：+1.01%（77.56% → 78.57%）<br>
</li>
<li>DPO版：-3.45%（77.56% → 74.11%），token也减少了50%左右。</li>
</ul>
<h1 id="chain-of-draftcod">Chain of Draft（CoD）</h1>
<p>CoT要求模型一步一步分析当前的问题，把复杂问题分解成可以快速解决的子问题。这种要求下，模型的输出其实就是在碎碎念：“我现在要煮一个水煮牛肉，先切牛肉……逆纹切薄片才嫩，这块肉怎么这么难切？刀该磨了……料酒、生抽、淀粉、蛋清……蛋清是不是放多了？算了先这样试试吧……腌二十分钟够不够？现在再尝试加点料酒……”</p>
<p>这样虽然能够让过程清晰一些，但是也显得有些啰嗦了。</p>
<p>CoD就参照人类专业一些的做法。比如学霸在做数学题的时候可能就不会碎碎念，而是首先把当前的条件都用简洁的方式表达出来，然后列几个候选方案的公式，最后验证结果。</p>
<p>因此相比CoT，CoD在system
prompt上就明确要求用更少的文字来进行思考：</p>
<img src="/bfa4f144/cod_prompt.png" class title="reasoning_compression2">
<p>另外CoD还会提供一些人工编写，包含简洁思考过程的case来作为示例。比如用于GSM8K的system
prompt和few-shot example是这样的：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">system_prompt: |</span><br><span class="line">  Think step by step, but only keep minimum draft for each thinking step, with 5 words at most.</span><br><span class="line">  Return the answer at the end of the response after a separator ####.</span><br><span class="line">format: |</span><br><span class="line">  Q: &#123;question&#125;</span><br><span class="line">  A: &#123;answer&#125;</span><br><span class="line">fewshot: </span><br><span class="line">  - question: |</span><br><span class="line">      There are 15 trees in the grove. Grove workers will plant trees in the</span><br><span class="line">      grove today. After they are done, there will be 21 trees. How many trees did</span><br><span class="line">      the grove workers plant today?</span><br><span class="line">    answer: |</span><br><span class="line">      21 - 15 = 6. #### 6</span><br><span class="line">  - question: | </span><br><span class="line">      If there are 3 cars in the parking lot and 2 more cars arrive, how many</span><br><span class="line">      cars are in the parking lot?</span><br><span class="line">    answer: |</span><br><span class="line">      3 + 2 = 5. #### 5</span><br><span class="line">  - question: |</span><br><span class="line">      Leah had 32 chocolates and her sister had 42. If they ate 35, how many</span><br><span class="line">      pieces do they have left in total?</span><br><span class="line">    answer: |</span><br><span class="line">      32 + 42 = 74; 74 - 35 = 39. #### 39</span><br><span class="line">  - question:</span><br><span class="line">      Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12</span><br><span class="line">      lollipops. How many lollipops did Jason give to Denny?</span><br><span class="line">    answer: |</span><br><span class="line">      20 - x = 12; x = 20 - 12 = 8. #### 8</span><br><span class="line">  - question: |</span><br><span class="line">      Shawn has five toys. For Christmas, he got two toys each from his mom and</span><br><span class="line">      dad. How many toys does he have now?</span><br><span class="line">    answer: |</span><br><span class="line">      2 * 2 = 4; 5 + 4 = 9. #### 9</span><br><span class="line">  - question: |</span><br><span class="line">      There were nine computers in the server room. Five more computers were</span><br><span class="line">      installed each day, from monday to thursday. How many computers are now in the</span><br><span class="line">      server room?</span><br><span class="line">    answer: |</span><br><span class="line">      5 * 4 = 20; 9 + 20 = 29. #### 29</span><br><span class="line">  - question: |</span><br><span class="line">      Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday,</span><br><span class="line">      he lost 2 more. How many golf balls did he have at the end of wednesday?</span><br><span class="line">    answer: |</span><br><span class="line">      58 - 23 = 35; 35 - 2 = 33. #### 33</span><br><span class="line">  - question: |</span><br><span class="line">      Olivia has $23. She bought five bagels for $3 each. How much money does</span><br><span class="line">      she have left</span><br><span class="line">    answer: |</span><br><span class="line">      5 * 3 = 15; 23 - 15 = 8. #### 8</span><br></pre></td></tr></table></figure>
<p>CoD基本上需要给每个任务提供不同的few-shot
example，这里还是有些工作量的。其他任务的prompt都能在论文github找到。</p>
<p>这些example很重要，原文发现没有使用这样示例样本，CoD的准确率就会大打折扣，甚至比不用CoT直接回答高不了多少：</p>
<img src="/bfa4f144/cod_perf.png" class title="reasoning_compression2">
<p>在使用较大模型时，在压缩reasoning长度上，CoD的效果还是不错的，最大能压缩到CoT的7%，而准确率还能基本持平：</p>
<img src="/bfa4f144/cod_perf_1.png" class title="reasoning_compression2">
<img src="/bfa4f144/cod_perf_2.png" class title="reasoning_compression2">
<img src="/bfa4f144/cod_perf_3.png" class title="reasoning_compression2">
<img src="/bfa4f144/cod_perf_4.png" class title="reasoning_compression2">
<p>不过在小模型上（1.5B、3B之类的）效果就不行了：</p>
<img src="/bfa4f144/cod_perf_bad.png" class title="reasoning_compression2">
<h1 id="sketch-of-thoughtsot">Sketch-of-Thought（SoT）</h1>
<h2 id="专业思维">专业思维</h2>
<p>SoT和CoD的大思路其实有些相似（名字也相关，sketch和draft嘛），都是认为CoT的碎碎念没有必要，应该用更加专业的人类思维方式来进行思考，不用啥都往思维链里放。</p>
<p>在这个framework下，SoT自己先提出三种抽象的专业思维方式：</p>
<ul>
<li>Conceptual Chaining<br>
</li>
<li>Chunked Symbolism<br>
</li>
<li>Expert Lexicons</li>
</ul>
<p>1、Conceptual Chaining</p>
<blockquote>
<p>Conceptual Chaining extracts essential terms and presents reasoning
as direct step-by-step pathways with minimal text</p>
</blockquote>
<p>Conceptual
chaining仿照人类对事物概念的快速联想，用最简洁的符号来表示这种联想，比如箭头→。这种形式不用太关注这个联想的具体关系是什么，比如看到雨你可以联想到伞，那就“雨→伞”，也可以联想到云，那就“雨→云”，也可以联想到树，“雨→树”。这种方式不用把二者的具体关系都碎碎念解释出来，但是也很符合人类的思考方式。</p>
<p>Conceptual chaining适合用于commonsense reasoning相关的任务：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">Q: What is the name of the currency used in Seoul?</span><br><span class="line">A: &lt;think&gt; #Seoul → #South Korea → Won &lt;/think&gt;</span><br><span class="line">Answer: Korean Wo</span><br></pre></td></tr></table></figure>
<p>具体的prompt是这样的：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">## **Role &amp; Objective**  </span><br><span class="line">You are a reasoning expert specializing in **structured concept linking** by connecting essential ideas in a logical sequence. Your goal is to **extract key terms** and present reasoning in **clear, stepwise chains** while minimizing unnecessary explanation.  </span><br><span class="line"></span><br><span class="line">This reasoning method follows a **conceptual chaining approach**, where information is **linked in structured steps** to establish relationships between ideas. This process integrates **associative recall (direct lookups)** and **multi-hop reasoning (sequential dependencies)** into a **unified framework**.  </span><br><span class="line"></span><br><span class="line">This method is most effective for:  </span><br><span class="line">- **Commonsense reasoning** (quickly linking familiar ideas)  </span><br><span class="line">- **Multi-hop inference** (tracing logical or causal dependencies)  </span><br><span class="line">- **Fact-based recall** (retrieving knowledge with minimal cognitive load)  </span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">## **How to Apply This Reasoning Method**  </span><br><span class="line">1. **Extract Key Concepts** → Identify the most relevant words or entities.  </span><br><span class="line">2. **Use Minimal Words** → Keep each reasoning step **concise and direct**.  </span><br><span class="line">3. **Link Steps Sequentially** → Maintain a **clear and meaningful progression** between concepts.  </span><br><span class="line">4. **Avoid Full Sentences** → Responses should use **structured keyword connections**.  </span><br><span class="line">5. **Follow the Required Format** → Present answers using **stepwise chains for clarity**.  </span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">## **Rules &amp; Directives**</span><br><span class="line">1. **Use Structured Concept Linking**</span><br><span class="line">   - Each step **must be logically connected**.</span><br><span class="line">   - Use arrows (`→`) to show dependencies.</span><br><span class="line"></span><br><span class="line">2. **Avoid Unnecessary Text**</span><br><span class="line">   - **Do not** restate the question.</span><br><span class="line">   - **Do not** use full sentences.</span><br><span class="line"></span><br><span class="line">3. **Maintain Logical Flow**</span><br><span class="line">   - Concepts must be **meaningfully ordered**.</span><br><span class="line">   - Ensure **each step contributes to the reasoning process**.</span><br><span class="line"></span><br><span class="line">4. **Output Format**</span><br><span class="line">   - Use the exact structured format:</span><br><span class="line">   ``</span><br><span class="line">   &lt;think&gt;</span><br><span class="line">   [shorthand reasoning]</span><br><span class="line">   &lt;/think&gt;</span><br><span class="line">   \boxed&#123;[Final answer]&#125;</span><br><span class="line">   ``</span><br><span class="line">   - The **final answer must be boxed**.</span><br><span class="line">   - **If the question is multiple-choice, return the correct letter option inside the box.**</span><br><span class="line">   - **Use minimal words in your response.**</span><br></pre></td></tr></table></figure>
<p>这个prompt主要分为三部分：</p>
<ul>
<li>第一部分：角色定位和目的说明<br>
</li>
<li>第二部分：介绍Conceptual Chaining和它适用的场景<br>
</li>
<li>第三部分：具体说明怎么使用Conceptual
Chaining来解决问题，都有什么细节要关注</li>
</ul>
<p>其他的prompt在<a href="https://github.com/ashishpatel26/sot/tree/main/sketch_of_thought/config/prompts">https://github.com/ashishpatel26/sot/tree/main/sketch_of_thought/config/prompts</a>可以找到。</p>
<p>2、Chunked Symbolism</p>
<p>人类看信息的时候会把信息分块，比如一段段看，或者读电话号码的时候，会把3个或者4个数字看做一个chunk。</p>
<p>放到reasoning里，就是让模型将复杂的数学推理过程拆解为更小的、可管理的chunk，并通过符号和公式紧凑表达，避免自然语言的冗余描述。</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">Q: A car accelerates at 2.5 m/sˆ2 for 10 seconds. If</span><br><span class="line">its initial velocity was 15 m/s, what is its final</span><br><span class="line">velocity?</span><br><span class="line">A: &lt;think&gt; a = 2.5 m/sˆ2, t = 10 s, vi = 15 m/s vf =</span><br><span class="line">15 + (2.5 × 10), vf = 40 m/s &lt;/think&gt;</span><br><span class="line">Answer: 40 m/s</span><br></pre></td></tr></table></figure>
<p>3、Expert Lexicons</p>
<p>缩写这个大家都很熟悉，在特定领域都有大量的专有缩写，这让我们可以减少很多冗余文本的使用。</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">Q: A patient with STEMI is given MONA therapy. They</span><br><span class="line">are allergic to aspirin. Are they at risk with this</span><br><span class="line">treatment?</span><br><span class="line">A: &lt;think&gt; STEMI → ST-Elevation MI, MONA → Morphine,</span><br><span class="line">O2, Nitrates, Aspirin, so Aspirin ∈ MONA &lt;/think&gt;</span><br><span class="line">Answer: Yes</span><br></pre></td></tr></table></figure>
<h2 id="router">Router</h2>
<p>以上这几种思考方式分别适合用于不同的场景。</p>
<p>Conceptual Chaining适用于常识推理、多跳推理、Fact-based
Recall。而Chunked Symbolism就适用于数学推理和符号逻辑问题。Expert
Lexicons则适合专业领域比如医学相关的推理。</p>
<p>因此需要一个router根据输入问题的不同，选择一个适合的prompt来处理。文中训练了一个DistilBERT来做这个分流，DistilBERT模型比较小效率很高。训练数据来自于14200条reasoning
task的数据，这些数据用GPT-4o + prompt打标，分成这三类中的一类。</p>
<img src="/bfa4f144/sot_pipeline.png" class title="reasoning_compression2">
<h2 id="效果">效果</h2>
<p>相比CoT，SoT在效果差不多的情况下，可以减少70%左右的token使用。</p>
<img src="/bfa4f144/sot_perf.png" class title="reasoning_compression2">
<h1 id="inftythink">InftyThink</h1>
<p>一句话，InftyThink把线性连续的reasoning过程，转化成包含reasoning
summary的迭代过程。</p>
<p>看下面这张图就大概明白了：</p>
<img src="/bfa4f144/infty_intro.png" class title="reasoning_compression2">
<p>比如原来的CoT是把任务拆解成多个子问题，然后一个一个连续地输出结果。现在InftyThink不一次思考完所有问题，而是思考一部分之后，用一个summary
prompt +
LLM把前面的思考过程总结一下，替换原来是思考过程，然后再让模型在summary的基础上继续思考。</p>
<p>summary prompt：</p>
<img src="/bfa4f144/infty_prompt.png" class title="reasoning_compression2">
<p>文中而用于生成summary的LLM是Llama-3.3-70B-Instruct。(一个问题，这里为什么不使用原模型来做summary呢？)</p>
<p>另外，怎么决定什么时候要做summary呢？首先定一个基础语义单位，比如是句子或者段落，模型生成的结果都会按这个切分。当一个完整的语义单位生成完之后，如果现有的思考长度超过某个阈值（比如4k
token），那就会触发summary。</p>
<p>由于summary比原思考过程短，且迭代越多短得越多，因此可以减少整个思考过程的长度。理论上InftyThink的做法支持无限长度的生成。</p>
<p>由于思考的方式变了，因此模型需要重新训练，来适应这种一段一段summary +
生成的方式。训练用的数据从CoT数据改造的包含summary的数据：</p>
<img src="/bfa4f144/infty_gendata.png" class title="reasoning_compression2">
<p>效果上，InftyThink在OpenR1-Math的准确率有所提升，整个过程的计算程度也更短：</p>
<img src="/bfa4f144/infty_perf.png" class title="reasoning_compression2">
<h1 id="小结">小结</h1>
<p>prompt
engineer的含金量还在提升，模型越强prompt效果越好。赶紧都从过往的人类思维研究里找一些方法放到prompt里。</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a9c496e3.html">DeepSeek-V3细节探索</a><br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="https://www.linsight.cn/6c0f6207.html">Qwen2.5-1M技术解析</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="https://www.linsight.cn/4519eadd.html">prompt压缩(一)</a><br>
<a href="https://www.linsight.cn/ea2871bf.html">prompt压缩(二)</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="https://www.linsight.cn/2dee4921.html">LLM水印</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
<a href="https://www.linsight.cn/b4d047c1.html">DeepSeek-VL2的细节</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】Token-Budget-Aware LLM Reasoning<br>
【2】Chain of Draft: Thinking Faster by Writing Less<br>
【3】Sketch-of-Thought: Efficient LLM Reasoning with Adaptive
Cognitive-Inspired Sketching<br>
【4】InftyThink: Breaking the Length Limits of Long-Context Reasoning in
Large Language Models</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>reasoning压缩</tag>
      </tags>
  </entry>
  <entry>
    <title>prompt压缩(一)</title>
    <url>/4519eadd.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>现在长思考模型助力agent，能够循环进行复杂任务的拆解和执行。为了告诉模型任务和能够调用的工具/当前观察到的信息等，输入prompt也是越来越长，10k甚至更长的输入已经是普遍的状况。虽然推理框架也越来越强，不过本着能省一点是一点想法，prompt压缩也是个值得考虑的方向，特别是在偏垂域的场景。</p>
<p>其实垂域任务的微调也算是prompt压缩的一种。比如对于通用模型，做法是加上任务描述：</p>
<p>"判断这段文本的情感类型，正向输出1，负向输出0。{text}"</p>
<p>现在我们直接构造数据对(text,
label)，而不需要加额外的任务描述；用这些数据微调模型，让它专注做情感分析。回想一下，这正是Bert的拿手任务。</p>
<p>这相当于把任务描述训进了模型参数中，从而减少了每次的输入文本量，即"判断这段文本的情感类型，正向输出1，负向输出0。"这段文字。</p>
<p>当然这种做法在现在看来太低效了。针对每个任务或者每个prompt微调一个模型在大规模模型的场景下成本太高，也不可能部署多个模型来处理各种任务，更重要的是没法顾及到所有的输入prompt。</p>
<p>今天先来学习下几个prompt压缩的工作。</p>
<h1 id="conditioning-contextscc">Conditioning Contexts（CC）</h1>
<p>Conditioning contexts是prompt压缩比较早期的工作了。</p>
<p>CC属于soft prompt compression。既然有soft prompt
compression，那肯定有hard prompt compression。简单来说，hard
compression主要研究一个输入里哪些token可以保留哪些token可以删掉，是相对离散的；而soft
compression则是把压缩的prompt带到了参数层面，是连续的，有点像prompt
tuning。</p>
<h2 id="方法">方法</h2>
<p>CC具体方案就是在输入question前面append一些embedding（这就是压缩过的prompt），训练目标就是最小化原prompt的输出和压缩prompt的输出之间的KL散度：</p>
<img src="/4519eadd/cc_method.png" class title="prompt_compression">
<p>方案确实和prompt tuning很像，区别就是prompt
tuning是学习正确label，而CC学习的是原prompt下的输出结果。</p>
<p>研究人员拿GPT2系列各个规模的模型做实验，在不同规模的模型上，KL散度的走势和值都很相近，这说明这样的soft
prompt是有在多个规模的模型使用的潜力的（比如在小模型上训练，在大模型上使用）；另外随着soft
prompt的长度的增加，KL散度越来越低：</p>
<img src="/4519eadd/cc_kl.png" class title="prompt_compression">
<h2 id="观察">观察</h2>
<p>另外，研究人员通过对general类型问题和细节问题在压缩前后准确率的比较，发现压缩过的soft
prompt更倾向于记住general的内容，而遗忘细节内容：</p>
<img src="/4519eadd/cc_specific.png" class title="prompt_compression">
<p>那么这样的方式训练出来的soft
prompt可以在多大程度上保留原prompt的信息呢？研究人员用一个reconstruction
task，即在压缩的soft prompt后加上“Now repeat the
text:”的要求，让模型尝试给出它看到的信息。注意即使只给“Now repeat the
text:”这句话，模型也会输出，因此需要进行一定的归一化。把输出的分布在原prompt
+ repeat和no prompt +
repeat之间进行归一化，把原prompt的token的概率可视化：</p>
<img src="/4519eadd/cc_repeat.png" class title="prompt_compression">
<p>heatmap中黄色是1（说明soft
prompt很好地保留了原prompt信息），紫色是0。可以看到大致的趋势是：（1）随着soft
prompt的长度n减小，损失越来越大（2）soft
prompt更倾向于记住原prompt中靠前部分的信息。</p>
<p>最后，文中还提出一个想法：soft
prompt更能让模型遵循相应的要求。这里做了一个实验：有两个prompt，一个要求模型谈论猫，另一个要求模型输出负面情感的内容，然后分别使用原prompt和压缩过的soft
prompt对模型的输出结果进行指令遵循情况的检验。结果发现，soft
prompt比原prompt的指令遵循情况更好一些：</p>
<img src="/4519eadd/cc_follow.png" class title="prompt_compression">
<h2 id="小结">小结</h2>
<p>整体来看，CC所使用的soft prompt和prompt
tuning很像，依然是一次训练只能针对一条prompt。因此如果要使用的话，比较适用于有超长固定system
prompt的场景，这样在推理的时候可以节省一些推理成本。当然，在效果上是有一些损失的，而且损失的是现在大家比较关注的细节信息。</p>
<p>不过原文也提出了几个有意思的点：</p>
<ul>
<li>压缩过的soft prompt可以提升模型指令遵循的效果<br>
</li>
<li>reconstruction task可用于检验prompt的压缩效果</li>
</ul>
<h1 id="selective-context">Selective Context</h1>
<p>前面的CC是soft prompt compression，这里要讲的selective
context就是hard prompt compression。</p>
<p>selective
context的大思路：输入prompt中不是每个token都一样重要，有些知识模型已经知道，就不必重复说，因此可以删掉一些token。</p>
<h2 id="self-information">self-information</h2>
<p>那怎么判断一个token重不重要呢，就是使用self-information。</p>
<p>Information
theory中，self-information表征在给定的分布下，一个event携带的信息量。</p>
<p>在language
modeling的context下，生成一个token就可以看作一个event。那self-information就可以写作：</p>
<p><span class="math display">\[I(x)=-\log_2P(x_t|x_0,x_1,...,x_{t-1})\]</span></p>
<p>I越大，x的信息量越多。</p>
<p>题外话一下，language
modeling中和self-information相关的还有entropy和perplexity：</p>
<p><span class="math display">\[H(S)=\frac{1}{N}\Sigma_tI(x_t)\]</span></p>
<p><span class="math display">\[PP(S)=2^{H(S)}\]</span></p>
<p>对于连续的token，有</p>
<p><span class="math display">\[\begin{aligned}I(x_0,x_1)&amp;=-\log_2P(x_0,x_1)\\&amp;=-\log_2P(x_0)P(x_1|x_0)\\&amp;=-\log_2P(x_0)-\log_2P(x_1|x_0)\\&amp;=I(x_0)+I(x_1)\end{aligned}\]</span></p>
<p>这说明token级别以上的lexical
unit（比如words、phrases和sentences）都可以通过token的自信息得分相加而得到，这点很重要。</p>
<h2 id="方法-1">方法</h2>
<p>selective context的方法有三步。</p>
<p>（1）计算token的self-information</p>
<p>这一步可以选用小一些的模型比如Llama-7B，而生成模型则是更大规模的Llama模型。(那么这里就有一个问题：用于压缩的模型和用于生成模型之间的关系是否支持这种对应)</p>
<p>在实操上有一个发现，LLM倾向于给靠后的lexical
unit打低分，因此实操中不把整个prompt一次输入，而是改成一个句子一个句子计算，这样就缓解了靠后的lexical
unit分数偏低的问题。</p>
<p>（2）（optional）聚合lexical unit</p>
<p>在token层面删除，可能导致文本的不连续，因此可以改为在phrase或者sentence级别删除内容。不过这也引入了新的复杂性：phrase和sentence边界的检测。实操上可以依赖传统的NLP工具来分割phrase和sentence。</p>
<p>在消融实验中，phrase level的效果最好，而sentence
level的效果最差：</p>
<img src="/4519eadd/sc_level.png" class title="prompt_compression">
<p>（3）eliminate不必要的部分</p>
<p>删除的时候，不是使用自信息的threshold，或者固定保留top-k个unit，而是按self-information从高到低排序，保留总和为top-p的lexical
unit。p相当于限定了保留信息的量，而从高到低排序保证了所用的unit是最少的，也就是最大的compression
rate。</p>
<p>p设为0.5时的一个例子：</p>
<img src="/4519eadd/sc_example.png" class title="prompt_compression">
<h2 id="实验">实验</h2>
<p>既然删除了部分lexical
unit，那模型输出结果就会变化，模型的效果很可能会下降。研究人员用4类任务实验，验证压缩的效果：（1）original
context reconstruction（2）summarisation（3）question
answering（4）conversation task。</p>
<p>各个任务的指标都是和原prompt相比。具体来说，用original
prompt下的模型输出作为标准，计算压缩prompt的输出和原输出的BLEU, METEOR,
ROUGE, and BERTScore。</p>
<p>在不同的压缩率（删除的lexical
unit比例）下，模型的在各个任务的平均结果：</p>
<img src="/4519eadd/sc_result.png" class title="prompt_compression">
<p>和random compression的比较</p>
<img src="/4519eadd/sc_compare.png" class title="prompt_compression">
<h2 id="小结-1">小结</h2>
<p>实验没有测原task的得分变化，感觉这里有点不完善。</p>
<p>Selective
context的好处是不用训练生成模型，而可以应用到所有的输入prompt。不过一个问题是，用于压缩的prompt的小模型和生成模型在分布上也存在一些对不齐的情况，因此效果是有一些损失的。</p>
<h1 id="llmlingua">LLMLingua</h1>
<p>LLMLingua是prompt压缩比较经典的工作了。</p>
<p>他们观察到prompt里不同的部分 --
instruction、demonstration和question三者所能用的压缩率是不同的。demonstration通常是一些示例，是instruction和question的具象化，因此会包含比较多的信息冗余，而instruction和question本身是和answer是更加相关的，因此不能压缩太多。</p>
<img src="/4519eadd/lingua_framework.png" class title="prompt_compression">
<h2 id="方法-2">方法</h2>
<p>1、coarse compression</p>
<p>基于上面的思路，LLMLingua首先对demonstration做一个coarse的compression。具体来说就是以完整的demonstration为单位，删掉一部分demonstration。</p>
<p>instruction和question的压缩率是预定义的（实操中这两个压缩率分别是τ_ins=0.85,τ_que=0.9），可以根据这两个部分的压缩率、整体的target压缩率和各个部分的原始长度计算coarse
demonstration compression这一步要删掉多少demonstration。</p>
<p>那么怎么决定保留哪些demonstration呢？就是用一个小模型计算demonstration的PPL，然后保留PPL大的文本。</p>
<img src="/4519eadd/lingua_algo1.png" class title="prompt_compression">
<p>k表示选择多少个demonstration。</p>
<p>（由于demonstration是粗粒度的选择，最终选的token数量和target的压缩率有出入，因此需要重新计算一下inteructino和question的压缩率）</p>
<p>2、fine compression</p>
<p>在粗粒度的删除之后，就要进行细粒度的Iterative Token-level Prompt
Compression（ITPC），把（instruction，删减过的demonstration，question）再进一步进行压缩。在这一步，token-level的dropout可能造成更多的信息损失，因此应该使用sentence-level的dropout以保持一定的lingusitic
integrity。</p>
<p>（1）分段</p>
<p>首先把文本切成segment（实操中segment的长度是100token）。</p>
<p>（2）计算条件概率</p>
<p>使用小模型 <span class="math inline">\(\mathcal{M}_s\)</span>
计算每个段 <span class="math inline">\(s_j\)</span>
中token的条件概率：</p>
<p><span class="math display">\[
p\left(s_{j, i} \mid s_{j,&lt;i}, \widetilde{s}_{&lt;j}\right)
\]</span></p>
<p>其中 <span class="math inline">\(\widetilde{s}_{&lt;j}\)</span>
表示前 <span class="math inline">\(j-1\)</span> 个段压缩后的结果。</p>
<p>（3）动态计算压缩阈值</p>
<p>根据段 <span class="math inline">\(s_j\)</span> 的压缩比例 <span class="math inline">\(\tau_{s_j}\)</span>，动态计算阈值 <span class="math inline">\(\gamma_j\)</span>，保留满足 <span class="math inline">\(p(s_{j,i}) &gt; \gamma_j\)</span> 的令牌。</p>
<p>（4）迭代压缩</p>
<p>将压缩后的段 <span class="math inline">\(\widetilde{s}_j\)</span>
拼接至后续段，重复步骤2-3，直到所有段被压缩。</p>
<h2 id="效果">效果</h2>
<p>LLMLingua在长文本上的效果还是可以的：</p>
<img src="/4519eadd/lingua_perf1.png" class title="prompt_compression">
<img src="/4519eadd/lingua_perf2.png" class title="prompt_compression">
<h1 id="gist">GIST</h1>
<h2 id="思路">思路</h2>
<p>长的prompt，比如system
prompt占用大量重复计算；通过cache可以减少计算，但是prompt很长的话还是需要比较多的缓存；通过finetune可以把prompt内化，不过这样每个prompt都需要训一个模型。context
distillation就是这样，不需要额外数据，内化prompt，一个模型学一个prompt：</p>
<p><span class="math display">\[
\mathcal{L}_{CD}\left(p_{CD}^{t},
t\right)=\mathbb{E}_{x}\left[D_{KL}\left(p_{LM}(y | t, x) \|
p_{CD}^{t}(y | x)\right)\right]
\]</span></p>
<p>t是prompt。</p>
<p>那么更好的情况应该是只训一个模型，可以处理所有的prompt；学习G(t)这样一个映射，让G(t)更短，并且G有足够的泛化性。GIST方法就是学习G(t)的一种方法。泛化的G，只训练一次，就可以支持各种prompt的压缩：</p>
<p><span class="math display">\[
\mathcal{L}_{G}(p_{G}, T)=\mathbb{E}_{t \sim T, x}[D_{KL}(p_{LM}(y \mid
t, x) \| p_{G}(y \mid G(t), x))]
\]</span></p>
<h2 id="训练">训练</h2>
<p>GIST方法首先在词表增加一个gist
token，注意只有一个。训练的时候在prompt和answer中间夹k个gist
token的copy，这k个gist token就用来学习怎么压缩prompt。</p>
<p>把gist token加在prompt和answer中间之后，还要修改attention
mask，让gist token后面的token只能看到gist token，而看不到原始的prompt
token；而gist token可以看到原始的prompt，相当于让gist
token成为把信息从prompt传递到answer的唯一桥梁，这就强制把prompt的信息都压缩到gist
token里。</p>
<p>各个模型结构下，attention mask的修改：</p>
<img src="/4519eadd/gist_mask.png" class title="prompt_compression">
<h2 id="实验-1">实验</h2>
<p>几个对照模型：</p>
<ul>
<li>positive control：不修改attention mask，等价于用原prompt微调<br>
</li>
<li>negative control：不输入task，相当于random gist token<br>
</li>
<li>discrete compression with
TF-IDF：用TF-IDF把prompt里的最关键词提取出来，比如Write a letter to your
boss asking for an increase in salary → salary，Given two integers, find
their average → average</li>
</ul>
<p>结合模型打分（GhatGPT）和Rouge-L，各个模型的效果：</p>
<img src="/4519eadd/gist_perf.png" class title="prompt_compression">
<p>GIST方法还是比较接近原prompt的效果的。</p>
<h1 id="小结-2">小结</h1>
<ul>
<li>prompt压缩在特定的场景下，还是有比较大的收益的；对于目前输入普遍很长的情况，如果考虑成本，这是一个值得考虑的方向<br>
</li>
<li>prompt压缩的评测还是更多关注怎么和原prompt输出对齐，这里要记得做归一化</li>
</ul>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a9c496e3.html">DeepSeek-V3细节探索</a><br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="https://www.linsight.cn/6c0f6207.html">Qwen2.5-1M技术解析</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="https://www.linsight.cn/2dee4921.html">LLM水印</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
<a href="https://www.linsight.cn/b4d047c1.html">DeepSeek-VL2的细节</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】Compressing Context to Enhance Inference Efficiency of Large
Language Models<br>
【2】Prompt Compression and Contrastive Conditioning for Controllability
and Toxicity Reduction in Language Models<br>
【3】Learning to compress prompts with gist tokens<br>
【4】LLMLingua: Compressing prompts for accelerated inference of large
language models</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>prompt压缩</tag>
      </tags>
  </entry>
  <entry>
    <title>prompt压缩(二)</title>
    <url>/ea2871bf.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>前文：<a href="https://mp.weixin.qq.com/s/7ugiuuhRaXV4P62C7GsA1w">prompt压缩(一)</a></p>
<h1 id="nano-capsulator">Nano-Capsulator</h1>
<p>Nano = NAtural laNguage
prOmpt，吐槽一下，这又是硬凑了一个缩写名字。</p>
<p>之前提到的soft
prompt需要针对生成模型进行一定的训练，无法在多个LLM之间通用；而类似selective
contexts这样，根据self-information或者ppl选择一些token删除的方法在压缩效果（保留原prompt效果）上略差一些，因此Nano-Capsulator就被设计出来，「在保留自然语言可读性的情况下压缩输入（获得Capsule
Prompt），既要保证效果，又能够在多个LLM之间通用」。</p>
<p>要达到这些目的，需要训练一个模型，Nano-Capsulator。Nano-Capsulator也是一个LLM，实验中用的是Vicuna-7B。推理的时候，Nano-Capsulator就用来根据long
prompt生成short prompt。</p>
<p>那么现在问题就是怎么训练Nano-Capsulator，让它能够在减少生成长度的同时，最大程度保留原prompt的语义信息。</p>
<p>一个直觉是，如果两个prompt的语义很相近，那么以这两个prompt为LLM的输入，输出的d-dimension
embedding应该也要相近。</p>
<p>具体来说，文中借助两个的instruction，replicating
instruction和summarizing instruction。replicating
instruction要求模型重复输出接收到的原始prompt，而summarizing
instruction就是用来压缩原prompt的指令。</p>
<p>假设原始prompt是<span class="math inline">\(K=\{k_{1},\cdots
k_{n}\}\)</span>，压缩后的prompt是<span class="math inline">\(C=\{c_{1},\cdots c_{m}\}\)</span>。replicating
instruction + 原prompt在Nano-Capsulator的输出embedding <span class="math inline">\(e_K\)</span> 就代表原prompt的语义，而summarizing
instruction + 原prompt在Nano-Capsulator的输出embedding <span class="math inline">\(e_C\)</span>
就代表压缩prompt的语义，那么二者的semantics loss就是</p>
<p><span class="math display">\[\mathcal{L}_{\mathrm{Comp}}=\mathbb{E}_C\left[D_{\mathrm{dist}}(e_K\parallel
e_C)\right]\quad(1)\]</span></p>
<p>distance理论上可以采样各种合理的计算方式，原文使用的是mean square
error。</p>
<p>到这里主要是为「原始prompt」和「压缩prompt」的相似度建了模，接下来还需要对「原始prompt+question的LLM生成结果」和「压缩prompt+question的LLM生成结果」的效果进行评估。</p>
<p>假设用于生成的LLM是 <span class="math inline">\(\mathcal{G}\)</span>，那「原始prompt+question的LLM生成结果」就是
<span class="math inline">\(\mathcal{G}\left(K_{i} \oplus
Q_{i}\right)\)</span>，「压缩prompt+question的LLM生成结果」就是 <span class="math inline">\(\mathcal{G}\left(C_{i} \oplus
Q_{i}\right)\)</span>。有了这两个生成结果，就可以使用一个reward function
<span class="math inline">\(I\)</span>，来评判使用压缩prompt后生成结果的效果。理论上
<span class="math inline">\(I\)</span> 可以是任意reward
function，原文中使用的是两个输出的hidden state embedding之间的mean
square error。能够使用mean square
error是基于能够拿到生成模型的参数的情况，如果使用的是API，那也可以使用其他类型的reward
function（注意这里的reward function概念和RL中的reward
function不太一样），比如可以用GPT4等模型对两个输出结果的差异度进行打分，总之就是要求两个输出结果的差异越小，<span class="math inline">\(\mathcal{R}_{cap }就应该越小\)</span>。</p>
<p>另外，不要忘记我们压缩prompt的一个主要目的是缩短输入，因此压缩prompt的长度不能太长。怎么把长度限制加入reward
function里呢？可以对 <span class="math inline">\(C\)</span>
做一个简单的cut-off，获得 <span class="math inline">\(\Phi(C)\)</span>。如果 <span class="math inline">\(C\)</span>
的长度超过了设置的长度，它就会被截断，那正常来说生成结果就不会好，获得的reward自然就低。</p>
<p>保持输出效果这个能力，原文叫prompt utility preservation。最终，prompt
utility preservation的loss完整版本是</p>
<p><span class="math display">\[\mathcal{R}_{cap
}=\mathbb{E}_{Q}\left[I\left\{\mathcal{G}\left(\Phi\left(C_{i}\right)
\oplus Q_{i}\right) \| \mathcal{G}\left(K_{i} \oplus
Q_{i}\right)\right\}\right]\quad(2)\]</span></p>
<p>最后结合semantics loss和prompt utility
preservation，获得最终的训练loss</p>
<p><span class="math display">\[\mathcal{L}_{Nano }=\mathcal{L}_{Comp
}\left(\cdot | \theta_{C}\right) * \mathcal{R}_{cap }\left(\cdot |
\theta^{*}\right)\]</span></p>
<p>这里用的是相乘。思路是，如果压缩prompt生成的结果不好，<span class="math inline">\(\mathcal{R}_{cap
}\)</span>就会大，这就给nano-capsulator的损失项<span class="math inline">\(\mathcal{L}_{Comp
}\)</span>施加一个大的惩罚系数。</p>
<p>Nano-Capsulator的整个训练流程如下：</p>
<img src="/ea2871bf/nano.png" class title="prompt_compression2">
<h1 id="llmlingua-2">LLMLingua-2</h1>
<p>LLMLingua-2的目标是做task-agnostic prompt
compression，也就是可以处理任意的prompt。</p>
<p>之前介绍的selective
contexts根据self-information选择token来删除，还有一些使用PPL来评估token的信息量的，这样做存在问题：（1）单向计算token的重要度并不是很可靠，缺乏完整的双向信息（2）这样的做法和prompt压缩的最终目标并不完全一致：prompt压缩是要在减少输入的情况下，保持模型的输出，仅仅在一定程度上保留输入token的信息并不能保证输出效果。</p>
<p>LLMLingua-2的方法是直接收集一批（原prompt，压缩prompt）的数据对，把原prompt中每个token是否保留当做一个二分类任务，训练一个Bert模型输出每个token「保留」的概率，选择「保留」概率高的token组合成压缩prompt。这个工作里两个主要事情就是（1）收集/处理数据（2）训练模型。</p>
<p>1、数据的收集和处理</p>
<p>收集数据主要是用GPT4：利用下面这个prompt要求GPT4对输入的prompt进行压缩：</p>
<img src="/ea2871bf/gua2_prompt.png" class title="prompt_compression2">
<p>这个prompt没有给GPT4施加太多的硬性规定，因为不同的文本在不同位置的token是否要压缩的情况都不一致，所以大思路上是都交给模型来处理，而只要求保留重要的文字，同时不要修改、打乱或者添加原本的文本（这些其实都是GPT4的幻觉问题）。</p>
<p>实际使用中发现GPT4会给很长的文本使用更大的压缩率，这其实是由于模型处理能力随着输入文本长度的增加而下降，这点和人一样，信息一多就overwhelm了。这样一来压缩的效果就不好。为了缓解这个问题，会按句子把原prompt切分成长度不超过512的段落，分别进行压缩。</p>
<p>获得GPT4的压缩结果之后，还需要进一步处理。主要是要解决由于压缩和GPT4指令遵循不好带来的几个问题：（1）Ambiguity（2）Variation（3）Reordering。这三个问题的示例和处理方法：</p>
<img src="/ea2871bf/gua2_algo.png" class title="prompt_compression2">
<p>为了评价GPT4的压缩文本的质量，引入两个指标：Variation Rate和Alignment
Gap。</p>
<p>Variation
Rate其实就是看压缩prompt里出现了多少原prompt没有的token/word。这些词肯定就是来自于GPT4的幻觉，变异的token/word越多，说明这条prompt的幻觉越严重，质量就越差。VR最高的5%数据就删掉不用了。</p>
<p>Alignment Gap稍微复杂一点，它由matching rate (MR)和 hitting rate (HR)
计算得到。MR是指原prompt中能在压缩prompt中找到对应token的比例，而HR是指压缩prompt能在原始prompt找到的token的比例。HR的理想值是1，因为一个好的压缩prompt所有token都是来自原始prompt的。AG
= HR -
MR，如果AG大，说明MR小，太小的MR说明压缩prompt所包含的原始prompt的token比例不高，可能无法很好地表征原始prompt，因此AG越小越好。实践中把AG最高的10%数据去掉了。</p>
<p>2、压缩模型</p>
<p>有了（原prompt，压缩prompt）数据之后，就可以用Bert模型训练每个token的二分类了，这和用Bert做NER类似。</p>
<p>推理的时候，每个原始prompt的token都能获得一个得分，表示模型认为这个token需要保留的概率。一般来说我们有一个目标的压缩率τ（比如0.2），如果原prompt有N个token，那么我们就需要保留τN个token组成压缩prompt。因此我们就根据Bert模型输出，选择概率最高的τN个token，保持原顺序组成压缩prompt。</p>
<h1 id="cpc">CPC</h1>
<p>CPC = Context-aware Prompt Compression。</p>
<p>CPC的思路和LLMLingua-2有点相似，都是构建一个数据集，标出一个输入中哪些内容重要，哪些内容不重要，然后用一个模型来学习这种分布。在推理的时候就只保留那些重要的内容。不过和LLMLingua-2不同的是，CPC不以token或者word为单位，而以句子为单位，这样可读性和连续性更好一些。另外就是构建数据集和训练模型的细节的不同。</p>
<p>1、构建训练数据集</p>
<p>CPC构建的训练数据集叫Context-aware Question-Relevance (CQR)
。CQR数据集中每条样本是包含（上下文，问题，正例(positive)，负例(negative)）的四元组。其中上下文+问题其实就是完整的原始输入，而正例和负例都是来自上下文的句子。正例就是重要的要保留的句子，而负例就是可以删去的句子。</p>
<p>压缩模型训练的时候就要学习根据上下文和问题来区分正例和负例句子。</p>
<p>要构建这样的数据集，首先从WikiText的文档开始，这些文档都是比较长的。第一步就是要从这些上下文里获取一些positive的句子。</p>
<p>positive的句子至少得是一致连贯的：如果一个句子英文单词的数量达到一定的比例，并且都由ASCII字符组成，那么就认为这个句子是一直连贯的。把符合要求的句子和上下文一起，用下面这个prompt生成QA对：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Prompt <span class="number">1</span> (Question Prompt):</span><br><span class="line">Here <span class="keyword">is</span> a text to consider: TEXT: <span class="string">&quot;text&quot;</span></span><br><span class="line">Read the sentence <span class="keyword">in</span> double brackets,</span><br><span class="line">namely, [[sentence]].</span><br><span class="line">Ask questions to this sentence, <span class="keyword">and</span> make</span><br><span class="line">sure the question <span class="keyword">is</span> <span class="keyword">not</span> answerable <span class="keyword">from</span></span><br><span class="line">this sentence alone without knowing the</span><br><span class="line">context.</span><br><span class="line">Reply <span class="keyword">in</span> this <span class="built_in">format</span>:</span><br><span class="line">Q: &#123;question <span class="number">1</span>&#125;</span><br><span class="line">A: &#123;answer <span class="number">1</span>&#125;</span><br><span class="line">Q: &#123;question <span class="number">2</span>&#125;</span><br><span class="line">A: &#123;answer <span class="number">2</span>&#125;</span><br></pre></td></tr></table></figure>
<p>常有的一种情况是，一个句子虽然不能直接回答问题，但是包含了回答问题所需要的元素，因此压缩模型需要关注这种情况：只有这一个positive不足以回答的问题。因此用下面这个prompt来检验生成的QA是否符合这种情况：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Prompt <span class="number">2</span> (Verification Prompt):</span><br><span class="line">You are given a piece of text, a question</span><br><span class="line"><span class="keyword">and</span> an answer. Verify whether it <span class="keyword">is</span></span><br><span class="line">possible to derive such an answer by</span><br><span class="line">considering only the given piece of text</span><br><span class="line">(you should rely only on the piece of</span><br><span class="line">text). Think step by step <span class="keyword">and</span> finish</span><br><span class="line">your thoughts <span class="keyword">with</span> one word: <span class="string">&quot;Yes&quot;</span> <span class="keyword">or</span></span><br><span class="line"><span class="string">&quot;No&quot;</span>. Answer <span class="string">&quot;Yes&quot;</span> <span class="keyword">if</span> <span class="keyword">and</span> only <span class="keyword">if</span> ALL the</span><br><span class="line">necessary information <span class="keyword">is</span> contained <span class="keyword">in</span> the</span><br><span class="line">text. If anything <span class="keyword">is</span> missing, then state</span><br><span class="line">what <span class="keyword">is</span> missing <span class="keyword">and</span> answer <span class="string">&quot;No&quot;</span>. Answer</span><br><span class="line"><span class="string">&quot;Yes&quot;</span> ONLY <span class="keyword">if</span> there <span class="keyword">is</span> no such information</span><br><span class="line"><span class="keyword">in</span> the answer that <span class="keyword">is</span> missing <span class="keyword">in</span> the text.</span><br><span class="line">Otherwise, answer <span class="string">&quot;No&quot;</span>!!</span><br><span class="line">&#123;A demonstration&#125;</span><br><span class="line">Text: &#123;context sentence&#125;</span><br><span class="line">Question: &#123;question&#125;</span><br><span class="line">Answer: &#123;answer&#125;</span><br><span class="line">Verification result: Yes/No</span><br></pre></td></tr></table></figure>
<p>只有验证结果为No的QA会保留。</p>
<p>下一步就是获取negative句子。获取negative句子有两步。第一步先用一个sentence
embedding模型，计算正例句子和问题Q的相似度η，然后计算上下文中所有其他句子和问题Q的相似度，对于相似度小于η的句子，都放到负例候选集里。</p>
<p>第二步会对负例候选集里的所有句子做一个校验。校验方法就是获取有这个句子和没有这个句子这两种情况下，模型对问题Q回答的情况。如果两种情况下答案A的KL散度大于阈值，则说明这个句子对答案还是有影响的，否则就说明它确实是一个不重要的句子（对于回答这个问题来说）。</p>
<p>收据收集的整体流程：</p>
<img src="/ea2871bf/cpc_datapipeline.png" class title="prompt_compression2">
<p>2、训练</p>
<p>接下来就要训练一个模型，用于在推理的时候判断每个句子是否要保留。这里用的是对比学习的方法来训练。具体来说，计算给定上下文C下，正例P和问题Q的相似度，以及负例N和问题Q的相似度，然后最大化正例的相似度，最小化负例的相似度：</p>
<p><span class="math display">\[Sim_{P}=\exp (cosine(\xi_{Q_{b}},
\xi_{(P_{b}, C_{b})}))\]</span></p>
<p><span class="math display">\[Sim_{N}=\exp (cosine(\xi_{Q_{b}},
Neg_{(b, ext)}))\]</span></p>
<p><span class="math display">\[\mathcal{L}_{SC}=-log \frac{exp
\left(Sim_{P}\right)}{exp \left(Sim_{P}\right)+\sum exp
\left(Sim_{N}\right)}\]</span></p>
<p>3、推理</p>
<p>推理的时候，把输入拆分成句子，计算每个句子和Q的相似度，并按照需要的压缩率保留相似度最高的句子。</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a9c496e3.html">DeepSeek-V3细节探索</a><br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="https://www.linsight.cn/6c0f6207.html">Qwen2.5-1M技术解析</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="https://www.linsight.cn/4519eadd.html">prompt压缩(一)</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="https://www.linsight.cn/2dee4921.html">LLM水印</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
<a href="https://www.linsight.cn/b4d047c1.html">DeepSeek-VL2的细节</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】Learning to Compress Prompt in Natural Language Formats<br>
【2】LLMLingua-2: Data Distillation for Efficient and Faithful
Task-Agnostic Prompt Compression<br>
【3】Prompt Compression with Context-Aware Sentence Encoding for Fast
and Improved LLM Inference</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>prompt压缩</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM训练框架：从优化器和精度讲到ZeRO</title>
    <url>/fe0adaa5.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>这篇文章主要从一个搞数据和训练策略的LLM算法工程师角度总结一下之前用到的训练框架相关知识，包括优化器、精度和混合精度训练和DP、ZeRO的相关内容。</p>
<h1 id="optimizer">Optimizer</h1>
<p>现在模型的优化器就是AdamW。虽然这几年也试过Lion和Muon等一些新兴的optimizer，不过实践中最稳当的暂时还是AdamW。</p>
<h2 id="从sgd到adamw">从SGD到AdamW</h2>
<p>先复习下从SGD到AdamW这些个优化器。</p>
<ol type="1">
<li>SGD</li>
</ol>
<p>SGD的更新公式：</p>
<p><span class="math display">\[
θ_{t+1} = θ_t - η \cdot g_t
\]</span></p>
<ul>
<li><span class="math inline">\(θ_t\)</span>：模型参数<br>
</li>
<li><span class="math inline">\(η\)</span>：learning rate<br>
</li>
<li><span class="math inline">\(g_t\)</span>：当前梯度（<span class="math inline">\(\nabla_θ L(θ_t)\)</span>）</li>
</ul>
<p>SGD只依赖当前最新计算出的梯度，直接更新模型的参数值。</p>
<ol start="2" type="1">
<li>Momentum SGD</li>
</ol>
<p>公式：</p>
<p><span class="math display">\[
\begin{aligned}
v_t &amp;= γ \cdot v_{t-1} + g_t \\
θ_{t+1} &amp;= θ_t - η \cdot v_t
\end{aligned}
\]</span></p>
<ul>
<li><span class="math inline">\(v_t\)</span>：包含动量项的梯度（加权移动平均的累积梯度）<br>
</li>
<li><span class="math inline">\(γ\)</span>：动量系数/加权系数（比如0.9，越大表示梯度更新越慢，设为0就等于SGD了）</li>
</ul>
<p>模型在训练初期，轮次之间的梯度变化比较大，梯度甚至可能发生180°大调头的情况，导致震荡，所以SGD不容易收敛。Momentum
SGD通过累积历史的梯度值，减少震荡，从而稳定训练，加速收敛。</p>
<ol start="3" type="1">
<li>AdaGrad</li>
</ol>
<p>AdaGrad尝试让不同的参数有自己的学习率，并且可以自适应调整。</p>
<p>公式：</p>
<p><span class="math display">\[
\begin{aligned}
G_t &amp;= G_{t-1} + g_t^2 \\
θ_{t+1} &amp;= θ_t - \frac{η}{\sqrt{G_t} + ϵ} g_t
\end{aligned}
\]</span></p>
<ul>
<li><span class="math inline">\(G_t\)</span>：梯度平方的累积值<br>
</li>
<li><span class="math inline">\(ϵ\)</span>：防止除零（如1e-8）</li>
</ul>
<p>如果一个参数的更新速度比较快，那么对应的G就会比较大，那么相应的学习率也会减小；反之则学习率会相对较大。</p>
<ol start="4" type="1">
<li>RMSProp</li>
</ol>
<p>AdaGrad中因为会累积所有历史梯度平方值，这样到后期每个参数的学习率都衰减到比较小，如果训练的step比较多，到后面就效率太低了。</p>
<p>RMSProp比AdaGrad多使用一个加权移动平均。</p>
<p>公式：</p>
<p><span class="math display">\[
\begin{aligned}
v_t &amp;= β \cdot v_{t-1} + (1-β) \cdot g_t^2 \\
θ_{t+1} &amp;= θ_t - \frac{η}{\sqrt{v_t + ϵ}} g_t
\end{aligned}
\]</span></p>
<ul>
<li><span class="math inline">\(v_t\)</span>：梯度平方的指数移动平均<br>
</li>
<li><span class="math inline">\(β\)</span>：加权移动平均衰减率（通常0.9）</li>
</ul>
<ol start="5" type="1">
<li>Adam</li>
</ol>
<p>把RMSProp和Momentum SGD的改进结合起来就是Adam了。</p>
<p>Adam = Adaptive + Momentum。</p>
<p>公式：</p>
<p><span class="math display">\[
\begin{aligned}
m_t &amp;= β_1 \cdot m_{t-1} + (1-β_1) \cdot g_t \\
v_t &amp;= β_2 \cdot v_{t-1} + (1-β_2) \cdot g_t^2 \\
\hat{m}_t &amp;= \frac{m_t}{1-β_1^t}, \quad \hat{v}_t =
\frac{v_t}{1-β_2^t} \\
θ_{t+1} &amp;= θ_t - \frac{η}{\sqrt{\hat{v}_t} + ϵ} \hat{m}_t
\end{aligned}
\]</span></p>
<ul>
<li><span class="math inline">\(m_t\)</span>：一阶动量，控制当前梯度的方向，初始状态m=0<br>
</li>
<li><span class="math inline">\(v_t\)</span>：二阶动量，控制当前梯度的大小，初始状态v=0<br>
</li>
<li><span class="math inline">\(β_1,β_2\)</span>：衰减率（通常<span class="math inline">\(β_1=0.9,β_2=0.999\)</span>）</li>
</ul>
<p>本来更新的时候直接使用</p>
<p><span class="math display">\[
θ_{t+1} = θ_t - \frac{η}{\sqrt{v_t} + ϵ} m_t
\]</span></p>
<p>就可以了，为什么还要给v和m做一个缩放修正呢？因为在一开始的时候，历史的动量值都是0，这样导致训练初期更新的梯度太小，因此在前期给缩放一下，基本上跑个几百几千步，这个这个缩放基本就趋近于1了。</p>
<ol start="6" type="1">
<li>AdamW</li>
</ol>
<p>为了防止过拟合，提高泛化性，模型训练的时候可以加入L2 norm。一般来说L2
norm是直接加在训练loss上的。</p>
<p>L2 norm项是这样的：</p>
<p><span class="math display">\[\frac{\lambda}{2}
\|\theta\|^2\]</span></p>
<p>直接加到训练loss上：</p>
<p><span class="math display">\[L&#39;(\theta) = L(\theta) +
\frac{\lambda}{2} \|\theta\|^2\]</span></p>
<p>那么梯度就有：</p>
<p><span class="math display">\[g_t=\nabla_\theta
L(\theta_{t-1})+\lambda\theta_{t-1}\]</span></p>
<p>Adam在对梯度进行缩放的时候，L2
norm的衰减项也会被缩放，因此就达不到本来想要的效果了。</p>
<p>所以就有了AdamW的改进，让L2 norm的正则化能力可以正常实现。</p>
<p>AdamW不把L2 norm加到loss项中，而是直接把对应梯度加到参数更新中。</p>
<p>公式：</p>
<p><span class="math display">\[
θ_{t+1} = θ_t - \frac{η}{\sqrt{\hat{v}_t} + ϵ} \odot \hat{m}_t - λ \cdot
θ_t
\]</span></p>
<ul>
<li><span class="math inline">\(λ\)</span>：L2 norm的权重衰减系数</li>
</ul>
<h2 id="adamw的显存需求">AdamW的显存需求</h2>
<p>AdamW训练的时候，除了模型参数，还需要维护一阶动量和二阶动量。</p>
<p>在全部使用fp32的情况下，假设模型的总参数量为<span class="math inline">\(\Phi\)</span>，那么模型本身参数所需的显存就是<span class="math inline">\(4\Phi\)</span>。</p>
<p>而AdamW维护的一阶动量和二阶动量的参数则是<span class="math inline">\(4\Phi+4\Phi=8\Phi\)</span>。</p>
<p>此外还有梯度值，每个模型参数有一个梯度，那么梯度所需的量也是<span class="math inline">\(4\Phi\)</span>。</p>
<p>那么模型参数 + 优化器参数 + 梯度总共就是<span class="math inline">\(16\Phi\)</span>的显存需求。</p>
<p>最后还有中间激活值，激活值的量和模型结构有关，对于transformer也和输入长度有关，再加上现在还有gradient
checkpoint等做法，所以激活值就得具体情况具体分析了。</p>
<h1 id="精度">精度</h1>
<p>说到LLM训练，就离不开训练精度的事。</p>
<h2 id="以fp32为例说明">以FP32为例说明</h2>
<p>先从FP32说起。FP32的二进制结构分为三部分：</p>
<ul>
<li>符号位（S，Sign）：1位，0表示正数，1表示负数。<br>
</li>
<li>指数位（E，Exponent）：8位，存储偏移后的指数值（为了能够表达正值和负值，加上了127的偏移量，实际指数为E
- 127）。<br>
</li>
<li>尾数位（M，Mantissa）：23位，存储规范化后的二进制小数部分（隐含前导1.）。</li>
</ul>
<p>十进制和FP32转换公式：</p>
<p><span class="math display">\[
(-1)^S \times 1.M \times 2^{E-127}
\]</span></p>
<p>举几个例子看看二进制和十进制的转换。</p>
<p>示例1. 十进制 → FP32（以9.625为例）</p>
<p>step 1：十进制转二进制</p>
<ul>
<li><strong>整数部分</strong>：<code>9</code> →
<code>1001</code>（二进制）。<br>
</li>
<li><strong>小数部分</strong>：<code>0.625</code> → 连续乘2取整：
<ul>
<li><code>0.625 × 2 = 1.25</code> →
取<code>1</code>，剩余<code>0.25</code><br>
</li>
<li><code>0.25 × 2 = 0.5</code> →
取<code>0</code>，剩余<code>0.5</code><br>
</li>
<li><code>0.5 × 2 = 1.0</code> →
取<code>1</code>，剩余<code>0</code><br>
</li>
<li>结果：<code>0.101</code>（二进制）。<br>
</li>
</ul></li>
<li><strong>合并</strong>：<code>9.625</code> →
<code>1001.101</code>。</li>
</ul>
<p>step 2：规范化科学计数法</p>
<ul>
<li><code>1001.101</code> →
<code>1.001101 × 2^3</code>（左移3位）。对于二进制来说，整数位一定是1。</li>
</ul>
<p>step 3：填充FP32三部分</p>
<ul>
<li><strong>符号位</strong>：<code>0</code>（正数）。<br>
</li>
<li><strong>指数位</strong>：<code>3 + 127 = 130</code> →
<code>10000010</code>（二进制）。<br>
</li>
<li><strong>尾数位</strong>：<code>001101</code> + 补零至23位 →
<code>00110100000000000000000</code>。</li>
</ul>
<p>最终9.625的FP32表示：</p>
<p><code>0 10000010 00110100000000000000000</code></p>
<p>（验证工具：<a href="https://www.h-schmidt.net/FloatConverter/IEEE754.html">IEEE-754
Converter,
https://www.h-schmidt.net/FloatConverter/IEEE754.html</a>）</p>
<p>示例2：FP32 → 十进制（反向解析出十进制）</p>
<p>二进制为：<code>0 10000010 00110100000000000000000</code></p>
<ul>
<li><strong>符号位</strong>：<code>0</code> → 正数。<br>
</li>
<li><strong>指数位</strong>：<code>10000010</code> →
十进制<code>130</code> → 实际指数<code>130 - 127 = 3</code>。<br>
</li>
<li><strong>尾数位</strong>：<code>001101...</code> →
隐含<code>1.</code> → <code>1.001101</code>（二进制）。</li>
</ul>
<p>计算十进制值：</p>
<p>step 1：将<code>1.001101</code>转为十进制：<br>
- <code>1.001101</code> = <span class="math inline">\(1 + 0 \times
2^{-1} + 0 \times 2^{-2} + 1 \times 2^{-3} + 1 \times 2^{-4} + 0 \times
2^{-5} + 1 \times 2^{-6}\)</span> = <span class="math inline">\(1 +
0.125 + 0.0625 + 0.015625\)</span> ≈ <code>1.203125</code>。<br>
step 2：乘以指数部分：<code>1.203125 × 2^3 = 9.625</code>。</p>
<ul>
<li><p><strong>特殊值处理</strong>：</p>
<ul>
<li>指数全<code>0</code>且尾数非零：非规格化数（极小值）。<br>
</li>
<li>指数全<code>1</code>且尾数全<code>0</code>：表示无穷大（<code>±∞</code>）。<br>
</li>
<li>指数全<code>1</code>且尾数非零：<code>NaN</code>（非数字）。</li>
</ul></li>
<li><p><strong>精度限制</strong>：某些十进制数（如<code>0.3</code>）无法精确表示为FP32，会存在舍入误差。</p></li>
</ul>
<h2 id="其他常用精度">其他常用精度</h2>
<p>目前LLM训练的除了FP32，还有FP16、BF16，以及更新的FP8。</p>
<p>这几个的对比：</p>
<table style="width:100%;">
<colgroup>
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th>格式</th>
<th>符号位</th>
<th>指数位</th>
<th>尾数位</th>
<th>总位数</th>
<th>数值范围(近似)</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>FP32</td>
<td>1</td>
<td>8</td>
<td>23</td>
<td>32</td>
<td>±1.2×10⁻³⁸ ~ ±3.4×10³⁸</td>
<td>高精度（约7位有效数字），通用计算标准，适合训练但资源消耗大。</td>
</tr>
<tr class="even">
<td>FP16</td>
<td>1</td>
<td>5</td>
<td>10</td>
<td>16</td>
<td>±6.1×10⁻⁵ ~ ±6.6×10⁴</td>
<td>内存占用减半，速度快但易溢出/下溢，需混合精度训练。</td>
</tr>
<tr class="odd">
<td>BF16</td>
<td>1</td>
<td>8</td>
<td>7</td>
<td>16</td>
<td>±1.2×10⁻³⁸ ~ ±3.4×10³⁸</td>
<td>指数范围同FP32，训练稳定但精度低（约2位有效数字），适合大模型。</td>
</tr>
<tr class="even">
<td>FP8(E4M3)</td>
<td>1</td>
<td>4</td>
<td>3</td>
<td>8</td>
<td>±1.56×10⁻⁵ ~ ±448</td>
<td>内存占用极低，适合推理；E4M3侧重精度，范围较小。</td>
</tr>
<tr class="odd">
<td>FP8(E5M2)</td>
<td>1</td>
<td>5</td>
<td>2</td>
<td>8</td>
<td>±3.9×10⁻⁸ ~ ±57344</td>
<td>E5M2侧重范围，精度更低，适合大动态范围计算。</td>
</tr>
</tbody>
</table>
<p>目前最常用的还是FP16和BF16（FP8我自己还没怎么用，先挖个坑，以后用熟了FP8再来填）。这俩的对比：</p>
<ul>
<li><strong>指数位</strong>：BF16与FP32相同（8位），FP16仅5位，因此表示范围小，更易溢出；<br>
</li>
<li><strong>尾数位</strong>：FP32（23位）&gt; FP16（10位）&gt;
BF16（7位）&gt; FP8（3/2位），精度依次降低。<br>
</li>
<li><strong>应用场景</strong>：FP32用于高精度训练，FP16/BF16都可以用于混合精度训练，FP8用于端侧设备推理。</li>
</ul>
<p>直观上，BF16的精度大概是在0.01到0.001之间，而BF16的精度是在0.001到0.0001之间。也就是说，如果一次梯度更新小于这个值，那么参数很可能没法正确地变化。</p>
<h1 id="混合精度训练">混合精度训练</h1>
<p>混合精度训练时减少显存使用，提升训练速度的方法。</p>
<p>为什么用混合精度训练，不直接使用低精度的格式进行训练？从前面精度的表格可以看到，无论是FP16还是BF16，要么在精度上有损失，要么在表达范围上有限制，因此直接用低精度格式训练，可能会在需要高精度或者大范围的部分导致不稳定。因此混合精度方案在大部分计算使用半精度的同时，用FP32对关键部分进行备份，在速度、显存和稳定性间取得平衡。</p>
<h2 id="显存">显存</h2>
<p>AdamW的单精度和半精度的混合精度训练如下（图上是FP16，也可以换成BF16）：</p>
<img src="/fe0adaa5/mix_precision_fp16.png" class title="混合精度训练">
<p>输入是FP16，前向计算激活值是FP16，loss值是FP32的，反向计算的值和梯度是FP16，AdamW的一阶和二阶动量是FP32，而AdamW更新模型参数权重用的是FP32，而在进行前后向计算的时候，模型参数用的是FP16的版本。</p>
<p>算一下显存：</p>
<p>模型参数：一份单精度一份半精度，总共就是<span class="math inline">\(2\Phi+4\Phi=6\Phi\)</span>。</p>
<p>优化器参数：每个参数有单精度的一阶动量和二阶动量，总共就是<span class="math inline">\(8\Phi\)</span>。</p>
<p>梯度：每个参数有半精度的梯度，<span class="math inline">\(2\Phi\)</span>。</p>
<p>那么总共就是<span class="math inline">\(16\Phi\)</span>的显存消耗。</p>
<p>从模型参数+优化器参数+梯度的显存消耗上看，单精度训练和混合精度（FP32
+ FP16/BF16）的显存消耗量是一样。但是，混合精度在效率上的收益有：</p>
<ul>
<li>有硬件支持下，半精度的计算更快，因此整体的计算速度更快。<br>
</li>
<li>激活值所需的显存减少一半，从而可以使用更大的batch。<br>
</li>
<li>一些原来单卡放不下的，现在能放下了，不用做张量并行或者流水并行。</li>
</ul>
<h2 id="训练">训练</h2>
<p>前面说到直接用半精度进行训练会有问题，那么混合精度训练具体是怎么解决这些问题的。</p>
<p>首先，半精度的精度不足，因此混合精度中，AdamW维护了一份FP32的模型权重，这个是真正用于更新模型的数据，这样可以保持较小的梯度更新也不会被舍弃。每次更新完之后，再把获得的FP32参数转成FP16，用于前后向计算。</p>
<p>另外，由于半精度值的精度问题，较小的梯度值可能直接变成0了，这样就导致没法训练参数了。那么一个解决方法就是像上面的图中那样，给loss值做一个scaling，变大一些，尽量远离太小的值。由于loss值变大，会导致梯度值也变大相应倍数，因此在更新完模型参数值之后，要做一个逆scaling，把值变回去。</p>
<img src="/fe0adaa5/loss_scaling.png" class title="混合精度训练">
<p>另外，还有一招：使用FP16进行乘法和存储，只使用FP32进行加法操作，避免累加误差。因为加法的误差会一直累积，因此用单精度计算。</p>
<h1 id="data-parallel">Data Parallel</h1>
<p>模型大，数据多，难免就需要分布式计算。其中，最常用的就是数据并行。其实我们训练百亿以下的模型，基本上都是只用数据并行。</p>
<p>使用最朴素的数据并行，每个GPU会维护一套完整的模型参数 + 优化器参数 +
梯度。每次更新，每个GPU用不同的数据「单独」进行训练，获取梯度，然后所有GPU会同步各自获得的梯度，计算个平均值，然后更新参数。每轮更新过后，模型参数会统一，而优化器状态则每个GPU有各自的版本（因此保存训练checkpoint的时候会有大量的优化器状态值）。</p>
<p>可以看到，每次更新时，各个GPU需要同步梯度，这就涉及到大量的卡间通讯，甚至节点间通讯。比如128卡训练模型，那么naive的数据同步方式就是两两之间都要进行数据传输和接收；那么训练一个14B模型，在用半精度的梯度的情况下，每张卡要发送127
* 28G = 3556G数据，同时要接收127 * 28G =
3556G的数据，而且随着集群的变大，这个数值还会增大。就算是A100，卡间带宽也只有2TB/s，那同步一次就是的1s多，这期间所有卡都得停下计算，等通讯完成。多节点之间的带宽更小，那GPU的利用率就更低了。</p>
<p>这也太低效了，因此实际上就不是这样同步数据的，而是用到了Ring
AllReduce的梯度同步算法。</p>
<h2 id="ring-allreduce算法">Ring AllReduce算法</h2>
<p>顾名思义，Ring
AllReduce把各个GPU组成一个ring，以ring的形式进行通讯，以减少通讯量。</p>
<img src="/fe0adaa5/ring.jpg" class title="混合精度训练">
<p>allreduce同步梯度数据的过程主要包含reduce-scatter和all-gather两个操作。</p>
<ol type="1">
<li>reduce-scatter</li>
</ol>
<p>假设一共有5个GPU，要同步梯度。那么把梯度数据均匀划分成A、B、C、D、E五块。</p>
<p>初始状态下，每个GPU持有的数据如下：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPU0</td>
<td>a0</td>
<td>b0</td>
<td>c0</td>
<td>d0</td>
<td>e0</td>
</tr>
<tr class="even">
<td>GPU1</td>
<td>a1</td>
<td>b1</td>
<td>c1</td>
<td>d1</td>
<td>e1</td>
</tr>
<tr class="odd">
<td>GPU2</td>
<td>a2</td>
<td>b2</td>
<td>c2</td>
<td>d2</td>
<td>e2</td>
</tr>
<tr class="even">
<td>GPU3</td>
<td>a3</td>
<td>b3</td>
<td>c3</td>
<td>d3</td>
<td>e3</td>
</tr>
<tr class="odd">
<td>GPU4</td>
<td>a4</td>
<td>b4</td>
<td>c4</td>
<td>d4</td>
<td>e4</td>
</tr>
</tbody>
</table>
<p>reduce-scatter的操作，每个GPU会发送自己持有的A、B、C、D、E中的其中一块数据，同时接收和自己发送的数据不同块的一块数据。</p>
<p>比如在这个例子中，GPU0发送a0，并接收e4，GPU1发送b1，并接收a0，以此类推。</p>
<p>第一次reduce-scatter操作之后：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPU0</td>
<td>a0</td>
<td>b0</td>
<td>c0</td>
<td>d0</td>
<td>e4+e0</td>
</tr>
<tr class="even">
<td>GPU1</td>
<td>a0+a1</td>
<td>b1</td>
<td>c1</td>
<td>d1</td>
<td>e1</td>
</tr>
<tr class="odd">
<td>GPU2</td>
<td>a2</td>
<td>b1+b2</td>
<td>c2</td>
<td>d2</td>
<td>e2</td>
</tr>
<tr class="even">
<td>GPU3</td>
<td>a3</td>
<td>b3</td>
<td>c2+c3</td>
<td>d3</td>
<td>e3</td>
</tr>
<tr class="odd">
<td>GPU4</td>
<td>a4</td>
<td>b4</td>
<td>c4</td>
<td>d3+d4</td>
<td>e4</td>
</tr>
</tbody>
</table>
<p>第二次reduce-scatter操作之后：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPU0</td>
<td>a0</td>
<td>b0</td>
<td>c0</td>
<td>d3+d4+d0</td>
<td>e4+e0</td>
</tr>
<tr class="even">
<td>GPU1</td>
<td>a0+a1</td>
<td>b1</td>
<td>c1</td>
<td>d1</td>
<td>e4+e0+e1</td>
</tr>
<tr class="odd">
<td>GPU2</td>
<td>a0+a1+a2</td>
<td>b1+b2</td>
<td>c2</td>
<td>d2</td>
<td>e2</td>
</tr>
<tr class="even">
<td>GPU3</td>
<td>a3</td>
<td>b1+b2+b3</td>
<td>c2+c3</td>
<td>d3</td>
<td>e3</td>
</tr>
<tr class="odd">
<td>GPU4</td>
<td>a4</td>
<td>b4</td>
<td>c2+c3+c4</td>
<td>d3+d4</td>
<td>e4</td>
</tr>
</tbody>
</table>
<p>第三次reduce-scatter操作之后：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPU0</td>
<td>a0</td>
<td>b0</td>
<td>c2+c3+c4+c0</td>
<td>d3+d4+d0</td>
<td>e4+e0</td>
</tr>
<tr class="even">
<td>GPU1</td>
<td>a0+a1</td>
<td>b1</td>
<td>c1</td>
<td>d3+d4+d0+d1</td>
<td>e4+e0+e1</td>
</tr>
<tr class="odd">
<td>GPU2</td>
<td>a0+a1+a2</td>
<td>b1+b2</td>
<td>c2</td>
<td>d2</td>
<td>e4+e0+e1+e2</td>
</tr>
<tr class="even">
<td>GPU3</td>
<td>a0+a1+a2+a3</td>
<td>b1+b2+b3</td>
<td>c2+c3</td>
<td>d3</td>
<td>e3</td>
</tr>
<tr class="odd">
<td>GPU4</td>
<td>a4</td>
<td>b1+b2+b3+b4</td>
<td>c2+c3+c4</td>
<td>d3+d4</td>
<td>e4</td>
</tr>
</tbody>
</table>
<p>第四次reduce-scatter操作之后：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPU0</td>
<td>a0</td>
<td>b1+b2+b3+b4+b0</td>
<td>c2+c3+c4+c0</td>
<td>d3+d4+d0</td>
<td>e4+e0</td>
</tr>
<tr class="even">
<td>GPU1</td>
<td>a0+a1</td>
<td>b1</td>
<td>c2+c3+c4+c0+c1</td>
<td>d3+d4+d0+d1</td>
<td>e4+e0+e1</td>
</tr>
<tr class="odd">
<td>GPU2</td>
<td>a0+a1+a2</td>
<td>b1+b2</td>
<td>c2</td>
<td>d3+d4+d0+d1+d2</td>
<td>e4+e0+e1+e2</td>
</tr>
<tr class="even">
<td>GPU3</td>
<td>a0+a1+a2+a3</td>
<td>b1+b2+b3</td>
<td>c2+c3</td>
<td>d3</td>
<td>e4+e0+e1+e2+e3</td>
</tr>
<tr class="odd">
<td>GPU4</td>
<td>a0+a1+a2+a3+a4</td>
<td>b1+b2+b3+b4</td>
<td>c2+c3+c4</td>
<td>d3+d4</td>
<td>e4</td>
</tr>
</tbody>
</table>
<p>假设共有N个GPU，经过N-1次操作之后，每个GPU上，都有1/N块数据是同步了所有GPU数据的。</p>
<p>在这个例子中，GPU0的B块是包含了完整的5个GPU的数据的，而GPU1则是C块是完整的，以此类推。</p>
<p>接下来，就需要用all-gather把每个GPU上这份同步了所有GPU数据的块传播给其他GPU。</p>
<ol start="2" type="1">
<li>all-gather</li>
</ol>
<p>其实all-gather和reduce-scatter的操作是很类似的，只不过reduce-scatter是相加/取平均，而all-gather是直接覆盖数据。</p>
<p>all-gather第一次操作后：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPU0</td>
<td>a0+a1+a2+a3+a4</td>
<td>b1+b2+b3+b4+b0</td>
<td>c2+c3+c4+c0</td>
<td>d3+d4+d0</td>
<td>e4+e0</td>
</tr>
<tr class="even">
<td>GPU1</td>
<td>a0+a1</td>
<td>b1+b2+b3+b4+b0</td>
<td>c2+c3+c4+c0+c1</td>
<td>d3+d4+d0+d1</td>
<td>e4+e0+e1</td>
</tr>
<tr class="odd">
<td>GPU2</td>
<td>a0+a1+a2</td>
<td>b1+b2</td>
<td>c2+c3+c4+c0+c1</td>
<td>d3+d4+d0+d1+d2</td>
<td>e4+e0+e1+e2</td>
</tr>
<tr class="even">
<td>GPU3</td>
<td>a0+a1+a2+a3</td>
<td>b1+b2+b3</td>
<td>c2+c3</td>
<td>d3+d4+d0+d1+d2</td>
<td>e4+e0+e1+e2+e3</td>
</tr>
<tr class="odd">
<td>GPU4</td>
<td>a0+a1+a2+a3+a4</td>
<td>b1+b2+b3+b4</td>
<td>c2+c3+c4</td>
<td>d3+d4</td>
<td>e4+e0+e1+e2+e3</td>
</tr>
</tbody>
</table>
<p>...</p>
<p>以此类推，最后得到</p>
<table style="width:100%;">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPU0</td>
<td>a0+a1+a2+a3+a4</td>
<td>b1+b2+b3+b4+b0</td>
<td>c2+c3+c4+c0+c1</td>
<td>d3+d4+d0+d1+d2</td>
<td>e4+e0+e1+e2+e3</td>
</tr>
<tr class="even">
<td>GPU1</td>
<td>a0+a1+a2+a3+a4</td>
<td>b1+b2+b3+b4+b0</td>
<td>c2+c3+c4+c0+c1</td>
<td>d3+d4+d0+d1+d2</td>
<td>e4+e0+e1+e2+e3</td>
</tr>
<tr class="odd">
<td>GPU2</td>
<td>a0+a1+a2+a3+a4</td>
<td>b1+b2+b3+b4+b0</td>
<td>c2+c3+c4+c0+c1</td>
<td>d3+d4+d0+d1+d2</td>
<td>e4+e0+e1+e2+e3</td>
</tr>
<tr class="even">
<td>GPU3</td>
<td>a0+a1+a2+a3+a4</td>
<td>b1+b2+b3+b4+b0</td>
<td>c2+c3+c4+c0+c1</td>
<td>d3+d4+d0+d1+d2</td>
<td>e4+e0+e1+e2+e3</td>
</tr>
<tr class="odd">
<td>GPU4</td>
<td>a0+a1+a2+a3+a4</td>
<td>b1+b2+b3+b4+b0</td>
<td>c2+c3+c4+c0+c1</td>
<td>d3+d4+d0+d1+d2</td>
<td>e4+e0+e1+e2+e3</td>
</tr>
</tbody>
</table>
<h2 id="ring-allreduce特点">Ring AllReduce特点</h2>
<p>Ring
AllReduce理论上已经是同步算法的最佳，它的特点是随着GPU数量的增多，整个过程所需的时间几乎保持不变，也就是通讯时间成本和机器数量无关！</p>
<p>这么一来，在使用更大集群的时候，节点间的通讯就不会成为提升线性扩展比的瓶颈。比如你原来128卡要训一天，那几乎可以认为256卡训半天就能达到相同的程度。</p>
<p>当然理论是理论，实际上当设备数非常大，还会有另外的问题。</p>
<p>OneFlow的这篇文章介绍得很清楚，可以一读：<a href="https://zhuanlan.zhihu.com/p/504957661">手把手推导Ring
All-reduce的数学性质(https://zhuanlan.zhihu.com/p/504957661)</a>。</p>
<h1 id="zero">ZeRO</h1>
<p>那么除了上面的混合精度方案，Ring
AllReduce的DP之外，还有没有什么方法能简单快捷减少显存，提升训练效率？兄弟，有的，而且很强，那就是ZeRO。</p>
<p>ZeRO = Zero Redundancy Optimizer</p>
<p>ZeRO核心是优化显存，减少训练所需的显存占用。</p>
<p>ZeRO有三个stage，ZeRO-1，ZeRO-2，ZeRO-3，对显存的优化逐步变强（但是代价也逐步增加）。</p>
<h2 id="zero-1">ZeRO-1</h2>
<p>原来呢，在FP32 + FP16的混合精度训练下，对于包含<span class="math inline">\(\Phi\)</span>个参数的模型，每个GPU都存有一份完整的模型参数、梯度和优化器状态：</p>
<ul>
<li>模型参数（FP16 + FP32）：<span class="math inline">\(6\Phi\)</span>（byte）<br>
</li>
<li>梯度（FP16）：<span class="math inline">\(2\Phi\)</span><br>
</li>
<li>优化器状态（FP32）：<span class="math inline">\(8\Phi\)</span></li>
</ul>
<p>每次同步完梯度之后，各个GPU会各自更新优化器状态。</p>
<p>这里面其实就有巨大的显存冗余，因为每个GPU都有一份一样的优化器状态，而AdamW的优化器状态又占了很大一部分显存（比如7B的模型就有28G的优化器状态）。</p>
<p>那ZeRO-1就想办法消除这个优化器状态的冗余。核心思想就是：</p>
<ul>
<li>partition：有N个GPU，就把优化器状态切分成N份，每个GPU在整个训练过程中，只保存和管理其中的一份。<br>
</li>
<li>distributed update：每个GPU只负责更新其所持有的那部分优化器状态
对应的 模型参数。</li>
</ul>
<p>有开ZeRO-1和没有开ZeRO-1，在流程上差别就在于梯度同步之后的操作。开ZeRO-1的情况下，同步梯度之后，由于每个GPU只有1/N的优化器状态，因此只能更新对应的1/N的模型参数。更新完1/N的参数之后，为了能在下次迭代时保持各个GPU上模型参数的一致性，就还要做一次all-gather来同步模型的参数。</p>
<p>显存上，每个GPU只需维护<span class="math inline">\(8\Phi/N\)</span>的优化器状态，而且GPU数越多N越大，那么每个GPU所需的显存就越少。这简直就是训练框架界的PDD：用得越多省得越多！有可能你一个模型本来8卡会CUDA
OOM，那在开ZeRO的情况下，可能多加点卡，比如32卡，就不会OOM了。</p>
<p>而在通讯量上，多了一次模型参数的all-gather，所以理论上是<span class="math inline">\(2\Phi+\Phi=3\Phi\)</span>个参数。但是这里还有个优化点，可以把实际的通讯量降到<span class="math inline">\(2\Phi\)</span>个参数。</p>
<p>回想一下，同步梯度的时候，分为reduce-scatter和all-gather两步，每步的通讯量都是<span class="math inline">\(\Phi\)</span>个参数。reduce-scatter让每个GPU拥有1/N块完整的梯度，all-gather把每个GPU拥有的这块完整梯度同步给其他所有GPU。但是在ZeRO-1的情况下，就可以不做梯度的all-gather。因为ZeRO-1的情况下，每个GPU只有1/N的优化器状态，也只会更新1/N的模型参数，同步整个模型的梯度没有意义。因此这个我们只需要对梯度做reduce-scatter，让每个GPU拥有需要更新的参数的那部分完整梯度就可以了！等每个GPU都更新完自己的那部分模型参数之后，再对模型参数做all-gather就可以了。这么一来ZeRO-1的通讯量完全没有增加，但是显存消耗量却减少了，这完全是免费午餐。</p>
<h2 id="zero-2-zero-3">ZeRO-2 &amp; ZeRO-3</h2>
<p>ZeRO-2在ZeRO-1的基础上，对梯度也进行了切分。每个GPU只有模型参数是完整的，而梯度和优化器状态都只会储存和管理1/N的小块，而不是完整的一份。ZeRO-2和ZeRO-1的通讯量一样，都是<span class="math inline">\(2\Phi\)</span>个参数：一次梯度的reduce-scatter，一次新参数的all-gather。</p>
<p>而ZeRO-3更进一步，每个GPU上连模型参数都是不完整的。forward计算的时候，要先做一次all-gather，计算完就把不属于自己的模型参数都释放掉。同样地，backward的时候也是类似操作。之后就是和ZeRO-1/2一样，同步新参数。因此总通讯量是<span class="math inline">\(3\Phi\)</span>个参数。ZeRO-3的通讯量会增大，而显存的训练则大大减小，颇有点时间换空间的意味。</p>
<p>DDP，ZeRO-1，ZeRO-2和ZeRO-3在7.5B参数的模型和64卡集群下，显存的消耗对比（没有包含激活值）：</p>
<img src="/fe0adaa5/zero.png" class title="混合精度训练">
<p>另外，模型计算过程中的激活值也是可以切割和分块维护的，这块就比较复杂了，要根据实际情况灵活设计要保存的activation和要切分的块。</p>
<h2 id="zero-offload">ZeRO-Offload</h2>
<p>ZeRO-Offload另辟蹊径，把放不进显存的变量放到内存上了。</p>
<p>ZeRO-Offload基于ZeRO-2的优化器状态和梯度分片策略，但进一步将这两类数据卸载（offload）到CPU内存中，同时利用CPU的计算能力执行部分低复杂度任务（如参数更新），将高计算复杂度的前向/反向传播保留在GPU，低复杂度的优化器更新卸载到CPU，避免CPU成为性能瓶颈。</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- Agent：<br>
<a href="https://www.linsight.cn/b242bfb3.html">Agent完全手册(零)：三大模块，三个理念</a><br>
- MoE：<br>
<a href="https://www.linsight.cn/a9c496e3.html">DeepSeek-V3细节探索</a><br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/37ee84bb.html">Qwen3实测&amp;技术报告</a><br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="https://www.linsight.cn/6c0f6207.html">Qwen2.5-1M技术解析</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="https://www.linsight.cn/4519eadd.html">prompt压缩(一)</a><br>
<a href="https://www.linsight.cn/ea2871bf.html">prompt压缩(二)</a><br>
<a href="https://www.linsight.cn/bfa4f144.html">reasoning压缩(一)</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a href="https://www.linsight.cn/657a6d17.html">基模型Cognitive
Behaviors对RL的影响</a><br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="https://www.linsight.cn/2dee4921.html">LLM水印</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
<a href="https://www.linsight.cn/d253d7b3.html">关于The Bitter
Lesson</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
<a href="https://www.linsight.cn/b4d047c1.html">DeepSeek-VL2的细节</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】深度学习分布式训练框架 Horovod --- (1)
基础知识，https://www.cnblogs.com/rossiXYZ/p/14856464.html<br>
【2】手把手推导Ring
All-reduce的数学性质，https://zhuanlan.zhihu.com/p/504957661<br>
【3】大模型涉及到的精度有多少种？FP32、TF32、FP16、BF16、FP8、FP4、NF4、INT8都有什么关联，一文讲清楚，https://zhuanlan.zhihu.com/p/673708074<br>
【4】十分钟速通优化器原理，通俗易懂（从SGD到AdamW），https://zhuanlan.zhihu.com/p/686410423<br>
【5】机器学习11种优化器推导过程详解(SGD,BGD,MBGD,Momentum,NAG,Adagrad,Adadelta,RMSprop,Adam,Nadma,Adamx)，https://blog.csdn.net/yangwohenmai1/article/details/124882119<br>
【6】【LLM101n】7：流行的LLM优化算法 -
AdamW，https://zhuanlan.zhihu.com/p/7272881104<br>
【7】Huge and Efficient!
一文了解大规模预训练模型高效训练技术，https://aiorang.com/article/PqmOhWF.html<br>
【8】大模型精度（FP16，FP32，BF16）详解与实践，https://www.53ai.com/news/qianyanjishu/2024052494875.html<br>
【9】LLM
时代，如何优雅地训练大模型？，https://zhuanlan.zhihu.com/p/660394604<br>
【10】deepspeed 滴 ZERO
介绍，https://blog.csdn.net/weixin_42253689/article/details/147568376<br>
【11】图解大模型训练之：数据并行下篇( DeepSpeed
ZeRO，零冗余优化)，https://zhuanlan.zhihu.com/p/618865052</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>预训练</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>关于The_Bitter_Lesson</title>
    <url>/d253d7b3.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>The Bitter Lesson的原文和译文放在后面了。可以先看看。</p>
<p>强化学习之父Rich
Sutton这篇2019年的短文最近出场率很高。这篇文章其实就讲一个事：基于人类认知的逻辑雕花，在拉长的时间线上来看，都终将被摩尔定律加持的search和learning所碾压。</p>
<p>说白了还是scaling
law。最近新版的GPT-4o令人震惊的效果就再次验证了这个道理。</p>
<img src="/d253d7b3/1.jpg" class title="The_Bitter_Lesson">
<p>在GPT-4o吉卜力风格爆火之前，ComfyUI串联stable diffusion model +
各种人工设计的workflow是少有的能出"商用级"图片的方法。但是不得不说ComfyUI的使用还是有些门槛的，显卡也好模型也好流程设计也好都是有成本的。因此也有很多人做起了售卖lora和workflow或者图片定制的生意，都挺好赚的。现在GPT-4o一下子把这些都碾压了。workflow？不存在的，老夫就是一把梭，一句话修图。无论是风格还是一致性，都已经达到比较高的水准。</p>
<img src="/d253d7b3/2.png" class title="The_Bitter_Lesson">
<p>其实时间再往前一点点，Gemini 2.0
Flash的效果基本上也完全可以当傻瓜式PS使用了。</p>
<p>说说自己。本科的时候，我用各种角点检测算子、边缘检测算子 +
统计计算图像的特征，再加上SVM和各种人工规则做了个机器人视觉寻路系统。中间各种人工逻辑给识别结果做处理和打补丁。那时觉得这就很厉害了，只要再继续雕花必定大有前途。然后过了没多久VGG-16和ResNet出来了，再往后就是CNN横扫天下的几年。无论什么任务，只要能转换成模型输入，就是一把梭训就完事了。所以后来有段时间变成了一个每天的任务就是造训练数据的算法工程师。这就是算力加持下learning的碾压。</p>
<p>这几年搞的人机聊天也是一样的。以前需要QA知识库+匹配模型+对话管理+指代消解+意图理解...，每个部分都需要投入人力时间开发调试。现在呢，啪，扔上去一个DeepSeek-V3/R1完事了。这也是算力加持下search和learning的无情碾压。</p>
<p>无论是从业界的发展，还是个人的经历来看，这确实是a bitter
lesson。有另外一句话跟这个道理很相关："人们总是倾向于高估技术的短期收益，而低估技术的长期影响。"22年的时候当我还在玩Bert玩得不亦乐乎的时候，ChatGPT就这样自然又悄无声息地降临了。回头去看，其实从GPT2到GPT3就已经有一些端倪了，但是我们当时都没有很重视它。</p>
<p>不过，虽然大道理是这样没错，但是一个问题是"算力加持下的search和learning"多久能够赶上现有技术+人类逻辑雕花。一个自然的想法是，如果暂时没法大幅提升模型本身的能力，那么用现有的材料给它安上一些工具总是有收益的。在不开上帝视角的情况下，没有人能够准确预知什么时候LLM的能力能够再上一层楼。现在没法解决的问题在哪个版本就能够解决，是三个月、六个月或者是一年？但是无论多久，在快速变化的市场下我们都不可能不做任何动作。就像我们总是能知道下个月一定会有更强的模型发布，但是我们也不可能把解决现在难题的希望完全寄托在未来的模型上。</p>
<p>流浪地球里，几十支队伍都去拯救同一个发动机，只要有一支队伍成功了那救援就成功了，这叫饱和式救援。从整个业界来说，这些基于人类逻辑的雕花也可以叫做饱和式发展。我们没有办法知道search和learning什么时候能够解决我们的问题，那就先做能做的，只要有一个成功了，起码这个台阶就算上去了。虽然search和learning作为最强力的一支部队，只要时间拉得够长就会赶上大部分的进展，但是万一哪天它卡住了（摩尔定律也在放缓），我们至少还有plan
B。</p>
<p>再从成本角度来看，目前强大的通用型模型成本肯定比只完成单一任务的垂域小模型要高。在生产力场景，垂域能力的需求量并不低，因此这里的成本不可不考虑。对于大厂如此，对于创业公司更是如此。只是目前大家都还只关注在效果上，等到效果提升的进展放缓，成本一定会再次成为重点问题之一。</p>
<p>而从个人角度来说，我相信即使过往的工作被碾压，也并非什么都没有留下：经验和眼界依然是很重要的财富。以前做预训练的时候热衷于研究各种结构变化，参数调整。现在来看这些都变成了屠龙之技。虽然现在没有龙了，但是屠龙技的学习还是锻炼了人的认知和思考，相信这种经历依然有它发挥价值的场景。就像三体中，丁仪在水滴到达地球前醒来，即使被冷冻了多年，但是他还是地球物理学中最顶峰的人之一，就是因为在那个基础物理被锁大多数人转向应用物理的年代，他是少数做过基础研究的人。这种经历让他拥有其他新时代没有的视角。</p>
<p>最后，从AI创业者的角度来看。一方面，AI创业者正在重蹈AI研究者的覆辙，不断有"走错"赛道被新模型碾压出局的人。另一方面，风浪越大鱼越贵，winner的回报将是丰厚的。</p>
<h1 id="the-bitter-lesson">The Bitter Lesson</h1>
<p>Rich Sutton</p>
<p>March 13, 2019</p>
<p>The biggest lesson that can be read from 70 years of AI research is
that general methods that leverage computation are ultimately the most
effective, and by a large margin. The ultimate reason for this is
Moore's law, or rather its generalization of continued exponentially
falling cost per unit of computation. Most AI research has been
conducted as if the computation available to the agent were constant (in
which case leveraging human knowledge would be one of the only ways to
improve performance) but, over a slightly longer time than a typical
research project, massively more computation inevitably becomes
available. Seeking an improvement that makes a difference in the shorter
term, researchers seek to leverage their human knowledge of the domain,
but the only thing that matters in the long run is the leveraging of
computation. These two need not run counter to each other, but in
practice they tend to. Time spent on one is time not spent on the other.
There are psychological commitments to investment in one approach or the
other. And the human-knowledge approach tends to complicate methods in
ways that make them less suited to taking advantage of general methods
leveraging computation. There were many examples of AI researchers'
belated learning of this bitter lesson, and it is instructive to review
some of the most prominent.</p>
<p>In computer chess, the methods that defeated the world champion,
Kasparov, in 1997, were based on massive, deep search. At the time, this
was looked upon with dismay by the majority of computer-chess
researchers who had pursued methods that leveraged human understanding
of the special structure of chess. When a simpler, search-based approach
with special hardware and software proved vastly more effective, these
human-knowledge-based chess researchers were not good losers. They said
that "brute force" search may have won this time, but it was not a
general strategy, and anyway it was not how people played chess. These
researchers wanted methods based on human input to win and were
disappointed when they did not.</p>
<p>A similar pattern of research progress was seen in computer Go, only
delayed by a further 20 years. Enormous initial efforts went into
avoiding search by taking advantage of human knowledge, or of the
special features of the game, but all those efforts proved irrelevant,
or worse, once search was applied effectively at scale. Also important
was the use of learning by self play to learn a value function (as it
was in many other games and even in chess, although learning did not
play a big role in the 1997 program that first beat a world champion).
Learning by self play, and learning in general, is like search in that
it enables massive computation to be brought to bear. Search and
learning are the two most important classes of techniques for utilizing
massive amounts of computation in AI research. In computer Go, as in
computer chess, researchers' initial effort was directed towards
utilizing human understanding (so that less search was needed) and only
much later was much greater success had by embracing search and
learning.</p>
<p>In speech recognition, there was an early competition, sponsored by
DARPA, in the 1970s. Entrants included a host of special methods that
took advantage of human knowledge---knowledge of words, of phonemes, of
the human vocal tract, etc. On the other side were newer methods that
were more statistical in nature and did much more computation, based on
hidden Markov models (HMMs). Again, the statistical methods won out over
the human-knowledge-based methods. This led to a major change in all of
natural language processing, gradually over decades, where statistics
and computation came to dominate the field. The recent rise of deep
learning in speech recognition is the most recent step in this
consistent direction. Deep learning methods rely even less on human
knowledge, and use even more computation, together with learning on huge
training sets, to produce dramatically better speech recognition
systems. As in the games, researchers always tried to make systems that
worked the way the researchers thought their own minds worked---they
tried to put that knowledge in their systems---but it proved ultimately
counterproductive, and a colossal waste of researcher's time, when,
through Moore's law, massive computation became available and a means
was found to put it to good use.</p>
<p>In computer vision, there has been a similar pattern. Early methods
conceived of vision as searching for edges, or generalized cylinders, or
in terms of SIFT features. But today all this is discarded. Modern
deep-learning neural networks use only the notions of convolution and
certain kinds of invariances, and perform much better.</p>
<p>This is a big lesson. As a field, we still have not thoroughly
learned it, as we are continuing to make the same kind of mistakes. To
see this, and to effectively resist it, we have to understand the appeal
of these mistakes. We have to learn the bitter lesson that building in
how we think we think does not work in the long run. The bitter lesson
is based on the historical observations that 1) AI researchers have
often tried to build knowledge into their agents, 2) this always helps
in the short term, and is personally satisfying to the researcher, but
3) in the long run it plateaus and even inhibits further progress, and
4) breakthrough progress eventually arrives by an opposing approach
based on scaling computation by search and learning. The eventual
success is tinged with bitterness, and often incompletely digested,
because it is success over a favored, human-centric approach.</p>
<p>One thing that should be learned from the bitter lesson is the great
power of general purpose methods, of methods that continue to scale with
increased computation even as the available computation becomes very
great. The two methods that seem to scale arbitrarily in this way are
search and learning.</p>
<p>The second general point to be learned from the bitter lesson is that
the actual contents of minds are tremendously, irredeemably complex; we
should stop trying to find simple ways to think about the contents of
minds, such as simple ways to think about space, objects, multiple
agents, or symmetries. All these are part of the arbitrary,
intrinsically-complex, outside world. They are not what should be built
in, as their complexity is endless; instead we should build in only the
meta-methods that can find and capture this arbitrary complexity.
Essential to these methods is that they can find good approximations,
but the search for them should be by our methods, not by us. We want AI
agents that can discover like we can, not which contain what we have
discovered. Building in our discoveries only makes it harder to see how
the discovering process can be done.</p>
<p>从70年人工智能研究中可以汲取的最大教训是：利用计算的通用方法最终总是最有效的，且优势巨大。其根本原因在于摩尔定律，或者说计算单位成本持续指数级下降的普遍规律。多数AI研究都假设agent的计算能力恒定（这种情况下利用人类知识就成了提升性能的唯一途径），但只需将时间线拉长到典型研究周期之外，海量计算资源终将唾手可得。研究者们为寻求短期突破，往往试图注入特定领域的人类知识，但长远来看真正重要的只有对计算资源的驾驭。这两种路径本不必相互冲突，但现实中往往背道而驰——投入其中一方的时间就无法用于另一方，研究者心理上也会对某种方法产生路径依赖。更关键的是，依赖人类知识的方法常会使系统复杂化，反而阻碍其发挥通用计算方法的优势。AI学界对此惨痛教训的领悟往往姗姗来迟，回顾几个典型案例极具启示意义。</p>
<p>在国际象棋领域，1997年击败世界冠军卡斯帕罗夫的制胜法宝正是大规模深度搜索。当时主流计算机象棋研究者对此深感沮丧，他们长期致力于利用人类对棋局特殊结构的理解。当配备专用软硬件的简单搜索方法展现出碾压性优势时，这些依赖人类知识的学者难以坦然认输，辩称"暴力搜索"只是侥幸获胜，既非通用策略，更不符合人类下棋方式。他们渴望基于人类智慧的方法获胜，失败令其倍感失落。</p>
<p>计算机围棋领域重现了相似的发展轨迹，只是迟到了二十年。初期研究大量投入在利用人类棋谱知识和围棋特性来规避搜索，但当大规模搜索配合自我对弈学习（这种价值函数学习方法在其他游戏乃至象棋中均有应用，尽管在1997年的冠军程序中未起主要作用）实现突破时，先前所有努力都被证明是徒劳甚至适得其反。自我对弈与广义的学习机制，本质上都是调动海量计算资源的途径。搜索与学习正是AI研究中驾驭巨量计算的两大核心技术。与象棋如出一辙，围棋研究者初期执着于人类经验（以减少搜索需求），直到后期全面拥抱搜索与学习才取得重大突破。</p>
<p>语音识别领域早在1970年代DARPA举办的竞赛中就显现端倪。参赛者既有利用人类知识（词汇、音素、声道等）的专门方法，也有基于隐马尔可夫模型(HMM)的统计方法——后者计算量更大但最终胜出。这逐渐引领自然语言处理领域长达数十年的范式转变，统计与计算最终占据主导。近期深度学习在语音识别的崛起，正是这一趋势的最新体现。深度学习方法更彻底地摆脱对人类知识的依赖，通过超大规模训练集上的学习与更强算力，实现了质的飞跃。与棋类研究相似，开发者总试图模仿自身思维模式构建系统，但当摩尔定律带来充足算力且找到有效利用途径时，这些预设反而成为阻碍，造成科研资源的巨大浪费。</p>
<p>计算机视觉领域同样经历了这种范式迁移。早期方法致力于边缘检测、广义柱体或SIFT特征提取，如今这些均被抛弃。现代深度学习神经网络仅依靠卷积与特定不变性概念，性能却远超从前。</p>
<p>这一深刻教训至今未被充分吸收——我们仍在重复同类错误。要认清并抵制这种倾向，必须理解其诱惑所在：我们必须咽下这颗苦果，即长期来看，将人类思维模式植入系统终将失败。历史经验表明：1)
AI研究者惯于将知识hard code进系统；2) 短期确实见效且令研究者欣慰；3)
长期却会遭遇瓶颈甚至阻碍发展；4)
突破性进展最终来自相反路径——通过搜索与学习实现计算规模扩张。这种成功往往伴随着苦涩，因其颠覆了以人类为中心的传统范式。</p>
<p>惨痛教训首先揭示了通用方法的强大威力：那些能随算力增长持续扩展的方法。搜索与学习正是两类具备无限扩展潜力的方法。其次，我们必须承认心智内容的极端复杂性：空间、物体、多智能体、对称性等概念本质上都是外部世界任意复杂的组成部分，不应被hard
code进系统。我们真正需要构建的是能发现并捕捉这种复杂性的元方法，其核心在于能自主寻找优质近似解——但探索过程应由方法本身完成，而非依赖人类预设。我们需要的是具备自主发现能力的AI智能体，而非装载人类已有知识的容器。预先植入我们的发现，只会遮蔽发现过程的本质规律。</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"><br>
博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a9c496e3.html">DeepSeek-V3细节探索</a><br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="https://www.linsight.cn/6c0f6207.html">Qwen2.5-1M技术解析</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="https://www.linsight.cn/4519eadd.html">prompt压缩(一)</a><br>
<a href="https://www.linsight.cn/ea2871bf.html">prompt压缩(二)</a><br>
<a href="https://www.linsight.cn/bfa4f144.html">reasoning压缩(一)</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="https://www.linsight.cn/2dee4921.html">LLM水印</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
<a href="https://www.linsight.cn/b4d047c1.html">DeepSeek-VL2的细节</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>创业</tag>
        <tag>scaling law</tag>
      </tags>
  </entry>
  <entry>
    <title>Qwen3实测 &amp; 技术报告</title>
    <url>/37ee84bb.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>Qwen3报告出来了，这次发报告的速度感觉比之前快一些。来看下报告披露了什么内容。</p>
<h1 id="实战感受">实战感受</h1>
<p>说报告内容之前，插几句直观的Qwen3使用感受。</p>
<p>最近都在搞Agent，因此之前Qwen3模型出来的时候，第一时间用旗舰的235B
MoE模型替换Agent业务中在用的DeepSeek-R1和V3，试试看效果怎么样。</p>
<p>这个Agent业务大致上就是一个类DeepSearch的框架，主prompt是一个大几k字符的英文prompt，加上一些比较细致的输出要求。给DeepSeek-R1，DeepSeek-V3和Qwen3-235B-A22B（think）跑了相同的60条case。</p>
<p>在格式/指令遵循上，主要看格式的输出是不是符合要求。R1和V3的60条都没有出错，而Qwen3-235B-A22B则跑了13条的时候就错了4条，也就没继续跑了。虽然这几条case都是一些不严重的错误，基本可以通过规则修复，但还是觉得有点奇怪，第一想法就是Qwen3不至于和V3/R1有这么大差距。于是尝试把原来的英文prompt翻译成中文的，再跑Qwen3-235B-A22B，发现指令遵循的效果好了不少，60条只错了1条，勉强算是达到可用水平了。</p>
<p>而效果上，在DeepSearch这个场景，Qwen3-235B-A22B（think）能够做到和DeepSeek-V3差不多的水平，至少在这几十条评测case中，没有看到明显的差距。</p>
<p>速度上，Qwen3-235B几乎能够做到R1/V3的两倍速度，这在实践上还是挺有优势的。</p>
<p>整体上，可以认为Qwen3-235B-A22B的中文能力和DeepSeek-V3接近，而Qwen3的参数量更小一些，在一些场景下可以平替DeepSeek-V3甚至R1；而英文场景上，Qwen3暂时还是和V3/R1有一些差距的。</p>
<p>BTW，发现DeepSeek-R1/V3在英文上的效果可能还略好于中文。</p>
<h1 id="模型">模型</h1>
<p>回到Qwen3报告上来，先看下模型设计。</p>
<p>Qwen3共有8个模型：</p>
<table>
<colgroup>
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th>模型</th>
<th>类型</th>
<th>层数</th>
<th>头数(Q/KV)</th>
<th>是否共享专家</th>
<th>总专家数/激活专家数</th>
<th>是否绑定嵌入</th>
<th>上下文长度</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Qwen3-0.6B</td>
<td>Dense</td>
<td>28</td>
<td>16 / 8</td>
<td>N/A</td>
<td>N/A</td>
<td>是</td>
<td>32K</td>
</tr>
<tr class="even">
<td>Qwen3-1.7B</td>
<td>Dense</td>
<td>28</td>
<td>16 / 8</td>
<td>N/A</td>
<td>N/A</td>
<td>是</td>
<td>32K</td>
</tr>
<tr class="odd">
<td>Qwen3-4B</td>
<td>Dense</td>
<td>36</td>
<td>32 / 8</td>
<td>N/A</td>
<td>N/A</td>
<td>是</td>
<td>128K</td>
</tr>
<tr class="even">
<td>Qwen3-8B</td>
<td>Dense</td>
<td>36</td>
<td>32 / 8</td>
<td>N/A</td>
<td>N/A</td>
<td>否</td>
<td>128K</td>
</tr>
<tr class="odd">
<td>Qwen3-14B</td>
<td>Dense</td>
<td>40</td>
<td>40 / 8</td>
<td>N/A</td>
<td>N/A</td>
<td>否</td>
<td>128K</td>
</tr>
<tr class="even">
<td>Qwen3-32B</td>
<td>Dense</td>
<td>64</td>
<td>64 / 8</td>
<td>N/A</td>
<td>N/A</td>
<td>否</td>
<td>128K</td>
</tr>
<tr class="odd">
<td>Qwen3-30B-A3B</td>
<td>MoE</td>
<td>48</td>
<td>32 / 4</td>
<td>否</td>
<td>128 / 8</td>
<td>否</td>
<td>128K</td>
</tr>
<tr class="even">
<td>Qwen3-235B-A22B</td>
<td>MoE</td>
<td>94</td>
<td>64 / 4</td>
<td>否</td>
<td>128 / 8</td>
<td>否</td>
<td>128K</td>
</tr>
</tbody>
</table>
<p>大部分设置都和前代是一样：</p>
<ul>
<li>RoPE<br>
</li>
<li>GQA<br>
</li>
<li>SwiGLU<br>
</li>
<li>较小模型tie
embedding把参数留给层数，较大模型就还是分别训练效果更好</li>
</ul>
<p>有变化的包括：</p>
<ul>
<li>相比Qwen二代，Qwen3去掉了QKV
bias，增加了QK-Norm，提高训练稳定性<br>
</li>
<li>MoE没有使用共享专家了<br>
</li>
<li>MoE使用lobal-batch load balancing loss，提高专家专业度</li>
</ul>
<p>MoE不使用共享专家这个倒是有点意外，后续值得探索一下。</p>
<h1 id="预训练">预训练</h1>
<p>预训练阶段，数据总量进一步提升到了36T
token，除了常规的数据收集，还用Qwen2.5-VL从文档中提取了一些数据。另外也用Qwen2.5、Qwen2.5-Coder和Qwen2.5-Math合成了T
token级别的训练数据。</p>
<p>训练还是多阶段：</p>
<table>
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>阶段</th>
<th>训练数据</th>
<th>长度</th>
<th>训练目标</th>
<th>特殊操作</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>通用阶段（S1）</td>
<td>约30T token，涵盖119种语言和方言</td>
<td>4,096</td>
<td>训练语言熟练度和通用世界知识</td>
<td>无</td>
</tr>
<tr class="even">
<td>推理阶段（S2）</td>
<td>约5T更高质量的token，增加STEM、Code、推理和合成数据比例</td>
<td>4,096</td>
<td>进一步提高推理能力</td>
<td>加速学习率衰减</td>
</tr>
<tr class="odd">
<td>长上下文阶段</td>
<td>数十B token，75%文本长度在16,384 - 32,768，25%文本长度在4,096 -
16,384</td>
<td>32,768</td>
<td>扩展模型上下文长度</td>
<td>使用ABF将RoPE基础频率从10,000提高到1,000,000；引入YARN和DCA以实现推理时序列长度提升四倍</td>
</tr>
</tbody>
</table>
<p>Base模型的评测效果不出意外，整体上是在更小参数量上获得更高的分数：</p>
<ul>
<li><strong>Qwen3-235B-A22B-Base优势显著</strong>：在多数任务上，Qwen3-235B-A22B-Base凭借更少的总参数或激活参数，超越DeepSeek-V3
Base、Llama-4-Maverick
Base、Qwen2.5-72B-Base等。对比参数约为其两倍的Llama-4-Maverick-Base，以及总参数约为其三倍的DeepSeek-V3-Base，Qwen3-235B-A22B-Base在大部分基准测试中表现更优。<br>
</li>
<li><strong>Qwen3
MoE基础模型表现出色</strong>：使用相同预训练数据时，Qwen3
MoE仅用1/5的激活参数就能达到与Qwen3 dense模型相近的水平。同时，Qwen3
MoE以不到1/2的激活参数和更少的总参数，超越Qwen2.5
MoE。即便激活参数仅为Qwen2.5 dense模型的1/10，Qwen3
MoE基础模型仍能comparable。<br>
</li>
<li><strong>Qwen3密集基础模型性能提升</strong>：Qwen3
dense模型在较高参数规模下，整体性能与Qwen2.5基础模型相当，在STEM、编码和推理基准测试中，部分Qwen3
dense模型性能甚至超越更高参数规模的Qwen2.5模型。例如，Qwen3-1.7B/4B/8B/14B/32B-Base分别与Qwen2.5-3B/7B/14B/32B/72B-Base性能可比，且在特定领域表现更优。</li>
</ul>
<h1 id="post-training">Post-training</h1>
<p>在post-training上，Qwen3有两个特性：</p>
<ul>
<li>1.strong-to-weak蒸馏：只有最大的旗舰MoE模型和最大的Dense模型是走常规训练流程获得的，其他几个都是用这俩蒸馏获得的。相比使用多阶段的post-training，使用teacher
model的output
logits蒸馏的效果更好，而且资源消耗量只有多阶段post-training的1/10。<br>
</li>
<li>2.thinking control：每个模型都既有thinking mode也有non-thinking
mode，可在启动配置中开关，也可在每个输入中通过soft开关调整。</li>
</ul>
<img src="/37ee84bb/post_train.png" class title="Qwen3">
<p>看上图，旗舰模型的post-training共有4个stage，前两个stage主要提升深度思考的能力，而后两个stage就把两种模式结合起来。</p>
<h2 id="post-training的各个phase">post-training的各个phase</h2>
<ol type="1">
<li>Long-CoT Cold Start</li>
</ol>
<p>数据：整理涵盖数学、代码、逻辑推理和一般 STEM
问题等多类别的综合数据集，数据集中每个问题都配有经过验证的参考答案或基于代码的测试用例，作为长思维链训练
“冷启动” 阶段的基础。（更详细的筛选策略在报告4.1）</p>
<p>Cold
Start的目的是为模型灌输基础推理模式，而不过分追求推理效果，以便在后续强化学习阶段有更大的提升空间和灵活性。因此，在这个阶段要使用比较少的数据量和训练步骤。</p>
<ol start="2" type="1">
<li>Reasoning RL</li>
</ol>
<p>这是提升模型推理思考能力的关键阶段。</p>
<p><strong>数据筛选标准</strong>：（1）未在Cold
Start阶段使用（2）冷启动模型可学习（3）具有挑战性以及（4）覆盖广泛子领域，最终收集到3,995对数据。<br>
<strong>训练方法与策略</strong>：用GRPO训练模型，用大batch size、 a high
number of rollouts per query和off-policy训练，可以提升训练效率。<br>
<strong>训练效果</strong>：在单次RL训练过程中，模型的训练奖励和验证性能不断提升，且无需手动调整超参数。以Qwen3-235B-A22B模型为例，其AIME24分数在170个RL训练step中从70.1提升到85.1。</p>
<ol start="3" type="1">
<li>Thinking Mode Fusion</li>
</ol>
<p>这一阶段的目的是将non-thinking能力整合到二阶段得到的thinking模型中。</p>
<p><strong>SFT数据构建</strong>：SFT数据融合“thikning”和“non-thinking”数据。“thikning”数据利用第二阶段的模型，通过rejection
sampling第一阶段的query生成；“non-thinking”数据涵盖code、数学、指令遵循等多领域任务。为提升低资源语言的任务效果，增加了翻译任务的比例。</p>
<p><strong>聊天模板设计</strong>：设计专用聊天模板，在用户query或system
message里，用“/think”和“/no
think”标志区分“thikning”和“non-thinking”样本。模型据此选择合适的思考模式。“non-thinking”样本的response保留空的思考内容，保证两种模式格式上的一致。模型默认是处于“thikning”模式，因此训练时还加入了无“/think”标志的样本，模型也要按“thikning”处理。对于多轮对话，随机插入标志，让模型按最后遇到的标志进行回复。</p>
<p><strong>思维预算控制</strong>：思维模式的融合让模型能基于不完整思维生成response，为思维预算控制创造条件。当模型思考长度达用户设定阈值，插入停止思维指令，强制模型生成response。这一能力在融合过程中自然形成，无需额外训练，提升了模型在不同场景下的推理效率和资源利用效率。</p>
<ol start="4" type="1">
<li>General RL</li>
</ol>
<p>和R1一样，最后阶段要进行通用数据的RL训练，以提升全场景下的模型能力。</p>
<p>General
RL最重要的工作就是建立强大的reward系统。Qwen3的reward系统涵盖20+任务，分别关注在提升多样能力，包括：</p>
<ul>
<li>确保模型准确理解并遵循用户指令的指令遵循能力；<br>
</li>
<li>使模型遵循特定格式规范的格式遵循能力；<br>
</li>
<li>提高模型在开放query中表现出的helpfulness,
engagement和style，以优化用户体验的偏好对齐能力；<br>
</li>
<li>训练模型正确调用工具接口，增强agent能力；<br>
</li>
<li>针对特定场景（如RAG），引导模型生成准确且合适的response，降低幻觉风险的特殊场景能力；<br>
等</li>
</ul>
<p>reward的类型也分成三种：</p>
<ul>
<li><strong>Rule-based
Reward</strong>：在推理RL阶段已广泛使用，可高精度评估模型输出的正确性，有效防止reward
hacking，适用于指令遵循和格式遵守等一般任务。<br>
</li>
<li><strong>Model-based Reward with Reference
Answer</strong>：为每个query提供参考答案，让Qwen2.5-72B-Instruct根据参考答案进行评分，能更灵活处理多样任务，避免单纯基于规则奖励可能出现的误判。<br>
</li>
<li><strong>Model-based Reward without Reference
Answer</strong>：利用人类偏好数据训练reward模型，为模型response打分，可处理更general的query。</li>
</ul>
<h2 id="strong-to-weak-distillation">Strong-to-Weak Distillation</h2>
<p>蒸馏已经是比较成熟的手段了，多个实践也都证明有稳定收益。</p>
<p>Qwen3的蒸馏主要分2个阶段：</p>
<ul>
<li><strong>Off-policy
Distillation</strong>：将teacher模型在“/think”和“/no
think”两种模式下生成的输出结合起来，用于蒸馏。这帮助轻量级学生模型培养基本的推理技能，以及在不同思维模式间切换的能力，为下一阶段的On-policy
Distillation训练打基础。<br>
</li>
<li><strong>On-policy
Distillation</strong>：具体操作是对prompt进行采样，学生模型以“/think”或“/no
think”模式生成response，然后通过对齐其输出的logits与教师模型（Qwen3-32B或Qwen3-235B-A22B）的logits，最小化KL散度，实现学生模型的微调。</li>
</ul>
<h1 id="小结">小结</h1>
<p>Qwen3主要的变化的点：</p>
<ul>
<li>MoE不使用共享专家<br>
</li>
<li>全线模型支持thinking和non-thinking切换，不过这种做法估计对纯thinking模式有损</li>
</ul>
<p>整体来说，Qwen3比Qwen2.5在效果和效率上都有一定提升，特别是融合思考模型，是大部分无法做大规模训练的算法团队的救星，有机会缓解被老板追着问耗时/成本能不能解决的问题了。</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- Agent：<br>
<a href="https://www.linsight.cn/b242bfb3.html">Agent完全手册(零)：三大模块，三个理念</a><br>
- MoE：<br>
<a href="https://www.linsight.cn/a9c496e3.html">DeepSeek-V3细节探索</a><br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="https://www.linsight.cn/6c0f6207.html">Qwen2.5-1M技术解析</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="https://www.linsight.cn/4519eadd.html">prompt压缩(一)</a><br>
<a href="https://www.linsight.cn/ea2871bf.html">prompt压缩(二)</a><br>
<a href="https://www.linsight.cn/bfa4f144.html">reasoning压缩(一)</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a href="https://www.linsight.cn/657a6d17.html">基模型Cognitive
Behaviors对RL的影响</a><br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="https://www.linsight.cn/2dee4921.html">LLM水印</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
<a href="https://www.linsight.cn/d253d7b3.html">关于The Bitter
Lesson</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
<a href="https://www.linsight.cn/b4d047c1.html">DeepSeek-VL2的细节</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】Qwen3 Technical Report,
https://github.com/QwenLM/Qwen3/blob/main/Qwen3_Technical_Report.pdf</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>Qwen</tag>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>基模型Cognitive Behaviors对RL的影响</title>
    <url>/657a6d17.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>简单读一下这篇：《Cognitive Behaviors that Enable Self-Improving
Reasoners, or, Four Habits of Highly Effective STaRs》</p>
<p>先说文章的结论：推理行为的存在，是模型能够在RL阶段获得显著提升的关键。这比答案是否正确更加重要。</p>
<p>文章相关代码都在：<a href="https://github.com/kanishkg/cognitive-behaviors">https://github.com/kanishkg/cognitive-behaviors</a></p>
<h1 id="基于countdown游戏的观察和实验">基于Countdown游戏的观察和实验</h1>
<h2 id="countdown游戏的观察">Countdown游戏的观察</h2>
<p>Countdown游戏是一个数学游戏，玩家必须使用四个基本算术运算 +,−,×,÷
组合一组输入数字，以获得目标数字。例如，给定数字 25、30、3、4 和目标数字
32，解决方案涉及通过一系列运算将这些数字组合起来：（30 −25 + 3）× 4 =
32。之所以叫Countdown是因为这是以前一个电视游戏节目，解题的时候会有个30s的倒计时，需要在限时内做出来才行。</p>
<p>研究人员以Countdown游戏的数据为训练数据，用强化学习（PPO）训练
Qwen-2.5-3B 和 Llama-3.2-3B，结果发现 Qwen
的学习轨迹更好，训练后期准确性大幅提高，而 Llama 提升有限。Qwen 在第 30
步左右就出现了质的提升，response明显增长，准确性也更高。训练结束时，Qwen
的准确率达到了大约 60%，远超过了 Llama 的 30%。</p>
<img src="/657a6d17/rl.png" class title="Cognitive_Behaviors">
<p>另外，在训练的后期，Qwen
的行为发生了一个有趣的变化：模型的思考从显式的验证文本 “8 * 35 is 280
which is too high”
过渡到隐式的思考。也就是模型不再碎碎念，而会更高效尝试不同的solution，直到找到正确的答案，而不需要使用文字来反思。</p>
<h2 id="cognitive-behaviors">Cognitive Behaviors</h2>
<p>那为啥 Llama 比较差，差在哪里？或者说 Qwen
具备什么特性有助于模型在RL阶段提升效果，如果可以找到这个原因，那就可以通过放大这个特性从而在RL阶段获得更大的提升了。</p>
<p>直觉上，二者相差之处就在思考过程的内容上。为了验证这个差异，研究人员关注在模型的四个cognitive
behaviors：</p>
<ol type="1">
<li><p>回溯：Backtracking or the explicit revision of approaches when
errors are detected (e.g., “This approach won’t work
because...”)；感觉也可以叫反思或者错误复盘之类的</p></li>
<li><p>验证：Verification or the systematic checking of intermediate
results (e.g., “Let’s verify this result by...”)</p></li>
<li><p>子目标拆解：Subgoal Setting, where a complex problem is broken
down into manageable steps (e.g., “To solve this, we first need
to...”)</p></li>
<li><p>Backward Chaining: where in a goal-directed reasoning problem,
the solution works backwards from a desired outcomes (e.g., “To reach
the target of 75, we need a number divisible by...”)</p></li>
</ol>
<p>这4个行为有别于模型中常规的线性思考和推理 --
这些行为使得更加动态的搜索轨迹成为可能。</p>
<p>那么怎么看模型是否具备以上的思考行为呢？文中使用few-shot prompt + GPT
4o-mini来判断模型输出中是否包含以上这些思考模式，以及包含多少：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prompts = [</span><br><span class="line">    <span class="comment"># 1. Answer-verification steps</span></span><br><span class="line">    <span class="string">f&quot;&quot;&quot;Here is a chain-of-reasoning that a Language Model generated while trying to play the game of CountDown with the numbers <span class="subst">&#123;numbers&#125;</span>. The goal is to reach the target number <span class="subst">&#123;target&#125;</span>. The chain-of-reasoning the model used is: <span class="subst">&#123;completion&#125;</span>. </span></span><br><span class="line"><span class="string">Evaluate whether the chain-of-reasoning contains any answer-verification steps. An example of an answer-verification step is: &#x27;This sequence results in 1, which is not equal to 22&#x27; and &#x27;Since 25 is not equal to 22&#x27; for explicit verification and &#x27;Too high!&#x27; or &#x27;This works!&#x27; for implicit verification. We want to mark instances where the chain-of-reasoning explicitly checks the current result against the target number. </span></span><br><span class="line"><span class="string">If you find any answer-verification steps, please count them and provide the count as between the tags &lt;count&gt; &lt;/count&gt;. If the chain-of-reasoning does not contain any answer-verification steps, please provide a count of 0 as &lt;count&gt;0&lt;/count&gt;.&quot;&quot;&quot;</span>,</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. Backtracking behavior</span></span><br><span class="line">    <span class="string">f&quot;&quot;&quot;Here is a chain-of-reasoning that a Language Model generated while trying to play the game of CountDown with the numbers <span class="subst">&#123;numbers&#125;</span>. The goal is to reach the target number <span class="subst">&#123;target&#125;</span>. The chain-of-reasoning the model used is: <span class="subst">&#123;completion&#125;</span>.</span></span><br><span class="line"><span class="string">Evaluate whether the chain-of-reasoning contains any backtracking behavior, where the model realizes a path won&#x27;t work and explicitly goes back to try a different approach. Due to the nature of the problem, any attempt at a new combination of numbers that does not directly use the result from the previous computation is considered backtracking. </span></span><br><span class="line"><span class="string">For example, in the reasoning trace with numbers [20, 7, 11, 78] - &quot;(78 - 20) - (11 - 7) = 58 - 4 = 54, (54 - 78) + 11 = -24 + 11 = -13, (-13 + 78) - 11 = 65 - 11 = 54, (78 - 58) + 11 = 20 + 11 = 31, (78 - 58) + (20 - 11) = 20 + 9 = 29, (78 - 20) + (11 - 7) = 58 + 4 = 62, (78 - 11) - (20 - 7) = 67 - 13 = 54, (78 - 20) + (11 / 7) = 58 + 1.5714 = 59.5714, (78 - 11) / (20 - 7) = 67 / 13 = 5, (78 - 20) + (11 + 7) = 58&quot;, there are 5 instances of backtracking to the initial numbers.</span></span><br><span class="line"><span class="string">Count the number of distinct backtracking instances and provide the count between the tags &lt;count&gt; &lt;/count&gt;. If the chain-of-reasoning does not contain any backtracking behavior, please provide a count of 0 as &lt;count&gt;0&lt;/count&gt;.&quot;&quot;&quot;</span>,</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Subgoal setting</span></span><br><span class="line">    <span class="string">f&quot;&quot;&quot;Here is a chain-of-reasoning that a Language Model generated while trying to play the game of CountDown with the numbers <span class="subst">&#123;numbers&#125;</span>. The goal is to reach the target number <span class="subst">&#123;target&#125;</span>. The chain-of-reasoning the model used is: <span class="subst">&#123;completion&#125;</span>.</span></span><br><span class="line"><span class="string">Evaluate whether the chain-of-reasoning contains any explicit subgoal setting, where the model breaks down the problem into smaller, intermediate goals. An example of subgoal setting is: &quot;First, I&#x27;ll try to get close to <span class="subst">&#123;target//<span class="number">2</span>&#125;</span>, then...&quot;.</span></span><br><span class="line"><span class="string">Count the number of distinct subgoals set and provide the count between the tags &lt;count&gt; &lt;/count&gt;. If the chain-of-reasoning does not contain any subgoal setting, please provide a count of 0 as &lt;count&gt;0&lt;/count&gt;.&quot;&quot;&quot;</span>,</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Backward-chaining</span></span><br><span class="line">    <span class="string">f&quot;&quot;&quot;Here is a chain-of-reasoning that a Language Model generated while trying to play the game of CountDown with the numbers <span class="subst">&#123;numbers&#125;</span>. The goal is to reach the target number <span class="subst">&#123;target&#125;</span>. The chain-of-reasoning the model used is: <span class="subst">&#123;completion&#125;</span>.</span></span><br><span class="line"><span class="string">Evaluate whether the chain-of-reasoning contains any backward-chaining behavior, where the model starts from the target number and works backwards to the initial numbers. An example of backward-chaining when the target is 24 and the numbers are 12 and 2 is: &quot;Let&#x27;s work backwards from the target. 24/2 = 12. So, 12*2=24.&quot; and if the target is 22 and the numbers are 25 and 3 is: &quot;Since the target is 22, and 22 + 3 = 25, ...&quot;.</span></span><br><span class="line"><span class="string">Count the number of distinct backward-chaining instances and provide the count between the tags &lt;count&gt; &lt;/count&gt;. If the chain-of-reasoning does not contain any backward-chaining behavior, please provide a count of 0 as &lt;count&gt;0&lt;/count&gt;.&quot;&quot;&quot;</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>结果发现 Qwen 的效果改进与cognitive
behaviors的出现相吻合，特别是verification和backtracking这两个模式：</p>
<img src="/657a6d17/behaviors.png" class title="Cognitive_Behaviors">
<p>而 Llama 就没有表现出这些认知行为。</p>
<h2 id="分析initial-policy">分析initial policy</h2>
<p>那为什么 Qwen 在RL过程中比 Llama 有更多的cognitive
behaviors呢？问题就出在初始模型initial policy这里。Qwen-2.5-3B 天然比
Llama-3.2-3B 和 Llama-3.1-70B 在这四种重要的cognitive
behaviors有更高的出现几率：</p>
<img src="/657a6d17/base.png" class title="Cognitive_Behaviors">
<p>这些观察说明：</p>
<ul>
<li>initial policy中这些认知行为对于提升test-time
compute的效果有帮助<br>
</li>
<li>随着模型规模提升，这些认知行为也会更多</li>
</ul>
<h2 id="优化initial-behaviors">优化initial behaviors</h2>
<p>既然initial
behaviors对RL的效果有这样的影响，那么如果我们能优化initial
behaviors，那RL阶段就有可能获得更好的效果。</p>
<p>方法就是基于Countdown游戏数据集，用Claude-3.5-Sonnet构造包含不同思考过程的数据，有以下四种类型：</p>
<ul>
<li>all strategies combined<br>
</li>
<li>backtracking only<br>
</li>
<li>backtracking with verification<br>
</li>
<li>backtracking with subgoal setting<br>
</li>
<li>backtracking with backward chaining</li>
</ul>
<p>还有一个negative的，也就是不包含这些认知行为的。</p>
<p>对应的prompt在<a href="https://github.com/kanishkg/cognitive-behaviors/blob/main/generate_cot_datasets/api_gen.py">https://github.com/kanishkg/cognitive-behaviors/blob/main/generate_cot_datasets/api_gen.py</a></p>
<p>要求模型输出各种认知行为的prompt都是system prompt。比如all
strategies的system prompt是这样的：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">I want to produce reasoning trajectories for the game of countdown. The goal here is to reach a target number by combining integers using basic arithmetic operations.</span><br><span class="line">Write your thoughts in &lt;think&gt; &lt;/think&gt; tags.</span><br><span class="line">The answer is a series of arithmetic operations (+, -, *, /) that results in the target number.</span><br><span class="line">Write the final answer in &lt;answer&gt; &lt;/answer&gt; tags.</span><br><span class="line">For the final answer, make sure that each step in the final answer is written as &lt;answer&gt; (number1 [+-*/] number2) [+-*/] number3 &lt;/answer&gt;.</span><br><span class="line">Answer should be a valid mathematical expression ONLY containing starting integers and NOT the target number.</span><br><span class="line">Otherwise, the grader will not be able to parse your answer.</span><br><span class="line">- Verify that you have reached the answer and backtrack to the start or an intermediate step.</span><br><span class="line">- Work backwards from the goal if it makes things easier.</span><br><span class="line">- Decompose the answer into sub-goals and try to reach them to then reach the target, if you are unable to reach the goal or a subgoal backtrack to a previous state.</span><br><span class="line">HINT: Set subgoals that are useful like factors of the target or multiples of the target. Or numbers close to the target.</span><br><span class="line">For example, you can say things like:</span><br><span class="line">1. When the target is 24 and you have [12, 2]: &quot;12+2 = 14. 14 is not 24, so let&#x27;s try something else. 12*2=24 and 24 was the goal, so the goal has been reached.&quot;</span><br><span class="line">2. When the target is 10 and you have [12, 2]: &quot;12+2 = 14. 14 is not 10, let&#x27;s try a different sequence of operations.&quot;</span><br><span class="line">3. When the target is 10 and you have [9, 3, 2]: &quot;Let&#x27;s try to reach 20 since it is a multiple of 10…&quot; If you can&#x27;t reach it, then try something else.</span><br><span class="line">4. When the target is 24 and you have [10, 2, 2]: &quot;Let&#x27;s first try to reach 12 since it is a factor of 24; 10 * 2 = 20, let&#x27;s try a different sequence. 10 + 2 = 12. Now, 12 * 2 = 24.&quot;</span><br><span class="line">5. For backward chaining, when the target is 24 and you have (12, 2): &quot;Let&#x27;s work backwards from the target. 24/2 = 12. So, 12*2=24.&quot; This is useful when setting subgoals.</span><br></pre></td></tr></table></figure>
<p>在这些不同的思考行为要求下，Claude-3.5-Sonnet的得分如下：</p>
<img src="/657a6d17/counting.png" class title="Cognitive_Behaviors">
<p>虽然在这些行为模式下并不总能推理出正确答案，但是思考行为是存在的。</p>
<p>用Claude-3.5-Sonnet生成的数据微调 Qwen 和 Llama
之后再进行RL，在效果上都有一定的提升；特别是 Llama，能够从明显比 Qwen
差提升到和 Qwen 持平：</p>
<img src="/657a6d17/effect.png" class title="Cognitive_Behaviors">
<p>另外，使用答案错误但具有正确行为的数据训练模型，与包含正确答案的数据集训练的模型效果相当：</p>
<img src="/657a6d17/incorrect.png" class title="Cognitive_Behaviors">
<p>这说明，「推理行为的存在，是模型能够在RL阶段获得显著提升的关键。这比答案是否正确更加重要」。</p>
<h1 id="推广到通用领域">推广到通用领域</h1>
<p>既然这样的方法在Countdown上有效，那么下一步就是考虑怎么推广到通用领域。</p>
<p>直觉上，通用的预训练数据应该是比较缺乏这些认知行为的。把 Qwen
预训练数据中的 OpenWebMath 和 FineMath 中随机20w条样本拿出来，用
Qwen-2.5-32B 分析里面包含了多少这些重要的 target behaviors。</p>
<p>放个分析用的prompt样例，比如backtracking：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line"># Task Description</span><br><span class="line">You will be provided with text from the internet.</span><br><span class="line">Evaluate whether the text contains any backtracking behavior, where the writer realizes a path won&#x27;t work and explicitly goes back to try a different approach. An example of backtracking is: &quot;Let me try again&quot;, &quot;Wait&quot;, &quot;I made a mistake&quot;, or &quot;we need to try a different sequence of operations&quot;. We want to mark instances where the writer abandons a thought and backtracks to a previous computation.</span><br><span class="line"></span><br><span class="line">Backtracking in mathematics might look like:</span><br><span class="line">- &quot;I started with the wrong formula. Let&#x27;s use integration by parts instead.&quot;</span><br><span class="line">- &quot;This approach leads to a contradiction. Going back to the original equation...&quot;</span><br><span class="line">- &quot;I see the error in my calculation. Let&#x27;s recalculate using...&quot;</span><br><span class="line">- &quot;This algebraic manipulation isn&#x27;t simplifying as expected. Let&#x27;s try factoring differently.&quot;</span><br><span class="line"></span><br><span class="line">Count the number of distinct backtracking instances and provide the count between the tags &lt;count&gt; &lt;/count&gt;. If the writer does not backtrack, please provide a count of 0 as &lt;count&gt;0&lt;/count&gt;.</span><br><span class="line"></span><br><span class="line"># Task Format</span><br><span class="line">Format your response in markdown as follows:</span><br><span class="line"></span><br><span class="line">## Thoughts</span><br><span class="line">[Brief description describing what behavior was noticed and where backtracking occurred]</span><br><span class="line"></span><br><span class="line">## Does backtrack?</span><br><span class="line">[yes/no]</span><br><span class="line"></span><br><span class="line">## Number of backtrack steps</span><br><span class="line">&lt;count&gt; [1/2/...] &lt;/count&gt;</span><br><span class="line"></span><br><span class="line"># Task to evaluate for backtracking</span><br><span class="line">&#123;response&#125;</span><br><span class="line"></span><br><span class="line"># Response</span><br></pre></td></tr></table></figure>
<p>结果确实比较少，加起来不超过20%。这还是和reasoning密切相关的数学数据集，其他更加通用的数据所包含的认知行为数量就可想而知了。</p>
<p>既然如此，那就用 OpenWebMath 构造两个数据集：</p>
<ul>
<li>behaviors minimized：几乎不包含任何cognitive behavior的数据<br>
</li>
<li>cognitive behaviors：都包含cognitive behavior的数据</li>
</ul>
<p>然后用 Qwen-2.5-32B 把这些数据都重写成 question-thought-answer
的格式，最后两个数据集分别都包含8.3M token。</p>
<p>Llama-3.2-3B 在这两个数据集上进行预训练 + RL之后，观察到：</p>
<ul>
<li>用 cognitive behaviors 数据训练过的 Llama 达到 Qwen 的水平，而
behaviors minimized 数据预训练的则没有明显改进<br>
</li>
<li>用 cognitive behaviors 数据训练过的 Llama
在整个训练过程都表现出丰富的reasoning behavior</li>
</ul>
<h1 id="小结">小结</h1>
<ul>
<li>从人类认知行为学习高级的思考方式应用于模型思考是个不错的路子，至少在达到人类专家水平的目标上是有帮助的<br>
</li>
<li>Qwen 确实是很不错的base模型<br>
</li>
<li>年轻人好好写prompt</li>
</ul>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a9c496e3.html">DeepSeek-V3细节探索</a><br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="https://www.linsight.cn/6c0f6207.html">Qwen2.5-1M技术解析</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="https://www.linsight.cn/4519eadd.html">prompt压缩(一)</a><br>
<a href="https://www.linsight.cn/ea2871bf.html">prompt压缩(二)</a><br>
<a href="https://www.linsight.cn/bfa4f144.html">reasoning压缩(一)</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="https://www.linsight.cn/2dee4921.html">LLM水印</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
<a href="https://www.linsight.cn/d253d7b3.html">关于The Bitter
Lesson</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
<a href="https://www.linsight.cn/b4d047c1.html">DeepSeek-VL2的细节</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】Cognitive Behaviors that Enable Self-Improving Reasoners, or,
Four Habits of Highly Effective STaRs</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>RL</tag>
        <tag>Reasoning</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM训练各种并行策略</title>
    <url>/4cd8532f.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>从一个搞数据和训练策略的LLM算法工程师角度，把LLM训练框架中的常用并行策略(的资料)大致理一下。</p>
<p>数据并行之前已经写了：<a href="https://mp.weixin.qq.com/s/tsQ40j_jm7VSnmNI2ShL0Q">LLM训练框架：从优化器和精度讲到ZeRO</a>。</p>
<p>这里把张量并行（TP）、流水并行（PP）和序列并行简单整理一下。</p>
<h1 id="张量并行tp">张量并行（TP）</h1>
<p>张量并行，Tensor Parallelism，TP（也有叫Model
Parallelism，MP的）：LLM中，有的tensor或者layer很大，单卡放不下（或者单卡不够放整个模型），那么就需要用TP把tensor分割成多块，每一块放在一张卡上，分别使用和计算。仅当某些算子需要完整的张量时，才会进行聚合。</p>
<h2 id="分块矩阵乘法">分块矩阵乘法</h2>
<p>TP的基本思想是对矩阵乘法进行拆分：</p>
<img src="/4cd8532f/mat_mul.png" class title="并行策略">
<p>那么矩阵乘法有两种拆分方法：（1）对矩阵A按列拆分（上图上）（2）对矩阵A按行拆分（上图下）。</p>
<p>注意，当对矩阵A按行拆分的时候，也要对矩阵X进行列的拆分，保持维度的一致。</p>
<p>当对矩阵A按行拆分的时候，X和A都是concat的关系，backward的时候可以分别计算X拆分出来的小矩阵的梯度，然后再拼接起来就可以得到X的完整梯度。</p>
<p>而当对矩阵A按列进行拆分时，X同时参与了两块GPU上的前向计算，因此X的完整梯度等于两张卡上X的梯度相加。</p>
<p>更加详细的说明可以参考：<a href="https://zhuanlan.zhihu.com/p/622212228">图解大模型训练之：张量模型并行(TP)，Megatron-LM，https://zhuanlan.zhihu.com/p/622212228</a>。</p>
<h2 id="mlp层的tp">MLP层的TP</h2>
<p>上面展示的是矩阵乘法的TP。那么如果我们的计算不仅是Y=XA，而还有个激活函数呢，比如Y=ACT(XA）。把矩阵A按行切分的方式，需要在进入激活函数的计算前，同步各个GPU得到的Y，这就有不少的通讯量；而把A按列切分的方式则可以直接进行激活函数的计算。</p>
<p>那么再进一步，如果是MLP层，那么Y=ACT(XA)B，在上面的基础上又多了个B矩阵的计算，该怎么切分呢。理想的状况应该是尽量减少计算中的同步操作（从而减少通讯量），提升框架整体的计算效率。</p>
<p>基于前面的分析，我们可以对A按列切割，那么各个GPU得到的Y就是concat的关系，为了和各个小Y能够直接进行计算，那么B应该是按行切分：</p>
<img src="/4cd8532f/mlp.png" class title="并行策略">
<h2 id="attention的tp">Attention的TP</h2>
<p>那么多头注意力如何做TP呢？先回顾一下多头注意力的计算，多头注意力本身就对Q、K、V在dim维度做了切分，然后concat起来。也就是说这多个头本身，天然就是可以并行，独立进行计算的。那么只需要把不同的注意力头放到不同的GPU上，我们就得到了多头注意力的TP了。</p>
<img src="/4cd8532f/attention.png" class title="并行策略">
<h2 id="embedding层的tp">Embedding层的TP</h2>
<p>最后还有embedding层。embedding层的做法是每块GPU维护一份embedding的子集，用id去gather向量的时候，各个GPU上分别获取，对于获取不到的id，则先用特殊向量比如零向量先表示，最后再allreduce各个GPU上的向量，替换掉零向量，就获得了完整的embedding输入了。</p>
<h1 id="流水并行">流水并行</h1>
<p>流水并行，Pipeline
Parallelism，PP：将网络按层切分，划分成多组，一张卡存一组。</p>
<p>TP是对模型宽度进行切分，而PP是对模型的高度进行切分。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 假设模型有8层：L0~L7</span><br><span class="line"># 两张卡：GPU0,GPU1</span><br><span class="line">=====================   =====================</span><br><span class="line">| L0 | L1 | L2 | L3 |   | L4 | L5 | L6 | L7 |</span><br><span class="line">=====================   =====================</span><br><span class="line">        GPU0                 GPU1</span><br></pre></td></tr></table></figure>
<p>按这个思路，我们可以直接实现naive
PP：假设模型有8层，把模型前4层放在一张卡，后4层放在另一张卡；前向的时候把中间激活数据从GPU0传给GPU1，反向的时候则把数据从GPU1传到GPU0。</p>
<p>naive
PP的问题是，当GPU0在跑前向的时候，GPU1是没事干的，反过来也有一样的问题，这就导致GPU有大量的空闲时间在等数据。而且随着PP的GPU数量的提升，这个空闲率就越来越高。比如设置8卡的PP，那么GPU0在做前向计算的时候，GPU1到7都在休息。真所谓是一卡有难，七卡围观。这些GPU的空余等待时间叫bubble。</p>
<img src="/4cd8532f/bubble.png" class title="并行策略">
<p>有N张卡的PP，卡的计算利用率就只有1/N。</p>
<p>那么怎么优化PP的GPU利用率呢。</p>
<p>一个自然的想法是，能不能在GPU0算下一个batch的前向数据时，让GPU1在算上一个batch数据的反向呢？是可以的，并且还可以把batch切分成更小的micro-batch，这样就能减少GPU的空闲等待时间。</p>
<p>这就是GPipe。GPipe单个batch进一步拆分为多个Micro-Batch，通过流水线调度不同Micro-Batch的前向和反向计算，减少设备空闲时间。</p>
<p>还有很多别的方案，比如Interleaved
Pipeline、1-Forward-1-Backward等，可以看看大佬们的做法。</p>
<p>GPipe的Micro-Batch优化了bubble的问题，那还有显存问题呢。比如GPU1在接收来自GPU0的前向数据时，自己也还有反向传播的中间层数据，这么一来显存就很吃紧了。一个方法就是用activation
checkpoint来减少显存的消耗。</p>
<p>实际上个人感觉流水并行是比较复杂的，也有很多不同的实现方法，可以看看框架大佬们的资料。</p>
<h1 id="d并行">3D并行</h1>
<p>3D = DP + TP + PP。</p>
<p>DP是对数据进行切分，TP是对模型宽度进行切分，而PP是对模型的高度进行切分。这三者是可以组合起来使用的。</p>
<img src="/4cd8532f/parallel.png" class title="并行策略">
<p>层内使用TP，层间使用PP，多组TP+PP之间使用DP。一般来说DP可以跨机，而TP和PP的通讯更多，应尽量避免跨机。</p>
<p>看下来自Bloom论文的图：</p>
<img src="/4cd8532f/bloom_3d.png" class title="并行策略">
<p>每个白色方框表示一块GPU，每组机器有48块GPU，每组都复制了一份模型完整参数。左侧表示数据并行DP，有8组机器，每组输入一批数据；右侧图的竖向示意了PP过程，有12行，模型横跨了这12行GPU，例如模型有48层，则每4层放在一行中；右侧图横向示意了TP过程，一行4块GPU，表示这一行的模型参数被平摊到4块GPU上。</p>
<p>看下DeepSpeed博客的版本：</p>
<p>下图是个三维的3D并行示意图。每种颜色表示一个节点，每个节点有4块GPU。上面16张卡和下面16张卡分别是一组，每组输入一份数据，这是数据并行。上面一组16张卡，假设模型有32
layer，一组GPU中每个节点存放8layer，每个节点的输出作为下一个节点的输入，例如GPU0的输出是GPU8的输入，这就是流水线并行。每个节点执行模型并行，意思是每个layer被分成了4分，放到一个节点的4个卡上。</p>
<img src="/4cd8532f/3D.png" class title="并行策略">
<p>下图是对上图的拓展示意。模型有32
layer，每8个layer放到一个节点，黄色框是一个节点，包含4个GPU。每个节点执行模型并行/张量并行，
MP-0、MP-1、MP-2、MP-3表示同一layer中的张量被切分成4份，分别放到4个GPU上。Rank
0 和Rank
1是数据并行。节点之间执行流水线并行，0<sub>7layer放在第一个节点，以此类推，最后的24</sub>31layer放到最后一个节点。</p>
<img src="/4cd8532f/3D_2.png" class title="并行策略">
<h1 id="序列并行">序列并行</h1>
<p>序列并行主要是解决LLM的输入数据长的问题。由于attention的计算复杂度是平方增长，中间激活值的量随着输入输出长度增长而暴增，naive
attention实现的情况下，比如10k长度的序列所需的显存是1k长度的100倍。</p>
<p>前面TP和PP都是切模型，而序列并行就是切数据。</p>
<p>主流的实现有这三种，对比一下：</p>
<table>
<colgroup>
<col style="width: 7%">
<col style="width: 30%">
<col style="width: 28%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th><strong>属性</strong></th>
<th><strong>Colossal-AI</strong></th>
<th><strong>Megatron-LM</strong></th>
<th><strong>DeepSpeed-Ulysses</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>核心目标</strong></td>
<td>突破序列长度限制，支持超长序列（如114K Token）</td>
<td>减少LayerNorm和Dropout的显存占用，优化张量并行下的显存效率</td>
<td>高效支持超长序列（百万Token）和大模型训练，结合ZeRO-3参数分片</td>
</tr>
<tr class="even">
<td><strong>通信机制</strong></td>
<td>环状通信（Ring
Self-Attention），分块传递QKV，All-Gather聚合结果</td>
<td>All-Gather和Reduce-Scatter聚合序列分片的中间激活值</td>
<td>All-to-All转置QKV矩阵，将序列分片转换为注意力头分片</td>
</tr>
<tr class="odd">
<td><strong>兼容性</strong></td>
<td>兼容数据并行、流水线并行、张量并行</td>
<td>主要与张量并行结合使用</td>
<td>与ZeRO-3和数据并行结合，支持FlashAttention优化库</td>
</tr>
<tr class="even">
<td><strong>无损性验证</strong></td>
<td>计算结果与单卡完全一致，实验验证Loss曲线和精度指标无差异</td>
<td>分布式与单卡输出的均方误差（MSE）为浮点误差量级（&lt;1e-7）</td>
<td>生成文本的困惑度（Perplexity）与单卡一致，数学等价性通过矩阵分块转置严格保证</td>
</tr>
</tbody>
</table>
<p>更详细的分析可以看这个：<a href="https://zhuanlan.zhihu.com/p/14665512019">LLM(31)：序列并行的典型方案与实现细节，https://zhuanlan.zhihu.com/p/14665512019</a>。</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- Agent：<br>
<a href="https://www.linsight.cn/b242bfb3.html">Agent完全手册(零)：三大模块，三个理念</a><br>
- MoE：<br>
<a href="https://www.linsight.cn/a9c496e3.html">DeepSeek-V3细节探索</a><br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/37ee84bb.html">Qwen3实测&amp;技术报告</a><br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="https://www.linsight.cn/6c0f6207.html">Qwen2.5-1M技术解析</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="https://www.linsight.cn/4519eadd.html">prompt压缩(一)</a><br>
<a href="https://www.linsight.cn/ea2871bf.html">prompt压缩(二)</a><br>
<a href="https://www.linsight.cn/bfa4f144.html">reasoning压缩(一)</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a href="https://www.linsight.cn/657a6d17.html">基模型Cognitive
Behaviors对RL的影响</a><br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="https://www.linsight.cn/2dee4921.html">LLM水印</a><br>
- 训练框架<br>
<a href="https://www.linsight.cn/fe0adaa5.html">LLM训练框架：从优化器和精度讲到ZeRO</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
<a href="https://www.linsight.cn/d253d7b3.html">关于The Bitter
Lesson</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
<a href="https://www.linsight.cn/b4d047c1.html">DeepSeek-VL2的细节</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】千亿参数开源大模型 BLOOM
背后的技术，https://zhuanlan.zhihu.com/p/615839149<br>
【2】图解大模型训练之：张量模型并行(TP)，Megatron-LM，https://zhuanlan.zhihu.com/p/622212228<br>
【3】大模型训练技术笔记总结，https://zhuanlan.zhihu.com/p/610139027<br>
【4】图解大模型训练之：流水线并行（Pipeline
Parallelism），以Gpipe为例，https://zhuanlan.zhihu.com/p/613196255<br>
【5】https://zzqq2199.github.io/2021/04/02/DAPPLE/ 【6】Megatron-LM:
Training Multi-Billion Parameter Language Models Using Model
Parallelism，https://arxiv.org/abs/1909.08053<br>
【7】LLM(31)：序列并行的典型方案与实现细节，https://zhuanlan.zhihu.com/p/14665512019</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>预训练</tag>
        <tag>分布式</tag>
        <tag>3D并行</tag>
      </tags>
  </entry>
  <entry>
    <title>最近阅读2-关于自适应深度思考、context engineering和模型训练</title>
    <url>/af7f9363.html</url>
    <content><![CDATA[<h1 id="自适应深度思考">自适应深度思考</h1>
<p>LLM加上深度思考之后，能够处理更为复杂的任务。但是深度思考的耗时长、成本高，并且不是所有输入都需要深度思考。</p>
<p>因此，一个思路是让模型根据输入的特性，使用不同的思考方式（无思考、短思考、长思考）。</p>
<h2 id="qwen3-分流">Qwen3 + 分流</h2>
<p>1、混合思考模式</p>
<p>Qwen3系列模型支持混合思考：可以进行深度思考，也可以不进行深度思考。不过是否进行深度思考需要手动设置。可以在apply_chat_template中设置enable_thinking（硬开关），也可以在每轮对话的user
input里使用临时设置（软开关），打开或者关闭深度思考。</p>
<p>虽然Qwen3模型本身没有自适应决定是否深度思考的功能，但是我们可以通过对输入query进行分流，配合深度思考的开关，达到「hard
case深度思考，easy case不思考」的效果。</p>
<p>2、思考预算</p>
<p>如果关闭了深度思考开关，Qwen3就进入了「完全不思考」状态。除了「完全不思考」，Qwen3还有「不完全思考」的能力。这个也是在混合思考的训练中获得的能力。</p>
<p>具体来说，用户可以设置模型的深度思考预算token数，比如1000，2000，4000。如果一个深度思考用完了预算给定的token量，那就会被强制结束深度思考（强制拼接上&lt;/think&gt;）输出最终结果，即使这时的深度思考还不完整。</p>
<p>模型最终的输出效果随着思考预算的提升而提升，因此我们可以根据需求，找一个效果和效率的平衡点。</p>
<h2 id="adacot">AdaCoT</h2>
<p>原文：《AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering
via Reinforcement Learning》</p>
<p>1、要解决的问题</p>
<p>在大模型深度思考的使用中，观察到：</p>
<ul>
<li>效率：对所有输入，模型无差别地使用长推理，这造成了浪费<br>
</li>
<li>效果：有深度思考比不深度思考，整体效果更好</li>
</ul>
<p>那么应该就存在一个较好的效率和效果的平衡：在减少一些推理的情况下，效果下降不多。</p>
<p>这个问题是一个Pareto-Optimal问题。具体来说，对于一个数据集，长思考的触发率T这么计算：</p>
<p><span class="math display">\[T(\theta) = \frac{1}{N} \sum_{i=1}^{N}
\mathbf{1}[\text{HasReasoning}(r_\theta(x_i))]\]</span></p>
<p>模型效果可用平均得分计算：</p>
<p><span class="math display">\[P(\theta) = \frac{1}{M} \sum_{j=1}^{M}
\text{Score}_j(\theta)\]</span></p>
<p>那所追求的帕累托最优的最终目标就是</p>
<p><span class="math display">\[\theta^* = \arg\max_\theta \{\lambda_P
\cdot P(\theta) - \lambda_T \cdot T(\theta)\}\]</span></p>
<p>其中 <span class="math inline">\(\lambda_P\)</span> 和 <span class="math inline">\(\lambda_T\)</span> 相当于是超参，更看重效果就增大
<span class="math inline">\(\lambda_P\)</span>，更看重效率就增大 <span class="math inline">\(\lambda_T\)</span>。</p>
<p>2、方法</p>
<p>论文中通过多阶段训练的方法，让模型习得自己决定要不要进行深度思考的能力。</p>
<p>（1）SFT</p>
<p>第一阶段是SFT，所用的数据中，部分query使用了深度思考，部分则直接出答案。文中用LLM
+ prompt，来对不同类型的数据是否需要深度思考进行判断。具体的标准：</p>
<table>
<colgroup>
<col style="width: 14%">
<col style="width: 55%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>类别</th>
<th>需触发深度思考的特征</th>
<th>无需触发思考的特征</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>推理复杂度</td>
<td>需多步推理/逻辑链/概念分解（如数学证明、策略分析）</td>
<td>直接回忆/常识问题（如事实查询、简单计算）</td>
</tr>
<tr class="even">
<td>知识深度</td>
<td>需跨领域知识整合/专业术语解释（如法律条款分析、医学诊断）</td>
<td>基础定义查询（如“水的沸点”）</td>
</tr>
<tr class="odd">
<td>解决方案性质</td>
<td>存在多路径/需创造性（如历史事件推演、开放式创作）</td>
<td>唯一标准答案（如“中国首都是？”）</td>
</tr>
<tr class="even">
<td>用户意图</td>
<td>隐含深层需求（如“分析经济危机成因”需推导）</td>
<td>明确检索需求（如“2025年春节日期”）</td>
</tr>
</tbody>
</table>
<p>和Qwen3一样，无论是否有触发深度思考，格式都是一致的，只是无思考时&lt;think&gt;和&lt;/think&gt;中间为空。</p>
<p>SFT数据中大约有67%的数据有深度思考。</p>
<p>（2）RL</p>
<p>第二阶段是RL，使用PPO。</p>
<p>奖励函数设计：</p>
<p><span class="math display">\[R(x,r) = R_{\text{base}}(x,r) - \alpha_1
\cdot P_{\text{miss}} - \alpha_2 \cdot P_{\text{over}} - \gamma \cdot
P_{\text{int}}\]</span></p>
<ul>
<li><span class="math inline">\(R_{\text{base}}\)</span>：基础质量奖励（如准确率）。<br>
</li>
<li><span class="math inline">\(P_{\text{miss}}\)</span>：应触发深度思考但未触发的惩罚。<br>
</li>
<li><span class="math inline">\(P_{\text{over}}\)</span>：不应触发但触发的惩罚。<br>
</li>
<li><span class="math inline">\(P_{\text{int}}\)</span>：格式错误惩罚。</li>
</ul>
<p><span class="math inline">\(\alpha_1\)</span>
增大强调了使用深度思考的重要性，相当于提升了 <span class="math inline">\(\lambda_P\)</span>；而 <span class="math inline">\(\alpha_2\)</span> 强调了效率的重要性，相当于提升了
<span class="math inline">\(\lambda_T\)</span>。</p>
<p>RL训练过程中，发现在一些领域比如数学，决策边界会出现崩溃：倾向于都思考或者都不思考，因此一个小技巧是对&lt;think&gt;后的第一个token不计算loss，这样可以保留SFT的思考比例，避免决策边界崩溃。</p>
<p>（3）Meta-reasoning</p>
<p>除了让模型自己决策要不要深度思考，也可以结合外部判断，比如对于复杂的case，在&lt;think&gt;后预先加入判断，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;think&gt;此问题需结合历史背景和人物心理：1.玄武门之变是权力斗争转折点 2.李世民需平衡道德与政治需求</span><br></pre></td></tr></table></figure>
<p>让模型在这个prefix上继续回答，整体效果有进一步的提升。</p>
<h2 id="adaptthink">AdaptThink</h2>
<p>原文：《AdaptThink: Reasoning Models Can Learn When to Think》</p>
<p>1、背景 &amp; 思路</p>
<p>AdaptThink和AdaCoT的观察类似：容易的case不思考效果更好，困难的case还是需要深度思考。思路上也是想办法让模型学会做二元决策：要不要使用深度思考。目标是保证准确率的情况下，尽量减少思考率。</p>
<p>2、方案</p>
<p>（1）RL</p>
<p>AdaptThink通过强化学习（PPO）来优化思考决策。主要是在PPO的advantage上，加上一项无思考的奖励δ
* 𝟙(y₁=&lt;/think&gt;)：</p>
<p>A(x,y) = δ * 𝟙(y₁=&lt;/think&gt;) + R(x,y) - R_ref(x)</p>
<p>（2）重要性采样</p>
<p>由于起始模型是完全深度学习的模型，自己很难探索到不进行深度思考的方法，因此在训练初期手动强制无思考比例为50%：强制第一个生成token为&lt;/think&gt;即为无思考，以此解决冷启动的探索问题。</p>
<p>（3）效果</p>
<p>论文用DeepSeek-R1-1.5B和DeepSeek-R1-7B模型在GSM8K/MATH/AIME上实验，token数下降了一半，效果还略略提升了（减少了过度思考带来的错误）。</p>
<h1 id="context-engineering">context engineering</h1>
<p>之前在<a href="https://mp.weixin.qq.com/s/GQC-7AmRPeW6p07cf5SEEg">Devin和Anthropic的Agent开发经验</a>提到context
engineering，这个概念越来越受重视了。</p>
<h2 id="langchainthe-rise-of-context-engineering">LangChain：The rise of
"context engineering"</h2>
<p>原文：<a href="https://blog.langchain.com/the-rise-of-context-engineering/">https://blog.langchain.com/the-rise-of-context-engineering/</a></p>
<p>prompt engineering可以视为context
engineering的一个子集。prompt工程对于简单的任务效果可以，但是如果要解决真正长期、复杂的问题，就要升级到context
engineering。</p>
<p>context engineering相比prompt engineering的主要变化有几点：</p>
<ul>
<li>系统性：提供给模型的信息更丰富，更系统，包括长短期的记忆信息、工具库、用户状态还有开发者信息等。<br>
</li>
<li>动态性：不同的情况和输入需要不同组合的context，而不是固定不变的模板。<br>
</li>
<li>结构化表达：如同人类沟通，信息格式决定理解效果。简明的错误描述远优于冗长的JSON块，工具参数的规范化设计同样关键。</li>
</ul>
<h2 id="factor-agents">12 Factor Agents</h2>
<p>原文：https://github.com/humanlayer/12-factor-agents/tree/main</p>
<p>从agent落地开发的角度给出的12个建议。</p>
<p>Factor 1: Natural Language to Tool Calls</p>
<p>把文字描述，转为结构化的工具。比较模型学习很数学和代码的推理能力，而结构化的输入更适合推理。</p>
<p>Factor 2: Own your prompts</p>
<p>虽然很多框架都提供了prompt，但是，最好还是自己写。</p>
<p>Factor 3: Own your context window</p>
<blockquote>
<p>Everything is context engineering.</p>
</blockquote>
<blockquote>
<p>Creating great context means: - The prompt and instructions you give
to the model - Any documents or external data you retrieve (e.g. RAG) -
Any past state, tool calls, results, or other history - Any past
messages or events from related but separate histories/conversations
(Memory) - Instructions about what sorts of structured data to
output</p>
</blockquote>
<p>简单来说就是提供更多的信息，其实就是context engineering的思路。</p>
<img src="/af7f9363/context_eng.png" class title="论文阅读2">
<p>Factor 4: Tools are just structured outputs</p>
<p>别把工具搞太复杂了，只要能结构化就行，不一定要是复杂的json schema --
可以多试试不同的结构。</p>
<p>Factor 5: Unify execution state and business state</p>
<p>execution state 和 business state
不必像传统app那样分得清清楚楚。其实从现在大部分的agent产品也能看到，内部的处理逻辑也经常暴露。</p>
<p>Factor 6: Launch/Pause/Resume with simple APIs</p>
<p>agent本质是一个程序，应当可以被用户、其他agent或者其他工具简单地启动和停止
-- 不要跑就停不下来无法控制。</p>
<p>Factor 7: Contact humans with tool calls</p>
<p>无论是否调用工具，让agent保持结构化的输出。</p>
<p>Factor 8: Own your control flow</p>
<p>掌握对agent全流程的控制，以方便实现：</p>
<ul>
<li>工具调用结果的摘要或缓存<br>
</li>
<li>对结构化输出使用LLM作为评判器<br>
</li>
<li>上下文窗口压缩或其他内存管理<br>
</li>
<li>日志记录、追踪和指标统计<br>
</li>
<li>客户端速率限制<br>
</li>
<li>持久化休眠/暂停/"等待事件"<br>
</li>
<li>等</li>
</ul>
<p>Factor 9: Compact Errors into Context Window</p>
<p>把流程中的错误信息，也在context
window上流转，这样的信息一定程度上让agent具备自我修复能力。</p>
<p>Factor 10: Small, Focused Agents</p>
<p>与其构建大而全的万能agent，不如开发小而精的专用agent，让每个agent专注做好一件事。</p>
<p>这条个人保留意见。短期内是这样，但是长期就未必了。</p>
<p>Factor 11: Trigger from anywhere, meet users where they are</p>
<p>允许用户通过Slack、电子邮件、短信或任何他们偏好的渠道触发agent，并支持agent通过相同渠道进行响应。</p>
<p>这个也是从产品角度出发，把agent打造成和人一样的存在。</p>
<p>Factor 12: Make your agent a stateless reducer</p>
<p>把agent看作简单的for循环的话，就能明白「无状态」。</p>
<h1 id="模型训练">模型训练</h1>
<h2 id="octothinker">OctoThinker</h2>
<p>原文：《OctoThinker: Mid-training Incentivizes Reinforcement Learning
Scaling》</p>
<p>1、背景</p>
<p>和《Cognitive Behaviors that Enable Self-Improving Reasoners, or,
Four Habits of Highly Effective
STaRs》的观察类似，OctoThinker发现Llama系列在RL之后效果不好，远不如Qwen系列。</p>
<p>论文对预训练-&gt;RL的数据适配做了探索。</p>
<p>2、工作</p>
<p>（1）Mid-training</p>
<p>目前的认知是RL提升的是模型的信心，而不是能力。模型的推理能力更多还是在预训练中获得。那么在RL之前，增加包含高质量数据的mid-training。</p>
<p>阶段1：200B高质量推理数据（85% MegaMath-Web-Pro-Max + 15% 代码/QA
数据）。经过这个训练之后，基座模型数学能力提升 10–20%（如 MATH500 从 7.4
→ 22.4），为后面的RL提供更好的起点。</p>
<p>阶段2：注入多样化推理行为，适配 RL
目标。具体来说，在阶段1的基础上，用三种不同的数据训练三个分支模型：</p>
<table>
<colgroup>
<col style="width: 13%">
<col style="width: 62%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th><strong>分支</strong></th>
<th><strong>数据配方</strong></th>
<th><strong>特点</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Long</strong></td>
<td>长链思维（CoT）数据为主（OpenR1-Math + AM-DeepSeek）</td>
<td>深度推理，易冗长但效果强</td>
</tr>
<tr class="even">
<td><strong>Short</strong></td>
<td>短链 QA 数据为主（MegaMath-QA + OpenMathInstruct2）</td>
<td>简洁响应，稳定性高</td>
</tr>
<tr class="odd">
<td><strong>Hybrid</strong></td>
<td>混合长短链数据（OpenMathInstruct2 + NuminaMath1.5 + OpenR1）</td>
<td>平衡深度与稳定性</td>
</tr>
</tbody>
</table>
<p>（2）RL</p>
<p>在Long/Short/Hybrid三个分支基础上，分别进行RL，得到OctoThinker-{Long/Short/Hybrid}。各个分支有不同的表现：</p>
<ul>
<li><strong>Long 分支</strong>：复杂任务（如
MATH）最强，但长度较长。<br>
</li>
<li><strong>Short 分支</strong>：简单任务（如 GSM8K）高效稳定。<br>
</li>
<li><strong>Hybrid 分支</strong>：综合性能最佳。</li>
</ul>
<p>OctoThinker-Long-3B 经 RL 后达到 Qwen2.5-3B 同等水平（MATH500：65.2
vs. 66.4）。</p>
<p>（3）结论</p>
<p>Qwen 预训练数据（高质量数学+对齐的 QA 分布）天然更适配 RL，而 Llama
需通过中期训练弥补。</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- Agent：<br>
<a href="https://www.linsight.cn/b242bfb3.html">Agent完全手册(零)：三大模块，三个理念</a><br>
<a href="https://www.linsight.cn/44c62dc5.html">DeepResearch的报告生成方法</a><br>
<a href="https://www.linsight.cn/7c2f9dcb.html">从RAG到DeepSearch</a><br>
<a href="https://www.linsight.cn/226b059f.html">agent调研(1)--MetaGPT,OpenManus和OWL</a><br>
<a href="https://www.linsight.cn/f93b3aaf.html">Devin和Anthropic的Agent开发经验</a><br>
- MoE：<br>
<a href="https://www.linsight.cn/a9c496e3.html">DeepSeek-V3细节探索</a><br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/37ee84bb.html">Qwen3实测&amp;技术报告</a><br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="https://www.linsight.cn/6c0f6207.html">Qwen2.5-1M技术解析</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="https://www.linsight.cn/4519eadd.html">prompt压缩(一)</a><br>
<a href="https://www.linsight.cn/ea2871bf.html">prompt压缩(二)</a><br>
<a href="https://www.linsight.cn/bfa4f144.html">reasoning压缩(一)</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a href="https://www.linsight.cn/657a6d17.html">基模型Cognitive
Behaviors对RL的影响</a><br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="https://www.linsight.cn/2dee4921.html">LLM水印</a><br>
- 训练框架<br>
<a href="https://www.linsight.cn/fe0adaa5.html">LLM训练框架：从优化器和精度讲到ZeRO</a><br>
<a href="https://www.linsight.cn/4cd8532f.html">LLM训练各种并行策略</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
<a href="https://www.linsight.cn/d253d7b3.html">关于The Bitter
Lesson</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
<a href="https://www.linsight.cn/b4d047c1.html">DeepSeek-VL2的细节</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepResearch的报告生成方法</title>
    <url>/44c62dc5.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>最近搞DeepResearch。</p>
<p>DeepResearch的任务是为用户提供一份全面深入的研究报告。</p>
<p>列一些典型的用户query：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">做一份端午从上海去东京的五天四夜购物攻略，预算八千</span><br><span class="line">洛阳龙门石窟和老君山三天两夜文化摄影攻略</span><br><span class="line">深入研究一下DeepResearch技术今年的发展趋势</span><br><span class="line">三胎政策在成都改善型住房需求中的刺激效果，并预测一下明年四室户型供需缺口</span><br></pre></td></tr></table></figure>
<p>都是一些较为复杂的，需要多步拆解处理的任务。用户中比较热门的任务类型包括「旅游攻略」，「专业研报」（如技术说明，专业分析："整理下MoE模型的演进过程"）还有「信息整合」（比如给出一份市面上20w以内的电车的对比资料）。</p>
<p>可以粗暴认为DeepResearch主要就由DeepSearch +
报告生成这两大模块组成。当然这个过程还可以由planner +
reflect循环调度。</p>
<p>这篇略过DeepSearch，先看看「报告生成」的模块。</p>
<p>假设我们已经有了比较合理、丰富的搜索结果了，那报告要怎么生成呢？</p>
<h1 id="报告的特点">报告的特点</h1>
<p>1、长度较长</p>
<p>DeepResearch的报告首先长度是比较长的，一般至少在几千个token，甚至上万token或者更长，具体就取决于话题和任务的复杂度。</p>
<p>2、图文并茂</p>
<p>除了大量的文字，报告还应该是图文并茂的。这里的「图」包含图片和数据图表（比如折线图、扇形图、表格等）。</p>
<p>3、排版和格式</p>
<p>为了提供给用户提供更好的阅读体验，报告应该支持比较好的排版。具体的排版就和输出报告的格式有关，常用的就是html、pdf、ppt。</p>
<p>比如我用coze空间做一份旅游攻略，prompt是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">给我做一份端午节出国旅行的攻略，东南亚，两个人，悠闲一点，预算6000</span><br></pre></td></tr></table></figure>
<p>跑了差不多半个小时之后就获得了一份html版本的图文攻略：</p>
<img src="/44c62dc5/dr_example.jpg" class title="研报">
<p>上面这个图还没截完整，后面还有好几个不同国家的出行方案。</p>
<h1 id="deepsearch的结果">DeepSearch的结果</h1>
<p>DeepSearch是报告生成的起点，先看下它都提供了什么。</p>
<p>1、general search</p>
<p>目前DeepSearch的结果主要是一系列网页搜索的结果，每个网页包含以下字段：</p>
<ul>
<li>title：网页标题<br>
</li>
<li>content：完整正文的「文本」内容<br>
</li>
<li>url：原文链接<br>
</li>
<li>summary：正文的简短总结<br>
</li>
<li>images：网页中的图片列表，包含图片的url和对应的caption</li>
</ul>
<p>其中title、content和url是常规的网页搜索结果字段，就不说了。</p>
<p>summary是额外添加的，在报告生成的处理逻辑里，对于不需要网页细节内容的部分，就可以使用summary进行处理，从而减少处理的token，节省时间和成本。</p>
<p>个人认为，成本问题是DeepResearch一个很重要的方向。如果DeepResearch要向大众推广，那么开发过程中，60%以上的时间都会在考虑怎么节省成本。</p>
<p>图片的caption也是搜索到网页后增加的，用于后续在报告中添加图片。</p>
<p>2、domain search</p>
<p>除了general search，还会有一些常用场景需要的搜索源，比如：</p>
<ul>
<li>导航工具：用于获取特定地点之间的交通信息，包括驾车、航班和火车，一般旅游攻略对这个有强需求。<br>
</li>
<li>美食工具：获取美食的价格评分和地点还有评价，也是旅游场景的所需要的。</li>
</ul>
<p>除了用现有工具，也可以针对自有数据建设向量搜索。</p>
<p>这些都可以整合成general
search的结果格式：标题、正文、摘要，url和图像是optional的。</p>
<h1 id="step1文字版的初稿">step1：文字版的初稿</h1>
<p>从搜索结果到最终报告，中间需要多个步骤。（有没有大佬已经在做一步端到端的生成？把所有数据都塞给模型，要求一步到位生成图文结果。目前这样做的效果比较差，模型窗口长度限制也是个问题）</p>
<p>报告的目标格式一般包含选择html、pdf和ppt，这些格式用户使用起来比较熟悉。</p>
<p>转成目标格式之前，首先要生成一份逻辑通顺，行文流畅，内容完整，包含文字和图片的初稿。</p>
<p>而这个初稿的生成又分为「生成文字」和「增加图像」两个阶段。</p>
<p>第一步我们就是要获得文字版本的初稿。</p>
<p>这里选择用markdown格式来生成文字初稿。因为markdown格式比较简单（能少用点token），模型生成的效果也好，支持多级的标题，公式以及图片的插入，基本能够满足我们的需求。</p>
<h2 id="直接生成的问题">直接生成的问题</h2>
<p>这个文字版本也没法一步直接生成。稍微讲一下直接生成的问题：</p>
<ul>
<li>搜索结果太多，假设一个网页平均有1000 token，那100个搜索结果就要100k
token，已经超过或者接近很多模型的窗口上限了；参考秘塔AI，经常出现100个200个甚至更多的网页引用，所以100k级别的输入并不会是少见的情况。<br>
</li>
<li>即使模型的窗口可以接受这个长度或者更长的输入，也容易出现lost in the
middle的情况；对于需要使用到原文细节信息的情况（比如旅行规划中的车次/航班号，出发时间，或者经济研报中多个地区多个维度的数据），要在大量的文本中准确捞到正确的内容是一个容易出错的事情。<br>
</li>
<li>目前大部分模型支持的生成长度在2k到4k，在更大长度的内容输出上，容易出现截断。</li>
</ul>
<h2 id="大纲生成">大纲生成</h2>
<p>直接生成会遇到问题，那么更好一点的做法是先生成文档的大纲（即各级标题），再根据大纲去填充细节。</p>
<p>生成大纲这一步就可以用上搜索结果中的summary了，因为生成大纲并不需要关注太多细节。</p>
<p>比如在制定旅游攻略的任务下，我们搜索到的内容基本可以分为交通、住宿、美食、景点、通讯等，我们只要让模型根据搜索内容的summary指定report的大纲就可以了。类似地，研究NLP深度模型的发展模型也可以根据搜索结果分为embedding模型、Bert时代、GPT时代、Agent等。</p>
<p>假设一个summary是30个token，那么即使有200个搜索结果，长度也只有6k
token，模型可以轻松处理。</p>
<p>生成大纲时，也有一些细节限制：</p>
<ul>
<li>要限定每级标题的数量，防止模型生成过多，并且限定标题级别数量，比如最多只能使用到3级标题。<br>
</li>
<li>要求各级标题之间尽量不要有overlap。<br>
</li>
<li>标题要起得明确清晰，让人单独看到这个标题也知道是什么意思（因为这些标题在设计上可能是要在报告完成前，展示给用户看的）。</li>
</ul>
<p>举一个例子。输入query =
"上海至东京国庆购物攻略：8000元预算五天四夜经济型方案"</p>
<p>制订的大纲各级标题是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[&quot;一、行程规划与交通安排\n1.1 机票选择策略\n1.2 机场至市区交通方案&quot;,</span><br><span class="line">&quot;二、住宿选择与区位分析\n2.1 银座商圈高性价比酒店\n2.2 新宿商圈经济型住宿&quot;,</span><br><span class="line">&quot;三、购物商圈深度攻略\n3.1 银座高端购物路线\n3.2 新宿平价消费指南\n3.3 表参道特色品牌挖掘&quot;,</span><br><span class="line">&quot;四、预算分配与消费控制\n4.1 8000元预算分解模型\n4.2 免税政策与退税实操&quot;,</span><br><span class="line">&quot;五、行程优化建议\n5.1 交通卡券组合方案\n5.2 错峰购物时段建议&quot;]</span><br></pre></td></tr></table></figure>
<p>上面这个例子共有5个一级标题，也就是5个大的chapter。</p>
<p>大纲格式也可以自行设计，结构化的也可以，只要模型能准确遵循就行。</p>
<p>这一步里其实有很多细节可以优化，比如传给LLM的搜索结果的排序和筛选，或者利用多次采样再合并获取更合理的大纲等。</p>
<h2 id="填充细节">填充细节</h2>
<p>得到大纲的标题之后，就要根据搜索结果填充每个chapter的细节。</p>
<p>这里可以并行来做：每个chapter调一个模型来填充细节。</p>
<p>prompt是类似这样的（简化版）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">你需要根据用户的要求，和文档的大纲，完成分配给你的章节的撰写。</span><br><span class="line"></span><br><span class="line">你需要根据搜索结果来完成这一章节。</span><br><span class="line"></span><br><span class="line">用户query: &#123;query&#125;</span><br><span class="line"></span><br><span class="line">大纲: &#123;outline&#125;</span><br><span class="line"></span><br><span class="line">分配给你的章节: &#123;chapter&#125;</span><br><span class="line"></span><br><span class="line">搜索结果: &#123;search_results&#125;</span><br></pre></td></tr></table></figure>
<p>前面分析了，全量的搜索结果过多，一起都塞给模型，可能导致结果不佳，成本也高。因此在这一步也不宜直接把所有搜索结果扔给模型去完成细节的编写，而是先从搜索结果里找到和当前要写的这个章节相关的条目。</p>
<p>比如在旅游规划任务下，有一个chapter是交通相关的内容。200个搜索结果里有40个涉及了飞机火车的班次信息，以及景点之间的交通工具推荐。那么在写这一个chapter的时候，就只需要给模型输入这40个搜索结果，而不需要200个搜索结果都给。</p>
<p>那怎么找到相关搜索条目呢？可以用BGE或者小的LLM给每个文档做一个打分或者匹配，以此筛选搜索结果。也可以在生成大纲的时候就要求模型把对应的条目编号和标题一同给出。</p>
<p>这一步同样有很多细节可以优化，比如：</p>
<ul>
<li>如果觉得以一级标题进行搜索结果匹配还是有太多结果，那可以进行二级或者三级标题的匹配，把章节拆得更细，从而减少每个章节编写的难度。<br>
</li>
<li>为了方便编写细节的模型理解，可以在生成大纲的时候增加一个长一点的解释，限定这一章需要补充的信息。<br>
</li>
<li>把章节细节的编写也设计成迭代的模型，逐步完善。</li>
</ul>
<p>值得单独拎出来说的，是关于字母和数字的细节。涉及字母和数字的通常是比较严谨的信息，比如火车/航班的班次，出发/到达时间，或者路途的公里数，开车所需的时间和住宿价格等。一方面，这些内容错一个字母或者数字就会给用户带来比较大的困惑，另一方面，数字通常涉及计算，而LLM的"口算"并不是很可靠。针对这些问题，可以额外添加一个利用计算器或者python代码验证字母和数字的环节，并把结果提供给章节编写的模型，从而减少计算错误和幻觉带来的问题。</p>
<p>最后，记得让模型给出reference，用于展示给用户。</p>
<h1 id="step2图文报告">step2：图文报告</h1>
<p>上面这几步做完之后，就有一个纯文本的report初稿了。但是呈现给用户，光有字不够，还得有图。</p>
<h2 id="图的类型">图的类型</h2>
<p>report里都有什么图？先来分个类。</p>
<p>1、来自检索结果（网页）的图</p>
<p>检索结果中包含一些可以直接使用的图片，这些图片可以直接插入到report的适当位置。</p>
<p>一种是如旅游景点的风景图，地标建筑照片等。这一类图片的特点是，插入到report时，在准确度上的要求相对比较低，只要别出现明显的图文不匹配（比如文字在介绍山，但是图片是海景），都还可以接受。</p>
<p>另外，也有可能出现对准确度有一些要求的情况，比如路线导航，车次的信息表。这类信息如果出错（火车的章节配了个航班的图）可能就会让用户的体验大打折扣。</p>
<p>再进一步，比如对于经济调研的研报，那么就有可能出现很多折线图、柱状图、扇形图或者信息密集的表格，这种图表每个字母每个数字都很重要，不能出错，不能和文本的信息对不上。</p>
<p>这些来自检索结果文档的图片，插入report的关键在于 -
要用对图，比如搜索的时候有可能搜到有矛盾的信息，那么LLM在总结完文本之后，我们需要知道应该用哪些文档的图片，不应该用哪些文档的图片
- 插对位置，这就要求我们知道每张图片的主要信息是什么</p>
<p>2、从其他来源获得的图</p>
<p>有些时候搜索结果文档里只有文字，或者文档中的图不是我们想要的图，那我们就可能需要根据用户需求和文本报告内容，自己从另外的来源获取合适的图。</p>
<p>（1）来源1：自己画数据图表</p>
<p>如果report中有一系列数据，比如某地不同月份的温度，或者不同厂商的市场占比，那么这些数据就可以生成图表，方便用户直观阅读。比如不同月份的温度可以画成折线图，不同厂商的占比可以画成扇形图。根据数据的类型，也可以制成柱状图、表格或者其他图表。</p>
<p>（2）来源2：图片搜索接口</p>
<p>假设我们在给用户制作旅游攻略的时候，查到有一处古镇适合游玩，我们想把这个古镇的资料作为攻略的一部分进行介绍，但是恰好搜到的网页只有文字，那么我们可以在制作report的时候，拿这个古镇的文字介绍去搜索图片，然后把搜到的图片插入到report中。</p>
<p>3、各种图的难度</p>
<p>上面分出来的这几种图片和图表，按开发难度排个序：</p>
<ul>
<li>level 1：常规的图表生成，如折线图、柱状图、表格等<br>
</li>
<li>level 2：插入来自文档的图片和图表<br>
</li>
<li>level 3：插入来自其他搜索源的图</li>
</ul>
<h2 id="加入图的方法">加入图的方法</h2>
<p>先说下插入「来自文档的图片」的方法。大致的思路就是和之前在<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a>中介绍的InternLM-XComposer类似。</p>
<p>InternLM-XComposer生成图文并茂文档的做法是这样的：</p>
<ul>
<li>（1）生成纯文本文档<br>
</li>
<li>（2）找到文本结果中可以/需要插入图像的位置，并生成对应的caption<br>
</li>
<li>（3）用上一步的caption进行图像检索，获得候选，并选择最符合的图像，获得最终结果</li>
</ul>
<p>稍微有点不同的是，InternLM-XComposer由于图片库比较大，所以它的做法是“假设某个位置需要图，并生成这张假想的图的caption”，然后根据这个caption去图库里找。</p>
<p>而在我们这个report生成的场景下，我们的图片库相对比较小。假设我们平均每个章节用到了30个搜索结果，每个搜索结果平均有3张图，那么我们的图库就有90张。如果按InternLM-XComposer的做法，很难在这么小的图库里找到对应的图，因此我们反过来，先跑出图库所有的图的caption，再把这些caption都提供给LLM，让模型来决定在哪里可以插入哪些图片。</p>
<h2 id="图表生成">图表生成</h2>
<p>要生成图表，一个方法是要求模型在report中包含数字的地方，判断是否适合插入图表，适合插入什么图表，然后调用工具或者写python代码生成图表，最后把生成结果贴到对应位置上就行。</p>
<p>而如果报告的目标格式是html，那么也可以在生成html的prompt中，直接要求模型判断和插入图表，html
+ css基本可以所有我们想要的图表。</p>
<h2 id="其他搜索源的图">其他搜索源的图</h2>
<p>假设我们在旅游攻略的展示策略上，要求一定要有足够的景点图，而搜索文档中又刚好没有符合要求的，那我们可以单独去搜索我们想要的图。</p>
<p>首先我们需要知道搜什么图。prompt可能是类似这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">你是一个配图专家，你的任务是给文本配上合适的图。</span><br><span class="line"></span><br><span class="line">你可以调用图片搜索工具，并利用关键字进行图片搜索。</span><br><span class="line"></span><br><span class="line">&#123;工具description&#125;</span><br><span class="line"></span><br><span class="line">请根据一下的文本，给出工具调用名称和关键词：</span><br><span class="line"></span><br><span class="line">&#123;chapter&#125;</span><br></pre></td></tr></table></figure>
<p>这部分的逻辑相对来说就比较定制化了。</p>
<h1 id="报告的格式">报告的格式</h1>
<p>报告常用的格式就是html，ppt和pdf了。其中html和ppt都可以转pdf，所以理论上只要支持html和ppt就可以了。</p>
<p>1、html</p>
<p>之前发现html的生成有一个不错的工具叫deepsite，<a href="https://enzostvs-deepsite.hf.space/">https://enzostvs-deepsite.hf.space/</a>。可以根据输入prompt直接生成漂亮的页面。后来发现后台其实就是DeepSeek。</p>
<p>试了在DeepSeek-R1和DeepSeek-V3上要求直接根据文案生成网页，效果不错，而且V3的效果比R1更好。前几天又发现Qwen
Chat也专门针对WebDev做了优化，Qwen3能够直接给出比较好的网页设计了。</p>
<p>随便给V3输了一组数据，生成的网页就挺漂亮的：</p>
<img src="/44c62dc5/v3_html.png" class title="研报">
<p>不过直接大模型生成html目前也有一些问题：</p>
<ul>
<li>对指令的遵循会比较差，容易出现幻觉，比如上面这个图，下面那行字就是模型自己加的。<br>
</li>
<li>复杂的页面设计，html代码很长，生成时间很久，还容易出现截断。</li>
</ul>
<p>2、ppt</p>
<p>ppt的生成就得靠专业的接口了，这个头部的几家AI公司都有这个能力。</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- Agent：<br>
<a href="https://www.linsight.cn/b242bfb3.html">Agent完全手册(零)：三大模块，三个理念</a><br>
- MoE：<br>
<a href="https://www.linsight.cn/a9c496e3.html">DeepSeek-V3细节探索</a><br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/37ee84bb.html">Qwen3实测&amp;技术报告</a><br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="https://www.linsight.cn/6c0f6207.html">Qwen2.5-1M技术解析</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="https://www.linsight.cn/4519eadd.html">prompt压缩(一)</a><br>
<a href="https://www.linsight.cn/ea2871bf.html">prompt压缩(二)</a><br>
<a href="https://www.linsight.cn/bfa4f144.html">reasoning压缩(一)</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a href="https://www.linsight.cn/657a6d17.html">基模型Cognitive
Behaviors对RL的影响</a><br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="https://www.linsight.cn/2dee4921.html">LLM水印</a><br>
- 训练框架<br>
<a href="https://www.linsight.cn/fe0adaa5.html">LLM训练框架：从优化器和精度讲到ZeRO</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
<a href="https://www.linsight.cn/d253d7b3.html">关于The Bitter
Lesson</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
<a href="https://www.linsight.cn/b4d047c1.html">DeepSeek-VL2的细节</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】https://mp.weixin.qq.com/s/iPJ7eLa3O6zILXi1HESkCQ</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>Agent</tag>
        <tag>DeepResearch</tag>
      </tags>
  </entry>
  <entry>
    <title>Agent完全手册(零)：三大模块，三个理念</title>
    <url>/b242bfb3.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>打算把agent相关内容拉出来专门写一个系列，持续更新。</p>
<p>作为第零篇，先看看Google的Agents白皮书和Anthropic开发者在agent开发上的一些经验。这里只整理一些关键的信息和自己的理解；更详细具体的内容可以看原文。</p>
<h1 id="google-agents-whitepaper">Google Agents whitepaper</h1>
<p>原文链接：<a href="https://archive.org/details/google-ai-agents-whitepaper">https://archive.org/details/google-ai-agents-whitepaper</a></p>
<p>白皮书内容主要分三部分：</p>
<ul>
<li>what is an agent，这部分是对agent的一个大致定义和理解<br>
</li>
<li>Tools，工具这块单独拉出来说，白皮书顺便也给G家的工具做了一下广告<br>
</li>
<li>实践的例子，用LangChain和Vertex做的agent，Vertex也是G家的</li>
</ul>
<h2 id="what-is-an-agent">what is an agent</h2>
<blockquote>
<p>In its most fundamental form, a Generative AI agent can be defined as
an application that attempts to achieve a goal by observing the world
and acting upon it using the tools that it has at its disposal.</p>
</blockquote>
<p>现阶段，粗暴地说agent就是LLM +
工具构成的一个能和环境交互并完成任务的系统。</p>
<p>一个典型的（Generative AI）agent可以分成三大块：</p>
<ul>
<li>Model：系统的思考中枢<br>
</li>
<li>Tools：agent的“手脚”，提供专业的处理能力，以及和环境交互的桥梁<br>
</li>
<li>Orchestration：编排层，主责协调和管理系统中各个组件的交互和协同</li>
</ul>
<img src="/b242bfb3/wp_agent.png" class title="agent_0">
<p>模型：现在可用的强大LLM已经很多，基本上开箱即用，不训练也是可以的。当然有些模型没有针对function
call等工具调用能力做过优化，如果是涉及到较多function
call，甚至有几百个上千个接口（这个数量已经足够构成一个单独的垂域了），那么针对场景进行一定的优化正常来说还是有收益的。</p>
<p>工具：现在最火的就是MCP了，基本上也是开箱即用。如果有些私有工具，那么也可以按照MCP的方案套一层就行了。</p>
<p>那么我们在开发的时候，操作空间比较大的应该就是在orchestration
layer。</p>
<blockquote>
<p>The orchestration layer describes a cyclical process that governs how
the agent takes in information, performs some internal reasoning, and
uses that reasoning to inform its next action or decision.</p>
</blockquote>
<p>简单来说编排层决定了模型获取信息和决策的方式。Orchestration中这个loop可长可短，具体取决于任务的难度和模型、工具的能力。</p>
<p>对比一下model和agent，分别从知识范围、推理模式、工具能力和逻辑编排这几个维度来看：</p>
<img src="/b242bfb3/wp_model_agent.png" class title="agent_0">
<ul>
<li>知识范围上，显然agent相比model有更动态&amp;更加广泛的知识范围<br>
</li>
<li>推理模式上，model一般是单次的，而agent能够和整个系统的其他模块进行多次交互<br>
</li>
<li>工具能力上，agent能够调用外部工具完成任务<br>
</li>
<li>逻辑编排上，agent有多种reasoning框架可以使用：CoT、ReAct等，当然model也能用，但是一般只能以prompt的形式单次线性地执行</li>
</ul>
<h2 id="cognitive-architectures-how-agents-operate">Cognitive
architectures: How agents operate</h2>
<p>Orchestration是cognitive
architecture的核心，负责管理agent的记忆，状态，思考和规划。而记忆状态思考规划又都和prompt
engineering息息相关。基于prompt来管理和规划的方法目前比较主流的方法有：</p>
<ul>
<li>ReAct<br>
</li>
<li>CoT<br>
</li>
<li>ToT<br>
等</li>
</ul>
<p>当然PE发展很快，还有很多其他方法。利用这些方法，agent能够自主决定下一步应该干什么。</p>
<p>prompt像是给agent提供了一个战略，在这个战略下，模型自发地根据当前情况设计具体战术。</p>
<h2 id="工具">工具</h2>
<p>谷歌的框架里，模型能够与之交互的主要工具类型有三种：</p>
<ul>
<li>Extensions，扩展程序<br>
</li>
<li>Functions，函数<br>
</li>
<li>Data stores，数据存储</li>
</ul>
<p>Extensions对实际执行任务的API进行了一层封装，同时能够提供说明和样例，是模型和环境直接交互的桥梁。</p>
<img src="/b242bfb3/wp_extension.png" class title="agent_0">
<img src="/b242bfb3/wp_extension2.png" class title="agent_0">
<p>而Functions和extensions相比，有两点主要的区别：</p>
<ul>
<li>1、模型给出function的调用命令和参数，但是并不实际执行<br>
</li>
<li>2、extension的执行在agent-side，而function在client side</li>
</ul>
<img src="/b242bfb3/wp_function.png" class title="agent_0">
<p>那么什么情况下会选择用function函数而不选择extension呢？举几个例子：</p>
<ul>
<li>API的调用需要在程序的另一层进行，而不是agent架构<br>
</li>
<li>安全或身份验证限制阻止agent直接调用API<br>
</li>
<li>API的调用涉及(人工)审核<br>
</li>
<li>调用中包含额外定制的业务逻辑</li>
</ul>
<p>最后还有一个Data stores，通常是向量数据库之类的。</p>
<p>几个工具的总结：</p>
<img src="/b242bfb3/wp_tools.png" class title="agent_0">
<h2 id="enhancing-model-performance-with-targeted-learning">Enhancing
model performance with targeted learning</h2>
<p>现在模型的通用能力都很强了，不过用到agent上，还是有可能出现一些场景和工具，超过了模型的预训练范围，那么就需要通过一些方法提升模型的领域能力。这些方法包括但不限于：</p>
<ul>
<li>In-context learning：推理的时候加上few-shot example<br>
</li>
<li>Retrieval-based in-context
learning：根据输入query，搜索一些最相关的工具和样例<br>
</li>
<li>Fine-tuning based learning：直接训练内化</li>
</ul>
<h1 id="anthropicbuilding-effective-agents">Anthropic：Building
effective agents</h1>
<p>Anthropic这篇博客介绍一些实际的agent经验，更加接地气一点。</p>
<p>原文链接：<a href="https://www.anthropic.com/engineering/building-effective-agents">https://www.anthropic.com/engineering/building-effective-agents</a></p>
<p>博客的作者还有一个相关的小演讲：<a href="https://www.youtube.com/watch?v=D7_ipDqhtwk">https://www.youtube.com/watch?v=D7_ipDqhtwk</a></p>
<h2 id="workflows">workflows</h2>
<p>讲agent之前，先讲讲workflow。</p>
<p>workflow我们日常已经用得很多了，写代码的逻辑，日常做饭的操作，都是workflow。这些工作一个特点就是任务相对是well-defined，大概有什么步骤，每个步骤干什么相对清晰，大不了加一个branch或者确定的loop。</p>
<p>总结几种常用的workflow。</p>
<p>1、Prompt Chaining</p>
<img src="/b242bfb3/an_prompt_chain.png" class title="agent_0">
<p>Prompt
Chaining将任务分解为static的子step，前一步的输出作为下一步输入。</p>
<p>Prompt
Chaining适用于「任务可明确拆解」的场景（如先生成营销文案，再翻译），需通过切分子任务来降低单次LLM调用的复杂度的情况。</p>
<p>2、Routing</p>
<img src="/b242bfb3/an_routing.png" class title="agent_0">
<p>Routing通过分类（LLM或传统算法）将输入导向不同的下游处理模块。</p>
<p>Routing适用于「输入类型差异大且需专用处理」的场景，如客服问题分类：退款请求→财务工具，技术问题→知识库检索；或者「成本优化」的场景，如简单问题分到小模型如Haiku，复杂问题用Sonnet。</p>
<p>3、Parallelization</p>
<img src="/b242bfb3/an_parallelization.png" class title="agent_0">
<p>Parallelization将任务同时下发给下游多路处理模块，并把多个下游模块的结果聚合起来，经过处理获得最终输出。</p>
<p>Parallelization适用于「需加速处理或多样化视角」的场景，如多维度评估模型性能，又或者关键任务需冗余验证（如敏感内容过滤需多数表决）。</p>
<p>Parallelization有两种变体：</p>
<ul>
<li>Sectioning：独立子任务 &amp; 执行（如内容生成与审核同步进行）<br>
</li>
<li>Voting：同一任务多次运行，聚合多次结果（如多LLM评审代码安全性）</li>
</ul>
<p>4、Orchestrator-Workers</p>
<img src="/b242bfb3/an_ow.png" class title="agent_0">
<p>Orchestrator动态分解任务，把子任务分配给Worker，最后合成结果。与Parallelization不同的是，这些子任务不是预先定义好的，而是根据输入动态生成的。这里其实已经有些agent的感觉。</p>
<p>这种workflow适用于「复杂且不可预测的任务」的场景，如跨多文件代码修改，需动态分析依赖关系；或者「多源信息整合」，如研究任务需从不同数据库检索并交叉验证。</p>
<p>5、Evaluator-Optimizer</p>
<img src="/b242bfb3/an_eo.png" class title="agent_0">
<p>Generator产出结果，evaluator提供feedback，并循环优化（类似人类写作的迭代修订）。</p>
<p>Evaluator-Optimizer适用于「存在明确评估标准」的场景，比如翻译的语义保真度；
或者「需多轮改进」的场景，比如复杂搜索任务，evaluator根据已有搜索结果决定是否继续搜索。</p>
<h2 id="agents">Agents</h2>
<h3 id="workflow和agent">workflow和agent</h3>
<p>LLM +
workflow已经cover了很多我们日常使用的场景，比如手机上用语音助手操作闹钟就是一个LLM
+
workflow的流程。RAG也是一种workflow，先搜索后回答。这些使用LLM的workflow系统其内部的调度主要依赖人的设计。</p>
<p>人设计的workflow相对固定，而agent则相对灵活。那是不是agent总是优于workflow，答案是no：</p>
<blockquote>
<p>When building applications with LLMs, we recommend finding the
simplest solution possible, and only increasing complexity when needed.
This might mean not building agentic systems at all. Agentic systems
often trade latency and cost for better task performance, and you should
consider when this tradeoff makes sense.</p>
</blockquote>
<p>灵活的调度，其背后是复杂度和成本的增加，对应的风险也有提升，如果业务没有那么复杂，那么单次的大模型调用基本可以满足需求，没有必要非要上agent。</p>
<h3 id="从workflow到agent">从workflow到agent</h3>
<p>一个典型的agent流程图：</p>
<img src="/b242bfb3/an_agent.png" class title="agent_0">
<p>和workflow相比，agent有这些特征：</p>
<ul>
<li>自主性：LLM自主规划步骤、调用工具，无需预先定义路径<br>
</li>
<li>环境交互：常常需要依赖工具执行结果作为事实依据</li>
</ul>
<p>agent适用于「开放性」的问题，比如GitHub
Issue的修复，期间需动态分析代码库。<br>
agent在执行过程中，需要做一些风险控制：</p>
<ul>
<li>设置停止条件（如最大迭代次数），防止agent陷入死循环，这个在目前阶段是比较常见的情况<br>
</li>
<li>沙盒测试和监控工具调用（防错误累积）；正常来说agent会有至少两三次调用，期间有多次操作比如工具调用，这些可能需要结构化输出的场景模型还是有可能出错的，需要对格式和内容进行一定的修正；另外模型写代码是比较天马行空的，有可能写出移除路径之类的代码，这在生产环境风险很大，最好搞个沙盒环境，免得被删库</li>
</ul>
<h3 id="agents-in-practice">Agents in Practice</h3>
<p>Anthropic从agent的实践中总结了一些实际经验，给了两个适用agent的案例。</p>
<p>1、案例一：Customer Support</p>
<p>适用原因：天然对话流与工具调用结合（调取订单数据、触发退款等操作），可量化成功指标（如按解决率计费）。</p>
<p>2、案例二：Coding Agents</p>
<p>适用原因：结构化问题空间（代码可通过测试验证），自动化反馈驱动迭代（测试失败→重新修改代码）。</p>
<p>不过代码agent目前也还有一些问题，比如功能性验证≠系统兼容性，仍需人工审核（如架构设计一致性）。这个目前的复杂度还是比较难由agent自己完全handle。</p>
<p>Anthropic用sonnet + agent搞了个优化SWE-bench效果的项目，地址在：<a href="https://www.anthropic.com/engineering/swe-bench-sonnet">https://www.anthropic.com/engineering/swe-bench-sonnet</a>。</p>
<p>另外还有一个让Claude操作电脑解决问题的project：<a href="https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo">https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo</a>。</p>
<h3 id="prompt-engineering-your-tools">Prompt Engineering your
Tools</h3>
<p>Anthropic给出一个重要经验：工具设计的质量直接影响agent效果，需要像设计人机交互（HCI）一样重视agent-computer接口（ACI）。在把工具给模型使用前，先让人类的使用者试下，看是否会在使用时遇到问题，比如参数的理解有没有问题，接口的设计是否合理，如果人类使用起来有困难，那么模型大概率也会遇到问题。</p>
<p>在SWE-bench的project中，工具优化的耗时占了大部分开发时间。</p>
<p>有几条具体的实践原则：</p>
<ul>
<li>格式优化：选择LLM易处理的格式，如避免JSON转义，优先Markdown；或者其他模型能在internet上看到的格式，保持阅读和生成的格式一致性<br>
</li>
<li>绝对路径：SWE-bench代理中，用绝对路径替代相对路径能减少一些错误<br>
</li>
<li>开发文档：包含示例、边界说明、常见错误，像给人类开发者用一样<br>
</li>
<li>区分相似工具：比如"文件编辑"需明确是全量重写还是差分更新，并考虑哪个方法对模型更友好（比如查分更新，只修改文档的一部分，那么就需要模型具备补全的能力）</li>
</ul>
<h2 id="开发agent的一些建议">开发agent的一些建议</h2>
<p>博客作者的演讲给了三个agent开发中的观点。</p>
<p>1、Don't build agents for everything</p>
<p>agent不是workflow的升级版，不要把原有的workflow都替换成agent。workflow和agent虽然略有重叠，但是更主要的应该是互补的关系，合作的关系，不是上下级的关系。</p>
<p>workflow可以处理确定性高的任务，而agent擅长处理模糊的问题。</p>
<p>workflow成本更低，而agent可能会在一个任务消耗极其大量的token（百万甚至更多），所以要好好考虑成本问题（以及耗时）。</p>
<img src="/b242bfb3/an_checklist.png" class title="agent_0">
<p>2、Keep it simple</p>
<blockquote>
<p>Agents are models using tools in a loop</p>
</blockquote>
<p>Anthropic认为重要的agent组件：</p>
<ul>
<li>environment<br>
</li>
<li>prompt<br>
</li>
<li>tools</li>
</ul>
<p>把精力放在优化这几个重要组件上，不要过度设计。</p>
<p>3、Think like your agents</p>
<p>agent可以说每次处理都是从零背景知识开始，模型能够看到的信息全都在prompt里了，所以请在prompt里把重要的相关信息都提供清楚，包括详细清晰的背景和任务描述，准确的工具和环境说明，还有详尽的历史记录和反馈。</p>
<p>如果人类无法在模型的环境下工作（能看到的prompt，能操作的工具），那么模型效果不好也就可以理解了。因此记得「设身处地」，跟模型「换位思考」。</p>
<h1 id="小结">小结</h1>
<ul>
<li>agent的三大模块：模型，工具，和调度（prompt + 相关配套）<br>
</li>
<li>agent开发三个理念：（1）不要拿着锤子看啥都是钉子，agent不是workflow的升级版，agent和workflow解决的问题是不一样的，不要到处套用（2）不要过度设计agent系统，先尝试从环境、工具和prompt入手优化（3）换位思考，从agent的角度出发思考，如果一个任务人难以完成，agent一样会遇到问题</li>
</ul>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="https://www.linsight.cn/a9c496e3.html">DeepSeek-V3细节探索</a><br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="https://www.linsight.cn/6c0f6207.html">Qwen2.5-1M技术解析</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="https://www.linsight.cn/4519eadd.html">prompt压缩(一)</a><br>
<a href="https://www.linsight.cn/ea2871bf.html">prompt压缩(二)</a><br>
<a href="https://www.linsight.cn/bfa4f144.html">reasoning压缩(一)</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a href="https://www.linsight.cn/657a6d17.html">基模型Cognitive
Behaviors对RL的影响</a><br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="https://www.linsight.cn/2dee4921.html">LLM水印</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
<a href="https://www.linsight.cn/d253d7b3.html">关于The Bitter
Lesson</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
<a href="https://www.linsight.cn/b4d047c1.html">DeepSeek-VL2的细节</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】Google Agent
whitepaper，https://archive.org/details/google-ai-agents-whitepaper<br>
【2】Building effective
agents，https://www.anthropic.com/engineering/building-effective-agents<br>
【3】Building effective
agents作者演讲，https://www.youtube.com/watch?v=D7_ipDqhtwk</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>Agent</tag>
      </tags>
  </entry>
  <entry>
    <title>从RAG到DeepSearch</title>
    <url>/7c2f9dcb.html</url>
    <content><![CDATA[<p>上次在<a href="https://mp.weixin.qq.com/s/tVmAPk6-ZTQCY0_aMmWT-g">《DeepResearch的报告生成方法》(https://mp.weixin.qq.com/s/tVmAPk6-ZTQCY0_aMmWT-g)</a>中讲了生成长篇图文report的方法，这里梳理一下目前从RAG到DeepSearch的一些经验。</p>
<h1 id="rag价值">RAG价值</h1>
<p>ChatGPT发布后不久，我们就在实际使用中发现了纯LLM模型的局限性和RAG的重要性，并开始做了一些尝试。那时对我们来说，RAG的价值主要有两个：</p>
<ul>
<li>能够解决时效相关的问题，比如查天气，看新闻，让模型知道今夕是何年<br>
</li>
<li>能够让模型在一定程度上，按我们想要的方式回答问题：
<ul>
<li>比如要搭建一个客服机器人，那当用户问起"你们产品怎么样"时，需要通过给模型提供相应品牌资料让它能正确回答--夸夸自家产品再给点购买建议，而不是靠幻觉胡说一通，或者是根据预训练数据中的一个差评来回答；</li>
<li>当用户的问题涉及比较有深度的专业知识时，模型的预训练数据可能不足以支持模型分析，就可以通过搜索文档给出质量更高的结果</li>
</ul></li>
</ul>
<p>其实这两个都可以归结为「通过提供额外信息，提升模型的应答质量」，从闭卷考试变成开卷考试。</p>
<h1 id="naive-rag">Naive RAG</h1>
<p>Naive RAG是最简单的RAG，但是也很实用。下图左边是它的流程：</p>

<p>上图右边是一个例子。这里的搜索展示的是分档的indexing和retrieval，实际上也可以是搜索接口（把搜索和应答LLM更好地解耦）。</p>
<p>Naive RAG在实际应用的时候，至少会遇到这几个问题：</p>
<ul>
<li>什么时候要搜，什么时候不搜<br>
</li>
<li>搜什么<br>
</li>
<li>怎么搜<br>
</li>
<li>模型怎么用搜到的东西</li>
</ul>
<p>下面一个个简单聊一下。</p>
<h2 id="什么时候搜">什么时候搜</h2>
<p>1、不是所有query都需要搜索</p>
<p>在开放域场景下，显然并不是所有问题都需要搜索资料才能回答，就像现在不是所有问题都需要开启深度思考一样。</p>
<p>对用户的一句"你好"进行深度思考或者搜索是没有必要的。除了浪费时间，成本也是个很大的考量，如果用的是三方的搜索API或者数据库，那是很贵的，毕竟搜索卖家都知道你很需要。</p>
<p>而且不必要的搜索文档可能还会影响模型回复的效果，带来负收益。</p>
<p>2、方法：用模型做判别</p>
<p>要缓解这个问题，一个简单有效的做法就是训练一个小的检索判别模型来判断当前的query（+历史对话信息）是否需要搜索。</p>
<p>数据准备上，可以把时效相关、专业知识相关和个人知识库相关的query都作为正例。这些query的特点是"没有资料很难答得好"。</p>
<p>还可以先把query过一遍应答模型，把「因为缺少知识库内容而答得不够好的」也作为正例加入训练数据。</p>
<p>实践上，这样的检索判别模型效果是不错的，基本上能做到85%+的准召。这也很合理，试想一下，如今在Agent中让模型自己选择是否调用检索工具的做法，其中就包含同样的检索判断，而Agent中模型的判断效果也是不错的。</p>
<p>3、小技巧：多召回一点</p>
<p>使用检索判别模型的一个小技巧是在推理时调低一点阈值（比如&gt;0.4就召回做检索），提升召回率。</p>
<p>虽然这样判别的准确率会下降一些，但是「不需要搜索但是搜索了」这个问题在RAG的后续流程还有机会解决，而一旦检索判别模型判断了不搜索，那么后面就很难解决「需要搜索但是没有搜」这个问题了。</p>
<p>4、产品设计：全部搜索</p>
<p>有些产品则是从业务形态上直接抛弃了搜索判别。比如面向个人知识库/专业知识库场景的RAG产品，一般就会默认检索，因为产品的定位就是如此。</p>
<p>如果一个用户跑去医学RAG询问计算机科学的问题，就像打开手机闹钟买衣服，有问题的是用户。这也算是用户心智的一种教育。（说是这么说，但是大众用户对AI产品的认知基本上还停留在22年以前，AI产品的用户教育任重道远，怎么给用户设计防呆方案也是以后会区分产品实用性的一大重点）</p>
<h2 id="搜什么">搜什么</h2>
<p>1、从对话内容到搜索query</p>
<p>这个问题主要就是怎么把用户query（和对话历史），转换成搜索query。</p>
<p>在大部分场景下，用户的query并不适合直接用于搜索：</p>
<ul>
<li>在多轮场景下，用户当前的query很可能包含指代，"明天天气怎么样"，"后天呢"这个时候就需要改写成"后天天气怎么样"<br>
</li>
<li>用户的问题比较复杂，比如多跳或者罗列，可能就涉及到多步搜索："今年GDP最高的城市的人口数是多少"，可能就需要先搜GDP最高的城市，再搜这个城市的人口数（当然这算是比较复杂的问题了，naive
RAG可能并不好处理）</li>
</ul>
<p>这个问题是一个和搜索工具紧密相关的问题：有时候同样的语义，不同的表述，搜索结果就大相径庭。甚至搜索工具或者搜索库改版了，都会对搜索结果产生影响。</p>
<p>2、方案</p>
<p>这就需要一个改写模型。那要怎么改写？</p>
<p>对于传统的搜索引擎，经验上，关键词组合的搜索效率是比长整句描述要好的。现在一个更好的方法是用强化学习，优化搜索结果，让模型根据反馈学会使用更高效的搜索query，比如Search-R1。</p>
<p>而对于自建搜索库的情况，则有这样一些方法：</p>
<ul>
<li>Query2Doc/Doc2Query：把用户query改写成伪文档去匹配数据库的文档，或者把数据库的文档进行分段summary<br>
</li>
<li>HyDE：用模型对用户query生成一个假设可回答问题的passage，再去匹配数据库文档<br>
</li>
<li>等</li>
</ul>
<p>很多方法其实已经不仅是"生成好的搜索query"这个范畴的了，而是和数据库进行更多形式和维度的交互，这就需要知识库和query生成同时配合。</p>
<h2 id="怎么搜">怎么搜</h2>
<p>1、问题很多</p>
<p>理想状态下，我们应该有一个功能特别强的搜索引擎，只要输入明确的搜索query，不用关心中间的细节，就能得到结果。</p>
<p>但是实际上，由于各种原因（除了效果本身，还有价格/地域/版权等），我们是没有这样完美的搜索工具可用的。而针对私有数据，也只能自建搜索库。</p>
<p>自建搜索库的标准配置就是粗召+精排。一般来说整个文档都比较长，因此需要先对文档进行分块。那问题就很多了：</p>
<ul>
<li>怎么分块，按段还是按句子，或者按多大长度，还是按语义分<br>
</li>
<li>入库的特征是什么，embedding，还是关键词，还是统计特征<br>
</li>
<li>embedding用多长的向量，什么模型<br>
</li>
<li>粗排召回数量是多少，精排策略是什么<br>
...</li>
</ul>
<p>2、一点通用经验</p>
<p>这些问题可以说根本没有标准答案，这里只简单说一些实践经验：</p>
<ul>
<li>不同类型的文档，适合不同的切分策略（废话了）：结构性较强的知识文档，比如维基百科这种，一般每一小段都会专注解释一个问题，因此适合分成小段；对于论文之类的文献，虽然也是知识性很强，但是一般在论文内上文和下文有比较强的逻辑联系，切分小段有可能导致丢失一些全局的信息或者逻辑联系；长篇小说之类的，一般一个故事会贯穿很多章节，受分块的影响就更大一些<br>
</li>
<li>大部分场景下，较小的块效果就不错了，比如512token或者1024token的长度，具体的长度最好做批量实验来定<br>
</li>
<li>块间重叠是提升效果的一个小技巧，不过相应的数据库也会变大一些<br>
</li>
<li>embedding模型的选择基本上也是基于榜单和实验效果来定，当然也会有成本的考虑点</li>
</ul>
<p>3、精排</p>
<p>精排单独拉出来说下。基于向量的检索和基于关键词的检索在不同场景有不同的优势，所以很多时候在粗召阶段会结合二者收集更全面的信息。因此就需要做精排，针对当前的query，给各个文档的重要性排个序。当然，即使粗召阶段只有单一搜索源，精排也可以提供query和搜索文档更好的匹配信息。</p>
<p>精排最经典就是用双塔模型来打相关性的分，BGE之类的就可以做。除了用双塔，也可以把query和单个搜索文档拼接在一起，用Bert类模型输出相关性的打分。或者用GPT类模型也可以。这样的做法都属于pointwise的打分方式，每次只看一个搜索文档和query的相关性。这样得到的是一个绝对分数，在不同case下这个分数是可以比较的。</p>
<p>既然有pointwise，就有pairwise。简单来说，pairwise的打分方式每次输入用户query和两个候选的检索文档，让模型给它们按相关性排个序。pairwise的方法为了获得所有候选文档的rank，需要进行两两比较，这样复杂度就比较高。如果为了保证效果的可信度，可能还要把两个候选文档交换位置，成本就更高了。</p>
<p>除了pointwise和pairwise，还有listwise的排序方式：把所有文档交给LLM，让LLM来决定排序。具体做法可以看RankGPT。</p>
<p>题外话，这里把加上各种优化手段建库 &amp; 精排的RAG仍然当做是naive
RAG，因为这些手段本质上都是搜索工具的优化，最后都封装在搜索API里，模型应该是可以不感知搜索API里的事情的。概念上，有些人会把带搜索优化的叫做retrieval
and rerank RAG。</p>
<h2 id="模型使用文档">模型使用文档</h2>
<p>搜到了文档，应答模型还得用得好。</p>
<p>正常来说，需要把用户问题、回复要求和搜索结果都放在prompt里，让模型生成回复。这种情况下，我们首先要面对的是长文本的问题。搜索结果凑个万八千token的并不困难。</p>
<p>虽然现在模型支持的输入窗口长度基本上是64k起步了，但是实际使用中，随着任务复杂性的提升和数据长度的增加，模型还是会出现问题：lost
in the
middle的现象，模型容易关注在输入数据的前面和后面部分，中间的就容易忽略了；这跟人的阅读习惯也很像，想快速阅读就会看个开头看个结尾，跟老师阅卷似的；因此组织搜索文档的时候可以根据精排的结果把重要度高的放在前面和后面。</p>
<p>另外，无论使用自检搜索还是使用搜索工具，都有可能返回错误或者无关的搜索结果（比如一个缩写在不同领域都存在，那么就有可能搜索文档里存在错误领域的资料）。这个时候模型需要学会判断和忽略错误的文档。</p>
<p>还有一种情况是缺了文档，比如某个query本来需要搜索日本和韩国的人口，结果搜索文档里只有日本的。这时就要防止模型由于信息缺失而产生幻觉。实践上这是个很容易出问题的点：当你要求模型必须回答这个问题，而资料又缺失的时候，模型很容易就开启胡说八道模式，无论是Qwen还是DeepSeek-R1都会有这样的情况。</p>
<p>搜索结果多了或者少了的情况，一种优化方式是合成这样的数据，然后做微调，让模型能够学会自己判断哪些是可用哪些是不可用的。现在有了reasoning模型，也可以直接prompt它们在思考中一一分析。</p>
<h2 id="进一步优化">进一步优化</h2>
<p>上面所提到的每一个步骤，每一步都可以用更强的LLM来提供更好的效果：</p>
<ul>
<li>比如更新的embedding模型提供更好的粗召效果<br>
</li>
<li>大参数量的reasoning模型的改写效果就比小模型要好，并且还能根据prompt提供不同的改写效果，以及给出一定的推理过程，提供可解释性<br>
</li>
<li>精排阶段给R1之类的大模型提供环境信息、对话历史、排序规则等，让它能以更贴近业务需求来排序（比如客服场景中想把推荐信息的优先级提高就可以这么做）<br>
</li>
<li>多模态模型一定程度上也可以处理图文输入，比如在输入阶段通过caption把用户query转换成文本</li>
</ul>
<h1 id="graph-rag">Graph RAG</h1>
<p>Naive
RAG（以及带上搜索优化的版本）适合处理那种「答案就存在于一个或少数几个文档」的情况，对于答案不存在一个地方的全局性问题，Naive
RAG基本就没法处理了。比如数据库是一本小说，用户要求总结一下主人公经历过的几个大事件，这个时候模型除非读完整本小说，否则基本上是很难回答这个问题的。而Naive
RAG的检索中，分块和检索只会返回一些离散的小段落，而且数量受到限制，因此应答模型是没有机会临时去读完整小说的。</p>
<p>回想一下，RAG的检索可以分为indexing和retrieval两个阶段。Graph
RAG在indexing阶段，用LLM把文档构建成包含实体和关系的graph。还是以上面的小说文档为例，它能够被构建出一个包含文中人物和事件互相关联的图谱，这样模型就能快速在图谱中找到和主角生平相关的信息，并进行总结。</p>
<p>Graph
RAG在资料入库的时候就需要构建图谱，并且每次更新可能会动到已建设的图谱，因此整体的indexing成本是比较高的。</p>
<p>Graph RAG和naive
RAG是可以结合起来使用的，这样就全局和细节信息都可以照顾到了。这也可以称为hybrid
RAG。</p>
<h1 id="rag-agent-agentic-rag">RAG + Agent = Agentic RAG</h1>
<p>前面提到的RAG，无论是retrieval and rerank RAG，graph RAG还是hybrid
RAG，基本上都是「输入-&gt;搜索-&gt;应答」这样的一个线性流程。</p>
<p>这样的流程适合解决「资料库中有答案」的问题，但是对于复杂的「多跳推理」，或者「有常规搜索工具以外的工具需求」的问题，就可能没法很好地处理。</p>
<p>要解决这些复杂问题，搜索只是其中一个步骤，但是只有搜索是不够的，系统还需要具备一定的自主决策能力，比如根据当前状况选择工具，复杂任务拆解成可执行的子任务等。</p>
<p>那么一个自然的选择就是用Agent来优化RAG。什么是Agent？其实现在Agent的概念已经相当泛用，边界也很模糊，我个人认知上是能够感知环境、自主思考决策并采取行动和环境交互的就算是Agent。</p>
<p>Agentic
RAG一个很大的特点就是自主性，因为要解决更复杂的问题，而这些问题需要什么手段来处理，人类是没法在事前完全定义的。这就使得Agentic
RAG必须具备一定的自主性，需要见机行事，因地制宜，利用手上的资源来制定case
by case的解决问题的方案。</p>
<h2 id="agentic-rag类型">Agentic RAG类型</h2>
<p>那Agent和RAG有哪些具体结合方式？</p>
<p>1、Tool Use &amp; Routing Agent</p>
<p>最简单的一类就是用agent做routing。</p>
<p>Router需要根据环境信息、prompt要求和输入query选择合适的工具。比如我们有一个数据库是医学数据库，另一个是计算机数据库，那么不同的query就要选择不同的搜索源。</p>
<p>实际上除了选择搜索源之外，也可以做其他工具的routing，比如计算器或者导航之类的。这其中就包含Tool
Use的能力，选择工具，使用适合的入参。</p>
<p>2、Query Planning Agent</p>
<p>这个主要是把用户的复杂query分解成subquery，每个subquery都可以走单独的pipeline来解决，最后再把多个subquery的结果合在一起。比如"世界最高楼和第十一高楼分别叫什么"，就可以拆解成两个query："世界最高楼叫什么"，"世界第十一高楼叫什么"。</p>
<p>多个subquery之间也可能有依赖关系，那有些subquery可以并行，有些就不行。比如"世界最高楼和第十一高楼的距离是多少"，就需要拆解成三个subquery：</p>
<ul>
<li>Q1："世界最高楼叫什么" --&gt; 获得答案A1<br>
</li>
<li>Q2："世界第十一高楼叫什么" --&gt; 获得答案A2<br>
</li>
<li>Q3："{A1}和{A2}的距离是多少"</li>
</ul>
<p>Q1和Q2可以并行，Q3就需要等Q1和Q2结束后才能执行。</p>
<p>Planner可以是一个LLM，也可以是一个复杂的系统，它内部甚至可能是个多agent系统，这取决于要解决的任务的复杂度。</p>
<p>3、ReAct Agent</p>
<p>把routing、tool
use和plan这几个能力合在一起，再加上环境信息的循环流转，让系统迭代地处理问题，就是ReAct
Agent。</p>
<p>比如在上面这个"世界最高楼和第十一高楼的距离是多少"的case里，Q3"{A1}和{A2}的距离是多少"有可能搜索不到结果，那么第一次循环之后，plan
LLM就需要重新规划方案，把问题拆解成：</p>
<ul>
<li>Q4："{A1}的经纬度是多少" --&gt; 调用地图工具，获得经纬度X<br>
</li>
<li>Q5："{A2}的经纬度是多少" --&gt; 调用地图工具，获得经纬度Y<br>
</li>
<li>Q6："计算{X}和{Y}的距离"</li>
</ul>
<h2 id="agentic-rag特性">Agentic RAG特性</h2>
<p>首先，Agentic
RAG的prompt会更加meta一点，规则和指令的抽象等级会更高。如果prompt的规则写得太细，人工干预的规则太多，反而会限制LLM的发挥空间。</p>
<p>传统RAG的检索操作相对更独立一些，缺乏上面提到的多任务拆解的能力，而Agentic
RAG能够拆解任务，并且把多个子任务之间的依赖处理清楚。</p>
<p>为了让LLM能够做出更合理的决策，Agentic
RAG的输入一般会提供更多的信息，比如用户所在的地点、时间、历史聊天记录都可能需要。</p>
<p>传统RAG一般是一次过输出结果，而Agentic
RAG则经常需要「循环」地规划思考、调用工具、整合结果。</p>
<table>
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Traditional RAG Systems</th>
<th>Agentic RAG Systems</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Prompt engineering</td>
<td>Relies heavily on manual prompt engineering and optimization
techniques.</td>
<td>Can dynamically adjust prompts based on context and goals, reducing
reliance on manual prompt engineering.</td>
</tr>
<tr class="even">
<td>Static nature</td>
<td>Limited contextual awareness and static retrieval
decision-making.</td>
<td>Considers conversation history and adapts retrieval strategies based
on context.</td>
</tr>
<tr class="odd">
<td>Overhead</td>
<td>Unoptimized retrievals and additional text generation can lead to
unnecessary costs.</td>
<td>Can optimize retrievals and minimize unnecessary text generation,
reducing costs and improving efficiency.</td>
</tr>
<tr class="even">
<td>Multi-step complexity</td>
<td>Requires additional classifiers and models for multi-step reasoning
and tool usage.</td>
<td>Handles multi-step reasoning and tool usage, eliminating the need
for separate classifiers and models.</td>
</tr>
<tr class="odd">
<td>Decision-making</td>
<td>Static rules govern retrieval and response generation.</td>
<td>Decide when and where to retrieve information, evaluate retrieved
data quality, and perform post-generation checks on responses.</td>
</tr>
<tr class="even">
<td>Retrieval process</td>
<td>Relies solely on the initial query to retrieve relevant
documents.</td>
<td>Perform actions in the environment to gather additional information
before or during retrieval.</td>
</tr>
<tr class="odd">
<td>Adaptability</td>
<td>Limited ability to adapt to changing situations or new
information.</td>
<td>Can adjust its approach based on feedback and real-time
observations.</td>
</tr>
</tbody>
</table>
<p>其实我觉得Agentic
RAG虽然套用了RAG的名字，但是实际上解决问题的方式和重点已经距离传统RAG比较远了：</p>
<ul>
<li>传统RAG更多关注在怎么搜索，比如前面提到的数据库建设，粗召精排和query改写等<br>
</li>
<li>Agentic
RAG更多关注在逻辑推理，任务规划和工具之间的配合使用，解决的任务也从简单单跳变成复杂多跳</li>
</ul>
<h1 id="deepsearch">DeepSearch</h1>
<p>DeepSearch也算是Agentic RAG的一种。从概念上来讲，Agentic
RAG可以说是一种架构，而DeepSearch则算是一种产品形态。不同的DeepSearch框架设计不同，但是目标都是提供完备的准确的信息。</p>
<p>DeepSearch本身可以直接和用户交互，也可以作为DeepResearch中的搜索模块，为report生成模块提供数据。二者在设计细节上稍微会有些不同，但是大致的流程设计是相通的。（其实DeepSearch和DeepResearch在概念上的边界也有点模糊,姑且认为DeepResearch提供的是更长的报告，而DeepSearch的结果相对简短一些）</p>
<p>Jina在<a href="https://jina.ai/news/a-practical-guide-to-implementing-deepsearch-deepresearch/">《DeepSearch/DeepResearch
实施实用指南》https://jina.ai/news/a-practical-guide-to-implementing-deepsearch-deepresearch/</a>中总结的DeepSearch核心设计原则是比较合理的：</p>
<blockquote>
<p>DeepSearch 的核心设计原则：搜索、阅读和推理的持续循环。</p>
</blockquote>
<h2 id="框架">框架</h2>
<p>以一个基础的DeepSearch方案为例，看下每个模块在干什么。</p>
<img src="/7c2f9dcb/deepsearch.png" class title="DeepSearch">
<p>③④⑤是个循环，一直到结果满意输出为止。</p>
<p>(先挖个坑：这里先给一个基础的DeepSearch方案，以后再写一篇整理更多种方案的。)</p>
<h2 id="planner-任务拆解">planner 任务拆解</h2>
<p>这一步主要就是根据当前的输入信息，决定接下来要干什么。</p>
<p>这里的输入信息不只有用户的query，而是包含之前循环中做过的任务和得到的结果（比如调用了什么工具，工具的结果是什么，也就是observation），以及环境信息（比如用户地点、时间、用户画像、长期记忆信息等）。</p>
<p>另外，我对它的描述是"接下来干什么"，而不是"下一步干什么"，因为很多情况下，planner是可以给出多步的计划的。比如用户的问题是"2024年中国GDP最高的三个城市的人口分别是多少"，那么规划出来的步骤应该是：</p>
<ul>
<li>step 1：查询"2024年中国GDP最高的城市" -&gt;
获取前三个城市A、B、C<br>
</li>
<li>step 2：查询城市A的人口<br>
</li>
<li>step 3：查询城市B的人口<br>
</li>
<li>step 4：查询城市C的人口</li>
</ul>
<p>step2/3/4虽然是依赖step1的结果的，但是在规划阶段就可以先用占位符把任务给规划出来了。</p>
<p>有一些产品比如Qwen
Chat或者Suna等，规划会分为一二级，类似一个目录。比如"对比一下近三个月新上市的国产手机的价格和配置，按销量排名列成列表"这个case，规划出来的可以是：</p>
<ul>
<li>手机新上市国产手机信息
<ul>
<li>查询"华为近三月新上市手机"<br>
</li>
<li>查询"荣耀近三月新上市手机"<br>
</li>
<li>查询"小米近三月新上市手机"<br>
</li>
<li>查询"OPPO近三月新上市手机"<br>
</li>
<li>查询"VIVO近三月新上市手机"<br>
</li>
</ul></li>
<li>获取销量数据
<ul>
<li>查询"华为xx手机销量"<br>
</li>
<li>查询"荣耀xx手机销量"<br>
</li>
<li>查询"小米xx手机销量"<br>
</li>
<li>查询"OPPOxx手机销量"<br>
</li>
<li>查询"VIVOxx手机销量"<br>
</li>
</ul></li>
<li>制作表格</li>
</ul>
<p>planner模块需要读入比较多的信息，并给出合理的规划（不然后面的模块可能无法执行），因此要求LLM的逻辑推理能力比较强。</p>
<h2 id="任务澄清">任务澄清</h2>
<p>用户既然用到了DeepSearch，很可能是要解决比较复杂的问题，那么就有可能要执行很久。因此准确理解用户的意图就很重要，不然跑个半小时给出来的结果并不是用户想要的，那用户体验就太差了，也很浪费token。</p>
<p>现在一些DeepSearch的产品会在初步规划完之后做一次展示和确认，在必要的情况下通过对话完善有歧义的地方，比如扣子空间和Skywork
AI。</p>
<p>理想情况下，在后续执行的任何过程中如果出现任务不清楚的地方，都应该可以和用户交互进行澄清，这样的效果是最好的。就像你工作中带了一个实习生，你在给他布置任务的过程中，他如果有什么不清楚的可以随时问题，这样能保证不容易出错。</p>
<p>不过这样的设计在产品上可能暂时不太成立：用户不可能一直盯着你执行，他只想要最终结果，如果长期的等待没有能拿到结果，体验会很不好。所以大部分的产品设计上不会进行太多次交互澄清，或者在澄清阶段等一小会没有反馈之后，就会直接按默认设置执行。</p>
<p>举个例子，比如用户说"帮我规划一个国庆节的东南亚旅游攻略"，那这时模型需要知道一个旅游攻略都需要什么输入参数：目的地、出发地、交通工具、人数、预算、天数，更高级一些有路线偏好比如人文、建筑、自然等。然后看用户的query缺失了哪些重要参数，让用户来选。如果模型本身的知识无法确定旅游攻略需要什么参数，那至少要懂得借助general
search去搜"制订旅游攻略需要先确定什么"。</p>
<p>实际开发上，可以有一个专门的agent来处理澄清的问题。甚至可以针对一个输入query，出一个简单的可视化界面，天工AI就是这样。比如上面这个旅游的case，让用户勾选每一个字段要的参数值：</p>
<ul>
<li>目的地：□泰国，□新加坡，□马来西亚，□越南，□老挝</li>
<li>预算：□3000-4999，□5000-6999，□7000-8999</li>
<li>...</li>
<li>路线偏好：□人文，□建筑，□宗教，□自然</li>
</ul>
<p>用户选好之后，再传给planner完善新的规划。</p>
<h2 id="信息获取">信息获取</h2>
<p>信息获取阶段，主要就是根据planner规划出来的子任务，获取对应的信息。</p>
<p>设计上，可以让planner直接输出function call的json
string，也可以让planner只输出搜索任务的自然语言描述，再由另外的function模型来生成各个工具的调用参数。个人经验，实操上二者的效果区别并不大。</p>
<p>DeepSearch的工具集一般包含多个搜索源，比如通用搜索引擎的接口，垂域数据库（比如专门搜商品的、搜美食的、搜菜谱的、搜车型的等）。</p>
<p>接口的介绍和功能用法的文档对LLM准确地使用工具很重要。</p>
<p>比如搜索引擎一般有一些使用技巧：加双引号表示必须包含，用减号表示排除关键词，用site:表示站内搜索等，这些都能帮助模型提高搜索效率。</p>
<p>如果有些概念不好用简短的语言介绍清楚，也可以加一个典型的例子，实践证明加example非常有效。当prompt中的指令和example出现矛盾时，LLM更愿意向example的做法靠近。</p>
<h3 id="信息过多">信息过多</h3>
<p>DeepSearch的搜索-阅读-推理是循环进行的，收集到的信息会越来越多，在后面的轮次中，生成function
call的模型的输入就很长，甚至有可能超出了模型窗口限制，导致循环无法继续。</p>
<p>因此每次搜索之后，对有用信息进行压缩就很重要。</p>
<p>1、精排</p>
<p>简单有效的一个方法是对搜索得到的每个文档和query/subquery进行打分匹配，过滤掉相关性低的。这可以缓解由于function的入参不佳，或者搜索接口准确率不够带来的问题。</p>
<p>2、去重</p>
<p>搜索时用同一个query获取多个文档，一般都会有一些信息重复。因此可以对搜索query进行信息去重。</p>
<p>具体做法是用一个小模型，对多个文档内容进行整合。要求仅删除冗余信息，尽量避免对原文的修改，以减少幻觉的引入。</p>
<p>最终把所有历史搜索结果压缩成一个或者几个去重过的文档，这些文档就是在最后会输出的文档，而原始搜索结果就不输出了。这些去重文档就比原始文档的长度减少了许多。</p>
<p>在DeepSearch进行下一个搜索循环中，function
call的模型需要分析这些去重过的文档已经包含什么，避免进行重复搜索。</p>
<h2 id="结果反思">结果反思</h2>
<p>结果反思的任务是做循环判断：当前的搜索结果（去重文档）是不是已经足够了，如果不够，那就继续拆解任务，继续搜索。</p>
<p>理论上，这个判断由planner来做也可以，用额外的结果反思模块的好处是可以用不同的模型，或者用更多的模型来协助判断。和人看问题的角度会有不同一样，不同的模型判断时也会从不同维度来分析，这样集思广益，可以避免planner一家独大。</p>
<p>还有一个好处是planner的任务少一点，压力也轻一些。</p>
<p>这个模块的功能和RL的reward有点相似。</p>
<h2 id="通用工具">通用工具</h2>
<p>这里还有一个模块是通用工具。通用工具包括计算器和Python代码解释器等。这些工具本身并不提供搜索功能，但是在DeepSearch的各个环节中都有可能要用到他们。</p>
<p>举个例子，比如用户要用3人出行，预算5000块，花在美食上的比例在30%到40%之间。那么无论是planner，还是function
call模块，都有可能需要计算每人每天的饮食预算，以此来设计对应的就餐地点。虽然模型本身也具备一定的计算能力，但是使用计算器可以保证结果的准确性，那这时就可以调用计算器了。</p>
<p>再举个例子，比如用户要求罗列一些以科幻为题的文章，字数要求在500字到600字之间。由于模型本身没法数字数，所以就需要调用代码解释器，写个小脚本来统计字数了。</p>
<p>随着模型能力的提升，这个工具池的大小一定会越来越大，能处理的问题也越来越复杂。目前来看，扩展到多模态的输入输出已经是一个必然。</p>
<h1 id="小结">小结</h1>
<ul>
<li>RAG适合解决搜索文档有答案的问题，但是复杂问题效果不好<br>
</li>
<li>Graph RAG能处理一些普通RAG无法解决的全局问题，但是成本高<br>
</li>
<li>DeepSearch是Agentic
RAG的一种，用Agent来处理routing（工具选择），分析推理（plan +
reflect）的任务，这个循环可以一直跑下去，知道结果满足需求<br>
</li>
<li>DeepSearch的大流程不复杂，但是作为产品，要考虑的细节问题非常多，所需能力也很综合<br>
</li>
<li>DeepSearch的形态还在不断变化中</li>
</ul>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- Agent：<br>
<a href="https://www.linsight.cn/b242bfb3.html">Agent完全手册(零)：三大模块，三个理念</a><br>
<a href="https://www.linsight.cn/44c62dc5.html">DeepResearch的报告生成方法</a><br>
- MoE：<br>
<a href="https://www.linsight.cn/a9c496e3.html">DeepSeek-V3细节探索</a><br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/37ee84bb.html">Qwen3实测&amp;技术报告</a><br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="https://www.linsight.cn/6c0f6207.html">Qwen2.5-1M技术解析</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="https://www.linsight.cn/4519eadd.html">prompt压缩(一)</a><br>
<a href="https://www.linsight.cn/ea2871bf.html">prompt压缩(二)</a><br>
<a href="https://www.linsight.cn/bfa4f144.html">reasoning压缩(一)</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a href="https://www.linsight.cn/657a6d17.html">基模型Cognitive
Behaviors对RL的影响</a><br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="https://www.linsight.cn/2dee4921.html">LLM水印</a><br>
- 训练框架<br>
<a href="https://www.linsight.cn/fe0adaa5.html">LLM训练框架：从优化器和精度讲到ZeRO</a><br>
<a href="https://www.linsight.cn/4cd8532f.html">LLM训练各种并行策略</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
<a href="https://www.linsight.cn/d253d7b3.html">关于The Bitter
Lesson</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
<a href="https://www.linsight.cn/b4d047c1.html">DeepSeek-VL2的细节</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】Retrieval-Augmented Generation for Large Language Models: A
Survey<br>
【2】12 RAG Pain Points and Proposed
Solutions，https://archive.is/bNbZo<br>
【3】Seven Failure Points When Engineering a Retrieval Augmented
Generation System<br>
【4】DeepSearch/DeepResearch
实施实用指南，https://jina.ai/news/a-practical-guide-to-implementing-deepsearch-deepresearch<br>
【5】https://www.zhihu.com/question/642650878/answer/1908280187600213891<br>
【6】RAG
切块Chunk技术总结与分块实现思路分享，https://zhuanlan.zhihu.com/p/19010809414<br>
【7】RAG 2.0
深入解读，https://zhuanlan.zhihu.com/p/1903437079603545114<br>
【8】DeepSearcher深度解读：Agentic
RAG的出现，传统RAG的黄昏，https://zilliz.com.cn/blog/DeepSearcher-Insights-Agentic-RAG<br>
【9】Agentic RAG Explained (RAG
Agent)，https://aisera.com/blog/agentic-rag/</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>Agent</tag>
        <tag>DeepResearch</tag>
        <tag>DeepSearch</tag>
      </tags>
  </entry>
  <entry>
    <title>最近阅读--关于数据合成、agent、reasoning和多任务</title>
    <url>/e96c7aac.html</url>
    <content><![CDATA[<p>新增一个栏目，写写平时读的文献，主要看思路。</p>
<h1 id="数据合成">数据合成</h1>
<h2 id="多轮对话数据合成review-instruct">多轮对话数据合成：Review-Instruct</h2>
<p>原文：ACL2025：《Review-Instruct: A Review-Driven Multi-Turn
Conversations Generation Method for Large Language Models》<a href="https://arxiv.org/abs/2505.11010">https://arxiv.org/abs/2505.11010</a></p>
<p>1、要解决的问题</p>
<p>现在的LLM使用的对话训练数据大部分是单轮的，而少量多轮的数据，上下文的逻辑联系也不多，导致在多轮对话场景下，LLM的能力相比单轮要差一些。</p>
<p>因此，要想办法提升多轮对话数据的上下文联系，和难度（最主要的就是难度）。</p>
<p>一些依赖LLM合成数据的方法，多样性和难度不足。</p>
<p>2、方法</p>
<p>Review-Instruct提出“Ask-Respond-Review”三阶段迭代流程，来提升多轮合成数据的难度。</p>
<p>具体来说，对于每一条query（相当于用户的第一轮问题），candidate
LLM先生成对应的response。在这个条件下，我们想要获取和这轮对话深度相关的后续问答。</p>
<p>先使用一组reviewer（每个reviewer是一个LLM
Agent）对前一轮生成的response进行评审，主要包括回答的深度、丰富度、相关性、连贯性等，并提出一些意见，比如“要在xx方面回答得更深入一些”“需要关注xx的长期影响力”。然后有一个chairman（也是一个LLM
Agent），整合多个reviewer的意见，然后对前一轮的response进行追问（相当于用户的下一轮追问），这样多轮对话在上下文中的关联性和难度就获得了提升。</p>
<h1 id="agent">Agent</h1>
<h2 id="toward-a-theory-of-agents-as-tool-use-decision-makers">Toward a
Theory of Agents as Tool-Use Decision-Makers</h2>
<p>原文：<a href="https://arxiv.org/abs/2506.00886">https://arxiv.org/abs/2506.00886</a></p>
<p>主要是一些认知和agent设计原则：</p>
<ul>
<li>知识边界：提升模型决策效率的一个要点是：知道自己知道什么，知道自己不知道什么<br>
</li>
<li>决策边界：模型需要知道自己什么时候“想”，什么时候“做”<br>
</li>
<li>理想情况是：知识边界和决策边界重合，知道的就想，不知道就找<br>
</li>
<li>想和做本质上应该是平等的，都是一个工具</li>
</ul>
<p>最后这一点，把think看作工具，和下面这个有些相关。</p>
<h2 id="anthropic把think当做工具">Anthropic：把think当做工具</h2>
<p>原文：<a href="https://www.anthropic.com/engineering/claude-think-tool">https://www.anthropic.com/engineering/claude-think-tool</a></p>
<p>1、think tool 和 extended thinking</p>
<p>extended thinking：如DeepSeek-R1的深度思考，在正式回答前。</p>
<p>think
tool：在回答过程中，可以停下来思考，模型自己决定什么时候需要停下来思考，就像在深度思考中调用工具一样自然。</p>
<p>2、think tool</p>
<p>说白了，think tool就是propmt +
LLM，根据不同的需要求，设计propmt，接收不同的输入和输出。description会明确介绍这个思考工具的功能，好让模型自己决定什么时候调用。</p>
<p>用不同的prompt就可以得到不同的思考工具。</p>
<p>决定是否使用think tool：ReAct方式来决定工具的调用。</p>
<p>think tool获取的结果再通过prompt加到原模型的上下文。</p>
<h1 id="reasoning模型">reasoning模型</h1>
<h2 id="innate-reasoning-is-not-enough">Innate Reasoning is Not
Enough</h2>
<p>原文：《Innate Reasoning is Not Enough: In-Context Learning Enhances
Reasoning Large Language Models with Less Overthinking》<a href="https://arxiv.org/abs/2503.19602">https://arxiv.org/abs/2503.19602</a></p>
<p>1、背景</p>
<p>CoT在非reasoning模型上很有效果，那CoT在reasoning模型上效果如何？对这个问题做了一些实验。</p>
<p>2、发现</p>
<p>以R1-Qwen-xxB各种规模的模型做实验。</p>
<p>（1）CoT在reasoning LLM也有效果</p>
<p>CoT在不同规模的reasoning模型和不同任务上的效果：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>小模型</th>
<th>大模型</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>简单任务</td>
<td>CoT提升较多</td>
<td>CoT提升较少</td>
</tr>
<tr class="even">
<td>困难任务</td>
<td>CoT提升较少</td>
<td>CoT提升较多</td>
</tr>
</tbody>
</table>
<p>总结来说，就是在模型容量适配的任务上，CoT效果更好。</p>
<p>在简单任务上，效果随着思考长度的增加，呈现先上升后下降的趋势。也就是，一定的思考可以提升效果，而过多的思考则损害效果。</p>
<p>（2）CoT能减少冗余思考</p>
<ul>
<li>直接prompt：生成的思考token数量分布分散，且存在大量冗余<br>
</li>
<li>Few-shot CoT：能有效集中思考token的分布，减少冗余<br>
</li>
<li>Zero-shot CoT：介于直接提示和Few-shot CoT之间</li>
</ul>
<p>直观上来想，有CoT或者few-shot，模型更容易理解指令，从而减少不必要的思考步骤。</p>
<p>Few-shot CoT中，One-shot CoT效果最佳。</p>
<p>（3）reasoning对反思词汇的attention异常高</p>
<p>分析attention
logits发现：reasoning模型对反思相关词汇（如“Wait”“Ensure”）分配了异常高的注意力。</p>
<p>而基础模型未表现出这种模式，表明reasoning
LLM在训练中过拟合了反思机制。</p>
<p>CoT提示通过外部引导缓解了这种过拟合行为。</p>
<h2 id="mixture-of-thought">Mixture-of-Thought</h2>
<p>原文：《Learning to Reason via Mixture-of-Thought for Logical
Reasoning》<a href="https://arxiv.org/abs/2505.15817">https://arxiv.org/abs/2505.15817</a></p>
<p>1、要解决的问题</p>
<p>让模型使用自然语言进行思考和推理是现在的主流方式，但是自然语言推理有其局限性，在很多问题上的效果不够好。</p>
<p>2、方法</p>
<p>（1）3种推理方式</p>
<p>既然一种推理方式有问题，那就多弄几种不同形式的推理CoT。</p>
<ul>
<li>自然语言推理（NL CoT）</li>
</ul>
<p>目前最常用。适合用于需要常识、逻辑深度较浅、开放语义场景的推理。</p>
<ul>
<li>代码推理（code CoT）</li>
</ul>
<p>结构化的推理方式，在这篇论文中仅写代码，不执行。</p>
<p>类定义、函数定义适用于规则约束、状态转移的场景。</p>
<ul>
<li>真值表推理（Truth Table CoT）</li>
</ul>
<p>这篇论文新创的，也是结构化的推理。真值表适用于多种情况需要遍历的场景。</p>
<p>（2）训练 &amp; 推理</p>
<p>这3种方式适用于不同的情况，那么一个使用方式就是对于一个任务，用三种方式都跑一遍结果，然后投票。</p>
<p>为了提升效果，需要训练模型。训练数据是合成的，三种都合成，然后根据正确答案进行过滤，保留合理的推理轨迹。</p>
<p>然后用合成的数据训练模型。这个「合成数据」-&gt;「训练」的模型会迭代几次，下次的数据合成模型就用这轮训出来的最佳模型做。</p>
<h1 id="训练方式">训练方式</h1>
<h2 id="主动学习self-adapting-language-models">主动学习：Self-Adapting
Language Models</h2>
<p>原文：<a href="https://arxiv.org/abs/2506.10943">https://arxiv.org/abs/2506.10943</a></p>
<p>提出SEAL学习框架。</p>
<p>1、要解决的问题</p>
<p>背景是让LLM来处理新任务。</p>
<p>LLM依赖SFT或者ICL来处理新的任务，但是SFT需要预先获取大量高质量的训练数据；ICL不用训练，但ICL无法持久化，效果也不如SFT。</p>
<p>2、思路</p>
<p>SFT是被动地学习，而人在学习的时候，除了被动学习，还有主动学习，就像学生除了听课（被动学习）还会整理笔记（主动学习），用更清晰的格式和说法来帮助自己理解知识。写技术博客也是一种以输出帮助学习的方法。那模型如果能自己整理知识，那应该也能学得更好。</p>
<p>3、方法</p>
<p>自己整理知识自己学习，简单来说就是自己生成数据，再训练自己。</p>
<p>比如说现在要在一个知识数据集上训练，以提升LLM在这个领域的知识水平。原始的训练数据集D={(C,τ)}，其中C是一个知识文档，τ是用于验证在这个文档上学习效果的数据，可以是C相关的知识问答题、选择题之类的（有答案的那种）。</p>
<p>本来微调是拿C直接训，而SEAL不拿C直接训，而是：</p>
<ul>
<li>1、用LLM从C生成多个self-edit（SE）数据<br>
</li>
<li>2、在每个SE上用LoRA训练LLM，获得LLM'<br>
</li>
<li>3、用τ评测第2步训练出来的LLM'，获得reward r<br>
</li>
<li>4、选择第3步中有正收益的SE，和对应的r，通过强化学习（ReST^EM方法）来更新LLM（不是LLM'）</li>
</ul>
<p>SE里除了来自C的增强数据，还有用于训练LLM的超参，这相当于让模型学习怎么更新自己的参数。</p>
<p>对于不同的数据类型，τ也不同，主要原则就是τ需要能够评测训练结果，获得reward。</p>
<h2 id="多任务免训练泛化text-to-lora">多任务免训练泛化：Text-to-LoRA</h2>
<p>原文：<a href="https://arxiv.org/abs/2506.06105">https://arxiv.org/abs/2506.06105</a></p>
<p>1、要解决的问题</p>
<p>如果我们有多个任务要处理，需要收集多个任务的数据，然后训练多个LoRA。这样的做法任务和LoRA一一对应，成本高，再加入新的任务的时候又要重新训练新的LoRA。</p>
<p>2、思路</p>
<p>如果能搞一个工具，这个工具的输入是文本，输出是LoRA，那么以后就不用训练LoRA了。Text-to-LoRA的目的就是搞这么一个工具。相当于获得LoRA泛化器，这个LoRA泛化器称之为T2L，是一个hypernetwork。hypernetwork的输出是另一个神经网络。</p>
<p>3、方法</p>
<p>T2L的输入是query，输出是LoRA参数。T2L本身也是个神经网络，原文设计了几个不同的变体，但是参数量都很少，比单个LoRA大不了多少。</p>
<p>获得T2L有两种方法。</p>
<p>（1）Reconstruction</p>
<p>如果我们已经有很多LoRA参数（实验是479）个，那么可以直接让T2L学习input
query到LoRA参数的构建，误差就是hypernetwork输出的AB矩阵和真实的AB矩阵的误差。</p>
<p>（2）直接SFT</p>
<p>如果我们没有训练好LoRA，但是有数据（input，label），那对于每条input，可以先获得T2L的AB矩阵，把AB矩阵加到Base
Model上，获得输出，计算输出和label的loss，反向传播训练T2L的参数。</p>
<p>4、效果</p>
<p>根据文中测试，T2L方法的效果介于单独LoRA训练和，和prompt
engineering之间。</p>
<p>5、收益</p>
<p>收益就是如果T2L
hypernetwork泛化性够好，那么就对于新任务，可以直接提升效果（效果优于prompt工程），而不需要训练新的LoRA了。</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- Agent：<br>
<a href="https://www.linsight.cn/b242bfb3.html">Agent完全手册(零)：三大模块，三个理念</a><br>
<a href="https://www.linsight.cn/44c62dc5.html">DeepResearch的报告生成方法</a><br>
<a href="https://www.linsight.cn/7c2f9dcb.html">从RAG到DeepSearch</a><br>
<a href="https://www.linsight.cn/226b059f.html">agent调研(1)--MetaGPT,OpenManus和OWL</a><br>
<a href="https://www.linsight.cn/f93b3aaf.html">Devin和Anthropic的Agent开发经验</a><br>
- MoE：<br>
<a href="https://www.linsight.cn/a9c496e3.html">DeepSeek-V3细节探索</a><br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/37ee84bb.html">Qwen3实测&amp;技术报告</a><br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="https://www.linsight.cn/6c0f6207.html">Qwen2.5-1M技术解析</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="https://www.linsight.cn/4519eadd.html">prompt压缩(一)</a><br>
<a href="https://www.linsight.cn/ea2871bf.html">prompt压缩(二)</a><br>
<a href="https://www.linsight.cn/bfa4f144.html">reasoning压缩(一)</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a href="https://www.linsight.cn/657a6d17.html">基模型Cognitive
Behaviors对RL的影响</a><br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="https://www.linsight.cn/2dee4921.html">LLM水印</a><br>
- 训练框架<br>
<a href="https://www.linsight.cn/fe0adaa5.html">LLM训练框架：从优化器和精度讲到ZeRO</a><br>
<a href="https://www.linsight.cn/4cd8532f.html">LLM训练各种并行策略</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
<a href="https://www.linsight.cn/d253d7b3.html">关于The Bitter
Lesson</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
<a href="https://www.linsight.cn/b4d047c1.html">DeepSeek-VL2的细节</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>agent调研(1)--MetaGPT,OpenManus和OWL</title>
    <url>/226b059f.html</url>
    <content><![CDATA[<h1 id="metagpt">MetaGPT</h1>
<p>MetaGPT的开发团队就是搞OpenManus的团队。</p>
<p>MetaGPT项目做得很早，23年就开始搞multi-agent了。MetaGPT主要是想通过多智能体的协作提升代码能力。做法上，大致上来说就是把一个软件团队所需的角色，比如产品经理、项目经理、架构师、开发工程师、测试工程师等都用LLM给角色扮演出来。不同的LLM角色和人类团队一样相互合作，共同开发项目：</p>
<img src="/226b059f/metagpt.png" class title="agent">
<p>MetaGPT对Agent和Multi-Agent的定义：</p>
<blockquote>
<p>智能体 = 大语言模型（LLM） + 观察 + 思考 + 行动 + 记忆</p>
</blockquote>
<blockquote>
<p>多智能体 = 智能体 + 环境 + 标准流程（SOP） + 通信 + 经济</p>
</blockquote>
<ul>
<li>SOP = Standard Operating
Procedures：是管理智能体行动和交互的既定程序，确保系统内部的有序和高效运作。例如，在汽车制造的SOP中，一个agent焊接汽车零件，而另一个安装电缆，保持装配线的有序运作。可以看出SOP非常依赖人为的设计，每个agent的操作空间、交互方式是被定义好的。<br>
</li>
<li>经济：这指的是多智能体环境中的价值交换系统，决定资源分配和任务优先级。</li>
</ul>
<p>MetaGPT中每个角色的输入输出都是格式化的信息，这样保证来自不同Agent的信息可以方便地被其他角色使用。</p>
<p>通信上，系统中设计了一个shared message
pool，各个Agent可以自由地在这里发布和订阅消息。和传统人工设计的信息流通方式相比，shared
message pool更方便解耦多个Agent之间的工作。比如产品经理发布 PRD →
消息池自动通知订阅该类型的架构师、工程师 →
后者按需读取，无需等待前者主动推送。</p>
<p>传统模式中，工程师必须等待项目经理分配任务才能开始工作；而在 MetaGPT
中，工程师可直接从消息池获取架构设计文档，甚至在产品经理完成 PRD
前，即可通过消息池的 “草稿” 版本提前介入，实现并行工作。</p>
<p>总体来说，MetaGPT是针对代码场景的定制化设计，同时各个LLM「在设计范围内具有一定自由度」的一个Multi-Agent
System。</p>
<h1 id="openmanus">OpenManus</h1>
<p>Manus出来当天，MetaGPT的团队基于原来的方案，花了几个小时复刻了一个OpenManus。后来也一直在更新，而且更新了很多版。</p>
<p>相比MetaGPT，OpenManus的大框架更为general，可以支持更多的场景。可以通过部分改动，快速定制支持不同的场景；也可以直接用通用模块，这样所有中间流程都交给LLM自己规划和处理。</p>
<p>1、组件</p>
<p>（1）Agent</p>
<p>OpenManus中的Agent的实现是这样的：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">        ┌─────────────┐</span><br><span class="line">        │  BaseAgent  │</span><br><span class="line">        └──────┬──────┘</span><br><span class="line">               │</span><br><span class="line">               ▼</span><br><span class="line">        ┌─────────────┐</span><br><span class="line">        │ ReActAgent  │</span><br><span class="line">        └──────┬──────┘</span><br><span class="line">               │</span><br><span class="line">               ▼</span><br><span class="line">        ┌─────────────┐</span><br><span class="line">        │ToolCallAgent│</span><br><span class="line">        └──────┬──────┘</span><br><span class="line">               │</span><br><span class="line">  ┌────────────┬────────────┐</span><br><span class="line">  │            │            │</span><br><span class="line">  ▼            ▼            ▼</span><br><span class="line">┌─────────┐ ┌───────┐ ┌───────┐</span><br><span class="line">│Planning │ │SWE    │ │Manus  │</span><br><span class="line">│Agent    │ │Agent  │ │       │</span><br><span class="line">└─────────┘ └───────┘ └───────┘</span><br></pre></td></tr></table></figure>
<p>采用了多层集成的方式来实现不同的Agent，而每个Agent都是一个ReAct
Agent，迭代地执行任务，直到任务完成或者超过循环次数。比如Manus
Agent设置的默认最大循环次数就是20轮。</p>
<ul>
<li>BaseAgent：抽象类，run是主入口，run中迭代执行step<br>
</li>
<li>ReActAgent：step中加入抽象函数reasoning和action<br>
</li>
<li>ToolCallAgent：引入工具，实装了think和action，think中决定使用什么工具<br>
</li>
<li>Manus：OpenManus中的主要Agent，相对通用</li>
</ul>
<p>每个Agent有自己的prompt，定义了角色和任务。</p>
<p>除了system
prompt，OpenManus还使用了一个NEXT_STEP_PROMPT。NEXT_STEP_PROMPT主要让模型选择下一步干什么。比如Manus
Agent的NEXT_STEP_PROMPT：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">NEXT_STEP_PROMPT = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Based on user needs, proactively select the most appropriate tool or combination of tools. For complex tasks, you can break down the problem and use different tools step by step to solve it. After using each tool, clearly explain the execution results and suggest the next steps.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>使用上，NEXT_STEP_PROMPT是以user角色输入的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, </span><br><span class="line">        <span class="string">&quot;content&quot;</span>: SYSTEM_PROMPT</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, </span><br><span class="line">        <span class="string">&quot;content&quot;</span>: query</span><br><span class="line">    &#125;, </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, </span><br><span class="line">        <span class="string">&quot;content&quot;</span>: NEXT_STEP_PROMPT</span><br><span class="line">    &#125;</span><br><span class="line">] </span><br></pre></td></tr></table></figure>
<p>我试了下，连续两个user角色输入，和把两次输入的content直接拼接在一起，response回复的差异不大。其实就可以把NEXT_STEP_PROMPT视作一个template，它和user
query一起构成了user
prompt，包含了任务query，以及设定好的instruction。</p>
<p>（2）Tool</p>
<p>OpenManus中的tool主要包括代码执行（python代码解释器、Bash等），信息搜索（Google
Search和BrowserUse），文件操作等。</p>
<p>所有工具都继承自 BaseTool 基类：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BaseTool</span>(ABC, BaseModel):</span><br><span class="line">    name: <span class="built_in">str</span></span><br><span class="line">    description: <span class="built_in">str</span></span><br><span class="line">    parameters: <span class="type">Optional</span>[<span class="built_in">dict</span>] = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">class</span> <span class="title class_">Config</span>:</span><br><span class="line">        arbitrary_types_allowed = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, **kwargs</span>) -&gt; <span class="type">Any</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Execute the tool with given parameters.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">await</span> self.execute(**kwargs)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">execute</span>(<span class="params">self, **kwargs</span>) -&gt; <span class="type">Any</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Execute the tool with given parameters.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">to_param</span>(<span class="params">self</span>) -&gt; <span class="type">Dict</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Convert tool to function call format.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&quot;type&quot;</span>: <span class="string">&quot;function&quot;</span>,</span><br><span class="line">            <span class="string">&quot;function&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;name&quot;</span>: self.name,</span><br><span class="line">                <span class="string">&quot;description&quot;</span>: self.description,</span><br><span class="line">                <span class="string">&quot;parameters&quot;</span>: self.parameters,</span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>每个工具都通过 parameters 字典定义其接受的参数，遵循 JSON Schema
格式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">parameters: <span class="built_in">dict</span> = &#123;</span><br><span class="line">    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;object&quot;</span>,</span><br><span class="line">    <span class="string">&quot;properties&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;command&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;description&quot;</span>: <span class="string">&quot;The command to execute...&quot;</span>,</span><br><span class="line">            <span class="string">&quot;enum&quot;</span>: [<span class="string">&quot;create&quot;</span>, <span class="string">&quot;update&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;get&quot;</span>, <span class="string">&quot;set_active&quot;</span>, <span class="string">&quot;mark_step&quot;</span>, <span class="string">&quot;delete&quot;</span>],</span><br><span class="line">            <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="comment"># 其他参数...</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;required&quot;</span>: [<span class="string">&quot;command&quot;</span>],</span><br><span class="line">    <span class="string">&quot;additionalProperties&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用的时候，工具通过to_param转成模型可接受的格式。</p>
<p>（3）Flow</p>
<p>Agent理解任务，输出action，Tool进行操作，返回结果。这中间的流转就需要Flow来调度。</p>
<p>个人理解上，Flow的设计可能和场景和任务比较有关联。比如一般来说就是串行的，模型
→ 工具 → 模型 → 工具 →
回复这样，而在一些场景下也可能有并行的，比如DeepSearch中拆解出来的任务是可以并行搜索的。</p>
<p>OpenManus中实现的PlanningFlow大致上就是串行为主的设计。</p>
<p>（4）Sandbox</p>
<p>Agent在执行任务过程中会对文件等进行很多操作，这其实是有风险的，因此需要一个沙箱环境来隔离，免得把主机搞崩了。</p>
<p>2、流程</p>
<p>OpenManus的流程是典型的：思考+行动循环。</p>
<p>3、架构分层</p>
<p>分为5层：</p>
<blockquote>
<ul>
<li>用户层：通过命令行界面与用户交互，处理输入并展示结果。入口文件为
run_flow.py。<br>
</li>
<li>Flow层：作为框架的调度中心，负责任务编排和执行策略。核心组件是
PlanningFlow。<br>
</li>
<li>Agent层：实现任务执行逻辑，具备思考(think)和行动(act)能力。包含
ToolCallAgent、PlanningAgent 和 Manus 等。<br>
</li>
<li>Tool层：提供可扩展的工具集，增强 Agent 的执行能力。核心工具为
PlanningTool。<br>
</li>
<li>LLM层：统一封装大语言模型接口，支持多种模型配置。</li>
</ul>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># OpenManus分层架构示意图</span><br><span class="line"></span><br><span class="line">┌───────────────┐</span><br><span class="line">│   用户层       │</span><br><span class="line">└──────┬────────┘</span><br><span class="line">       ↓</span><br><span class="line">┌───────────────┐</span><br><span class="line">│   Flow层      │</span><br><span class="line">└──────┬────────┘</span><br><span class="line">       ↓</span><br><span class="line">┌───────────────┐</span><br><span class="line">│   Agent层     │</span><br><span class="line">└──────┬────────┘</span><br><span class="line">       ├──────────────┐</span><br><span class="line">       ↓              ↓</span><br><span class="line">┌───────────────┐ ┌───────────────┐</span><br><span class="line">│   LLM层       │ │   Tool层      │</span><br><span class="line">└───────────────┘ └───────────────┘</span><br></pre></td></tr></table></figure>
<p>详细的可以看<a href="https://zhuanlan.zhihu.com/p/1897429266813125945">《OpenManus
代码框架详解》</a></p>
<h1 id="owl">OWL</h1>
<p>OWL = OPTIMIZED WORKFORCE LEARNING</p>
<p>OWL最近出了技术报告，看下它提供了什么信息。</p>
<p>首先是分析现有的multi-agent system的问题：</p>
<ul>
<li>目前的MAS大部分是domain-specific的，cross-domain
transferability受限。<br>
</li>
<li>推理上：一般需要针对业务场景定制，无法一个系统通用处理不同场景下的问题，比如MetaGPT，就是针对代码场景的设计，依赖SOP进行开发。<br>
</li>
<li>训练上：大部分多Agent框架在训练的时候需要优化多个Agent，比如MALT（Multi-Agent
LLM Training）。</li>
</ul>
<p>OWL的设计包含三个主要模块：</p>
<ul>
<li>Domain-Agnostic Planner：做abstract task
decompositions，不和任何场景绑定<br>
</li>
<li>Coordinator：把Planner拆解出来的task分配给不同的worker<br>
</li>
<li>Domain Agent（Worker
Nodes）：worker，封装工具API，负责完成某项具体任务</li>
</ul>
<p>下图左边是传统的框架，右边是OWL的framework，推理的WORKFORCE和训练的Optimized
Workforce Learning：</p>
<img src="/226b059f/owl_overview.png" class title="agent">
<p>WORKFORCE的一个示意图：</p>
<img src="/226b059f/owl_workforce.png" class title="agent">
<p>其中有一个task
channel，coordinator和worker的信息发布和结果发布都在这里，各个worker之间是互不相见的，仅由coordinator来调配。</p>
<p>个人感觉OWL的核心设计其实就是Planner和Coordinator的拆分。通常来说，这二者的任务是可以由同一个角色来完成的。拆分开之后，在不同的任务场景下，Coordinator和worker都可以不动，仅通过特异化训练Planner来适配不同场景。</p>
<p>Planner使用了Qwen+SFT+DPO，效果就有比较不错的提升。</p>
<p>OWL附录里提供各个Agent包括Worker的prompt，基本上是比较标准的agent
prompt。有一点可以参考的就是，prompt中把整体的角色定义放在的system
prompt，而具体的信息和规则就放在user
prompt。这应该和模型训练时会更多follow system
prompt的大原则的特点相关。</p>
<p>总体上，OWL的拆解有点像职场那样，进行了层级的拆解：有领导planner，只管拆任务，拆完任务就做完了；coordinator是基层管理者，负责worker工作的发布和管理。这有点上升到管理哲学的层面了，到底这样的架构有多大的实际收益，个人有点打问号。</p>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- Agent：<br>
<a href="https://www.linsight.cn/b242bfb3.html">Agent完全手册(零)：三大模块，三个理念</a><br>
<a href="https://www.linsight.cn/44c62dc5.html">DeepResearch的报告生成方法</a><br>
<a href="https://www.linsight.cn/7c2f9dcb.html">从RAG到DeepSearch</a><br>
- MoE：<br>
<a href="https://www.linsight.cn/a9c496e3.html">DeepSeek-V3细节探索</a><br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/37ee84bb.html">Qwen3实测&amp;技术报告</a><br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="https://www.linsight.cn/6c0f6207.html">Qwen2.5-1M技术解析</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="https://www.linsight.cn/4519eadd.html">prompt压缩(一)</a><br>
<a href="https://www.linsight.cn/ea2871bf.html">prompt压缩(二)</a><br>
<a href="https://www.linsight.cn/bfa4f144.html">reasoning压缩(一)</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a href="https://www.linsight.cn/657a6d17.html">基模型Cognitive
Behaviors对RL的影响</a><br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="https://www.linsight.cn/2dee4921.html">LLM水印</a><br>
- 训练框架<br>
<a href="https://www.linsight.cn/fe0adaa5.html">LLM训练框架：从优化器和精度讲到ZeRO</a><br>
<a href="https://www.linsight.cn/4cd8532f.html">LLM训练各种并行策略</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
<a href="https://www.linsight.cn/d253d7b3.html">关于The Bitter
Lesson</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
<a href="https://www.linsight.cn/b4d047c1.html">DeepSeek-VL2的细节</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】OWL: Optimized Workforce Learning for General Multi-Agent
Assistance in Real-World Task
Automation，https://github.com/camel-ai/owl/blob/main/assets/OWL_Technical_Report.pdf<br>
【2】MetaGPT，https://docs.deepwisdom.ai/v0.4/zh/guide/tutorials/concepts.html<br>
【3】OpenManus
代码框架详解，https://zhuanlan.zhihu.com/p/1897429266813125945<br>
【4】[源码学习]
通过OpenManus了解Agent系统的实现，https://zhuanlan.zhihu.com/p/1904129741365154580<br>
【5】从prompt看OpenManus的实现思路，https://zhuanlan.zhihu.com/p/1889982149991580008<br>
【6】OpenManus LLM
工具调用机制详解，https://zhuanlan.zhihu.com/p/1886362220297967012</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>Agent</tag>
      </tags>
  </entry>
  <entry>
    <title>Qwen2.5-1M技术解密</title>
    <url>/6c0f6207.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>在看Qwen2.5-1M的方案之前，先把Qwen2.5-1M中用到的技术，DCA，MInference
1.0和chunked prefill学习一下。</p>
<p>其他长文本相关文章：</p>
<p><a href="https://mp.weixin.qq.com/s/Ci9tAMIER0Aj96sK81HNcw?token=1318369845&amp;lang=zh_CN">LLM长上下文的问题</a><br>
<a href="https://mp.weixin.qq.com/s/FTewPxSr5fcwkxAgRZm7Wg?token=1318369845&amp;lang=zh_CN">解锁大模型长上下文能力</a><br>
<a href="https://mp.weixin.qq.com/s/NaTtwURRw7lsG55QTIaVsA?token=1318369845&amp;lang=zh_CN">大模型推理窗口-从有限到无限大</a><br>
<a href="https://mp.weixin.qq.com/s/QEHdtJKsY7lIU0aK8CeEkg?token=1318369845&amp;lang=zh_CN">理解LLM位置编码:RoPE</a></p>
<h1 id="dual-chunk-attention-dca">Dual Chunk Attention (DCA)</h1>
<p>DCA是一个不用进行训练，就可以有效进行窗口长度外推的方法。（LLM位置编码现在默认都是基于RoPE的了，DCA也是。）</p>
<p>DCA可以不用训练进行外推，也是可以用在训练中的，而且有训练的效果肯定比不训练的外推更好。在不训练的情况下，DCA就可以把在4k窗口训练的模型外推到32k，这相比其他主流外推方案（PI、NTK、YaRN等）都算是比较强的。</p>
<h2 id="方案">方案</h2>
<p>来看下DCA是怎么做的。假设现在有一个模型，训练窗口的长度为6，在处理sequence
length = 12的输入时，relative position matrix M是这样的：</p>
<img src="/6c0f6207/dca_ori.png" class title="Qwen2.5-1M">
<p>M中出现了 ≥
6的相对距离，这些距离的值在训练的时候模型没有见过，这就导致了输出效果变差。</p>
<p>DCA的大致思路就是重新构造这个relative position matrix
M：把长输入（超过预训练窗口长度）拆分成多个chunk，在这个基础上计算三种attention：（1）intra-chunk
attention（2）inter-chunk attention（3）successive-chunk attention。</p>
<p>1、intra-chunk attention</p>
<p>既然使用没有训练过的相对距离值会影响模型效果，那在M中就只使用不大于训练长度的距离值。把输入切分成多个大小固定的chunk，保证chunk
size s ≤ pretrain window size
c。这样每个chunk内部的token间距离单独计数，就不会出现超过预训练长度的距离了。</p>
<p>下面这个例子就是pretrain window size c = 10，chunk size s =
6，当前输入长度为12时，intra-chunk attention对应的M的值：</p>
<img src="/6c0f6207/dca_intra_chunk.png" class title="Qwen2.5-1M">
<p>实验中，通常把s设置为 <span class="math inline">\(\frac{3}{4}c\)</span> 的大小。</p>
<p>实现上只要修改q和k的position
index就可以了。比如原来q和k的位置下标都是</p>
<p>0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11</p>
<p>现在切分成两个长度为6的chunk，每个chunk内部的token index都是</p>
<p>0, 1, 2, 3, 4, 5</p>
<p>对于整个input就是</p>
<p>0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5</p>
<p>公式化来说就是</p>
<p><span class="math display">\[
P_{q}^{Intra} = P_{k} = [0, 1, \cdots, l - 1] \bmod s
\]</span></p>
<p><span class="math display">\[
M[i][j] = P_{q}^{Intra}[i] - P_{k}[j]
\]</span></p>
<p>2、inter-chunk attention</p>
<p>仅使用intra-chunk
attention相当于把各个chunk视为独立（相当于截断了），每个token只能看到和自己同个chunk的信息，这当然就会损失不同chunk之间的信息关联，因此就要加入inter-chunk
attention来让模型捕捉多个chunk之间的关系。</p>
<p>由于要保证M中的值 &lt;
c，所以这里不能直接使用原始的token距离。观察到在原始的M中，有效的距离值构成下三角矩阵，而q的位置总是大于k的位置的，因此inter-chunk
attention对应的值总是 &gt; 0的。</p>
<p>在符合这两个条件的前提下，一个方法就是把q的位置index全部设到最大，即预训练窗口长度大小（=
c - 1），对应的M的值如下：</p>
<img src="/6c0f6207/dca_inter_chunk.png" class title="Qwen2.5-1M">
<p><span class="math display">\[
P_{q}^{Inter} = [\underbrace{c - 1, c - 1, \cdots, c - 1}_{l \text{
elements}}]
\]</span></p>
<p><span class="math display">\[
M[i][j] = P_{q}^{Intra}[i] - P_{k}[j] = c - 1 - P_{k}[j] \geq c - s
\]</span></p>
<p>inter-chunk
attention没有对chunk的具体位置纳入考虑，而只给出两个chunk的相对前后关系。比如“chunk1和chunk3”的inter-chunk
attention与“chunk1和chunk4”的inter-chunk
attention所使用的相对位置是一样的，模型没法据此区分chunk3和chunk4的相对位置。（不过decoder-only的模型本身具有一定的位置编码能力，这里只是说没有显式地在位置编码中体现）</p>
<p>（其实inter-chunk
attention可不可以有别的设计呢，个人感觉是可以的，只是原文中使用的方案如此；总之只要保证使用不超过训练窗口大小的位置编码的情况下计算chunk之间的attention就可以）</p>
<p>3、successive-chunk attention</p>
<p>上面这种inter-chunk
attention的设计在计算「不相邻」的chunk的时候是没有问题的，但是对于两个相邻的chunk就有问题了。</p>
<p>比如看上面图中，红色子矩阵的第一行，inter-chunk距离是</p>
<p>9，8，7，6，5，4</p>
<p>继续往后数就进入到intra-chunk
attention的范围，看上上图，下一个距离是0。</p>
<p>连起来就是</p>
<p>9，8，7，6，5，4，0</p>
<p>从4到0这里有个突变，这样的距离不连续，会加重模型对attention机制理解的负担。因此对于相邻的chunk，它们之间的inter-chunk
attention要稍微修改一下。</p>
<p>对于每个chunk，position的index变成：</p>
<p><span class="math display">\[P_{\mathbf{q}}^{\mathrm{Succ}}=[\overbrace{\underbrace{s,s+1,\ldots,s+w-1}_{\text{the
same for all chunks}}}^{w\mathrm{~elements}}]\]</span></p>
<p>这里有一个local window size
w，w的物理意义可以认为是：在这两个相邻chunk的之内，每个token都可以保证正常看到的窗口到小。在这个窗口内，距离的计算和原生的RoPE是保持一致的。而在w之外，则会出现分辨率的降低。</p>
<p>文中提到w的值可以直接设置为c - s，比如s = <span class="math inline">\(\frac{3}{4}c\)</span>，那就有w = <span class="math inline">\(\frac{1}{4}c\)</span>。</p>
<p>在这个具体的例子里，原来的q的position indices是：</p>
<p><span class="math display">\[P_{\mathbf{q}}^{\mathrm{Inter}}=[\underbrace{9,9,9,9,9,9}_{\mathrm{chunk~}0},\underbrace{9,9,9,9,9,9,9}_{\mathrm{chunk~}1}]\]</span></p>
<p>现在变成</p>
<p><span class="math display">\[P_{\mathbf{q}}^{\mathrm{Succ}}=[\underbrace{6,7,8,9,9,9}_{\mathrm{chunk~}0},\underbrace{6,7,8,9,9,9}_{\mathrm{chunk~}1}]\]</span></p>
<img src="/6c0f6207/dca_successive_chunk.png" class title="Qwen2.5-1M">
<h2 id="完整dca">完整DCA</h2>
<p>结合上面这三种方式的attention，最终的relative position
matrix是这样的：</p>
<p><span class="math display">\[
M[i][j] =
\begin{cases}
P_{q}^{Intra}[i] - P_{k}[j] &amp; \text{if } \lfloor i / s\rfloor -
\lfloor j / s\rfloor = 0 \\
P_{q}^{Succ}[i] - P_{k}[j] &amp; \text{if } \lfloor i / s\rfloor -
\lfloor j / s\rfloor = 1 \\
P_{q}^{Inter}[i] - P_{k}[j] &amp; \text{if } \lfloor i / s\rfloor -
\lfloor j / s\rfloor &gt; 1
\end{cases}
\]</span></p>
<p>注意inter-chunk attention只用在不相邻的chunk之间了。</p>
<p>再看一个例子配合理解：在s = 4，c = 8，w =
3，输入长度为12的情况下：</p>
<img src="/6c0f6207/dca_example.png" class title="Qwen2.5-1M">
<p>在ppl和几个长文本benchmark上，DCA（CHUNKLLAMA）也能比其他放好一些：</p>
<img src="/6c0f6207/dca_perf.png" class title="Qwen2.5-1M">
<h2 id="dca和flash-attention一起使用">DCA和Flash Attention一起使用</h2>
<p>DCA还有一个好处，就是可以和Flash Attention一起用。</p>
<p>Pseudocode of DCA with FlashAttention：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># q: 1 x d query vector (tensor with shape [1, d])</span></span><br><span class="line"><span class="comment"># i: the absolute index of q (integer)</span></span><br><span class="line"><span class="comment"># K, V: i x d matrices for keys and values (tensors with shape [i, d])</span></span><br><span class="line"><span class="comment"># s: chunk size (integer)</span></span><br><span class="line"><span class="comment"># P_k, P_q_intra, P_q_succ, P_q_inter: position ids (lists of integers)</span></span><br><span class="line">n = math.floor(i/s) <span class="comment"># Number of chunks before the current chunk</span></span><br><span class="line"><span class="comment"># Apply rotary position embeddings to the entire key matrix K</span></span><br><span class="line">K = apply_rotary_pos_emb(K, P_k) <span class="comment"># K is [i, d] after embedding</span></span><br><span class="line"><span class="comment"># ------------- Intra-chunk Attention, casual=True -------------</span></span><br><span class="line">q_intra = apply_rotary_pos_emb(q, P_q_intra[i]) <span class="comment"># q_intra is [1, d]</span></span><br><span class="line"><span class="comment"># Select intra-chunk keys and values</span></span><br><span class="line">K_intra = K[s*n:i] <span class="comment"># K_intra is [(i - s*n), d]</span></span><br><span class="line">V_intra = V[s*n:i] <span class="comment"># V_intra is [(i - s*n), d]</span></span><br><span class="line"><span class="comment"># Compute output and softmax attention map for intra-chunk attention</span></span><br><span class="line">o_intra, map_intra = Flash(q_intra, K_intra, V_intra) <span class="comment"># o_intra is [1, d], map_intra is [1, i - s*n]</span></span><br><span class="line"><span class="comment"># ------------- Successive-chunk Attention, casual=False -----------</span></span><br><span class="line">q_succ = apply_rotary_pos_emb(q, P_q_succ[i]) <span class="comment"># q_succ is [1, d]</span></span><br><span class="line"><span class="comment"># Select successive-chunk keys and values</span></span><br><span class="line">K_succ = K[s*(n-<span class="number">1</span>):s*n] <span class="comment"># K_succ is [s, d]</span></span><br><span class="line">V_succ = V[s*(n-<span class="number">1</span>):s*n] <span class="comment"># V_succ is [s, d]</span></span><br><span class="line"><span class="comment"># Compute output and softmax attention map for successive-chunk attention</span></span><br><span class="line">o_succ, map_succ = Flash(q_succ, K_succ, V_succ) <span class="comment"># o_succ is [1, d], map_succ is [1, s]</span></span><br><span class="line"><span class="comment"># ------------- Inter-chunk Attention, casual=False -----------</span></span><br><span class="line">q_inter = apply_rotary_pos_emb(q, P_q_inter[i]) <span class="comment"># q_inter is [1, d]</span></span><br><span class="line"><span class="comment"># Select inter-chunk keys and values</span></span><br><span class="line">K_inter = K[:s*(n-<span class="number">1</span>)] <span class="comment"># K_inter is [s*(n-1), d]</span></span><br><span class="line">V_inter = V[:s*(n-<span class="number">1</span>)] <span class="comment"># V_inter is [s*(n-1), d]</span></span><br><span class="line"><span class="comment"># Compute output and softmax attention map for inter-chunk attention</span></span><br><span class="line">o_inter, map_inter = Flash(q_inter, K_inter, V_inter) <span class="comment"># o_inter is [1, d], map_inter is [1, s*(n-1)]</span></span><br><span class="line"><span class="comment"># Normalization</span></span><br><span class="line"><span class="comment"># Sum the attention maps for each attention type to get normalizers</span></span><br><span class="line">sum_intra = map_intra.<span class="built_in">sum</span>(-<span class="number">1</span>) <span class="comment"># sum_intra is a scalar</span></span><br><span class="line">sum_inter = map_inter.<span class="built_in">sum</span>(-<span class="number">1</span>) <span class="comment"># sum_inter is a scalar</span></span><br><span class="line">sum_succ = map_succ.<span class="built_in">sum</span>(-<span class="number">1</span>) <span class="comment"># sum_succ is a scalar</span></span><br><span class="line">normalizer = sum_intra + sum_inter + sum_succ <span class="comment"># normalizer is a scalar</span></span><br><span class="line"><span class="comment"># Concatenate attention outputs and divide by normalizer</span></span><br><span class="line">output = (sum_intra*o_intra, sum_succ*o_succ, sum_inter*o_inter) / normalizer <span class="comment"># output is [1, d]</span></span><br></pre></td></tr></table></figure>
<h1 id="minference-1.0">MInference 1.0</h1>
<p>MInference
1.0是一个理论有损的推理加速框架，加速的是pre-filling的阶段。在1M上下文长度的情况下，首字的推理速度相比Flash
Attention-2快10倍（8B模型，单卡A100条件下）：</p>
<img src="/6c0f6207/minfer_speed.png" class title="Qwen2.5-1M">
<p>虽然理论有损，但是在下游任务上的实验，可以做到很接近完全attention计算的效果。</p>
<p>（作者在知乎上有亲自解读MInference
1.0，写得比较接地气，挺实在；原文在 <a href="https://zhuanlan.zhihu.com/p/707815545">https://zhuanlan.zhihu.com/p/707815545</a>）</p>
<h2 id="长上下文的推理瓶颈">长上下文的推理瓶颈</h2>
<p>先看下长上下文情况下推理效率的问题。这里说的长上下文，是几百k甚至M级别的长度。在这样的长度下，pre-filling的耗时就很长，其中大部分是花在attention上的。</p>
<img src="/6c0f6207/minfer_prefill.png" class title="Qwen2.5-1M">
<p>一个8B的模型，在单卡A100上，对1M的输入进行pre-filling，耗时就要30分钟，说明pre-filling在很长的上下文时是一个效率瓶颈。MInference
1.0针对的就是pre-filling阶段的问题。</p>
<p>细看长文本下的attention计算，作者发现attention计算在long-context场景下是很稀疏的，也就是少部分的token贡献了大部分的attention
score。这也很符合直觉，毕竟在几万甚至几十万token的上下文中，不可能每个token都跟当前token紧密相关。</p>
<p>对于一个128k的prompt，它的attention
matrix大小是128k×128k。对模型每层的每个头，如果仅保留attention中top-4k的column，就可以召回96.4%的attention
score，说明这4k
token就贡献了大部分的注意力得分，而剩下的124k基本上都是near-zero
element，在attention计算中贡献率很低。</p>
<img src="/6c0f6207/minfer_sparse.png" class title="Qwen2.5-1M">
<p>如果把这条prompt的top4k indice应用到另一个128k
prompt，召回率就只有83.7%了。</p>
<img src="/6c0f6207/minfer_dynamic_sparsity.png" class title="Qwen2.5-1M">
<p>这说明不同的prompt有不同的topk
pattern，attention的稀疏分布是input-dependent的，其根据输入的不同，稀疏的分布也大不相同。因此之前一些人为设计的，固定的稀疏attention
pattern都无法很好解决这个dynamicity的问题：</p>
<img src="/6c0f6207/minfer_attention_pattern.png" class title="Qwen2.5-1M">
<p>那么综合起来，长文本推理的瓶颈之一就是pre-filling阶段复杂的attention计算，而attention计算的特点有两个：「Attention
is Dynamically Sparse」。</p>
<p>一个理想的efficient long-context
attention需要兼顾attention的sparsity与dynamicity：根据attention的输入，动态地估计出一个input-dependent的稀疏mask，从而完成attention的稀疏运算。</p>
<h2 id="注意力的稀疏模式">注意力的稀疏模式</h2>
<p>虽然attention的稀疏分布是input-dependent的，但是也不是完全没有规律，还是有一些模式的。参考StreamLLM和LM-infinite（<a href="https://mp.weixin.qq.com/s/NaTtwURRw7lsG55QTIaVsA?token=1318369845&amp;lang=zh_CN">大模型推理窗口-从有限到无限大</a>）中的分析，attention的sparse
pattern主要可以分成A-shape、Vertical-Slash (VS), 和
Block-Sparse三类。</p>
<p>A-shape就是StreamLLM中所用的，集中在local和initial
token的计算；Vertical-Slash
(VS)中的vertical则是对应某些特殊token，而slash斜线则是强调相对位置关系；而Block-Sparse的分块聚集模式则相当于有多个局部重点。</p>
<p>为什么这些pattern可以高效运算，但Top-K sprase
attention不可以？因为这三种pattern都呈现出了空间聚集性，这就为GPU加速提供了条件。GPU
kernels可以使用 64×64（A-shape和block-sparse heads）或 64×1（v-s
heads）的block来高效地完成稀疏运算。而Top-K sprase attention (i.e.,
对每个Q只计算top-k个K向量)，由于其过分fine-grained的sparse分布，需要花费很长的时间build
sparse index，且在GPUs上使用block进行运算时会产生大量的无效运算。</p>
<img src="/6c0f6207/minfer_pattern.png" class title="Qwen2.5-1M">
<h2 id="实现加速">实现加速</h2>
<p>接下来的问题就是怎么对这三种pattern的attention计算进行加速。</p>
<blockquote>
<p>在MInference明确三种稀疏pattern后，其将完成以下三个步骤：（1）给定一定的FLOPs
budget下，为每个attention
head寻找最优的pattern。（2）对每一个input，动态计算最优的sparse分布（e.g.,
竖线-斜线的位置，或block的index）。（3）根据第二步得到的sprase
index，进行attention的稀疏运算。</p>
</blockquote>
<p>在给定的FLOPs预算下，为了搜索最优的sparse
pattern，MInference提出了Kernel-Aware Sparse Pattern
Search。它能够给出（1）当前的head属于三种pattern中的哪一种（2）计算当前head的最优稀疏率。</p>
<p>这里有一个关键发现，就是作者发现attention head的sparse
pattern种类是input-independent的，因此sparse pattern
search是可以offline提前完成的。</p>
<h2 id="效果">效果</h2>
<p>在效果上，虽然MInference理论上是有损的，但是具体任务上效果挺好：</p>
<img src="/6c0f6207/minfer_perf.png" class title="Qwen2.5-1M">
<h1 id="chunked-prefill">chunked prefill</h1>
<p>原文是《SARATHI: Efficient LLM Inference by Piggybacking Decodes with
Chunked Prefills》，现在在很多推理框架都用上了，比如vllm。</p>
<p>（这块比较底层，我只能写个大概）</p>
<p>LLM在推理的时候可以分成prefill和decode。在输入很长的情况下，prefill的时候要处理所有token，显存的占用就很高，而GPU的利用率就没那么高；而在decode的时候，只处理一个token，显存的占用就没那么高。</p>
<p>无论是显存占用高，利用率低还显存占率低，利用率高，都不是对GPU资源的最好利用。最好的情况就是显存占用和计算利用率都高。那么一个方法就是在一个batch里既有prefill又有decode。这样就能榨干GPU的所有能力。</p>
<p>基于这个思路，大概的做法就是把各个输入prompt都切分成小一点的chunk。在推理的时候，这些chunk有些事prefill，有些是decode，通过现有显存和算力的余量来调度二者的比例，从而提升GPU整体的利用率和吞吐量。</p>
<h1 id="qwen2.5-1m">Qwen2.5-1M</h1>
<p>终于来到了Qwen2.5-1M。</p>
<p>听名字就知道，Qwen2.5这次把模型窗口提升到了1M。文中共report了三个模型，其中两个开源的，分别是wen2.5-7B-Instruct-1M和Qwen2.5-14B-Instruct-1M，另外还有一个通过API形式提供的Qwen2.5-Turbo，是个MoE模型（猜测应该是Qwen2.5-Max）。</p>
<p>模型结构上，Qwen2.5-1M集成Qwen2.5模型的设计，这个没什么好说的。</p>
<h2 id="多阶段预训练">多阶段预训练</h2>
<p>Qwen2.5-1M的预训练分成5个阶段，渐进式提升训练的窗口长度。随着长度的提升，RoPE的base
frequency也跟着增大：</p>
<table>
<thead>
<tr class="header">
<th>Phase</th>
<th>window size</th>
<th>base frequency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>4,096</td>
<td>10,000</td>
</tr>
<tr class="even">
<td>2</td>
<td>32,768</td>
<td>1,000,000</td>
</tr>
<tr class="odd">
<td>3</td>
<td>65,536</td>
<td>1,000,000</td>
</tr>
<tr class="even">
<td>4</td>
<td>131,072</td>
<td>5,000,000</td>
</tr>
<tr class="odd">
<td>5</td>
<td>262,144</td>
<td>10,000,000</td>
</tr>
</tbody>
</table>
<p>在各个阶段中，使用的数据里有75%和当前的窗口长度相同，而另外25%的数据则是较短的。这样可以保证模型长短文本能力的平衡。</p>
<p>各个阶段的模型在评测benchmark RULER上的指标如下：</p>
<img src="/6c0f6207/1m_ptm.png" class title="Qwen2.5-1M">
<p>可以看到，在最后训练长度为256k的阶段之后，128k的评测指标也有明显的提升。这点和《Why
does the effective context length of llms fall
short?》中观察到的一致。</p>
<h2 id="预训练数据">预训练数据</h2>
<p>真实世界的数据很少有达到128k甚至256k长度的，而那些达到这个长度的，往往也没有在信息上有真实的长距离依赖。因此需要借助合成数据的力量。合成数据主要包括下面这三种。</p>
<p>1、Fill in the Middle</p>
<p>FIM起源于训练代码补全能力，这个在之前讲代码的篇章《<a href="https://mp.weixin.qq.com/s/OllCcuxugOqf0aLkc9K-Mg?token=1318369845&amp;lang=zh_CN">代码大模型(一)--业界现状</a>》中也有提到。FIM要求模型根据跟定的上文和下文，补充中间的部分。</p>
<p>2、Keyword-Based and Position-Based Retrieval</p>
<p>让模型根据关键词检索对应的段落，增强其识别和连接文本不同部分相关信息的能力，同时提高其对序列中位置关系的理解。</p>
<p>3、Paragraph Reordering</p>
<p>段落被随机排列，模型需要对它们重新排序。这对于模型生成连贯的长文本文本很重要。</p>
<h2 id="sft数据和qwen-agent">SFT数据和Qwen-Agent</h2>
<p>跟Llama-3和LongAlign类似，Qwen2.5-1M从预训练语料中选择长文档，并根据这些长文档来生成QA，来进行SFT。生成的任务有很多，包括summarization,
information retrieval, multi-hop question answering, reasoning,
coding等等。</p>
<p>那么具体怎么从长文档获得QA呢？这里就借助了Qwen-Agent框架。Qwen-Agent又是啥呢？简单来说，就是通过RAG的方式，让较短窗口的模型（比如8k），可以处理长文档的一个框架。</p>
<p>Qwen-Agent包括三个level。</p>
<p>1、level 1：RAG</p>
<p>最naive的做法就是RAG。</p>
<p>假设现在有一个输入query，以及相关的1M长度的文档，要根据这个文档应答这个query。由于模型的窗口长度只有8k，因此第一步就要把文档切分成较短的段落，比如512
token的长度。接下来，就要从这些段落里找到和query相关的部分：</p>
<ul>
<li>step
1：把query中的信息和非信息部分分开。简单来说，信息部分就是需要检索的，非信息部分就是不需要检索的。比如query="回答时请用2000字详尽阐述，我的问题是，自行车是什么时候发明的？请用英文回复。"，就分为{"信息":
["自行车是什么时候发明的"], "指令": ["回答时用2000字", "尽量详尽",
"用英文回复"]}<br>
</li>
<li>step
2：要求模型从query的信息部分推导出多语言关键词。例如，短语"自行车是什么时候发明的"会转换为{"关键词_英文":
["bicycles", "invented", "when"], "关键词_中文": ["自行车", "发明",
"时间"]}<br>
</li>
<li>step 3：用BM25找到最相关的chunk</li>
</ul>
<img src="/6c0f6207/agent_level1.png" class title="Qwen2.5-1M">
<p>实践上，基于向量的检索在这里并没有太大的优势，而且带来的计算负担会大很多，因此还是用BM25比较实惠。</p>
<p>2、level 2：分块阅读</p>
<p>分块阅读相当于是RAG的进化版。</p>
<p>上面这样的RAG很方便快捷，但是也有问题。有时会出现一些相关chunk和query重叠不足（比如一个关键信息刚好被切分成前后两个chunk），导致检索失败的情况。为了解决这个问题，分块阅读采用了一种暴力的检索方式，具体来说分为三步：</p>
<ul>
<li>step
1：对于「每个」chunk，让模型评估它和query的相关性。如果相关则输出相关的句子。<br>
</li>
<li>step
2：把相关句子拿出来作为搜索输入，用BM25检索出最相关的chunk。<br>
</li>
<li>step 3：基于检索到的上下文生成答案。</li>
</ul>
<img src="/6c0f6207/agent_level2.png" class title="Qwen2.5-1M">
<p>3、level 3：逐步推理</p>
<p>在基于文档的问题回答中，一个典型的挑战是多跳推理。</p>
<p>比如这么一个问题：“与第五交响曲创作于同一世纪的交通工具是什么？”。模型首先需要确定子问题的答案，“第五交响曲是在哪个世纪创作的？”，得到“19世纪”，然后才可以知道“自行车于19世纪发明”跟问题相关。</p>
<p>Tool-calling agent或ReAct agent是经典的解决方案。因此，将level
2封装为一个工具，由工具调用智能体（Lv3-智能体）调用。tool-calling
agent进行多跳推理的流程如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Ask the Lv3-Agent a question.</span><br><span class="line"><span class="keyword">while</span> (the Lv3-Agent cannot answer the question based on its memory) &#123;</span><br><span class="line">    The Lv3-Agent proposes a new sub-question to be answered.</span><br><span class="line">    The Lv3-Agent asks the Lv2-Agent the sub-question.</span><br><span class="line">    Add the Lv2-Agent<span class="string">&#x27;s response to the Lv3-Agent&#x27;</span>s memory.</span><br><span class="line">&#125;</span><br><span class="line">The Lv3-Agent provides the final answer to the original question.</span><br></pre></td></tr></table></figure>
<img src="/6c0f6207/agent_level3.png" class title="Qwen2.5-1M">
<p>为了验证这个Agent的有效性，做了下实验。使用一个32k训练+外推256k的模型，以及4k-RAG和4k-Agent三个方案。在LV-Eval和NeedleBench上，效果是这样的：</p>
<img src="/6c0f6207/agent_perf.png" class title="Qwen2.5-1M">
<p>实验结果说明了以下几点：</p>
<ul>
<li>在短上下文场景中，4k-RAG的表现可能不如32k模型。这可能是由于RAG方案难以检索到正确的信息或理解多个片段造成的。<br>
</li>
<li>随着文档长度的增加，4k-RAG越发表现出超越32k模型的趋势。这一趋势表明32k模型在处理长上下文方面并没有训练到最优的状态。<br>
</li>
<li>4k-Agent始终表现优于32k模型和4k-RAG。它分块阅读所有上下文的方式使它能够避免原生模型在长上下文上训练不足而带来的限制。</li>
</ul>
<h2 id="sft">SFT</h2>
<p>SFT也分为两个阶段。第一阶段为短文本SFT，最大长度为32768，而第二阶段使用长短数据混合，长度从32768到262144不等。</p>
<p>Qwen2.5-1M还做了强化学习，不过只在8k的样本上机型了DPO。从结果上看，8k的DPO对长文本也有提升：</p>
<img src="/6c0f6207/1m_rl.png" class title="Qwen2.5-1M">
<p>不过这里的RL感觉有点匆忙，更像是跟了一下RL的风，似乎没有做一些深度的探索。</p>
<h2 id="推理">推理</h2>
<p>1、Length Extrapolation</p>
<p>推理时，首先就是使用前面提到的DCA，可以把256k的训练窗口再往外推。</p>
<p>另外就是使用YaRN的注意力缩放：</p>
<p><span class="math display">\[\mathrm{softmax}\left(\frac{\mathbf{q^Tk}}{t\sqrt{D}}\right),\mathrm{where}\sqrt{\frac{1}{t}}=0.1\ln(s)+1\]</span></p>
<p>这个在长窗口模型中也算是标配了。Qwen2.5-1M中始终是吧缩放和DCA一起使用的。</p>
<p>在NIAH和Passkey Retrieval上验证DCA的效果，还是不错的：</p>
<img src="/6c0f6207/1m_extrapolation.png" class title="Qwen2.5-1M">
<p>2、使用MInference</p>
<p>模型已经在256k的窗口训练过了，通过MInference可以把推理的窗口提升到1M。为了提升吞吐量，MInference会配合chunked
prefill使用。</p>
<img src="/6c0f6207/1m_minfer_chunk.png" class title="Qwen2.5-1M">
<p>3、sparse attention配合DCA</p>
<p>在结合MInference和DCA的时候，发现有些case出现了performance
drop。猜测可能的情况是DCA中非连续的距离影响了MInference
pattern的感知，一个解决方法就是在处理这些pattern的时候，恢复距离值的连续。</p>
<p>4、Sparsity Refinement</p>
<p>还记得，MInference需要离线先计算每个头的pattern，但是在长度为1M的情况下，计算每个头的attention
score所需要的显存太大了。Qwen2.5-1M的方法是sparsity
refinement。简单来说，就是随着长度增加，跟踪MInference的pattern的attention
score召回值，如果召回值低于阈值，那么久增加vertical或者slash的预算，把更多的值纳入计算。这样虽然会稍微增加MInference在推理时的计算量，但是在1M长度下能大大提升召回率：</p>
<img src="/6c0f6207/1m_attn_refine.png" class title="Qwen2.5-1M">
<h2 id="效果-1">效果</h2>
<img src="/6c0f6207/1m_perf.png" class title="Qwen2.5-1M">
<h1 id="小结">小结</h1>
<ul>
<li><p>在long CoT火热的背景下，长文本能力的重要性再次被强调</p></li>
<li><p>不仅需要模型能够完成大海捞针这的任务，在几百k甚至更大的长度下进行reasoning也是一个必要的需求</p></li>
<li><p>Qwen2.5-1M用了DCA、MInference、chunked prefill和Sparsity
Refinement等方案，结合很多效率和效果的优化，看起来效果是不错的</p></li>
<li><p>数据和算法上的篇幅占比相比工程优化减少了，只搞数据和调参远远不够了</p></li>
<li><p>工程上的有损加速方案应该是未来有前途的一个方向</p></li>
</ul>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a9c496e3.html">DeepSeek-V3细节探索</a><br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】使用Qwen-Agent将上下文记忆扩展到百万量级，https://qwenlm.github.io/zh/blog/qwen-agent-2405/<br>
【2】Training-Free Long-Context Scaling of Large Language
Models，https://arxiv.org/abs/2402.17463<br>
【3】Qwen2.5-1M Technical Report，https://arxiv.org/abs/2501.15383<br>
【4】MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via
Dynamic Sparse Attention，https://arxiv.org/abs/2407.02490<br>
【5】单卡可Million-context推理TTFT 10倍加速 - MInference
1.0，https://zhuanlan.zhihu.com/p/707815545<br>
【6】SARATHI: Efficient LLM Inference by Piggybacking Decodes with
Chunked Prefills，https://arxiv.org/abs/2308.16369<br>
【7】vLLM调度器解密（下）：chunked
prefill是如何进一步优化的？，https://zhuanlan.zhihu.com/p/6144374775</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>transformer</tag>
        <tag>长上下文</tag>
        <tag>窗口外推</tag>
        <tag>SFT</tag>
        <tag>pretrain</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepSeek-VL2</title>
    <url>/b4d047c1.html</url>
    <content><![CDATA[<p>【本文已在同名 微信公众号 / 知乎 / <a href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>看DeepSeek-VL2细节之前，先简单介绍下DeepSeek-VL2提到的recaption方案和visual
prompt数据。</p>
<h1 id="recaptionpixelprose">recaption：PixelProse</h1>
<h2 id="why-recaption">why recaption</h2>
<p>PixelProse是《From Pixels to Prose: A Large Dataset of Dense Image
Captions》中提供的一个（合成）caption数据集，共有16M条样本；论文同时也介绍了他们构造PixelProse的recaption方案。</p>
<p>为什么要做recaption？因为caption数据由来已久，业界有许多开源的caption数据集，这些数据集的收集、处理方式各不相同，数据内容和质量参差不齐。直接用这些数据训练会带入很多我们不想要的噪声，效果也不太好。</p>
<p>通过具体case的分析，主要有这么些问题：</p>
<ul>
<li>数据里存在一些NSFW和Child Sexual Abuse Material
(CSAM)的内容，这在很多场景都不合适甚至不合法<br>
</li>
<li>很多样本的图片和caption关联性太差，比如过于简短，或者缺乏准确的描述，这导致VL模型没法很好学习语言和图像之间的细节对齐关系<br>
</li>
<li>文字是VL模型和SD模型要学习的一个重点，但是现有的caption数据很多都没有给出图中文字的详细内容，使得模型很难学习文字</li>
</ul>
<h2 id="方案">方案</h2>
<p>针对分析出来的这些问题，合成高质量的caption数据的流程设计成这样：</p>
<img src="/b4d047c1/prose_pipeline.png" class title="DeepSeek-VL2">
<p>一步一步来看。</p>
<p>1、过滤</p>
<p>source data有三个，CommonPool、CC12M 和
RedCaps。当然如果现在我们要再多，那可以多加点数据集进去。首先，这些数据集通过NSFW分类模型和commercial
Google APIs进行内容过滤，仅保留图片内容合适合法的数据。</p>
<p>2、选择prompt</p>
<p>接下来，会从下面5个预定义的prompt中随机选择一个，用于让Gemini生成新的prompt。</p>
<img src="/b4d047c1/prose_prompt.png" class title="DeepSeek-VL2">
<p>3、加入alt-text</p>
<p>在生成的时候，会随机加入图片的alt-text到prompt中。参考《CapsFusion:
Rethinking Image-Text Data at
Scale》的发现，加入alt-text有机会提升生成结果细节的准确性。</p>
<p>4、加入Negative Descriptions</p>
<p>无论是VLM还是diffusion模型，对于negative描述的指令遵循能力总是比较差。比如跟diffusion模型说“画一幅没有大象的画”，最终画出来的画就有大象。</p>
<p>为了增强模型对negative
instruction的遵循能力，随机让Gemini增加一些途中不存在的物体的描述。比如“途中有5个红色的苹果”，就会再加上negative
description “但是没有任何梨子出现在途中”。</p>
<p>5、优化Text Recognition</p>
<p>文字能力是VLM和diffusion模型都很重要的一个能力，如果没有文字识别能力，多模态模型无法识别图片上的路标、广告牌、标签等信息，而diffusion模型在生成包含文字的图像时也会是一团乱码。</p>
<p>为了增强模型的文字理解能力，可以看到前面的5个prompt里都包含一个要求：</p>
<blockquote>
<p>If any text is present in the image, mention where it is, and the
font.Describe the text in detail with quotation marks.</p>
</blockquote>
<p>不过生成caption的模型识别文字的准确率不是100%，甚至可能只有七八成的准确率，所以后面还要check一下。</p>
<p>在校验之前，先用watermark
model分类一下，对于不包含watermark，且出现文字的图片，再用OCR模型进行识别。小于15个pixel的text
region会被抛弃。</p>
<p>最终check的结果表明大概有76%的文字可以被caption模型正确识别：</p>
<img src="/b4d047c1/prose_ocr_acc.png" class title="DeepSeek-VL2">
<img src="/b4d047c1/prose_ocr_case.png" class title="DeepSeek-VL2">
<p>当然OCR模型本身也不是100%正确的，对于样式复杂的情况，OCR模型也识别不准，不过整体上这个准确率校验还是可以参考的。</p>
<h2 id="pixelprose">PixelProse</h2>
<p>新合成的PixelProse文本长度相比原caption更长，包含更多细节信息：</p>
<img src="/b4d047c1/prose_length.png" class title="DeepSeek-VL2">
<p>从文本的分词结果上看，PixelProse所包含的名词多样性也更丰富：</p>
<img src="/b4d047c1/prose_noun.png" class title="DeepSeek-VL2">
<h1 id="visual-prompt">Visual Prompt</h1>
<p>这一part主要是讲一下visual prompt。</p>
<p>在纯文本的场景，prompt的使用大家都很熟悉的。而在多模态场景，一般来说用户指令也是以文本的形式给出，比如“图上这个人多少岁了”，“这只狗是什么品种”这样。</p>
<p>假设现在有一张图，上面有很多人，你想要针对其中某个人对模型进行询问。如果用文本进行描述的话，就有点困难：这些人可能没有很整齐地排列，衣着也没有鲜明特点；哪怕能够通过位置或者特征进行文字描述，这也会给模型的理解和识别造成困难。</p>
<p>回想一下，如果是在和人交流，那么要准确定位图上的一个人，最简单的方法就是用手指一下，或者拿笔在对应位置画个圈/箭头。那跟模型交流的时候也可以这么干：</p>
<img src="/b4d047c1/vp_example.png" class title="DeepSeek-VL2">
<p>这个圈/箭头就是visual prompt。</p>
<p>如果模型具备和这些圈/箭头进行交互的能力，那么用户在交互的时候就会更加自然。</p>
<h2 id="数据">数据</h2>
<p>要训练这样的能力，首先就要有数据。《ViP-LLaVA: Making Large
Multimodal Models Understand Arbitrary Visual
Prompts》就搞了一批数据。</p>
<p>1、source data</p>
<p>visual
prompt的数据还是通过数据合成获得。源数据就是现有各种物体识别/实体分割的数据，这些数据包含物体的位置和类型/名称信息，很方便改造成visual
prompt数据。</p>
<p>2、visual prompt type</p>
<p>研究人员定义了一下巴中visual
prompt类型，用于标识图像中的物体，总共有8种：</p>
<img src="/b4d047c1/vp_prompt.png" class title="DeepSeek-VL2">
<p>个人认为，这8种其实可以分成3个大类：</p>
<p>（1）外框</p>
<p>椭圆、长方形、三角形、物体的mask都属于把物体框起来的方式，只是有的框比较粗糙，有的比较精细。</p>
<p>在构造这类visual
prompt的时候，为了引入一定的随机性，会对外框的ratio、颜色和大小进行一定的随机变化，只要保证主要物体还在框里就行。</p>
<p>（2）箭头</p>
<p>箭头和把物体圈起来的做法不同，箭头一般画在物体附近，而且有方向性。</p>
<p>（3）涂鸦</p>
<p>scribble，contour和point其实都是类似涂鸦的方式，只是涂鸦的精细程度不同，point是最简陋的，contour是最精细的，而scribble介于两者之间。scribble是用贝塞尔曲线工具模拟人类轨迹画的。</p>
<h1 id="deepseek-vl2">DeepSeek-VL2</h1>
<p>DeepSeek-VL2开源了三个规模的模型，都是MoE：</p>
<ul>
<li>DeepSeek-VL2-Tiny：总参数3B，激活参数0.57B<br>
</li>
<li>DeepSeek-VL2-Small：总参数16B，激活参数2.4B<br>
</li>
<li>DeepSeek-VL2：总参数27B，激活参数4.1B</li>
</ul>
<p>原文给出的效果对比：</p>
<img src="/b4d047c1/ds_perf.png" class title="DeepSeek-VL2">
<p>不过这张图比的是激活参数。其实直接看总参数，DeepSeek-VL2的效果也是很不错的，只是没有看激活参数的优势那么大。从另一个角度想，如果DeepSeek通过模型架构和计算框架优化，可以把MoE+MLA结构做到和同样激活参数的dense模型相同效率的话，这么对比也不是不行。</p>
<p>DeepSeek-VL2相比前一代，主要有3个优化点：</p>
<ul>
<li>动态高分辨率vision encoding<br>
</li>
<li>LLM架构优化<br>
</li>
<li>数据构建pipeline优化</li>
</ul>
<p>LLM架构优化其实就是MoE +
MLA，带来的语言模型效率和效果提升，这部分在<a href="https://mp.weixin.qq.com/s/alKnPog2LYSRQdm9wy1_QA">《DeepSeek-V3细节探索》</a>中有细说，此处就不展开。三个开源模型的具体结构参数：</p>
<img src="/b4d047c1/ds_model.png" class title="DeepSeek-VL2">
<p>最小的Tiny模型没有使用MLA，而是使用MHA，这和我们之前对MLA的认知是一致的：模型每个头的大小并不需要很多，模型增大更多是增加头数，而MLA需要在头数更多的场景下才能发挥效率和效果的优势，因此模型越大MLA优势越大，而在小模型上MLA则不容易发挥优势。</p>
<p>另外，只有最大的DeepSeek-VL2使用了expert correction bias和sigmoid
routing function，这俩都跟expert parallelism有关。</p>
<p>另外有点奇怪的是只有small版本的vocab是102400，其他两个都是129280</p>
<p>DeepSeek-VL2整体框架还是标准的三件套：</p>
<img src="/b4d047c1/ds_overview.png" class title="DeepSeek-VL2">
<h2 id="动态分辨率dynamic-tiling-strategy">动态分辨率：Dynamic Tiling
Strategy</h2>
<p>使用高分辨率 + 动态分辨率基本上已经是现在的标准做法。</p>
<p>DeepSeek-VL2三个规模的模型使用的vision
encoder都是SigLIP-SO400M-384，这是一个基础分辨率为384 ×
384的模型。基于这个分辨率，定义了一批候选分辨率，这些候选分辨率的width和height都是384的倍数：</p>
<p><span class="math display">\[
C_R = \{(m\cdot 384, n\cdot 384) \mid m \in \mathbb{N}, n \in
\mathbb{N}, 1 \leq m, n, mn \leq 9\}
\]</span></p>
<p>对于每一个原始图像，会保持ratio进行resize到每个候选分辨率，并选择使用所需padding最少的候选resolution。</p>
<p>最后还会加上一个原图的缩略图，因此总用有（1 + m ×
n）个tile，每个tile都是384 × 384的大小，由vision encoder来单独处理。</p>
<p>以上是vision encoder的输出。接下来是VL Adaptor的处理。</p>
<p>SigLIP-SO400M-384使用的patch size = 14，每个tile会产生27 × 27个visual
embedding，会通过pixel unshuffle，把visual embedding的数量减少到14 ×
14个。</p>
<p>另外，为了帮助模型识别visual
embedding的位置关系，在缩略图和子图的每行visual embedding最后都会加一个
token，标识一下这一行embedding的结束。</p>
<p>这么一来总的token数就变成：</p>
<p>14 × (14 + 1) + 14m × (14n + 1)</p>
<p>最终得到的图像feature按这样排布：</p>
<img src="/b4d047c1/ds_tiling.png" class title="DeepSeek-VL2">
<p>动态分辨率的方案到这里就结束了。不知道有没有细心的同学发现，上面的基础分辨率384并不是patch
size 14的整数倍数（384 / 14 ≈
27.4），我也有点奇怪，搜索之下发现确实有问题：原来SigLIP-SO400M-384的真实分辨率并不是384，而是14
× 27 = 378，384只是由于历史遗留问题一直保持这么个写法。原链接在 <a href="https://huggingface.co/google/siglip-so400m-patch14-384/discussions/4">https://huggingface.co/google/siglip-so400m-patch14-384/discussions/4</a>。（这简直和“2020年东京奥运会在2021举办”有异曲同工之妙）。</p>
<h2 id="多阶段训练">多阶段训练</h2>
<p>DeepSeek-VL2的训练分三个阶段：</p>
<ul>
<li>对齐：训练adaptor和vision encoder，冻结LLM<br>
</li>
<li>预训练：全参训练<br>
</li>
<li>SFT：全参训练</li>
</ul>
<img src="/b4d047c1/ds_train.png" class title="DeepSeek-VL2">
<h2 id="数据-1">数据</h2>
<p>1、对齐</p>
<p>在对齐阶段，DeepSeek-VL2只用ShareGPT4v数据：包含1.2M条caption和conversation样本。</p>
<p>2、预训练</p>
<p>预训练阶段使用了70%的VL数据和30%纯文本数据。</p>
<p>（1）Interleaved image-text data</p>
<p>主要来自WIT、WikiHo和OBELICS，它们的混合比例通过在eepSeek-VL2-Tiny上实验确定；还有一个in-house数据集来增强真实世界知识的覆盖。</p>
<p>（2）Image captioning data</p>
<p>对现有的caption数据进行recaption处理，参考PixelProse的做法，在生成新caption的时候加入：</p>
<ul>
<li>OCR hints<br>
</li>
<li>meta information (e.g., location, camera settings)<br>
</li>
<li>original captions</li>
</ul>
<p>recaption之后还是存在一些质量问题，因此用DeepSeek
Chat对文本质量再进行打分和过滤，这样一来caption效果得到了有效提升</p>
<p>（3）OCR数据</p>
<p>包括LaTeX OCR和12M
RenderedText数据集，和一些in-house数据集，主要是中英文的。</p>
<p>（4）VQA数据</p>
<p>包括：<br>
- General VQA<br>
- Table, chart and document understanding<br>
- Web-to-code and plot-to-Python generation<br>
- QA with visual prompt</p>
<p>（5）Visual grounding data</p>
<p>数据样式：</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>Prompt: Locate <span class="op">&lt;|</span>ref<span class="op">|&gt;&lt;</span>query<span class="op">&gt;&lt;|/</span>ref<span class="op">|&gt;</span> <span class="kw">in</span> the given image.</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>Response: <span class="op">&lt;|</span>ref<span class="op">|&gt;&lt;</span>query<span class="op">&gt;&lt;|/</span>ref<span class="op">|&gt;&lt;|</span>det<span class="op">|&gt;</span>[[x1, y1, x2, y2],...]<span class="op">&lt;|/</span>det<span class="op">|&gt;</span></span></code></pre></div>
<blockquote>
<p>&lt;|ref|&gt;, &lt;|/ref|&gt;, &lt;|det|&gt;, &lt;|/det|&gt; are
special tokens. &lt;query&gt; is a place-holder for either the category
name (e.g., “car”) or description of the object (e.g., “the leftmost
person”). [[x1, y1, x2, y2], ...] is a list of bounding boxes, where
each bounding box corresponds to an object’s position. The coordinates
x1, y1 and x2, y2 specify the top-left and bottom-right corners
respectively, normalized to values between 0 and 999 according to the
resolution of the image.</p>
</blockquote>
<p>还另外构建了负样本，把一些object从原图上消去，以增加模型robustness。</p>
<p>（6）Grounded conversation data</p>
<p>数据样式：</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>Prompt: <span class="op">&lt;|</span>grounding<span class="op">|&gt;</span>Can you describe the content of the image?</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>Response: Two <span class="op">&lt;|</span>ref<span class="op">|&gt;</span>dogs<span class="op">&lt;|/</span>ref<span class="op">|&gt;&lt;|</span>det<span class="op">|&gt;</span>[[x1, y1, x2, y2],...]<span class="op">&lt;|/</span>det<span class="op">|&gt;</span> are running on the grass.</span></code></pre></div>
<p>3、SFT</p>
<p>（1）General visual question-answering</p>
<p>现有的VQA数据集有一些问题，包括：</p>
<ul>
<li>response太短<br>
</li>
<li>OCR质量差<br>
</li>
<li>有幻觉</li>
</ul>
<p>因此把original
question、image和OCR信息放在一起，重生成response，以提升数据质量。</p>
<p>（2）OCR and document understanding</p>
<p>预训练后模型的OCR能力已经很强了，因此sft阶段专注选出低质量样本，提升数据质量。</p>
<p>（3）Table and chart understanding</p>
<p>同OCR类似</p>
<p>（4）Reasoning, logic, and mathematics</p>
<p>发现detailed
response在小模型上的训练效果并不好，小模型对简洁的response的学习能力更好。</p>
<p>（5）Textbook and academic questions</p>
<p>使用了包含跨学科、大学水平的教科书内容的内部数据集。</p>
<p>（6）Web-to-code and plot-to-Python generation</p>
<p>对于开源数据也重新生成response提高质量。</p>
<p>（7）Visual grounding</p>
<p>把query翻译成了中文，还加了一个negative sample。</p>
<p>（8）Grounded conversation</p>
<p>使用《Groma: Localized visual tokenization for grounding multimodal
large language models》和《Flickr30k entities: Collecting
region-to-phrase correspondences for richer image-to-sentence
models》数据集构建对话数据。</p>
<p>（9）Text-Only datasets</p>
<p>使用了很多数据，但是没有给出比例。</p>
<p>一个总结，在数据这块DeepSeek-VL2在强调多样性的同时，也用现有的模型构建更强的pipeline重新生成response以提高数据质量。</p>
<h2 id="cases">cases</h2>
<img src="/b4d047c1/ds_case1.png" class title="DeepSeek-VL2">
<img src="/b4d047c1/ds_case2.png" class title="DeepSeek-VL2">
<h1 id="小结">小结</h1>
<ul>
<li>现有的多模态数据质量有高有低，直接使用可能有很好的效果<br>
</li>
<li>数据多样性能够有效提升模型训练效果<br>
</li>
<li>按这个趋势MoE有可能再次火起来？如果硬件的优化跟上，MoE说不定能成为attention一样的标准方案。MLA也同样有这个可能。</li>
</ul>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- MoE：<br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/a9c496e3.html">DeepSeek-V3细节探索</a><br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for
Advanced Multimodal
Understanding，https://www.arxiv.org/abs/2412.10302<br>
【2】From Pixels to Prose: A Large Dataset of Dense Image Captions,
https://arxiv.org/abs/2406.10328<br>
【3】ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary
Visual Prompts, https://arxiv.org/abs/2312.00784<br>
【4】关于SigLIP-SO400M-384的输入分辨率：https://huggingface.co/google/siglip-so400m-patch14-384/discussions/4</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>transformer</tag>
        <tag>SFT</tag>
        <tag>预训练</tag>
        <tag>DeepSeek</tag>
        <tag>多模态</tag>
        <tag>CV</tag>
      </tags>
  </entry>
  <entry>
    <title>Devin和Anthropic的Agent开发经验</title>
    <url>/f93b3aaf.html</url>
    <content><![CDATA[<p>看下最近两篇关于Agent开发经验的文章。</p>
<h1 id="dont-build-multi-agents">Don’t Build Multi-Agents</h1>
<p>来自Devin开发团队：<a href="https://cognition.ai/blog/dont-build-multi-agents#a-theory-of-building-long-running-agents">https://cognition.ai/blog/dont-build-multi-agents#a-theory-of-building-long-running-agents</a></p>
<p>引入了Context Engineering的概念。</p>
<p>一般的agent设计流程是这样的：planner把输入task拆分成sub-task，然后分配给不同的sub-agent「独立」执行。这种并行方案可能出现的问题是，各个sub-agent在处理不同的任务时，理解可能有偏差，处理任务的风格也不同，从而可能导致最终sub-task的结果不能合理地合在一起。</p>
<p>比如一个做小游戏的任务，做背景的sub-agent做了个超级玛丽的背景，但是设计人物的sub-agent搞了个黑悟空的形象设计，这就不统一。</p>
<p>要缓解这个问题，有人会把原始prompt也带给子agent，希望能提供更多一致信息。这种信息共享的方式有效，但还不够好，因为信息共享得太少。</p>
<p>Prompt Engineering → Context
Engineering：把原始的planner信息，包括原始输入query，planner的对话、思考和action（都是context）等，都带给各个sub-agent，以尽量提供更多信息。</p>
<p>但这仍然无法完全解决问题。更进一步的方法是，各个sub-agent不要并行处理任务，而是串行处理。上一个agent的处理结果会传给下一个agent。</p>
<p>这又会引起另一个问题，随着链路上的agent增加，context越来越多，模型逐渐处理不了了。一个优化方法是使用一个小模型来压缩context(this
is in fact something we’ve done at Cognition)。</p>
<img src="/f93b3aaf/devin.png" class title="agent经验">
<h1 id="how-we-built-our-multi-agent-research-system">How we built our
multi-agent research system</h1>
<p>来自Anthropic：<a href="https://www.anthropic.com/engineering/built-multi-agent-research-system">https://www.anthropic.com/engineering/built-multi-agent-research-system</a></p>
<p>1、MAS的优势</p>
<p>Research类的工作不确定性很大，需要case by case设计方案，甚至step by
step调整。这种unpredictability就很适合Agent来处理。</p>
<p>单个人的智慧是有上限的，而集体的智慧更好scale，就像人类社会，集合在一起分工协作，进步的速度就比每个人单干快很多。MAS
&gt; sing agent。</p>
<p>The essence of search is compression: distilling insights from a vast
corpus.
Sub-agent可以从不同的角度预先做好这种compression，然后把最重要的token给到lead
research
agent。实践上，MAS特别擅长广度优先搜索，每个agent关注不同的方向，比单agent系统的效率更高。通过给多个agent分配资源，MAS的推理能力也得到了扩展。但是这样的缺点也很明显：token消耗很快，MAS是普通对话交互的15倍。所以，MAS适合用来处理「价值足够覆盖成本」的任务。另外需要所有agent共享信息，或者有复杂依赖关系的任务并不适合MAS(这和《Don’t
Build Multi-Agents》对上了)，比如多数coding任务（上下文强烈依赖）。</p>
<p>适合MAS是三种场景：（1）子任务高度可并行的（2）信息量超出single
context window的（3）有大量复杂工具调用的。</p>
<p>MAS架构：</p>
<img src="/f93b3aaf/anthropic.png" class title="agent经验">
<p>2、Research Agents的Prompt Engineering经验</p>
<ul>
<li>Think like your agents：设身处地，观察agent在prompt下的行为<br>
</li>
<li>Teach the orchestrator how to
delegate：分配的任务要详细，目标明确，可执行<br>
</li>
<li>Scale effort to query
complexity：定一些资源分配规则，难度越大的任务，分配的资源越多，反之越少，别让agent杀鸡用牛刀<br>
</li>
<li>工具设计和选择：不用说了，重中之重<br>
</li>
<li>Let agents improve
themselves：这点有意思，把badcase给模型看，它能提出合理的方案，比如重写prompt和工具描述<br>
</li>
<li>搜索策略：学习人的方法，先广度后聚焦<br>
</li>
<li>Guide the thinking process：用深度思考模型，不用说了<br>
</li>
<li>并行起来：sub-agent并行，多工具并行</li>
</ul>
<p>一个大原则：prompt
engineering注重high-level的策略启发，而不是硬性的规则（prompt要写抽象点）。最佳prompt并非严格指令，而是定义分工架构、解题方法和资源预算的协作框架。参考：（开源Cookbook中的提示词样例）Research
MSA的prompt：<a href="https://github.com/anthropics/anthropic-cookbook/blob/main/patterns/agents/prompts/research_lead_agent.md">https://github.com/anthropics/anthropic-cookbook/blob/main/patterns/agents/prompts/research_lead_agent.md</a>。</p>
<p>Anthropic提供的样子有三个agent（整体结构看上图）：</p>
<ul>
<li>（1）citations_agent（主要用来加citation）<br>
</li>
<li>（2）research_lead_agent（相当于manager）<br>
</li>
<li>（3）research_subagent（内部也有单独的逻辑）</li>
</ul>
<p>research_lead_agent：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#### 一、**任务指导框架：结构化研究流程**</span><br><span class="line">**核心机制**：通过四阶段流程（评估分解→query分类→计划制定→执行监控）系统化处理query。</span><br><span class="line"></span><br><span class="line">1. **评估分解阶段**  </span><br><span class="line">   - 要求agent将用户query拆解为：核心概念、关键实体、所需数据点、时间/上下文限制（例：分析&quot;2025年AI金融agent最佳方案&quot;时需识别&quot;AI技术趋势&quot;、&quot;金融监管环境&quot;等实体）  </span><br><span class="line">   - 明确用户深层需求：通过反问&quot;用户最关心什么？期望的最终形式是详细报告还是对比分析？&quot;（例：当query要求&quot;比较欧盟国家税制&quot;时，需预判用户需要可视化对比图表）</span><br><span class="line"></span><br><span class="line">2. **查询类型判断**  </span><br><span class="line">   - **Depth-first**：单主题多视角（例：&quot;肥胖症成因&quot;需基因/环境/心理/社会经济多维度分析）  </span><br><span class="line">   - **Breadth-first**：多子问题并行（例：&quot;财富500强CEO信息收集&quot;需按公司分段委托子agent）  </span><br><span class="line">   - **Straightforward**：单线程解决（例：&quot;东京当前人口&quot;只需一个子agent验证权威数据源）</span><br><span class="line"></span><br><span class="line">3. **计划制定原则**  </span><br><span class="line">   - 深度优先需定义3-5种方法论（例：分析&quot;2008金融危机原因&quot;时部署经济模型/监管漏洞/行为金融学三个子agent）  </span><br><span class="line">   - 广度优先强调子任务边界清晰化（例：&quot;前端框架对比&quot;需严格划分React/Vue/Angle的评估维度）  </span><br><span class="line">   - 每个步骤需通过必要性测试：&quot;该步骤能否拆分？是否需要多视角？输出形式是否明确？&quot;</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">#### 二、**行文约束机制**</span><br><span class="line">**核心目标**：确保输出专业、高效、无冗余。</span><br><span class="line"></span><br><span class="line">1. **子agent指令规范**  </span><br><span class="line">   - 必须包含：单一研究目标、输出格式定义、可靠来源白名单（例：半导体供应链分析需指定TSMC财报/SEMI行业报告为优先来源）  </span><br><span class="line">   - 禁止任务重叠：每个子agent需有独特研究领域（例：欧盟税制比较时，北欧/西欧子agent不得重复研究同一国家）</span><br><span class="line"></span><br><span class="line">2. **最终报告控制**  </span><br><span class="line">   - 严格禁止Markdown引用：由独立agent专门处理文献引用  </span><br><span class="line">   - 强制使用`complete_task`工具提交报告，禁止子agent参与最终撰写  </span><br><span class="line">   - 信息密度要求：在Slack/Asana工具集成场景中，需明确指导子agent使用`slack_search`等内部工具（例：用户任务涉及内部文档时，需创建专属Google Drive子agent）</span><br><span class="line"></span><br><span class="line">3. **伦理约束**  </span><br><span class="line">   - 禁止部署可能产生有害内容的子agent（例：涉及种族/暴力等敏感query时立即终止研究）  </span><br><span class="line">   - 数据验证机制：对关键数值/日期进行多源交叉验证（例：CEO年龄信息需对比公司年报/LinkedIn资料）</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">#### 三、**流程控制策略**</span><br><span class="line">**核心方法**：动态资源分配与效率优化。</span><br><span class="line"></span><br><span class="line">1. **子agent数量控制**  </span><br><span class="line">   - 复杂度分级机制：  </span><br><span class="line">     - 简单查询（1个子agent）：如&quot;香蕉的营养成分&quot;  </span><br><span class="line">     - 中等复杂度（3-5个）：如&quot;AI对医疗的影响&quot;需临床/经济/技术/法规四个子agent  </span><br><span class="line">     - 上限约束：任何情况不超过20个子agent（例：财富500 CEO信息采集需分10组，每组50人）</span><br><span class="line"></span><br><span class="line">2. **并行执行策略**  </span><br><span class="line">   - 强制使用`run_blocking_subagent`工具并行启动子agent（例：同时启动3个前端框架评估子agent）  </span><br><span class="line">   - 依赖关系管理：优先执行关键路径任务（例：先获取欧盟国家列表再启动区域税制研究）</span><br><span class="line"></span><br><span class="line">3. **动态终止机制**  </span><br><span class="line">   - 边际效益判断：当新增信息价值下降时立即终止研究（例：已确认top5初创公司名单后停止后续检索）  </span><br><span class="line">   - 时间约束响应：剩余时间不足时直接进入报告撰写阶段</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">#### 四、**特殊场景处理**</span><br><span class="line">**工具集成规范**：  </span><br><span class="line">- 内部工具强制使用原则：当检测到Asana/Slack等工具可用时，必须创建专属子agent（例：用户查询涉及内部任务时，需部署Asana子agent检索特定项目ID的任务列表）  </span><br><span class="line">- 工具探索要求：对新工具至少进行两次基础操作测试（例：首次使用`slack_user_profile`需尝试查询用户基础信息）</span><br><span class="line"></span><br><span class="line">**冲突解决协议**：  </span><br><span class="line">- 信息矛盾处理流程：优先采用最新数据源，其次选择权威性更高的来源（例：政府统计数据优先于媒体报导）  </span><br><span class="line">- 贝叶斯更新机制：根据子agent反馈动态调整研究重点（例：当某医疗技术负面报告出现时，增加风险评估子agent）</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">通过该prompt的精细设计，系统实现了从复杂问题拆解到高效资源分配的全流程控制，同时通过严格的格式规范和伦理约束确保输出质量与安全性。每个机制都配有具体操作示例（如半导体供应链分析指令模板），使agent能在保持灵活性的同时遵循系统级规则。</span><br></pre></td></tr></table></figure>
<p>research_subagent：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">### 任务指导要点</span><br><span class="line"></span><br><span class="line">1. **结构化研究流程（OODA循环）**  </span><br><span class="line">   - 模型必须遵循 **Observe（观察）- Orient（定向）- Decide（决定）- Act（行动）** 循环：  </span><br><span class="line">     - **Observe**: 分析当前已收集的信息、剩余需求及可用工具（例：原prompt要求&quot;review the requirements of the task&quot;）。  </span><br><span class="line">     - **Orient**: 根据新发现调整策略（例：若某工具无效，切换其他工具或调整query）。  </span><br><span class="line">     - **Decide**: 选择最佳工具和query（例：优先使用内部工具如`google_drive_search`）。  </span><br><span class="line">     - **Act**: 执行工具调用（例：`web_fetch`获取完整网页内容）。  </span><br><span class="line"></span><br><span class="line">2. **研究预算与工具调用次数**  </span><br><span class="line">   - 根据任务复杂度动态调整工具调用次数：  </span><br><span class="line">     - 简单任务（如&quot;tax deadline&quot;）≤5次，中等任务5次，复杂任务约10次，极复杂任务≤15次（例：原prompt明确分级标准）。  </span><br><span class="line">     - 绝对上限为20次工具调用或100个来源，接近15次时必须停止并提交报告（例：`complete_task`工具的触发条件）。  </span><br><span class="line"></span><br><span class="line">3. **工具选择优先级**  </span><br><span class="line">   - **内部工具强制优先**：若任务涉及用户个人数据或内部上下文（如Gmail、Google Drive），必须优先使用（例：&quot;Internal tools strictly take priority&quot;）。  </span><br><span class="line">   - **Web Fetch核心作用**：在以下情况必须调用`web_fetch`：  </span><br><span class="line">     - 需要网站详细信息（例：原prompt要求&quot;complete contents of websites&quot;）。  </span><br><span class="line">     - 跟进`web_search`结果（例：&quot;core loop&quot;为先用搜索生成query，再用`web_fetch`获取完整内容）。  </span><br><span class="line">     - 用户提供URL时（例：直接解析URL内容）。  </span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 行为约束要点</span><br><span class="line"></span><br><span class="line">1. **搜索策略优化**  </span><br><span class="line">   - **Query设计原则**：  </span><br><span class="line">     - 简短（≤5词），适度宽泛以提高命中率（例：&quot;keep queries shorter&quot;）。  </span><br><span class="line">     - 根据结果质量调整特异性（例：若结果过多则缩小范围，过少则放宽）。  </span><br><span class="line">   - **禁止重复查询**：避免相同query重复调用工具（例：&quot;NEVER repeatedly use the exact same queries&quot;）。  </span><br><span class="line"></span><br><span class="line">2. **信息质量与来源批判**  </span><br><span class="line">   - **识别不可靠来源**：需标记以下问题：  </span><br><span class="line">     - 推测性语言（如&quot;could&quot;、&quot;may&quot;）、聚合网站、被动语态匿名来源（例：原prompt列举&quot;speculation&quot;和&quot;news aggregators&quot;）。  </span><br><span class="line">     - 营销语言、片面数据（例：&quot;marketing language for a product&quot;）。  </span><br><span class="line">   - **冲突信息处理**：按时效性、来源质量、一致性排序，无法解决时报告冲突（例：&quot;prioritize based on recency&quot;）。  </span><br><span class="line"></span><br><span class="line">3. **计算工具限制**  </span><br><span class="line">   - **避免滥用REPL工具**：仅用于无依赖的JavaScript计算（例：&quot;repl tool does not have access to a DOM&quot;）。  </span><br><span class="line">   - **简单计算自行处理**：如计数等任务无需调用工具（例：&quot;use your own reasoning to do things like count entities&quot;）。  </span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 流程控制要点</span><br><span class="line"></span><br><span class="line">1. **并行工具调用**  </span><br><span class="line">   - 允许同时调用2个独立工具以提升效率（例：&quot;invoke 2 relevant tools simultaneously&quot;），例如同时执行`web_search`和`gmail_search`。  </span><br><span class="line"></span><br><span class="line">2. **终止条件与资源保护**  </span><br><span class="line">   - **硬性终止规则**：工具调用次数≥20或来源数≥100时强制终止（例：&quot;absolute maximum upper limit&quot;）。  </span><br><span class="line">   - **软性终止判断**：当信息增量下降时主动停止（例：&quot;stop gathering sources when seeing diminishing returns&quot;）。  </span><br><span class="line"></span><br><span class="line">3. **报告格式与时效性**  </span><br><span class="line">   - **内部思考详细，报告简洁**：推理过程需详细记录，但最终报告需信息密集（例：&quot;Be detailed in your internal process, but concise in reporting&quot;）。  </span><br><span class="line">   - **即时提交结果**：任务完成后立即调用`complete_task`，避免冗余研究（例：&quot;as soon as the task is done, immediately use complete_task&quot;）。  </span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 原prompt关键机制示例</span><br><span class="line">- **内部工具强制使用**：若用户启用了Slack或Asana工具，模型必须优先使用这些工具（例：&quot;user intentionally enabled them, so you MUST use these&quot;）。  </span><br><span class="line">- **Web Fetch与Search联动**：先用`web_search`生成初步结果，再用`web_fetch`抓取高潜力URL的完整内容（例：&quot;core loop&quot;设计）。  </span><br><span class="line">- **冲突信息标记**：若发现某新闻网站预测未来事件，需在报告中注明&quot;预测&quot;而非作为事实呈现（例：&quot;note this explicitly in the final report&quot;）。</span><br></pre></td></tr></table></figure>
<p>citations_agent，这个比较短，直接放原文了：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">You are an agent for adding correct citations to a research report. You are given a report within &lt;synthesized_text&gt; tags, which was generated based on the provided sources. However, the sources are not cited in the &lt;synthesized_text&gt;. Your task is to enhance user trust by generating correct, appropriate citations for this report.</span><br><span class="line"></span><br><span class="line">Based on the provided document, add citations to the input text using the format specified earlier. Output the resulting report, unchanged except for the added citations, within &lt;exact_text_with_citation&gt; tags. </span><br><span class="line"></span><br><span class="line">**Rules:**</span><br><span class="line">- Do NOT modify the &lt;synthesized_text&gt; in any way - keep all content 100% identical, only add citations</span><br><span class="line">- Pay careful attention to whitespace: DO NOT add or remove any whitespace</span><br><span class="line">- ONLY add citations where the source documents directly support claims in the text</span><br><span class="line"></span><br><span class="line">**Citation guidelines:**</span><br><span class="line">- **Avoid citing unnecessarily**: Not every statement needs a citation. Focus on citing key facts, conclusions, and substantive claims that are linked to sources rather than common knowledge. Prioritize citing claims that readers would want to verify, that add credibility to the argument, or where a claim is clearly related to a specific source</span><br><span class="line">- **Cite meaningful semantic units**: Citations should span complete thoughts, findings, or claims that make sense as standalone assertions. Avoid citing individual words or small phrase fragments that lose meaning out of context; prefer adding citations at the end of sentences</span><br><span class="line">- **Minimize sentence fragmentation**: Avoid multiple citations within a single sentence that break up the flow of the sentence. Only add citations between phrases within a sentence when it is necessary to attribute specific claims within the sentence to specific sources</span><br><span class="line">- **No redundant citations close to each other**: Do not place multiple citations to the same source in the same sentence, because this is redundant and unnecessary. If a sentence contains multiple citable claims from the *same* source, use only a single citation at the end of the sentence after the period</span><br><span class="line"></span><br><span class="line">**Technical requirements:**</span><br><span class="line">- Citations result in a visual, interactive element being placed at the closing tag. Be mindful of where the closing tag is, and do not break up phrases and sentences unnecessarily</span><br><span class="line">- Output text with citations between &lt;exact_text_with_citation&gt; and &lt;/exact_text_with_citation&gt; tags</span><br><span class="line">- Include any of your preamble, thinking, or planning BEFORE the opening &lt;exact_text_with_citation&gt; tag, to avoid breaking the output</span><br><span class="line">- ONLY add the citation tags to the text within &lt;synthesized_text&gt; tags for your &lt;exact_text_with_citation&gt; output</span><br><span class="line">- Text without citations will be collected and compared to the original report from the &lt;synthesized_text&gt;. If the text is not identical, your result will be rejected.</span><br><span class="line"></span><br><span class="line">Now, add the citations to the research report and output the &lt;exact_text_with_citation&gt;.</span><br></pre></td></tr></table></figure>
<p>3、Agent评测经验</p>
<ul>
<li>早期的时候，少量的样本就足够了，一二十个。这样反馈快，效率高，别一上来就弄几百上千个。<br>
</li>
<li>对于有明确答案的任务，LLM-as-judge准确性很高。<br>
</li>
<li>需要人工来测的：比如发现测试机遗漏的边界情况，这种是自动化评测做得不好的。</li>
</ul>
<p>4、Production reliability and engineering challenges</p>
<p>实践中，MAS需要长时间运行，在多个步骤间流转，因此我们需要：</p>
<ul>
<li>持久化执行代码：建立错误恢复机制（不能简单重启，否则会牺牲用户体验）<br>
</li>
<li>智能错误处理：当工具失效时通知智能体自主调整（结合Claude的适应力与重试逻辑/检查点等确定性保障）<br>
</li>
<li>状态续传系统：从错误发生点恢复而非从头开始</li>
</ul>
<p>由于agent的处理路径不固定，调试的方法也和以往不同。</p>
<h1 id="小结">小结</h1>
<ul>
<li>历史记忆和信息流转的设计仍然是MAS的关键</li>
</ul>
<hr>
<p>博客：<a href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- Agent：<br>
<a href="https://www.linsight.cn/b242bfb3.html">Agent完全手册(零)：三大模块，三个理念</a><br>
<a href="https://www.linsight.cn/44c62dc5.html">DeepResearch的报告生成方法</a><br>
<a href="https://www.linsight.cn/7c2f9dcb.html">从RAG到DeepSearch</a><br>
<a href="https://www.linsight.cn/226b059f.html">agent调研(1)--MetaGPT,OpenManus和OWL</a><br>
- MoE：<br>
<a href="https://www.linsight.cn/a9c496e3.html">DeepSeek-V3细节探索</a><br>
<a href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a href="https://www.linsight.cn/37ee84bb.html">Qwen3实测&amp;技术报告</a><br>
<a href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a href="https://www.linsight.cn/6c0f6207.html">Qwen2.5-1M技术解析</a><br>
<a href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a href="https://www.linsight.cn/4519eadd.html">prompt压缩(一)</a><br>
<a href="https://www.linsight.cn/ea2871bf.html">prompt压缩(二)</a><br>
<a href="https://www.linsight.cn/bfa4f144.html">reasoning压缩(一)</a><br>
- 推理加速：<br>
<a href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a href="https://www.linsight.cn/657a6d17.html">基模型Cognitive
Behaviors对RL的影响</a><br>
<a href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a href="https://www.linsight.cn/2dee4921.html">LLM水印</a><br>
- 训练框架<br>
<a href="https://www.linsight.cn/fe0adaa5.html">LLM训练框架：从优化器和精度讲到ZeRO</a><br>
<a href="https://www.linsight.cn/4cd8532f.html">LLM训练各种并行策略</a><br>
- 项目应用：<br>
<a href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
<a href="https://www.linsight.cn/d253d7b3.html">关于The Bitter
Lesson</a><br>
- CV：<br>
<a href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
<a href="https://www.linsight.cn/b4d047c1.html">DeepSeek-VL2的细节</a><br>
- 大模型算法题：<br>
<a href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a href="http://www.linsight.cn/1736008.html">(3)</a>、 <a href="http://www.linsight.cn/1736008.html">(4)</a>、 <a href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】Don’t Build
Multi-Agents，https://cognition.ai/blog/dont-build-multi-agents#a-theory-of-building-long-running-agents<br>
【2】How we built our multi-agent research
system，https://github.com/anthropics/anthropic-cookbook/blob/main/patterns/agents/prompts/research_lead_agent.md</p>
]]></content>
      <categories>
        <category>CS</category>
        <category>NLP</category>
        <category>Agent</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LLM</tag>
        <tag>Agent</tag>
      </tags>
  </entry>
</search>
