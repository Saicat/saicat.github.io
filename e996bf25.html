<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon/favicon_io/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon/favicon_io/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon/favicon_io/favicon-16x16.png">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.28/fancybox/fancybox.css" integrity="sha256-6cQIC71/iBIYXFK+0RHAvwmjwWzkWd+r7v/BX3/vZDc=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"saicat.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="之前搞了一年多的LLM基座预训练，计算资源都花了上千万。来总结一下经验，别浪费。 有些经验现在已经成为实践共识，有些可能和主流认知有些不同，但是这些都是实打实的真实经验，是真金白银试出来的。 直接上干货。 模型初始化 随机初始化模型参数，进行大规模预训练，收敛到较好效果所需的token数太多了。 早期用Qwen1.5-0.5B的模型结构做实验，训了8T+token的通用数据，效果也才勉勉强强，不能">
<meta property="og:type" content="article">
<meta property="og:title" content="花费千万试出来的LLM预训练经验">
<meta property="og:url" content="https://saicat.github.io/e996bf25.html">
<meta property="og:site_name" content="Linsight">
<meta property="og:description" content="之前搞了一年多的LLM基座预训练，计算资源都花了上千万。来总结一下经验，别浪费。 有些经验现在已经成为实践共识，有些可能和主流认知有些不同，但是这些都是实打实的真实经验，是真金白银试出来的。 直接上干货。 模型初始化 随机初始化模型参数，进行大规模预训练，收敛到较好效果所需的token数太多了。 早期用Qwen1.5-0.5B的模型结构做实验，训了8T+token的通用数据，效果也才勉勉强强，不能">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://saicat.github.io/images/qrcode.jpg">
<meta property="og:image" content="https://saicat.github.io/images/wechat.png">
<meta property="article:published_time" content="2025-06-15T07:38:23.000Z">
<meta property="article:modified_time" content="2025-06-15T08:58:52.186Z">
<meta property="article:author" content="Lin">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="预训练">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://saicat.github.io/images/qrcode.jpg">


<link rel="canonical" href="https://saicat.github.io/e996bf25.html">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://saicat.github.io/e996bf25.html","path":"e996bf25.html","title":"花费千万试出来的LLM预训练经验"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>花费千万试出来的LLM预训练经验 | Linsight</title><meta name="robots" content="noindex">
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Linsight</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">聊聊AI技术，也聊聊其他的</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">1.</span> <span class="nav-text">模型初始化</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="nav-number">2.</span> <span class="nav-text">训练数据</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">3.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AF%84%E6%B5%8B"><span class="nav-number">4.</span> <span class="nav-text">评测</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reference"><span class="nav-number">5.</span> <span class="nav-text">Reference</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Lin"
      src="/images/avatar/Picasso_Elephant.png">
  <p class="site-author-name" itemprop="name">Lin</p>
  <div class="site-description" itemprop="description">AI | NLP</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">89</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">80</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="mailto:331603034@qq.com" title="E-Mail → mailto:331603034@qq.com" rel="noopener me" target="_blank"><i class="fa-regular fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

<!--
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/1.js?i=5acfv0hqzp5&amp;s=220&amp;m=1&amp;v=false&amp;r=false&amp;b=000000&amp;n=false&amp;c=ff0000" async="async"></script>
-->

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://saicat.github.io/e996bf25.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar/Picasso_Elephant.png">
      <meta itemprop="name" content="Lin">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Linsight">
      <meta itemprop="description" content="AI | NLP">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="花费千万试出来的LLM预训练经验 | Linsight">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          花费千万试出来的LLM预训练经验
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-06-15 15:38:23 / 修改时间：16:58:52" itemprop="dateCreated datePublished" datetime="2025-06-15T15:38:23+08:00">2025-06-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CS/" itemprop="url" rel="index"><span itemprop="name">CS</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CS/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CS/NLP/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>18 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>之前搞了一年多的LLM基座预训练，计算资源都花了上千万。来总结一下经验，别浪费。</p>
<p>有些经验现在已经成为实践共识，有些可能和主流认知有些不同，但是这些都是实打实的真实经验，是真金白银试出来的。</p>
<p>直接上干货。</p>
<h1 id="模型初始化">模型初始化</h1>
<p>随机初始化模型参数，进行大规模预训练，收敛到较好效果所需的token数太多了。</p>
<p>早期用Qwen1.5-0.5B的模型结构做实验，训了8T+token的通用数据，效果也才勉勉强强，不能说差，但是没有什么特别的惊喜，而成本实在是太高了。8T训练量相比一两T的训练量的提升不是很多，至少在没有用高质量数据退火的时候，收益比较小。</p>
<p>这时老板开始问，"能不能用上已有的模型"。一个方法是直接接着开源模型继续训，但是这样的问题是，没有办法定制我们想要的模型。</p>
<p>比如开源模型有7B的，有4B的，有1.5B的，而业务上需要一个3B参数量模型，这时就没法直接用开源模型训了。另外即使参数规模对得上，有些也有版权问题，比如Qwen2.5-4B就没有商用许可，所以直接用也有风险。</p>
<p>那么用已有的模型参数进行新模型参数的初始化是一个可行的方法。</p>
<p>1、大模型 → 小模型</p>
<p>比如14B模型可商用，那可以想办法用它来初始化一个初始效果比较好的3B模型。</p>
<p>Sheared LLaMA和Weight
Subcloning这两个初始化方法的效果都比较好。这两个方法都是通过参数裁剪，把大的模型裁剪成一个小模型。</p>
<p>Sheared
LLaMA要训练一个mask，通过mask给参数的重要性进行打分，然后保留重要性高的。mask所需的训练量并不大，几B到几十B的token就足够了。</p>
<p>一个注意点，mask的训练需要选择质量高的token，比如代码数学知识之类的，不要混太多简单的、质量一般的数据。这个数据的选择和我们训练模型的目的相关，如果你是要做一个代码模型，那么自然在选择的时候就应该使用更多code数据，以保留更多对code数据敏感的参数。</p>
<p>而Weight
Subcloning不需要训练，直接用公式计算各个neuron和层的重要度，然后联合相关维度，保留重要度高的就行。计算神经元的重要性的时候，同样需要根据输入的数据来获得激活值。输入数据的选择原则和Sheared
LLaMA一样。</p>
<p>Sheared LLaMA由于有训练，裁剪完之后刚开始训练的loss比Weight
Subcloning低（Sheared
LLaMA跑个几十步就能到2.x的样子），但是跑个一万step左右基本就都收敛到相同水平了，甚至Weight
Subcloning在后期的loss还略略低一点（但是在评测任务效果上基本是持平的）。</p>
<p>Sheared LLaMA和Weight
Subcloning都有一个限制，就是模型的高矮胖瘦可以改，比如hidden size，head
num，layer
num，但是模块的类型不能改，比如激活函数或者attention类型，大模型是什么类型，小模型就还得是什么类型。</p>
<p>实践中，把模型参数量裁剪到原模型的1/5甚至再小一点（14B →
7B、4B、3B、2B）都还是work的。</p>
<p>相比随机初始化，Sheared LLaMA和Weight
Subcloning达到相同loss和下游任务评测指标所需的token数，要少很多。原本8T才能达到的效果（以预训练loss为指标），结合高效初始化和蒸馏，基本500B就能达到。</p>
<p>一个经验，至少在0.5B~72B这个范围并没有发现从零初始化模型在效果上的优势，所以可以放心使用已有模型的参数进行初始化，而不用担心随机初始化的优化空间更大的情况出现。</p>
<p>2、小模型 → 大模型</p>
<p>上面是针对大规模预训练，从大模型初始化小模型的方法。也有从小模型初始化大模型的方法，比如Bert2BERT和Llama
Pro，在一些场景下效果也还可以。</p>
<p>虽然思路比较旧，做法比较简单，但是可迁移性还可以。</p>
<p>个人觉得，这些小模型 →
大模型的方法主要适合小容量、方向明确的训练。通过提升少量参数，获取复杂任务下的一点提升。比如现在要做某个封闭域的对话能力，而且训练数据量比较多可以用来做预训练，那么用这种方法就可以。</p>
<p>但是不要对最终效果抱太高的期望，一般是在一两个点这样的提升水平。</p>
<p>3、dense → sparse</p>
<p>MoE的训练我们也探索了。MoE模型随机初始化相比Dense更容易遭遇不稳定的loss，经常出现很大的spike，然后需要很长的时间来恢复（&gt;50k
step）。</p>
<p>因为router的存在，MoE模型整体对精度更加敏感。尝试过简单地把混合精度训练改成全单精度训练，整个过程就会稳定很多，但是这样效率也很低，GPU的利用率连50%都不到，不可scale。</p>
<p>一个经典的初始化方法是Sparse Upcycling。Sparse
Upcycling方法很稳，至少是一个效果效率都不错的基线方法。</p>
<p>Sparse
Upcycling一般是用同层数、同attention设置的dense模型，通过FFN复制的方式来获得MoE模型。</p>
<p>几个注意点：</p>
<ul>
<li>（1）现在都是使用细粒度专家，一个FFN会被裁剪成多份专家，裁剪之前需要做一些neuron粒度的顺序打乱来帮助打破专家对称性；<br>
</li>
<li>（2）另外会需要对部分FFN参数进行随机初始化，我们实验了50%<sub>99.5%的原参数保留比例，也就是有0.5%到50%的随机参数进行重新随机初始化，结果是50%</sub>70%的收敛效果和收敛速度是比较好的。</li>
</ul>
<p>关于各种初始化的详细内容，可看<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/_3-hX3e1TqGEOo6O41JJ_A?token=1318369845&amp;lang=zh_CN">LLM高效预训练(一)</a>，<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/S_Nx5I4C2gxwumG4GMxkDg?token=1318369845&amp;lang=zh_CN">LLM高效预训练(二)</a>。</p>
<h1 id="训练数据">训练数据</h1>
<p>包含多个阶段的训练数据。</p>
<p>1、phase1：通用预训练数据</p>
<p>这一阶段数据大部分是真实数据，一些特定领域的比如代码、数学会有合成数据。合成数据的量相对少，而且目的更明确，就是针对下游某些能力的。先看真实数据，合成数据的处理后面讲。</p>
<p>（1）基础清洗</p>
<p>这部分比较老生常谈了。个人经验是，现在能够收集到的数据量已经非常足够，所以基础清洗可以狠一点，不怕错杀：</p>
<ul>
<li>ppl：太高和太低的都不要<br>
</li>
<li>格式：太多分行的，太多短词的（比如疑似网页导航栏），通通不要<br>
</li>
<li>长度：太短的不要<br>
</li>
<li>其他规则：url/数据类型/字符比例/安全相关字词/日志识别/带太多重复内容的（引起LLM复读机）<br>
</li>
<li>...等等等等</li>
</ul>
<p>再加上人工打标数据训练出来的二分类模型（轻量级的比如fasttext），把低质量的筛掉。</p>
<p>更多细节可参考<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/bgg0HKaKokRVhXixrlQRhg?token=1318369845&amp;lang=zh_CN">Llama3.1--预训练要点一览</a>和<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/5MfmrrcYeK4d6gfaDL8GWA?token=1318369845&amp;lang=zh_CN">Qwen2技术报告</a>细节。</p>
<p>（2）去重</p>
<p>去重可以说是提升通用数据质量最为重要的一步，用minhash做文章/段落粒度的去重。minhash的效率还是比较可以的，如果有集群可用。可以先把minhash都算出来，再进行去重处理。</p>
<p>去重的原则是宁杀错不放过，因为重复数据对模型的能力影响很大。</p>
<p>（3）分类</p>
<p>代码数据、数学数据、高教育性数据(比如chinese-fineweb-edu这种)的挖掘，还有通用数据的标签打标，比如体育、音乐、时政等等。好消息是这些基本有可用的体系和模型，可以在已有资源的基础上稍作修改就行。</p>
<p>经验上是提高education
score、代码、数学、知识类数据的比例对整体效果有一些帮助，用更少的训练量就可以达到更低的loss。</p>
<p>个人经验，配比可以不用做得那么细，比如在第一阶段中代码数学数据到底占30%好还是40%好，不是最重要的，反正比自然比例高就可以了。在预训练的早期，还是需要通用数据来提升模型基础语言能力的，没有这个为基础，直接学难度大的效果并不好。</p>
<p>2、长文本预训练数据</p>
<p>长上下文的训练所需的数据比较少，几B到几十B就完全足够了。</p>
<p>把多个不怎么相关的文档拼接在一起，可以用，但是效果不好。因为这样相当于只是训练了位置编码，和Positional
Skip-wise的效果差不多。</p>
<p>略好一点的方法是拼接相关文档，比如论文，把有reference关系的文章拼接在一起。</p>
<p>更好的长文本数据是天然的长文本、有长上下文依赖的文本，比如书籍，特别是大学课本这种，逻辑清晰的。另外，github的项目也是长文本。</p>
<p>长数据在领域分布上有bias，因此筛选的时候记得统计一些这些数据的领域分布情况，需要的时候调整一下，避免分布过于集中。</p>
<p>有一些文章提出构造阅读理解之类的长文本，比如拼接10篇文章，然后再最后提几道题，每道题需要阅读不同的文章来回答。实践上，预训练加这种数据没什么太大的提升。</p>
<p>长文本的相关内容可看<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/VpX8ODoSTzw4FJmzB9kTug">Qwen2.5-1M技术解析</a>，<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/Ci9tAMIER0Aj96sK81HNcw?token=1318369845&amp;lang=zh_CN">LLM长上下文的问题</a>，<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/FTewPxSr5fcwkxAgRZm7Wg?token=1318369845&amp;lang=zh_CN">解锁大模型长上下文能力</a>，<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/NaTtwURRw7lsG55QTIaVsA?token=1318369845&amp;lang=zh_CN">大模型推理窗口-从有限到无限大</a>。</p>
<p>3、退火阶段数据</p>
<p>很多模型都证明了，LR退火阶段对提升模型最终效果很重要。而这个阶段提升数据质量的收益很大。模型在使用时所需的能力主要是语言能力、知识储备和推理能力。</p>
<p>推理能力主要来自数学和代码数据，知识储备主要来自educational
data（比如学科试题、教科书），语言能力则是高质量网页数据。</p>
<p>高质量网页数据主要是从真实数据里用比较严格的阈值筛选的，教科书也都是真实数据，除此之外，学科试题、数学、代码都混了大量的合成数据。</p>
<p>总结下学科试题、数学和代码数据的合成，合成数据都是使用LLM生成的。</p>
<p>（1）多样性</p>
<p>合成数据最重要的就是多样性。没错，先不谈格式类型质量，最重要的就是多样性。因为用LLM合成数据，结果都和模型预训练学到分布很相关。</p>
<p>一个提升多样性的方法是利用解码的随机性，提高温度，提高top k/top
p等。这是一个方法，但是为了保持合成数据质量，随机性也不能提得太高，特别是针对数学和代码这种准确性要求比较高的场景。基本上通过解码生成多条之后再进行质量筛选，也就能留下几条，而且这几条相似度还是高的。</p>
<p>另一个方法就是通过prompt来提升多样性。同样的prompt输出的结果相似，那就用很多种prompt。腾讯的Persona
Hub就很好用。Persona
Hub给了10亿个不同的人物描述，我们可以把这些不同的描述加到prompt里，比如生成数学题：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">prompt = &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">根据以下任务的描述，想象一个其日常生活中的场景，出一道和这个场景相关的数学应用题。</span><br><span class="line"></span><br><span class="line">人物描述：&#123;persona&#125;</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">person = [</span><br><span class="line">  &quot;一个23岁的卡车司机，身高178cm，短头发，单身，爱吃海鲜...&quot;,</span><br><span class="line">  &quot;小A，一个化学家，北京人，本科就读于...&quot;,</span><br><span class="line">  ...</span><br><span class="line">]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>由于人物的描述各不相同，最终输出的结果多样性就比较高。</p>
<p>在这个基础上，还可以加入其他维度的参数，提升多样性。比如：</p>
<ul>
<li>难度等级：题目的难度，低、中、高，或者小学，初中，高中，大学，考研等，再细一点也可以<br>
</li>
<li>话题：可以从persona提取top
k个人物关键词，比如"职业"、"学习"、"日常"、"水果"等，然后随机选择一个，要求模型出题必须和这个或者几个关键词相关<br>
</li>
<li>few-shot
example：我们已经有一些高质量题目数据集了，可以从里面随机挑几条，排列组合，和persona一起，一方面提升多样性，一方面也能提升合成数据的质量<br>
</li>
<li>etc.</li>
</ul>
<p>总结来说，就是生成的prompt里需要有一个随机数，这个随机数是以自然语言的形式存在的，这个随机数约多样，生成多样性越好。</p>
<p>（2）质量</p>
<p>对于数学、代码和知识考题类的数据：</p>
<ul>
<li>模型越大，效果越好<br>
</li>
<li>专门模型比通用模型效果好</li>
</ul>
<p>质量评判的时候可以直接LLM-as-judge来给数据质量打分。也可以用已有的开源模型打分，比如huggingface上就有给代码/数学质量打分的模型。</p>
<p>代码还可以通过执行反馈来确定代码是否正确，OpenAI有提供工具。</p>
<p>在这个基础上，一些数据进化的方法也可以用来提升数据的质量和难度，不过这个最好根据数据类型来定制进化的流程。</p>
<p>更详细内容看<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/R1DbxO9lF3xSp2j-Q5-IKQ?token=1318369845&amp;lang=zh_CN">训练数据合成(一)</a>，<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/B6CclTLHvjI03itaSySj9Q?token=1318369845&amp;lang=zh_CN">训练数据合成(二)</a>，<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/LRy0zC9fen8m7XpK1TsB7A?token=1318369845&amp;lang=zh_CN">训练数据合成(三)</a>。</p>
<h1 id="训练">训练</h1>
<p>1、蒸馏</p>
<p>就是student模型学习teacher模型的logits。</p>
<p>随机初始化的模型所需的token数多，蒸馏起来成本高。但是用大模型初始化的模型，所需的训练token小很多(&lt;=10%)，因此蒸馏是个好选择。</p>
<p>一个好的选择是蒸馏的teacher模型和用于初始化小模型的大模型是同一个，或者用同系列的更大模型，这样出来的效果相比直接训练有提升。</p>
<p>蒸馏温度有个先高后低的设置，这样训练的前期学习的范围更广，而后期专注收敛到更好的水平。</p>
<p>2、Maximal Update Parameterization (µP)</p>
<p>MuP是个模型参数化的方法，通俗地说就是设计模型的超参。用这个方法初始化的模型可以保证在不同规模下，训练的LR和BS可以通用。</p>
<p>这个方法的本意是利用小模型调参，然后直接迁移到巨大的模型，从而减少调参成本。不过MuP的参数化方法和通常的模型有些不同，这就可能导致模型最终收敛的效果不一定和通常的模型一致。</p>
<p>不过实验下来确实MuP的训练超参迁移能力不错，大小模型之间的一致性比较强。</p>
<p>而如果要训练的模型比较小，直接网格搜就行了（比如用10B数据），然后看哪个组合loss下降地快，简单粗暴有效。</p>
<p>3、LR：WSD和Cosine</p>
<p>一般训练的LR是warmup+cosine。</p>
<p>WSD的好处是方便做实验。因为cosine要预先设置训练步数，而WSD可以在中间任何时候进入下一个阶段。</p>
<p>效果上，同样的训练token数下，WSD是不如cosine的，至少在S阶段时这样。要把WSD的效果提上来，D的阶段要足够长，最终的LR要足够低，期间的数据质量要提高。</p>
<p>4、多阶段退火</p>
<p>这个是比较重要的。</p>
<p>LR退火阶段是提升模型效果指标的关键。这个阶段要提升数学、代码、学科、和高质量通用数据的比例。</p>
<p>但是实践中，发现退火阶段直接把多种高质量数据混在一起，无论怎么调比例，都会出现跷跷板效应：代码数学加多了，可能通用能力就变差了，或者学科数据加多了，数学就受影响了。</p>
<p>因此退火阶段不直接混在一起训，而是分成多个小阶段，每个小阶段主要训一种数据，这样还可以调LR和BS。</p>
<p>训练单一一种数据的时候，还需要调配内部的数据比例。比如代码数据，合成的要多少，python要多少，其他语言要多少，题目要多少，github项目要多少，这些都需要搜索配比。</p>
<p>确定了配比之后，先分别用单一一种数据，测出每种数据的最佳LR，结果是：</p>
<p>学科 &gt; 数学 &gt; code &gt; 通用 (&gt; Tool Use)</p>
<p>因此小阶段的训练也按这个顺序来排，先训学科，后训通用。</p>
<p>（1）step 1：学科</p>
<p>这一阶段主要提升学科能力，对标的评测数据主要是MMLU、Ceval和CMMLU这种。</p>
<p>这一阶段使用较大的LR（3e-4），数据主要包含教材（30%），合成的学科选择题（50%），小部分的通用选择题（10%），还有小部分的通用数据（10%），总共120B+token的数据。（虽然总的数据有这么多，但是最佳checkpoint在几十B的时候就达到了，因此实际训练的token数就几十B。）</p>
<p>通用数据主要是维持一下模型的语言能力，不要因为学习学科知识和做选择题给搞得不会说话了。</p>
<p>以Qwen2.5为基座，训完这一阶段之后，MMLU、Ceval的效果其实就已经明显超过Qwen官方模型了。</p>
<p>（2）step 2：加上数学</p>
<p>在第一阶段的基础上，加入60%的数学数据。这里做过实验，发现再进一步提升数学数据的比例，效果也不会有进一步提高了，甚至有点下降，并且由于学科数据更少，导致学科能力也下降更多。因此从全局分数的出发点考虑，只加入了60%的数学数据。整体的数据比例是
学科：数学 = 40%：60%。</p>
<p>这一阶段训练中，学科能力得分会比第一阶段低，但是会比传统那种一开始就直接混合多种数据的做法要高。这一阶段训练完之后Ceval还能比Qwen2.5基线高6分左右。</p>
<p>这一阶段训个几十B，数学能力也达到最佳，而学科能力略下降之后也稳定了。</p>
<p>加入的数学数据里依然有大量的合成数学题，主要覆盖中学和大学的各种数学考题。学习率上，发现小一点更好，LR=8e-5就比1e-4好。</p>
<p>（3）step 3：加上代码</p>
<p>一个在之前实验已经验证了的规律：数学和代码能力有一定的相关性，也就是数学能力的提升有助于提升代码能力，反过来也一样。因此这一阶段使用的代码数据比例没有特别高，差不多是40%。整体的数据比例是
学科：数学：代码 = 20%：40%：40%。</p>
<p>学科减少的比例更大一些，因为学科数据在退火阶段训练的时间最长（各个step都有），而且早期的LR更大，所以整体来看，即使在后面的阶段减少比例，学科能力也不会下降太多。</p>
<p>这一阶段LR = 5e-5，训练量也是几十B token。</p>
<p>（4）step 4：综合提升</p>
<p>这是退火的最后阶段，也是预训练的最后阶段，这一阶段会加上一些通用的数据，用来保持语言能力，也会加上一些SFT阶段的数据，让模型的评测效果进一步提升。比如我们想要增强模型的Tool
Use能力，所以加入的SFT数据就有不少function call的数据。</p>
<p>大致的数据比例是 学科：数学：代码：通用：SFT =
15%：25%：25%：10%：30%。</p>
<p>LR = 3e-5，一直decay到0了，总共也是训练了几十B数据。</p>
<p>注意这么多阶段中，没有一条数据是重复使用的，所以准备退火数据的时候要预留多一点。</p>
<p>5、batch内数据分配</p>
<p>训练中，每个batch都是按我们想要的数据比例混合。</p>
<p>在构造batch的时候，会一直跟踪各类数据的token数据，保证比例是按预定比例来的。</p>
<h1 id="评测">评测</h1>
<p>评测的数据主要是两类。一类是opencompass的数据集，主要测试模型的代码、数学、语言、知识能力。还有一类是更具体的下游任务数据，比如Tool
Use任务的评测集，或者业务相关的数据集。</p>
<p>评测的时候，既要看opencompass的通用能力指标，也要SFT之后看下游任务的指标。要注意的是，这二者的关系并不总是正相关的。</p>
<p>另外预训练loss和任务的评测指标也不总是正相关，大致上来说是“loss低不一定效果好，但是loss高效果一定不好”。</p>
<hr>
<p>博客：<a target="_blank" rel="noopener" href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a target="_blank" rel="noopener" href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- Agent：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/b242bfb3.html">Agent完全手册(零)：三大模块，三个理念</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/44c62dc5.html">DeepResearch的报告生成方法</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/7c2f9dcb.html">从RAG到DeepSearch</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/226b059f.html">agent调研(1)--MetaGPT,OpenManus和OWL</a><br>
- MoE：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/a9c496e3.html">DeepSeek-V3细节探索</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/37ee84bb.html">Qwen3实测&amp;技术报告</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/6c0f6207.html">Qwen2.5-1M技术解析</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/4519eadd.html">prompt压缩(一)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/ea2871bf.html">prompt压缩(二)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/bfa4f144.html">reasoning压缩(一)</a><br>
- 推理加速：<br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/657a6d17.html">基模型Cognitive
Behaviors对RL的影响</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/2dee4921.html">LLM水印</a><br>
- 训练框架<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/fe0adaa5.html">LLM训练框架：从优化器和精度讲到ZeRO</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/4cd8532f.html">LLM训练各种并行策略</a><br>
- 项目应用：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/d253d7b3.html">关于The Bitter
Lesson</a><br>
- CV：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/b4d047c1.html">DeepSeek-VL2的细节</a><br>
- 大模型算法题：<br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a target="_blank" rel="noopener" href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a target="_blank" rel="noopener" href="http://www.linsight.cn/1736008.html">(3)</a>、 <a target="_blank" rel="noopener" href="http://www.linsight.cn/1736008.html">(4)</a>、 <a target="_blank" rel="noopener" href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a target="_blank" rel="noopener" href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a target="_blank" rel="noopener" href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a target="_blank" rel="noopener" href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a target="_blank" rel="noopener" href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】长文详解--LLM高效预训练(一)，https://www.linsight.cn/dcb57672.html#sheared-llama<br>
【2】LLM高效预训练(二)，https://mp.weixin.qq.com/s/S_Nx5I4C2gxwumG4GMxkDg?token=1318369845&amp;lang=zh_CN<br>
【3】LLM高效预训练(一)https://mp.weixin.qq.com/s/_3-hX3e1TqGEOo6O41JJ_A?token=1318369845&amp;lang=zh_CN<br>
【4】LLM高效预训练(二)https://mp.weixin.qq.com/s/S_Nx5I4C2gxwumG4GMxkDg?token=1318369845&amp;lang=zh_CN<br>
【5】训练数据合成(一)，https://mp.weixin.qq.com/s/R1DbxO9lF3xSp2j-Q5-IKQ?token=1318369845&amp;lang=zh_CN<br>
【6】训练数据合成(二)，https://mp.weixin.qq.com/s/B6CclTLHvjI03itaSySj9Q?token=1318369845&amp;lang=zh_CN<br>
【7】训练数据合成(三)，https://mp.weixin.qq.com/s/LRy0zC9fen8m7XpK1TsB7A?token=1318369845&amp;lang=zh_CN<br>
【8】MiniCPM，https://mp.weixin.qq.com/s/qxr62jbqjf4wr1YJ-P6haQ?token=1318369845&amp;lang=zh_CN<br>
【9】Qwen2.5-1M技术解析，https://mp.weixin.qq.com/s/VpX8ODoSTzw4FJmzB9kTug<br>
【10】LLM长上下文的问题，https://mp.weixin.qq.com/s/Ci9tAMIER0Aj96sK81HNcw?token=1318369845&amp;lang=zh_CN<br>
【11】解锁大模型长上下文能力，https://mp.weixin.qq.com/s/FTewPxSr5fcwkxAgRZm7Wg?token=1318369845&amp;lang=zh_CN<br>
【12】大模型推理窗口-从有限到无限大，https://mp.weixin.qq.com/s/NaTtwURRw7lsG55QTIaVsA?token=1318369845&amp;lang=zh_CN</p>
<!-- flag of hidden posts -->
    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Lin
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://saicat.github.io/e996bf25.html" title="花费千万试出来的LLM预训练经验">https://saicat.github.io/e996bf25.html</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i> NLP</a>
              <a href="/tags/LLM/" rel="tag"><i class="fa fa-tag"></i> LLM</a>
              <a href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83/" rel="tag"><i class="fa fa-tag"></i> 预训练</a>
          </div>

        

    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Lin</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">749k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">22:43</span>
  </span>
</div>
<div class="busuanzi-count">
</div>

<!--
-->


<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.28/fancybox/fancybox.umd.js" integrity="sha256-ytMJGN3toR+a84u7g7NuHm91VIR06Q41kMWDr2pq7Zo=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/fancybox.js"></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"Saicat/comment-utterance","issue_term":"pathname","theme":"github-light"}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
