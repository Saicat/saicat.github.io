<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon/favicon_io/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon/favicon_io/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon/favicon_io/favicon-16x16.png">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.28/fancybox/fancybox.css" integrity="sha256-6cQIC71/iBIYXFK+0RHAvwmjwWzkWd+r7v/BX3/vZDc=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"saicat.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="【本文已在同名 微信公众号 &#x2F; 知乎 &#x2F; 个人博客linsight.cn 上线】  大语言模型虽然效果很好，但是推理时，朴素的自回归解码策略需要逐个串行解码，耗时较长，这给用户的耐心带来了很大挑战。如今各家大模型提供商基本都有对外提供大模型的体验平台，而模型的推理效率自然也成了一个重要的竞争点。">
<meta property="og:type" content="article">
<meta property="og:title" content="大模型推理加速-投机解码">
<meta property="og:url" content="https://saicat.github.io/f5c015c.html">
<meta property="og:site_name" content="Linsight">
<meta property="og:description" content="【本文已在同名 微信公众号 &#x2F; 知乎 &#x2F; 个人博客linsight.cn 上线】  大语言模型虽然效果很好，但是推理时，朴素的自回归解码策略需要逐个串行解码，耗时较长，这给用户的耐心带来了很大挑战。如今各家大模型提供商基本都有对外提供大模型的体验平台，而模型的推理效率自然也成了一个重要的竞争点。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://saicat.github.io/f5c015c/fi_example.png">
<meta property="og:image" content="https://saicat.github.io/f5c015c/fi_sd_algo.png">
<meta property="og:image" content="https://saicat.github.io/f5c015c/acce_alog.png">
<meta property="og:image" content="https://saicat.github.io/f5c015c/formula.png">
<meta property="og:image" content="https://saicat.github.io/f5c015c/fi_expected_token_num.png">
<meta property="og:image" content="https://saicat.github.io/f5c015c/fi_alpha.png">
<meta property="og:image" content="https://saicat.github.io/f5c015c/fi_choose_gamma.png">
<meta property="og:image" content="https://saicat.github.io/f5c015c/fi_speed_and_op_table.png">
<meta property="og:image" content="https://saicat.github.io/f5c015c/fi_speed_and_op.png">
<meta property="og:image" content="https://saicat.github.io/f5c015c/fi_walltime.png">
<meta property="og:image" content="https://saicat.github.io/f5c015c/fi_t5_result.png">
<meta property="og:image" content="https://saicat.github.io/f5c015c/fi_alpha.png">
<meta property="og:image" content="https://saicat.github.io/images/qrcode.jpg">
<meta property="article:published_time" content="2024-05-13T08:47:13.000Z">
<meta property="article:modified_time" content="2024-05-25T03:38:10.437Z">
<meta property="article:author" content="Lin">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="transformer">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="推理加速">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://saicat.github.io/f5c015c/fi_example.png">


<link rel="canonical" href="https://saicat.github.io/f5c015c.html">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://saicat.github.io/f5c015c.html","path":"f5c015c.html","title":"大模型推理加速-投机解码"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>大模型推理加速-投机解码 | Linsight</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Linsight</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">聊聊技术，也聊聊其他的</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%83%8C%E6%99%AF"><span class="nav-number">1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#speculative-decoding%E7%AE%97%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">speculative decoding算法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#speculative-sampling%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%80%A7"><span class="nav-number">3.</span> <span class="nav-text">speculative sampling的正确性</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#approximation-model%E7%9A%84%E8%AF%84%E4%BC%B0"><span class="nav-number">4.</span> <span class="nav-text">approximation model的评估</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%80%97%E6%97%B6%E4%BC%98%E5%8C%96%E7%9A%84%E5%88%86%E6%9E%90"><span class="nav-number">5.</span> <span class="nav-text">耗时优化的分析</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%88%90%E6%9C%AC%E7%9A%84%E5%88%86%E6%9E%90"><span class="nav-number">6.</span> <span class="nav-text">计算成本的分析</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#gamma-%E7%9A%84%E9%80%89%E6%8B%A9"><span class="nav-number">7.</span> <span class="nav-text">\(\gamma\)
的选择</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#approximation-model%E7%9A%84%E9%80%89%E6%8B%A9"><span class="nav-number">8.</span> <span class="nav-text">approximation model的选择</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">9.</span> <span class="nav-text">实验</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93"><span class="nav-number">10.</span> <span class="nav-text">小结</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reference"><span class="nav-number">11.</span> <span class="nav-text">Reference</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Lin"
      src="/images/avatar/Picasso_Elephant.png">
  <p class="site-author-name" itemprop="name">Lin</p>
  <div class="site-description" itemprop="description">AI | NLP</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">68</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">67</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="mailto:331603034@qq.com" title="E-Mail → mailto:331603034@qq.com" rel="noopener me" target="_blank"><i class="fa-regular fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

<!--
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/1.js?i=5acfv0hqzp5&amp;s=220&amp;m=1&amp;v=false&amp;r=false&amp;b=000000&amp;n=false&amp;c=ff0000" async="async"></script>
-->

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://saicat.github.io/f5c015c.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar/Picasso_Elephant.png">
      <meta itemprop="name" content="Lin">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Linsight">
      <meta itemprop="description" content="AI | NLP">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="大模型推理加速-投机解码 | Linsight">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          大模型推理加速-投机解码
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-05-13 16:47:13" itemprop="dateCreated datePublished" datetime="2024-05-13T16:47:13+08:00">2024-05-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-05-25 11:38:10" itemprop="dateModified" datetime="2024-05-25T11:38:10+08:00">2024-05-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CS/" itemprop="url" rel="index"><span itemprop="name">CS</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CS/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CS/NLP/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>10k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>18 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>【本文已在同名 微信公众号 / 知乎 / <a target="_blank" rel="noopener" href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>大语言模型虽然效果很好，但是推理时，朴素的自回归解码策略需要逐个串行解码，耗时较长，这给用户的耐心带来了很大挑战。如今各家大模型提供商基本都有对外提供大模型的体验平台，而模型的推理效率自然也成了一个重要的竞争点。</p>
<p>speculative
decoding，译作投机解码，就是推理加速的一个比较巧妙的方案。本篇将介绍投机解码的基础思路。</p>
<h1 id="背景">背景</h1>
<p>2022年11月，Google在《Fast Inference from Transformers via
Speculative
Decoding》里提出投机解码的策略；DeepMind稍晚一点，在2023年初的《Accelerating
Large Language Model Decoding with Speculative
Sampling》也提出了一样的解码策略。（以这两家的关系，很可能私底下就沟通过这个idea了）Google的论文相比DeepMind的，做了更多的实验和分析，更为详尽一些。</p>
<p>在speculative
decoding之前，研究人员已经在模型推理加速这个方向做了不少工作：<br>
- 模型蒸馏：以Hinton的《Distilling the Knowledge in a Neural
Network》为代表，以及后面衍生出的各种蒸馏方法（参考《Knowledge
Distillation: A
Survey》），可以把规模更大的、性能更强的模型的能力，部分迁移到规模较小的模型上，在效果上相比直接训练小模型有一定的提升。transformer上蒸馏相关的经典工作有《TinyBERT:
Distilling BERT for Natural Language Understanding》和《DistilBERT, a
distilled version of BERT: smaller, faster, cheaper and
lighter》等。<br>
- 模型量化：如《Quantized Neural Networks: Training Neural Networks with
Low Precision Weights and Activations》、《LLM.int8(): 8-bit Matrix
Multiplication for Transformers at Scale》、《Zeroquant: Efficient and
affordable post-training quantization for large-scale
transformers》等，把模型参数量化到int8、int4以及更低的精度，在减少空间需求的同时，最大化地保持模型的推理效果。<br>
- 高效模型结构设计：如使用稀疏层的《Sparse is Enough in Scaling
Transformers》，减少KV缓存需求的MQA《Fast Transformer Decoding: One
Write-Head is All You Need》、GQA《《GQA: Training Generalized
Multi-Query Transformer Models from Multi-Head
Checkpoints》》以及最近DeepSeek-V2中的MLA等，还有通过进化算法进行高效架构搜索的工作《Primer:
Searching for Efficient Transformers for Language Modeling》。</p>
<p>以上这些做法对不同的输入一视同仁，采用一个全局来看有收益的方案来统一处理，达到推理加速的目的。</p>
<p>相对地，也有一些其他的方案，认为不是每一步推理都适合一样处理：某些推理step需要大模型，而另一些step只需要高效的小模型，从而根据输入，动态地决定模型参与计算的参数，相关工作有：<br>
- 《Dynamic Neural Networks: A Survey》<br>
- 《Adaptive Attention Span in Transformers》<br>
- 《Consistent Accelerated Inference via Confident Adaptive
Transformers》<br>
- 《Why should we add early exits to neural networks?》<br>
- 《Controlling Computation versus Quality for Neural Sequence
Models》<br>
- 《The Right Tool for the Job: Matching Model and Instance
Complexities》<br>
- 《Depth-Adaptive Transformer》<br>
- 等</p>
<p>MoE也属于动态激活的方案之一。</p>
<p>而《Training compute-optimal large language models》的scaling
law则指出模型规模没有原先预想的影响那么大，可以通过增加训练数据等方法让小模型逼近大模型的效果。</p>
<p>以上这些方案虽然可以在一定程度上提升推理效率，但是要么需要重新训练模型，要么对模型的效果有损害。</p>
<p>也有一些方案在解码的方法上进行优化，比如《Blockwise Parallel Decoding
for Deep Autoregressive Models》和《Lossless Acceleration for Seq2seq
Generation with Aggressive Decoding》。</p>
<p>speculative
decoding也是一个在解码策略上进行优化的方法。投机解码可以在不用训练原模型的基础上，提升2x-3x的推理速度，并且保证结果和原模型完全一致，没有任何效果损失。</p>
<h1 id="speculative-decoding算法">speculative decoding算法</h1>
<p>回想一下，自回归语言模型在训练的时候，在每一个位置，会根据当前及前面所有的token，预测下一个token。由于强制学习的特性，所有token可以一起训练。在某种特别的情况下，模型对当前的输入拟合得特别好，就有可能出现每个token的预测，都完美命中下一个输入token的情况。举个例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">位置：一  二  三  四</span><br><span class="line">输入：我  爱  中  国</span><br><span class="line">输出：爱  中  国  EOS</span><br></pre></td></tr></table></figure>
<p>而在推理的时候，这种依赖前面所有token的特性，使得自回归模型只能一个一个串行地解码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">step1：输入“我”，输出“爱”；</span><br><span class="line">step2：输入“我爱”，输出“中”；</span><br><span class="line">step3：输入“我爱中”，输出“国”；</span><br><span class="line">step4：输入“我爱中国”，输出“EOS”；</span><br></pre></td></tr></table></figure>
<p>现在，假设我们有一个神奇海螺，你只要输入“我”，就会输出“爱 中 国
EOS”四个token作为草稿，我们就可以拿着这四个draft
token一起放到原来的模型，跑一下各个位置的输出，进行验证，跟训练时的前向推理一样：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">位置：一  二  三  四</span><br><span class="line">输入：我  爱  中  国</span><br><span class="line">输出：爱  中  国  EOS</span><br></pre></td></tr></table></figure>
<p>然后就会发现模型的输出和神奇海螺给出的草稿完全一致，那就相当于我们只进行了一次模型推理，就解码了四个token，并且和原模型的效果完全一致。并且一般情况下，模型对一个位置进行预测和对四个位置进行预测的耗时基本没有太大的差异，也就是说在这个例子下，模型解码速度提升到了将近4倍。</p>
<p>当然，神奇海螺不会总是能够给出和模型一模一样的结果，除非它就是模型本身。因此，在上面这个例子中，输入“我”之后，神奇海螺有可能给出的是“爱
中 华 EOS”这四个draft
token。这种情况下，我们把这些token一起输入到模型进行验证</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">位置：一  二  三  四</span><br><span class="line">输入：我  爱  中  华</span><br><span class="line">输出：爱  中  国  EOS</span><br></pre></td></tr></table></figure>
<p>会发现神奇海螺给出的“爱”和“中”命中了模型的结果，但是“华”没对上。不过这种情况下，跑一次模型推理也能解码出两个token，推理效率依然有提升。</p>
<p>部分情况下，神奇海螺给出的结果也可能完全跑偏，比如给它输入“我”，它有可能输出“叫
小
明”，这就和原模型一个都没对上。但是只要统计上，神奇海螺给出的草稿平均命中token数
&gt; 0，我们就有机会获得推理加速。</p>
<p>使用神奇海螺的这个思路其实就是speculative
decoding的主要思路，而你肯定也已经猜到了，神奇海螺其实就是一个规模比较小的模型，论文中把它称为approximation
model或者draft model，而我们想要加速的原模型则叫target model。</p>
<p>论文给出的一个例子如下</p>
<img src="/f5c015c/fi_example.png" class title="例子">
<p>绿色的就是approximation model给出并命中target
model验证结果的token，红色的是错误的token，蓝色则是修正后的token。</p>
<p>在这个例子中，target模型只推理了9次，就解码出了38个token，推理速度获得了较大提升。</p>
<p>看完了例子，现在对投机解码算法给出正式的描述。</p>
<p><span class="math inline">\(M_p\)</span> 是target model， <span class="math inline">\(M_q\)</span> 是approximation
model，prefix是当前的输入。</p>
<p>首先 <span class="math inline">\(M_q\)</span> 给出 <span class="math inline">\(\gamma\)</span> 个draft token，然后 <span class="math inline">\(M_p\)</span> 并行地对这 <span class="math inline">\(\gamma\)</span> 个draft
token进行验证，根据验证结果，按顺序把通过验证的token加入到当前序列中；如果出现被
<span class="math inline">\(M_p\)</span>
拒绝的token，这些token则按规则重新抽样。</p>
<p>Google论文给出的投机解码算法描述如下图。</p>
<img src="/f5c015c/fi_sd_algo.png" class title="投机解码算法">
<p>（DeepMind版本的算法描述在下面）</p>
<p>这里注意，投机解码单次运行能解码的token数量，除了这 <span class="math inline">\(n\)</span> 个被接受的draft token，还有 <span class="math inline">\(M_p\)</span>
对这些草稿进行验证时顺便推理出来的一个额外token，因此最终可以得到 <span class="math inline">\(n+1\)</span> 个token。因此如果approximation
model每次给出 <span class="math inline">\(\gamma\)</span> 个draft
token，理论上最多可以获得 <span class="math inline">\(\gamma+1\)</span>
新解码token，而最少也能有1个（来自target模型）。</p>
<p>投机解码的原理大致就是这样，思路还是很巧妙的，但是要实际应用还有几个问题需要解决，比如：<br>
- 关于投机采样speculative sampling：target model怎么对approximation
model给出的token进行验证？在一个draft
token被拒绝之后，怎么重新采样？<br>
- 怎么选择 <span class="math inline">\(\gamma\)</span> 才合理？<br>
- 怎么选择approximation model，用什么指标表征approximation
model的质量？</p>
<p>另外，DeepMind论文的给出投机解码算法如下，可以对照Google的算法，方便理解。（DeepMind所用的符号有所不同，本篇采用Google论文的符号描述。）</p>
<img src="/f5c015c/acce_alog.png" class title="DeepMind投机解码算法">
<p>里面的 <span class="math inline">\((.)_+\)</span> 操作表示 <span class="math inline">\((f(x))_+=\frac{\max(0,f(x))}{\sum_x\max(0,f(x))}\)</span>
。</p>
<h1 id="speculative-sampling的正确性">speculative sampling的正确性</h1>
<p>我们希望投机解码的最终结果，和target
model自回归解码的结果一致，即完全无损，因此需要对投机采样做一些设计和分析。</p>
<p>首先，当前在transformer的解码上已经有很多策略，包括但不限于argmax、top-k采样、使用温度等。而大部分操作都是在logits上进行操作，这相当于改变了模型的输出分布。而在最终分布上的采样操作，都是相同的。因此我们可以只在朴素的标准采样上进行分析，而结果可以推广到其他的解码策略上。</p>
<p>假设 <span class="math inline">\(p(x)\)</span> 是target model <span class="math inline">\(M_p\)</span> 在当前输入下的分布， <span class="math inline">\(q(x)\)</span> 是approximation model <span class="math inline">\(M_q\)</span> 在当前输入下的分布。</p>
<p>投机解码的做法是，先采样 <span class="math inline">\(x\sim
q(x)\)</span>，如果 <span class="math inline">\(q(x)\leq
p(x)\)</span>，就保留 <span class="math inline">\(x\)</span>，否则就以
<span class="math inline">\(1-\frac{p(x)}{q(x)}\)</span> 的概率拒绝
<span class="math inline">\(x\)</span>，并在分布 <span class="math inline">\(p&#39;(x)=norm(max(0,p(x)-q(x)))\)</span>
对被拒绝的 <span class="math inline">\(x\)</span>
重新采样，并结束当前的投机解码。</p>
<p>其中 <span class="math inline">\(norm(max(0,p(x)-q(x)))=\frac{\max(0,p(x)-q(x))}{\sum_x\max(0,p(x)-q(x))}\)</span>
。</p>
<p>看起来并不复杂。一个问题是，为什么这样从 <span class="math inline">\(q(x)\)</span> 采样之后，我们得到的结果符合分布
<span class="math inline">\(p(x)\)</span>？即按这样的概率进行拒绝之后，结果和target
model自己解码一样？</p>
<p>从公式上来说，approximation model的抽样有 <span class="math inline">\(\tilde{x}\sim q\)</span>。假设 <span class="math inline">\(X\)</span> 是最终结果，我们的目标就是证明 <span class="math inline">\(\mathbb{P}(X=x)=p(x)\)</span>。</p>
<p>而要使得 <span class="math inline">\(X=x\)</span>，只有 <span class="math inline">\(\tilde{x}=x\)</span> 且 <span class="math inline">\(\tilde{x}\)</span> 被接受，或者在 <span class="math inline">\(\tilde{x}\)</span> 被拒绝之后重新采样到 <span class="math inline">\(\tilde{x}=x\)</span> 两种情况，即有</p>
<p><span class="math display">\[\mathbb{P}(X=x)\\=\mathbb{P}(\tilde{x}=x)\mathbb{P}(\tilde{x}\textit{
accepted}|\tilde{x}=x)\\+\mathbb{P}(\tilde{x}\textit{
rejected})\mathbb{P}(X=x|\tilde{x}\textit{ rejected})\]</span></p>
<p>对于第一项，有</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathbb{P}(\tilde{x}=x)\mathbb{P}(\tilde{x}\text{
ассерґе}d|\tilde{x}=x)\\=&amp;q(x)\min\left(1,\frac{p(x)}{q(x)}\right)\\=&amp;\min\left(q(x),p(x)\right)
\end{aligned}
\]</span></p>
<p>而第二项里</p>
<p><span class="math display">\[\begin{gathered}
\mathbb{P}(\tilde{x}\textit{ rejected})=1-\mathbb{P}(\tilde{x}\textit{
accepted}) \\
=1-\sum_{x^{\prime}}\mathbb{P}(X=x^{\prime},\tilde{x}\text{ ассерґе}d)
\\
=1-\sum_{x&#39;}\min(q(x&#39;),p(x&#39;)) \\
=\sum_{x&#39;}\max(0,p(x&#39;)-q(x&#39;)) \\
\end{gathered}\]</span></p>
<p>上式第三行到第四行的解释：第三行相当于计算1减区域b的面积，而区域a+区域b的面积和为1，因此第三行相当于区域a的面积，即
<span class="math inline">\(\sum_{x&#39;}\max(0,p(x&#39;)-q(x&#39;))\)</span>。</p>
<img src="/f5c015c/formula.png" class title="图解">
<p>从采样规则，有</p>
<p><span class="math display">\[\mathbb{P}(X=x|\tilde{x}\text{
rejected})=\frac{\max(0,p(x)-q(x))}{\sum_x\max(0,p(x)-q(x))}\]</span></p>
<p>因此</p>
<p><span class="math display">\[\mathbb{P}(\tilde{x}\text{
rejected})\mathbb{P}(X=x|\tilde{x}\text{
rejected})=\max(0,p(x)-q(x))\]</span></p>
<p>最终有</p>
<p><span class="math display">\[\mathbb{P}(X=x)\\=\min(q(x),p(x))+\max(0,p(x)-q(x))\\=p(x)\]</span></p>
<p>因此按照前面设计的规则进行采样，就能保证结果和target
model自己解码出来的一样。</p>
<h1 id="approximation-model的评估">approximation model的评估</h1>
<p>approximation model的一个采样 <span class="math inline">\(x\sim
q(x)\)</span> 被target model接受的概率为 <span class="math inline">\(\beta\)</span>，我们把这个概率叫acceptance
rate接受率。</p>
<p>那么其期望值 <span class="math inline">\(E(\beta)\)</span>
就是approximation model对target model拟合质量一个很好的评估指标。</p>
<p><span class="math inline">\(E(\beta)\)</span>
越大，每个token被接受的概率越大，那么每次投机解码能获得的输出token越多。</p>
<p>我们令 <span class="math inline">\(\alpha=E(\beta)\)</span>，并且为简化起见，假设
<span class="math inline">\(\beta\)</span>
的分布是i.i.d.的，那么跑一次投机解码能够获得的token数量是一个capped
geometric variable，其期望值如下式</p>
<p><span class="math display">\[E(\#\textit{ generated
tokens})=\frac{1-\alpha^{\gamma+1}}{1-\alpha}\]</span></p>
<p>不同 <span class="math inline">\(\gamma\)</span> 下的图像如下</p>
<img src="/f5c015c/fi_expected_token_num.png" class title="解码数量期望值">
<p>而 <span class="math inline">\(\alpha\)</span> 是可以推算的。</p>
<p>首先定义一个 <span class="math inline">\(M_p\)</span> 和 <span class="math inline">\(M_q\)</span> 之间的divergence <span class="math inline">\(D_{LK}\)</span></p>
<p><span class="math display">\[\begin{aligned}D_{LK}(p,q)=\sum_x|p(x)-M(x)|=\sum_x|q(x)-M(x)|\end{aligned}\]</span></p>
<p>其中 <span class="math inline">\(M(x)=\frac{p(x)+q(x)}2\)</span>。</p>
<p>而</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\sum_x|p(x)-M(x)|\\=&amp;\sum_x\frac{|p-q|}{2}\\=&amp;1-\sum_x\frac{p+q-|p-q|}2\\=&amp;1-\sum_x\min(p(x),q(x))
\end{aligned}
\]</span></p>
<p>因此有</p>
<p><span class="math display">\[D_{LK}(p,q)=1-\sum_x\min(p(x),q(x))\]</span></p>
<p><span class="math inline">\(D_{LK}(p,q)\)</span>越小，则 <span class="math inline">\(M_p\)</span> 和 <span class="math inline">\(M_q\)</span> 越相近。如果 <span class="math inline">\(D_{LK}(p,q)=0\)</span>，说明 <span class="math inline">\(p=q\)</span>；如果 <span class="math inline">\(D_{LK}(p,q)=1\)</span>，说明 <span class="math inline">\(p\)</span> 和 <span class="math inline">\(q\)</span> 两个分布完全没有交叉的部分。</p>
<p>根据 <span class="math inline">\(\beta\)</span> 的定义，有</p>
<p><span class="math display">\[
\begin{aligned}
\beta=&amp;E_{x\sim q(x)}\begin{cases}1&amp;q(x)\leq
p(x)\\\frac{p(x)}{q(x)}&amp;q(x)&gt;p(x)\end{cases}\\
=&amp;E_{x\thicksim q(x)}\min(1,\frac{p(x)}{q(x)})\\
=&amp;\sum_x\min(p(x),q(x))\\
=&amp;1-D_{LK}(p,q)
\end{aligned}
\]</span></p>
<p>最终得到</p>
<p><span class="math display">\[\alpha=E(\beta)=1-E(D_{LK}(p,q))=E(\min(p,q))\]</span></p>
<p>实验中，不同approximation model和target model之间测得的 <span class="math inline">\(\alpha\)</span> 值如下表所示</p>
<img src="/f5c015c/fi_alpha.png" class title="各种模型的alpha">
<h1 id="耗时优化的分析">耗时优化的分析</h1>
<p>定义cost coefficient <span class="math inline">\(c\)</span>，表示
<span class="math inline">\(M_q\)</span> 单次推理 和 <span class="math inline">\(M_p\)</span> 单次推理的比值。</p>
<p>和仅与模型相关的 <span class="math inline">\(\alpha\)</span>
不同，<span class="math inline">\(c\)</span>
的具体值会受到硬件、推理框架等影响。在论文的实验中 <span class="math inline">\(c\)</span> 的值大部分小于0.05。</p>
<p>假设 <span class="math inline">\(M_p\)</span> 每次推理所需的时间为
<span class="math inline">\(T\)</span>，则一次投机解码所需的时间为 <span class="math inline">\(Tc\gamma+T\)</span>。</p>
<p>根据前面的推算，投机解码每次能获得的token数为 <span class="math inline">\(E(\#\textit{ generated
tokens})=\frac{1-\alpha^{\gamma+1}}{1-\alpha}\)</span>
个，因此每个token所需的时间为 <span class="math inline">\(\frac{(c\gamma+1)(1-\alpha)}{1-\alpha^{\gamma+1}}T\)</span>。综上，使用投机解码在推理时间上的improvement
factor为</p>
<p><span class="math display">\[\frac{1-\alpha^{\gamma+1}}{(1-\alpha)(\gamma
c+1)}\]</span></p>
<p>只要 <span class="math inline">\(\alpha&gt;c\)</span>，就一定存在能提升解码效率的
<span class="math inline">\(\gamma\)</span>，并且improvement
factor至少为 <span class="math inline">\(\frac{1+\alpha}{1+c}\)</span>（<span class="math inline">\(\gamma=1\)</span>时）。</p>
<h1 id="计算成本的分析">计算成本的分析</h1>
<p><span class="math inline">\(M_p\)</span> 同时对 <span class="math inline">\(\gamma+1\)</span>
个token进行验证。如果一个token被接受了，那么推理效率就获得了提升；如果token被拒绝了，那么相关的计算就没有实际收益，就会有计算的“浪费”。</p>
<p>假设 <span class="math inline">\(\hat{c}\)</span> 是 <span class="math inline">\(M_q\)</span> 和 <span class="math inline">\(M_p\)</span> 计算一个token的arithmetic
operations的比例，<span class="math inline">\(\hat{T}\)</span> 是 <span class="math inline">\(M_p\)</span> 解码一个token所需的arithmetic
operations。</p>
<p>那么一次投机解码的计算量就是 <span class="math inline">\(\hat{T}\hat{c}\gamma+\hat{T}(\gamma+1)\)</span>，这个计算量除以投机解码每次获得的token数
<span class="math inline">\(\frac{1-\alpha^{\gamma+1}}{1-\alpha}\)</span>
就得到平均每个token的计算量为 <span class="math inline">\(\hat{T}\frac{(1-\alpha)(\gamma\hat{c}+\gamma+1)}{1-\alpha^{\gamma+1}}\)</span>。</p>
<p><span class="math inline">\(\alpha\)</span> 越大，<span class="math inline">\(\frac{(1-\alpha)(\gamma\hat{c}+\gamma+1)}{1-\alpha^{\gamma+1}}\)</span>
这个比值越小，平均计算成本越低。</p>
<p>另外，使用投机解码减少了KV cache和显存的读写。</p>
<h1 id="gamma-的选择"><span class="math inline">\(\gamma\)</span>
的选择</h1>
<p>给定 <span class="math inline">\(\alpha\)</span> 和 <span class="math inline">\(c\)</span>，最佳的 <span class="math inline">\(\gamma\)</span> 应该最大化walltime improvement
factor <span class="math inline">\(\frac{1-\alpha^{\gamma+1}}{(1-\alpha)(\gamma
c+1)}\)</span>。</p>
<p>下图给出不同 <span class="math inline">\(\alpha\)</span> 和 <span class="math inline">\(c\)</span> 下，最佳的 <span class="math inline">\(\gamma\)</span> 值</p>
<img src="/f5c015c/fi_choose_gamma.png" class title="gamma的选择">
<p>推理速度和总计算量之间有tradeoff，即增大 <span class="math inline">\(\gamma\)</span>
会提升推理速度，同时也会带来更多的计算成本，如下所示</p>
<img src="/f5c015c/fi_speed_and_op_table.png" class title="计算量和加速">
<img src="/f5c015c/fi_speed_and_op.png" class title="计算量和加速">
<img src="/f5c015c/fi_walltime.png" class title="walltime">
<p>实际上，<span class="math inline">\(\beta\)</span>
并不是固定的常数，因此实际上我们可以通过在投机解码的过程中预测 <span class="math inline">\(\beta\)</span> 值来选择 <span class="math inline">\(\gamma\)</span>，这是未来的一个改进方向。</p>
<h1 id="approximation-model的选择">approximation model的选择</h1>
<p>论文的实验中，一部分使用现成的模型作为approximation
model。这种情况下，让approximation model的参数规模比target
model小两个数量级是比较好的选择，能够平衡推理加速和计算量。</p>
<p>有趣的是，即使使用很简单的模型，比如n-gram模型作为approximation
model，也能获得不错的 <span class="math inline">\(\alpha\)</span>
值。</p>
<p>另外，在一些特殊的任务，比如摘要任务，由于生成结果往往会从输入的原文里摘取内容，因此使用一个会从输入里copy
token的approximation model可能会得到较高的 <span class="math inline">\(\alpha\)</span> 值。</p>
<p>approximation model的另一个选择是如《Blockwise parallel decoding for
deep autoregressive models》使用的非自回归模型。</p>
<h1 id="实验">实验</h1>
<p>论文在翻译任务和摘要任务上测试了投机解码的效果。使用了T5的较小规模模型作为approximation
model，来加速T5-XXL的推理，效果如下表，最高能达到3倍+的推理加速。</p>
<img src="/f5c015c/fi_t5_result.png" class title="T5系列加速效果">
<p>此外，论文对更多样的模型组合测试了 <span class="math inline">\(\alpha\)</span> 值，如下表所示</p>
<img src="/f5c015c/fi_alpha.png" class title="各种模型的alpha">
<p>可以观察到，比target model小几个数量级的approximation
model倾向于产生介于0.5和0.9之间的 <span class="math inline">\(\alpha\)</span>
值。还注意到，对于所有模型，用于采样的分布越尖（即T比较小，如argmax），
<span class="math inline">\(\alpha\)</span> 值越高。</p>
<h1 id="小结">小结</h1>
<ul>
<li>投机解码可以在完全无损的情况下，把推理速度提升2~3倍<br>
</li>
<li>即使使用最简单的n-gram模型，也能在投机解码的策略下获得推理速度提升<br>
</li>
<li>正常来说，使用比target model小两个数量级的approximation
model就有较好的效果</li>
</ul>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a target="_blank" rel="noopener" href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a target="_blank" rel="noopener" href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a target="_blank" rel="noopener" href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/1736008.html">大模型算法题(3)</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/1736008.html">大模型算法题(4)</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/336f2f3e.html">大模型算法题(5)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】Fast Inference from Transformers via Speculative Decoding
https://arxiv.org/abs/2211.17192<br>
【2】Accelerating Large Language Model Decoding with Speculative
Sampling https://arxiv.org/abs/2302.01318</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Lin
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://saicat.github.io/f5c015c.html" title="大模型推理加速-投机解码">https://saicat.github.io/f5c015c.html</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i> NLP</a>
              <a href="/tags/transformer/" rel="tag"><i class="fa fa-tag"></i> transformer</a>
              <a href="/tags/LLM/" rel="tag"><i class="fa fa-tag"></i> LLM</a>
              <a href="/tags/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/" rel="tag"><i class="fa fa-tag"></i> 推理加速</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/45ee1a6d.html" rel="prev" title="大模型推理窗口-从有限到无限大">
                  <i class="fa fa-angle-left"></i> 大模型推理窗口-从有限到无限大
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/7c04944d.html" rel="next" title="大模型算法题(6)">
                  大模型算法题(6) <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Lin</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">553k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">16:45</span>
  </span>
</div>
<div class="busuanzi-count">
</div>

<!--
-->


<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.28/fancybox/fancybox.umd.js" integrity="sha256-ytMJGN3toR+a84u7g7NuHm91VIR06Q41kMWDr2pq7Zo=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/fancybox.js"></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"Saicat/comment-utterance","issue_term":"pathname","theme":"github-light"}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
