---
title: 最近阅读2-关于自适应深度思考、context engineering和模型训练
tags:
  - NLP
  - LLM
categories:
  - CS
  - NLP
  - LLM
abbrlink: af7f9363
date: 2025-07-05 19:58:17
---

# 自适应深度思考  

LLM加上深度思考之后，能够处理更为复杂的任务。但是深度思考的耗时长、成本高，并且不是所有输入都需要深度思考。  

因此，一个思路是让模型根据输入的特性，使用不同的思考方式（无思考、短思考、长思考）。  

## Qwen3 + 分流  

1、混合思考模式  

Qwen3系列模型支持混合思考：可以进行深度思考，也可以不进行深度思考。不过是否进行深度思考需要手动设置。可以在apply_chat_template中设置enable_thinking（硬开关），也可以在每轮对话的user input里使用临时设置（软开关），打开或者关闭深度思考。  

虽然Qwen3模型本身没有自适应决定是否深度思考的功能，但是我们可以通过对输入query进行分流，配合深度思考的开关，达到「hard case深度思考，easy case不思考」的效果。  

2、思考预算  

如果关闭了深度思考开关，Qwen3就进入了「完全不思考」状态。除了「完全不思考」，Qwen3还有「不完全思考」的能力。这个也是在混合思考的训练中获得的能力。  

具体来说，用户可以设置模型的深度思考预算token数，比如1000，2000，4000。如果一个深度思考用完了预算给定的token量，那就会被强制结束深度思考（强制拼接上\</think\>）输出最终结果，即使这时的深度思考还不完整。  

模型最终的输出效果随着思考预算的提升而提升，因此我们可以根据需求，找一个效果和效率的平衡点。  

## AdaCoT  

原文：《AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning》  

1、要解决的问题  

在大模型深度思考的使用中，观察到：  

- 效率：对所有输入，模型无差别地使用长推理，这造成了浪费  
- 效果：有深度思考比不深度思考，整体效果更好  

那么应该就存在一个较好的效率和效果的平衡：在减少一些推理的情况下，效果下降不多。  

这个问题是一个Pareto-Optimal问题。具体来说，对于一个数据集，长思考的触发率T这么计算：  

$$T(\theta) = \frac{1}{N} \sum_{i=1}^{N} \mathbf{1}[\text{HasReasoning}(r_\theta(x_i))]$$  

模型效果可用平均得分计算：  

$$P(\theta) = \frac{1}{M} \sum_{j=1}^{M} \text{Score}_j(\theta)$$  

那所追求的帕累托最优的最终目标就是  

$$\theta^* = \arg\max_\theta \{\lambda_P \cdot P(\theta) - \lambda_T \cdot T(\theta)\}$$  

其中 $\lambda_P$ 和 $\lambda_T$ 相当于是超参，更看重效果就增大 $\lambda_P$，更看重效率就增大 $\lambda_T$。  

2、方法  

论文中通过多阶段训练的方法，让模型习得自己决定要不要进行深度思考的能力。  

（1）SFT  

第一阶段是SFT，所用的数据中，部分query使用了深度思考，部分则直接出答案。文中用LLM + prompt，来对不同类型的数据是否需要深度思考进行判断。具体的标准：  

| 类别                | 需触发深度思考的特征                                                                 | 无需触发思考的特征                          |
|---------------------|-------------------------------------------------------------------------------|------------------------------------------|
| 推理复杂度          | 需多步推理/逻辑链/概念分解（如数学证明、策略分析）                                | 直接回忆/常识问题（如事实查询、简单计算）       |
| 知识深度            | 需跨领域知识整合/专业术语解释（如法律条款分析、医学诊断）                         | 基础定义查询（如“水的沸点”）                |
| 解决方案性质        | 存在多路径/需创造性（如历史事件推演、开放式创作）                                 | 唯一标准答案（如“中国首都是？”）            |
| 用户意图            | 隐含深层需求（如“分析经济危机成因”需推导）                                      | 明确检索需求（如“2025年春节日期”）         |

和Qwen3一样，无论是否有触发深度思考，格式都是一致的，只是无思考时\<think\>和\</think\>中间为空。  

SFT数据中大约有67%的数据有深度思考。  

（2）RL  

第二阶段是RL，使用PPO。  

奖励函数设计：  

$$R(x,r) = R_{\text{base}}(x,r) - \alpha_1 \cdot P_{\text{miss}} - \alpha_2 \cdot P_{\text{over}} - \gamma \cdot P_{\text{int}}$$  

- $R_{\text{base}}$：基础质量奖励（如准确率）。  
- $P_{\text{miss}}$：应触发深度思考但未触发的惩罚。  
- $P_{\text{over}}$：不应触发但触发的惩罚。  
- $P_{\text{int}}$：格式错误惩罚。  

$\alpha_1$ 增大强调了使用深度思考的重要性，相当于提升了 $\lambda_P$；而 $\alpha_2$ 强调了效率的重要性，相当于提升了 $\lambda_T$。  

RL训练过程中，发现在一些领域比如数学，决策边界会出现崩溃：倾向于都思考或者都不思考，因此一个小技巧是对\<think\>后的第一个token不计算loss，这样可以保留SFT的思考比例，避免决策边界崩溃。  

（3）Meta-reasoning  

除了让模型自己决策要不要深度思考，也可以结合外部判断，比如对于复杂的case，在\<think\>后预先加入判断，比如：  

```
<think>此问题需结合历史背景和人物心理：1.玄武门之变是权力斗争转折点 2.李世民需平衡道德与政治需求
```

让模型在这个prefix上继续回答，整体效果有进一步的提升。  

## AdaptThink  

原文：《AdaptThink: Reasoning Models Can Learn When to Think》  

1、背景 & 思路  

AdaptThink和AdaCoT的观察类似：容易的case不思考效果更好，困难的case还是需要深度思考。思路上也是想办法让模型学会做二元决策：要不要使用深度思考。目标是保证准确率的情况下，尽量减少思考率。  

2、方案  

（1）RL  

AdaptThink通过强化学习（PPO）来优化思考决策。主要是在PPO的advantage上，加上一项无思考的奖励δ * 𝟙(y₁=\</think\>)：  

A(x,y) = δ * 𝟙(y₁=\</think\>) + R(x,y) - R_ref(x)  

（2）重要性采样  

由于起始模型是完全深度学习的模型，自己很难探索到不进行深度思考的方法，因此在训练初期手动强制无思考比例为50%：强制第一个生成token为\</think\>即为无思考，以此解决冷启动的探索问题。  

（3）效果  

论文用DeepSeek-R1-1.5B和DeepSeek-R1-7B模型在GSM8K/MATH/AIME上实验，token数下降了一半，效果还略略提升了（减少了过度思考带来的错误）。  

# context engineering  

之前在[Devin和Anthropic的Agent开发经验](https://mp.weixin.qq.com/s/GQC-7AmRPeW6p07cf5SEEg)提到context engineering，这个概念越来越受重视了。  

## LangChain：The rise of "context engineering"  

原文：[https://blog.langchain.com/the-rise-of-context-engineering/](https://blog.langchain.com/the-rise-of-context-engineering/)  

prompt engineering可以视为context engineering的一个子集。prompt工程对于简单的任务效果可以，但是如果要解决真正长期、复杂的问题，就要升级到context engineering。  

context engineering相比prompt engineering的主要变化有几点：  

- 系统性：提供给模型的信息更丰富，更系统，包括长短期的记忆信息、工具库、用户状态还有开发者信息等。  
- 动态性：不同的情况和输入需要不同组合的context，而不是固定不变的模板。  
- 结构化表达：如同人类沟通，信息格式决定理解效果。简明的错误描述远优于冗长的JSON块，工具参数的规范化设计同样关键。  

## 12 Factor Agents  

原文：https://github.com/humanlayer/12-factor-agents/tree/main  

从agent落地开发的角度给出的12个建议。  

Factor 1: Natural Language to Tool Calls  

把文字描述，转为结构化的工具。比较模型学习很数学和代码的推理能力，而结构化的输入更适合推理。  

Factor 2: Own your prompts  

虽然很多框架都提供了prompt，但是，最好还是自己写。  

Factor 3: Own your context window  

> Everything is context engineering.  

> Creating great context means:
> - The prompt and instructions you give to the model
> - Any documents or external data you retrieve (e.g. RAG)
> - Any past state, tool calls, results, or other history
> - Any past messages or events from related but separate histories/conversations (Memory)
> - Instructions about what sorts of structured data to output

简单来说就是提供更多的信息，其实就是context engineering的思路。  

{% asset_img context_eng.png 论文阅读2 %}  

Factor 4: Tools are just structured outputs  

别把工具搞太复杂了，只要能结构化就行，不一定要是复杂的json schema -- 可以多试试不同的结构。  

Factor 5: Unify execution state and business state  

execution state 和 business state 不必像传统app那样分得清清楚楚。其实从现在大部分的agent产品也能看到，内部的处理逻辑也经常暴露。  

Factor 6: Launch/Pause/Resume with simple APIs  

agent本质是一个程序，应当可以被用户、其他agent或者其他工具简单地启动和停止 -- 不要跑就停不下来无法控制。  

Factor 7: Contact humans with tool calls  

无论是否调用工具，让agent保持结构化的输出。  

Factor 8: Own your control flow  

掌握对agent全流程的控制，以方便实现：  

- 工具调用结果的摘要或缓存  
- 对结构化输出使用LLM作为评判器  
- 上下文窗口压缩或其他内存管理  
- 日志记录、追踪和指标统计  
- 客户端速率限制  
- 持久化休眠/暂停/"等待事件"  
- 等  

Factor 9: Compact Errors into Context Window  

把流程中的错误信息，也在context window上流转，这样的信息一定程度上让agent具备自我修复能力。  

Factor 10: Small, Focused Agents  

与其构建大而全的万能agent，不如开发小而精的专用agent，让每个agent专注做好一件事。  

这条个人保留意见。短期内是这样，但是长期就未必了。  

Factor 11: Trigger from anywhere, meet users where they are  

允许用户通过Slack、电子邮件、短信或任何他们偏好的渠道触发agent，并支持agent通过相同渠道进行响应。  

这个也是从产品角度出发，把agent打造成和人一样的存在。  

Factor 12: Make your agent a stateless reducer  

把agent看作简单的for循环的话，就能明白「无状态」。  

# 模型训练  

## OctoThinker  

原文：《OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling》  

1、背景  

和《Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs》的观察类似，OctoThinker发现Llama系列在RL之后效果不好，远不如Qwen系列。  

论文对预训练->RL的数据适配做了探索。  

2、工作  

（1）Mid-training  

目前的认知是RL提升的是模型的信心，而不是能力。模型的推理能力更多还是在预训练中获得。那么在RL之前，增加包含高质量数据的mid-training。  

阶段1：200B高质量推理数据（85% MegaMath-Web-Pro-Max + 15% 代码/QA 数据）。经过这个训练之后，基座模型数学能力提升 10–20%（如 MATH500 从 7.4 → 22.4），为后面的RL提供更好的起点。  

阶段2：注入多样化推理行为，适配 RL 目标。具体来说，在阶段1的基础上，用三种不同的数据训练三个分支模型：  

| **分支**       | **数据配方**                                                                 | **特点**                     |
|----------------|----------------------------------------------------------------------------|------------------------------|
| **Long**       | 长链思维（CoT）数据为主（OpenR1-Math + AM-DeepSeek）                          | 深度推理，易冗长但效果强     |
| **Short**      | 短链 QA 数据为主（MegaMath-QA + OpenMathInstruct2）                         | 简洁响应，稳定性高           |
| **Hybrid**     | 混合长短链数据（OpenMathInstruct2 + NuminaMath1.5 + OpenR1）               | 平衡深度与稳定性             |

（2）RL  

在Long/Short/Hybrid三个分支基础上，分别进行RL，得到OctoThinker-{Long/Short/Hybrid}。各个分支有不同的表现：  

- **Long 分支**：复杂任务（如 MATH）最强，但长度较长。  
- **Short 分支**：简单任务（如 GSM8K）高效稳定。  
- **Hybrid 分支**：综合性能最佳。  

OctoThinker-Long-3B 经 RL 后达到 Qwen2.5-3B 同等水平（MATH500：65.2 vs. 66.4）。  

（3）结论  

Qwen 预训练数据（高质量数学+对齐的 QA 分布）天然更适配 RL，而 Llama 需通过中期训练弥补。  

***  

博客：[http://www.linsight.cn/](http://www.linsight.cn/)  
知乎：[Linsight](https://www.zhihu.com/people/us4ever)  
微信公众号：Linsight  
![](/images/qrcode.jpg)
博主微信号(添加请注明来意)：  
![](/images/wechat.png)  

***  

【推荐文章】  
- Agent：  
[Agent完全手册(零)：三大模块，三个理念](https://www.linsight.cn/b242bfb3.html)  
[DeepResearch的报告生成方法](https://www.linsight.cn/44c62dc5.html)  
[从RAG到DeepSearch](https://www.linsight.cn/7c2f9dcb.html)  
[agent调研(1)--MetaGPT,OpenManus和OWL](https://www.linsight.cn/226b059f.html)  
[Devin和Anthropic的Agent开发经验](https://www.linsight.cn/f93b3aaf.html)  
- MoE：  
[DeepSeek-V3细节探索](https://www.linsight.cn/a9c496e3.html)  
[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  
[DeepSeek-V2和MLA](https://www.linsight.cn/83c49df0.html)  
[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  
[成本10w刀的JetMoE](https://www.linsight.cn/f3acf042.html)  
[MoE的top-p routing](https://www.linsight.cn/224c42da.html)  
[对MoE模型的一些观察](https://www.linsight.cn/5e1d14b3.html)  
[从dense到MoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  
[MoE路由--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  
- 端侧模型：  
[苹果智能系统模型--AFM](https://www.linsight.cn/1e34e252.html)  
[MiniCPM](https://www.linsight.cn/376db710.html)  
[适合移动设备的语言模型--MobileLLM](https://www.linsight.cn/5ac36d34.html)  
[phi系列模型](https://www.linsight.cn/fe13b56f.html)  
[Gemma2](https://www.linsight.cn/cf3f1f81.html)  
[苹果的OpenELM](https://www.linsight.cn/f845f3e4.html)  
[bilibili的index-1.9B](https://www.linsight.cn/770b63e1.html)  
- 预训练：  
[Qwen3实测&技术报告](https://www.linsight.cn/37ee84bb.html)  
[代码大模型(一)--业界现状](https://www.linsight.cn/a0b50049.html)  
[代码大模型(二)--OpenCoder](https://www.linsight.cn/7856bcc1.html)  
[LLM高效预训练(一)](https://www.linsight.cn/dcb57672.html)  
[LLM高效预训练(二)](https://www.linsight.cn/1e2e35a7.html)  
[Llama3.1--预训练要点一览](https://www.linsight.cn/7d7294cb.html)  
[Qwen2技术报告](https://www.linsight.cn/a8f8b641.html)  
[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  
[InternLM系列模型](https://www.linsight.cn/7f3d361.html)  
[GLM4报告的一些技术点](https://www.linsight.cn/a5206abd.html)  
[从Yuan2.0到Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  
[从loss视角理解大模型涌现能力](https://www.linsight.cn/f5fb75e4.html)  
- 数据：  
[训练数据合成(一)](https://www.linsight.cn/85132189.html)  
[训练数据合成(二)](https://www.linsight.cn/2a22baeb.html)  
[训练数据合成(三)](https://www.linsight.cn/e259c7b2.html)  
[LLM预训练数据策略(一)](https://www.linsight.cn/2c2cdc34.html)  
[预训练数据处理--长度分解](https://www.linsight.cn/210dbccd.html)  
- 长上下文：  
[Qwen2.5-1M技术解析](https://www.linsight.cn/6c0f6207.html)  
[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  
[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  
[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  
[prompt压缩(一)](https://www.linsight.cn/4519eadd.html)  
[prompt压缩(二)](https://www.linsight.cn/ea2871bf.html)  
[reasoning压缩(一)](https://www.linsight.cn/bfa4f144.html)  
- 推理加速：  
[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  
[大模型推理加速-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  
- 对齐：  
[深度求索DeepSeek-R1详解](https://www.linsight.cn/9e4b4e6d.html)  
[基模型Cognitive Behaviors对RL的影响](https://www.linsight.cn/657a6d17.html)  
[Llama3.1--post-training要点一览](https://www.linsight.cn/93328a2a.html)  
[模型平均 -- model soup](https://www.linsight.cn/bb8fcf21.html)  
[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  
[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  
[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  
[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  
- Transformer：  
[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  
[LLM的重复生成和ICL](https://www.linsight.cn/7381cae3.html)  
[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  
[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  
[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  
[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  
[RoPE的远距离衰减](https://www.linsight.cn/f0902f1a.html)  
[LLM水印](https://www.linsight.cn/2dee4921.html)  
- 训练框架  
[LLM训练框架：从优化器和精度讲到ZeRO](https://www.linsight.cn/fe0adaa5.html)  
[LLM训练各种并行策略](https://www.linsight.cn/4cd8532f.html)  
- 项目应用：  
[一个模型支持智能助手系统](https://www.linsight.cn/9c593ccd.html)  
[关于The Bitter Lesson](https://www.linsight.cn/d253d7b3.html)  
- CV：  
[CV入门--关于Vision Transformer](https://www.linsight.cn/a11e2633.html)  
[CV入门--无监督学习](https://www.linsight.cn/ae81a87b.html)  
- 多模态：  
[多模态入门(一)--CLIP](https://www.linsight.cn/3069051d.html)  
[多模态入门(二)--Flamingo,LLaVA系列和BLIP系列](https://www.linsight.cn/569d722c.html)  
[多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列](https://www.linsight.cn/f16505b3.html)  
[多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B](https://www.linsight.cn/e00debee.html)  
[多模态入门(五)--InternVL系列](https://www.linsight.cn/52c8a4f9.html)  
[小米的移动UI多模态模型--MobileVLM](https://www.linsight.cn/96393d3b.html)  
[DeepSeek-VL2的细节](https://www.linsight.cn/b4d047c1.html)  
- 大模型算法题：  
[(1)](http://www.linsight.cn/3345028a.html)、
[(2)](http://www.linsight.cn/ad0bba9d.html)、
[(3)](http://www.linsight.cn/1736008.html)、
[(4)](http://www.linsight.cn/1736008.html)、
[(5)](http://www.linsight.cn/336f2f3e.html)、
[(6)](http://www.linsight.cn/7c04944d.html)、
[(7)](https://www.linsight.cn/dd614e12.html)、
[(8)](https://www.linsight.cn/e287b9c3.html)、
[(9)](https://www.linsight.cn/fb9c8882.html)  
