---
title: 最近阅读--关于数据合成、agent、reasoning和多任务
tags:
  - NLP
  - LLM
categories:
  - CS
  - NLP
  - LLM
abbrlink: e96c7aac
date: 2025-06-28 11:42:22
---

新增一个栏目，写写平时读的文献，主要看思路。  

# 数据合成  

## 多轮对话数据合成：Review-Instruct  

原文：ACL2025：《Review-Instruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models》[https://arxiv.org/abs/2505.11010](https://arxiv.org/abs/2505.11010)  

1、要解决的问题  

现在的LLM使用的对话训练数据大部分是单轮的，而少量多轮的数据，上下文的逻辑联系也不多，导致在多轮对话场景下，LLM的能力相比单轮要差一些。  

因此，要想办法提升多轮对话数据的上下文联系，和难度（最主要的就是难度）。  

一些依赖LLM合成数据的方法，多样性和难度不足。  

2、方法  

Review-Instruct提出“Ask-Respond-Review”三阶段迭代流程，来提升多轮合成数据的难度。  

具体来说，对于每一条query（相当于用户的第一轮问题），candidate LLM先生成对应的response。在这个条件下，我们想要获取和这轮对话深度相关的后续问答。  

先使用一组reviewer（每个reviewer是一个LLM Agent）对前一轮生成的response进行评审，主要包括回答的深度、丰富度、相关性、连贯性等，并提出一些意见，比如“要在xx方面回答得更深入一些”“需要关注xx的长期影响力”。然后有一个chairman（也是一个LLM Agent），整合多个reviewer的意见，然后对前一轮的response进行追问（相当于用户的下一轮追问），这样多轮对话在上下文中的关联性和难度就获得了提升。  

# Agent

## Toward a Theory of Agents as Tool-Use Decision-Makers  

原文：[https://arxiv.org/abs/2506.00886](https://arxiv.org/abs/2506.00886)  

主要是一些认知和agent设计原则：  

- 知识边界：提升模型决策效率的一个要点是：知道自己知道什么，知道自己不知道什么  
- 决策边界：模型需要知道自己什么时候“想”，什么时候“做”  
- 理想情况是：知识边界和决策边界重合，知道的就想，不知道就找  
- 想和做本质上应该是平等的，都是一个工具  

最后这一点，把think看作工具，和下面这个有些相关。  

## Anthropic：把think当做工具  

原文：[https://www.anthropic.com/engineering/claude-think-tool](https://www.anthropic.com/engineering/claude-think-tool)  

1、think tool 和 extended thinking  

extended thinking：如DeepSeek-R1的深度思考，在正式回答前。  

think tool：在回答过程中，可以停下来思考，模型自己决定什么时候需要停下来思考，就像在深度思考中调用工具一样自然。  

2、think tool  

说白了，think tool就是propmt + LLM，根据不同的需要求，设计propmt，接收不同的输入和输出。description会明确介绍这个思考工具的功能，好让模型自己决定什么时候调用。  

用不同的prompt就可以得到不同的思考工具。  

决定是否使用think tool：ReAct方式来决定工具的调用。  

think tool获取的结果再通过prompt加到原模型的上下文。  

# reasoning模型  

## Innate Reasoning is Not Enough  

原文：《Innate Reasoning is Not Enough: In-Context Learning Enhances Reasoning Large Language Models with Less Overthinking》[https://arxiv.org/abs/2503.19602](https://arxiv.org/abs/2503.19602)  

1、背景  

CoT在非reasoning模型上很有效果，那CoT在reasoning模型上效果如何？对这个问题做了一些实验。  

2、发现  

以R1-Qwen-xxB各种规模的模型做实验。  

（1）CoT在reasoning LLM也有效果  

CoT在不同规模的reasoning模型和不同任务上的效果：  

|  | 小模型 | 大模型 |
| --- | --- | --- |
| 简单任务 | CoT提升较多 | CoT提升较少 |
| 困难任务 | CoT提升较少 | CoT提升较多 |

总结来说，就是在模型容量适配的任务上，CoT效果更好。  

在简单任务上，效果随着思考长度的增加，呈现先上升后下降的趋势。也就是，一定的思考可以提升效果，而过多的思考则损害效果。  

（2）CoT能减少冗余思考  

- 直接prompt：生成的思考token数量分布分散，且存在大量冗余  
- Few-shot CoT：能有效集中思考token的分布，减少冗余  
- Zero-shot CoT：介于直接提示和Few-shot CoT之间  

直观上来想，有CoT或者few-shot，模型更容易理解指令，从而减少不必要的思考步骤。  

Few-shot CoT中，One-shot CoT效果最佳。  

（3）reasoning对反思词汇的attention异常高  

分析attention logits发现：reasoning模型对反思相关词汇（如“Wait”“Ensure”）分配了异常高的注意力。  

而基础模型未表现出这种模式，表明reasoning LLM在训练中过拟合了反思机制。  

CoT提示通过外部引导缓解了这种过拟合行为。  

## Mixture-of-Thought  

原文：《Learning to Reason via Mixture-of-Thought for
Logical Reasoning》[https://arxiv.org/abs/2505.15817](https://arxiv.org/abs/2505.15817)  

1、要解决的问题  

让模型使用自然语言进行思考和推理是现在的主流方式，但是自然语言推理有其局限性，在很多问题上的效果不够好。  

2、方法  

（1）3种推理方式  

既然一种推理方式有问题，那就多弄几种不同形式的推理CoT。  

- 自然语言推理（NL CoT）  

目前最常用。适合用于需要常识、逻辑深度较浅、开放语义场景的推理。  

- 代码推理（code CoT）  

结构化的推理方式，在这篇论文中仅写代码，不执行。  

类定义、函数定义适用于规则约束、状态转移的场景。  

- 真值表推理（Truth Table CoT）  

这篇论文新创的，也是结构化的推理。真值表适用于多种情况需要遍历的场景。  

（2）训练 & 推理  

这3种方式适用于不同的情况，那么一个使用方式就是对于一个任务，用三种方式都跑一遍结果，然后投票。  

为了提升效果，需要训练模型。训练数据是合成的，三种都合成，然后根据正确答案进行过滤，保留合理的推理轨迹。  

然后用合成的数据训练模型。这个「合成数据」->「训练」的模型会迭代几次，下次的数据合成模型就用这轮训出来的最佳模型做。  

# 训练方式

## 主动学习：Self-Adapting Language Models  

原文：[https://arxiv.org/abs/2506.10943](https://arxiv.org/abs/2506.10943)  

提出SEAL学习框架。  

1、要解决的问题  

背景是让LLM来处理新任务。  

LLM依赖SFT或者ICL来处理新的任务，但是SFT需要预先获取大量高质量的训练数据；ICL不用训练，但ICL无法持久化，效果也不如SFT。  

2、思路  

SFT是被动地学习，而人在学习的时候，除了被动学习，还有主动学习，就像学生除了听课（被动学习）还会整理笔记（主动学习），用更清晰的格式和说法来帮助自己理解知识。写技术博客也是一种以输出帮助学习的方法。那模型如果能自己整理知识，那应该也能学得更好。  

3、方法  

自己整理知识自己学习，简单来说就是自己生成数据，再训练自己。  

比如说现在要在一个知识数据集上训练，以提升LLM在这个领域的知识水平。原始的训练数据集D={(C,τ)}，其中C是一个知识文档，τ是用于验证在这个文档上学习效果的数据，可以是C相关的知识问答题、选择题之类的（有答案的那种）。  

本来微调是拿C直接训，而SEAL不拿C直接训，而是：  

- 1、用LLM从C生成多个self-edit（SE）数据  
- 2、在每个SE上用LoRA训练LLM，获得LLM'  
- 3、用τ评测第2步训练出来的LLM'，获得reward r  
- 4、选择第3步中有正收益的SE，和对应的r，通过强化学习（ReST^EM方法）来更新LLM（不是LLM'） 

SE里除了来自C的增强数据，还有用于训练LLM的超参，这相当于让模型学习怎么更新自己的参数。  

对于不同的数据类型，τ也不同，主要原则就是τ需要能够评测训练结果，获得reward。  

## 多任务免训练泛化：Text-to-LoRA  

原文：[https://arxiv.org/abs/2506.06105](https://arxiv.org/abs/2506.06105)  

1、要解决的问题  

如果我们有多个任务要处理，需要收集多个任务的数据，然后训练多个LoRA。这样的做法任务和LoRA一一对应，成本高，再加入新的任务的时候又要重新训练新的LoRA。  

2、思路  

如果能搞一个工具，这个工具的输入是文本，输出是LoRA，那么以后就不用训练LoRA了。Text-to-LoRA的目的就是搞这么一个工具。相当于获得LoRA泛化器，这个LoRA泛化器称之为T2L，是一个hypernetwork。hypernetwork的输出是另一个神经网络。  

3、方法  

T2L的输入是query，输出是LoRA参数。T2L本身也是个神经网络，原文设计了几个不同的变体，但是参数量都很少，比单个LoRA大不了多少。  

获得T2L有两种方法。  

（1）Reconstruction  

如果我们已经有很多LoRA参数（实验是479）个，那么可以直接让T2L学习input query到LoRA参数的构建，误差就是hypernetwork输出的AB矩阵和真实的AB矩阵的误差。  

（2）直接SFT  

如果我们没有训练好LoRA，但是有数据（input，label），那对于每条input，可以先获得T2L的AB矩阵，把AB矩阵加到Base Model上，获得输出，计算输出和label的loss，反向传播训练T2L的参数。  

4、效果  

根据文中测试，T2L方法的效果介于单独LoRA训练和，和prompt engineering之间。  

5、收益  

收益就是如果T2L hypernetwork泛化性够好，那么就对于新任务，可以直接提升效果（效果优于prompt工程），而不需要训练新的LoRA了。  

***  

博客：[http://www.linsight.cn/](http://www.linsight.cn/)  
知乎：[Linsight](https://www.zhihu.com/people/us4ever)  
微信公众号：Linsight  
![](/images/qrcode.jpg)
博主微信号(添加请注明来意)：  
![](/images/wechat.png)  

***  

【推荐文章】  
- Agent：  
[Agent完全手册(零)：三大模块，三个理念](https://www.linsight.cn/b242bfb3.html)  
[DeepResearch的报告生成方法](https://www.linsight.cn/44c62dc5.html)  
[从RAG到DeepSearch](https://www.linsight.cn/7c2f9dcb.html)  
[agent调研(1)--MetaGPT,OpenManus和OWL](https://www.linsight.cn/226b059f.html)  
[Devin和Anthropic的Agent开发经验](https://www.linsight.cn/f93b3aaf.html)  
- MoE：  
[DeepSeek-V3细节探索](https://www.linsight.cn/a9c496e3.html)  
[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  
[DeepSeek-V2和MLA](https://www.linsight.cn/83c49df0.html)  
[昆仑万维-SkyworkMoE](https://www.linsight.cn/1d5bcd45.html)  
[成本10w刀的JetMoE](https://www.linsight.cn/f3acf042.html)  
[MoE的top-p routing](https://www.linsight.cn/224c42da.html)  
[对MoE模型的一些观察](https://www.linsight.cn/5e1d14b3.html)  
[从dense到MoE -- sparse upcycling](https://www.linsight.cn/a0824e29.html)  
[MoE路由--expert choice routing](https://www.linsight.cn/2c8bbc7.html)  
- 端侧模型：  
[苹果智能系统模型--AFM](https://www.linsight.cn/1e34e252.html)  
[MiniCPM](https://www.linsight.cn/376db710.html)  
[适合移动设备的语言模型--MobileLLM](https://www.linsight.cn/5ac36d34.html)  
[phi系列模型](https://www.linsight.cn/fe13b56f.html)  
[Gemma2](https://www.linsight.cn/cf3f1f81.html)  
[苹果的OpenELM](https://www.linsight.cn/f845f3e4.html)  
[bilibili的index-1.9B](https://www.linsight.cn/770b63e1.html)  
- 预训练：  
[Qwen3实测&技术报告](https://www.linsight.cn/37ee84bb.html)  
[代码大模型(一)--业界现状](https://www.linsight.cn/a0b50049.html)  
[代码大模型(二)--OpenCoder](https://www.linsight.cn/7856bcc1.html)  
[LLM高效预训练(一)](https://www.linsight.cn/dcb57672.html)  
[LLM高效预训练(二)](https://www.linsight.cn/1e2e35a7.html)  
[Llama3.1--预训练要点一览](https://www.linsight.cn/7d7294cb.html)  
[Qwen2技术报告](https://www.linsight.cn/a8f8b641.html)  
[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  
[InternLM系列模型](https://www.linsight.cn/7f3d361.html)  
[GLM4报告的一些技术点](https://www.linsight.cn/a5206abd.html)  
[从Yuan2.0到Yuan2.0-M32](https://www.linsight.cn/3df0cd42.html)  
[从loss视角理解大模型涌现能力](https://www.linsight.cn/f5fb75e4.html)  
- 数据：  
[训练数据合成(一)](https://www.linsight.cn/85132189.html)  
[训练数据合成(二)](https://www.linsight.cn/2a22baeb.html)  
[训练数据合成(三)](https://www.linsight.cn/e259c7b2.html)  
[LLM预训练数据策略(一)](https://www.linsight.cn/2c2cdc34.html)  
[预训练数据处理--长度分解](https://www.linsight.cn/210dbccd.html)  
- 长上下文：  
[Qwen2.5-1M技术解析](https://www.linsight.cn/6c0f6207.html)  
[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  
[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  
[大模型推理窗口-从有限到无限大](http://www.linsight.cn/45ee1a6d.html)  
[prompt压缩(一)](https://www.linsight.cn/4519eadd.html)  
[prompt压缩(二)](https://www.linsight.cn/ea2871bf.html)  
[reasoning压缩(一)](https://www.linsight.cn/bfa4f144.html)  
- 推理加速：  
[大模型推理加速-投机解码](http://www.linsight.cn/f5c015c.html)  
[大模型推理加速-MEDUSA](https://www.linsight.cn/7bbe2df6.html)  
- 对齐：  
[深度求索DeepSeek-R1详解](https://www.linsight.cn/9e4b4e6d.html)  
[基模型Cognitive Behaviors对RL的影响](https://www.linsight.cn/657a6d17.html)  
[Llama3.1--post-training要点一览](https://www.linsight.cn/93328a2a.html)  
[模型平均 -- model soup](https://www.linsight.cn/bb8fcf21.html)  
[大模型偏好对齐-DPO](http://www.linsight.cn/473f2b43.html)  
[大模型偏好对齐-ODPO](http://www.linsight.cn/da871ebe.html)  
[大模型偏好对齐-simPO](http://www.linsight.cn/280fa97a.html)  
[大模型偏好对齐-IPO](http://www.linsight.cn/4fe7b810.html)  
- Transformer：  
[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  
[LLM的重复生成和ICL](https://www.linsight.cn/7381cae3.html)  
[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  
[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  
[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  
[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  
[RoPE的远距离衰减](https://www.linsight.cn/f0902f1a.html)  
[LLM水印](https://www.linsight.cn/2dee4921.html)  
- 训练框架  
[LLM训练框架：从优化器和精度讲到ZeRO](https://www.linsight.cn/fe0adaa5.html)  
[LLM训练各种并行策略](https://www.linsight.cn/4cd8532f.html)  
- 项目应用：  
[一个模型支持智能助手系统](https://www.linsight.cn/9c593ccd.html)  
[关于The Bitter Lesson](https://www.linsight.cn/d253d7b3.html)  
- CV：  
[CV入门--关于Vision Transformer](https://www.linsight.cn/a11e2633.html)  
[CV入门--无监督学习](https://www.linsight.cn/ae81a87b.html)  
- 多模态：  
[多模态入门(一)--CLIP](https://www.linsight.cn/3069051d.html)  
[多模态入门(二)--Flamingo,LLaVA系列和BLIP系列](https://www.linsight.cn/569d722c.html)  
[多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列](https://www.linsight.cn/f16505b3.html)  
[多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B](https://www.linsight.cn/e00debee.html)  
[多模态入门(五)--InternVL系列](https://www.linsight.cn/52c8a4f9.html)  
[小米的移动UI多模态模型--MobileVLM](https://www.linsight.cn/96393d3b.html)  
[DeepSeek-VL2的细节](https://www.linsight.cn/b4d047c1.html)  
- 大模型算法题：  
[(1)](http://www.linsight.cn/3345028a.html)、
[(2)](http://www.linsight.cn/ad0bba9d.html)、
[(3)](http://www.linsight.cn/1736008.html)、
[(4)](http://www.linsight.cn/1736008.html)、
[(5)](http://www.linsight.cn/336f2f3e.html)、
[(6)](http://www.linsight.cn/7c04944d.html)、
[(7)](https://www.linsight.cn/dd614e12.html)、
[(8)](https://www.linsight.cn/e287b9c3.html)、
[(9)](https://www.linsight.cn/fb9c8882.html)  
