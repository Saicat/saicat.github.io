---
title: 大模型算法题(3)
tags:
  - NLP
  - LLM
  - 算法题
categories:
  - CS
  - NLP
  - LLM
abbrlink: '17360081'
date: 2024-04-05 14:08:31
---

![](/images/cover.png)  

【往期文章】

[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  
[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  
[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  
[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  
[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  
[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  
[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  
[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  

***  

【本文已在同名 微信公众号 / 知乎 / [个人博客linsight.cn](http://www.linsight.cn/) 上线】  

本系列将持续整理一些LLM中关键的、细节的、值得关注的内容，持续更新~如有错漏，欢迎指正~

***  

# 1.旋转位置编码RoPE有什么优缺点？  

优点：RoPE以绝对位置编码的方式实现了相对位置编码，使得能够在不破坏注意力形式的情况下，以“加性编码”的方式让模型学习相对位置。①相比其他相对位置编码来说，实现简单，计算量少。②可以应用于线性注意力。③RoPE具有远程衰减的特性，使得每个位置天然能够更关注到附近的信息。  

缺点：RoPE相比训练式的绝对位置编码具有一定的外推能力，如可以在2k数据长度训练的模型进行略长于2k的推理。但是相比于Alibi等位置编码，其直接外推能力并不算特别好，需要通过线性插值、NTK插值、YaRN等方式来优化外推能力。  

# 2.batchnorm中的momentum怎么影响训练效果  

batchnorm在训练时计算每个batch内的均值和方差用于normalization，同时统计一个全局均值和方差用于推理。全局均值和方差计算公式为：  

moving_mean = momentum × moving_mean + (1.0 − momentum) × mean  

moving_var = momentum × moving_var + (1.0 − momentum) × var  

小的momentum值对应快的更新速度，能够更快地向真实分布靠近，但是同时也会导致更大的波动；如果更新过慢，则可能导致训练结束时还没有统计到真实的分布，是欠拟合的状态。如果batch size比较小，每个mini batch和全局差异较大，就不应该用太大的momentum。  

# 3.多头注意力相比单头有什么好处？  

多头注意力使用多个维度较低的子空间分别进行学习。  

一般来说，相比单头的情况，多个头能够分别关注到不同的特征，增强了表达能力。多个头中，会有部分头能够学习到更高级的特征，并减少注意力权重对角线值过大的情况。  

比如部分头关注语法信息，部分头关注知识内容，部分头关注近距离文本，部分头关注远距离文本，这样减少信息缺失，提升模型容量。  

另外虽然多头注意力的整体计算量比单头要大一点，但是并行度也高一些。  

# 4.kv cache为什么能加速推理？  

对于GPT类模型，使用的是单向注意力，每个位置只能看到自己和前面的内容。  

在进行自回归解码的时候，新生成的token会加入序列，一起作为下一次解码的输入。  

由于单向注意力的存在，新加入的token并不会影响前面序列的计算，因此可以把已经计算过的每层的kv值保存起来，这样就节省了和本次生成无关的计算量。  

通过把kv值存储在速度远快于显存的L2缓存中，可以大大减少kv值的保存和读取，这样就极大加快了模型推理的速度。  

# 5.ReLU有什么优缺点？  

优点：（1）计算快，前向只需要进行max(0, x)计算，后向则是直接透传；（2）有激活值的时候，梯度恒定为1，不会爆炸/消失；  

缺点：（1）均值不为0，分布产生偏移（2）输入值小于0时，梯度再也无法回传过来，导致神经元坏死。  

***  

读到这了，来一发点赞收藏关注吧~

博客：[http://www.linsight.cn/](http://www.linsight.cn/)  
知乎：[Linsight](https://www.zhihu.com/people/us4ever)  
微信公众号：Linsight  
![](/images/qrcode.jpg)  

***  

【往期文章】

[MoE模型的前世今生](http://www.linsight.cn/44e38c1b.html)  
[LLM长上下文的问题](http://www.linsight.cn/c4da56c0.html)  
[解锁大模型长上下文能力](http://www.linsight.cn/cc852861.html)  
[理解Attention:从起源到MHA,MQA和GQA](http://www.linsight.cn/3dc22f96.html)  
[Yi技术报告-划重点看细节](http://www.linsight.cn/41b6a819.html)  
[transformer中normalization的二三事](http://www.linsight.cn/6a40bfa5.html)  
[从代码实现看normalization-到底做了什么](http://www.linsight.cn/b70b4a2d.html)  
[稀疏注意力计算:sliding window attention](http://www.linsight.cn/c61d17e3.html)  
[理解LLM位置编码:RoPE](http://www.linsight.cn/a051710f.html)  
[大模型算法题(1)](http://www.linsight.cn/3345028a.html)  
[大模型算法题(2)](http://www.linsight.cn/ad0bba9d.html)  