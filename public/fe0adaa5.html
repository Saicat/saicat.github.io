<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon/favicon_io/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon/favicon_io/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon/favicon_io/favicon-16x16.png">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.28/fancybox/fancybox.css" integrity="sha256-6cQIC71/iBIYXFK+0RHAvwmjwWzkWd+r7v/BX3/vZDc=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"saicat.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="【本文已在同名 微信公众号 &#x2F; 知乎 &#x2F; 个人博客linsight.cn 上线】  这篇文章主要从一个搞数据和训练策略的LLM算法工程师角度总结一下之前用到的训练框架相关知识，包括优化器、精度和混合精度训练和DP、ZeRO的相关内容。">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM训练框架：从优化器和精度讲到Zero">
<meta property="og:url" content="https://saicat.github.io/fe0adaa5.html">
<meta property="og:site_name" content="Linsight">
<meta property="og:description" content="【本文已在同名 微信公众号 &#x2F; 知乎 &#x2F; 个人博客linsight.cn 上线】  这篇文章主要从一个搞数据和训练策略的LLM算法工程师角度总结一下之前用到的训练框架相关知识，包括优化器、精度和混合精度训练和DP、ZeRO的相关内容。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://saicat.github.io/fe0adaa5/mix_precision_fp16.png">
<meta property="og:image" content="https://saicat.github.io/fe0adaa5/loss_scaling.png">
<meta property="og:image" content="https://saicat.github.io/fe0adaa5/ring.jpg">
<meta property="og:image" content="https://saicat.github.io/fe0adaa5/zero.png">
<meta property="og:image" content="https://saicat.github.io/images/qrcode.jpg">
<meta property="og:image" content="https://saicat.github.io/images/wechat.png">
<meta property="article:published_time" content="2025-05-17T08:33:15.000Z">
<meta property="article:modified_time" content="2025-05-18T11:16:10.350Z">
<meta property="article:author" content="Lin">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="预训练">
<meta property="article:tag" content="分布式">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://saicat.github.io/fe0adaa5/mix_precision_fp16.png">


<link rel="canonical" href="https://saicat.github.io/fe0adaa5.html">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://saicat.github.io/fe0adaa5.html","path":"fe0adaa5.html","title":"LLM训练框架：从优化器和精度讲到Zero"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>LLM训练框架：从优化器和精度讲到Zero | Linsight</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Linsight</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">聊聊技术，也聊聊其他的</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#optimizer"><span class="nav-number">1.</span> <span class="nav-text">Optimizer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%8Esgd%E5%88%B0adamw"><span class="nav-number">1.1.</span> <span class="nav-text">从SGD到AdamW</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#adamw%E7%9A%84%E6%98%BE%E5%AD%98%E9%9C%80%E6%B1%82"><span class="nav-number">1.2.</span> <span class="nav-text">AdamW的显存需求</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%B2%BE%E5%BA%A6"><span class="nav-number">2.</span> <span class="nav-text">精度</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A5fp32%E4%B8%BA%E4%BE%8B%E8%AF%B4%E6%98%8E"><span class="nav-number">2.1.</span> <span class="nav-text">以FP32为例说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E5%B8%B8%E7%94%A8%E7%B2%BE%E5%BA%A6"><span class="nav-number">2.2.</span> <span class="nav-text">其他常用精度</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83"><span class="nav-number">3.</span> <span class="nav-text">混合精度训练</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%98%BE%E5%AD%98"><span class="nav-number">3.1.</span> <span class="nav-text">显存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">3.2.</span> <span class="nav-text">训练</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#data-parallel"><span class="nav-number">4.</span> <span class="nav-text">Data Parallel</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#ring-allreduce%E7%AE%97%E6%B3%95"><span class="nav-number">4.1.</span> <span class="nav-text">Ring AllReduce算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ring-allreduce%E7%89%B9%E7%82%B9"><span class="nav-number">4.2.</span> <span class="nav-text">Ring AllReduce特点</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#zero"><span class="nav-number">5.</span> <span class="nav-text">ZeRO</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#zero-1"><span class="nav-number">5.1.</span> <span class="nav-text">ZeRO-1</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#zero-2-zero-3"><span class="nav-number">5.2.</span> <span class="nav-text">ZeRO-2 &amp; ZeRO-3</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#zero-offload"><span class="nav-number">5.3.</span> <span class="nav-text">ZeRO-Offload</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reference"><span class="nav-number">6.</span> <span class="nav-text">Reference</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Lin"
      src="/images/avatar/Picasso_Elephant.png">
  <p class="site-author-name" itemprop="name">Lin</p>
  <div class="site-description" itemprop="description">AI | NLP</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">81</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">77</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="mailto:331603034@qq.com" title="E-Mail → mailto:331603034@qq.com" rel="noopener me" target="_blank"><i class="fa-regular fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

<!--
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/1.js?i=5acfv0hqzp5&amp;s=220&amp;m=1&amp;v=false&amp;r=false&amp;b=000000&amp;n=false&amp;c=ff0000" async="async"></script>
-->

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://saicat.github.io/fe0adaa5.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar/Picasso_Elephant.png">
      <meta itemprop="name" content="Lin">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Linsight">
      <meta itemprop="description" content="AI | NLP">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="LLM训练框架：从优化器和精度讲到Zero | Linsight">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          LLM训练框架：从优化器和精度讲到Zero
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-05-17 16:33:15" itemprop="dateCreated datePublished" datetime="2025-05-17T16:33:15+08:00">2025-05-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-05-18 19:16:10" itemprop="dateModified" datetime="2025-05-18T19:16:10+08:00">2025-05-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CS/" itemprop="url" rel="index"><span itemprop="name">CS</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CS/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CS/NLP/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>12k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>22 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>【本文已在同名 微信公众号 / 知乎 / <a target="_blank" rel="noopener" href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>这篇文章主要从一个搞数据和训练策略的LLM算法工程师角度总结一下之前用到的训练框架相关知识，包括优化器、精度和混合精度训练和DP、ZeRO的相关内容。</p>
<h1 id="optimizer">Optimizer</h1>
<p>现在模型的优化器就是AdamW。虽然这几年也试过Lion和Muon等一些新兴的optimizer，不过实践中最稳当的暂时还是AdamW。</p>
<h2 id="从sgd到adamw">从SGD到AdamW</h2>
<p>先复习下从SGD到AdamW这些个优化器。</p>
<ol type="1">
<li>SGD</li>
</ol>
<p>SGD的更新公式：</p>
<p><span class="math display">\[
θ_{t+1} = θ_t - η \cdot g_t
\]</span></p>
<ul>
<li><span class="math inline">\(θ_t\)</span>：模型参数<br>
</li>
<li><span class="math inline">\(η\)</span>：learning rate<br>
</li>
<li><span class="math inline">\(g_t\)</span>：当前梯度（<span class="math inline">\(\nabla_θ L(θ_t)\)</span>）</li>
</ul>
<p>SGD只依赖当前最新计算出的梯度，直接更新模型的参数值。</p>
<ol start="2" type="1">
<li>Momentum SGD</li>
</ol>
<p>公式：</p>
<p><span class="math display">\[
\begin{aligned}
v_t &amp;= γ \cdot v_{t-1} + g_t \\
θ_{t+1} &amp;= θ_t - η \cdot v_t
\end{aligned}
\]</span></p>
<ul>
<li><span class="math inline">\(v_t\)</span>：包含动量项的梯度（加权移动平均的累积梯度）<br>
</li>
<li><span class="math inline">\(γ\)</span>：动量系数/加权系数（比如0.9，越大表示梯度更新越慢，设为0就等于SGD了）</li>
</ul>
<p>模型在训练初期，轮次之间的梯度变化比较大，梯度甚至可能发生180°大调头的情况，导致震荡，所以SGD不容易收敛。Momentum
SGD通过累积历史的梯度值，减少震荡，从而稳定训练，加速收敛。</p>
<ol start="3" type="1">
<li>AdaGrad</li>
</ol>
<p>AdaGrad尝试让不同的参数有自己的学习率，并且可以自适应调整。</p>
<p>公式：</p>
<p><span class="math display">\[
\begin{aligned}
G_t &amp;= G_{t-1} + g_t^2 \\
θ_{t+1} &amp;= θ_t - \frac{η}{\sqrt{G_t} + ϵ} g_t
\end{aligned}
\]</span></p>
<ul>
<li><span class="math inline">\(G_t\)</span>：梯度平方的累积值<br>
</li>
<li><span class="math inline">\(ϵ\)</span>：防止除零（如1e-8）</li>
</ul>
<p>如果一个参数的更新速度比较快，那么对应的G就会比较大，那么相应的学习率也会减小；反之则学习率会相对较大。</p>
<ol start="4" type="1">
<li>RMSProp</li>
</ol>
<p>AdaGrad中因为会累积所有历史梯度平方值，这样到后期每个参数的学习率都衰减到比较小，如果训练的step比较多，到后面就效率太低了。</p>
<p>RMSProp比AdaGrad多使用一个加权移动平均。</p>
<p>公式：</p>
<p><span class="math display">\[
\begin{aligned}
v_t &amp;= β \cdot v_{t-1} + (1-β) \cdot g_t^2 \\
θ_{t+1} &amp;= θ_t - \frac{η}{\sqrt{v_t + ϵ}} g_t
\end{aligned}
\]</span></p>
<ul>
<li><span class="math inline">\(v_t\)</span>：梯度平方的指数移动平均<br>
</li>
<li><span class="math inline">\(β\)</span>：加权移动平均衰减率（通常0.9）</li>
</ul>
<ol start="5" type="1">
<li>Adam</li>
</ol>
<p>把RMSProp和Momentum SGD的改进结合起来就是Adam了。</p>
<p>Adam = Adaptive + Momentum。</p>
<p>公式：</p>
<p><span class="math display">\[
\begin{aligned}
m_t &amp;= β_1 \cdot m_{t-1} + (1-β_1) \cdot g_t \\
v_t &amp;= β_2 \cdot v_{t-1} + (1-β_2) \cdot g_t^2 \\
\hat{m}_t &amp;= \frac{m_t}{1-β_1^t}, \quad \hat{v}_t =
\frac{v_t}{1-β_2^t} \\
θ_{t+1} &amp;= θ_t - \frac{η}{\sqrt{\hat{v}_t} + ϵ} \hat{m}_t
\end{aligned}
\]</span></p>
<ul>
<li><span class="math inline">\(m_t\)</span>：一阶动量，控制当前梯度的方向，初始状态m=0<br>
</li>
<li><span class="math inline">\(v_t\)</span>：二阶动量，控制当前梯度的大小，初始状态v=0<br>
</li>
<li><span class="math inline">\(β_1,β_2\)</span>：衰减率（通常<span class="math inline">\(β_1=0.9,β_2=0.999\)</span>）</li>
</ul>
<p>本来更新的时候直接使用</p>
<p><span class="math display">\[
θ_{t+1} = θ_t - \frac{η}{\sqrt{v_t} + ϵ} m_t
\]</span></p>
<p>就可以了，为什么还要给v和m做一个缩放修正呢？因为在一开始的时候，历史的动量值都是0，这样导致训练初期更新的梯度太小，因此在前期给缩放一下，基本上跑个几百几千步，这个这个缩放基本就趋近于1了。</p>
<ol start="6" type="1">
<li>AdamW</li>
</ol>
<p>为了防止过拟合，提高泛化性，模型训练的时候可以加入L2 norm。一般来说L2
norm是直接加在训练loss上的。</p>
<p>L2 norm项是这样的：</p>
<p><span class="math display">\[\frac{\lambda}{2}
\|\theta\|^2\]</span></p>
<p>直接加到训练loss上：</p>
<p><span class="math display">\[L&#39;(\theta) = L(\theta) +
\frac{\lambda}{2} \|\theta\|^2\]</span></p>
<p>那么梯度就有：</p>
<p><span class="math display">\[g_t=\nabla_\theta
L(\theta_{t-1})+\lambda\theta_{t-1}\]</span></p>
<p>Adam在对梯度进行缩放的时候，L2
norm的衰减项也会被缩放，因此就达不到本来想要的效果了。</p>
<p>所以就有了AdamW的改进，让L2 norm的正则化能力可以正常实现。</p>
<p>AdamW不把L2 norm加到loss项中，而是直接把对应梯度加到参数更新中。</p>
<p>公式：</p>
<p><span class="math display">\[
θ_{t+1} = θ_t - \frac{η}{\sqrt{\hat{v}_t} + ϵ} \odot \hat{m}_t - λ \cdot
θ_t
\]</span></p>
<ul>
<li><span class="math inline">\(λ\)</span>：L2 norm的权重衰减系数</li>
</ul>
<h2 id="adamw的显存需求">AdamW的显存需求</h2>
<p>AdamW训练的时候，除了模型参数，还需要维护一阶动量和二阶动量。</p>
<p>在全部使用fp32的情况下，假设模型的总参数量为<span class="math inline">\(\Phi\)</span>，那么模型本身参数所需的显存就是<span class="math inline">\(4\Phi\)</span>。</p>
<p>而AdamW维护的一阶动量和二阶动量的参数则是<span class="math inline">\(4\Phi+4\Phi=8\Phi\)</span>。</p>
<p>此外还有梯度值，每个模型参数有一个梯度，那么梯度所需的量也是<span class="math inline">\(4\Phi\)</span>。</p>
<p>那么模型参数 + 优化器参数 + 梯度总共就是<span class="math inline">\(16\Phi\)</span>的显存需求。</p>
<p>最后还有中间激活值，激活值的量和模型结构有关，对于transformer也和输入长度有关，再加上现在还有gradient
checkpoint等做法，所以激活值就得具体情况具体分析了。</p>
<h1 id="精度">精度</h1>
<p>说到LLM训练，就离不开训练精度的事。</p>
<h2 id="以fp32为例说明">以FP32为例说明</h2>
<p>先从FP32说起。FP32的二进制结构分为三部分：</p>
<ul>
<li>符号位（S，Sign）：1位，0表示正数，1表示负数。<br>
</li>
<li>指数位（E，Exponent）：8位，存储偏移后的指数值（为了能够表达正值和负值，加上了127的偏移量，实际指数为E
- 127）。<br>
</li>
<li>尾数位（M，Mantissa）：23位，存储规范化后的二进制小数部分（隐含前导1.）。</li>
</ul>
<p>十进制和FP32转换公式：</p>
<p><span class="math display">\[
(-1)^S \times 1.M \times 2^{E-127}
\]</span></p>
<p>举几个例子看看二进制和十进制的转换。</p>
<p>示例1. 十进制 → FP32（以9.625为例）</p>
<p>step 1：十进制转二进制</p>
<ul>
<li><strong>整数部分</strong>：<code>9</code> →
<code>1001</code>（二进制）。<br>
</li>
<li><strong>小数部分</strong>：<code>0.625</code> → 连续乘2取整：
<ul>
<li><code>0.625 × 2 = 1.25</code> →
取<code>1</code>，剩余<code>0.25</code><br>
</li>
<li><code>0.25 × 2 = 0.5</code> →
取<code>0</code>，剩余<code>0.5</code><br>
</li>
<li><code>0.5 × 2 = 1.0</code> →
取<code>1</code>，剩余<code>0</code><br>
</li>
<li>结果：<code>0.101</code>（二进制）。<br>
</li>
</ul></li>
<li><strong>合并</strong>：<code>9.625</code> →
<code>1001.101</code>。</li>
</ul>
<p>step 2：规范化科学计数法</p>
<ul>
<li><code>1001.101</code> →
<code>1.001101 × 2^3</code>（左移3位）。对于二进制来说，整数位一定是1。</li>
</ul>
<p>step 3：填充FP32三部分</p>
<ul>
<li><strong>符号位</strong>：<code>0</code>（正数）。<br>
</li>
<li><strong>指数位</strong>：<code>3 + 127 = 130</code> →
<code>10000010</code>（二进制）。<br>
</li>
<li><strong>尾数位</strong>：<code>001101</code> + 补零至23位 →
<code>00110100000000000000000</code>。</li>
</ul>
<p>最终9.625的FP32表示：</p>
<p><code>0 10000010 00110100000000000000000</code></p>
<p>（验证工具：<a target="_blank" rel="noopener" href="https://www.h-schmidt.net/FloatConverter/IEEE754.html">IEEE-754
Converter,
https://www.h-schmidt.net/FloatConverter/IEEE754.html</a>）</p>
<p>示例2：FP32 → 十进制（反向解析出十进制）</p>
<p>二进制为：<code>0 10000010 00110100000000000000000</code></p>
<ul>
<li><strong>符号位</strong>：<code>0</code> → 正数。<br>
</li>
<li><strong>指数位</strong>：<code>10000010</code> →
十进制<code>130</code> → 实际指数<code>130 - 127 = 3</code>。<br>
</li>
<li><strong>尾数位</strong>：<code>001101...</code> →
隐含<code>1.</code> → <code>1.001101</code>（二进制）。</li>
</ul>
<p>计算十进制值：</p>
<p>step 1：将<code>1.001101</code>转为十进制：<br>
- <code>1.001101</code> = <span class="math inline">\(1 + 0 \times
2^{-1} + 0 \times 2^{-2} + 1 \times 2^{-3} + 1 \times 2^{-4} + 0 \times
2^{-5} + 1 \times 2^{-6}\)</span> = <span class="math inline">\(1 +
0.125 + 0.0625 + 0.015625\)</span> ≈ <code>1.203125</code>。<br>
step 2：乘以指数部分：<code>1.203125 × 2^3 = 9.625</code>。</p>
<ul>
<li><p><strong>特殊值处理</strong>：</p>
<ul>
<li>指数全<code>0</code>且尾数非零：非规格化数（极小值）。<br>
</li>
<li>指数全<code>1</code>且尾数全<code>0</code>：表示无穷大（<code>±∞</code>）。<br>
</li>
<li>指数全<code>1</code>且尾数非零：<code>NaN</code>（非数字）。</li>
</ul></li>
<li><p><strong>精度限制</strong>：某些十进制数（如<code>0.3</code>）无法精确表示为FP32，会存在舍入误差。</p></li>
</ul>
<h2 id="其他常用精度">其他常用精度</h2>
<p>目前LLM训练的除了FP32，还有FP16、BF16，以及更新的FP8。</p>
<p>这几个的对比：</p>
<table style="width:100%;">
<colgroup>
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th>格式</th>
<th>符号位</th>
<th>指数位</th>
<th>尾数位</th>
<th>总位数</th>
<th>数值范围(近似)</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>FP32</td>
<td>1</td>
<td>8</td>
<td>23</td>
<td>32</td>
<td>±1.2×10⁻³⁸ ~ ±3.4×10³⁸</td>
<td>高精度（约7位有效数字），通用计算标准，适合训练但资源消耗大。</td>
</tr>
<tr class="even">
<td>FP16</td>
<td>1</td>
<td>5</td>
<td>10</td>
<td>16</td>
<td>±6.1×10⁻⁵ ~ ±6.6×10⁴</td>
<td>内存占用减半，速度快但易溢出/下溢，需混合精度训练。</td>
</tr>
<tr class="odd">
<td>BF16</td>
<td>1</td>
<td>8</td>
<td>7</td>
<td>16</td>
<td>±1.2×10⁻³⁸ ~ ±3.4×10³⁸</td>
<td>指数范围同FP32，训练稳定但精度低（约2位有效数字），适合大模型。</td>
</tr>
<tr class="even">
<td>FP8(E4M3)</td>
<td>1</td>
<td>4</td>
<td>3</td>
<td>8</td>
<td>±1.56×10⁻⁵ ~ ±448</td>
<td>内存占用极低，适合推理；E4M3侧重精度，范围较小。</td>
</tr>
<tr class="odd">
<td>FP8(E5M2)</td>
<td>1</td>
<td>5</td>
<td>2</td>
<td>8</td>
<td>±3.9×10⁻⁸ ~ ±57344</td>
<td>E5M2侧重范围，精度更低，适合大动态范围计算。</td>
</tr>
</tbody>
</table>
<p>目前最常用的还是FP16和BF16（FP8我自己还没怎么用，先挖个坑，以后用熟了FP8再来填）。这俩的对比：</p>
<ul>
<li><strong>指数位</strong>：BF16与FP32相同（8位），FP16仅5位，因此表示范围小，更易溢出；<br>
</li>
<li><strong>尾数位</strong>：FP32（23位）&gt; FP16（10位）&gt;
BF16（7位）&gt; FP8（3/2位），精度依次降低。<br>
</li>
<li><strong>应用场景</strong>：FP32用于高精度训练，FP16/BF16都可以用于混合精度训练，FP8用于端侧设备推理。</li>
</ul>
<p>直观上，BF16的精度大概是在0.01到0.001之间，而BF16的精度是在0.001到0.0001之间。也就是说，如果一次梯度更新小于这个值，那么参数很可能没法正确地变化。</p>
<h1 id="混合精度训练">混合精度训练</h1>
<p>混合精度训练时减少显存使用，提升训练速度的方法。</p>
<p>为什么用混合精度训练，不直接使用低精度的格式进行训练？从前面精度的表格可以看到，无论是FP16还是BF16，要么在精度上有损失，要么在表达范围上有限制，因此直接用低精度格式训练，可能会在需要高精度或者大范围的部分导致不稳定。因此混合精度方案在大部分计算使用半精度的同时，用FP32对关键部分进行备份，在速度、显存和稳定性间取得平衡。</p>
<h2 id="显存">显存</h2>
<p>AdamW的单精度和半精度的混合精度训练如下（图上是FP16，也可以换成BF16）：</p>
<img src="/fe0adaa5/mix_precision_fp16.png" class title="混合精度训练">
<p>输入是FP16，前向计算激活值是FP16，loss值是FP32的，反向计算的值和梯度是FP16，AdamW的一阶和二阶动量是FP32，而AdamW更新模型参数权重用的是FP32，而在进行前后向计算的时候，模型参数用的是FP16的版本。</p>
<p>算一下显存：</p>
<p>模型参数：一份单精度一份半精度，总共就是<span class="math inline">\(2\Phi+4\Phi=6\Phi\)</span>。</p>
<p>优化器参数：每个参数有单精度的一阶动量和二阶动量，总共就是<span class="math inline">\(8\Phi\)</span>。</p>
<p>梯度：每个参数有半精度的梯度，<span class="math inline">\(2\Phi\)</span>。</p>
<p>那么总共就是<span class="math inline">\(16\Phi\)</span>的显存消耗。</p>
<p>从模型参数+优化器参数+梯度的显存消耗上看，单精度训练和混合精度（FP32
+ FP16/BF16）的显存消耗量是一样。但是，混合精度在效率上的收益有：</p>
<ul>
<li>有硬件支持下，半精度的计算更快，因此整体的计算速度更快。<br>
</li>
<li>激活值所需的显存减少一半，从而可以使用更大的batch。<br>
</li>
<li>一些原来单卡放不下的，现在能放下了，不用做张量并行或者流水并行。</li>
</ul>
<h2 id="训练">训练</h2>
<p>前面说到直接用半精度进行训练会有问题，那么混合精度训练具体是怎么解决这些问题的。</p>
<p>首先，半精度的精度不足，因此混合精度中，AdamW维护了一份FP32的模型权重，这个是真正用于更新模型的数据，这样可以保持较小的梯度更新也不会被舍弃。每次更新完之后，再把获得的FP32参数转成FP16，用于前后向计算。</p>
<p>另外，由于半精度值的精度问题，较小的梯度值可能直接变成0了，这样就导致没法训练参数了。那么一个解决方法就是像上面的图中那样，给loss值做一个scaling，变大一些，尽量远离太小的值。由于loss值变大，会导致梯度值也变大相应倍数，因此在更新完模型参数值之后，要做一个逆scaling，把值变回去。</p>
<img src="/fe0adaa5/loss_scaling.png" class title="混合精度训练">
<p>另外，还有一招：使用FP16进行乘法和存储，只使用FP32进行加法操作，避免累加误差。因为加法的误差会一直累积，因此用单精度计算。</p>
<h1 id="data-parallel">Data Parallel</h1>
<p>模型大，数据多，难免就需要分布式计算。其中，最常用的就是数据并行。其实我们训练百亿以下的模型，基本上都是只用数据并行。</p>
<p>使用最朴素的数据并行，每个GPU会维护一套完整的模型参数 + 优化器参数 +
梯度。每次更新，每个GPU用不同的数据「单独」进行训练，获取梯度，然后所有GPU会同步各自获得的梯度，计算个平均值，然后更新参数。每轮更新过后，模型参数会统一，而优化器状态则每个GPU有各自的版本（因此保存训练checkpoint的时候会有大量的优化器状态值）。</p>
<p>可以看到，每次更新时，各个GPU需要同步梯度，这就涉及到大量的卡间通讯，甚至节点间通讯。比如128卡训练模型，那么naive的数据同步方式就是两两之间都要进行数据传输和接收；那么训练一个14B模型，在用半精度的梯度的情况下，每张卡要发送127
* 28G = 3556G数据，同时要接收127 * 28G =
3556G的数据，而且随着集群的变大，这个数值还会增大。就算是A100，卡间带宽也只有2TB/s，那同步一次就是的1s多，这期间所有卡都得停下计算，等通讯完成。多节点之间的带宽更小，那GPU的利用率就更低了。</p>
<p>这也太低效了，因此实际上就不是这样同步数据的，而是用到了Ring
AllReduce的梯度同步算法。</p>
<h2 id="ring-allreduce算法">Ring AllReduce算法</h2>
<p>顾名思义，Ring
AllReduce把各个GPU组成一个ring，以ring的形式进行通讯，以减少通讯量。</p>
<img src="/fe0adaa5/ring.jpg" class title="混合精度训练">
<p>allreduce同步梯度数据的过程主要包含reduce-scatter和all-gather两个操作。</p>
<ol type="1">
<li>reduce-scatter</li>
</ol>
<p>假设一共有5个GPU，要同步梯度。那么把梯度数据均匀划分成A、B、C、D、E五块。</p>
<p>初始状态下，每个GPU持有的数据如下：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPU0</td>
<td>a0</td>
<td>b0</td>
<td>c0</td>
<td>d0</td>
<td>e0</td>
</tr>
<tr class="even">
<td>GPU1</td>
<td>a1</td>
<td>b1</td>
<td>c1</td>
<td>d1</td>
<td>e1</td>
</tr>
<tr class="odd">
<td>GPU2</td>
<td>a2</td>
<td>b2</td>
<td>c2</td>
<td>d2</td>
<td>e2</td>
</tr>
<tr class="even">
<td>GPU3</td>
<td>a3</td>
<td>b3</td>
<td>c3</td>
<td>d3</td>
<td>e3</td>
</tr>
<tr class="odd">
<td>GPU4</td>
<td>a4</td>
<td>b4</td>
<td>c4</td>
<td>d4</td>
<td>e4</td>
</tr>
</tbody>
</table>
<p>reduce-scatter的操作，每个GPU会发送自己持有的A、B、C、D、E中的其中一块数据，同时接收和自己发送的数据不同块的一块数据。</p>
<p>比如在这个例子中，GPU0发送a0，并接收e4，GPU1发送b1，并接收a0，以此类推。</p>
<p>第一次reduce-scatter操作之后：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPU0</td>
<td>a0</td>
<td>b0</td>
<td>c0</td>
<td>d0</td>
<td>e4+e0</td>
</tr>
<tr class="even">
<td>GPU1</td>
<td>a0+a1</td>
<td>b1</td>
<td>c1</td>
<td>d1</td>
<td>e1</td>
</tr>
<tr class="odd">
<td>GPU2</td>
<td>a2</td>
<td>b1+b2</td>
<td>c2</td>
<td>d2</td>
<td>e2</td>
</tr>
<tr class="even">
<td>GPU3</td>
<td>a3</td>
<td>b3</td>
<td>c2+c3</td>
<td>d3</td>
<td>e3</td>
</tr>
<tr class="odd">
<td>GPU4</td>
<td>a4</td>
<td>b4</td>
<td>c4</td>
<td>d3+d4</td>
<td>e4</td>
</tr>
</tbody>
</table>
<p>第二次reduce-scatter操作之后：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPU0</td>
<td>a0</td>
<td>b0</td>
<td>c0</td>
<td>d3+d4+d0</td>
<td>e4+e0</td>
</tr>
<tr class="even">
<td>GPU1</td>
<td>a0+a1</td>
<td>b1</td>
<td>c1</td>
<td>d1</td>
<td>e4+e0+e1</td>
</tr>
<tr class="odd">
<td>GPU2</td>
<td>a0+a1+a2</td>
<td>b1+b2</td>
<td>c2</td>
<td>d2</td>
<td>e2</td>
</tr>
<tr class="even">
<td>GPU3</td>
<td>a3</td>
<td>b1+b2+b3</td>
<td>c2+c3</td>
<td>d3</td>
<td>e3</td>
</tr>
<tr class="odd">
<td>GPU4</td>
<td>a4</td>
<td>b4</td>
<td>c2+c3+c4</td>
<td>d3+d4</td>
<td>e4</td>
</tr>
</tbody>
</table>
<p>第三次reduce-scatter操作之后：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPU0</td>
<td>a0</td>
<td>b0</td>
<td>c2+c3+c4+c0</td>
<td>d3+d4+d0</td>
<td>e4+e0</td>
</tr>
<tr class="even">
<td>GPU1</td>
<td>a0+a1</td>
<td>b1</td>
<td>c1</td>
<td>d3+d4+d0+d1</td>
<td>e4+e0+e1</td>
</tr>
<tr class="odd">
<td>GPU2</td>
<td>a0+a1+a2</td>
<td>b1+b2</td>
<td>c2</td>
<td>d2</td>
<td>e4+e0+e1+e2</td>
</tr>
<tr class="even">
<td>GPU3</td>
<td>a0+a1+a2+a3</td>
<td>b1+b2+b3</td>
<td>c2+c3</td>
<td>d3</td>
<td>e3</td>
</tr>
<tr class="odd">
<td>GPU4</td>
<td>a4</td>
<td>b1+b2+b3+b4</td>
<td>c2+c3+c4</td>
<td>d3+d4</td>
<td>e4</td>
</tr>
</tbody>
</table>
<p>第四次reduce-scatter操作之后：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPU0</td>
<td>a0</td>
<td>b1+b2+b3+b4+b0</td>
<td>c2+c3+c4+c0</td>
<td>d3+d4+d0</td>
<td>e4+e0</td>
</tr>
<tr class="even">
<td>GPU1</td>
<td>a0+a1</td>
<td>b1</td>
<td>c2+c3+c4+c0+c1</td>
<td>d3+d4+d0+d1</td>
<td>e4+e0+e1</td>
</tr>
<tr class="odd">
<td>GPU2</td>
<td>a0+a1+a2</td>
<td>b1+b2</td>
<td>c2</td>
<td>d3+d4+d0+d1+d2</td>
<td>e4+e0+e1+e2</td>
</tr>
<tr class="even">
<td>GPU3</td>
<td>a0+a1+a2+a3</td>
<td>b1+b2+b3</td>
<td>c2+c3</td>
<td>d3</td>
<td>e4+e0+e1+e2+e3</td>
</tr>
<tr class="odd">
<td>GPU4</td>
<td>a0+a1+a2+a3+a4</td>
<td>b1+b2+b3+b4</td>
<td>c2+c3+c4</td>
<td>d3+d4</td>
<td>e4</td>
</tr>
</tbody>
</table>
<p>假设共有N个GPU，经过N-1次操作之后，每个GPU上，都有1/N块数据是同步了所有GPU数据的。</p>
<p>在这个例子中，GPU0的B块是包含了完整的5个GPU的数据的，而GPU1则是C块是完整的，以此类推。</p>
<p>接下来，就需要用all-gather把每个GPU上这份同步了所有GPU数据的块传播给其他GPU。</p>
<ol start="2" type="1">
<li>all-gather</li>
</ol>
<p>其实all-gather和reduce-scatter的操作是很类似的，只不过reduce-scatter是相加/取平均，而all-gather是直接覆盖数据。</p>
<p>all-gather第一次操作后：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPU0</td>
<td>a0+a1+a2+a3+a4</td>
<td>b1+b2+b3+b4+b0</td>
<td>c2+c3+c4+c0</td>
<td>d3+d4+d0</td>
<td>e4+e0</td>
</tr>
<tr class="even">
<td>GPU1</td>
<td>a0+a1</td>
<td>b1+b2+b3+b4+b0</td>
<td>c2+c3+c4+c0+c1</td>
<td>d3+d4+d0+d1</td>
<td>e4+e0+e1</td>
</tr>
<tr class="odd">
<td>GPU2</td>
<td>a0+a1+a2</td>
<td>b1+b2</td>
<td>c2+c3+c4+c0+c1</td>
<td>d3+d4+d0+d1+d2</td>
<td>e4+e0+e1+e2</td>
</tr>
<tr class="even">
<td>GPU3</td>
<td>a0+a1+a2+a3</td>
<td>b1+b2+b3</td>
<td>c2+c3</td>
<td>d3+d4+d0+d1+d2</td>
<td>e4+e0+e1+e2+e3</td>
</tr>
<tr class="odd">
<td>GPU4</td>
<td>a0+a1+a2+a3+a4</td>
<td>b1+b2+b3+b4</td>
<td>c2+c3+c4</td>
<td>d3+d4</td>
<td>e4+e0+e1+e2+e3</td>
</tr>
</tbody>
</table>
<p>...</p>
<p>以此类推，最后得到</p>
<table style="width:100%;">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPU0</td>
<td>a0+a1+a2+a3+a4</td>
<td>b1+b2+b3+b4+b0</td>
<td>c2+c3+c4+c0+c1</td>
<td>d3+d4+d0+d1+d2</td>
<td>e4+e0+e1+e2+e3</td>
</tr>
<tr class="even">
<td>GPU1</td>
<td>a0+a1+a2+a3+a4</td>
<td>b1+b2+b3+b4+b0</td>
<td>c2+c3+c4+c0+c1</td>
<td>d3+d4+d0+d1+d2</td>
<td>e4+e0+e1+e2+e3</td>
</tr>
<tr class="odd">
<td>GPU2</td>
<td>a0+a1+a2+a3+a4</td>
<td>b1+b2+b3+b4+b0</td>
<td>c2+c3+c4+c0+c1</td>
<td>d3+d4+d0+d1+d2</td>
<td>e4+e0+e1+e2+e3</td>
</tr>
<tr class="even">
<td>GPU3</td>
<td>a0+a1+a2+a3+a4</td>
<td>b1+b2+b3+b4+b0</td>
<td>c2+c3+c4+c0+c1</td>
<td>d3+d4+d0+d1+d2</td>
<td>e4+e0+e1+e2+e3</td>
</tr>
<tr class="odd">
<td>GPU4</td>
<td>a0+a1+a2+a3+a4</td>
<td>b1+b2+b3+b4+b0</td>
<td>c2+c3+c4+c0+c1</td>
<td>d3+d4+d0+d1+d2</td>
<td>e4+e0+e1+e2+e3</td>
</tr>
</tbody>
</table>
<h2 id="ring-allreduce特点">Ring AllReduce特点</h2>
<p>Ring
AllReduce理论上已经是同步算法的最佳，它的特点是随着GPU数量的增多，整个过程所需的时间几乎保持不变，也就是通讯时间成本和机器数量无关！</p>
<p>这么一来，在使用更大集群的时候，节点间的通讯就不会成为提升线性扩展比的瓶颈。比如你原来128卡要训一天，那几乎可以认为256卡训半天就能达到相同的程度。</p>
<p>当然理论是理论，实际上当设备数非常大，还会有另外的问题。</p>
<p>OneFlow的这篇文章介绍得很清楚，可以一读：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/504957661">手把手推导Ring
All-reduce的数学性质(https://zhuanlan.zhihu.com/p/504957661)</a>。</p>
<h1 id="zero">ZeRO</h1>
<p>那么除了上面的混合精度方案，Ring
AllReduce的DP之外，还有没有什么方法能简单快捷减少显存，提升训练效率？兄弟，有的，而且很强，那就是ZeRO。</p>
<p>ZeRO = Zero Redundancy Optimizer</p>
<p>ZeRO核心是优化显存，减少训练所需的显存占用。</p>
<p>ZeRO有三个stage，ZeRO-1，ZeRO-2，ZeRO-3，对显存的优化逐步变强（但是代价也逐步增加）。</p>
<h2 id="zero-1">ZeRO-1</h2>
<p>原来呢，在FP32 + FP16的混合精度训练下，对于包含<span class="math inline">\(\Phi\)</span>个参数的模型，每个GPU都存有一份完整的模型参数、梯度和优化器状态：</p>
<ul>
<li>模型参数（FP16 + FP32）：<span class="math inline">\(6\Phi\)</span>（byte）<br>
</li>
<li>梯度（FP16）：<span class="math inline">\(2\Phi\)</span><br>
</li>
<li>优化器状态（FP32）：<span class="math inline">\(8\Phi\)</span></li>
</ul>
<p>每次同步完梯度之后，各个GPU会各自更新优化器状态。</p>
<p>这里面其实就有巨大的显存冗余，因为每个GPU都有一份一样的优化器状态，而AdamW的优化器状态又占了很大一部分显存（比如7B的模型就有28G的优化器状态）。</p>
<p>那ZeRO-1就想办法消除这个优化器状态的冗余。核心思想就是：</p>
<ul>
<li>partition：有N个GPU，就把优化器状态切分成N份，每个GPU在整个训练过程中，只保存和管理其中的一份。<br>
</li>
<li>distributed update：每个GPU只负责更新其所持有的那部分优化器状态
对应的 模型参数。</li>
</ul>
<p>有开ZeRO-1和没有开ZeRO-1，在流程上差别就在于梯度同步之后的操作。开ZeRO-1的情况下，同步梯度之后，由于每个GPU只有1/N的优化器状态，因此只能更新对应的1/N的模型参数。更新完1/N的参数之后，为了能在下次迭代时保持各个GPU上模型参数的一致性，就还要做一次all-gather来同步模型的参数。</p>
<p>显存上，每个GPU只需维护<span class="math inline">\(8\Phi/N\)</span>的优化器状态，而且GPU数越多N越大，那么每个GPU所需的显存就越少。这简直就是训练框架界的PDD：用得越多省得越多！有可能你一个模型本来8卡会CUDA
OOM，那在开ZeRO的情况下，可能多加点卡，比如32卡，就不会OOM了。</p>
<p>而在通讯量上，多了一次模型参数的all-gather，所以理论上是<span class="math inline">\(2\Phi+\Phi=3\Phi\)</span>个参数。但是这里还有个优化点，可以把实际的通讯量降到<span class="math inline">\(2\Phi\)</span>个参数。</p>
<p>回想一下，同步梯度的时候，分为reduce-scatter和all-gather两步，每步的通讯量都是<span class="math inline">\(\Phi\)</span>个参数。reduce-scatter让每个GPU拥有1/N块完整的梯度，all-gather把每个GPU拥有的这块完整梯度同步给其他所有GPU。但是在ZeRO-1的情况下，就可以不做梯度的all-gather。因为ZeRO-1的情况下，每个GPU只有1/N的优化器状态，也只会更新1/N的模型参数，同步整个模型的梯度没有意义。因此这个我们只需要对梯度做reduce-scatter，让每个GPU拥有需要更新的参数的那部分完整梯度就可以了！等每个GPU都更新完自己的那部分模型参数之后，再对模型参数做all-gather就可以了。这么一来ZeRO-1的通讯量完全没有增加，但是显存消耗量却减少了，这完全是免费午餐。</p>
<h2 id="zero-2-zero-3">ZeRO-2 &amp; ZeRO-3</h2>
<p>ZeRO-2在ZeRO-1的基础上，对梯度也进行了切分。每个GPU只有模型参数是完整的，而梯度和优化器状态都只会储存和管理1/N的小块，而不是完整的一份。ZeRO-2和ZeRO-1的通讯量一样，都是<span class="math inline">\(2\Phi\)</span>个参数：一次梯度的reduce-scatter，一次新参数的all-gather。</p>
<p>而ZeRO-3更进一步，每个GPU上连模型参数都是不完整的。forward计算的时候，要先做一次all-gather，计算完就把不属于自己的模型参数都释放掉。同样地，backward的时候也是类似操作。之后就是和ZeRO-1/2一样，同步新参数。因此总通讯量是<span class="math inline">\(3\Phi\)</span>个参数。ZeRO-3的通讯量会增大，而显存的训练则大大减小，颇有点时间换空间的意味。</p>
<p>DDP，ZeRO-1，ZeRO-2和ZeRO-3在7.5B参数的模型和64卡集群下，显存的消耗对比（没有包含激活值）：</p>
<img src="/fe0adaa5/zero.png" class title="混合精度训练">
<p>另外，模型计算过程中的激活值也是可以切割和分块维护的，这块就比较复杂了，要根据实际情况灵活设计要保存的activation和要切分的块。</p>
<h2 id="zero-offload">ZeRO-Offload</h2>
<p>ZeRO-Offload另辟蹊径，把放不进显存的变量放到内存上了。</p>
<p>ZeRO-Offload基于ZeRO-2的优化器状态和梯度分片策略，但进一步将这两类数据卸载（offload）到CPU内存中，同时利用CPU的计算能力执行部分低复杂度任务（如参数更新），将高计算复杂度的前向/反向传播保留在GPU，低复杂度的优化器更新卸载到CPU，避免CPU成为性能瓶颈。</p>
<hr>
<p>博客：<a target="_blank" rel="noopener" href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a target="_blank" rel="noopener" href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"> 博主微信号(添加请注明来意)：<br>
<img src="/images/wechat.png"></p>
<hr>
<p>【推荐文章】<br>
- Agent：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/b242bfb3.html">Agent完全手册(零)：三大模块，三个理念</a><br>
- MoE：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/a9c496e3.html">DeepSeek-V3细节探索</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/83c49df0.html">DeepSeek-V2和MLA</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/1d5bcd45.html">昆仑万维-SkyworkMoE</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/f3acf042.html">成本10w刀的JetMoE</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/224c42da.html">MoE的top-p
routing</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/5e1d14b3.html">对MoE模型的一些观察</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/a0824e29.html">从dense到MoE -- sparse
upcycling</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/2c8bbc7.html">MoE路由--expert choice
routing</a><br>
- 端侧模型：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/1e34e252.html">苹果智能系统模型--AFM</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/376db710.html">MiniCPM</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/5ac36d34.html">适合移动设备的语言模型--MobileLLM</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/fe13b56f.html">phi系列模型</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/cf3f1f81.html">Gemma2</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/f845f3e4.html">苹果的OpenELM</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/770b63e1.html">bilibili的index-1.9B</a><br>
- 预训练：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/37ee84bb.html">Qwen3实测&amp;技术报告</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/a0b50049.html">代码大模型(一)--业界现状</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/7856bcc1.html">代码大模型(二)--OpenCoder</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/dcb57672.html">LLM高效预训练(一)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/1e2e35a7.html">LLM高效预训练(二)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/7d7294cb.html">Llama3.1--预训练要点一览</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/a8f8b641.html">Qwen2技术报告</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/7f3d361.html">InternLM系列模型</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/a5206abd.html">GLM4报告的一些技术点</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/3df0cd42.html">从Yuan2.0到Yuan2.0-M32</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/f5fb75e4.html">从loss视角理解大模型涌现能力</a><br>
- 数据：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/85132189.html">训练数据合成(一)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/2a22baeb.html">训练数据合成(二)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/e259c7b2.html">训练数据合成(三)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/2c2cdc34.html">LLM预训练数据策略(一)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/210dbccd.html">预训练数据处理--长度分解</a><br>
- 长上下文：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/6c0f6207.html">Qwen2.5-1M技术解析</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/45ee1a6d.html">大模型推理窗口-从有限到无限大</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/4519eadd.html">prompt压缩(一)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/ea2871bf.html">prompt压缩(二)</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/bfa4f144.html">reasoning压缩(一)</a><br>
- 推理加速：<br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/f5c015c.html">大模型推理加速-投机解码</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/7bbe2df6.html">大模型推理加速-MEDUSA</a><br>
- 对齐：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/9e4b4e6d.html">深度求索DeepSeek-R1详解</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/657a6d17.html">基模型Cognitive
Behaviors对RL的影响</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/93328a2a.html">Llama3.1--post-training要点一览</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/bb8fcf21.html">模型平均 -- model
soup</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/473f2b43.html">大模型偏好对齐-DPO</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/da871ebe.html">大模型偏好对齐-ODPO</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/280fa97a.html">大模型偏好对齐-simPO</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/4fe7b810.html">大模型偏好对齐-IPO</a><br>
- Transformer：<br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/7381cae3.html">LLM的重复生成和ICL</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/f0902f1a.html">RoPE的远距离衰减</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/2dee4921.html">LLM水印</a><br>
- 项目应用：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/9c593ccd.html">一个模型支持智能助手系统</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/d253d7b3.html">关于The Bitter
Lesson</a><br>
- CV：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/a11e2633.html">CV入门--关于Vision
Transformer</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/ae81a87b.html">CV入门--无监督学习</a><br>
- 多模态：<br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/3069051d.html">多模态入门(一)--CLIP</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/569d722c.html">多模态入门(二)--Flamingo,LLaVA系列和BLIP系列</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/f16505b3.html">多模态入门(三)--MiniGPT4,DeepSeekVL,InternVL系列和QwenVL系列</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/e00debee.html">多模态入门(四)--CogVLM,VILA,MM1,MM1.5和Pixtral-12B</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/52c8a4f9.html">多模态入门(五)--InternVL系列</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/96393d3b.html">小米的移动UI多模态模型--MobileVLM</a><br>
<a target="_blank" rel="noopener" href="https://www.linsight.cn/b4d047c1.html">DeepSeek-VL2的细节</a><br>
- 大模型算法题：<br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/3345028a.html">(1)</a>、 <a target="_blank" rel="noopener" href="http://www.linsight.cn/ad0bba9d.html">(2)</a>、 <a target="_blank" rel="noopener" href="http://www.linsight.cn/1736008.html">(3)</a>、 <a target="_blank" rel="noopener" href="http://www.linsight.cn/1736008.html">(4)</a>、 <a target="_blank" rel="noopener" href="http://www.linsight.cn/336f2f3e.html">(5)</a>、 <a target="_blank" rel="noopener" href="http://www.linsight.cn/7c04944d.html">(6)</a>、 <a target="_blank" rel="noopener" href="https://www.linsight.cn/dd614e12.html">(7)</a>、 <a target="_blank" rel="noopener" href="https://www.linsight.cn/e287b9c3.html">(8)</a>、 <a target="_blank" rel="noopener" href="https://www.linsight.cn/fb9c8882.html">(9)</a></p>
<h1 id="reference">Reference</h1>
<p>【1】深度学习分布式训练框架 Horovod --- (1)
基础知识，https://www.cnblogs.com/rossiXYZ/p/14856464.html<br>
【2】手把手推导Ring
All-reduce的数学性质，https://zhuanlan.zhihu.com/p/504957661<br>
【3】大模型涉及到的精度有多少种？FP32、TF32、FP16、BF16、FP8、FP4、NF4、INT8都有什么关联，一文讲清楚，https://zhuanlan.zhihu.com/p/673708074<br>
【4】十分钟速通优化器原理，通俗易懂（从SGD到AdamW），https://zhuanlan.zhihu.com/p/686410423<br>
【5】机器学习11种优化器推导过程详解(SGD,BGD,MBGD,Momentum,NAG,Adagrad,Adadelta,RMSprop,Adam,Nadma,Adamx)，https://blog.csdn.net/yangwohenmai1/article/details/124882119<br>
【6】【LLM101n】7：流行的LLM优化算法 -
AdamW，https://zhuanlan.zhihu.com/p/7272881104<br>
【7】Huge and Efficient!
一文了解大规模预训练模型高效训练技术，https://aiorang.com/article/PqmOhWF.html<br>
【8】大模型精度（FP16，FP32，BF16）详解与实践，https://www.53ai.com/news/qianyanjishu/2024052494875.html<br>
【9】LLM
时代，如何优雅地训练大模型？，https://zhuanlan.zhihu.com/p/660394604<br>
【10】deepspeed 滴 ZERO
介绍，https://blog.csdn.net/weixin_42253689/article/details/147568376<br>
【11】图解大模型训练之：数据并行下篇( DeepSpeed
ZeRO，零冗余优化)，https://zhuanlan.zhihu.com/p/618865052<br>
【】</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Lin
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://saicat.github.io/fe0adaa5.html" title="LLM训练框架：从优化器和精度讲到Zero">https://saicat.github.io/fe0adaa5.html</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i> NLP</a>
              <a href="/tags/LLM/" rel="tag"><i class="fa fa-tag"></i> LLM</a>
              <a href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83/" rel="tag"><i class="fa fa-tag"></i> 预训练</a>
              <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" rel="tag"><i class="fa fa-tag"></i> 分布式</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/37ee84bb.html" rel="prev" title="Qwen3实测 & 技术报告">
                  <i class="fa fa-angle-left"></i> Qwen3实测 & 技术报告
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Lin</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">674k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">20:25</span>
  </span>
</div>
<div class="busuanzi-count">
</div>

<!--
-->


<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.28/fancybox/fancybox.umd.js" integrity="sha256-ytMJGN3toR+a84u7g7NuHm91VIR06Q41kMWDr2pq7Zo=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/fancybox.js"></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"Saicat/comment-utterance","issue_term":"pathname","theme":"github-light"}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
