<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon/favicon_io/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon/favicon_io/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon/favicon_io/favicon-16x16.png">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.28/fancybox/fancybox.css" integrity="sha256-6cQIC71/iBIYXFK+0RHAvwmjwWzkWd+r7v/BX3/vZDc=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"saicat.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="【本文已在同名 微信公众号 &#x2F; 知乎 &#x2F; 个人博客linsight.cn 上线】  2024年3、4月这段时间，很多MoE模型扎堆发布，包括Qwen1.5-MoE、DBRX、Jamba和Mistral等。">
<meta property="og:type" content="article">
<meta property="og:title" content="MoE模型的前世今生">
<meta property="og:url" content="https://saicat.github.io/44e38c1b.html">
<meta property="og:site_name" content="Linsight">
<meta property="og:description" content="【本文已在同名 微信公众号 &#x2F; 知乎 &#x2F; 个人博客linsight.cn 上线】  2024年3、4月这段时间，很多MoE模型扎堆发布，包括Qwen1.5-MoE、DBRX、Jamba和Mistral等。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/xiaomi_moe.jpg">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/vanilla_moe.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/rnn_moe.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/softplus.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/rnn_moe_load_function.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/rnn_moe_perf.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/rnn_moe_137b.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/rnn_moe_specilized.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/gshard_moe_family.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/gshard_model.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/gshard_algo_1.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/gshard_perf.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/switch_transformer_structure.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/switch_transformer_diff_expert_capacity.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/switch_transformer_capacity_effect.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/switch_transformer_init.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/switch_transformer_dropout.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/switch_transformer_scaling_step.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/switch_transformer_scaling_time.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/switch_transformer_scaling_dense.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/switch_transformer_sft_result.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/switch_transformer_distill.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/switch_transformer_distill_diff_model.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/switch_transformer_distill_sft.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/glam_related_model.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/glam_compare_gpt3.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/glam_compare_gpt3_2.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/glam_model.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/glam_family.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/glam_perf.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/st_moe_models.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/st_moe_remove_multiplications.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/st_moe_more_dense_layer.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/st_moe_more_add_noise.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/st_moe_z_loss_result.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/st_moe_round_error.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/st_moe_capacity_factor.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/st_moe_capacity_factor_speed.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/st_moe_perf.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/st_moe_encoder_specialization.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/st_moe_multiling_specialization.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/ds_moe_perf.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/ds_moe_structure.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/ds_model_param.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/ds_moe_comparison.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/ds_moe_upper_bound_2b.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/ds_moe_upper_bound_13b.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/ds_moe_ablation.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/ds_moe_expert_specialization.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/ds_moe_less_activated_expert.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/ds_2b_less_expert.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/ds_model_param.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/ds_16b_perf_1.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/ds_16b_perf_2.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/ds_moe_145b.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/dbrx_perf.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/dbrx_infer_efficiency.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/qwen1.5_moe_perf.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/qwen1.5_moe_params.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/qwen1.5_moe_tps.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/mistral_8_7b_perf.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/mistral_8_7b_active_perf.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/mistral_8_22b_reasoning.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/mistral_8_22b_multiling.png">
<meta property="og:image" content="https://saicat.github.io/44e38c1b/mistral_8_22b_code.png">
<meta property="og:image" content="https://saicat.github.io/images/qrcode.jpg">
<meta property="article:published_time" content="2024-03-30T01:56:05.000Z">
<meta property="article:modified_time" content="2024-05-10T06:51:36.425Z">
<meta property="article:author" content="Lin">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="transformer">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="MoE">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://saicat.github.io/44e38c1b/xiaomi_moe.jpg">


<link rel="canonical" href="https://saicat.github.io/44e38c1b.html">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://saicat.github.io/44e38c1b.html","path":"44e38c1b.html","title":"MoE模型的前世今生"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>MoE模型的前世今生 | Linsight</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Linsight</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">聊聊AI技术，也聊聊其他的</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%97%B6%E9%97%B4%E7%BA%BF"><span class="nav-number">1.</span> <span class="nav-text">时间线</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8A%E5%8F%A4%E6%97%B6%E4%BB%A3"><span class="nav-number">1.1.</span> <span class="nav-text">上古时代</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rnn%E6%97%B6%E4%BB%A3"><span class="nav-number">1.2.</span> <span class="nav-text">RNN时代</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#transformer%E6%97%B6%E4%BB%A3"><span class="nav-number">1.3.</span> <span class="nav-text">Transformer时代</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gpt%E6%97%B6%E4%BB%A3"><span class="nav-number">1.4.</span> <span class="nav-text">GPT时代</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A5%A0%E5%9F%BA%E5%B7%A5%E4%BD%9C"><span class="nav-number">2.</span> <span class="nav-text">奠基工作</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lstm-moe"><span class="nav-number">3.</span> <span class="nav-text">LSTM MoE</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%83%8C%E6%99%AF"><span class="nav-number">3.1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1"><span class="nav-number">3.2.</span> <span class="nav-text">模型设计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1"><span class="nav-number">3.3.</span> <span class="nav-text">负载均衡</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">3.4.</span> <span class="nav-text">实验</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#gshard"><span class="nav-number">4.</span> <span class="nav-text">GShard</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#switch-transformer"><span class="nav-number">5.</span> <span class="nav-text">Switch Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1-1"><span class="nav-number">5.1.</span> <span class="nav-text">模型设计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1-1"><span class="nav-number">5.2.</span> <span class="nav-text">负载均衡</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C-1"><span class="nav-number">5.3.</span> <span class="nav-text">实验</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#glam"><span class="nav-number">6.</span> <span class="nav-text">GLaM</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#st-moe"><span class="nav-number">7.</span> <span class="nav-text">ST-MoE</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A8%B3%E5%AE%9A%E6%80%A7%E4%B8%8E%E6%95%88%E6%9E%9C%E5%88%86%E6%9E%90"><span class="nav-number">7.1.</span> <span class="nav-text">稳定性与效果分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1-2"><span class="nav-number">7.2.</span> <span class="nav-text">模型设计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C-2"><span class="nav-number">7.3.</span> <span class="nav-text">实验</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#deepseekmoe"><span class="nav-number">8.</span> <span class="nav-text">DeepseekMoE</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1-3"><span class="nav-number">8.1.</span> <span class="nav-text">模型设计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1-2"><span class="nav-number">8.2.</span> <span class="nav-text">负载均衡</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C-3"><span class="nav-number">8.3.</span> <span class="nav-text">实验</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dbrx"><span class="nav-number">9.</span> <span class="nav-text">DBRX</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#qwen1.5-moe"><span class="nav-number">10.</span> <span class="nav-text">Qwen1.5-MoE</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#mistral"><span class="nav-number">11.</span> <span class="nav-text">Mistral</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#mistral-8x7b"><span class="nav-number">11.1.</span> <span class="nav-text">Mistral 8x7B</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mistral-8x22b"><span class="nav-number">11.2.</span> <span class="nav-text">Mistral 8x22B</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93"><span class="nav-number">12.</span> <span class="nav-text">小结</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reference"><span class="nav-number">13.</span> <span class="nav-text">Reference</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Lin"
      src="/images/avatar/Picasso_Elephant.png">
  <p class="site-author-name" itemprop="name">Lin</p>
  <div class="site-description" itemprop="description">AI | NLP</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">85</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">80</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="mailto:331603034@qq.com" title="E-Mail → mailto:331603034@qq.com" rel="noopener me" target="_blank"><i class="fa-regular fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

<!--
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/1.js?i=5acfv0hqzp5&amp;s=220&amp;m=1&amp;v=false&amp;r=false&amp;b=000000&amp;n=false&amp;c=ff0000" async="async"></script>
-->

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://saicat.github.io/44e38c1b.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar/Picasso_Elephant.png">
      <meta itemprop="name" content="Lin">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Linsight">
      <meta itemprop="description" content="AI | NLP">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="MoE模型的前世今生 | Linsight">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          MoE模型的前世今生
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-03-30 09:56:05" itemprop="dateCreated datePublished" datetime="2024-03-30T09:56:05+08:00">2024-03-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-05-10 14:51:36" itemprop="dateModified" datetime="2024-05-10T14:51:36+08:00">2024-05-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CS/" itemprop="url" rel="index"><span itemprop="name">CS</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CS/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CS/NLP/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>33k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1:01</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>【本文已在同名 微信公众号 / 知乎 / <a target="_blank" rel="noopener" href="http://www.linsight.cn/">个人博客linsight.cn</a> 上线】</p>
<hr>
<p>2024年3、4月这段时间，很多MoE模型扎堆发布，包括Qwen1.5-MoE、DBRX、Jamba和Mistral等。</p>
<p>下面这个表格列出了部分近期发布的MoE工作</p>
<center>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">模型</th>
<th style="text-align: center;">发布时间</th>
<th style="text-align: center;">备注</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">GPT4</td>
<td style="text-align: center;">2023年3月</td>
<td style="text-align: center;">23年6月George
Hotz爆料GPT4是8×220B模型</td>
</tr>
<tr class="even">
<td style="text-align: center;">Mistral-8×7B</td>
<td style="text-align: center;">2023年12月</td>
<td style="text-align: center;">Mistral AI，开源</td>
</tr>
<tr class="odd">
<td style="text-align: center;">LLAMA-MoE</td>
<td style="text-align: center;">2023年12月</td>
<td style="text-align: center;">github开源项目</td>
</tr>
<tr class="even">
<td style="text-align: center;">DeepSeek-MoE</td>
<td style="text-align: center;">2024年1月</td>
<td style="text-align: center;">幻方量化，国内首个开源MoE模型，有技术报告</td>
</tr>
<tr class="odd">
<td style="text-align: center;">abab6</td>
<td style="text-align: center;">2024年1月</td>
<td style="text-align: center;">MiniMax，号称千亿MoE，无开源，无细节发布</td>
</tr>
<tr class="even">
<td style="text-align: center;">天工2.0</td>
<td style="text-align: center;">2024年2月</td>
<td style="text-align: center;">昆仑万维，无开源，无细节发布</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Step-2</td>
<td style="text-align: center;">2024年3月</td>
<td style="text-align: center;">阶跃星辰，无开源，无细节发布</td>
</tr>
<tr class="even">
<td style="text-align: center;">MM1</td>
<td style="text-align: center;">2024年3月</td>
<td style="text-align: center;">苹果，多模态MoE，无开源，有技术报告</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Grok-1</td>
<td style="text-align: center;">2024年3月</td>
<td style="text-align: center;">X，开源</td>
</tr>
<tr class="even">
<td style="text-align: center;">Qwen1.5-MoE-A2.7B</td>
<td style="text-align: center;">2024年3月</td>
<td style="text-align: center;">阿里巴巴，开源</td>
</tr>
<tr class="odd">
<td style="text-align: center;">DBRX</td>
<td style="text-align: center;">2024年3月</td>
<td style="text-align: center;">Databricks，开源</td>
</tr>
<tr class="even">
<td style="text-align: center;">Jamba</td>
<td style="text-align: center;">2024年3月</td>
<td style="text-align: center;">AI21，开源</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Mistral-8×22B</td>
<td style="text-align: center;">2024年4月</td>
<td style="text-align: center;">Mistral AI，开源</td>
</tr>
<tr class="even">
<td style="text-align: center;">WizardLM-2-8×22B</td>
<td style="text-align: center;">2024年4月</td>
<td style="text-align: center;">微软，开源</td>
</tr>
<tr class="odd">
<td style="text-align: center;">天工3.0</td>
<td style="text-align: center;">2024年4月</td>
<td style="text-align: center;">昆仑万维，400BMoE</td>
</tr>
<tr class="even">
<td style="text-align: center;">Arctic</td>
<td style="text-align: center;">2024年4月</td>
<td style="text-align: center;">Snowflake，480B，Dense-MoE
Hybrid，开源</td>
</tr>
</tbody>
</table>
</center>
<p>MoE模型目前风头正劲，就连前不久小米汽车发布会上，雷总也弄了个多模态MoE大模型做汽车智能中控</p>
<img src="/44e38c1b/xiaomi_moe.jpg" class title="小米汽车多模态MoE模型">
<p>相信今年接下来的这段时间，MoE还会给我们带来更多的大新闻。</p>
<p>本篇将初步梳理MoE相关的一些经典工作和几个近期发布的中文MoE模型，从背景、思路和效果来了解MoE模型。</p>
<p>到文章发出的2024年4月为止，个人认为DeepSeek-MoE和Qwen1.5-MoE是中文领域做得比较好的两个工作，赶时间的朋友可以优先关注这两个工作。</p>
<h1 id="时间线">时间线</h1>
<p>这里先对后面会涉及的MoE相关工作，大致按时间线梳理一下，也列出一些关键信息包括模型结构、模型规模等。</p>
<p>（很多经典的MoE工作都出自Google）</p>
<h2 id="上古时代">上古时代</h2>
<p>首先是很多MoE相关论文都会引用的，发表在1991年的论文<a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf">《Adaptive
Mixtures of Local Experts》</a>，这篇文章出自Geoffrey Hinton和Michael I.
Jordan两位大神之手。虽然在更早的时候就有MoE相关概念的工作，如原文所提到的，1988年这个概念就有了</p>
<blockquote>
<p>This idea was first presented by Jacobs and Hinton at the
Connectionist Summer School in Pittsburg in 1988.</p>
</blockquote>
<p>但是大部分MoE文章还是认为是这个工作奠定了MoE的基础。</p>
<h2 id="rnn时代">RNN时代</h2>
<p>时隔二十多年，Google在2017年1月发布了<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1701.06538">《Outrageously Large Neural
Networks: The Sparsely-Gated Mixture-of-Experts
Layer》</a>，把MoE带进了LSTM，训出了最大137B参数，专家数达到128k的LSTM模型。</p>
<h2 id="transformer时代">Transformer时代</h2>
<ol type="1">
<li><p>2020年6月，Google发布<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.16668">《GShard: Scaling Giant Models
with Conditional Computation and Automatic
Sharding》</a>，把MoE应用在encoder-decoder结构的transformer模型上，每两层将一个FFN层替换成一个MoE层，训出了模型参数量从12.5B到600B的一系列MoE模型，每层最大专家数也达到2048个。</p></li>
<li><p>2021年1月，Google发布<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.03961">《Switch Transformers: Scaling
to Trillion Parameter Models with Simple and Efficient Sparsity》</a>
，在T5（encoder-decoder结构）的基础上，把FFN层替换成MoE层，并简化了routing策略，训出了最大1.6T参数量的switch
transformer。Switch
Transformers对scaling、蒸馏等做了很多详细的探索，影响深远，是很重要的一个工作。</p></li>
<li><p>2022年2月，Google发布<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2202.08906">《ST-MoE: Designing Stable and
Transferable Sparse Expert
Models》</a>，也是一个基于encoder-decoder结构的MoE模型，最大模型有269B的总参数，32B的激活参数。ST-MoE可以说不仅仅是一个MoE工作，对于模型结构、工程实现、训练策略等都做了很多分析，个人认为其重要程度相比Switch
Transformer都有过之而无不及。</p></li>
</ol>
<h2 id="gpt时代">GPT时代</h2>
<ol type="1">
<li><p>2021年12月，Google发布了GLaM，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.06905">《GLaM: Efficient Scaling of
Language Models with
Mixture-of-Experts》</a>，训出了最大为1.2T参数量的decoder-only模型。（从encoder-decoder到decoder-only，可以看到Google内部在模型结构方向上也有很多不同的尝试）</p></li>
<li><p>2024年1月，幻方量化发布<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.06066">《DeepSeekMoE: Towards Ultimate
Expert Specialization in Mixture-of-Experts Language
Models》</a>，对在23年12月开源的DeepSeekMoE，给出了一些细节。</p></li>
<li><p>2024年，Databricks的DBRX、阿里的Qwen1.5-MoE-A2.7B、Mistral
AI的Mistral-8x22B等陆续发布。</p></li>
</ol>
<h1 id="奠基工作">奠基工作</h1>
<p>Geoffrey Hinton和Michael I. Jordan的<a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf">《Adaptive
Mixtures of Local Experts》</a>是大多数MoE论文都会引用的最早工作。</p>
<ol type="1">
<li>思路</li>
</ol>
<p>这篇文章大致的思路是这样的：对于比较复杂的任务，一般可以拆分为多个子任务。比如要求计算输入文本中有多少个动词和名词，那就可以拆分为“数动词”和“数名词”这两个子任务。</p>
<p>而一个模型如果要同时学习多个子任务，多个子任务相互之间就会互相影响，模型的学习就会比较缓慢、困难，最终的学习效果也不好。</p>
<p>因此这篇文章提出了一种由多个分开的子网络组成的监督学习方法。这些分开的网络，在训练过程中，分别学习处理整个训练数据集中的一个子集，也就是一个子任务。这个思路就是现代MoE的思路，每个子网络（也就是一个expert）学习处理一部分内容。</p>
<p>文章里把这个MoE的方法应用于vowel discrimination
task，即元音辨别任务，验证了MoE设计的有效性。元音辨别指的是语音学中区分不同元音的能力，在语音学中，模型需要学习辨别不同的元音因素，以便准确地理解和识别语音输入。通过让多个子模型分别学习分别学习不同元音（a、e、i、o、u）辨别的子任务，最终效果得到了提升。</p>
<ol start="2" type="1">
<li>模型设计</li>
</ol>
<p>下图展示的就是这个MoE的思路：各个expert network和gating
network接收同样的输入，每个expert给出各自的处理结果；而gating
network输出每个expert的权重，就像一个开关一样，控制着每个expert对当前输入的打开程度，只是这个开关不是离散的，而是stochastic的，给出的不是true和false，而是权重。</p>
<img src="/44e38c1b/vanilla_moe.png" class title="Vanilla MoE">
<ol start="3" type="1">
<li>损失函数优化</li>
</ol>
<p>实际上，MoE这个idea在这篇文章之前就有了。如论文中所提，Jacobs和Hinton在1988就讨论过。但是之前的工作在loss的设计上，和ensemble更相近，多个expert之间更倾向于合作，每个expert会学习其他expert的residual部分。</p>
<p>具体来说，对于case <span class="math inline">\(c\)</span>，假设第
<span class="math inline">\(d^c\)</span> 是对应的ground truth，第 <span class="math inline">\(i\)</span> 个expert的输出是 <span class="math inline">\(o_{i}^c\)</span>，<span class="math inline">\(p_{i}^c\)</span> 是gating network给第 <span class="math inline">\(i\)</span>
个expert分配的权重，那么以前的工作所使用的损失函数 <span class="math inline">\(E^{c}\)</span> 计算如下</p>
<p><span class="math display">\[E^{c}=\left|\left|d^{c}-\sum_{i}p_{i}^{c}o_{i}^{c}\right|\right|^{2}\]</span></p>
<p>这样的损失计算方式，是把期望输出和所有expert输出的混合结果进行比较。</p>
<p>这样做的结果是，在训练过程中，每个expert学习的其实是其他expert的组合结果所剩下的残差。这样的学习目标并不能很好迫使每个expert单独输出好的结果，因此不能得到稀疏的模型。</p>
<p>从另一个角度来看，这个损失计算把所有专家耦合在了一起。即当一个expert的输出发生了变化，所有expert的组合结果也会变化，其他所有的expert也需要做相应的改动来适应这个变化。因此各个expert之间更加倾向于合作，而不是相互竞争并单独给出好的结果，让gating
network输出稀疏的结果。</p>
<p>虽然可以使用如增加辅助损失函数的做法，迫使模型给出稀疏激活的结果，但是这样相当于增加了很强的先验正则化，对模型最终效果也是有损害的。</p>
<p>而Hinton和Jordan在这个工作里，提出更简单的做法是对loss计算进行修改，使得各个expert之间的关系从合作变成竞争。</p>
<p>假设gating network每次随机选择一个expert，损失计算如下</p>
<p><span class="math display">\[E^{c}=\langle\|\mathbf{d}^c-\mathbf{o}_i^c\|^2\rangle=\sum_{i}p_{i}^{c}\left|\left|d^{c}-o_{i}^{c}\right|\right|^{2}\]</span></p>
<p>在这个损失函数中，每个expert的输出结果会单独和期望结果进行对比，这就要求每个expert单独给出完整的结果，而不是仅学习其他expert的残差。</p>
<p>这样的loss计算具有localization的特性，即如果一个训练case错了，那么会被修改的主要是被gating
network选中且出错的expert，以及负责分配权重的gating
network，而不会很大地影响其他expert。</p>
<p>此外，localization还体现在，每个expert只会负责处理输入空间中某个特定子空间的向量，而不是完整的输入空间。</p>
<p>这样一来，不同的expert之间不会直接相互影响，虽然还是有间接的影响，比如某个expert的输出变了，gating
network可能会分配新的权重，但是至少不会改变其他expert
error的符号（+，-），即优化的方向。</p>
<p>最终的结果是，对于给定的输入，这样的系统会倾向于以高权重分配单一一个expert来预测结果（但其他权重还不是真正的0，不是真正的稀疏）。</p>
<ol start="4" type="1">
<li>实操技巧</li>
</ol>
<p>上面提出的这个loss计算，理论上没有问题，实际上也能训练，但是为了得到更好的效果，作者把原loss计算作了如下变化：先指数化再求和，最后再取对数，得到了优化loss。看下变化前后的对比</p>
<p><span class="math display">\[\text{原loss：}E^{c}=\sum_{i}p_{i}^{c}\left|\left|d^{c}-o_{i}^{c}\right|\right|^{2}\]</span></p>
<p><span class="math display">\[\text{优化loss：}E^c=-log\sum_ip_i^ce^{-\frac12\|\mathbf{d}^c-\mathbf{o}_i^c\|^2}\]</span></p>
<p>这样做有什么好处呢？来对比一下原loss函数和优化后的loss函数的求导结果</p>
<p><span class="math display">\[\text{原loss导数：}\frac{\partial
E^c}{\partial\mathbf{o}_i^c}=-2p_i^c(\mathbf{d}^c-\mathbf{o}_i^c)\]</span></p>
<p><span class="math display">\[\text{优化loss导数：}\frac{\partial
E^c}{\partial\mathbf{o}_i^c}=-\left[\frac{p_i^ce^{-\frac{1}{2}\|\mathbf{d}^c-\mathbf{o}_i^c\|^2}}{\sum_jp_j^ce^{-\frac{1}{2}\|\mathbf{d}^c-\mathbf{o}_j^c\|^2}}\right](\mathbf{d}^c-\mathbf{o}_i^c)\]</span></p>
<p>相比原loss函数的导数，优化后的loss函数的导数，把当前第 <span class="math inline">\(i\)</span>
个expert的表现，和其他expert联系起来了。这样能够更好地衡量expert <span class="math inline">\(i\)</span>
对当前case的处理结果好坏。特别是在训练初期，gating
network的权重是近似平均分配的，那么使用原loss函数的结果是，对当前case效果最好的expert，学习速度是最慢的（因为loss最小）；而优化的loss函数则可以让当前最好的expert的学习速度最快。相当于让“有天赋”的专家在对应的子任务上尽快提高水平。这样就强化了localization的特征，使得各个expert更快拟合到自己擅长的部分，加速训练。</p>
<p>（BTW，优化后的这个loss导数，和现在的对比学习形式上看起来也很相似）</p>
<p>这个工作在今天看来不很复杂，但是思路还是很踏实有效的，给MoE奠定了基础。</p>
<h1 id="lstm-moe">LSTM MoE</h1>
<p>Google在2017年1月发布了 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1701.06538">《OUTRAGEOUSLY LARGE NEURAL
NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS
LAYER》</a>，把MoE应用到了LSTM上，训出了最大137B的LSTM模型。这样规模的模型哪怕放在7年后的今天，也是巨无霸的存在，需要解决很多工程问题。</p>
<p>相比1991年的工作，这里做到了真正的稀疏激活，从而可以在实际计算量较少的情况下，训练巨大的模型。</p>
<h2 id="背景">背景</h2>
<p>虽然当时Transformer还没出来，大规模模型的竞赛也还不像今天这么激烈，但是在多个领域中（文本、图像、音频），已经有不少工作反复证实了一件事：模型容量越大，能训出来的效果越好，上限越高。但是模型越大，需要的训练数据也就越多，二者共同作用下，就造成了训练开销基本是随着模型增大，以平方关系在增长。</p>
<p>在这个背景下就出现一些conditional
computation，条件计算的工作来解决这个问题。conditional
computation就是根据输入，有选择地只激活部分网络模块。那么MoE其实就是一种条件计算的实现。由于不用激活全部参数，训练所需的计算量就大大减小，整体计算成本就不用以平方速度增长。</p>
<p>虽然理论上计算量的成本下来了，不过实操起来还是会遇到几个问题：</p>
<ul>
<li>训练的时候，在MoE结构下，每个expert的batch size比整个模型的batch
size小了。<br>
比如模型的batch
size是32，一共有16个expert，那实际上一次迭代平均每个expert只能分到2个训练样本。而batch
size对训练效率影响是很大的，大的batch
size摊小了参数传输和更新的成本。如果直接增大模型的batch
size，又会受显存和通讯效率的限制。<br>
</li>
<li>训练数据量不足。<br>
要训大模型就需要大量的数据，让模型参数充分学习。在当时的背景下，大规模的NLP数据是比较缺的。当然如今数据集多了很多，特别是预训练数据，这个问题现在来看没有那么突出了。<br>
</li>
<li>损失函数的设计。<br>
如何使用合适的损失函数来训练模型，提升效果，并且使得模型的负载比较均衡，这是一个不容易解决的问题。<br>
</li>
<li>集群通讯问题。<br>
一个GPU集群的计算能力可能比设备间网络带宽的总和高出数千倍，因此设备间的通讯很可能成为训练效率的瓶颈。为了计算效率，就要使得设备内计算量和所需的通讯量的比值，达到相应的比例。<br>
</li>
<li>GPU计算特点。<br>
GPU做数学计算很快，但是并不擅长做branching（if/else），因此MoE的工作基本上都是用gating
network来控制参数的激活。这个严格来说不算是新的挑战了，应该说是根据计算设备沿用下来的设计。</li>
</ul>
<p>要解决好这些问题，才能训出比较好的模型来。</p>
<h2 id="模型设计">模型设计</h2>
<ol type="1">
<li>整体结构</li>
</ol>
<p>先看下模型结构的设计。</p>
<p>论文里使用的是两个LSTM层，中间夹着一个MoE层，最上面和最下面分别还有一个embedding层和一个任务输出层，结构如下图所示</p>
<img src="/44e38c1b/rnn_moe.png" class title="LSTM MoE">
<p>每个expert是一个简单的feed-forward neural
network。一共有n个expert，gating network输出是一个稀疏的n维向量</p>
<p><span class="math display">\[\begin{aligned}y=\sum_{i=1}^nG(x)_iE_i(x)\end{aligned}\]</span></p>
<p><span class="math inline">\(E_{i}(x)\)</span> 是第 <span class="math inline">\(i\)</span> 个expert的输出，<span class="math inline">\(G(x)_{i}\)</span> 是gating network给出的第 <span class="math inline">\(i\)</span> 个expert的权重。</p>
<p>如果 <span class="math inline">\(G(x)_{i}\)</span>
为0，就不用计算对应的那个expert了，节省了计算。</p>
<p>如果expert的数量特别多，可以用two-level hierarchical
MoE，即使用两层gating network，第一层的gating
network先选择一个包含一批expert的分支，每个分支又有一个单独的gating
network来选择具体的expert。类似word2vec训练所用的hierarchical
softmax。这样做可以节省一些计算。</p>
<ol start="2" type="1">
<li>gating network</li>
</ol>
<p>那具体gating network怎么设计呢？</p>
<p>如果对输入进行线性变换，再简单加上一个softmax，那得到的是一个非稀疏的gating
function</p>
<p><span class="math display">\[\begin{aligned}G_\sigma(x)=Softmax(x\cdot
W_g)\end{aligned}\]</span></p>
<p>在这个基础上，使用一个topk函数，只保留最大的k个值，其他都设为﹣∞（softmax之后变成0），这样就能只选择部分expert，得到了稀疏性。</p>
<p>论文提到，虽然理论上这个形式的sparsity（topk）会造成gating
function的不连续，不过在实操中暂时没有遇到相关问题。</p>
<p>在这个基础上，在输入再加上一个Gaussian
noise，这个noise的大小由另外一个可学习的参数来控制。整体的计算公式如下</p>
<p><span class="math display">\[\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\end{aligned}\]</span></p>
<p><span class="math display">\[KeepTopK(v,k)_i=\begin{cases}v_i&amp;\text{if
}v_i\text{ is in the top }k\text{ elements of
}v.\\-\infty&amp;\text{otherwise.}\end{cases}\]</span></p>
<p><span class="math display">\[\begin{aligned}H(x)_i=(x\cdot
W_g)_i+StandardNormal()\cdot Softplus((x\cdot
W_{noise})_i)\end{aligned}\]</span></p>
<p>其中用来调整noise的非线性函数softplus是个类似ReLU的激活函数，但是更为光滑，函数图像如下</p>
<img src="/44e38c1b/softplus.png" class title="softplus">
<p>这里添加噪声的原因和负载均衡有关，下面来分析下负载均衡。</p>
<h2 id="负载均衡">负载均衡</h2>
<p>在MoE模型训练的实验中观察到，如果不对gating
network进行干预，任由模型自由学习，那么最终模型会倾向于收敛到“总是选那几个固定的expert”的状态，而其他expert几乎不会被使用。这就是负载不均衡的状态，如果这些专家分布在不同的计算设备上，结果就是有些设备输入排队特别长，而有些设备基本处于闲置状态，这明显不是我们想要的。</p>
<p>这种负载不均衡的状态有自我加强的属性，因为一旦开始出现部分专家被较多选中激活，这些专家就会得到更充分的训练，从而获得更好的效果，进而又提升被选中激活的概率。</p>
<p>针对这种情况，之前有一些工作使用hard
constraint来缓解，比如当某个expert激活次数达到上限，就把它从候选集合中移除。hard
constraint明显会对模型效果有影响。而这篇论文使用的是一种soft
constraint。</p>
<p>具体来说，对于每个expert，定义了一个它在当前这批输入数据里的重要性指标，如以下公式所示</p>
<p><span class="math display">\[Importance(X)=\sum_{x\in
X}G(x)\]</span></p>
<p><span class="math inline">\(G(x)\)</span> 是gating
network给出的权重，是一个维度等于expert数量的向量。</p>
<p>基于这个重要性指标，论文定义了一个辅助损失 <span class="math inline">\(L_{importance}\)</span>，训练时和模型的交叉熵损失加到一起。<span class="math inline">\(L_{importance}\)</span> 的计算方式如下</p>
<p><span class="math display">\[L_{importance}(X)=w_{importance}\cdot
CV(Importance(X))^2\]</span></p>
<p>其中权重 <span class="math inline">\(w_{importance}\)</span>
是手动设置的超参，实验的推荐值是0.1，CV是coefficient of variation。</p>
<p>coefficient of
variation离散系数，是概率分布离散程度的一个归一化量度，定义为标准差
<span class="math inline">\(\sigma\)</span> 和 均值 <span class="math inline">\(\mu\)</span> 的比值。</p>
<p>对于MoE来说，确定激活的expert数之后，均值是固定的。如果expert的gating很不平衡，标准差就会很大，离散系数也会很大，使得
<span class="math inline">\(L_{importance}\)</span> 变大。</p>
<p>但是这里还是有问题，虽然均衡的负载可以推导出 <span class="math inline">\(L_{importance}\)</span> 较小的结论，但是 <span class="math inline">\(L_{importance}\)</span>
较小却不能保证负载均衡。也就是说 <span class="math inline">\(L_{importance}\)</span>
较小只是负载均衡一个必要不充分条件。</p>
<p>比如一个expert可能以很高的权重被分配到一个样本，而另一个expert可能以不太高的权重被分配到好几个样本。这种情况下对所有输入数据的gating权重进行求和，仍然可能呈现出均匀的表象（离散系数比较小），但这并不符合我们的要求。</p>
<p>为了解决这个问题，需要额外再加上一个损失 <span class="math inline">\(L_{load}\)</span>
。这里就要用到添加在每个expert输出上的随机噪音了。</p>
<p>我们想要各个expert的负载均衡，也就是每个专家需要处理的样本数基本一致，但是分配到各个专家的样本数是个离散值，因此没有办法直接用于back
propagation，而 <span class="math inline">\(L_{load}\)</span>
就是对各个expert负载的一个平滑评估。</p>
<p>回想一下前面在设计MoE的时候，定义了 <span class="math inline">\(H(x)\)</span> 为KeepTopK函数的输入</p>
<p><span class="math display">\[\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}H(x)_i=(x\cdot
W_g)_i+StandardNormal()\cdot Softplus((x\cdot
W_{noise})_i)\end{aligned}\]</span></p>
<p>那么这里先定义一个 <span class="math inline">\(kth\_excluding(H(x),k,i)\)</span>，表示在除去
<span class="math inline">\(H(x)\)</span> 中的第 <span class="math inline">\(i\)</span> 个分量之后，排在第 <span class="math inline">\(k\)</span> 大的值。基于这个，再定义 <span class="math inline">\(P(x,i)\)</span>
为：固定其他分量已经选取好的noise，重新给第 <span class="math inline">\(i\)</span> 个分量再添加一次noise，结果比 <span class="math inline">\(kth\_excluding(H(x),k,i)\)</span>
大的概率，公式如下</p>
<p><span class="math display">\[\begin{aligned}P(x,i)=Pr\Big((x\cdot
W_g)_i+StandardNormal()\cdot Softplus((x\cdot
W_{noise})_i)\\&gt;kth\_excluding(H(x),k,i)\Big)\end{aligned}\]</span></p>
<p>通过这个noise，我们把“第 <span class="math inline">\(i\)</span>
个专家是否处理这个输入”的离散值，变成“第 <span class="math inline">\(i\)</span>
个专家处理这个输入的概率”这样一个平滑的估计，<span class="math inline">\(P(x,i)\)</span>
就表示这个概率。这个概率可以简化写成</p>
<p><span class="math display">\[\begin{aligned}P(x,i)&amp;=\Phi\Big(\frac{(x\cdot
W_g)_i-kth\_excluding(H(x),k,i)}{Softplus((x\cdot
W_{noise})_i)}\Big)\end{aligned}\]</span></p>
<p>其中 <span class="math inline">\(\Phi\)</span>
是标准正态分布的CDF。</p>
<p>接下来就可以把第 <span class="math inline">\(i\)</span>
个expert的负载定义为</p>
<p><span class="math display">\[\begin{aligned}Load(X)_i=\sum_{x\in
X}P(x,i)\end{aligned}\]</span></p>
<p>有了每个expert的负载衡量，就可以和前面第一个负载均衡损失一样，计算新的负载均衡损失了</p>
<p><span class="math display">\[L_{load}(X)=w_{load}\cdot
CV(Load(X))^2\]</span></p>
<p><span class="math inline">\(w_{load}\)</span>
是手动设置的超参，实验的推荐值是0.1。</p>
<p>相比前面的 <span class="math inline">\(L_{importance}(X)\)</span>，<span class="math inline">\(Load(X)\)</span>
是对负载是否均衡更细粒度的评估。</p>
<p>论文中提到一个细节，在刚开始训练的时候，希望模型分配的expert尽量均衡，因此把
<span class="math inline">\(W_g\)</span> 和 <span class="math inline">\(W_{noise}\)</span>
都设为0，这样相当于没有信号，也没有噪音。</p>
<p>最终使用负载均衡之后的效果如下</p>
<img src="/44e38c1b/rnn_moe_load_function.png" class title="负载平衡效果">
<p>使用这两个负载均衡损失之后，能达到接近完全平均分配的效果。</p>
<h2 id="实验">实验</h2>
<ol type="1">
<li>解决工程问题</li>
</ol>
<p>针对前面提出的一些工程问题，论文给出一些方案</p>
<p>（1）batch size减小</p>
<p>由于稀疏激活的原因，每个expert的batch
size会变小。假设每次在n个expert中选择k个，模型训练的batch
size为b，那么每个expert的batch
size就是kb/n。论文通过以下这几种方法来提升每个expert的batch size：<br>
-
混合使用数据并行和模型并行。本来在使用数据并行的情况下，每个模型副本是异步处理各自的数据的。而这里做了优化，各个副本的batch是同步处理的，这样就可以把多个模型副本的batch组合起来。对于非MoE部分的参数，依然使用标准的数据并行机制；而对于每个expert，则在整个集群中只保留一个副本。如果模型分布在d个设备上，那每个expert就能得到一个kbd/n的batch
size。 - 对于LSTM模型，在时间步上展开，就能把batch
size提升相应的倍数。</p>
<p>（2）集群通讯问题</p>
<p>另一个挑战就是平衡集群计算量和通讯量的关系。</p>
<p>对于每个expert来说，主要的通讯就是input和output的传输。而每个专家的主要计算量就是两个全连接层，大小分别为[input_size,
hidden_size]和[hidden_size,
output_size]。对于GPU来说，计算速度可能是通讯速度的1000倍，那我们就需要把计算量设计得足够大。最简单的做法就是把hidden_size提高，使得每个expert的内部计算量比通讯量大1000倍，以保证通讯不会成为训练的瓶颈。</p>
<ol start="2" type="1">
<li>模型容量 &amp; 参数效率</li>
</ol>
<p>为了验证模型容量提升带来的收益，以及MoE模型的参数效率（即和dense模型同样推理计算量下能达到的效果），训练了包含4/32/256个expert的flat
MoE模型，和包含256/1024/4096个expert的hierarchical
MoE模型。每个expert大约是1M参数量，对于所有flat模型都是激活4个expert，而对于hierarchical
MoE是每层gating激活2个。</p>
<p>效果如下图。左边的图显示，随着模型容量提升，测试的ppl有明显下降。右边的图将相近模型容量的dense模型和MoE模型的效果放在一起对比，可以看到MoE模型在相同模型容量下，效果更好</p>
<img src="/44e38c1b/rnn_moe_perf.png" class title="效果">
<ol start="3" type="1">
<li>更大的模型</li>
</ol>
<p>前面几个模型训练用的数据量不是很大，模型最大也只有4B左右，训练不久就出现diminishing
returns。</p>
<p>为了验证更大数据集 + 更大模型的收益，在100B
token的语料上，分别训了包含32, 256, 1024，4096, 16384, 65536,
和131072个expert的MoE模型，最大的模型达到了137B的参数量。</p>
<p>各个模型对比如下表。整体来看，增加数据和模型容量，是可以继续获得提升的。</p>
<img src="/44e38c1b/rnn_moe_137b.png" class title="137模型效果">
<p>从这里还可以看出，在专家数量不太多时，提升专家数量效果有提升，但是收益会慢慢减小，甚至会出现专家数量太多，效果反而下降的情况。</p>
<ol start="4" type="1">
<li>Expert Specialization</li>
</ol>
<p>按照MoE的设计思路，不同的专家应该学习到不同的子任务，但是实际上是否是这样呢？</p>
<p>论文里把模型中不同的专家分配到token拿出看，发现确实有比较强的specialization效果，不同的专家处理不同的内容，如下所示</p>
<img src="/44e38c1b/rnn_moe_specilized.png" class title="RNN MoE 专门化">
<h1 id="gshard">GShard</h1>
<ol type="1">
<li>简介</li>
</ol>
<p>2018年，随着Bert的发布，transformer结构彻底火了起来。2020年6月，Google发布《GShard:
Scaling Giant Models with Conditional Computation and Automatic
Sharding》，把MoE用到了encoder-decoder结构的transformer模型上。MoE开始变成我们现在熟悉的样子了。</p>
<p>GShard这个工作做了很多的实验，训了很多规模巨大的MoE模型，最大的达到了600B。训练的一系列模型的参数如下表</p>
<img src="/44e38c1b/gshard_moe_family.png" class title="GShard MoE family">
<p>在expert数量的设计上，延续上面LSMT MoE工作的思路 --
expert越多，效果越好。（站在24年这个时间节点来看，太多的expert未必适合；但是也不能说这个思路一定错误，毕竟事物的发展是螺旋式的，就像ChatGPT出来之前大多数人都在魔改各种Bert，而GPT已经坐了几年冷板凳了。）</p>
<p>GShard论文中很大的篇幅在介绍工程实现和优化，这也是MoE模型训练最大的痛点。关于工程框架的内容比较硬核，因此这里不会展开讲太多，而是关注在模型算法层面上。</p>
<ol start="2" type="1">
<li>模型设计</li>
</ol>
<p>先来看下模型设计。</p>
<p>Google在那段时间走的是encoder-decoder
transfomer的技术路线，因此GShard也是基于encoder-decoder
transfomer的模型结构。</p>
<p>GShard的模型设计是，在encoder和decoder中，每两层把其中一个FFN层替换成MoE层。对于总共有N层的模型，则有N/2个MoE层，如下图</p>
<img src="/44e38c1b/gshard_model.png" class title="GShard模型结构">
<p>每层会选择最多top-2 expert来激活。为什么是最多，后面解释。</p>
<p>GShard在上面这篇LSTM MoE论文的基础上，改进了gating
function和auxiliary loss function。</p>
<p>从公式来看，MoE层的具体计算如下</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{G}_{s,E}&amp; =\mathrm{GATE}(x_s)  \\
\mathrm{FFN}_e(x_s)&amp; =wo_e\cdot\text{ReLU}(wi_e\cdot x_s)  \\
y_{s}&amp; =\sum_{e=1}^E\mathcal{G}_{s,e}\cdot\mathrm{FFN}_e(x_s)
\end{aligned}\]</span></p>
<p>其中 <span class="math inline">\(x_s\)</span> 是MoE的输入token，<span class="math inline">\(w_i\)</span> 和 <span class="math inline">\(w_o\)</span>
分别是输入输出的线性变换矩阵。向量<span class="math inline">\(\mathcal{G}_{s}\)</span> 就是gating
function的输出。</p>
<p>GShard在gating
function的设计上提出了两个要求：（1）负载均衡（2）高效扩展。</p>
<p>负载均衡和前面讲的一样，很好理解。而为什么要高效扩展，因为如果要对N个token分别进行E个expert的分配，在N能达到百万甚至千万级别，而E也有几百上千的情况下，就需要一个高效的分布式实现，以免其他计算资源等待gating
function。</p>
<p>为了满足这些要求，gating function提出了以下机制</p>
<p>（1）专家容量 expert capacity</p>
<p>为了确保负载平衡，我们不希望有少量expert需要处理很多token，因此强制规定了每一个expert所负责处理的token数量有一个最大值，这个最大值就叫专家容量，在这里设置为2N/E，相当于平均分配的量。</p>
<p>这个expert capacity通过GATE(·)给每个expert维护一个计数器 <span class="math inline">\(c_e\)</span>
来监控。如果一个token所选的两个专家当前处理量都已经超过设定的专家容量，那么这个token就不会被当前层的任何expert处理，而是直接通过残差链接透传到下一层。</p>
<p>（2）分组分配 Local group dispatching</p>
<p>给所有输入token分成了G组，不同的组并行处理，每个组相应地也把组内专家容量变成2N/EG。</p>
<p>这样做相当于在前向推理时，把大的batch拆分成小的batch，每个小的batch就是一个group。这样做的好处是通讯的时候（特别是all2all）只需要在每个group内进行就可以了，减少了通讯量。</p>
<p>而进行反向计算的时候这些group可以合起来一起用，相当于进行了gradient
accumulation。</p>
<p>（3）辅助损失函数 Auxiliary loss</p>
<p>光设置专家容量并不能使得gating负载均衡，而且会导致大量溢出。参考前面LSTM
MoE的工作，这里也定义了一个辅助损失函数，来帮助负载均衡。辅助损失函数设计如下</p>
<p><span class="math display">\[\ell_{aux}=\frac1E\sum_{e=1}^E\frac{c_e}S\cdot
m_e\]</span></p>
<p><span class="math inline">\(S\)</span> 是token数，<span class="math inline">\(E\)</span> 是专家数，<span class="math inline">\(c_e\)</span> 是分配给第 <span class="math inline">\(e\)</span> 个专家的token数，<span class="math inline">\(m_e\)</span> 是第 <span class="math inline">\(e\)</span> 个expert在 <span class="math inline">\(S\)</span> 个token中获得的平均权重。</p>
<p>思路是，本来是要算 <span class="math inline">\(\frac{c_e}S\)</span>
的平方的，但这是离散值不可导，因此把平方中的一个 <span class="math inline">\(\frac{c_e}S\)</span> 换成了 <span class="math inline">\(m_e\)</span> ， <span class="math inline">\(m_e\)</span> 是第 <span class="math inline">\(e\)</span> 个expert在 <span class="math inline">\(S\)</span>
个token中获得的平均权重。在平均分配的情况下，这个loss达到最小。</p>
<p>相比前面的负载均衡损失，这个loss的设计就简单许多。</p>
<p>gating的整个算法如下</p>
<img src="/44e38c1b/gshard_algo_1.png" class title="GShard gating 算法">
<p>（4）随机路由 Random routing</p>
<p>前面提到，每层会选择最多top-2
expert来激活，就是因为有随机路由的机制。直观来说，就是认为如果top-1专家的权重很高，而第二个专家的权重如果较小，那很有可能只用第一个专家就足够解决问题了。</p>
<p>随机路由的机制是top-1的专家永远会被激活，而第二个专家如果权重很小，就认为它可以被忽略。具体来说，会以与第二个专家的权重g2成比例的概率激活第二个专家。</p>
<ol start="3" type="1">
<li>效果</li>
</ol>
<p>最后看一下模型在翻译任务上的效果</p>
<img src="/44e38c1b/gshard_perf.png" class title="GShard效果">
<h1 id="switch-transformer">Switch Transformer</h1>
<p>2022年4月，距离ChatGPT发布还有半年，Google发布了《Switch
Transformers: Scaling to Trillion Parameter Models with Simple and
Efficient Sparsity》（实际上2021年Google就提出Switch
Transformer了）。</p>
<p>Switch
Transformer和GShard一样，是encoder-decoder结构，基于T5开发的，具有1.6T的参数，2048个expert。</p>
<p>和前面的很多工作一样，Switch
Transformer有一个出发点，那就是参数量越大，模型效果越好，并且可以通过稀疏激活来减少总计算量。</p>
<p>但是相比其他工作，Switch
Transformer给出了一个更为具体的描述，那就是模型参数量可以是一个独立于总计算量的，单独的缩放轴。也就是说，在改变参数量的同时，（几乎）不改变训练和推理的计算量，就可以带来效果的提升。因此Switch
Transformer关注在“同样的FLOPS/token的计算量”下，如何扩大模型，提升效果。</p>
<p>Switch Transformer所做的工作还是比较多的，包括：</p>
<p>（1）模型结构简化：简化了Transformer上的MoE架构，提出Switch
Transformer架构。</p>
<p>（2）MoE to
dense：把训出来的效果较好的MoE模型蒸馏到dense模型，在压缩MoE模型99%的参数的情况下，效果还是比直接训练dense模型好。</p>
<p>（3）训练和微调技术：<br>
- 首次使用bf16成功训练MoE模型<br>
- 更适合MoE结构的模型初始化<br>
- 增加的专家正则化，改善了稀疏模型的微调和多任务训练</p>
<p>（4）训练框架：结合数据、模型和专家并行性，训练了超过1T参数的MoE模型。</p>
<p>（5）多语言：在多语言数据集上训练，发现101种语言效果普遍有提升。</p>
<p>（6）训练效率：在同样的FLOPS/token的计算量下，Switch
Transformer模型收敛速度有数倍的提升。</p>
<h2 id="模型设计-1">模型设计</h2>
<p>Switch
Transformer的模型结构如下图，类似GShard，把transformer每层的FFN替换成MoE层</p>
<img src="/44e38c1b/switch_transformer_structure.png" class title="Switch Transformer 模型结构">
<p>Switch Transformer一个重要的改进点就是简化了gating
function的做法（Switch Transformer论文里叫routing）。</p>
<p>之前的工作大多探索了选择k个expert的做法，而Switch
Transformer则直接把gating简化为只选择1个expert，即k=1。这样的MoE层叫做Switch
layer。</p>
<p>这样简化之后，routing的实现更简单，router的计算量小了，也减少了通讯量。</p>
<h2 id="负载均衡-1">负载均衡</h2>
<p>同GShard一样，Switch Transformer规定了一个专家容量expert
capacity，来限制每个expert在一个batch里能处理的最大token数。</p>
<p>如果一个token被分配到了一个已经满载的expert，就会出现overflow，那这个token在本层就不会被处理，而是直接通过残差链接，透传给下一层。这点也同GShard一样。</p>
<p>在Switch Transformer，专家容量通过容量系数capacity factor来控制。</p>
<p><span class="math display">\[\text{expert
capacity}=\left(\frac{\text{tokens per batch}}{\text{number of
experts}}\right)\times\text{capacity factor}.\]</span></p>
<p>一个大的capacity
factor意味着每个expert能够处理更多的token，从而减少overflow情况的发生，但是计算量和通讯量的压力也会增大，所以这是一个需要权衡的参数。</p>
<p>下图给出了一个不同capacity factor下的例子</p>
<img src="/44e38c1b/switch_transformer_diff_expert_capacity.png" class title="不同的expert capacity">
<p>那么如何设定expert capacity呢？</p>
<p>如果capacity
factor为1的话，只有在完全平均分配的时候，才不会出现overflow的情况。而太大的capacity
factor则可能造成算力和存储的浪费。</p>
<p>首先，实验中发现expert的数量和overflow的数量之间没有什么关系，所以在所有实验中，所有MoE和Switch
Transformer模型都用128个专家。</p>
<p>不同的capacity
factor对模型影响如下表。可以看到，大的容量系数相对来说能取得更好的效果（因为更少的overflow），但是相应地，大容量系数的模型处理速度就会慢一些。</p>
<img src="/44e38c1b/switch_transformer_capacity_effect.png" class title="expert capacity的效果">
<p>经验上，低的token丢弃率对模型的scaling很重要，想要训练超大规模的模型，就要解决这个问题。而通过负载均衡损失就可以确保良好的平衡，使得在使用较小容量系数的情况下，overflow尽量少，从而兼顾效果和计算速度。</p>
<p>关键问题来到负载均衡损失怎么设计。</p>
<p>给定 <span class="math inline">\(N\)</span> 个expert，和包含 <span class="math inline">\(T\)</span> 个token的batch <span class="math inline">\(\mathcal{B}\)</span>，负载均衡损失是这么计算的</p>
<p><span class="math display">\[\begin{aligned}\text{loss}&amp;=\alpha\cdot
N\cdot\sum_{i=1}^Nf_i\cdot P_i\end{aligned}\]</span></p>
<p><span class="math inline">\(f_{i}\)</span> 表示被分配到第 <span class="math inline">\(i\)</span> 个expert的token数，这个不可导</p>
<p><span class="math display">\[\begin{aligned}f_i=\frac{1}{T}\sum_{x\in\mathcal{B}}\mathbb{1}\{\text{argmax
}p(x)=i\}\end{aligned}\]</span></p>
<p><span class="math inline">\(P_i\)</span>
表示整个batch每个token分配给第<span class="math inline">\(i\)</span>
个expert的概率的总和，这个可导</p>
<p><span class="math display">\[\begin{aligned}P_i=\frac{1}{T}\sum_{x\in\mathcal{B}}p_i(x).\end{aligned}\]</span></p>
<p>这个损失的设计其实和GShard中的也是一样的。</p>
<p>在完美平均分配的情况下，<span class="math inline">\(f\)</span> 和
<span class="math inline">\(P\)</span> 这两个向量都是 <span class="math inline">\(1/N\)</span>，这个时候负载均衡损失是最小的。</p>
<p><span class="math inline">\(\alpha\)</span>
扫描了1e-5到1e-1，发现设为1e-2，已经足够大保持负载平衡，同时不过分影响模型收敛。</p>
<p>观察到 <span class="math inline">\(\sum_{i=1}^N(f_i\cdot
P_i)=\sum_{i=1}^N(\frac1N\cdot\frac1N)=\frac1N\)</span>，所以负载均衡loss还乘了个
<span class="math inline">\(N\)</span>，这样可以保持无论使用多少个expert，在平均分配的情况下，loss都能保持相同的常数。</p>
<h2 id="实验-1">实验</h2>
<ol type="1">
<li>一些训练的trick</li>
</ol>
<p>（1）选择性地使用bf16</p>
<p>半精度训练会带来一些训练的不稳定。因此选择性地使用bf16，具体来说，routing
function内部使用单精度，其他部分使用半精度，这样既不影响通讯，也能提高效果。</p>
<p>为什么选择在routing提高精度？因为softmax对误差特别敏感，exponential计算会极大放大输入中的rounding
error，因此高精度对routing很重要。</p>
<p>（2）较小的参数初始化</p>
<p>从截断正态分布中抽取元素来初始化的模型参数，平均值 <span class="math inline">\(\mu=0\)</span>，标准差<span class="math inline">\(\sigma=\sqrt{s}/n\)</span>，其中s是超参，n是权重张量中的输入单元数量（e.g.
fan-in）。</p>
<p>论文建议将默认的Transformer初始化尺度s=1.0减少10倍。这个方案在实验中既提高了质量又降低了训练不稳定性的可能性。初始化实验对比如下表</p>
<img src="/44e38c1b/switch_transformer_init.png" class title="初始化对比">
<p>（3）增大dropout</p>
<p>由于Switch
Transformer参数量很大，在微调的时候更容易过拟合，因此一个简单的方法就是增大dropout，效果如下</p>
<img src="/44e38c1b/switch_transformer_dropout.png" class title="dropout效果">
<p>可以看到大的dropout有效果，并且dense层保持0.1，只有expert层增大dropout效果更好。</p>
<ol start="2" type="1">
<li>scaling</li>
</ol>
<p>对Switch Transformer结构预训练的scaling做了一些实验。</p>
<p>（1）Step-Basis</p>
<p>首先是验证在固定训练step的条件下，增大expert数量带来的提升，如下图所示。</p>
<p>左边是不同规模的模型在相同step下收敛的结果，可以看到在保持相同计算量的条件下，只通过增大专家数量来提升规模，就有明显的收益。右边则展示训练过程中，不同规模的模型在各个step下的效果。</p>
<img src="/44e38c1b/switch_transformer_scaling_step.png" class title="step scaling">
<p>（2）Time-Basis</p>
<p>虽然Switch
Transformer可以保持计算量不变的情况下提升模型规模，但是专家数量的增多会带来额外的通讯成本，所以即使训练的step数相同，实际的训练时间也不同。因此这里要回答的问题是，给定一个固定的训练时长，Switch
Transformer是否相比dense模型仍有收益。</p>
<p>答案是肯定的。下图展示以训练时长为横轴，Switch
Transformer和dense模型的效果对比。Switch
Transformer收敛到dense模型最终效果的时间只有dense模型的1/7。</p>
<img src="/44e38c1b/switch_transformer_scaling_time.png" class title="time scaling">
<p>（3）和更大的dense模型对比</p>
<p>前面Switch
Transformer和dense模型的比较，是基于相同计算量的前提。那么Switch
Transformer是否具备超越更大规模dense模型的能力？</p>
<p>下图在Step-Basis和Time-Basis对比了64个专家的Switch
Transformer和T5-Large。无论是相同step还是相同时间下，Switch
Transformer都有明显优势。</p>
<img src="/44e38c1b/switch_transformer_scaling_dense.png" class title="dense对比">
<ol start="3" type="1">
<li>SFT效果对比</li>
</ol>
<p>在GLUE和SuperGLUE等下游任务上微调，和dense模型对比。</p>
<p>对于各个模型，每两百步进行一次eval，选最好的效果，尽量保证公平。结果如下表，大部分任务都有明显的提升。</p>
<img src="/44e38c1b/switch_transformer_sft_result.png" class title="sft对比">
<ol start="4" type="1">
<li>模型蒸馏</li>
</ol>
<p>虽然Switch
Transformer在相同计算量下效果更好，但是部署几百B甚至T级别的模型，还是不太方便，因此考虑把稀疏模型蒸馏到dense模型上来进行推理。</p>
<p>论文中给出了几个蒸馏的技巧：<br>
- 初始化的时候，把Switch
Transformer模型中的非稀疏部分用于初始化dense模型<br>
- 蒸馏所用的label，25%来自教师模型，75%来自ground truth，加权求和</p>
<p>预训练模型的蒸馏效果如下，相比无蒸馏训练的dense模型，把同样计算量的稀疏模型蒸馏到dense模型，dense模型大约能获得Switch
Transformer提升部分30%的增益。</p>
<img src="/44e38c1b/switch_transformer_distill.png" class title="蒸馏">
<p>更进一步，用不同规模的稀疏模型下进行蒸馏，结果如下表，可以实现高达99%的压缩率</p>
<img src="/44e38c1b/switch_transformer_distill_diff_model.png" class title="蒸馏">
<p>除了预训练模型，微调模型也可以蒸馏，效果如下，在SuperGLUE也有一定的提升</p>
<img src="/44e38c1b/switch_transformer_distill_sft.png" class title="sft蒸馏">
<h1 id="glam">GLaM</h1>
<ol type="1">
<li>简介</li>
</ol>
<p>2021年12月Google发表了《GLaM: Efficient Scaling of Language Models
with
Mixture-of-Experts》，训练出最大参数量为1.2T，每层包含64个专家，每个token激活参数量为96.6B的MoE模型。</p>
<p>相比Switch Transformer，GLaM的训练数据量要大得多，达到了1.6T
token。</p>
<p>下表是论文中给出的，当时一些大规模模型的对比</p>
<img src="/44e38c1b/glam_related_model.png" class title="glam和相关模型">
<p>虽然模型总参数量比GPT-3（175B）大很多，但是训练成本却比GPT-3低很多，推理速度也更快，而且在多个NLP任务上的效果都超越了GPT-3，如下所示。</p>
<img src="/44e38c1b/glam_compare_gpt3.png" class title="glam和gpt3对比">
<img src="/44e38c1b/glam_compare_gpt3_2.png" class title="glam和gpt3对比">
<ol start="2" type="1">
<li>模型设计</li>
</ol>
<p>模型设计上，和Switch
Transformer一样，每两层把一个FFN替换成MoE层。但是和Switch
Transformer不同，GLaM用回了每次激活两个expert的方案，模型结构如下图。</p>
<img src="/44e38c1b/glam_model.png" class title="glam模型">
<p>除此之外，模型在结构上海做了一些其他改动：</p>
<p>（1）位置编码</p>
<p>使用XLNET的相对位置编码。</p>
<p>（2）激活函数</p>
<blockquote>
<p>In the non-MoE Transformer feed-forward sub-layers, we replace the
first linear projection and the activation function with the Gated
Linear Unit，which computes the component-wise product of two linear
transformation of the input, followed by a Gaussian Error Linear
Unit.</p>
</blockquote>
<ol start="3" type="1">
<li>实验</li>
</ol>
<p>训练中的一些trick：</p>
<p>（1）参考《Lingvo: a modular and scalable framework for
sequence-to-sequence
modeling》，在梯度出现NaN或者Inf的时候就跳过那一步更新。</p>
<p>（2）如果在BP更新的时候遇到NaN或者Inf，则重新加载更早的checkpoint并跳过有问题的数据来避免NaN或者Inf。</p>
<p>论文训了一系列模型来探索MoE，这些模型的设置如下表</p>
<img src="/44e38c1b/glam_family.png" class title="glam模型系列">
<p>GLaM和dense模型的评测结果如下</p>
<img src="/44e38c1b/glam_perf.png" class title="glam模型效果">
<p>可以看到GLaM MoE的有效参数效率一致高于dense模型。</p>
<h1 id="st-moe">ST-MoE</h1>
<p>2022年2月，Google发表了《ST-MOE: DESIGNING STABLE AND TRANSFERABLE
SPARSE EXPERT
MODELS》。ST-MoE可以说不仅仅是一个MoE工作，对于模型结构、工程实现、训练策略等都做了很多分析，可以说是MoE的必读论文。</p>
<p>ST-MoE最大模型包含269B总参数量，和与32B
dense模型相当的激活计算量。论文中把模型称为称为Stable Transferable
Mixture-of-Experts，或者ST-MoE-32B。</p>
<p>在MoE层的使用上，ST-MoE比Switch
Transformer更“节省”一点，每四层才替换1个MoE层。</p>
<p>论文中主要训了两个规模的ST-MoE模型，分别有4B和269B的总参数量。ST-MoE以及其他用于对比的模型参数如下表</p>
<img src="/44e38c1b/st_moe_models.png" class title="ST-MoE模型及对比模型的参数">
<h2 id="稳定性与效果分析">稳定性与效果分析</h2>
<p>论文通过对乘性操作、噪音和裁剪这几个内容进行探索，来指导模型的设计。</p>
<ol type="1">
<li>乘性操作对模型稳定性和效果的影响</li>
</ol>
<p>论文首先研究了乘性操作对模型的训练稳定性和最终效果的影响。</p>
<p>之前已经有一些工作表明更多的乘法对模型效果有收益。</p>
<blockquote>
<p>Some architectural improvements involve more multiplications than
additions or do not sum many items at once</p>
</blockquote>
<p>（1）GELU Gated Linear Units (GEGLU)</p>
<p>第一个例子是关于激活函数的。GLU是一个对两个输入向量进行component-wise相乘的操作，之后被扩展成GELU-Linear
FFN变体，用于替换transformer中的ReLU FFN变体，其计算如下</p>
<p><span class="math display">\[\begin{aligned}FFN_{GEGLU}(x,W,V,b,c)=GELU(xW+b)\odot(xV+c)\end{aligned}\]</span></p>
<p>这样在一些其他工作里已经被证明了对模型效果有提升。</p>
<p>（2）RMSNorm</p>
<p>第二个例子是RMSNorm中的缩放参数，也就是下面公式的 <span class="math inline">\(g\)</span>。</p>
<p><span class="math display">\[y_i=\frac{x_i}{\sqrt{\frac1d\sum_{i=1}^dx_i^2}}\cdot
g_i\]</span></p>
<p>ST-MoE针对GEGLU和RMSNorm这两个乘性操作，做了实验，结果如下表。</p>
<img src="/44e38c1b/st_moe_remove_multiplications.png" class title="移除乘法操作的影响">
<p>发现移除乘性操作可以使模型稳定性更好（训练中发散的情况减少），但是最终效果变差了。</p>
<p>（3）增加dense层</p>
<p>ST-MoE还验证了在expert层增加更多dense层的效果。结果发现增加更多的乘法交互（增加dense层），可以在带来效果收益的同时，基本不影响推理速度，如下表所示。</p>
<img src="/44e38c1b/st_moe_more_dense_layer.png" class title="更多的dense层">
<p>（4）增加一个bias</p>
<p>在FFN层的第一个矩阵乘法后面增加一个可学习的bias
B，分别通过加法和乘法加入</p>
<p><span class="math display">\[\text{FFN}_{\text{GEGLU}}+\text{Add
Bias}(x)=[(\text{GELU}(xW_{11})\odot xW_{12})+B]W_2\]</span></p>
<p><span class="math display">\[\mathrm{FFN}_{\mathrm{GEGLU}}+\mathrm{Mult~Bias}(x)=[(\mathrm{GELU}(xW_{11})\odot
xW_{12})\odot B]W_2\]</span></p>
<p>乘法的收敛速度更快，效果也更好。</p>
<p>上面这些实验显示，后续在模型效果的探索方向可以往多使用乘性操作去考虑。</p>
<ol start="2" type="1">
<li>noise对模型稳定性和效果的影响</li>
</ol>
<p>接下来ST-MoE探索了“噪音可以提升模型稳定性”的假设。</p>
<p>通过input-jitter，给router的输入logits乘以一个在[1e-2,
1e2]之间的均匀随机变量来添加噪音。</p>
<img src="/44e38c1b/st_moe_more_add_noise.png" class title="增加noise">
<p>结果是增加noise之后，有助于让模型的收敛更加稳定，但是对模型最终效果有负面影响。</p>
<p>这里论文还提到，小模型上的结果不一定能直接推广到更大的模型上，比如在小模型上稳定的配置，在大模型就可能就不稳定了。因此还是需要在大模型上也进行充分实验。</p>
<ol start="3" type="1">
<li>限制激活值和梯度值对模型稳定性和效果的影响</li>
</ol>
<p>对activation和gradient进行限制是目前广泛应用的提升模型训练稳定性的手段。在反向传播过程中，通过裁剪梯度的范数来缓解梯度爆炸，就是一种常用的限制手段。</p>
<p>但是在ST-MoE训练269B的大规模模型时，发现裁剪会使得模型收敛的效果很差。</p>
<p>为了解决这个问题，ST-MoE在训练中引入了router z-loss，形式如下。</p>
<p><span class="math display">\[L_z(x)=\frac{1}{B}\sum_{i=1}^B\left(\log\sum_{j=1}^Ne^{x_j^{(i)}}\right)^2\]</span></p>
<p><span class="math inline">\(B\)</span> 是token的数量，<span class="math inline">\(N\)</span> 是专家数，<span class="math inline">\(x\in\mathcal{R}^{B\times N}\)</span>
是router的输入。</p>
<p>z-loss会对进入router的较大的logits值进行惩罚，以达到尽量减少进入指数函数的较大误差的目的。什么意思呢？后面来解释，先看下使用z-loss的效果。</p>
<img src="/44e38c1b/st_moe_z_loss_result.png" class title="z-loss效果">
<p>ST-MoE认为，在模型训练过程中，由于精度不足或者其他问题，会产生很大的值，从而引入误差。而对梯度进行裁剪是在误差发生之后，并且裁剪本身也造成了数据的不连续性，某种程度上，裁剪本身也是一种误差。相反地，z-loss自然地鼓励模型产生较小的对数值，因此可以更精确地建模。</p>
<p>z-loss乘以一个权重超参 <span class="math inline">\(c_z\)</span>
加入到模型训练的总损失中，如下式所示。</p>
<p><span class="math display">\[L_{tot}=L_{CE}+c_BL_B+c_zL_Z\]</span></p>
<p>ST-MoE经过实验，选择了<span class="math inline">\(c_z=0.001\)</span>。</p>
<p><span class="math inline">\(L_B\)</span> 是 auxiliary load balance
loss负载均衡损失，ST-MoE这里使用了和GShard/Switch
Transformer所用的相同的损失计算，这里回顾一下：</p>
<p><span class="math display">\[\begin{aligned}\text{loss}&amp;=\alpha\cdot
N\cdot\sum_{i=1}^Nf_i\cdot P_i\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}f_i=\frac{1}{T}\sum_{x\in\mathcal{B}}\mathbb{1}\{\text{argmax
}p(x)=i\}\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}P_i=\frac{1}{T}\sum_{x\in\mathcal{B}}p_i(x).\end{aligned}\]</span></p>
<p><span class="math inline">\(N\)</span> 是专家数， <span class="math inline">\(\mathcal{B}\)</span>是包含 <span class="math inline">\(T\)</span> 个token的batch。<span class="math inline">\(f_{i}\)</span> 表示被分配到第 <span class="math inline">\(i\)</span> 个expert的token数，这个不可导；<span class="math inline">\(P_i\)</span> 表示整个batch每个token分配给第<span class="math inline">\(i\)</span> 个expert的概率的总和，这个可导。</p>
<ol start="4" type="1">
<li>数据精度对训练效率和训练效果的影响</li>
</ol>
<p>目前大部分的大模型训练都使用混合精度训练：模型权重以float32格式存储以进行梯度更新，然后在正向和反向传播的矩阵乘法中转换为bfloat16；此外，所有激活值都以bfloat16存储和操作，而allreduce通信可以在bfloat16或float32数值精度中进行。</p>
<p>对于ST-MoE-32B的训练，allreduce的数值使用半精度可以加速训练，然而这也会使训练变得不稳定，因此ST-MoE保持allreduce的数值精度为float32。</p>
<p>bfloat16和float32在不同范围的舍入误差如下表所示</p>
<img src="/44e38c1b/st_moe_round_error.png" class title="bf16精度损失">
<p>可以看到，表达的数值越大，舍入误差越大。而z-loss限制了数值大小，也就将误差值限制在比较小的范围。</p>
<p>MoE模型天生对舍入误差敏感，因为它们由于router的使用而有更多的指数函数，而指数函数会将小的输入误差放大很多，这就加剧舍入误差所导致的训练不稳定。</p>
<p>另外，ST-MoE有一个策略：只有当排第二的专家的权重大于等于第一的专家的1/5时，token才会被路由到其第二位专家，否则第二个专家就会被忽略。</p>
<p>因此虽然舍入误差不会改变softmax运算中各个概率的排序，但它确实会影响MoE中第二个专家的激活。</p>
<h2 id="模型设计-2">模型设计</h2>
<p>dense模型的设计有scaling
law进行指导，但是MoE模型的设计比dense模型多出几个要考虑的点：</p>
<p>（1）使用多少个expert</p>
<p>（2）怎么routing</p>
<p>（3）专家容量系数怎么定</p>
<p>（4）硬件的影响</p>
<p>（这里提到MoE模型的scaling law工作：《Unified scaling laws for routed
language models》，可以了解一下）</p>
<ol type="1">
<li>使用多少个expert</li>
</ol>
<p>ST-MoE认为，从以往的经验来看，在总专家数量较少的情况下（如8/16/32），提升专家数量，能有收益。但是在特别稀疏的情况下（如激活专家数量&lt;1%），或者总专家数较大（比如&gt;256）之后，提升专家数量收益就很小了。</p>
<p>从另一个角度来看，如果一个计算核心使用&gt;1个专家，那么就会出现比较大的加载参数张量的成本，因此建议每个计算核心使用&lt;=1个专家。</p>
<ol start="2" type="1">
<li>routing和capacity factor</li>
</ol>
<p>论文做了一系列实验来探索capacity factor的选择，如下表所示</p>
<img src="/44e38c1b/st_moe_capacity_factor.png" class title="capacity factor">
<p>从这些实验中得到几个结论：</p>
<p>（1）训练和推理的capacity factor增大都会有收益</p>
<p>（2）如果硬件资源足够，推理的capacity
facotr可以设得比训练的时候大，会有进一步提升</p>
<p>（3）激活的expert数量提升会有收益，但是收益随着capacity
factor提升而越来越小</p>
<p>当然，选择capacity
factor还要看硬件的特性，如果通讯很快，可以适当增大capacity
factor，否则就不能选择太大的。</p>
<p>下表展示了不同capacity factor对推理速度的影响</p>
<img src="/44e38c1b/st_moe_capacity_factor_speed.png" class title="不同capacity factor推理速度">
<h2 id="实验-2">实验</h2>
<ol type="1">
<li>ST-MoE效果</li>
</ol>
<p>ST-MoE-32B在下游任务上和以往最佳结果对比如下表，ST-MoE-32B刷新了超过一半任务的最佳效果</p>
<img src="/44e38c1b/st_moe_perf.png" class title="不同capacity ST-MoE-32B效果">
<ol start="2" type="1">
<li>Expert Specialization</li>
</ol>
<p>论文还对各个专家的专业化进行了追踪，发现decoder中几乎没有专业化的迹象，各种类型的token近乎随机分配给不同的专家。而在encoder中则表现出了高度专业化的特征，如下表</p>
<img src="/44e38c1b/st_moe_encoder_specialization.png" class title="encoder专业化">
<p>此外，还发现在多语言的模型的encoder中，专业化的情况并不想原先预想那样，按不同语言划分，而是每个专家都会处理一种语言的一部分token，如下表</p>
<img src="/44e38c1b/st_moe_multiling_specialization.png" class title="多语言专业化">
<h1 id="deepseekmoe">DeepseekMoE</h1>
<p>2024年1月，幻方量化开源了DeepseekMoE，是国内首个开源的MoE大模型。幻方还发布了论文《DeepSeekMoE:
Towards Ultimate Expert Specialization in Mixture-of-Experts Language
Models》，给出了一些DeepSeekMoE的细节内容，颇为实在了。</p>
<p>DeepSeekMoE在其他MoE工作的基础上，进一步给出了2个模型设计的主要思路：</p>
<p>（1）对expert的粒度进行细分，以提供更多样的expert激活组合；</p>
<p>（2）对expert的类型进行区分，从所有expert中保留一部分作为shared
expert共享专家，这部分专家对所有输入都保持激活。</p>
<p>这样的做法可以帮助每个expert达到更高程度的专业化(specialization)的水平，更好地学习不同的专业知识。</p>
<p>DeepSeekMoE先在2B的较小MoE模型上进行了充分的实验，然后把方案应用到16B参数的MoE模型上，并获得了较好的效果。其中DeepSeekMoE-16B不需要量化就可以在40GB显存的设备上运行。</p>
<p>DeepSeekMoE-2B模型具有和稠密2B模型相当的性能，而DeepSeekMoE-16B则具有和7B稠密模型相当的性能，且计算量仅为稠密模型的40%。</p>
<p>DeepSeekMoE-16B的参数效率相比稠密模型有明显的优势，如下图所示</p>
<img src="/44e38c1b/ds_moe_perf.png" class title="deepseek moe">
<p>并且DeepSeekMoE-2B和16B模型都开源了。</p>
<p>在前面实验的基础上，幻方还训练了DeepSeekMoE-145B的超大MoE模型，具有和稠密的DeepSeek-67B模型相当的表现，但计算量更小。这个后续也有机会放出来。</p>
<h2 id="模型设计-3">模型设计</h2>
<p>MoE，mixture of
expert，顾名思义，一个最初始的motivation就是让不同expert学习不同的内容，然后再混合起来。</p>
<p>比如最上面提到的1991年的工作里，就是让不同的expert学习不同的元音特征，以此提升特征提取的准确率。</p>
<p>但是当前大部分的MoE架构都会遇到“knowledge hybridity”和“knowledge
redundancy”的问题，即知识的杂糅和冗余：</p>
<p>（1）知识冗余</p>
<p>有些基础的常识在不同的领域都需要用到，每个expert就都会学一点，这样这些常识就被多个expert重复学习了。</p>
<p>（2）知识杂糅</p>
<p>在expert数量不够多的情况下，一个expert就可能要负责学习多个领域的内容。以学习高中知识为例，在只有两个expert的时候，只能一个expert学习理科知识，另一个学习文科知识；当我们有8个expert的时候，不同expert就可以分别学习语文、英语、历史、地理、物理、生物、化学、数学知识。显然后者所学知识的专业化程度更高。</p>
<p>知识的杂糅和冗余阻碍了专家专业化(expert
specialization)的程度，也就阻碍了模型达到MoE结构理论上限性能。</p>
<p>我们期望每个expert能够学习到non-overlap &amp; foucusd
knowledge的知识。</p>
<p>针对上面的问题，DeepSeekMoE的架构设计有2个主要策略：</p>
<p>（1）Fine-Grained Expert Segmentation</p>
<p>参数总量不变的情况下，将expert分成更细的粒度（每个expert更小）。这样可以带来更灵活的激活组合，让每个expert可以有更强的specialization。比如原本是16个expert选择激活2个，那么总的组合数是120种；如果把每个expert缩小为原来的1/4，那在总参数量和激活数量不变的情况下，是64个expert选择激活8个，那么总的排列组合数就是
<span class="math inline">\(\binom{64}8=4,426,165,368\)</span>
，排列组合数比原来多了很多。</p>
<p>（2）Shared Expert Isolation</p>
<p>把部分expert分离出来，保持永远激活。我们期望这部分专家能够学到在多个领域间都通用的common
knowledge。这样的策略同样可以使得其他expert能够提高专业化的程度，并且减少不同expert间的知识冗余。还是以学习高中知识为例，数学、物理和化学都需要算术能力，如果让学这三个领域的expert都学习算术技能，就会有冗余；我们可以把通用算术的技能剥离出来，由一个助手专门负责算术任务，相当于给他们发了一个计算器，这样学习数学、物理和化学的expert就能把更多的精力放在专业知识上，也就能达到更好的专业化效果。</p>
<p>下图展示了在传统MoE结构上增加Fine-Grained Expert Segmentation和Shared
Expert Isolation策略的设计</p>
<img src="/44e38c1b/ds_moe_structure.png" class title="deepseek moe 结构">
<p>（expert isolation的思路最早可以追溯到2022年1月发表的《DeepSpeed-MoE:
Advancing Mixture-of-Experts Inference and Training to Power
Next-Generation AI Scale》，这里就不展开了。）</p>
<p>假设传统的MoE模型每层的expert数量为 <span class="math inline">\(N\)</span>，激活expert数为 <span class="math inline">\(K\)</span>，DeepSeekMoE使用的细粒度expert大小为原来的
<span class="math inline">\(1/m\)</span>，那DeepSeekMoE每层就有 <span class="math inline">\(mN\)</span> 个expert，激活的expert数量为 <span class="math inline">\(mK\)</span>个。假设 <span class="math inline">\(T\)</span> 为输入长度，<span class="math inline">\(L\)</span> 为模型层数，<span class="math inline">\(e_i^l\)</span> 表示第 <span class="math inline">\(i\)</span>
个expert，DeepSeekMoE可以公式化为以下表示（忽略了layernorm）</p>
<p><span class="math display">\[\mathbf{u}_{1:T}^l=\text{Self-Att}\left(\mathbf{h}_{1:T}^{l-1}\right)+\mathbf{h}_{1:T}^{l-1}\]</span></p>
<p><span class="math display">\[\mathbf{h}_t^l=\sum_{i=1}^{mN}\left(g_{i,t}\text{
FFN}_i\left(\mathbf{u}_t^l\right)\right)+\mathbf{u}_t^l\]</span></p>
<p><span class="math display">\[g_{i,t}=\begin{cases}s_{i,t},&amp;s_{i,t}\in\text{Topk}(\{s_{j,t}|1\leqslant
j\leqslant mN\},mK)\\0,&amp;\text{otherwise,}\end{cases}\]</span></p>
<p><span class="math display">\[s_{i,t}=\mathrm{Softmax}_i\left({\mathbf{u}_t^l}^T\mathbf{e}_i^l\right)\]</span></p>
<h2 id="负载均衡-2">负载均衡</h2>
<p>如之前工作反复提及的，如果任由MoE模型自主学习gating，可能会遇到两个问题</p>
<p>（1）routing
collapse：专家分配的不均衡，也就是gating倾向于总是选择特定的少量expert，并且这种情况还会自我增强。</p>
<p>（2）计算效率问题：多设备间，不平衡的负载可能会成为计算效率的瓶颈。</p>
<p>针对routing collapse的问题，DeepSeekMoE引入一个expert-level balance
loss，如下所示</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{L}_{\mathrm{ExpBal}}&amp; =\alpha_1\sum_{i=1}^{N&#39;}f_iP_i
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
f_{i}&amp;
=\frac{N^{\prime}}{K^{\prime}T}\sum_{t=1}^T\mathbb{1}(\text{Token
}t\text{ selects Expert }i)
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
P_{i}&amp; =\frac1T\sum_{t=1}^Ts_{i,t}
\end{aligned}\]</span></p>
<p><span class="math inline">\(\alpha_1\)</span> 叫做expert-level
balance factor，是人工设定的超参。</p>
<p>而 <span class="math inline">\(f_i\)</span> 和 <span class="math inline">\(P_i\)</span> 和Switch
Transformer里的设定基本一样。</p>
<p>在Switch Transformer里， <span class="math inline">\(f_i\)</span>
表示分配到第 <span class="math inline">\(i\)</span>
个expert的token数量。在DeepSeekMoE这里也是一样的含义，只是多乘了一个系数
<span class="math inline">\(N&#39;/K&#39;\)</span> ，其中 <span class="math inline">\(N&#39;=mN-K_s\)</span>，<span class="math inline">\(K&#39;=mK-K_s\)</span>，<span class="math inline">\(K_s\)</span>
是划分出来的共享expert的数量。这个系数是个常数，可以拿到求和符号外面，这样DeepSeekMoE里的
<span class="math inline">\(f_i\)</span> 就和Switch
Transformer里的完全一样了。</p>
<p><span class="math inline">\(N&#39;/K&#39;\)</span>
这个系数可以使得在使用不同的数量的expert时，在完美平均分配的情况下，负载均衡loss都是相同的常数。</p>
<p><span class="math inline">\(P_i\)</span> 表示所有每个token分配给第
<span class="math inline">\(i\)</span> 个expert的权重的总和，和Switch
Transformer里的含义一样。</p>
<p>注意这里 <span class="math inline">\(f_i\)</span> 是不可导的，<span class="math inline">\(P_i\)</span> 是可导的。</p>
<p>针对多设备间负载均衡的问题，DeepSeekMoE引入一个device-level balance
loss，如下所示</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{L}_{\mathrm{DevBal}}&amp; =\alpha_2\sum_{i=1}^Df_i&#39;P_i&#39;
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
f_i^{\prime}&amp; =\frac1{|\mathcal{E}_i|}\sum_{j\in\mathcal{E}_i}f_j
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
P_{i}^{\prime}&amp; =\sum_{j\in\mathcal{E}_i}P_j
\end{aligned}\]</span></p>
<p><span class="math inline">\(\alpha_2\)</span> 叫做device-level
balance factor，是人工设定的超参。</p>
<p><span class="math inline">\(\mathcal{E}_i\)</span> 指第 <span class="math inline">\(i\)</span> 个设备。</p>
<p>device-level balance loss形式上和expert-level balance loss一样，只是
<span class="math inline">\(f_i\)</span> 和 <span class="math inline">\(P_i\)</span>
对应的对象从单个expert变成单个设备了。</p>
<p>当我们的目标是缓解计算瓶颈时，我们不需要强制执行expert间的均匀分配，而只需确保设备之间计算量的平衡。比如我们每层有64个expert，均匀分布在8个设备上，我们只需要每个设备处理的token数平衡即可，在设备内部即使所有token都是同一个expert处理的，依然能满足设备间负载平衡的要求。</p>
<p>相比expert间严格的负载平衡，只要求设备间的平衡是更松的限制条件，这样缓解了因为过度的负载平衡而损害模型性能的问题。</p>
<h2 id="实验-3">实验</h2>
<ol type="1">
<li>小规模模型验证</li>
</ol>
<p>为了验证以上策略的有效性，先拿100B
token的语料数据在DeepSeekMoE-2B模型做实验。词表也是通过BPE在语料上训练的8k词表，后面训练更大规模模型的时候再扩大词表。</p>
<p>DeepSeekMoE-2B模型参数初始化方差为0.006，使用multi-head
attention，前向激活参数量约0.3B，具体参数如下表</p>
<img src="/44e38c1b/ds_model_param.png" class title="模型超参">
<p>relative expert
size指的是DeepSeekMoE所用的细粒度expert的大小和正常FFN层大小的比值。</p>
<p>训练的具体参数设置如下</p>
<center>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">属性</th>
<th style="text-align: center;">数值</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">optimizer</td>
<td style="text-align: center;">AdamW</td>
</tr>
<tr class="even">
<td style="text-align: center;">adam_beta_1</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr class="odd">
<td style="text-align: center;">adam_beta_2</td>
<td style="text-align: center;">0.95</td>
</tr>
<tr class="even">
<td style="text-align: center;">adam_weight_decay</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">warmup schedule</td>
<td style="text-align: center;">linear</td>
</tr>
<tr class="even">
<td style="text-align: center;">warmup step</td>
<td style="text-align: center;">2000</td>
</tr>
<tr class="odd">
<td style="text-align: center;">max lr</td>
<td style="text-align: center;">1.08e-3</td>
</tr>
<tr class="even">
<td style="text-align: center;">dropout</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">sequence length</td>
<td style="text-align: center;">2k</td>
</tr>
<tr class="even">
<td style="text-align: center;">batch size</td>
<td style="text-align: center;">2k</td>
</tr>
<tr class="odd">
<td style="text-align: center;">total step</td>
<td style="text-align: center;">25,000</td>
</tr>
</tbody>
</table>
</center>
<p>其他训练细节：<br>
- 所有expert放在单个GPU上，没有使用device-level balance loss<br>
- expert-level balance factor设为0.01<br>
- 训练到80%的时候，学习率乘以0.316，训练到90%的时候，再乘以0.316</p>
<p>使用相同的100B训练数据，训了DeepSeekMoE-2B，在包含语言模型和下游任务的benchmark上和其他4个模型做对比：dense，hash
layer（也是一种moe，《Hash layers for large sparse models》），Switch
Transformer，GShard。效果对比如下</p>
<img src="/44e38c1b/ds_moe_comparison.png" class title="deepseek moe 效果">
<p>可以得到几个结论：<br>
- 更大的模型参数量和稀疏的架构，使得Hash Layer和Switch
Transformer和具有同样激活参数的dense模型相比，有明显的优势。<br>
- 同样的模型参数下，GSshard比Hash Layer和Switch
Transformer有更多激活参数，效果也更好<br>
- 同样的模型参数和激活参数下，DeepSeekMoE效果比GShard有明显优势。</p>
<p>为了进一步探索DeepSeekMoE架构带来的收益，提升了dense模型和GShard模型的激活参数，直到效果和DeepSeekMoE-2B差不多。</p>
<p>结果dense模型和GShard模型需要分别扩大到16倍和1.5倍的参数量，才能达到DeepSeekMoE-2B相近的效果，如下表所示</p>
<img src="/44e38c1b/ds_moe_upper_bound_2b.png" class title="deepseek moe upper bound">
<p>DeepSeekMoE的优势在更大规模的情况下，依然成立。训了DeepSeekMoE-13B,
对比参数量提升至1.2和1.5倍的GShard，DeepSeekMoE-13B依然能match，具体如下表</p>
<img src="/44e38c1b/ds_moe_upper_bound_13b.png" class title="deepseek moe upper bound">
<ol start="2" type="1">
<li>DeepSeekMoE架构消融实验</li>
</ol>
<p>针对DeepSeekMoE架构的两个主要设计，shared expert和fine-grained
expert进行消融实验。使用不同数量的共享专家和不同粒度的expert进行效果对比，结果如下图。</p>
<img src="/44e38c1b/ds_moe_ablation.png" class title="deepseek moe upper bound 消融实验">
<p>（1）对比蓝色和橙色，可以看到增加共享专家带来了收益</p>
<p>（2）绿色和红色在橙色的基础上进一步把专家颗粒分得更细，效果进一步提升</p>
<p>（3）共享专家和路由专家的比例：在总共64个expert的情况下，对比了1/2/4个共享专家的情况，结果并没有显著差别，在pile上的loss分别是1.808,1.806,1.811。最终选择了共享专家和激活路由专家1:3（2+6）的比例。</p>
<ol start="3" type="1">
<li>expert specialization的分析</li>
</ol>
<p>通过实验来验证DeepSeekMoE中expert specialization的优化。</p>
<p>（1）前面实验看到DeepSeekMoE-2B和1.5倍参数量的GShard模型效果相当。在这个基础上，通过禁用不同数量的top专家，而只能从次优的专家中选择进行回答。</p>
<p>实验结果如下</p>
<img src="/44e38c1b/ds_moe_expert_specialization.png" class title="专家专门化">
<p>发现DeepSeekMoE损失更大，说明DeepSeekMoE每个专家的专业化程度更好，必要性更高。</p>
<p>（2）另外，通过禁用DeepSeekMoE的共享专家，而额外激活一个专家，发现loss也大大提升。这个结果突出了共享专家的关键功能，并表明共享专家捕捉到了与路由专家不共享的基本且重要的知识，使得它无法被路由专家替代。</p>
<p>（3）只激活更少专家，也能和GShard达到相同水平，这一观察结果支持了DeepSeekMoE可以更准确和高效地获取所需知识的观点。</p>
<img src="/44e38c1b/ds_moe_less_activated_expert.png" class title="激活更少专家">
<p>此外还从零训了一个只用1个共享专家和3个激活专家的2b模型（正常是2个共享专家+6个激活专家），也比GShard好，说明DeepSeekMoE的有效参数效率更高</p>
<img src="/44e38c1b/ds_2b_less_expert.png" class title="2B激活更少专家">
<ol type="1">
<li>DeepSeekMoE-16B</li>
</ol>
<p>DeepSeekMoE-16B模型使用了2T数据训练（和LLAMA2-7B对齐）训练，并使用了100k的词表。其他参数如下表所示</p>
<img src="/44e38c1b/ds_model_param.png" class title="模型超参">
<p>论文中提到，除了第一层以外，其他层都使用了MoE层。</p>
<p>第一层不使用MoE是因为观察到第一层的负载均衡loss在训练中收敛得特别慢。</p>
<p>DeepSeekMoE-16B每层有64个专家，其中有2个作为共享专家保持永远激活，加上6个通过gating
function选择激活的，每个token共使用8个专家。每个token会激活16.4B中的2.8B参数。</p>
<p>这里没有把专家的dimension再减小，是因为如果专家太小，计算效率就下降得太厉害。</p>
<p>训练中使用的其他设置：<br>
- lr = 4.2e-4<br>
- 训练进行到80%和90%的时候，lr都会缩小到0.316倍<br>
- batch size = 4.5k，训练窗口长度是4k，因此每个batch有18M
token，2T数据差不多是10.6w步<br>
- 使用了pipeline parallelism</p>
<p>expert level balance
loss的系数设得比较小，0.001，因为实验中发现设得再大并不能进一步优化负载平衡，反而会损害模型效果。</p>
<p>DeepSeekMoE-16B和DeepSeek-7B模型的对比如下</p>
<img src="/44e38c1b/ds_16b_perf_1.png" class title="和DeepSeek-7B对比">
<p>DeepSeekMoE-16B和LLAMA2-7B模型的对比如下</p>
<img src="/44e38c1b/ds_16b_perf_2.png" class title="和LLAMA2-7B对比">
<ol start="5" type="1">
<li>DeepSeekMoE-145B</li>
</ol>
<p>幻方还用245B的token训练了DeepSeekMoE-145B，模型效果上达到DeepSeek-67B的同等水平</p>
<img src="/44e38c1b/ds_moe_145b.png" class title="145b">
<h1 id="dbrx">DBRX</h1>
<p>2024年3月27日，Databricks开源了DBRX，一个拥有有132B参数，激活参数为36B的MoE模型。</p>
<p>结构上，DBRX使用了RoPE、GLU、GQA，采用了fine-grained
expert的设计，每层有16个专家，每个token激活其中4个。相比Mixtral和Grok-1在8个专家中激活2个，DBRX有更多的专家组合方式。</p>
<p>DBRX训练的上下文长度为32k，并使用了12T文本和代码token进行训练。DBRX在3072个H100上完成预训练，加上post-training、效果评估、red-team优化，整个过程耗费3个月时间。</p>
<p>DBRX整体效果超过GPT-3.5，与Gemini 1.0
Pro相当，并且具有比较强的代码能力，甚至超过了在代码上专门优化过的模型，如CodeLLaMA-70B，如下图所示。</p>
<img src="/44e38c1b/dbrx_perf.png" class title="DBRX效果">
<p>推理效率效率上，DBRX也领先于其他模型。</p>
<img src="/44e38c1b/dbrx_infer_efficiency.png" class title="推理效率">
<h1 id="qwen1.5-moe">Qwen1.5-MoE</h1>
<p>2024年3月28日，阿里放出了Qwen1.5-MoE-A2.7B，以2.7B的模型参数，达到了Qwen1.5-7B模型的相近效果。</p>
<p>Qwen1.5-MoE-A2.7B参考了DeepSeekMoE和DBRX的工作，采用了fine-grained
expert的做法，总共有64个专家，每个token激活8个专家，其中有4个为共享专家。</p>
<p>Qwen1.5-MoE-A2.7B使用Qwen-1.8B进行初始化，并在初始化阶段引入随机性，这样可以显著加快收敛速度，并得到更好的收敛结果。</p>
<p>Qwen1.5-MoE-A2.7B和其他模型效果对比如下</p>
<img src="/44e38c1b/qwen1.5_moe_perf.png" class title="Qwen1.5-MoE-A2.7B效果">
<p>虽然Qwen1.5-MoE-A2.7B总参数量较大，但激活的non-embedding参数量远小于7B模型，如下表所示</p>
<img src="/44e38c1b/qwen1.5_moe_params.png" class title="Qwen1.5-MoE-A2.7B参数量">
<p>实践中，Qwen1.5-MoE-A2.7B相比于Qwen1.5-7B，训练成本降低了75%。</p>
<p>推理性能上，在A100-80G用vLLM部署Qwen1.5-7B和Qwen1.5-MoE-A2.7B模型进行了性能测试。</p>
<p>输入/输出token数都设置为1000，输出token数设置为1000，TPS和throughput如下</p>
<img src="/44e38c1b/qwen1.5_moe_tps.png" class title="Qwen1.5-MoE-A2.7B TPS">
<p>虽然MoE模型对内存需求更大，但是由于稀疏激活以及共享专家的设计，但是在速度和吞吐量上都比dense模型更好。Qwen1.5-MoE-A2.7B与Qwen1.5-7B相比，速度提高了约1.74倍。</p>
<h1 id="mistral">Mistral</h1>
<h2 id="mistral-8x7b">Mistral 8x7B</h2>
<p>2023年12月11日，Mistral
AI开源Mistral-8x7B，每个token激活8个专家中的2个。</p>
<p>Mistral-8x7B支持32k推理窗口和多语言，并且代码能力较好。和LLAM2-70B以及GPT-3.5的对比如下。</p>
<img src="/44e38c1b/mistral_8_7b_perf.png" class title="Mistral 8x7B效果">
<p>Mistral-8x7B在大多数任务表现优于LLAM2-70B，且推理速度提高了6倍。</p>
<p>而和激活参数量相近的LLAM2-13B比，优势更为明显</p>
<img src="/44e38c1b/mistral_8_7b_active_perf.png" class title="Mistral 8x7B同样激活参数量下效果">
<h2 id="mistral-8x22b">Mistral 8x22B</h2>
<p>2024年4月17日，Mistral
AI开源Mistral-8x22B模型，一个总参数为141B，激活参数为39B的超大MoE模型。</p>
<p>Mistral-8x22B支持多语言，并且具有较强的数学和代码能力。此外，推理窗口长度也从Mistral-8x7B的32k增加到64k。Mistral-8x22B还具备function
call的能力。</p>
<p>在各个维度的评测结果如下</p>
<img src="/44e38c1b/mistral_8_22b_reasoning.png" class title="Mistral 8x22B reasoning效果">
<img src="/44e38c1b/mistral_8_22b_multiling.png" class title="Mistral 8x22B 多语言效果">
<img src="/44e38c1b/mistral_8_22b_code.png" class title="Mistral 8x22B 代码与数学效果">
<h1 id="小结">小结</h1>
<ul>
<li>现有的工作都表明，MoE模型相比dense模型具有更高的参数效率，即同样的计算量下，MoE模型普遍能有更优的效果<br>
</li>
<li>因此MoE不仅能支持更大规模模型的训练，在较小规模模型上使用MoE架构也有很大收益<br>
</li>
<li>但是相比dense模型，MoE模型的训练也需要考虑更多内容，包括专家数量、激活数量和专家容量的设计，负载均衡的问题，如何在多设备上的并行等，训练难度更大<br>
</li>
<li>结构上，共享专家和细粒度专家目前被验证效果较好<br>
</li>
<li>负载均衡上，GShard和Switch Transformer的负载均衡损失被广泛采用<br>
</li>
<li>推理时需要对底层框架进行优化以适配MoE机制，否则难以发挥MoE的性能优势</li>
</ul>
<hr>
<p>读到这了，来一发点赞收藏关注吧~</p>
<p>博客：<a target="_blank" rel="noopener" href="http://www.linsight.cn/">http://www.linsight.cn/</a><br>
知乎：<a target="_blank" rel="noopener" href="https://www.zhihu.com/people/us4ever">Linsight</a><br>
微信公众号：Linsight<br>
<img src="/images/qrcode.jpg"></p>
<hr>
<p>【往期文章】</p>
<p><a target="_blank" rel="noopener" href="http://www.linsight.cn/44e38c1b.html">MoE模型的前世今生</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/c4da56c0.html">LLM长上下文的问题</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/cc852861.html">解锁大模型长上下文能力</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/3dc22f96.html">理解Attention:从起源到MHA,MQA和GQA</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/41b6a819.html">Yi技术报告-划重点看细节</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/6a40bfa5.html">transformer中normalization的二三事</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/b70b4a2d.html">从代码实现看normalization-到底做了什么</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/c61d17e3.html">稀疏注意力计算:sliding
window attention</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/a051710f.html">理解LLM位置编码:RoPE</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/3345028a.html">大模型算法题(1)</a><br>
<a target="_blank" rel="noopener" href="http://www.linsight.cn/ad0bba9d.html">大模型算法题(2)</a></p>
<hr>
<h1 id="reference">Reference</h1>
<p>【1】Adaptive Mixtures of Local Experts
https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf<br>
【2】Outrageously Large Neural Networks: The Sparsely-Gated
Mixture-of-Experts Layer https://arxiv.org/abs/1701.06538<br>
【3】GShard: Scaling Giant Models with Conditional Computation and
Automatic Sharding https://arxiv.org/abs/2006.16668<br>
【4】Switch Transformers: Scaling to Trillion Parameter Models with
Simple and Efficient Sparsity https://arxiv.org/abs/2101.03961<br>
【5】GLaM: Efficient Scaling of Language Models with Mixture-of-Experts
https://arxiv.org/abs/2112.06905<br>
【6】ST-MoE: Designing Stable and Transferable Sparse Expert Models
https://arxiv.org/abs/2202.08906<br>
【7】DeepSeekMoE: Towards Ultimate Expert Specialization in
Mixture-of-Experts Language Models
https://arxiv.org/abs/2401.06066<br>
【8】Introducing DBRX: A New State-of-the-Art Open LLM
https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm<br>
【9】Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated
Parameters https://qwenlm.github.io/zh/blog/qwen-moe/</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Lin
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://saicat.github.io/44e38c1b.html" title="MoE模型的前世今生">https://saicat.github.io/44e38c1b.html</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i> NLP</a>
              <a href="/tags/transformer/" rel="tag"><i class="fa fa-tag"></i> transformer</a>
              <a href="/tags/LLM/" rel="tag"><i class="fa fa-tag"></i> LLM</a>
              <a href="/tags/MoE/" rel="tag"><i class="fa fa-tag"></i> MoE</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/41b6a819.html" rel="prev" title="Yi技术报告-划重点看细节">
                  <i class="fa fa-angle-left"></i> Yi技术报告-划重点看细节
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/17360081.html" rel="next" title="大模型算法题(3)">
                  大模型算法题(3) <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Lin</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">708k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">21:28</span>
  </span>
</div>
<div class="busuanzi-count">
</div>

<!--
-->


<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.28/fancybox/fancybox.umd.js" integrity="sha256-ytMJGN3toR+a84u7g7NuHm91VIR06Q41kMWDr2pq7Zo=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/fancybox.js"></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"Saicat/comment-utterance","issue_term":"pathname","theme":"github-light"}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
